1992.tmi-1.7,P91-1022,0,0.474028,"of the smallest possible couples. The type of alignment we will be discussing takes the sentence to be the segmentation unit. Figure 1 illustrates such an alignment. Clearly, a corpus of properly aligned bitext constitutes an extremely valuable source of information, not only to researchers in bilingual lexicography and terminology, but also for a range of applications. While producing alignments by hand is extremely time-consuming and requires the skills of individuals with a good knowledge of both languages, there exist programs that produce relatively reliable alignments at a minimal cost [Brown et al. 1991, Gale and Church 1991]. And in fact, for some applications, it is sufficient that the alignment for a given bitext be only partially correct, as long as there is a way of automatically extracting a subset of that bitext for the alignment of which there is a high level of confidence. For other applications however, much can be gained from a program that is capable of producing high-quality alignments for an entire piece of bitext. This is the case for translation revision and evaluation [Isabelle, 1991]. The first of these gains is obvious: to allow one to visualize a text and its translation"
1992.tmi-1.7,J90-2002,0,0.146448,"Missing"
1992.tmi-1.7,P91-1023,0,0.279504,"sible couples. The type of alignment we will be discussing takes the sentence to be the segmentation unit. Figure 1 illustrates such an alignment. Clearly, a corpus of properly aligned bitext constitutes an extremely valuable source of information, not only to researchers in bilingual lexicography and terminology, but also for a range of applications. While producing alignments by hand is extremely time-consuming and requires the skills of individuals with a good knowledge of both languages, there exist programs that produce relatively reliable alignments at a minimal cost [Brown et al. 1991, Gale and Church 1991]. And in fact, for some applications, it is sufficient that the alignment for a given bitext be only partially correct, as long as there is a way of automatically extracting a subset of that bitext for the alignment of which there is a high level of confidence. For other applications however, much can be gained from a program that is capable of producing high-quality alignments for an entire piece of bitext. This is the case for translation revision and evaluation [Isabelle, 1991]. The first of these gains is obvious: to allow one to visualize a text and its translation side-by-side, with exp"
1993.tmi-1.17,C88-1053,1,\N,Missing
1993.tmi-1.17,C90-3044,0,\N,Missing
1993.tmi-1.17,P91-1022,0,\N,Missing
2001.mtsummit-papers.36,J93-2003,0,0.0436816,"we clean up the resulting model by filtering out dubious associations. The motivation behind this process is essentially practical. We do not believe that separating the identification of salient units from their bilingual mapping is a promising approach. It would be much better to look for a translation model which allows ;=<?&gt; associations. Of course, the problem for such an approach is to find a way to cope with the well known malediction of multidimensionality (any group of source words being potentially associated to any target group one). More advanced models such as IBM models 3 to 5 (Brown et al., 1993) which permit * <@; associations may be seen as a step in this direction. More recently, the 2-stage model described by Och (Och and Weber, 98; Och et al., 99) seems to be another alternative — at least in a task comparable to the Verbmobil one — as it allows certain hidden structural information to be captured. best results. An exemple of the output of this process is reported in Table 1 for a pair of sentences from the Hansard corpus. H K L O MNN 9QKRSKTU9 NNP VXW Y[ZV^] Gb c bdfee g[h E_a` ` _Xi jk c qr AaB  D & cl E H Km,nIo,p*&apos; (2) Identifying monolingual salient sequences"
2001.mtsummit-papers.36,langlais-etal-2000-evaluation,1,0.898765,"rtion point. Evaluation An implementation of T RANS T YPE which allows the completion of words was evaluated in two ways. In a theoretical evaluation, a simulated user generates character by character the target part of a test corpus, accepting as soon as it is helpful the first completion provided by T RANS T YPE. It was shown that under this scenario, a user could save about two thirds of the keystrokes needed to produce a translation (Foster et al., 1997). An in-situ evaluation involving ten translators who were asked to translate the same text using T RANS T YPE has also been carried out (Langlais et al., 2000b). Some interesting observations emerged which motivate the present study. Only one translator actually managed to translate faster using T RANS T YPE; this suggests that even in a very simple scenario, target-text mediated interactive translation is at least viable. Lack of training time is probably one reason for these otherwise disappointing results. The fact that real users do not systematically watch the screen when typing may also account for part of the problem. A qualitative survey revealed that most users (actually nine out of ten) liked T RANS T YPE and would be eager to try it in t"
2001.mtsummit-papers.36,W97-0311,0,0.0227295,"nslation (e.g. bill/facture vs bill/projet de loi), lexicons are often the only means for a user to influence the translation engine. As T RANS T YPE is deeply useroriented, we feel it would be a desirable extension to the system if users were allowed to introduce specific lexicons. This extension can be seen as a first step toward an adaptative version of T RANS T YPE, which is a very challenging issue that we hope to study further. Automatic acquisition of lexicons from bilingual corpora Many studies have addressed the problem of automatically acquiring bilingual lexicons (see for instance (Melamed, 1997; Ohomori and Higashida, 1999; Rapp, 1999; Tanaka and Matsuo, 1999; Jacquemin, 99) for recent ones). These studies are by nature difficult if not impossible to compare. Therefore, we investigate a simpler version of the approach described in (Langlais et al., 2000a) that basically involves three steps. First, we identify monolingually salient units using various statistical metrics and/or filters. Second, we group together in our training corpus words which belong to the units selected in the previous step in order to train a new translation model where both words and sequences of words (units"
2001.mtsummit-papers.36,1999.tmi-1.9,0,0.0455546,"bill/facture vs bill/projet de loi), lexicons are often the only means for a user to influence the translation engine. As T RANS T YPE is deeply useroriented, we feel it would be a desirable extension to the system if users were allowed to introduce specific lexicons. This extension can be seen as a first step toward an adaptative version of T RANS T YPE, which is a very challenging issue that we hope to study further. Automatic acquisition of lexicons from bilingual corpora Many studies have addressed the problem of automatically acquiring bilingual lexicons (see for instance (Melamed, 1997; Ohomori and Higashida, 1999; Rapp, 1999; Tanaka and Matsuo, 1999; Jacquemin, 99) for recent ones). These studies are by nature difficult if not impossible to compare. Therefore, we investigate a simpler version of the approach described in (Langlais et al., 2000a) that basically involves three steps. First, we identify monolingually salient units using various statistical metrics and/or filters. Second, we group together in our training corpus words which belong to the units selected in the previous step in order to train a new translation model where both words and sequences of words (units hereafter) are linked across"
2001.mtsummit-papers.36,P99-1067,0,0.0120337,"e loi), lexicons are often the only means for a user to influence the translation engine. As T RANS T YPE is deeply useroriented, we feel it would be a desirable extension to the system if users were allowed to introduce specific lexicons. This extension can be seen as a first step toward an adaptative version of T RANS T YPE, which is a very challenging issue that we hope to study further. Automatic acquisition of lexicons from bilingual corpora Many studies have addressed the problem of automatically acquiring bilingual lexicons (see for instance (Melamed, 1997; Ohomori and Higashida, 1999; Rapp, 1999; Tanaka and Matsuo, 1999; Jacquemin, 99) for recent ones). These studies are by nature difficult if not impossible to compare. Therefore, we investigate a simpler version of the approach described in (Langlais et al., 2000a) that basically involves three steps. First, we identify monolingually salient units using various statistical metrics and/or filters. Second, we group together in our training corpus words which belong to the units selected in the previous step in order to train a new translation model where both words and sequences of words (units hereafter) are linked across languages."
2001.mtsummit-papers.36,P97-1061,0,0.0170624,"emple of the output of this process is reported in Table 1 for a pair of sentences from the Hansard corpus. H K L O MNN 9QKRSKTU9 NNP VXW Y[ZV^] Gb c bdfee g[h E_a` ` _Xi jk c qr AaB  D & cl E H Km,nIo,p*&apos; (2) Identifying monolingual salient sequences Distributional filters The literature abounds in measures that help to decide whether words that happen to co-occur are linguistically significant or not. In this study, we rated the coherence of any sequence of words seen in a training corpus by means of two measures: a likelihood-based one (Dunning, 93) and an entropy-based one (Shimohata et al., 1997). Observing the output produced by these methods, it is immediatly apparent that neither metric guarantees that the best ranked units are those that we would ourselves manually select as salient. In particular, it is clear that many sequences overlap; which further complicating the selection process. For this reason, we applied a cascade filter to remove well-rated but non-salient units. Below, we report on the results of a filtering process (called DIST) which removes any sequence seen only once or having a likelihood ratio lower than 5.0; DIST also removes some sequences that overlap with ot"
2001.mtsummit-papers.36,1999.tmi-1.11,0,0.021485,"cons are often the only means for a user to influence the translation engine. As T RANS T YPE is deeply useroriented, we feel it would be a desirable extension to the system if users were allowed to introduce specific lexicons. This extension can be seen as a first step toward an adaptative version of T RANS T YPE, which is a very challenging issue that we hope to study further. Automatic acquisition of lexicons from bilingual corpora Many studies have addressed the problem of automatically acquiring bilingual lexicons (see for instance (Melamed, 1997; Ohomori and Higashida, 1999; Rapp, 1999; Tanaka and Matsuo, 1999; Jacquemin, 99) for recent ones). These studies are by nature difficult if not impossible to compare. Therefore, we investigate a simpler version of the approach described in (Langlais et al., 2000a) that basically involves three steps. First, we identify monolingually salient units using various statistical metrics and/or filters. Second, we group together in our training corpus words which belong to the units selected in the previous step in order to train a new translation model where both words and sequences of words (units hereafter) are linked across languages. Last but not least, we cl"
2001.mtsummit-papers.36,W99-0604,0,\N,Missing
2001.mtsummit-papers.36,A00-1019,1,\N,Missing
2001.mtsummit-papers.36,P98-2162,0,\N,Missing
2001.mtsummit-papers.36,C98-2157,0,\N,Missing
2003.mtsummit-papers.15,J93-2003,0,0.00571176,"isted, sentences were split at an arbitrary token boundary. For the take2 experiments, however, we decided to simplify this process by discarding segment pairs for which the source sentence was longer than 40 words. 4 Translation Engines The problem of statistical translation can be viewed as an optimization problem where, given a source string F = hf1 , . . . , fj i, a translation model Ptm (·) and a language model Plm (·), we try to find a target string E = he1 , . . . , ei i that maximizes the joint probability Plm (E) ∗ Ptm (F |E). As such, it is an instance of the noisy channel approach (Brown et al., 1993), in which an output signal is ‘decoded’ in order to recover the original input; algorithms which perform this task are known as decoders. The probabilities Plm (·) and Ptm (·) are derived by training on monolingual and bilingual corpora. The language model used in this work was a trigram model trained by an existing in-house package, and the translation models considered were the IBM model 2, also trained using an in-house tool, and IBM model 4, trained using the GIZA++ package (Och and Ney, 2000).4 One important choice for rescoring is how to represent the sets of translations that are outpu"
2003.mtsummit-papers.15,1994.amta-1.10,0,0.0296286,"Missing"
2003.mtsummit-papers.15,P01-1030,0,0.497158,"gov/ speech/tests/mt/resources/. As time quickly passed we rapidly learned the second lesson of this exercise: building a statistical MT engine, even largely from pre-existing components, is by no means straightforward. Several unexpected problems arose which in retrospect are quite interesting. First, training very large models with publicly available packages is feasible only to the extent that the corresponding memory demands can be met (see section 4.1). Second, dealing with multiple decoders inevitably complicates development. However, thanks to the multiple decoder strategy suggested by Germann et al. (2001) we managed to get a decent IBM 4 decoder. Third, we observed that decoding in itself involves a compromise between quality of results and the length of the delay before results are emitted: tuning for performance is a delicate and time-consuming process. The immediate aim of the NIST exercise we participated in was to translate 919 Chinese sentences (length varying from 5 to 101 words, with an average of 29) into English. Additionally, the rescoring strategy required the translation of some 20,000 sentences for the purpose of training the rescoring layer. In the event, slower than expected pr"
2003.mtsummit-papers.15,P98-2158,0,0.279627,"de approximation to the joint probability of those words). This last heuristic was introduced as a normalization in order to moderate the strong influence that frequent words in the training corpus tend to have at translation time. This means that we only compare paths using the same number of source and target words, and whose source words have similar a priori probabilities. 4.2.3 Inverted Alignment Decoder In an attempt to gauge the influence of both decoders and models on the overall translation quality, we also implemented a version of the inverted alignment search algorithm described by Nießen et al. (1998) for IBM 2 models. The basic idea of this method is to expand hypotheses along the positions of the target string while progressively covering the source. The algorithm allows any target word to be aligned to none, one or several consecutive source words (up to a maximum that was set to 3 in the reported experiments); thus, this search accounts for the notion of fertility which is not explicitly captured by IBM 2 models. A hypothesis is fully determined by four parameters: the source and target positions, the source coverage in words and the target word found at the target position. Therefore,"
2003.mtsummit-papers.15,E99-1010,0,0.0211525,"tures. Establishing the limits of the package (maximum input size, etc.) took at least another week of monitoring, excluding computation time. We soon found that using GIZA++ to train a translation model on a corpus of more than one million sentence pairs was impractical. Beyond this point, memory is saturated and the system is forced to swap, drastically increasing the time required. IBM model 4 conditions the distortion probabilities on the class of the centroid source and target words. To acquire these classes, we used the program mkcls.5 In contrast to the bilingual procedure described by Och (1999), for which no ready-made solution was immediately available, this permits only the creation of monolingual classes. Using this program, source and target vocabulary were processed into 50 classes; this required around ten hours of computation. In total, the full training of an IBM model 4 on a corpus of around one million sentence pairs requires two to three days of computation on a 1GB memory Pentium 4 desk computer. 4.2 Multiple Decoders Three different decoders, all previously described in the statistical MT literature, were implemented. One advantage of having several decoders at one’s di"
2003.mtsummit-papers.15,P00-1056,0,0.115271,"int probability Plm (E) ∗ Ptm (F |E). As such, it is an instance of the noisy channel approach (Brown et al., 1993), in which an output signal is ‘decoded’ in order to recover the original input; algorithms which perform this task are known as decoders. The probabilities Plm (·) and Ptm (·) are derived by training on monolingual and bilingual corpora. The language model used in this work was a trigram model trained by an existing in-house package, and the translation models considered were the IBM model 2, also trained using an in-house tool, and IBM model 4, trained using the GIZA++ package (Och and Ney, 2000).4 One important choice for rescoring is how to represent the sets of translations that are output from the base model. There are at least two possibilities: n-best lists— explicit enumerations of the candidate translations—and word graphs (Ueffing et al., 2002), which are capable of storing much larger sets of translations implicitly. Word graphs are potentially more powerful than n-best lists, but they are also more complex to implement. Furthermore, they constrain the rescoring layer to respect the factorization inherent in the graph. Because of these problems, we chose a simple n-best list"
2003.mtsummit-papers.15,P02-1038,0,0.0861279,"in the NIST evaluation was a desire to learn what was really involved in putting together a working statistical MT system. Deadlines appeared reassuringly distant and we had a plan to achieve good performance despite the limitations mentioned in the previous section. We put our hope into a rescoring approach built on top of a roughly state-of-the-art translation model such as IBM Model 4. Rescoring has been extensively used in automatic speech recognition (ASR) on n-best lists or word-graphs (Ortmanns et al., 1997; Rose and Riccardi, 1999), and has more recently been proposed for use in SMT (Och and Ney, 2002; Soricut et al., 2002; Ueffing et al., 2002). The first step was to install the necessary packages, train translation and language models, and begin work on decoders for IBM Model 4. We also designed a rescoring infrastructure that could host any component able to return a score on a source-target sentence pair. Up to this point we were on familiar territory, since the work was based on a clean and well aligned bitext derived in-house from the Canadian Hansard. By the time the training corpora were made available, a good deal of code had been written, if not fully tested. We were confronted b"
2003.mtsummit-papers.15,soricut-etal-2002-using,0,0.0119268,"tion was a desire to learn what was really involved in putting together a working statistical MT system. Deadlines appeared reassuringly distant and we had a plan to achieve good performance despite the limitations mentioned in the previous section. We put our hope into a rescoring approach built on top of a roughly state-of-the-art translation model such as IBM Model 4. Rescoring has been extensively used in automatic speech recognition (ASR) on n-best lists or word-graphs (Ortmanns et al., 1997; Rose and Riccardi, 1999), and has more recently been proposed for use in SMT (Och and Ney, 2002; Soricut et al., 2002; Ueffing et al., 2002). The first step was to install the necessary packages, train translation and language models, and begin work on decoders for IBM Model 4. We also designed a rescoring infrastructure that could host any component able to return a score on a source-target sentence pair. Up to this point we were on familiar territory, since the work was based on a clean and well aligned bitext derived in-house from the Canadian Hansard. By the time the training corpora were made available, a good deal of code had been written, if not fully tested. We were confronted by an obvious corollary"
2003.mtsummit-papers.15,W02-1021,0,0.0431082,"earn what was really involved in putting together a working statistical MT system. Deadlines appeared reassuringly distant and we had a plan to achieve good performance despite the limitations mentioned in the previous section. We put our hope into a rescoring approach built on top of a roughly state-of-the-art translation model such as IBM Model 4. Rescoring has been extensively used in automatic speech recognition (ASR) on n-best lists or word-graphs (Ortmanns et al., 1997; Rose and Riccardi, 1999), and has more recently been proposed for use in SMT (Och and Ney, 2002; Soricut et al., 2002; Ueffing et al., 2002). The first step was to install the necessary packages, train translation and language models, and begin work on decoders for IBM Model 4. We also designed a rescoring infrastructure that could host any component able to return a score on a source-target sentence pair. Up to this point we were on familiar territory, since the work was based on a clean and well aligned bitext derived in-house from the Canadian Hansard. By the time the training corpora were made available, a good deal of code had been written, if not fully tested. We were confronted by an obvious corollary of the statistical MT"
2006.jeptalnrecital-poster.23,J90-2002,0,0.601185,"Missing"
2006.jeptalnrecital-poster.23,2003.mtsummit-papers.15,1,0.841473,"Missing"
2006.jeptalnrecital-poster.23,koen-2004-pharaoh,0,0.14163,"Missing"
2006.jeptalnrecital-poster.23,2005.mtsummit-papers.11,0,0.0436754,"Missing"
2006.jeptalnrecital-poster.23,W05-0820,0,0.0221996,"Missing"
2006.jeptalnrecital-poster.23,2002.jeptalnrecital-long.2,0,0.0726745,"Missing"
2006.jeptalnrecital-poster.23,P00-1056,0,0.156796,"Missing"
2006.jeptalnrecital-poster.23,P02-1038,0,0.0750405,"Missing"
2006.jeptalnrecital-poster.23,N04-1021,0,0.0385281,"Missing"
2006.jeptalnrecital-poster.23,P02-1040,0,0.0928412,"Missing"
2006.jeptalnrecital-poster.23,W05-0822,1,0.856627,"Missing"
2009.mtsummit-papers.2,J93-2003,0,0.011587,",  is the frequency of source phrase  and target phrase  linked to each other. Most of the longer phrases are seen only once in the training corpus. Therefore, relative frequencies overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being em"
2009.mtsummit-papers.2,W05-0801,0,0.014402,"occur; otherwise 0. ability assuming a binomial distribution. Without simplification, this probability can be expressed by: &quot;BC 3,   D 8 81  ED E 8,  81,  ? F G 8  (9) However, because this probability is very small, we follow (Johnson et al., 2007) in computing the negative of the natural logs of the p-value associated with the hypergeometric distribution as the feature functions: HBC ,   0log ∑L &quot;  (10) ,8,  BC Therefore, the higher the value of HBC ,  , the stronger the association between phrase and phrase  . 4) Link probability (Moore, 2005) is the conditional probability of the phrase pair being linked given that they co-occur in a given sentence pair. It is estimated as:   ,  :&quot;,   8 ,  (11) where ,   is the number of times that and  are linked in sentence pairs. 3.3 Phrase-table smoothing As (Foster et al., 2006) shows, the phrase table can be improved by applying smoothing techniques. A motivation for this is our observation that the phrase pairs which co-occur only once: 3,  1 are amazingly frequent in the phrase table even when the training corpus is very large. To compensate for this ov"
2009.mtsummit-papers.2,P03-1021,0,0.00640825,"overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being employed; • Distortion model, which assigns a penalty based on the number of source words which are skipped when generating a new target phrase; • Language model(s) trained using SRILM to"
2009.mtsummit-papers.2,P02-1038,0,0.0597455,"hrases in the target language,  is the source language string, 2 where ,  is the frequency of source phrase  and target phrase  linked to each other. Most of the longer phrases are seen only once in the training corpus. Therefore, relative frequencies overestimate the phrase translation probabilities. To overcome the overestimation problem, lexical weights are introduced. There are various approaches for estimating the lexical weights. The first approach is introSMT Baseline Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard"
2009.mtsummit-papers.2,J03-1002,0,0.011046,"Missing"
2009.mtsummit-papers.2,P02-1040,0,0.0760108,"Missing"
2009.mtsummit-papers.2,P96-1041,0,0.128888,"Missing"
2009.mtsummit-papers.2,P08-1010,0,0.0264371,"Missing"
2009.mtsummit-papers.2,J96-1001,0,0.197505,"Missing"
2009.mtsummit-papers.2,J93-1003,0,0.216972,"Missing"
2009.mtsummit-papers.2,P06-1091,0,0.025513,"Missing"
2009.mtsummit-papers.2,W06-1607,1,0.856654,"Missing"
2009.mtsummit-papers.2,D07-1103,1,0.848548,"imes among N sentencepairs by chance, given the marginal frequencies 3 and 3 . We computed this prob1 There are at least three ways to count the number of co-occurrence of  and  , if one or both of them have more than one occurrence in a given sentence pair (Melamed, 1998). We choose to count the cooccurrence as 1 if they both occur; otherwise 0. ability assuming a binomial distribution. Without simplification, this probability can be expressed by: &quot;BC 3,   D 8 81  ED E 8,  81,  ? F G 8  (9) However, because this probability is very small, we follow (Johnson et al., 2007) in computing the negative of the natural logs of the p-value associated with the hypergeometric distribution as the feature functions: HBC ,   0log ∑L &quot;  (10) ,8,  BC Therefore, the higher the value of HBC ,  , the stronger the association between phrase and phrase  . 4) Link probability (Moore, 2005) is the conditional probability of the phrase pair being linked given that they co-occur in a given sentence pair. It is estimated as:   ,  :&quot;,   8 ,  (11) where ,   is the number of times that and  are linked in sentence pairs. 3.3 Phra"
2009.mtsummit-papers.2,N03-1017,0,0.0126144,"the features to predict the usefulness of a new association feature given the existing features. 1 Introduction Phrase-based translation is one of the dominant current approaches to statistical machine translation (SMT). A phrase translation model, incorporated in a data structure known as a phrase table, is the most important component of a phrasebased SMT system, since the translations are generated by concatenating target phrases stored in the phrase table. The pairs of source and corresponding target phrases are extracted from the word-aligned bilingual training corpus (Och and Ney, 2003; Koehn et al., 2003). These phrase pairs together with useful feature functions, are called collectively a phrase translation model or phrase table. A phrase translation model embedded in a state-of-the-art phrase-based SMT system normally exploits the feature functions involving conditional translation probabilities and lexical weights (Koehn et al., 2003). The phrase-based conditional translation probabilities are estimated from the relative frequencies of the source and target phrase in a given phrase pair. To avoid over-training, lexical weights are used to validate the quality of a phrase translation pair. T"
2009.mtsummit-papers.2,P06-1096,0,0.03176,"Missing"
2009.mtsummit-papers.2,W04-3243,0,0.0167998,"into account. The statistics can be organized in a contingency table, e.g. in Table 1. When collecting the statistics of the data, we only need to count 3,  , 3 , 3  and N; the other counts could be easily calculated accordingly. Then, we may compute the following association features: 1) Dice coefficient (Dice, 1945) as in Equation (7). It compares the co-occurrence count of phrase pair and  with the sum of the independent occurrence counts of and  . 45,  678,  8 98  (7) 2) Log-likelihood-ratio (Dunning, 1993) as in Equation (8) which is presented by Moore (2004). ::; ,  ?78?, ? 3? ,  ? =>   ∑  ?)&,1+  ?)& ,1 + 8?78 ? ∑ (8) where ? and  ? are variables ranging over  1A and & , 1 + respectively, the values @,  3 ? ,  ? is the joint count for the values of ? and  ?, 3?  and 3 ?  are the frequencies of values of ? and  ?. 3) Hyper-geometric distribution is the probability of the phrase-pair globally cooccurring 3,   times among N sentencepairs by chance, given the marginal frequencies 3 and 3 . We computed this prob1 There are at least three ways to count the number of co"
2009.mtsummit-papers.2,C96-2141,0,0.433628,"inear framework (Och and Ney, 2002) by introducing the hidden word alignment variable  (Brown et al., 1993). M ~ ~ e * = arg max(∑m=1 λm H m (~ e , f , a) ) e, a (1) ~ H m (~ e , f , a) are feature functions, and weights λm are typically optimized to maximize the scoring function (Och, 2003). The SMT system we applied in our experiments is Portage, a state-of-the-art phrase-based translation system (Ueffing et al., 2007). The models and feature functions which are employed by the decoder are: • Phrase translation model(s) estimated on the word alignments generated using IBM model 2 and HMMs (Vogel et al., 1996), with standard phrase conditional translation probabilities and lexical weights being employed; • Distortion model, which assigns a penalty based on the number of source words which are skipped when generating a new target phrase; • Language model(s) trained using SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing method (Chen et al, 1996); • Word/phrase penalties. 3 3.1 Phrase Translation Model with Association-based Features Traditional phrase-table features A typical phrase translation model exploits features estimating phrase conditional translation probabilities and lexical"
2009.mtsummit-papers.2,2003.mtsummit-papers.53,0,0.0461404,"Missing"
2009.mtsummit-papers.2,N04-1033,0,0.0438114,"Missing"
2009.mtsummit-papers.2,W04-3227,0,0.0445291,"Missing"
2010.amta-papers.24,N04-4006,0,0.0180521,"The most obvious is to condition other models, for instance the TM, on document context. The methods outlined above could probably be adapted to this relatively easily, provided suitable smoothing techniques were used. A harder challenge is to find better ways of conditioning on the document structure information. One interesting possibility would be to assume a latent hierarchical structure for the features in order to apply recent hierachical adaptation techniques (Finkel and Manning, 2009), suitably modified for multinomial language and translation models, possibly using MAP combinations (Bacchiani et al., 2004). A related idea is to treat the features themselves as hints rather than performing the kind of hard matching used by the methods above. The multiple-feature smoothing methods are a step in this direction, and it would be interesting to apply a similar approach to the whole training corpus, in order to more accurately learn relations between features. Finally, it would be interesting to experiment with other structured domains to determine if the Hansard is an outlier either in its availability of structured information or in the degree to which this is useful for translation. Conclusion And"
2010.amta-papers.24,N03-2002,0,0.0253597,"y depends on an IBM-model training process that degrades badly on small data. For this preliminary effort, we therefore concentrated on the language model. As just mentioned, the challenge in constructing a language model p(w|h, d), where h is an ngram context for word w, is data sparsity due to the conditioning on d. The normal method for dealing with sparsity in h—backing off to shorter contexts (Goodman, 2001)—cannot be applied directly because there is no natural back-off ordering for the features in d. Methods for handling similar situations have been devised in a factored model setting (Bilmes and Kirchhoff, 2003), but these lack straightforward training procedures. We opted instead for two simple solutions: splitting d into its component features di and training a specific model for each; and clustering rare vectors d together to increase the amount of data available for each. 2.1 Feature-Specific Models The training procedure for feature-specific models is extremely simple. For each feature di in d: 1. Partition the target half of the training corpus into sets of sentences characterized by each different value that di can take on. 2. Train an LM on each corpus partition. This yields models p(w|h, dij"
2010.amta-papers.24,N09-1025,0,0.0246649,"pose that a training corpus in which each sentence pair is tagged with document features is available. (Obviously, this will apply only in cases where the training and testing domains are identical or closely related.) Given the typical SMT model structure, there are three main sites for incorporating conditioning on d: the top-level loglinear model1 , the language model (LM), and the translation model (TM). The log-linear model is not ideal for directly capturing dependence on d, since it is trained on a small development set of approximately 1000 sentences: even using a MIRA-like algorithm (Chiang et al., 2009), and assuming simple 1 That is, log p(t|s, d), assumed to be a weighted linear combination of features that can be interpreted as log probabilities. features that connect target words to d, one would have to carefully select only a very small subset of all potential features. The language and translation models can use much larger feature sets, but they also face a sparsity problem in that the number of training examples available for a particular vector d can be arbitrarily small. This is especially severe for the translation model, which lacks the powerful backoff-based smoothing algorithms"
2010.amta-papers.24,W08-0334,0,0.0880097,"mation, our method is related to information-retrieval inspired approaches to domain adaptation (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007), which seek parts of the training corpus that resemble sentences in the current document. However, those approaches pool matches across sentences, and build a single adapted model for each source document, rather than (potentially) one for each sentence. Our method is also different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, and also, unlike all previous work we are aware of, characterize sentences along multiple axes (eg, a sentence might be attributed to a particular speaker and belong to a specific section within a document). Finally, unlike anaphora resolution and discourse analysis applied to translation (Marcu et al., 2000), our method does not explicitly depend on the content of sentences other than the current one. We formalize our notion of document translation in section 2, and present two a"
2010.amta-papers.24,N09-1068,0,0.0362832,"ove, using a linear combination of feature-specific models. 5 There are many possibilities for extending this exploratory work. The most obvious is to condition other models, for instance the TM, on document context. The methods outlined above could probably be adapted to this relatively easily, provided suitable smoothing techniques were used. A harder challenge is to find better ways of conditioning on the document structure information. One interesting possibility would be to assume a latent hierarchical structure for the features in order to apply recent hierachical adaptation techniques (Finkel and Manning, 2009), suitably modified for multinomial language and translation models, possibly using MAP combinations (Bacchiani et al., 2004). A related idea is to treat the features themselves as hints rather than performing the kind of hard matching used by the methods above. The multiple-feature smoothing methods are a step in this direction, and it would be interesting to apply a similar approach to the whole training corpus, in order to more accurately learn relations between features. Finally, it would be interesting to experiment with other structured domains to determine if the Hansard is an outlier e"
2010.amta-papers.24,W07-0717,1,0.950916,"ch different value that di can take on. 2. Train an LM on each corpus partition. This yields models p(w|h, dij ), one for each jth value of each ith feature. These could be used directly for translating test-set sentences characterized by dij , but there is still no guarantee that dij occurs often enough in the training corpus to produce an LM that generalizes well. We therefore smooth using a word-level mixture with a global LM: ps (w|h, dij ) = αij p(w|h, dij ) + (1 − αij )p(w|h). (2) To set mixture weights αij , we used a dynamic smoothing technique similar to dynamic LM domain adaptation (Foster and Kuhn, 2007). First, steps 1 and 2 above are repeated on the source half of the training corpus to produce a set of source-language LMs p0 (w|h, dij ) that correspond one to one with their target-language counterparts. Then, for each dij that occurs in the current source document to 0 is learned usbe translated, a mixture weight αij ing the EM algorithm to maximize the probability of the source sentences tagged with dij , according to the source-language counterpart of (2).2 Finally, the source-side weights are simply transferred to the tar0 → α ) and used in (2). Although this get side (αij ij procedure"
2010.amta-papers.24,2005.eamt-1.17,0,0.025122,"val inspired approaches to domain adaptation (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007), which seek parts of the training corpus that resemble sentences in the current document. However, those approaches pool matches across sentences, and build a single adapted model for each source document, rather than (potentially) one for each sentence. Our method is also different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, and also, unlike all previous work we are aware of, characterize sentences along multiple axes (eg, a sentence might be attributed to a particular speaker and belong to a specific section within a document). Finally, unlike anaphora resolution and discourse analysis applied to translation (Marcu et al., 2000), our method does not explicitly depend on the content of sentences other than the current one. We formalize our notion of document translation in section 2, and present two alternative modeling techniques. Section 3 descri"
2010.amta-papers.24,W07-0711,0,0.0214402,"eing able to share information across partitions. As this does not occur with the all-versusone approach, we also tried mixing over all values of a given feature (ie, over all LMs learned from the training corpus for that feature): ps (w|h, dij ) = X αijk p(w|h, dik ), (3) k P where k αijk = 1, and for notational convenience we designate the global model—included in the mixture—as p(w|h, dij0 ). The weights αijk were learned using the dynamic smoothing procedure above. To counter overfitting due to the large number of parameters, we incorporated MAP smoothing into the EM procedure, following (He, 2007). This uses a modified M-step update: p(x) = [c(x) + P λp0 (x)]/( x c(x) + λ), where p(x) is a mixturecomponent probability, c(x) is a corresponding expected count, and p0 (x) is a prior probability for which we used weights αi learned by pooling all values of feature di in the current source document. The prior weight λ was set to 10, based on preliminary 2 For values of dij that don’t occur in the training corpus, is set to zero. 0 αij experiments with a development set.3 The procedure we have just described constructs a family of smoothed language models ps (w|h, dij ) for feature di that i"
2010.amta-papers.24,2005.eamt-1.19,0,0.0283769,"ccur in an introductory paragraph or in the main body of a text; it may be a quotation or a line spoken by a character in a work of fiction; or it may play a particular rhetorical role within a monograph. All these factors can influence translation, and hence Our approach is to characterize each sentence in a document with a vector of feature values derived from the document’s structure, then translate it with a model optimized for those values. In its use of sentence-level information, our method is related to information-retrieval inspired approaches to domain adaptation (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007), which seek parts of the training corpus that resemble sentences in the current document. However, those approaches pool matches across sentences, and build a single adapted model for each source document, rather than (potentially) one for each sentence. Our method is also different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, a"
2010.amta-papers.24,W07-0733,0,0.0372484,"operate at sentence-level granularity, and are trained on corpus partitions. Unlike us, they modify all components of their log-linear model, they make only a single binary distinction (interrogative versus declarative sentences), and they use a maxent classifier to assign these properties to source sentences rather than relying on document structure. Other relevant work is on domain adaptation. One way of viewing our approach is that it splits the source document into many micro-domains, and attempts to adapt to each. From this viewpoint, recent work on SMT adaptation (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007; Tam et al., 2007) is applicable, in addition to the IR approaches already mentioned. However, most of this work does not deal with adaptation along different axes simultaneously (for instance adapting to topic and genre). An exception is (Matsoukas et al., 2009), who build a model for weighting phrasepair joint counts during relative-frequency TM estimation that can depend on arbitrary features of the training corpus. Their feature weights are set discriminatively using a dev set and do not distinguish between sentences in that set, but it might be possible to extend the ap"
2010.amta-papers.24,W04-3250,0,0.0347196,"r to be slightly better than the log-linear. This may be due to problems with MERT, since the log-linear combinations involve five language model features instead of just one as in all other approaches in tables 3 and 4. Finally, the results for English to French translation appear to be somewhat better than for French to English translation, with the maximum gain over the baseline being approximately 0.5 BLEU points in the former case, and 0.3 BLEU points in the latter. The gains in both cases are statistically significant at the 0.95 level, however, according to paired bootstrap resampling (Koehn, 2004). This also holds for the results from the linear 1-versus-all combination in both translation settings; this technique offers a good combination of performance and efficiency, and is a good candidate for the best approach among the ones tested in this paper. 4 Related Work As mentioned above, we are unaware of any previous work on translating structured documents in SMT. The closest related work is due to Finch and Sumita (2008). Like us, they use models that operate at sentence-level granularity, and are trained on corpus partitions. Unlike us, they modify all components of their log-linear"
2010.amta-papers.24,D07-1036,0,0.0397225,"Missing"
2010.amta-papers.24,A00-2002,0,0.156394,"lso different from previous work that relies on intrinsic surface cues to categorize sentences for translation according to whether they are interrogative or declarative (Finch and Sumita, 2008), or match an ad hoc class (Hasan and Ney, 2005). We rely on extrinsic properties derived from the whole document, and also, unlike all previous work we are aware of, characterize sentences along multiple axes (eg, a sentence might be attributed to a particular speaker and belong to a specific section within a document). Finally, unlike anaphora resolution and discourse analysis applied to translation (Marcu et al., 2000), our method does not explicitly depend on the content of sentences other than the current one. We formalize our notion of document translation in section 2, and present two alternative modeling techniques. Section 3 describes English/French translation experiments with a version of the Canadian Hansard corpus that has rich structural information encoded in XML markup. Related work is discussed in section 4, and section 5 concludes and suggests some possibilities for future work. 2 Document Translation SMT seeks the translation hypothesis tˆ that has highest probability according to a model co"
2010.amta-papers.24,D09-1074,0,0.0119958,"erties to source sentences rather than relying on document structure. Other relevant work is on domain adaptation. One way of viewing our approach is that it splits the source document into many micro-domains, and attempts to adapt to each. From this viewpoint, recent work on SMT adaptation (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007; Tam et al., 2007) is applicable, in addition to the IR approaches already mentioned. However, most of this work does not deal with adaptation along different axes simultaneously (for instance adapting to topic and genre). An exception is (Matsoukas et al., 2009), who build a model for weighting phrasepair joint counts during relative-frequency TM estimation that can depend on arbitrary features of the training corpus. Their feature weights are set discriminatively using a dev set and do not distinguish between sentences in that set, but it might be possible to extend the approach to allow for the sentencelevel dependencies required for handling structural features. We tested both these methods on a corpus of structured Hansard documents, using five features capturing mostly complementary information. Results were mildly positive across the spectrum o"
2010.amta-papers.24,P03-1021,0,0.162969,"λ was set to 10, based on preliminary 2 For values of dij that don’t occur in the training corpus, is set to zero. 0 αij experiments with a development set.3 The procedure we have just described constructs a family of smoothed language models ps (w|h, dij ) for feature di that is specific to the current source document D. To decode a sentence from D, we use the value dij that di takes on for that sentence to select the appropriate member of the family. We tried two ways of combining model families from different document features di : a log-linear combination, with model weights set by MERT (Och, 2003); and a linear combination, with weights set to maximize the likelihood of a target-language development corpus. Both combinations involve one weight per document feature di . 2.2 Clustered Models A weakness of the approach outlined in the previous section is that it implicitly assumes features are independent. Clearly this will not always be the case. Furthermore, many feature vectors occur often enough in the training corpus to allow reliable LMs to be produced. To capitalize on this, we used a simple clustering method that attempts to group lowfrequency vectors together to increase the reli"
2010.amta-papers.24,P07-1066,0,0.0217577,"e trained on corpus partitions. Unlike us, they modify all components of their log-linear model, they make only a single binary distinction (interrogative versus declarative sentences), and they use a maxent classifier to assign these properties to source sentences rather than relying on document structure. Other relevant work is on domain adaptation. One way of viewing our approach is that it splits the source document into many micro-domains, and attempts to adapt to each. From this viewpoint, recent work on SMT adaptation (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007; Tam et al., 2007) is applicable, in addition to the IR approaches already mentioned. However, most of this work does not deal with adaptation along different axes simultaneously (for instance adapting to topic and genre). An exception is (Matsoukas et al., 2009), who build a model for weighting phrasepair joint counts during relative-frequency TM estimation that can depend on arbitrary features of the training corpus. Their feature weights are set discriminatively using a dev set and do not distinguish between sentences in that set, but it might be possible to extend the approach to allow for the sentencelevel"
2010.amta-papers.24,N04-1033,0,0.0313305,"39.71 39.89 39.75 39.62 39.81 39.73 39.65 39.88 39.89 39.87 39.96 Table 3: Results for French to English translation. Feature weights were set using Och’s MERT algorithm (Och, 2003) to maximize dev-set BLEU score. The training corpus was word-aligned using both HMM and IBM2 models; the phrase table consists of the union of phrases extracted from these separate alignments, with a phrase length limit of 7. It was filtered to retain the top 30 translations for each source phrase using the TM part of the current loglinear model. Lexical probabilities were estimated using the method described in (Zens and Ney, 2004). 3.3 Results A preliminary step to generating translation results is clustering the corpus as described in section 2.2. Table 2 shows the results, for various frequencythreshold (f) values (for the 20n value, clustering was run until exactly 20 clusters remained). The results are similar for clustering on the English (for translation into English) and French (for translation into French) sides of the corpus. The largest cluster size is identical for f = 0 (no clustering) and for f = 1k, indicating that it was not merged at this threshold; in fact, the largest 100 or so clusters survived intac"
2010.amta-papers.24,C04-1059,0,0.0369248,"Missing"
2011.iwslt-evaluation.19,P07-1004,0,0.0497399,"Missing"
2011.iwslt-evaluation.19,D08-1076,0,0.0846177,"Missing"
2011.iwslt-evaluation.19,D07-1103,1,0.763628,"Missing"
2011.iwslt-evaluation.19,N03-1017,0,0.0718706,"Missing"
2011.iwslt-evaluation.19,C10-1069,1,0.846539,"Missing"
2011.iwslt-evaluation.19,P05-1074,0,0.0938211,"Missing"
2011.iwslt-evaluation.19,N06-1003,0,0.100229,"Missing"
2011.iwslt-evaluation.19,D10-1064,0,\N,Missing
2011.iwslt-evaluation.19,J93-2003,0,\N,Missing
2011.iwslt-evaluation.19,C96-2141,0,\N,Missing
2011.iwslt-evaluation.19,D09-1040,0,\N,Missing
2011.iwslt-evaluation.19,P02-1040,0,\N,Missing
2011.iwslt-evaluation.19,W06-1607,1,\N,Missing
2011.iwslt-evaluation.19,N04-1033,0,\N,Missing
2011.mtsummit-papers.30,J93-2003,0,0.05186,"SRF = Ȝ1 ×log[c(s,t)]+Ȝ2 ×log[c(s)]+Ȝ3×log[c(t)]? (7) MERT can still choose λ’s that are equivalent to using the two RF estimates. In principle, nothing is lost and a degree of freedom, achieved by dropping the additive constraint, is gained (but this extra degree of freedom may lead to search errors). The obvious objection is that this “3-count” replacement for the two RF features doesn’t model probabilities. However, the inclusion of PRF(t|s) among features can’t be justified probabilistically either. Originally, the objective function for SMT was derived via Bayes’s Theorem as P(s|t)×P(t) (Brown et al., 1993). The inclusion of P(t|s) happened later – it’s a heuristic that defies Bayes’s Theorem (Och and Ney, 2002). Once the forward and backward estimates have been unpacked into their three constituent counts, these counts can be transformed and generalized slightly by adding or subtracting constants (while ensuring the logarithm is defined). Therefore, we have two different log-linear feature sets obtained from the three basic statistics: 3-count: {log[c(s,t)], log[c(s)], log[c(t)]} Generalized 3-count: {log[c(s,t)+k1], log[c(s)+k2], log[c(t)+k3]}; where k1, k2, k3&gt; -11. “Generalized 3-count” is r"
2011.mtsummit-papers.30,D08-1076,0,0.0305775,"tice MERT details We evaluated several new and several known techniques with our in-house phrase-based SMT system, whose decoder resembles Moses (Koehn et al., 2007). In addition to phrase count features, all systems had forward and backward lexical probabilities, of the type described in (Zens and Ney, 2004), and lexicalized and distance-based distortion models. The LW estimates employed in our experiments are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. Weights on the feature functions are found by lattice MERT (LMERT) (Macherey et al., 2008). These authors pruned the lattices output by their decoder; they also aggregated lattices over iterations (clarified via personal communication with W. Macherey). By contrast, an earlier version of LMERT employed by our group (Larkin et al. 2010) did not involve pruning or aggregation. Initially, we followed the Larkin et al. algorithm: this provides rapid convergence to reasonable optima. However, we decided that some aggregation should be tried to discourage random walk behaviour. In experiments for this paper, we found that without lattice aggregation, adding features led to worse optima o"
2011.mtsummit-papers.30,P03-1021,0,0.0244482,") ≤ τ ; N [c ( s , t ) ≤ τ ] = ® ¯0 otherwise. (1) (2) c ( s, t ) − D + α (t ) × Pb ( s ) c(t ) (6) This feature “punishes” low-frequency phrase pairs. In the paper (Mauser et al., 2007), three different low-frequency features were used, with the three values of τ lying in the interval between 0.9 and 3.0 (the system in the paper allows fractional values of c(s,t)). 3 Unpacking and Transforming Feature Functions Taking just the two RF features from Equation (3), we have: SRF = Ȝ1×log[PRF(t|s)]+ Ȝ2 ×log[PRF(s|t)] The λ’s are often estimated by the minimum error rate training (MERT) algorithm (Och, 2003). For the following two, the implementation details are as in (Foster et al., 2006). Good-Turing: observed counts c are modified according to the formula (Church and Gale, 1991): cg = (c + 1) nc +1 / nc (4) 270 = Ȝ1×(log[c(s,t)]–log[c(s)])+Ȝ2×(log[c(s,t)]–log[c(t)]) = (Ȝ1+Ȝ2)× log[c(s,t)]–Ȝ1×log[c(s)]–Ȝ2×log[c(t)]. This is a combination of three terms, with an additive constraint. Wouldn’t it be simpler to fit the following expression: SRF = Ȝ1 ×log[c(s,t)]+Ȝ2 ×log[c(s)]+Ȝ3×log[c(t)]? (7) MERT can still choose λ’s that are equivalent to using the two RF estimates. In principle, nothing is lost"
2011.mtsummit-papers.30,P02-1038,0,0.650975,"e frequency (RF) estimates of “forward” probability PRF(t|s) and “backward” probability PRF(s|t) are c ( s, t ) PRF (t |s) = c ( s) c ( s, t ) PRF ( s |t ) = c (t ) where cg is a modified count replacing c in subsequent RF estimates, and nc is the number of events having count c. Kneser-Ney (modified): an absolute discounting variant with PKN (s |t ) = These RF estimates are often combined with “lexical weighting” (LW) estimates of the same probabilities PLW(t|s) and PLW(s|t), based on cooccurrence counts of the individual words making up s and t. Thus, the TM score is typically of this form (Och and Ney, 2002): STM = Ȝ1×log[PRF(t|s)]+ Ȝ2×log[PRF(s|t)]+ (3) Ȝ3 ×log[PLW(t|s)]+Ȝ4×log[PLW(s|t)]. (5) where α (t ) = D × n1+ (*, t ) / c(t ) , and Pb ( s ) = n1+ ( s,*) / ¦s n1+ (s,*) . Here, n1+(*,t) is the number of unique sourcelanguage phrases t is aligned with; n1+(s,*) has an analogous definition. PKN(t|s) is defined symmetrically. Kneser-Ney gives a bonus to phrase pairs (s,t) such that s and t have been aligned to many different phrases. Modified Kneser-Ney defines different discounts D depending on the value of c(s,t). We used “KN3”, where D1 is used when c(s,t) = 1, D2 when c(s,t) = 2, and D3 when"
2011.mtsummit-papers.30,J03-1002,0,0.00498293,"is solves the memory and speed problems and provides better performance. 4.2 Results were obtained for Chinese-to-English (CE), and French-to-English (FE). There were two CE data conditions. The first is the small data condition where only the FBIS 3 corpus (10.5M target words) is used to train the translation model. For this condition, we built the phrase table using two phrase extractors: the inhouse one extracts phrase pairs from merged counts of symmetrized IBM model 2 (Brown et al., 1993) and HMM (Vogel et al., 1996) word alignments, while the other one extracts phrase pairs from GIZA++ (Och and Ney 2003) IBM model 4 word alignments (in all other experiments, we only used the in-house extractor). The second is the large data condition where the pa3 272 Data LDC2003E14 rallel training data are from the NIST4 2009 CE evaluation (112.6M target words). We used the same two language models (LMs) for both CE conditions: a 5-gram LM trained on the target side of the large data corpus, and a 6-gram LM trained on the English Gigaword v4 corpus. We used the same development and test sets for the two CE data conditions. The development set comprises mainly data from the NIST 2005 test set, and also some"
2011.mtsummit-papers.30,D07-1103,1,0.912224,"Missing"
2011.mtsummit-papers.30,N03-1017,0,0.0136316,"owing. Section 2 will introduce some existing smoothing techniques. In section 3, we will unpack and transform the two RF feature functions and Kneser-Ney phrase table smoothing. Section 4 is the experiments and discussion. Section 5 ends the paper with conclusion and future work. 2 Existing Smoothing Techniques The phrase table consists of conditional probabilities of co-occurrence for source-language phrases s and target-language phrases t. “Relative frequency” (RF) estimates for these probabilities are obtained from a phrase pair extraction procedure applied to a bilingual training corpus (Koehn et al., 2003). Let c(s) be the count of a source phrase s, c(t) the count of a target phrase t, and c(s,t) the number of times s and t are aligned to form a phrase pair. Relative frequency (RF) estimates of “forward” probability PRF(t|s) and “backward” probability PRF(s|t) are c ( s, t ) PRF (t |s) = c ( s) c ( s, t ) PRF ( s |t ) = c (t ) where cg is a modified count replacing c in subsequent RF estimates, and nc is the number of events having count c. Kneser-Ney (modified): an absolute discounting variant with PKN (s |t ) = These RF estimates are often combined with “lexical weighting” (LW) estimates of"
2011.mtsummit-papers.30,W04-3250,0,0.109273,"e abbreviation for each system, we give in brackets 4 http://www.nist.gov/speech/tests/mt (http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_Const rainedResources.pdf provides the list of resources from which large data was drawn). 5 http://www.statmt.org/wmt10/ 273 the number of log-linear plus other weights that must be tuned for the non-lexical phrase count component of each: e.g., KN3 has two probability estimates, with associated log-linear weights λ1 and λ2 tuned by MERT, and three discounts D1, D2, and D3 (shared by forward and backward probabilities), giving (2+3) weights. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In Table 1-3, Symbols ** or * indicates that the result is significant better than the baseline at level p&lt;0.01 or p&lt;0.05 respectively. In-house extractor system (#wts) RF (2+0) 3CT (3+0) RF+LF3 (5+0) GT (2+0) RF+ELF (3+0) Gen3CT (3+3) Gen3CT+2EN (5+3) Gen3CT+Gen2EN (5+5) KN3 (2+3) KN3+ELF (3+3) NIST test 2006 2008 29.85 23.57 29.48 23.24 29.87 23.60 29.91 23.61 30.46** 24.16** 30.21 23.62 30.47** 23.90* 30.76** 24.36** 27.56 +0.85 30.91** 24.53** 27.72 +1.01 GIZA++ extractor system (#wts) RF (2+0) 3CT (3+0) RF+LF3 (5+0) GT (2+"
2011.mtsummit-papers.30,W10-1717,1,0.881935,"Missing"
2012.amta-papers.7,W05-0909,0,0.0140376,"kin et al., 2010). We use the following feature functions The parameters of the log-linear model are tuned by optimizing BLEU on the development set using MIRA (Chiang et al., 2008).4 Phrase extraction is done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The translation performance was measured using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). 4 Results We show how SMT output degrades with increasing alignment noise. We see that, surprisingly, even relatively high levels of noise have little impact on translation performance. We then compare the robustness of PBMT systems to that of Translation Memories, a commom computer-aided translation tool. 4.1 Impact on translation performance Figure 3 shows how translation performance, as estimated by BLEU (circles), degrades when the number of misaligned sentence pairs increases. Not surprisingly, increasing the noise level produces a general decrease in performance. Although there are var"
2012.amta-papers.7,D08-1024,0,0.0199038,"Missing"
2012.amta-papers.7,W10-1717,1,0.881245,"Missing"
2012.amta-papers.7,moore-2002-fast,0,0.356444,"Missing"
2012.amta-papers.7,P02-1040,0,0.0867124,"ecent NIST and WMT evaluations (Larkin et al., 2010). We use the following feature functions The parameters of the log-linear model are tuned by optimizing BLEU on the development set using MIRA (Chiang et al., 2008).4 Phrase extraction is done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The translation performance was measured using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). 4 Results We show how SMT output degrades with increasing alignment noise. We see that, surprisingly, even relatively high levels of noise have little impact on translation performance. We then compare the robustness of PBMT systems to that of Translation Memories, a commom computer-aided translation tool. 4.1 Impact on translation performance Figure 3 shows how translation performance, as estimated by BLEU (circles), degrades when the number of misaligned sentence pairs increases. Not surprisingly, increasing the noise level produces a general decrease"
2013.mtsummit-papers.23,D11-1033,0,0.0819155,"to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Because of the fact that linear mixtures occur within the log-probabilities assigned to phrase pairs, they are not accessible to standard tuning algorithms without non-trivial modification, and hence are usually trained nondiscriminatively for maximum likelihood. Our simulation is loos"
2013.mtsummit-papers.23,2011.iwslt-evaluation.18,0,0.0330396,"have been refined by (Sennrich, 2012b), who applied them to multiple sub-corpora and investigated alternative techniques such as count weighting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Beca"
2013.mtsummit-papers.23,2011.mtsummit-papers.30,1,0.785655,"bc gale bn gale nw gale wl hkh hkl hkn isi ne lex others nw fbis sinorama unv2 PT size (# pairs) 4.4M 3.6M 4.6M 1.8M 1.4M 62.2M 8.2M 26.6M 26.1M 1.9M 7.8M 19.6M 17.0M 360.1M weights mixtm-ml mixtm-samp 0.033 0.142 0.022 0.091 0.059 0.126 0.082 0.279 0.017 0.176 0.120 0.008 0.002 0.024 0.035 0.021 0.045 0.011 0.003 0.030 0.050 0.042 0.142 0.026 0.053 0.020 0.341 0.005 Table 4: Comparison of weights assigned to Chinese sub-corpora by standard and sampled ML mixture models. Table 3: Mixture TM Adaptation. ditional phrase pair estimates in both directions were obtained using Kneser-Ney smoothing (Chen et al., 2011), and were weighted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch"
2013.mtsummit-papers.23,N13-1114,1,0.88535,"Missing"
2013.mtsummit-papers.23,N12-1047,1,0.842261,"hted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 corpus Results Our first experiment compares a non-adapted baseline, trained on all available corpora, with standard maximum likelihood (ML) mixture TM adaptation as described in section 2.1 and the sampled variant described in section 2.2. The results are presented in table 3, in the form of BLEU scores averaged over both test sets for each language pair. ML mix187 ture adaptation (mixtm-ml) yields significant gains of 0.6 and 0.9 for Arabic and Chinese over the unadapted model. However, in our setting, this underperforms assigning uniform weights to all component models (mixtm-uni). Eq"
2013.mtsummit-papers.23,D08-1024,0,0.0797579,"Missing"
2013.mtsummit-papers.23,W07-0717,1,0.958969,"ues intended to address this problem have attracted significant research attention in recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it"
2013.mtsummit-papers.23,D10-1044,1,0.891722,"much better on the dev set than simplex. We conclude that it is very unlikely that a more sophisticated optimizer working with a better objective than raw BLEU would be able to find a set of linear weights that outperform the test results of our features.6 6 As noted above, however, it almost certainly does not accomplish this feat while remaining within the space of linear combinations. 188 4 Related Work Domain adaptation for SMT is currently a very active topic, encompassing a wide variety of approaches. TM linear mixture models of the kind we study here were first proposed by Foster et al (2010), and have been refined by (Sennrich, 2012b), who applied them to multiple sub-corpora and investigated alternative techniques such as count weighting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference or"
2013.mtsummit-papers.23,D08-1089,0,0.0891952,"0.035 0.021 0.045 0.011 0.003 0.030 0.050 0.042 0.142 0.026 0.053 0.020 0.341 0.005 Table 4: Comparison of weights assigned to Chinese sub-corpora by standard and sampled ML mixture models. Table 3: Mixture TM Adaptation. ditional phrase pair estimates in both directions were obtained using Kneser-Ney smoothing (Chen et al., 2011), and were weighted (each directional estimate separately) for adaptation. We also used lexical estimates in both directions which were not weighted (no sub-corpus-specific values were available). Additional features included a hierarchical lexical reordering model (Galley and Manning, 2008) (6 features), standard distortion and word penalties (2 features), a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM (14 features total). The decoder used a distortion limit of 7, and at most 30 translations for each source phrase. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 corpus Results Our first experiment compares a non-adapted baseline, trained on all available corpora, with standard maximum likelihood (ML) mixture TM adaptation as described in section 2.1 and the sampled variant described in section 2.2. The re"
2013.mtsummit-papers.23,W12-3154,0,0.053233,"ownweighting them severely, thereby discarding whatever useful information they might possess. Although linear mixtures are attractive for adaptation, they have the potential disadvantage that it is difficult to tune mixture weights directly for an SMT error metric such as BLEU. This is because, in order to allow for decoder factoring, models must be mixed at the local level, ie over ngrams or phrase/rule pairs. Thus the linear mixtures oc1 The assumption that train and test domains are fairly similar is shared by most current work on SMT domain adaptation, which focusses on modifying scores. Haddow and Koehn (2012) show that coverage problems dominate scoring problems when train and test are more distant. Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 183–190. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. cur inside the normal log probabilities that are assigned to these entities, making mixture weights inaccessible to standard tuning algorithms. Notice that log-linear mixtures do not suffer from this problem, since c"
2013.mtsummit-papers.23,W07-0733,0,0.432798,"have attracted significant research attention in recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning th"
2013.mtsummit-papers.23,P07-2045,0,0.00636067,"Missing"
2013.mtsummit-papers.23,P03-1021,0,0.0290162,"solution to the problem of setting linear mixture weights is to sidestep it by optimizing some other criterion, typically dev-set likelihood (Sennrich, 2012b), instead of BLEU. This achieves good empirical results, but leaves open the question of whether tuning linear weights directly for BLEU would work better, as one might naturally expect. This question—which to our knowledge has not yet been answered in the literature—is the one we address in this paper. Modifying standard tuning algorithms to accommodate local linear mixtures presents a challenge that varies with the algorithm. In MERT (Och, 2003) for instance, one could replace exact line maximization within Powell’s algorithm with a general-purpose line optimizer capable of handling local linear mixtures at the same time as the usual log-linear weights. For expected BLEU methods (Rosti et al., 2011), handling local weights would require computing gradient information for them. For MIRA (Chiang et al., 2008), the required modifications are somewhat less obvious. Rather than tackling the problem directly by attempting to modify our tuning algorithm, we opted for a simpler indirect approach in which features are used to simulate a discr"
2013.mtsummit-papers.23,P12-1099,1,0.872569,"ting to capture the amount of subcorpus-internal evidence for each phrase pair. Sennrich (2012a) reports interesting results using clustering to automate the identification of coherent sub-corpora. (Aminzadeh et al., 2012) examine the interaction between adaptation and MAP smoothing, and compare different combining techniques. Other ways of combining various component models have also been proposed, including “fill-up” with a predefined preference order (Bisazza et al., 2011), using multiple decoder paths (Koehn and Schroeder, 2007), and ensemble combination of complete models in the decoder (Razmara et al., 2012). Mixture approaches have also been used for LM adaptation (Foster and Kuhn, 2007). Finally, data selection approaches (Axelrod et al., 2011) can be seen as an extreme form of mixture modeling in which weights are either 0 or 1. 5 Conclusion Our main aim in this paper has been to investigate whether SMT features inspired by various prototypical linear mixtures can be used to simulate discriminative training of linear mixture models for TM adaptation. Because of the fact that linear mixtures occur within the log-probabilities assigned to phrase pairs, they are not accessible to standard tuning"
2013.mtsummit-papers.23,W11-2119,0,0.0207874,"ether tuning linear weights directly for BLEU would work better, as one might naturally expect. This question—which to our knowledge has not yet been answered in the literature—is the one we address in this paper. Modifying standard tuning algorithms to accommodate local linear mixtures presents a challenge that varies with the algorithm. In MERT (Och, 2003) for instance, one could replace exact line maximization within Powell’s algorithm with a general-purpose line optimizer capable of handling local linear mixtures at the same time as the usual log-linear weights. For expected BLEU methods (Rosti et al., 2011), handling local weights would require computing gradient information for them. For MIRA (Chiang et al., 2008), the required modifications are somewhat less obvious. Rather than tackling the problem directly by attempting to modify our tuning algorithm, we opted for a simpler indirect approach in which features are used to simulate a discriminatively-trained linear mixture, relying on the ability of MIRA to handle large feature sets. Our aim in doing so is not to achieve a close mathematical approximation, but rather to use features to capture the kinds of information we expect to be inherent"
2013.mtsummit-papers.23,2012.eamt-1.43,0,0.50844,"recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning them low probabilities. To avoid the co"
2013.mtsummit-papers.23,E12-1055,0,0.566545,"recent years. One simple and popular approach is mixture adaptation, in which models trained on various sub-corpora are weighted according to their relevance for the current domain. Mixture adaptation is most effective when there are many heterogeneous sub-corpora, at least some of which are not too different from the test domain.1 It typically requires only a small in-domain sample—a development or test set—in order to obtain weights, and has been shown to work well with all major SMT model types: language (Foster and Kuhn, 2007), translation (Koehn and Schroeder, 2007; Foster et al., 2010; Sennrich, 2012b), and reordering (Chen et al., 2013). There are two main types of mixture model: linear mixtures, which combine weighted probabilities from component models additively; and loglinear mixtures, which combine multiplicatively. Linear mixtures frequently perform better (Foster et al., 2010), especially when there are a relatively large number of sub-corpora, and when the models derived from them are not smooth. This is likely due to the “veto power” that any component model exercises within a log-linear combination: it can suppress hypotheses by assigning them low probabilities. To avoid the co"
2013.mtsummit-papers.24,2011.mtsummit-papers.35,0,0.333245,"Missing"
2013.mtsummit-papers.24,W12-3156,1,0.792219,"r addresses this gap, by proposing a mechanism that automatically propagates post-editor corrections to further machine-translated sentences within a document. We call this process Post-edit Propagation, or PEPr for short. One recurrent complain from post-editors is that they often have to fix the same error repeatedly. Repeated errors can happen for a number of reasons: if the MT system’s training data was too small or heterogeneous, or if it was from a different domain as the document under consideration, then it is not uncommon for a given word or phrase to be systematically mistranslated. Carpuat & Simard (2012) have shown that SMT systems tend to be highly consistent, meaning that multiple occurrences of any given source-language word or phrase will tend to be translated by the same targetlanguage phrase. If that translation happens not to be appropriate in the current context, i.e. if the system is consistently wrong, then the post-editor will need to fix that translation several times. As pointed out by Lagoudaki (2008), post-editing systems should have the ability to “learn from the decisions/choices made by users (e.g. which potential translations are preferred, which were rejected and why), so"
2013.mtsummit-papers.24,N10-1080,0,0.0117456,"We further discuss the effect of varying this parameter in Section 3.4. Table 2 presents the results of these experiments. The impact of PEPr is measured in terms of WER and BLEU gain (for convenience, we report WER scores as 100-WER, so that larger values denote better translations, and negative “gains” can be interpreted as “losses”) 3 . For each corpus and language, we first report the scores obtained by the 3 TER would arguably have been a better metric to evaluate the potential of our approach in a post-editing setting; in practice, however, WER is known to behave very similarly to TER (Cer et al., 2010). Corpus ECB en→fr 100-WER BLEU MT +PEPr-MTLM +PEPr-GLM MT +PEPr-MTLM +PEPr-GLM 32.24 +5.71 +6.53 32.65 +3.45 +5.38 25.87 +6.16 +7.08 22.56 +7.42 +9.27 MT +PEPr-MTLM +PEPr-GLM fr→en MT +PEPr-MTLM +PEPr-GLM Science Abstracts en→fr MT +PEPr-MTLM +PEPr-GLM fr→en MT +PEPr-MTLM +PEPr-GLM Science Digests en→fr MT +PEPr-MTLM +PEPr-GLM fr→en MT +PEPr-MTLM +PEPr-GLM 32.75 +3.83 +4.76 30.05 +3.27 +4.56 25.05 +5.37 +6.56 25.13 +5.35 +7.44 37.00 -0.94 -0.10 39.64 +0.84 -0.50 24.00 +0.73 -0.16 24.82 +0.74 -0.04 36.96 +0.37 -1.88 39.68 +0.56 -0.64 23.93 +0.37 +0.09 24.81 +0.57 -0.16 fr→en EMEA en→fr System"
2013.mtsummit-papers.24,N12-1047,1,0.880515,"SMT, the reordering (or distortion) model controls the relative order of SL and TL words. Local word reorderings are typically captured within phrase pairs in the phrase table, and need not be handled by these models. Phrase-level reordering is already a complicated matter, and it is doubtful that we can reliably model it in this extreme sparse-data environment. We also conjecture that phrase-level reorderings are a rare event in post-editing, and therefore completely inhibit them for PEPr. Parameter Optimization Parameters of the APE systems’ log-linear model are optimized using batch-MIRA (Cherry and Foster, 2012). This procedure normally assumes a development set, which is repeatedly translated with a single translation system. In a PEPr setting, the system dynamically changes after each post-edited sentence. We nevertheless make the assumption that it is possible to find a set of parameters that is globally optimal under these varying conditions. We modify the decoding step in the optimization loop so that each development set sentence is translated by a different system, as described above. In our current setting, the parameters controlling the LM mixture are optimized manually on the development se"
2013.mtsummit-papers.24,W07-0717,1,0.83565,"Missing"
2013.mtsummit-papers.24,D11-1084,0,0.0151339,"updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-based SMT system in an APE setting, with incremental training. Experiments simulating post-editing sessions suggest that our method is particularly effective when translating technical documents with high levels of internal repetition. Because the method is designed to work with extremely small amounts of training data, we believe that it can be implemented into an efficient,"
2013.mtsummit-papers.24,2010.amta-papers.21,0,0.568393,"Automatic post-editing using SMT was proposed in Simard et al. (2007). The idea of dynamically updating an APE system after each sentence revised by a translator was the subject of an early proposal by Knight and Chander (1994). As far as we are aware, it has not been investigated experimentally in previous work, although as noted above certain commercial TM systems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-b"
2013.mtsummit-papers.24,P07-2045,0,0.00533868,"n for the do-nothing option, because it is inhibited as soon as a word appears in the phrase table: if on its first occurrence a word w is post-edited to v, then that correction will automatically be applied the second time w is encountered. Instead, we explicitly include in the phrase table unedited versions of all input phrases. This can be achieved by adding the pair htk , tk i to Ak ’s training set; this way, the phrase table contains phrase pairs that explicitly sanction leaving a word or phrase unedited in the APE output. While phrase pair extraction is performed using standard methods (Koehn et al., 2007), a wordlevel alignment between machine translations ti and reference translations ri is required, which, in standard SMT (or APE) approaches, is computed using IBM-style translation models. In a PEPr scenario, there is typically too little data to train such a model: in many cases, the training corpus will be ridiculously small. But since the “source” and “target” are the same language, a straightforward alternative is to obtain the alignment from the minimal sequence of edits e = e1 ...em corresponding to the word-level Levenshtein distance between ti and ri . To achieve more precise alignme"
2013.mtsummit-papers.24,2008.amta-srw.4,0,0.0397365,"terogeneous, or if it was from a different domain as the document under consideration, then it is not uncommon for a given word or phrase to be systematically mistranslated. Carpuat & Simard (2012) have shown that SMT systems tend to be highly consistent, meaning that multiple occurrences of any given source-language word or phrase will tend to be translated by the same targetlanguage phrase. If that translation happens not to be appropriate in the current context, i.e. if the system is consistently wrong, then the post-editor will need to fix that translation several times. As pointed out by Lagoudaki (2008), post-editing systems should have the ability to “learn from the decisions/choices made by users (e.g. which potential translations are preferred, which were rejected and why), so that errors in future translation assemblies are reduced.” Developers of commercial translation memory (TM) systems have already taken note of this requirement. The idea behind TM technology is that translators should not have to translate the same material more than once. To achieve this, all translations are archived, new texts are systematically compared to the contents of the archive, and when segments are found"
2013.mtsummit-papers.24,N10-1062,0,0.204901,"entally in previous work, although as noted above certain commercial TM systems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-based SMT system in an APE setting, with incremental training. Experiments simulating post-editing sessions suggest that our method is particularly effective when translating technical documents with high levels of internal repetition. Because the method is designed to work with extremely"
2013.mtsummit-papers.24,W04-3225,1,0.68915,"nd it could also be interesting to leave its setting to the user, as a way of controlling how aggressive or conservative the system is. 4 Related Work Automatic post-editing using SMT was proposed in Simard et al. (2007). The idea of dynamically updating an APE system after each sentence revised by a translator was the subject of an early proposal by Knight and Chander (1994). As far as we are aware, it has not been investigated experimentally in previous work, although as noted above certain commercial TM systems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editin"
2013.mtsummit-papers.24,N07-1064,1,0.942338,"weight of prior (background) and newly acquired (local) knowledge. We argue that the PEPr task can be effectively carried out by a phrase-based statistical MT system. The system is then effectively an automatic post-editing (APE) system with online learning capabilities. We detail how this idea can be realized (Section 2), then validate our design with experiments that simulate PE sessions (Section 3). Related work is reviewed in Section 4. 2 PEPr using Phrase-based SMT An APE system is one that performs transformations on MT output. If this APE system is based on a SMT system, as proposed in Simard et al. (2007), then it can be seen as a system that attempts to translate target-language (TL) MT output into proper (human quality) TL text. Typically, it will be trained on a corpus of post-edited MT: pairs of machine-translated sentences along with their post-edited counterparts. To perform post-edit propagation, we propose to use an SMT system in an APE setting: the system will be trained incrementally, using pairs of machine-translated and post-edited segments as they are produced. The whole process is assumed to take place within the context of a single document D. The general set-up is illustrated i"
2013.mtsummit-papers.24,W10-2602,0,0.012337,"ems allow dynamic updates. Similar ideas have been explored for SMT, beginning with Nepveu et al. (2004), who used a cache-based approach to incorporate recent wordfor-word translations and ngrams into an early interactive SMT system. Hardt and Elming (2010) applied a similar strategy to a modern phrasebased SMT system, using heuristic IBM4-based word alignment techniques to augment a local 197 phrase table with material from successive postedited sentences. Two related themes in SMT research are incremental training (Levenberg et al., 2010) and context-based adaptation without user feedback (Tiedemann, 2010; Gong et al., 2011). These techniques have not yet been applied to automatic post-editing, outside the work of Hardt and Elming (2010). 5 Conclusion We have proposed a method to perform automatic post-edit propagation (PEPr), using a phrase-based SMT system in an APE setting, with incremental training. Experiments simulating post-editing sessions suggest that our method is particularly effective when translating technical documents with high levels of internal repetition. Because the method is designed to work with extremely small amounts of training data, we believe that it can be implemente"
2014.amta-researchers.10,D11-1033,0,0.0469667,"Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets line"
2014.amta-researchers.10,W09-0432,0,0.0871166,"e six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. n"
2014.amta-researchers.10,P13-1141,0,0.0260984,"s. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing ne"
2014.amta-researchers.10,N13-1114,1,0.258614,"c-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indicate a particular combination of all these factors (Chen et al., 2013b). Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training data vary across domains, and translations across domains are unreliable. Therefore, we can often get better performance by adapting the SMT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches that have been tried for SMT model adaptation include self-training, data selection, data weighting, context-based DA, and topic-based DA. etc. We will review these techniques in the next Section. Among all these approaches, d"
2014.amta-researchers.10,P13-1126,1,0.392167,"c-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indicate a particular combination of all these factors (Chen et al., 2013b). Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training data vary across domains, and translations across domains are unreliable. Therefore, we can often get better performance by adapting the SMT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches that have been tried for SMT model adaptation include self-training, data selection, data weighting, context-based DA, and topic-based DA. etc. We will review these techniques in the next Section. Among all these approaches, d"
2014.amta-researchers.10,2011.mtsummit-papers.30,1,0.752758,"development and test sets. We use the evaluation sets from NIST 06, 08, and 09 as our development set and two test sets, respectively. All Chinese and Arabic dev and test sets have 4 references. 5.2 System Experiments were carried out with an in-house, state-of-the-art phrase-based system. The whole corpora were first word-aligned using IBM2, HMM, and IBM4 models, then split to subcorpora according to genre and origin; the phrase table was the union of phrase pairs extracted from these alignments, with a length limit of 7. The translation model (TM) was Kneser-Ney smoothed in both directions (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7, lexical weighting in both directions, word count, a distance-based reordering model, a 4-gram language model (LM) trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 5.3 Results This paper has described three modifications to existing techniques: smoothing for log-linear mixtures and two new versions of VSM - grouped VSM and distributional VSM. First, we did Al-Onaizan & Simar"
2014.amta-researchers.10,P08-2040,1,0.834358,"st improvements are for five of the six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data select"
2014.amta-researchers.10,N13-1003,0,0.0205184,"aseline, original VSM adaptation and distributional VSM adaptation. Table 5 compares original VSM and distributional VSM. It shows that the original VSM adaptation reported in (Chen et al., 2013b) does yield improvement over a non-adaptive baseline. However, if instead of computing a domain similarity score, we directly maximize BLEU on the dev set by tuning the weights of distribution features, we got further significant improvements. On the Chinese task, the further improvements were +0.7-0.8 BLEU, while on Arabic, the further improvements were smaller but still significant: +0.3-0.4 BLEU. (Cherry, 2013) showed that in the case of reordering features, directly maximizing BLEU outperforms maximum entropy optimization; the experiments in Table 5 yield a similar conclusion. Given that recently developed tuning algorithms such as MIRA can handle a very large feature set, we may consider having all possible features directly tuned to maximize BLEU (or similar criteria). Now, we compare all six DA techniques, Table 6 reports the results. All techniques improved on all test sets across two language pairs over the non-adaptive baseline, and all these improvements are significant. From the average abs"
2014.amta-researchers.10,N12-1047,1,0.887245,"Missing"
2014.amta-researchers.10,P11-2080,0,0.184624,"or Translation Model Adaptation Boxing Chen Roland Kuhn George Foster Boxing.Chen@nrc-cnrc.gc.ca Roland.Kuhn@nrc-cnrc.gc.ca George.Foster@nrc-cnrc.gc.ca National Research Council Canada, Ottawa, Canada Abstract In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s style. The word “domain” is often used to indica"
2014.amta-researchers.10,D13-1107,0,0.0120428,"better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phra"
2014.amta-researchers.10,P11-2071,0,0.152169,"Missing"
2014.amta-researchers.10,D12-1025,0,0.0440133,"lations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into the SMT system. 3 Mixture model adaptation There are several adaptation scenarios for SMT, of which the most common is 1) The training material is heterogeneous, with some parts of it that are not too far from the test domain; 2) A bilingual dev set drawn from the test domain is available. A common approach to DA for this scenario is: 1) split the training data into sub-corpora by domain; 2) train sub-models or features on each sub-corpus; 3)"
2014.amta-researchers.10,P13-2119,0,0.0500397,"can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearl"
2014.amta-researchers.10,P12-2023,0,0.0197208,"in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into th"
2014.amta-researchers.10,2012.iwslt-papers.17,0,0.0367148,"Missing"
2014.amta-researchers.10,D10-1044,1,0.962878,"on about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixt"
2014.amta-researchers.10,W07-0717,1,0.867177,"mparison of Mixture and Vector Space Techniques for Translation Model Adaptation Boxing Chen Roland Kuhn George Foster Boxing.Chen@nrc-cnrc.gc.ca Roland.Kuhn@nrc-cnrc.gc.ca George.Foster@nrc-cnrc.gc.ca National Research Council Canada, Ottawa, Canada Abstract In this paper, we propose two extensions to the vector space model (VSM) adaptation technique (Chen et al., 2013b) for statistical machine translation (SMT), both of which result in significant improvements. We also systematically compare the VSM techniques to three mixture model adaptation techniques: linear mixture, log-linear mixture (Foster and Kuhn, 2007), and provenance features (Chiang et al., 2011). Experiments on NIST Chinese-to-English and Arabic-to-English tasks show that all methods achieve significant improvement over a competitive non-adaptive baseline. Except for the original VSM adaptation method, all methods yield improvements in the +1.7-2.0 BLEU range. Combining them gives further significant improvements of up to +2.6-3.3 BLEU over the baseline. 1 Introduction The translation of a source-language expression to a target language might differ across genres, topics, national origins, and dialects, or the author’s or publication’s s"
2014.amta-researchers.10,W06-1607,1,0.711799,"are then learned under the standard SMT log-linear framework. Experiments (see 5.3) show that this simple smoothing greatly improves the performance of log-linear mixture adaptation. 1 We use the models trained on the whole training data to align the dev set. This can be done with mgiza (http://www.kyloo.net/software/doku.php/mgiza) Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 127 3.3 Provenance features Provenance features (Chiang et al., 2011) are applied to lexical weights. There are slight variations in computing lexical weights (Foster et al., 2006), they all use forward and backward word translation probabilities T (s|t) and T (t|s) estimated from the word-aligned parallel text. The conditional probability for word pair (s, t) in a translation table is computed as below: count(s, t) . s count(si , t) T (s|t) = P (4) i We adopt the approach proposed in (Zens and Ney, 2004) to compute the lexical weights. It assumes that all source words are conditionally independent: plw (s|t) = n Y p(si |t) (5) i=1 and adopts a “noisy-or” combination, so that p(si |t) = 1 − m Y (1 − T (si |tj )) (6) j=1 where n and m are number of source and target word"
2014.amta-researchers.10,D08-1089,0,0.0150038,"as our development set and two test sets, respectively. All Chinese and Arabic dev and test sets have 4 references. 5.2 System Experiments were carried out with an in-house, state-of-the-art phrase-based system. The whole corpora were first word-aligned using IBM2, HMM, and IBM4 models, then split to subcorpora according to genre and origin; the phrase table was the union of phrase pairs extracted from these alignments, with a length limit of 7. The translation model (TM) was Kneser-Ney smoothed in both directions (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7, lexical weighting in both directions, word count, a distance-based reordering model, a 4-gram language model (LM) trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 5.3 Results This paper has described three modifications to existing techniques: smoothing for log-linear mixtures and two new versions of VSM - grouped VSM and distributional VSM. First, we did Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors"
2014.amta-researchers.10,2013.mtsummit-papers.23,1,0.900528,"Missing"
2014.amta-researchers.10,D11-1084,0,0.0212675,"from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which e"
2014.amta-researchers.10,D13-1109,0,0.187192,"Missing"
2014.amta-researchers.10,koen-2004-pharaoh,0,0.0610681,"-lin w/ smooth Chinese-English MT06 MT08 36.0 29.4 35.8 29.0 37.9**++ 31.1**++ Arabic-English MT08 MT09 46.4 49.2 47.7** 50.0** 48.2**+ 50.6**++ Table 3: Results of log-linear mixtures with or without smoothing. */** or +/++ means result is significantly better than baseline or system without smoothing (p < 0.05 or p < 0.01, respectively). experiments to see if these three modifications yield statistically significant improvements over the original techniques. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrap-resampling test for significance testing. We set the λ in Equation 3 and Equation 7 to 0.01 by looking at performance on the Arabic-English dev set. Table 3 shows the results of log-linear mixture model with or without smoothing. Without smoothing, log-linear mixture model DA hurt the performance of Chinese-to-English on both test sets, but gave moderate improvements on Arabic-to-English. The Chinese task has more (14) sub-corpora than the Arabic one, and they seem more diverse; as suggested earlier, the “veto power” of small sub-corpora seems to be particularly harmful"
2014.amta-researchers.10,W07-0733,0,0.0353986,"is can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for ea"
2014.amta-researchers.10,D07-1036,0,0.299343,"Missing"
2014.amta-researchers.10,D09-1074,0,0.268931,"uhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data"
2014.amta-researchers.10,P10-2041,0,0.0572885,"adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different"
2014.amta-researchers.10,P02-1040,0,0.100025,"=broadcast conversation, bn=broadcast news, ng=newsgroup, wl=weblog. baseline log-lin w/o smooth log-lin w/ smooth Chinese-English MT06 MT08 36.0 29.4 35.8 29.0 37.9**++ 31.1**++ Arabic-English MT08 MT09 46.4 49.2 47.7** 50.0** 48.2**+ 50.6**++ Table 3: Results of log-linear mixtures with or without smoothing. */** or +/++ means result is significantly better than baseline or system without smoothing (p < 0.05 or p < 0.01, respectively). experiments to see if these three modifications yield statistically significant improvements over the original techniques. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrap-resampling test for significance testing. We set the λ in Equation 3 and Equation 7 to 0.01 by looking at performance on the Arabic-English dev set. Table 3 shows the results of log-linear mixture model with or without smoothing. Without smoothing, log-linear mixture model DA hurt the performance of Chinese-to-English on both test sets, but gave moderate improvements on Arabic-to-English. The Chinese task has more (14) sub-corpora than the Arabic one, and they seem more diverse; as"
2014.amta-researchers.10,P12-1099,1,0.850794,"stem (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approac"
2014.amta-researchers.10,2008.iwslt-papers.6,0,0.124773,"for five of the six methods (those other than the original VSM), with improvements for these five all in the range +1.7-2.0 BLEU; 3) combining some of these techniques yields further significant improvement. The best combination yields improvements of +2.6-3.3 BLEU over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of"
2014.amta-researchers.10,E12-1055,0,0.0466168,"ence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixture combine sub-models trained on different domain data sets linearly or log-linearly (Foster and Kuhn, 2007). Log-linear mixture DA can employ the same discriminative tuning algorithm used to combine the log-linear high-level components of a typical SMT system (the translation model, language model, reordering model, etc.), but linear mixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method"
2014.amta-researchers.10,P13-1082,0,0.0200048,"ixture DA seems to work better. Thus, (Foster et al., 2010; Sennrich, 2012; Chen et al., 2013a; George Foster and Kuhn, 2013) studied linear mixture adaptation. (Koehn and Schroeder, 2007), instead, combined the sub-models via Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 125 alternative paths. Provenance features (Chiang et al., 2011) compute a separate set of lexical weights for each sub-corpus, then all these lexical weights are combined log-linearly. (Razmara et al., 2012) used ensemble decoding to mix multiple translation models. (Sennrich et al., 2013; Cui et al., 2013) extended the mixture model method to multi-domain DA. At sentence level, (Matsoukas et al., 2009) used a rich feature set to compute weights for each sentence in the training data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev"
2014.amta-researchers.10,P07-1066,0,0.0266431,"he in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase ta"
2014.amta-researchers.10,W10-2602,0,0.0203879,"data. A sentence from a corpus whose domain is close to that of the in-domain dev set would receive a high weight. At a finer level of granularity, (Foster et al., 2010) used a rich feature set to compute phrase pair weights. Vector space model adaptation (Chen et al., 2013b) is another phrase-level data weighting approach; it weights each phrase pair with the similarity score between the in-domain dev data and each phrase pair in the training data based on vector space model in which vectors (for the entire dev data, or for each phrase pair) represent domain profiles. The cache-based method (Tiedemann, 2010; Gong et al., 2011) is a form of context-based adaptation technique. In the tradition of (Kuhn and De Mori, 1990) it uses a cache to consider the local or document-level context when choosing translations. (Carpuat et al., 2013) employed word sense disambiguation, using local context to distinguish the translations for different domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and K"
2014.amta-researchers.10,J07-1003,0,0.303587,"Missing"
2014.amta-researchers.10,N04-1033,0,0.0435798,"iza) Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 127 3.3 Provenance features Provenance features (Chiang et al., 2011) are applied to lexical weights. There are slight variations in computing lexical weights (Foster et al., 2006), they all use forward and backward word translation probabilities T (s|t) and T (t|s) estimated from the word-aligned parallel text. The conditional probability for word pair (s, t) in a translation table is computed as below: count(s, t) . s count(si , t) T (s|t) = P (4) i We adopt the approach proposed in (Zens and Ney, 2004) to compute the lexical weights. It assumes that all source words are conditionally independent: plw (s|t) = n Y p(si |t) (5) i=1 and adopts a “noisy-or” combination, so that p(si |t) = 1 − m Y (1 − T (si |tj )) (6) j=1 where n and m are number of source and target words in the phrase pair (s, t) respectively. To compute the provenance features, we first estimate the word translation tables T (s|t) and T (t|s) trained on the N sub-corpora. However, many word pairs are unseen for the word translation table of a given sub-corpus. Following (Chiang et al., 2011), we smooth the translation tables:"
2014.amta-researchers.10,P13-1140,0,0.155795,"domains. Work on topic-based DA includes (Tam et al., 2007), where latent semantic analysis (LSA) models topics for SMT adaptation, (Eidelman et al., 2012; Hewavitharana et al., 2013) which employs a latent Dirichlet allocation (LDA) topic model, and (Eva Hasler and Koehn, 2012), which employs hidden topic Markov models (HTMMs), adding a sentence topic distribution as an SMT system feature. Other DA approaches include mining translations from comparable data to translate OOVs and capturing new senses in new domains (Daume III and Jagarlamudi, 2011; Irvine et al., 2013). (Dou and Knight, 2012; Zhang and Zong, 2013) learned bilingual lexica or phrase tables from in-domain monolingual data with a decipherment method, then incorporated them into the SMT system. 3 Mixture model adaptation There are several adaptation scenarios for SMT, of which the most common is 1) The training material is heterogeneous, with some parts of it that are not too far from the test domain; 2) A bilingual dev set drawn from the test domain is available. A common approach to DA for this scenario is: 1) split the training data into sub-corpora by domain; 2) train sub-models or features on each sub-corpus; 3) weight these sub-model"
2014.amta-researchers.10,C04-1059,0,0.214758,"over the baseline. 2 Reviewing of SMT adaptation techniques Most approaches to DA can be classified into one of five categories: self-training, data selection, data weighting, context-based DA, and topic-based DA. Recently, several new approaches have also been studied. With self-training (Ueffing and Ney, 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data. Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013) search for monolingual or bilingual parallel sentences that are similar to the in-domain data according to some criterion, then add them to the training data. Data weighting approaches can be seen as a generalization of data selection: instead of making a binary include vs. not-include decision about a given sentence or sentence pair, one weights each data item according to its closeness to the in-domain data. This can be applied at corpus, sentence, or phrase level. At corpus level, linear and log-linear mixtur"
2014.amta-researchers.10,P13-2122,0,\N,Missing
2014.amta-researchers.3,C14-1181,0,0.0319255,"Missing"
2014.amta-researchers.3,P11-1087,0,0.135447,"this makes clustering expensive. Och (1999) focuses on bilingual word clustering and discusses ideas similar to bitoken clustering, though not in the context of phrase-based SMT. Uszkoreit and Brant (2008) describe a highly efficient distributed version of exchange clustering. Faruqui and Dyer (2013) propose a bilingual word clustering method whose objective function combines same-language and cross-language mutual information. Applied to named entity recognition (NER), this yields significant improvements. Turian, Ratinov and Bengio (2010) apply Brown clustering to NER and chunking. Finally, Blunsom and Cohn (2011) improve Brown clustering by using a Bayesian prior to smooth estimates, by incorporating trigrams, and by exploiting morphological information. For word clustering, we chose a widely used program, mkcls: Blunsom and Cohn (2011) note its strong performance. We could have used POSs, but they have definitions that vary across languages; mkcls can be applied in a uniform way (though with the disadvantage that it gives each word a fixed class, instead of several possible classes as with POSs). Niesler et al (1998) found that automatically derived word classes outperform POSs. Until recently, the o"
2014.amta-researchers.3,J92-4003,0,0.619168,"Missing"
2014.amta-researchers.3,2011.mtsummit-papers.30,1,0.832739,"l-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 33 2.3. Experimental systems Eng&lt;>Fre Hansard experiments were performed with Portage, the National Research Council of Canada’s phrase-based system (this is the system described in Foster et al, 2013). The corpus was word-aligned with HMM and IBM2 models; the phrase table was the union of phrase pairs from these alignments, with a length limit of 7. We applied Kneser-Ney smoothing to find bidirectional conditional phrase pair estimates, and obtained bidirectional Zens-Ney lexical estimates (Chen et al, 2011). Hierarchical lexical reordering (Galley and Manning, 2008) was used. Additional features included standard distortion and word penalties (2 features) and a 4-gram LM trained on the target side of the parallel data: 13 features in total. The decoder used cube pruning and a distortion limit of 7. Our Chinese and Arabic baselines are strong phrase-based systems, similar to our entries in evaluations like NIST. The hierarchical lexical reordering model (HRM) of (Galley and Manning, 2008) along with the sparse reordering features of (Cherry, 2013) was used. Phrase extraction pools counts over sym"
2014.amta-researchers.3,N13-1003,0,0.247077,"g coarse biLMs like “(10,000, 10,000)” or “10,000 bi(400,400)” is infeasible. 1.2. Related work This section will discuss work on coarse models, source-side contextual information for SMT, and lexical clustering techniques (including mkcls, used for our experiments). Uszkoreit and Brant (2008) explored coarse LMs for SMT. Wuebker et al (2013) describe coarse LMs, translation models (TMs), and reordering models (RMs). Best performance was obtained with a system containing both word-based and coarse models. Prior to our current work, we experimented with discriminative hierarchical RMs (DHRMs) (Cherry, 2013). These combine the hierarchical RM (HRM) of (Galley and Manning, 2008) with sparse features conditioned on word classes for phrases involved in reordering; word classes are obtained from mkcls. Like Cherry (2013), we found that DHRM outperformed the HRM version for Ara>Eng and Chi>Eng. However, experiments with English-French Hansard data showed only small gains for DHRM over HRM. Thus, while all the Ara>Eng and Chi>Eng experiments reported in this paper employ DHRM - a coarse reordering model - none of the Eng&lt;>Fre experiments do. In prior experiments, we also studied coarse phrase translati"
2014.amta-researchers.3,N12-1047,1,0.924126,"information from source words outside the current phrase pair is incorporated only indirectly, via target words that are translations of these source words, if the relevant target words are close enough to the current target word to affect LM scores. BiLMs address this by aligning each target word in the training data with source words to create “bitokens”. An N-gram bitoken LM is then trained. A coarse biLM is one whose words and/or bitokens have been clustered into classes. Our best results were obtained by combining coarse biLMs with coarse LMs. We tune our system with batch lattice MIRA (Cherry and Foster, 2012), which supports loglinear combinations that have many features. Figure 1 shows word-based and coarse biLMs for Eng>Fre. A target word and its aligned source words define a bitoken. Unaligned target words (e.g., French word “d’ ” in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”, aligned with two instances of “nous”) is duplicated: each target word aligned with it receives a copy of that source word. Figure 1. Creating bitokens & bitoken classes for a bilingual language model (biLM) BiLMs c"
2014.amta-researchers.3,P11-1105,0,0.0599442,"experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IBM” clustering, each word in vocabulary V initially defines a single class. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC"
2014.amta-researchers.3,N13-1073,0,0.0431711,"8) was used. Additional features included standard distortion and word penalties (2 features) and a 4-gram LM trained on the target side of the parallel data: 13 features in total. The decoder used cube pruning and a distortion limit of 7. Our Chinese and Arabic baselines are strong phrase-based systems, similar to our entries in evaluations like NIST. The hierarchical lexical reordering model (HRM) of (Galley and Manning, 2008) along with the sparse reordering features of (Cherry, 2013) was used. Phrase extraction pools counts over symmetrized word alignments from IBM2, HMM, IBM4, Fastalign (Dyer et al, 2013), and forced leave-one-out phrase alignment; the HRM pools counts in the same way. Phrase tables were Kneser-Ney smoothed as for the Eng&lt;>Fre experiments, and combined with mixture adaptation (Foster, 2007); indicator features tracked which extraction techniques produced each phrase. The Chinese system incorporated additional adaptation features (Foster et al, 2013). For both Arabic and Chinese, four LMs per system were trained: one LM on the English Gigaword corpus (5-gram with Good-Turing smoothing), one LM on monolingual webforum data and two LMs trained on selected material from the parall"
2014.amta-researchers.3,P13-2136,0,0.0343622,"Missing"
2014.amta-researchers.3,W14-1616,0,0.0813318,"ar et al, 2013; Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IBM” clustering, each word in vocabulary V initially defines a single class. Al-Onaizan & Simard (Eds."
2014.amta-researchers.3,2010.amta-papers.24,1,0.7713,"od-Turing for coarse models); we would use 8-gram coarse models (results differed only slightly along the range from 6-grams to 8-grams, but were marginally better for 8- Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 32 grams). We then began a second round of Eng>Fre experiments with the same devtest (see 2.4 and Table 4); the results informed all subsequent experiments. 2.2. Experimental data For English-French experiments in both directions, we used the high-quality Hansard corpus of Canadian parliamentary proceedings from 2001-2009 (Foster et al, 2010). We reserved the most recent five documents (from December 2009) for development and testing material, and extracted the dev and test corpora shown in Table 1. Some of the documents were much larger than typical devtest sizes, so we sampled subsets of them for the dev and test sets. Corpus # sentence pairs # words (English) Train 2.9M 60.5M Tune 2,002 40K Devtest1 2,148 43K Devtest2 2,166 45K Test1 (blind test) 1,975 39K Test2 (blind test) 2,340 49K Table 1. Corpus sizes for English&lt;>French Hansard data # words (French) 68.6M 45K 48K 50K 44K 55K For Ara>Eng and Chi>Eng, we used large-scale tr"
2014.amta-researchers.3,W07-0717,1,0.726634,"ing words/bitokens could be improved – e.g.., as in Blunsom and Cohn (2011). As a reviewer helpfully pointed out, coarse models of the same type but different granularities could be trained more efficiently with true IBM clustering (Brown et al, 1992) to create a hierarchy for words or bitokens that would yield many different granularities after a single run, rather than by running mkcls several times (once per granularity).  Coarse models could be used for domain adaptation - e.g., via mixture models that combine in-domain and out-of-domain or general-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012). In-domain statistics will be better-estimated in a coarse mixture than in a word-based one.  “Mirror-image” word-based or coarse target-to-source biLMs could be used to rescore N-best lists or lattices. If there has been word reordering, these would apply context information not seen during decoding. E.g., let source “A B C D E F G H” generate hypothesis “a b f h g c d e”, and “A” be aligned with “a”, “B” with “b”, etc. With trigram word-based biLMs, the trigrams involving “f” seen during decoding are “a_A b_B f_F”, “b_B f_F h_H”, and “f_F h_H g_G”. During mirror-image resc"
2014.amta-researchers.3,D08-1089,0,0.353237,"Missing"
2014.amta-researchers.3,D08-1039,0,0.0287366,"ective for morphologically rich languages (e.g., Ammar et al, 2013; Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IBM” clustering, each word in vocabulary V initiall"
2014.amta-researchers.3,D11-1125,0,0.0340526,"and combined with mixture adaptation (Foster, 2007); indicator features tracked which extraction techniques produced each phrase. The Chinese system incorporated additional adaptation features (Foster et al, 2013). For both Arabic and Chinese, four LMs per system were trained: one LM on the English Gigaword corpus (5-gram with Good-Turing smoothing), one LM on monolingual webforum data and two LMs trained on selected material from the parallel corpora (4-gram with Kneser-Ney smoothing); in the case of Chinese, the latter two LMs were mixture-adapted.. Both systems used the sparse features of (Hopkins and May, 2011; Cherry, 2013). The decoder used cube pruning and a distortion limit of 8. Tuning for all systems was performed with batch lattice MIRA (Cherry and Foster, 2012). The metric is the original IBM BLEU, with case-insensitive matching of n-grams up to n = 4. For all systems, we performed five random replications of parameter tuning (Clark et al, 2011). For Eng&lt;> Fre, coarse models were trained on all of “Train”. For Ara>Eng, word classes and two static coarse LMs were trained on “all” and “webforum” (no linear mixing), but biLMs were trained on “small”. For Chi>Eng, word classes and a large stati"
2014.amta-researchers.3,W07-0733,0,0.020693,"The method for hard-clustering words/bitokens could be improved – e.g.., as in Blunsom and Cohn (2011). As a reviewer helpfully pointed out, coarse models of the same type but different granularities could be trained more efficiently with true IBM clustering (Brown et al, 1992) to create a hierarchy for words or bitokens that would yield many different granularities after a single run, rather than by running mkcls several times (once per granularity).  Coarse models could be used for domain adaptation - e.g., via mixture models that combine in-domain and out-of-domain or general-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012). In-domain statistics will be better-estimated in a coarse mixture than in a word-based one.  “Mirror-image” word-based or coarse target-to-source biLMs could be used to rescore N-best lists or lattices. If there has been word reordering, these would apply context information not seen during decoding. E.g., let source “A B C D E F G H” generate hypothesis “a b f h g c d e”, and “A” be aligned with “a”, “B” with “b”, etc. With trigram word-based biLMs, the trigrams involving “f” seen during decoding are “a_A b_B f_F”, “b_B f_F h_H”, and “f_F h_H g_G”. D"
2014.amta-researchers.3,J06-4004,0,0.276637,"Missing"
2014.amta-researchers.3,W11-2124,0,0.461366,"rc.gc.ca Eric Joanis Eric.Joanis@nrc.gc.ca George Foster* George.Foster@nrc.gc.ca All authors originally at: National Research Council, Ottawa, Canada K1A 0R6 * This author is now at Google Inc., Mountain View, California 94043 Abstract Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013). We create new models by replacing words with word classes in features applied during decoding; we call these “coarse models”. We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse"
2014.amta-researchers.3,E99-1010,0,0.37772,"Missing"
2014.amta-researchers.3,E12-1055,0,0.0220636,"d be improved – e.g.., as in Blunsom and Cohn (2011). As a reviewer helpfully pointed out, coarse models of the same type but different granularities could be trained more efficiently with true IBM clustering (Brown et al, 1992) to create a hierarchy for words or bitokens that would yield many different granularities after a single run, rather than by running mkcls several times (once per granularity).  Coarse models could be used for domain adaptation - e.g., via mixture models that combine in-domain and out-of-domain or general-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012). In-domain statistics will be better-estimated in a coarse mixture than in a word-based one.  “Mirror-image” word-based or coarse target-to-source biLMs could be used to rescore N-best lists or lattices. If there has been word reordering, these would apply context information not seen during decoding. E.g., let source “A B C D E F G H” generate hypothesis “a b f h g c d e”, and “A” be aligned with “a”, “B” with “b”, etc. With trigram word-based biLMs, the trigrams involving “f” seen during decoding are “a_A b_B f_F”, “b_B f_F h_H”, and “f_F h_H g_G”. During mirror-image rescoring, the biLM t"
2014.amta-researchers.3,P10-1040,0,0.132951,"Missing"
2014.amta-researchers.3,P08-1086,0,0.06041,"Missing"
2014.amta-researchers.3,D13-1138,0,0.0689579,"Missing"
2014.amta-researchers.3,N13-1002,0,0.0639192,"oarse language models. These are particularly effective for morphologically rich languages (e.g., Ammar et al, 2013; Bisazza and Monz, 2014). In unpublished earlier experiments, we found that coarse LM combinations can yield better results than using just one. Besides biLMs (Niehues et al, 2011), there are other ways of incorporating additional source-language information in SMT. These include spectral clustering for HMM-based SMT (Zhao, Xing and Waibel, 2005), stochastic finite state transducers based on bilingual ngrams (Casacuberta and Vidal, 2004; Mariño et al, 2006; Crego and Yvon, 2010; Zhang et al, 2013), the lexicalized approach of (Hasan et al, 2008), factored Markov backoff models (Feng et al, 2014) and the “operation sequence model” (OSD) of (Durrani et al, 2011 and 2014). (Durrani et al 2014) appeared after our current paper was submitted. Our work and theirs shares an underlying motivation in which mkcls is applied to make earlier models more powerful, though the OSD models and ours are very different. We chose to implement biLMs primarily because this is easy to do in a phrase-based system. Automatic word clustering was described in (Jelinek, 1991; Brown et al, 1992). In “Brown” or “IB"
2016.amta-researchers.8,D11-1033,0,0.260007,"high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity betwee"
2016.amta-researchers.8,W15-3003,0,0.0599708,"ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domai"
2016.amta-researchers.8,K16-1031,1,0.892142,"15) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain and four language directions, show that this SSCNN method yields signiﬁcantly higher BLEU scores for the"
2016.amta-researchers.8,W12-3131,0,0.0978263,"an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO"
2016.amta-researchers.8,P14-1129,0,0.0302692,"a subset of data to be used for training an SMT system from a bilingual corpus, the user must specify the number N of sentence pairs to be chosen. The N sentence pairs with the highest global scores S(s, t) will be selected. This method is symmetrical - the roles of the source-language and target-language sides of the corpus are the same - and bilingual, because the IBM model 1 measures the degree to which each target sentence t is a good translation of its partner s, and vice versa. 2.2 Data Selection with Neural Net Joint Model (NNJM) The Neural Network Joint Model (NNJM), as described in (Devlin et al., 2014), is a joint language and translation model based on a feedforward neural net (NN). It incorporats a wide span of contextual information from the source sentence, in addition to the traditional n-gram information from preceding target-language words. Speciﬁcally, when scoring a target word wi , the NNJM inputs not only the n − 1 preceding words wi−n+1 , ..., wi−1 , but also 2m + 1 source words: the source word si most closely aligned with wi along with the m source words si−m , ..., si−1 to the left of si and the m source words si+1 , ..., si+m to the right of si . The NNJMs used in our experi"
2016.amta-researchers.8,P13-2119,0,0.133955,"picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language mod"
2016.amta-researchers.8,2015.mtsummit-papers.10,0,0.686015,"of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain"
2016.amta-researchers.8,2012.amta-papers.7,1,0.926825,"Missing"
2016.amta-researchers.8,2010.eamt-1.26,0,0.0690526,"ent over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URF"
2016.amta-researchers.8,N15-1011,0,0.0239437,"de up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al"
2016.amta-researchers.8,P14-1062,0,0.00850774,"on and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts"
2016.amta-researchers.8,D14-1181,0,0.00555345,"pair is made up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation"
2016.amta-researchers.8,W04-3250,0,0.208233,"f-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property for Arabic-to-English. In the table, the bilingua"
2016.amta-researchers.8,D07-1036,0,0.0723693,"Missing"
2016.amta-researchers.8,2011.iwslt-papers.5,0,0.0415047,"_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT s"
2016.amta-researchers.8,P10-2041,0,0.263994,"election, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language m"
2016.amta-researchers.8,J05-4003,0,0.19056,"pairs, can beneﬁt NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-lang"
2016.amta-researchers.8,W11-2124,0,0.186448,"nce. Essentially, it scores the extent to which both the source and target sentence are in-domain, but does not in any way penalize bad translations. We say that such a method is “symmetric”: it incorporates equal amounts of information from the source and the target language, but it is not “bilingual”: it does not incorporate information about the quality of translations. The main motivation for this paper is to explore CNN-based data selection techniques that are bilingual. It is based on semi-supervised CNNs that use bitokens as units instead of source or target words (Marino et al., 2006; Niehues et al., 2011). For the bitoken semi-supervised CNN, we should use the abbreviation “Bi-SSCNN”. We also experiment with the bilingual method that combines IBM model 1 and language model (LM) scores and neural network joint model. In this paper, we carried out experiments reported on two language pairs: Chinese-toEnglish and Arabic-to-English. We ﬁx the number of training sentences to be chosen for the data selection techniques so that they can be fairly compared, and measure the BLEU score on test data from the resulting MT systems. It turns out that three techniques have roughly the same performance in ter"
2016.amta-researchers.8,P02-1040,0,0.0972421,"ence as the criterion. This is considered to be a state-of-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property fo"
2016.amta-researchers.8,W16-2323,0,0.0611686,"Missing"
2016.amta-researchers.8,2014.amta-researchers.3,1,0.757845,"h SSCNNs that take as input the bitokens of (Marino et al., 2006; Niehues et al., 2011). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 2: Bitoken sequence. The paper (Niehues et al., 2011) describes a “bilingual language model” (biLM): the idea that SMT systems would beneﬁt from wider contextual information from the source sentence. BiLMs provide this context by aligning each target word in the training data with source words to create bitokens. An n-gram bitoken LM for the sequence of target words is then trained. Figure 2 (taken from (Stewart et al., 2014)) shows how a bitoken sequence is obtained from a word-aligned sentence pair for the English to French language pair. Unaligned target words (e.g., French word “d´’’ in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”) aligned with two instances of “nous” is duplicated: each target word aligned with it receives a copy of that source word. The word embeddings for bitokens are learned directly by word2vec, treating each bitoken as a word. For instance, in the French sentence shown in Figure 2,"
2016.amta-researchers.8,P15-2058,0,0.0114968,"ng sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts of in-domain data are available, we chose to u"
2016.amta-researchers.8,I08-2088,0,0.0573278,"e two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the d"
2016.amta-researchers.8,C04-1059,0,0.0497328,"will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for b"
2020.emnlp-main.465,N19-1423,0,0.0337366,"n tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks. 1 Introduction The widely successful masked language modeling paradigm popularized by BERT (Devlin et al., 2019) has recently been adapted to conditional masked language model (CMLM) training for semiautoregressive sequence generation (Ghazvininejad et al., 2019), where model predictions are conditioned on the complete input sequence and the observed (non-masked) portion of the output sequence. The CMLM’s simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors, such as iterative refinement of token sequences (Lee et al., 2018), refinement of non-linguistic intermediate representations (Kaiser et al., 2018; Shu e"
2020.emnlp-main.465,D19-1633,0,0.358732,"ference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks. 1 Introduction The widely successful masked language modeling paradigm popularized by BERT (Devlin et al., 2019) has recently been adapted to conditional masked language model (CMLM) training for semiautoregressive sequence generation (Ghazvininejad et al., 2019), where model predictions are conditioned on the complete input sequence and the observed (non-masked) portion of the output sequence. The CMLM’s simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors, such as iterative refinement of token sequences (Lee et al., 2018), refinement of non-linguistic intermediate representations (Kaiser et al., 2018; Shu et al., 2020) and learning to predict parallel edit operations (Stern et al., 2019; Gu et al., 2019). It is not obvious how to best perform inference wi"
2020.emnlp-main.465,D17-1036,1,0.810285,"rations T is less than the sentence length N it is semi-autoregressive; and when N = T it is fully autoregressive. Due to the use of a uniform distribution over reference contexts, training is agnostic to these different regimes. In general, we seek to minimize T without trading off too much quality. The challenge in doing so is to identify the subset of predictions that are most likely to provide suitable conditioning context for future iterations (Mansimov et al., 2019). Structural or linguistic dependencies in the output may also play an important role for resolving linguistic ambiguities (Martins and Kreutzer, 2017). M (t) {1,2,3} {} {1,2,3} {2,3} {3} {} {1,2,3} {2} {} t 0 1 0 1 2 3 0 1 2 • update-all: update tokens and scores at all positions, no constraint on new mask2 • update-masked: update tokens at masked positions only, no constraint on new mask3 • update-masked-sub: update tokens at masked positions only, new mask must be a subset of the current one In this paper we focus on the update-masked-sub strategy. It is empirically competitive (Section 4.1), and interesting because it corresponds to a valid probabilistic factorization of the target sequence, governed by a latent variable M = M (0) . . ."
2020.emnlp-main.465,W18-6319,0,0.0312515,"Missing"
2020.emnlp-main.465,D18-1149,0,0.233344,"uction The widely successful masked language modeling paradigm popularized by BERT (Devlin et al., 2019) has recently been adapted to conditional masked language model (CMLM) training for semiautoregressive sequence generation (Ghazvininejad et al., 2019), where model predictions are conditioned on the complete input sequence and the observed (non-masked) portion of the output sequence. The CMLM’s simplicity and its clear links to the very active field of linguistic representation learning are advantages over its semiautoregressive competitors, such as iterative refinement of token sequences (Lee et al., 2018), refinement of non-linguistic intermediate representations (Kaiser et al., 2018; Shu et al., 2020) and learning to predict parallel edit operations (Stern et al., 2019; Gu et al., 2019). It is not obvious how to best perform inference with the CMLM. Starting from a partially-observed output sequence, the optimal choice to complete it within a single step would be to generate the most likely token at each unobserved (masked) position independently. However, it is less clear how to progress from an initial, completely masked sequence to a final hypothesis semi-autoregressively over a number of"
2020.iwslt-1.27,P19-1126,1,0.860606,"Missing"
2020.iwslt-1.27,P18-1008,1,0.805687,"tandard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) data. For EnFr, we use newstest 2012+2013 for development, and newstest 2014 for test. For DeEn, we validate on newstest 2013 and report results on newstest 2015. We use BPE (Sennrich et al., 2016) on the training data to construct a 32K-type vocabulary that is shared between the source and target languages. 5.1 Models Our streaming and re-translation models are implemented in Lingvo (Shen et al., 2019), sharing architecture and hyper-parameters wherever possible. Our RNMT+ architecture (Chen et al., 2018) consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). Both encoder and decoder LSTMs have 512 hidden units, apply per-gate layer normalization (Ba et al., 2016), and use residual skip connections after the second layer. The models are regularized using a dropout of 0.2 and label smoothing of 0.1 (Szegedy et al., 2016). Models are optimized using 32-way data parallelism with Google Cloud’s TPUv3, using Adam (Kingma and Ba, 2015) with the learning rate schedule described in Chen et al. (2018) and a batch size of 4,096 sentence-pairs. Che"
2020.iwslt-1.27,N18-2079,0,0.609248,"anslation, the goal is to translate an incoming stream of source words with as low latency as possible. A typical application is speech translation, where we often assume the eventual output modality to also be speech. In a speech-to-speech scenario, target words must be appended to existing output with no possibility for revision. The corresponding translation task, which we refer to as streaming translation, has received considerable recent attention, generating custom approaches designed to maximize quality and minimize latency (Cho and Esipova, 2016; Gu et al., ∗ Equal contributions 2017; Dalvi et al., 2018; Ma et al., 2019a). However, for applications where the output modality is text, such as live captioning, the prohibition against revising output is overly stringent. The ability to revise previous partial translations makes simply re-translating each successive source prefix a viable strategy. Compared to streaming models, re-translation has the advantage of low latency, since it always attempts a translation of the complete source prefix, and high final-translation quality, since it is not restricted to preserving previous output. It has the disadvantages of higher computational cost, and a"
2020.iwslt-1.27,E17-1099,0,0.428399,"ion techniques that have not previously been studied together. (2) We provide the first empirical comparison of re-translation and streaming models, demonstrating that re-translation operating in a very low-revision regime can match or beat the quality-latency trade-offs of streaming models. (3) We test a 0-revision configuration of re-translation, and show that it is surprisingly competitive, due to the effectiveness of data augmentation with prefix pairs. 2 Related Work Cho and Esipova (2016) propose the first streaming techniques for NMT, using heuristic agents based on model scores, while Gu et al. (2017) extend their work with agents learned using reinforcement learning. Ma et al. (2019a) recently broke new ground by integrating their read-write agent directly into NMT training. Similar to Dalvi et al. (2018), they employ a simple agent that first reads k source to220 Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 220–227 c July 9-10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 kens, and then proceeds to alternate between writes and reads until the source sentence has finished. This agent is easily integr"
2020.iwslt-1.27,P02-1040,0,0.108727,"veral latency metrics content-aware, including average proportion (Cho and Esipova, 2016), consecutive wait (Gu et al., 2017), average lagging (Ma et al., 2019a), and differentiable average lagging (Arivazhagan et al., 2019b). We opt for differentiable average lagging (DAL) because of its interpretability and because it sidesteps some problems with average lagging (Cherry and Foster, 2019). It can be thought of as the average number of source tokens a system lags behind a perfectly simultaneous translator:  J  1 X 0 j−1 DAL = gj − J γ j=1 Translation quality is measured by calculating BLEU (Papineni et al., 2002) on the final output of each PTL; that is, standard corpus-level BLEU on complete translations. Specifically, we report tokenized, cased BLEU calculated by an internal tool. We make no attempt to directly measure the quality where γ = J/I accounts for the source and target having different lengths, and g 0 adjusts g to incorporate a minimal time cost of γ1 for each token:  gj  0  j=1 gj = 0 max gj , gj−1 + γ1 j &gt; 1 221 Source 1: Neue 2: Arzneimittel 3: k¨onnten 4: Lungen5: und 6: Eierstockkrebs 7: verlangsamen Content Delay Output New New Medicines New Medicines New drugs New drugs New drug"
2020.iwslt-1.27,P16-1162,0,0.0399594,"a randomly-selected fraction of their original lengths, 1/3 in this example. No effort is made to ensure that the two halves of the prefix pair are semantically equivalent. avoid confusion with Ma et al. (2019a)’s wait-k training, we refer to wait-k used for re-translation as wait-k inference.2 5 Experiments We use standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) data. For EnFr, we use newstest 2012+2013 for development, and newstest 2014 for test. For DeEn, we validate on newstest 2013 and report results on newstest 2015. We use BPE (Sennrich et al., 2016) on the training data to construct a 32K-type vocabulary that is shared between the source and target languages. 5.1 Models Our streaming and re-translation models are implemented in Lingvo (Shen et al., 2019), sharing architecture and hyper-parameters wherever possible. Our RNMT+ architecture (Chen et al., 2018) consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). Both encoder and decoder LSTMs have 512 hidden units, apply per-gate layer normalization (Ba et al., 2016), and use residual skip connections after the second layer. The mod"
2020.iwslt-1.27,D19-1137,0,0.734803,"reaming baselines above; and a more powerful Bidi+Beam system using bidirectional encoding and beam search of size 20, designed to test the impact of an improved base model. Training data is augmented through the proportional prefix training method unless stated otherwise (§ 4.1). Beam-search bias β is varied in the range 0.0 to 1.0 in increments of 0.2. When wait-k inference is enabled, k is varied in 1, 2, 4, 6, 8, 10, 15, 20, 30. Note that we do not need to re-train to test different values of β or k. 2 When wait-k truncation is combined with beam search, its behavior is similar to that of Zheng et al. (2019b): sequences are scored accounting for “future” tokens that will not be shown to the user. 223 5.2 Translation with few revisions Biased search and wait-k inference used together can reduce re-translation’s revisions, as measured by normalized erasure (NE in § 3.3), to negligible levels (Arivazhagan et al., 2019a). But how does retranslation compare to competing approaches? To answer this, we compare the quality-latency tradeoffs achieved by re-translation in a low-revision regime to those of our streaming baselines. First, we need a clear definition of low-revision re-translation. By manual"
2020.iwslt-1.27,D19-1144,0,0.309083,"reaming baselines above; and a more powerful Bidi+Beam system using bidirectional encoding and beam search of size 20, designed to test the impact of an improved base model. Training data is augmented through the proportional prefix training method unless stated otherwise (§ 4.1). Beam-search bias β is varied in the range 0.0 to 1.0 in increments of 0.2. When wait-k inference is enabled, k is varied in 1, 2, 4, 6, 8, 10, 15, 20, 30. Note that we do not need to re-train to test different values of β or k. 2 When wait-k truncation is combined with beam search, its behavior is similar to that of Zheng et al. (2019b): sequences are scored accounting for “future” tokens that will not be shown to the user. 223 5.2 Translation with few revisions Biased search and wait-k inference used together can reduce re-translation’s revisions, as measured by normalized erasure (NE in § 3.3), to negligible levels (Arivazhagan et al., 2019a). But how does retranslation compare to competing approaches? To answer this, we compare the quality-latency tradeoffs achieved by re-translation in a low-revision regime to those of our streaming baselines. First, we need a clear definition of low-revision re-translation. By manual"
2020.wmt-1.140,W05-0909,0,0.366937,"human evaluations for large scale comparisons and system development. The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standar"
2020.wmt-1.140,P16-2013,0,0.0508388,"Missing"
2020.wmt-1.140,W19-5204,1,0.929349,"is work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts— Translationese (Koppel and Ordan, 2011)—as well as source-independent artifacts—Translation Universals (Mauranen and Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Tran"
2020.wmt-1.140,2020.emnlp-main.5,1,0.864243,"ncerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standard references with paraphrased references, even when still using surface overlap metrics such as BLEU (Papin"
2020.wmt-1.140,P17-1012,1,0.826853,"e-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements. 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessor"
2020.wmt-1.140,W09-0421,0,0.0403691,"monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study was conducted for neural MT (Bogoychev and Sennrich, 2019). 3 Experimental Setup We first describe data and models, then present our human evaluation protocol. 3.1 Data We ran all experiments on the WMT 2019 English→German news translation task (Barrault et al., 2019). The task provides ∼38M parallel sentences. As Ger"
2020.wmt-1.140,J10-4005,0,0.0167351,". 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system d"
2020.wmt-1.140,P11-1132,0,0.0201247,"uch as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts— Translationese (Koppel and Ordan, 2011)—as well as source-independent artifacts—Translation Universals (Mauranen and Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Trans"
2020.wmt-1.140,2009.mtsummit-papers.9,0,0.0764009,"s that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have"
2020.wmt-1.140,E12-1026,0,0.0255666,"trics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source i"
2020.wmt-1.140,J12-4004,0,0.0211266,"trics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source i"
2020.wmt-1.140,W19-5358,0,0.166306,"for the first time that using these scores for system development can lead to significant improvements. 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore res"
2020.wmt-1.140,D17-1262,0,0.0207085,"e text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test senten"
2020.wmt-1.140,W07-0716,0,0.0811537,"Missing"
2020.wmt-1.140,E17-1083,0,0.0225836,"uning, especially for Statistical Machine Translation (SMT). Madnani et al. (2007) introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g. (Marton et al., 2009), but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B L"
2020.wmt-1.140,D09-1040,0,0.0472392,"recently been shown to be useful for system evaluation (Freitag et al., 2020). Our work considers applying the same methodology for system tuning. There is some earlier work relying on automated paraphrases for system tuning, especially for Statistical Machine Translation (SMT). Madnani et al. (2007) introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g. (Marton et al., 2009), but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also"
2020.wmt-1.140,W19-5333,0,0.176462,"efits: it can help identify choices which improve BLEU on standard references but have limited impact on final human evaluations; or those that result in better translations for the human reader, but worse in terms of standard reference BLEU. Conversely, it might turn out that paraphrased references are not robust enough to support system development due to the presence of ‘metric honeypots’: settings that produce poor translations, but which are nevertheless assigned high BLEU scores. To address these points, we revisit the major design choices of the best English→German system from WMT2019 (Ng et al., 2019) step-by-step, and measure their impact on standard reference BLEU as well as on paraphrased BLEU. This allows us to measure the extent to which steps such as data cleaning, back-translation, fine-tuning, ensemble decoding and reranking benefit standard reference BLEU more than paraphrase BLEU. Revisiting these development choices with the two metrics results in two systems with quite different behaviors. We conduct a human evaluation for adequacy and fluency to assess the overall impact of designing a system using paraphrased BLEU. Our main findings show that optimizing for paraphrased BLEU i"
2020.wmt-1.140,P03-1021,0,0.240098,"enerated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts— Translationese (Koppel and Ordan, 2011)—as well as source-independent artifacts—Translation Universals (Mauranen and Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For"
2020.wmt-1.140,P02-1040,0,0.112525,"ranslations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development. The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems whic"
2020.wmt-1.140,W18-6319,0,0.0224486,"erence, newstest2018.orig-en.p, as part of our work. 3.2 Models For our translation models, we adopt the transformer implementation from Lingvo (Shen et al., 2019), using the transformer-big model size (Vaswani et al., 2017). We use a vocabulary of 32k subword units and exponentially moving averaging of checkpoints (EMA decay) with the weight decrease parameter set to α = 0.999 (Buduma and Locascio, 2017). We used a batch size of around 32k sentences in all our experiments. We report B LEU (Papineni et al., 2002) in addition to human evaluation. All B LEU scores are calculated with sacreBLEU (Post, 2018)1 . 3.3 Human Evaluation To collect human rankings, we ran side-by-side evaluation for overall quality and fluency. We hired 20 linguists and divided them equally between the two evaluations. Each evaluation included 1,000 items with each item being rated exactly once. We acquired only a single rating per sentence from the professional linguists as we found that they were 1 BLEU+case.mixed+lang.ende+numrefs.1+smooth.exp+ SET+tok.13a+version.1.4.12 SET ∈{wmt18, wmt19, wmt19/google/ar, wmt19/google/arp, wmt19/google/wmtp} 1185 more reliable than crowd workers (Toral, 2020). We evaluated the orig"
2020.wmt-1.140,W05-0908,0,0.169955,"Missing"
2020.wmt-1.140,2020.acl-main.691,1,0.731231,"age more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study wa"
2020.wmt-1.140,P19-1605,1,0.835756,"atistical Machine Translation (SMT). Madnani et al. (2007) introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g. (Marton et al., 2009), but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases (Mallinson et al., 2017; Roy and Grangier, 2019). This makes their use difficult for evaluation since (Freitag et al., 2020) suggests that substantial paraphrasing – ‘paraphrase as much as possible‘ – is necessary for evaluation. Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning (He and Way, 2010; Servan and Schwenk, 2011) with Minimum Error Rate Training, MERT (Och, 2003). This work showed some specific cases where Translation Error Rate (TER) was superior to B LEU. Our work is also rela"
2020.wmt-1.140,W17-0230,0,0.0152883,"the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation, Lembersky et al. (2012b) and Stymne (2017) explored how the translation direction affects translation results. Holmqvist et al. (2009) noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for targetoriginal sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study was conducted for neural MT (Bogoychev and Sennrich, 2019). 3 Experimental Setup We first describe data and models, then present our human evaluation protocol. 3.1 Data We ran all experiments on the WMT 2019 English→German news"
2020.wmt-1.140,2020.eamt-1.20,0,0.31806,"tric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements. 1 Introduction Machine Translation (MT) has shown impressive progress in recent years. Neural architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have greatly contributed to this improvement, especially for languages with abundant training data (Bojar et al., 2016, 2018; Barrault et al., 2019). This progress creates novel challenges for the evaluation of machine translation, both for human (Toral, 2020; L¨aubli et al., 2020) and automated evaluation protocols (Lo, 2019; Zhang et al., 2019). Both types of evaluation play an important role in machine translation (Koehn, 2010). While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validat"
2020.wmt-1.140,W18-6312,0,0.0284513,"Kujam¨aki, 2004). The professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translati"
2020.wmt-1.140,W18-6314,0,0.156971,"Missing"
2020.wmt-1.140,D19-1571,0,0.0203297,"n and orig-de subsets. The orig-en.p sets use paraphrased references instead of standard references. Our experiments compared newstest2018.joint and newstest2018.orig-en.p for system tuning. The standard newstest2018 and newstest2019 sets are newstest2018.joint and newstest2019.orig-en, respectively. only for standard reference B LEU. Similar to the WMT 2019 winning submission, we include the ensemble approach in our system that is optimized on the joint B LEU scores. However, we do not include it in our system optimized on B LEU P. 4.3 Reranking Finally, we extend the noisy-channel approach (Yee et al., 2019) which consists of re-ranking the top-50 beam search output of either the ensemble model (when tuned for B LEU) or the fine-tuned model (when tuned for B LEU P). Instead of using 4 features—forward probability, backward probability, language model and word penalty—we use 11 forward probabilities, 10 backward probabilities and 2 language model scores. Different to (Ng et al., 2019), we did not pick the re-ranking weights through random search, but used MERT (Och, 2003) for efficient tuning. The 11 different forward translation scores come from different English→German NMT models that are replic"
2020.wmt-1.140,W19-5208,0,0.0744881,"he professional translation community studies both systematic biases inherent to translated texts (Baker, 1993; Selinker, 1972), as well as biases resulting specifically from interference from the source text (Toury, 1995). For MT, Freitag et al. (2019) point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well (Toral et al., 2018; Zhang and Toral, 2019; Graham et al., 2019). More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation (Fomicheva and Specia, 2016; Ma et al., 2017). Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied (Kurokawa et al., 2009; Lembersky et al., 2012a; Bogoychev and Sennrich, 2019; Riley et al., 2020). 1184 Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT e"
2020.wmt-1.140,2006.amta-papers.25,0,0.212798,"at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development. The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics (Barrault et al., 2019). Most metrics such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006) measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed (Banerjee and Lavie, 2005; Lo, 2019; Zhang et al., 2019). Orthogonal to the work of building improved metrics, Freitag et al. (2020) hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human ‘translationese‘ effects. These standard references might favor systems which excel at reproducing these e"
2021.naacl-main.91,2020.wmt-1.140,1,0.761763,"ons. This has several disadvantages. First, high-quality reference translations are expensive to create. This means that in practice, evaluation is usually carried out with relatively small, carefully curated test corpora. The need for careful preparation limits the number of domains for which an MT system can be conveniently assessed, and small test-set sizes can make it difficult to draw robust conclusions (Card et al., 2020). Second, enshrining ground truth in a small number of references (usually just one) is inherently problematic, since valid translations can vary along many dimensions; Freitag et al. (2020b) demonstrate that different (correct) references for the same test set can result in different system rankings according to the same reference-based metric. Finally, scoring the similarity between an MT hypothesis and a reference translation involves recognizing the extent to which they are mutual paraphrases. When gross discrepancies exist, this is a relatively easy problem 1 for which surface-level metrics can provide a reliExcept Gujarati, which was absent from their training able signal, but capturing the subtle errors typical corpus. 1158 Proceedings of the 2021 Conference of the North"
2021.naacl-main.91,2020.emnlp-main.5,1,0.816965,"ons. This has several disadvantages. First, high-quality reference translations are expensive to create. This means that in practice, evaluation is usually carried out with relatively small, carefully curated test corpora. The need for careful preparation limits the number of domains for which an MT system can be conveniently assessed, and small test-set sizes can make it difficult to draw robust conclusions (Card et al., 2020). Second, enshrining ground truth in a small number of references (usually just one) is inherently problematic, since valid translations can vary along many dimensions; Freitag et al. (2020b) demonstrate that different (correct) references for the same test set can result in different system rankings according to the same reference-based metric. Finally, scoring the similarity between an MT hypothesis and a reference translation involves recognizing the extent to which they are mutual paraphrases. When gross discrepancies exist, this is a relatively easy problem 1 for which surface-level metrics can provide a reliExcept Gujarati, which was absent from their training able signal, but capturing the subtle errors typical corpus. 1158 Proceedings of the 2021 Conference of the North"
2021.naacl-main.91,W19-5302,0,0.158498,"ly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities. 1 Introduction of high-quality MT is more difficult, and it is not clear whether it is substantially easier than scoring the similarity between texts in different languages. These problems can be avoided by looking only at the source text when assessing MT output. There is evidence that this is the best practice for human evaluation (Toral, 2020). Moreover, it has recently been investigated for automatic metrics as well (Yankovskaya et al., 2019; Lo, 2019; Zhao et al., 2020; Ma et al., 2019). Such reference-free metrics are flexible and scalable, but since they are essentially performing the same task as an MT model, they raise a circularity concern: if we can reliably score MT output, why wouldn’t we use the scoring model to produce better output? One answer to this is practical: the scoring model might be too large to deploy, or it might not easily support efficient inference (Yu et al., 2016). A more interesting answer is that a scoring model could be set up to provide a signal that is complementary to the systems under evaluation. That is, it might be capable of correctly ran"
2021.naacl-main.91,2020.acl-main.448,0,0.0431198,"roportional to a unigram estimate q α (˜ x|x). 1160 for 18 language pairs. For each language pair, we compute a metric score for each system, then use correlation with the provided human scores to assess the quality of our metric.3 Following Ma et al. (2019) we measure correlation using Pearson’s coefficient, and use Williams’ test (Williams, 1959) to compute the significance of correlation differences, with a p-value &lt; 0.05. Ma et al. (2019) note that correlation scores are unrealistically high for many language pairs, and suggest using only the best k systems for small values of k. However, Mathur et al. (2020) show that this results in noisy and unreliable estimates. We adopt their suggestion to instead remove outlier systems whose scores have large deviations from the median according to the formula: ˜ |h − h| &gt; 2.5, ˜ 1.483 × medianh (|h − h|) ˜ is where h is a system-level human score, and h the median score across all systems for a given language pair. To summarize a metric’s performance across a set of language pairs, we report the weighted average of its Pearson correlations across languages. We first apply the Fisher Z-transformation to normalize raw language-specific correlations, then weig"
2021.naacl-main.91,2020.acl-main.64,0,0.021386,"ularizing token-level probabilities (Monte-Carlo dropout, subword sampling) and for combining them into system-level scores (summary statistics over tokens, confidence thresholds over sentences). Finally, we analyze the results of our best model, measuring how its performance depends on various factors: language pair and human-judgment methodology, output quality, proximity to the systems under evaluation, and size of the test set. 2 Related Work Reference-free evaluation is widely used for many NLP tasks such as grammatical error correction (Napoles et al., 2016), dialog (Sinha et al., 2020; Mehri and Eskenazi, 2020) and text generation (Ethayarajh and Sadigh, 2020). There has been recent interest in reference-free evaluation for MT, which was a joint track between the WMT 2019 metrics task (Ma et al., 2019) and quality estimation task (Fonseca et al., 2019). Reference-free metrics competed head-to-head with standard metrics, and generally did worse. However, the results from the best reference-free systems, UNI+ (Yankovskaya et al., 2019) and YiSi-2 (Lo, 2019) were surprisingly close to the standard metric scores on the language pairs for which they were evaluated. UNI+ computes word-level embeddings for"
2021.naacl-main.91,D16-1228,0,0.0239105,"iginal architecture); and different methods for regularizing token-level probabilities (Monte-Carlo dropout, subword sampling) and for combining them into system-level scores (summary statistics over tokens, confidence thresholds over sentences). Finally, we analyze the results of our best model, measuring how its performance depends on various factors: language pair and human-judgment methodology, output quality, proximity to the systems under evaluation, and size of the test set. 2 Related Work Reference-free evaluation is widely used for many NLP tasks such as grammatical error correction (Napoles et al., 2016), dialog (Sinha et al., 2020; Mehri and Eskenazi, 2020) and text generation (Ethayarajh and Sadigh, 2020). There has been recent interest in reference-free evaluation for MT, which was a joint track between the WMT 2019 metrics task (Ma et al., 2019) and quality estimation task (Fonseca et al., 2019). Reference-free metrics competed head-to-head with standard metrics, and generally did worse. However, the results from the best reference-free systems, UNI+ (Yankovskaya et al., 2019) and YiSi-2 (Lo, 2019) were surprisingly close to the standard metric scores on the language pairs for which they"
2021.naacl-main.91,P02-1040,0,0.113179,"ent paraphrase recognition when used in zero-shot mode to compare MT output with reference sentences in the same language. On the WMT 2019 metrics task, their method (Prism) beat or tied all previous reference-based metrics on all languages.1 Although it was not the main focus of their work, Prism achieved a new state-of-the-art as a referencefree metric, simply scoring target given source text using an MT model, in a post-competition comparison to the 2019 “Quality Estimation as a metric” shared task (Ma et al., 2019). Traditional automatic metrics for machine translation (MT), such as BLEU (Papineni et al., 2002), score MT output by comparing it to one or more reference translations. This has several disadvantages. First, high-quality reference translations are expensive to create. This means that in practice, evaluation is usually carried out with relatively small, carefully curated test corpora. The need for careful preparation limits the number of domains for which an MT system can be conveniently assessed, and small test-set sizes can make it difficult to draw robust conclusions (Card et al., 2020). Second, enshrining ground truth in a small number of references (usually just one) is inherently pr"
2021.naacl-main.91,2020.acl-main.252,0,0.0422351,"Missing"
2021.naacl-main.91,2020.acl-main.220,0,0.0201059,"rent methods for regularizing token-level probabilities (Monte-Carlo dropout, subword sampling) and for combining them into system-level scores (summary statistics over tokens, confidence thresholds over sentences). Finally, we analyze the results of our best model, measuring how its performance depends on various factors: language pair and human-judgment methodology, output quality, proximity to the systems under evaluation, and size of the test set. 2 Related Work Reference-free evaluation is widely used for many NLP tasks such as grammatical error correction (Napoles et al., 2016), dialog (Sinha et al., 2020; Mehri and Eskenazi, 2020) and text generation (Ethayarajh and Sadigh, 2020). There has been recent interest in reference-free evaluation for MT, which was a joint track between the WMT 2019 metrics task (Ma et al., 2019) and quality estimation task (Fonseca et al., 2019). Reference-free metrics competed head-to-head with standard metrics, and generally did worse. However, the results from the best reference-free systems, UNI+ (Yankovskaya et al., 2019) and YiSi-2 (Lo, 2019) were surprisingly close to the standard metric scores on the language pairs for which they were evaluated. UNI+ compute"
2021.naacl-main.91,2020.emnlp-main.8,0,0.10668,"’t we use the scoring model to produce better output? One answer to this is practical: the scoring model might be too large to deploy, or it might not easily support efficient inference (Yu et al., 2016). A more interesting answer is that a scoring model could be set up to provide a signal that is complementary to the systems under evaluation. That is, it might be capable of correctly ranking competing MT hypotheses even when its own preferred hypothesis is worse on average than those of the systems it is evaluating. In our experiments we find that this can indeed be the case. In recent work, Thompson and Post (2020) showed that a single multilingual MT model trained on 39 languages can achieve excellent paraphrase recognition when used in zero-shot mode to compare MT output with reference sentences in the same language. On the WMT 2019 metrics task, their method (Prism) beat or tied all previous reference-based metrics on all languages.1 Although it was not the main focus of their work, Prism achieved a new state-of-the-art as a referencefree metric, simply scoring target given source text using an MT model, in a post-competition comparison to the 2019 “Quality Estimation as a metric” shared task (Ma et"
2021.naacl-main.91,W19-5410,0,0.0480859,"Missing"
2021.naacl-main.91,2020.acl-main.756,0,0.0282354,"y applied at the sentence systems under evaluation and find no evidence that level, and it can make use of powerful “glass-box” this is a source of bias. Despite using no references, features which capture the internals of an MT sysour model achieves approximate parity with BLEU tem. In contrast, reference-free evaluation is most both in system-level correlation with human judg- naturally applied at the system (test-set) level, and ment, and when used for pairwise comparisons. ideally should make no assumptions about the sys1159 tems under evaluation. The second task is parallelcorpus mining (Zhang et al., 2020; Yang et al., 2019), which aims to identify valid translations at various levels of granularity. Its scoring aspect is similar to reference-free evaluation, but it is applied to a different input distribution, attempting to identify human-generated translation pairs rather than scoring MT outputs for a given human-generated source text. 3 Methods We P aim to generate a quality score s(X, Y ) = x,y s(x, y) for source and target texts X, Y which consist of segment (nominally, sentence) pairs x, y. We assume no document or ordering information among segments, and do not directly evaluate scores"
2021.naacl-main.91,2020.acl-main.151,0,0.0381675,"Missing"
2021.naacl-main.91,D19-1053,0,0.0224356,"ce and MT output sentences using pre-trained multilingual BERT and LASER (Artetxe and Schwenk, 2019) models, then feeds averaged vectors to a neural classifier trained to predict human scores from previous MT metrics tasks. YiSi-2 is similar, except that it works in an unsupervised fashion, computing similarities between mBERT embeddings for aligned source and target words, and returning an F-measure statistic. In more recent work, Zhao et al. (2020) adopt a similar approach based on mBERT, aligning representations from multilingual embedding spaces before computing distances with MoverScore (Zhao et al., 2019), and adding a GPT-based target-side language model. We demonstrate improvements over the original The current state-of-the-art in reference-free evaluPrism metric due to model capacity and different methods for combining probabilities; surprisingly, ation for MT is represented by the Prism approach (Thompson and Post, 2020) which we extend here. we find little gain from adjusting the domain or languages in the original multilingual corpus (alIt is worth distinguishing reference-free evaluathough we show that a competition-grade English- tion from two related tasks that share formal simiGerman"
2021.naacl-main.91,2020.eamt-1.20,0,0.0185314,"t by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities. 1 Introduction of high-quality MT is more difficult, and it is not clear whether it is substantially easier than scoring the similarity between texts in different languages. These problems can be avoided by looking only at the source text when assessing MT output. There is evidence that this is the best practice for human evaluation (Toral, 2020). Moreover, it has recently been investigated for automatic metrics as well (Yankovskaya et al., 2019; Lo, 2019; Zhao et al., 2020; Ma et al., 2019). Such reference-free metrics are flexible and scalable, but since they are essentially performing the same task as an MT model, they raise a circularity concern: if we can reliably score MT output, why wouldn’t we use the scoring model to produce better output? One answer to this is practical: the scoring model might be too large to deploy, or it might not easily support efficient inference (Yu et al., 2016). A more interesting answer is that a sc"
A00-1019,J96-1002,0,0.00664237,"ere a(t', s) E [0, 1] are context-dependent interpolation coefficients. For example, the translation model could have a higher weight at the start of a sentence but the contribution of the language model might become more important in the middle or the end of the sentence• A study of the weightings for these two models is described elsewhere• In the work described here we did not use the contribution of the language model (that is, a(t', s) = O, V t', s). Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work (Brown et al., 1993; Berger et al., 1996; Och and Weber, 98; Wang and Waibel, 98; Wu and Wong, 98). These studies report improvements on some specific tasks (task-oriented limited vocabulary) which by nature are very different from the task TRANSTYPE is devoted to. Furthermore, the underlying decoding strategies are too time consuming for our application• We therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models - - Ms and M~ - both based on IBM-like model 2(Brown et al., 1993). Ms was trained on single words and Mu, described 136 in section"
A00-1019,1997.mtsummit-papers.1,0,0.096049,"Missing"
A00-1019,P95-1032,0,0.0543477,"Missing"
A00-1019,P93-1003,0,0.027342,"Missing"
A00-1019,W97-0311,0,0.0428982,"Missing"
A00-1019,P97-1061,0,0.0366885,"ord model, A u the 10 best units according to the unit model. Modeling = et • a • premier disait 3 p(w~) Finding Monolingual Units Finding relevant units in a text has been explored in many areas of natural language processing. Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus. For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus. This method allows the efficient retrieval of arbitrary length n-grams (Nagao and Mori, 94; Haruno et al., 96; Ikehara et al., 96; Shimohata et al., 1997; Russell, 1998). The literature abounds in measures that can help to decide whether words that co-occur are linguistically significant or not. In this work, the strength of association of a sequence of words w [ = w l , . . . , w n is computed by two measures: a likelihood-based one p(w'~) (where g is the likelihood ratio given in (Dunning, 93)) and an entropy-based one e(w'~) (Shimohata et al., 1997). Letting T stand for the training text and m a token: 137 Intuitively, the first measurement accounts for the fact that parts of a sequence of words that should be considered as a whole should n"
A00-1019,P94-1033,0,0.046262,"Missing"
A00-1019,P98-2221,0,0.048473,"Missing"
A00-1019,P98-2230,0,0.027694,"Missing"
A00-1019,W99-0604,0,\N,Missing
A00-1019,C90-2045,0,\N,Missing
A00-1019,C96-1006,0,\N,Missing
A00-1019,J93-2003,0,\N,Missing
A00-1019,C94-1101,0,\N,Missing
A00-1019,C96-1070,0,\N,Missing
A00-1019,C90-3008,0,\N,Missing
A00-1019,C96-2098,0,\N,Missing
A00-1019,P99-1067,0,\N,Missing
A00-1019,P98-2158,0,\N,Missing
A00-1019,C98-2153,0,\N,Missing
A00-1019,P97-1037,0,\N,Missing
A00-1019,P96-1003,0,\N,Missing
A00-1019,P94-1032,0,\N,Missing
A00-1019,C86-1077,0,\N,Missing
A00-1019,C96-1089,0,\N,Missing
A00-1019,C98-2225,0,\N,Missing
A00-1019,P98-2162,0,\N,Missing
A00-1019,C98-2157,0,\N,Missing
A00-1019,C98-2216,0,\N,Missing
A00-1019,langlais-etal-2000-evaluation,1,\N,Missing
A00-1019,P99-1041,0,\N,Missing
A00-1019,C96-1097,0,\N,Missing
A00-1019,P97-1047,0,\N,Missing
C04-1046,2003.mtsummit-papers.52,1,\N,Missing
C04-1046,J93-2003,0,\N,Missing
C04-1046,N03-2021,0,\N,Missing
C04-1046,J96-1002,0,\N,Missing
C04-1046,P02-1040,0,\N,Missing
C04-1046,J04-4002,0,\N,Missing
C04-1046,P02-1038,0,\N,Missing
C04-1046,P00-1016,0,\N,Missing
C04-1046,W03-0413,1,\N,Missing
C08-1115,E06-1005,1,\N,Missing
C08-1115,W07-0728,1,\N,Missing
C08-1115,W03-1729,1,\N,Missing
C08-1115,P07-1040,0,\N,Missing
C08-1115,W07-0732,1,\N,Missing
C08-1115,W07-0724,1,\N,Missing
C08-1115,W07-0718,0,\N,Missing
C08-1115,P07-1019,0,\N,Missing
C08-1115,W07-0717,1,\N,Missing
C08-1115,P03-1021,0,\N,Missing
C10-1069,N06-1003,0,0.104358,"enk et al. (2007) on continuous space Ngram models, where a neural network is employed to smooth translation probabilities. However, both Schwenk et al.’s smoothing technique 608 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, August 2010 and the system to which it is applied are quite different from ours. Phrase clustering is also somewhat related to work on paraphrases for SMT. Key papers in this area include (Bannard and Callison-Burch, 2005), which pioneered the extraction of paraphrases from bilingual parallel corpora, (Callison-Burch et al., 2006) which showed that paraphrase generation could improve SMT performance, (Callison-Burch, 2008) and (Zhao et al., 2008) which showed how to improve the quality of paraphrases, and (Marton et al., 2009) which derived paraphrases from monolingual data using distributional information. Paraphrases typically help SMT systems trained on under 100K sentence pairs the most. The phrase clustering algorithm in this paper outputs groups of source-language and targetlanguage phrases with similar meanings: paraphrases. However, previous work on paraphrases for SMT has aimed at finding translations for sour"
C10-1069,W08-2119,0,0.0531444,"Missing"
C10-1069,J10-4005,0,0.0154983,"n with small (less than 100K sentence pairs) amounts of training data, contrary to what one would expect from the paraphrasing literature. We have only begun to explore the original smoothing approach described here. 1 Introduction and Related Work The source-language and target-language “phrases” employed by many statistical machine translation (SMT) systems are anomalous: they are arbitrary sequences of contiguous words extracted by complex heuristics from a bilingual corpus, satisfying no formal linguistic criteria. Nevertheless, phrase-based systems perform better than word-based systems (Koehn 2010, pp. 127-129). In this paper, we look at what happens when we cluster together these anomalous but useful entities. Here, we apply phrase clustering to obtain better estimates for “backward” probability P(s|t) and “forward” probability P(t|s), where s is a source-language phrase, t is a target-language phrase, and phrase pair (s,t) was seen at least once in training data. The current work is thus related to work on smoothing P(s|t) and P(t|s) – see (Foster et al., 2006). The relative frequency estimates for P(s|t) and P(t|s) are PRF (s |t ) =# (s, t ) /# t and PRF (t |s ) =# (s , t ) /# s , w"
C10-1069,D08-1076,0,0.0318877,"word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a loglinear feature combination that includes language models, the distortion components, relative frequency estimators PRF(s|t) and PRF(t|s) and lexical weight estimators PLW(s|t) and PLW(t|s). The PLW() components are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. The phrasecluster-based components PPC(s|t) and PPC(t|s) are incorporated as additional loglinear feature functions. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 4.1 Data We evaluated our method on C-E and F-E tasks. For each pair, we carried out experiments on training corpora of different sizes. C-E data were from the NIST1 2009 evaluation; all the allowed bilingual corpora except the UN corpus, Hong Kong Hansard and Hong Kong Law corpus were used to estimate the translation model. For C-E, we trained two 5-gram language models: the first on the English side of the parallel data, and the second on the English Gigaword corpus. Our C-E development set is made up mainly of data from the NIST 2005 test set; it also includes some balanced-genre web-text"
C10-1069,P02-1040,0,0.0842533,"reference is provided for each source input sentence. Two language models are used in this task: one is the English side of the parallel data, and the second is the English side of the GigaFrEn corpus. Table 2 summarizes the training, development and test corpora for F-E tasks. 4.2 Amount of clustering and metric For both C-E and E-F, we assumed that phrases seen only once in training data couldn’t be clustered reliably, so we prevented these “count 1” phrases from participating in clustering. The key 2 http://www.statmt.org/wmt10/ 613 Results and discussion Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Our first experiment evaluated the effects of the phrase clustering features given various amounts of training data. Figure 4 gives the BLEU score improvements for the two language pairs, with results for each pair averaged over two test sets (training data size shown as #sentences). The improvement is largest for medium amounts of training data. Since the F-E training data has more words per sentence than C-E, the two peaks would have been closer together if we’d put #words on the x axis: improvements for both tasks peak aroun"
C10-1069,N04-1033,0,0.051161,"partial French phrase cluster 4 We carried out experiments on a standard onepass phrase-based SMT system with a phrase table derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a loglinear feature combination that includes language models, the distortion components, relative frequency estimators PRF(s|t) and PRF(t|s) and lexical weight estimators PLW(s|t) and PLW(t|s). The PLW() components are based on (Zens and Ney, 2004); Foster et al. (2006) found this to be the most effective lexical smoothing technique. The phrasecluster-based components PPC(s|t) and PPC(t|s) are incorporated as additional loglinear feature functions. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 4.1 Data We evaluated our method on C-E and F-E tasks. For each pair, we carried out experiments on training corpora of different sizes. C-E data were from the NIST1 2009 evaluation; all the allowed bilingual corpora except the UN corpus, Hong Kong Hansard and Hong Kong Law corpus were used to estimate the transla"
C10-1069,P08-1089,0,0.0638985,"ver, both Schwenk et al.’s smoothing technique 608 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 608–616, Beijing, August 2010 and the system to which it is applied are quite different from ours. Phrase clustering is also somewhat related to work on paraphrases for SMT. Key papers in this area include (Bannard and Callison-Burch, 2005), which pioneered the extraction of paraphrases from bilingual parallel corpora, (Callison-Burch et al., 2006) which showed that paraphrase generation could improve SMT performance, (Callison-Burch, 2008) and (Zhao et al., 2008) which showed how to improve the quality of paraphrases, and (Marton et al., 2009) which derived paraphrases from monolingual data using distributional information. Paraphrases typically help SMT systems trained on under 100K sentence pairs the most. The phrase clustering algorithm in this paper outputs groups of source-language and targetlanguage phrases with similar meanings: paraphrases. However, previous work on paraphrases for SMT has aimed at finding translations for source-language phrases in the system’s input that weren’t seen during system training. Our approach is completely useless"
C10-1069,D09-1040,0,\N,Missing
C10-1069,D08-1021,0,\N,Missing
C10-1069,D07-1045,0,\N,Missing
C10-1069,W06-1607,1,\N,Missing
C10-1069,P05-1074,0,\N,Missing
C10-1069,P05-1033,0,\N,Missing
C10-1069,P07-1019,0,\N,Missing
C96-1067,C90-3008,0,0.0186597,"em&apos;s output; and interactive M T (IMT), which involves a dialog between person and machine. Of the three, I M T is the most ambitious and theoretically the most powerflfl. It has a potential advantage over postedition in t h a t information imparted to the system may help it to avoid cascading errors that would later require much greater effort to correct; and it has a potential advantage over preedition in that knowledge of the machine&apos;s current state may be useful in reducing the number of analyses the human is required to provide. Existing approaches to I M T (Blanchon, 1994; Boitet, 1990; Brown and Nirenburg, 1990; Kay, 1973; M a r u y a m a and Watanabe, 1990; Whitelock et al., 1986; Zajac, 1988) place the MT system 394 ca in control of the translation process and for the most part limit the h u m a n &apos; s role to performing various source language disambiguations on deman& Although this arrangement is appropriate for some applications, notably those in which the user&apos;s knowledge of tile target language may be limited, or where there are multiple target, languages, it is not well suited to tile needs of professional oi&apos; other highly skilled translators. The lack of direct human control over the tinal t"
C96-1067,W93-0301,0,0.236137,"Missing"
C96-1067,H93-1038,0,0.0458674,"Missing"
C96-1067,1991.mtsummit-papers.13,0,0.0368099,"et language may be limited, or where there are multiple target, languages, it is not well suited to tile needs of professional oi&apos; other highly skilled translators. The lack of direct human control over the tinal target text (modulo postedition) is a serious drawback in this case, and it is not clear that, for a competent translator, disambiguat, ing a source text, is much easier than translating it. This conclusion is supported by the fact that true I M T is not, to our knowledge, used in most modern translator&apos;s support environments, eg (Eurolang, 1995; I,&apos;rederking et al., 1993; IBM, 1995; Kugler et al., 1991; Nirenburg, 1992; ~li&apos;ados, 1995). Such environments, when they incorporate MT at all, tend to do so wholesale, giving the user control over whether and when an M T component is invoked, as well as extensive postediting facilities for modifying its outtmt, but not the ability to intervene while it is operating. In our view, this state of affairs should not be taken as evidence that I M T for skilled translators is an inherently bad idea. We feel that there is an alternate approach which has the potential to avoid most of the problems with conventional I M T in this context,: use the target te"
C96-1067,1992.tmi-1.7,1,0.749219,"r cxamples from many other countries might generate the French sentence shown. The state-transition probabilities (horizontal arrows) are all 1/9 for model 1, and depend on the next state for model 2, eg p((froms, 6} I&apos;) = a(516). The output probabilities (vertical arrows) depend on the words involved, eg p(d&apos; I {from~, 6}) = p(d&apos; I from ). matrices for these models, they have the property that- unlike HMM&apos;s in general they generate target-language words independently. The probability of generating hypothesis t at position i is just: Isl p(tls, i ) = E P ( t l s i ) a ( j method described in (Simard et al., 1992), with non one-to-one aliglmmnts arid sentences longer than 50 words filtered out; the ret~fine(l material consisted of 36M English words and 37M Fren(:h words. 4 Hypothesis Generation li) j=0 where sj is the j t h source text token (so is a null token), p(tlsj) is a word-for-word translation probability, and a ( j l i ) is a position alignment probability (equal to 1/( M + 1) for inodel 1). We introduced a simple enhancement to the IBM models designed to extend their coverage, and make them more compact. It is based on the observation that there are (at least) three classes of English forms w"
C96-1067,C86-1077,0,0.0100899,"rson and machine. Of the three, I M T is the most ambitious and theoretically the most powerflfl. It has a potential advantage over postedition in t h a t information imparted to the system may help it to avoid cascading errors that would later require much greater effort to correct; and it has a potential advantage over preedition in that knowledge of the machine&apos;s current state may be useful in reducing the number of analyses the human is required to provide. Existing approaches to I M T (Blanchon, 1994; Boitet, 1990; Brown and Nirenburg, 1990; Kay, 1973; M a r u y a m a and Watanabe, 1990; Whitelock et al., 1986; Zajac, 1988) place the MT system 394 ca in control of the translation process and for the most part limit the h u m a n &apos; s role to performing various source language disambiguations on deman& Although this arrangement is appropriate for some applications, notably those in which the user&apos;s knowledge of tile target language may be limited, or where there are multiple target, languages, it is not well suited to tile needs of professional oi&apos; other highly skilled translators. The lack of direct human control over the tinal target text (modulo postedition) is a serious drawback in this case, and"
C96-1067,C88-2160,0,0.0523146,"Missing"
C96-1067,C90-2045,0,\N,Missing
C96-1067,J93-2003,0,\N,Missing
C96-1067,C94-1017,0,\N,Missing
C96-1067,C90-3006,0,\N,Missing
C96-1067,C90-2006,0,\N,Missing
D07-1103,N03-1017,0,0.134973,"tional c Natural Language Learning, pp. 967–975, Prague, June 2007. 2007 Association for Computational Linguistics 2 2.1 Background Theory Our Approach to Statistical Machine Translation We define a phrasetable as a set of source phrases (ngrams) s˜ and their translations (m-grams) t˜, along with associated translation probabilities p(˜ s|t˜) and p(t˜|˜ s). These conditional distributions are derived from the joint frequencies c(˜ s, t˜) of source / target n, m-grams observed in a word-aligned parallel corpus. These joint counts are estimated using the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). Phrases are limited to 8 tokens in length (n, m ≤ 8). Given a source sentence s, our phrase-based SMT system tries to find the target sentence ˆt that is the most likely translation of s. To make search more efficient, we use the Viterbi approximation and seek the most likely combination of t and its alignment a with s, rather than just the most likely t: ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t t,a where a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK ); t˜k are target phrases such that t = t˜1 ...t˜K ; s˜k are"
D07-1103,2001.mtsummit-papers.68,0,0.083145,"Missing"
D07-1103,N04-1033,0,0.0158942,"Missing"
D07-1103,W04-3243,0,\N,Missing
D07-1103,J93-2003,0,\N,Missing
D07-1103,P02-1040,0,\N,Missing
D07-1103,W06-1607,1,\N,Missing
D07-1103,D08-1076,0,\N,Missing
D10-1044,N04-4006,0,0.0192611,"s led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007). However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus. This suggests a direct parallel to (1): ! ! αi pi (s|t), (2) α ˆ = argmax p˜(s, t) log α s,t i where p˜(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004). For the TM, this is: p(s|t) = cI (s, t) + β po (s|t) , cI (t) + β (3) where cI (s, t) is the count in the IN phrase table of pair (s, t), po (s|t)""is its probability under the OUT TM, and cI (t) = s! cI (s! , t). This is motivated by taking β po (s|t) to be the parameters of a Dirichlet prior on phrase probabilities, then maximizing posterior estimates p(s|t) given the IN corpus. Intuitively, it places more weight on OUT when less evidence from IN is available. To set β, we used the same criterion as for α, over a dev corpus: βˆ = argmax β ! s,t p˜(s, t) log cI (s, t) + β po (s|t) . cI (t) +"
D10-1044,W09-0432,0,0.316476,"ombining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus"
D10-1044,W07-0722,0,0.0718684,"ond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The fea"
D10-1044,P07-1033,0,0.119667,"Missing"
D10-1044,W08-0334,0,0.206659,"tive weights are set as follows: ! ! p˜(w, h) log αi pi (w|h), (1) α ˆ = argmax α i w,h where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and p˜(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this. It is not immediately obvious how to formulate an equivalent to equation (1) for an adapted TM, because there is no well-defined objective for learning TMs from parallel corpora. This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007). However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus. This suggests a direct parallel to (1): ! ! αi pi (s|t), (2) α ˆ = argmax p˜(s, t) log α s,t i where p˜(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004). For the TM, this is: p(s|t) = cI (s, t) + β po (s|t) , cI (t)"
D10-1044,N09-1068,0,0.0403739,"t splits each feature into domain-specific and general copies. At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances. However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t). As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Se"
D10-1044,W07-0717,1,0.846999,"reported in. Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership. Finally, we make some improvements to baseline approaches. We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set. This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU. A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only. For comparison to information-retrieval inspired baselines, eg (L¨u et al., 2007), we select sentences from OUT using language model perplexities from IN. This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results. The paper is structured as follows. Section 2 describes our baseline techniques for SMT adaptation, and section 3 describes the instance-weighting approach. Experiments are presented in section 4. Section 5 covers"
D10-1044,W09-0439,1,0.776725,"baseline approach is to concatenate data from IN and OUT. Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN. When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination. An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003). This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009). 2.2 Linear Combinations Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates. This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain. This leads to a linear combination of domain-specific probabilities, with weights in [0, 1], normalized to sum to 1. Linear weights are d"
D10-1044,2005.eamt-1.19,0,0.676063,"rom IN is available. To set β, we used the same criterion as for α, over a dev corpus: βˆ = argmax β ! s,t p˜(s, t) log cI (s, t) + β po (s|t) . cI (t) + β 2 Using non-adapted IBM models trained on all available IN and OUT data. 453 The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 2.3 Sentence Selection Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004). The matching sentence pairs are then added to the IN corpus, and the system is re-trained. Although matching is done at the sentence level, this information is subsequently discarded when all matches are pooled. To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model. The number of top-ranked pairs to retain is chosen to optimize dev-set BLEU score. 3 Instance We"
D10-1044,P07-1034,0,0.585228,"ss effective in our setting, where IN and OUT are disparate. The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity. Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT. Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Our second contribution is to apply instance 451 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451–459, c MIT, Massachusetts, USA, 9-11 October 2010. !2010 Crown in Right of Canada. weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of do"
D10-1044,W07-0733,0,0.744354,"es, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t). As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al.,"
D10-1044,N03-1017,0,0.0138788,"27 1,894 1,664 1,357 Table 1: Corpora 8 www.itl.nist.gov/iad/mig//tests/mt/2009 456 The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa. Le m´edicament de r´ef´erence de Silapo est EPREX/ERYPO, qui contient de l’´epo´etine alfa. — I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court. Je voudrais pr´eciser, a` l’adresse du commissaire Liikanen, qu’il n’est pas ais´e de recourir aux tribunaux nationaux. Figure 1: Sentence pairs from EMEA (top) and Europarl text. We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count. Feature weights were set using Och’s MERT algorithm (Och, 2003). The corpus was wordaligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model. 4.2 Results Table 2 shows results for both settings and all"
D10-1044,D07-1036,0,0.765764,"Missing"
D10-1044,D09-1074,0,0.764711,"s in Natural Language Processing, pages 451–459, c MIT, Massachusetts, USA, 9-11 October 2010. !2010 Crown in Right of Canada. weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language. For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in. Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership. Finally, we make some improvements to baseline approaches. We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set. This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU. A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only. For comparison to information-retrieval inspired bas"
D10-1044,P03-1021,0,0.0597961,"dure for generating the phrase table from which the TM distributions are derived. 2.1 Simple Baselines The natural baseline approach is to concatenate data from IN and OUT. Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN. When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination. An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003). This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009). 2.2 Linear Combinations Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates. This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain. This leads"
D10-1044,2009.mtsummit-posters.17,0,0.0173592,"Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out"
D10-1044,P07-1066,0,0.266726,"in (IN) for which a smaller amount of parallel There is a fairly large body of work on SMT adaptation. We introduce several new ideas. First, we aim to explicitly characterize examples from OUT as belonging to general language or not. Previous approaches have tried to find examples that are similar to the target domain. This is less effective in our setting, where IN and OUT are disparate. The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity. Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT. Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Our second c"
D10-1044,P07-1004,0,0.160253,"nt work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve i"
D10-1044,P05-1058,0,0.0263712,"rk, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted"
D10-1044,2007.mtsummit-papers.68,0,0.0959361,"rand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted within a logistic model to give an overall weight that i"
D10-1044,C04-1059,0,0.614506,"Missing"
D10-1044,D08-1076,0,\N,Missing
D17-1263,D16-1025,0,0.041234,"winning 4 of these and tying for first or second in the other 5, according to the official human ranking. Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016). These evaluations indicate that NMT performs better on average than previous technologies, but they do not help us understand what aspects of the translation have improved. Some groups have conducted more detailed error analyses. Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort overall, with substantial improvements in lexical, morphological and word order errors. NMT consistently out-performed PBMT, but its performance degraded faster as sentence length increased. Later, Toral and S´anchez-Cartagena (2017) conducted a similar study, examining the outputs of competition-grade systems for the 9 WMT 2016 directi"
D17-1263,N13-1003,1,0.850294,"model of Galley and Manning (2008). We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64k and 32k. Words not in the vocabulary were mapped to one of 100 mkcls classes. We trained for 60 epochs of 20k × 128 minibatches, yielding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7. We include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural s"
D17-1263,N12-1047,1,0.308919,"mkcls classes. We trained for 60 epochs of 20k × 128 minibatches, yielding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7. We include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural systems To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent units (Cho et al., 2014). We used 512-dimensio"
D17-1263,D14-1179,0,0.0082884,"Missing"
D17-1263,P14-1129,0,0.0261041,"k 4.3 Table 1: Corpus statistics. The WMT12/13 eval sets are used for dev, and the WMT14 eval set is used for test. and corpus organization, but mapping characters to lowercase. Table 1 gives corpus statistics. 4.2 Phrase-based systems To ensure a competitive PBMT baseline, we performed phrase extraction using both IBM4 and HMM alignments with a phrase-length limit of 7; after frequency pruning, the resulting phrase table contained 516M entries. For each extracted phrase pair, we collected statistics for the hierarchical reordering model of Galley and Manning (2008). We trained an NNJM model (Devlin et al., 2014) on the HMM-aligned training corpus, with input and output vocabulary sizes of 64k and 32k. Words not in the vocabulary were mapped to one of 100 mkcls classes. We trained for 60 epochs of 20k × 128 minibatches, yielding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language"
D17-1263,J94-4004,0,0.130952,"uation is robustness to sparse data. To control for this, when crafting source and reference sentences, we chose words that occurred at least 100 times in our training corpus (section 4.1).1 2487 1 With two exceptions: spilt (58 occurrences), which is The challenging aspect of the test set we are presenting stems from the fact that the source English sentences have been chosen so that their closest French equivalent will be structurally divergent from the source in some crucial way. Translational divergences have been extensively studied in the past—see for example (Vinay and Darbelnet, 1958; Dorr, 1994). We expect the level of difficulty of an MT test set to correlate well with its density in divergence phenomena, which we classify into three main types: morpho-syntactic, lexico-syntactic and purely syntactic divergences. 3.1 Morpho-syntactic divergences In some languages, word morphology (e.g. inflections) carries more grammatical information than in others. When translating a word towards the richer language, there is a need to recover additional grammatically-relevant information from the context of the target language word. Note that we only include in our set cases where the relevant in"
D17-1263,D08-1089,0,0.0139265,"Missing"
D17-1263,P07-1019,0,0.0213604,"ding a final dev-set perplexity of 6.88. Our set of log-linear features consisted of forward and backward Kneser-Ney smoothed phrase probabilities and HMM lexical probabilities (4 features); hierarchical reordering probabilities (6); the NNJM probability (1); a set of sparse features as described by Cherry (2013) (10,386); wordcount and distortion penalties (2); and 5-gram language models trained on the French half of the training corpus and the French monolingual corpus (2). Tuning was carried out using batch lattice MIRA (Cherry and Foster, 2012). Decoding used the cube-pruning algorithm of Huang and Chiang (2007), with a distortion limit of 7. We include two phrase-based systems in our comparison: PBMT-1 has data conditions that exactly match those of the NMT system, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural systems To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent units (Cho et al., 2014). We used 512-dimensional word embeddings with source and target vocabulary sizes of 90k, a"
D17-1263,W16-2316,0,0.0796396,"s received little attention thus far. 2 Related Work A number of recent papers have evaluated NMT using broad performance metrics. The WMT 2016 News Translation Task (Bojar et al., 2016) evaluated submitted systems according to both BLEU and human judgments. NMT systems were submitted to 9 of the 12 translation directions, winning 4 of these and tying for first or second in the other 5, according to the official human ranking. Since then, controlled comparisons have used BLEU to show that NMT outperforms strong PBMT systems on 30 translation directions from the United Nations Parallel Corpus (Junczys-Dowmunt et al., 2016a), and on the IWSLT English-Arabic tasks (Durrani et al., 2016). These evaluations indicate that NMT performs better on average than previous technologies, but they do not help us understand what aspects of the translation have improved. Some groups have conducted more detailed error analyses. Bentivogli et al. (2016) carried out a number of experiments on IWSLT 2015 EnglishGerman evaluation data, where they compare machine outputs to professional post-edits in order to automatically detect a number of error categories. Compared to PBMT, NMT required less postediting effort overall, with subs"
D17-1263,D13-1176,0,0.0157253,"ror analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system’s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English–French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach. 1 Figure 1: Example challenge set question. Introduction The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality. For “easy” language pairs such as English/French or English/Spanish in particular, neural (NMT) systems are much closer to human performance than previous statistical techniques (Wu et al., 2016). This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences. As NMT continues to improve, these metrics will inevitably lose their effectiveness. Another challenge posed by NMT systems i"
D17-1263,C90-2037,0,0.88258,"upted version. Using this technique, they are able to determine that a recently-proposed character-based model improves generalization on unseen words, but at the cost of introducing new grammatical errors. Our approach differs from these studies in a number of ways. First, whereas others have analyzed sentences drawn from an existing bitext, we conduct our study on sentences that are manually constructed to exhibit canonical examples of specific linguistic phenomena. We focus on phenomena that we expect to be more difficult than average, resulting in a particularly challenging MT test suite (King and Falkedal, 1990). These sentences are designed to dive deep into linguistic phenomena of interest, and to provide a much finer-grained analysis of the strengths and weaknesses of existing technologies, including NMT systems. However, this strategy also necessitates that we work on fewer sentences. We leverage the small size of our challenge set to manually evaluate whether the system’s actual output correctly handles our phenomena of interest. Manual evaluation side-steps some of the pitfalls that can come with Sennrich (2016)’s contrastive pairs, as a ranking of two contrastive sentences may not necessarily"
D17-1263,P02-1040,0,0.113514,"trengths of neural systems, but also insight into which linguistic phenomena remain out of reach. 1 Figure 1: Example challenge set question. Introduction The advent of neural techniques in machine translation (MT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) has led to profound improvements in MT quality. For “easy” language pairs such as English/French or English/Spanish in particular, neural (NMT) systems are much closer to human performance than previous statistical techniques (Wu et al., 2016). This puts pressure on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which exploit surface-matching heuristics that are relatively insensitive to subtle differences. As NMT continues to improve, these metrics will inevitably lose their effectiveness. Another challenge posed by NMT systems is their opacity: while it was usually clear which phenomena were ill-handled ∗ Work performed while at NRC. by previous statistical systems—and why—these questions are more difficult to answer for NMT. We propose a new evaluation methodology centered around a challenge set of difficult examples that are designed using expert linguistic knowledge to probe an MT system’s capa"
D17-1263,P16-1162,0,0.00842166,"em, in that it does not use the language model trained on the French monolingual corpus, while PBMT-2 uses both language models. Neural systems To build our NMT system, we used the Nematus toolkit,5 which implements a single-layer neural sequence-to-sequence architecture with attention (Bahdanau et al., 2015) and gated recurrent units (Cho et al., 2014). We used 512-dimensional word embeddings with source and target vocabulary sizes of 90k, and 1024-dimensional state vectors. The model contains 172M parameters. We preprocessed the data using a BPE model learned from source and target corpora (Sennrich et al., 2016). Sentences longer than 50 words were discarded. Training used the Adadelta algorithm (Zeiler, 2012), with a minibatch size of 100 and gradients clipped to 1.0. It ran for 5 epochs, writing a checkpoint model every 30k minibatches. Following Junczys-Dowmunt et al. (2016b), we averaged the parameters from the last 8 checkpoints. To decode, we used the AmuNMT decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. While our primary results will focus on the above PBMT and NMT systems, where we can describe replicable configurations, we have also evaluated Google’s production system,6 whic"
D17-1263,E17-1100,0,0.106584,"Missing"
D17-1263,W16-2301,0,\N,Missing
D18-1461,P18-1008,1,0.92754,"t approaches, interest in character-level processing fell off, but has recently been reignited with the work of Lee et al. (2017). They propose a specialized character-level encoder, connected to an unmodified character-level RNN decoder. They address the modeling and efficiency challenges of long character sequences using a convolutional layer, max-pooling over time, and highway layers. We agree with their conclusion that character-level translation is effective, but revisit the question 3.1 Methods Baseline Sequence-to-Sequence Model We adopt a simplified version of the LSTM architecture of Chen et al. (2018) that achieves state-ofthe-art performance on the competitive WMT14 English-French and English-German benchmarks. This incorporates bidirectional LSTM (BiLSTM) layers in the encoder, concatenating the output from forward and backward directions before feeding the next layer. Output from the top encoder layer is projected down to the decoder dimension and used in an additive attention mechanism computed over the bottom decoder layer. The decoder consists of unidirectional layers, all of which use the encoder context vectors computed from attention weights over the bottom layer. For both encoder"
D18-1461,P16-2058,0,0.12014,"Missing"
D18-1461,W17-2627,0,0.0277599,"en an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches (Johnson et al., 2017). Translating characters instead of word fragments avoids these problems, and gives the system access to all available information about source and target sequences. However, it presents significant modeling and computational challenges. Longer sequences incur linear per-layer cost and quadratic attention cost, and require information to be retained over longer temporal spans. Finer temporal granularity also creates the potential for attention jitter (Gulcehre et al., 2017). Perhaps most significantly, since the meaning of a word is not a compositional function of its characters, the system must learn to memorize many character sequences, a different task from the (mostly) compositional operations it performs at higher levels of linguistic abstraction. In this paper, we show that a standard LSTM sequence-to-sequence model works very well for characters, and given sufficient depth, consistently outperforms identical models operating over word fragments. This result suggests that a productive line of research on character-level models is to seek architectures that"
D18-1461,P18-1007,0,0.0707514,"ing (BPE; Sennrich et al., 2016). Although these are effective, they involve hyperparameters ∗ *Equal contributions that should ideally be tuned for each language pair and corpus, an expensive step that is frequently omitted. Even when properly tuned, the representation of the corpus generated by pipelined external processing is likely to be sub-optimal. For instance, it is easy to find examples of word fragmentations, such as fling → fl + ing, that are linguistically implausible. NMT systems are generally robust to such infelicities—and can be made more robust through subword regularization (Kudo, 2018)—but their effect on performance has not been carefully studied. The problem of finding optimal segmentations becomes more complex when an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches (Johnson et al., 2017). Translating characters instead of word fragments avoids these problems, and gives the system access to all available information about source and target sequences. However, it presents significant modeling and computational challenges. Longer sequences incur linear per-layer cost and quadratic attention cost, and requir"
D18-1461,Q17-1026,0,0.353472,"of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295–4305 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tationally cheaper. One approach to this problem is temporal compression: reducing the number of state vectors required to represent input or output sequences. We evaluate various approaches for performing temporal compression, both according to a fixed schedule; and, more ambitiously, learning compression decisions with a Hierarchical Multiscale architecture (Chung et al., 2017). Following recent work by Lee et al. (2017), we focus on compressing the encoder. Our contributions are as follows: • The first large-scale empirical investigation of the translation quality of standard LSTM sequence-to-sequence architectures operating at the character level, demonstrating improvements in translation quality over word fragments, and quantifying the effect of corpus size and model capacity. • A comparison of techniques to compress character sequences, assessing their ability to trade translation quality for increased speed. • A first attempt to learn how to compress the source sequence during NMT training by using the H"
D18-1461,P16-1100,0,0.0391557,"m their pooling solution for reducing sequence length, along with similar ideas from the speech community (Chan et al., 2016), when devising fixed-schedule reduction strategies in Section 3.3. One of our primary contributions is an extensive invesigation of the efficacy of a typical LSTM-based NMT system when operating at the character-level. The vast majority of existing studies compare a specialized character-level architecture to a distinct word-level one. To the best of our knowledge, only a small number of papers have explored running NMT unmodified on character sequences; these include: Luong and Manning (2016) on WMT’15 English-Czech, Wu et al. (2016) on WMT’14 English-German, and Bradbury et al. (2016) on IWSLT German-English. All report scores that either trail behind or reach parity with word-level models. Only Wu et al. (2016) compare to word fragment models, which they show to outperform characters by a sizeable margin. We revisit the question of character- versus fragment-level NMT here, and reach quite different conclusions. 3 Related Work Early work on modeling characters in NMT focused on solving the out-of-vocabulary and softmax bottleneck problems associated with wordlevel models (Ling e"
D18-1461,W18-6319,0,0.0292093,"Missing"
D18-1461,E17-2060,0,0.0437512,"d as being identical or of roughly the same quality. The remaining 53 exhibited a large variety of differences. Table 4 summarizes the errors that were most easily characterized. BPE and character sys6 Recall that we use batches containing 16,384 tokens— corresponding to a fixed memory budget—for both character and BPE models. Thus character models are slowed not only by having longer sentences, but also by parallelizing across fewer sentences in each batch. 7 The annotating author does not speak German. 8 Our annotator also looked specifically for agreement and negation errors, as studied by Sennrich (2017) for English-toGerman character-level NMT. However, neither system exhibited these error types with sufficient frequency to draw meaningful conclusions. 4300 Figure 1: Test BLEU for character and BPE translation as architectures scale from 1 BiLSTM encoder layer and 2 LSTM decoder layers (1×2+2) to our standard 6×2+8. The y-axis spans 6 BLEU points for each language pair. Error Type Lexical Choice Compounds Proper Names Morphological Other lexical Dropped Content Figure 2: BLEU versus training corpus size in millions of sentence pairs, for the EnFr language-pair. Figure 3: Training time per se"
D18-1461,P16-1162,0,0.805951,"ation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins. 1 Introduction Neural Machine Translation (NMT) has largely replaced the complex pipeline of Phrase-Based MT with a single model that is trained end-to-end. However, NMT systems still typically rely on preand post-processing operations such as tokenization and word fragmentation through byte-pair encoding (BPE; Sennrich et al., 2016). Although these are effective, they involve hyperparameters ∗ *Equal contributions that should ideally be tuned for each language pair and corpus, an expensive step that is frequently omitted. Even when properly tuned, the representation of the corpus generated by pipelined external processing is likely to be sub-optimal. For instance, it is easy to find examples of word fragmentations, such as fling → fl + ing, that are linguistically implausible. NMT systems are generally robust to such infelicities—and can be made more robust through subword regularization (Kudo, 2018)—but their effect on"
D18-1461,P16-1008,0,0.0579145,"Missing"
foster-etal-2002-text,J99-4005,0,\N,Missing
foster-etal-2002-text,J93-2003,0,\N,Missing
foster-etal-2002-text,C00-2123,0,\N,Missing
foster-etal-2002-text,P98-2158,0,\N,Missing
foster-etal-2002-text,C98-2153,0,\N,Missing
foster-etal-2002-text,W00-0707,1,\N,Missing
foster-etal-2002-text,W02-1020,1,\N,Missing
foster-etal-2002-text,P00-1006,1,\N,Missing
H05-2001,1997.mtsummit-papers.18,0,0.145618,"Missing"
J14-2011,W06-1615,0,0.0805197,"Missing"
J14-2011,J93-2003,0,0.0705832,"Missing"
J14-2011,P07-1033,0,0.208696,"Missing"
J14-2011,P11-2071,0,0.0790595,"Missing"
J14-2011,N09-1068,0,0.0262553,"al one in which both labeled and unlabeled training data are available for a collection of domains that may or may not include the test domain. (Incidentally, the 20-Newsgroups data used in the book for textclassification examples falls more naturally into this heterogeneous case than into the binary train/test split to which it is cast.) This section would have benefitted from an attempt to situate the approaches considered here within the more general setting, which is barely acknowledged throughout the book. Pointers to representative relevant work, both in NLP (Daum´e III 2007; Finkel and Manning 2009) and machine learning (BenDavid et al. 2010; Dredze, Kulesza, and Crammer 2010), would also have been an asset. Chapter 2 is a lengthy chapter that lays out basic techniques in a bid to make the book self-contained. This is always a tricky proposition, as it must tread a fine line between boring the expert and baffling the beginner. The author strikes a good balance here by emphasizing practical advice over theoretical completeness, and providing experimental results to underscore various points. Although a few passages would probably cause beginners to stumble, for instance one that invokes t"
J14-2011,P08-2015,0,0.0270091,"of the test domain often comes in the form of an unlabeled sample, and hence semi-supervised techniques constitute an important class of adaptation strategies. Søgaard’s book has at its core the intersection of these two important topics, although it also covers semi-supervised techniques without considering data bias, and techniques for handling bias that are not semi-supervised. Before proceeding, I should declare a bias of another sort, which is that of a machine translation (MT) researcher toward a book that cites only two MT papers related to semisupervised learning or domain adaptation (Habash 2008; Daum´e III and Jagarlamudi 2011), neither of which is highly representative of the fairly substantial MT work on these topics. I have done my best to apply my neural adaptation faculty to the material in this book; non-MT readers might find it helpful to do the same with this review. The book begins with two chapters of introductory material on machine learning and NLP that occupy half of its 80 pages (exclusive of bibliography). The main topics of semi-supervised learning and adaptation are then presented in approximately equalsized portions, with adaptation split into two chapters covering"
J14-2011,P07-1034,0,0.0446545,"ased algorithm where neighbors vote on each node’s label, with votes weighted by distance. A similar voting takes place in editing and condensation, which are methods for identifying a subset of prototypical data points (similar to support vectors in support vector machines) in order to speed up nearest-neighbor search. Unlabeled data can be used to improve this process by essentially providing greater resolution. Chapter 4 is the first to grapple explicitly with domain mismatch, and begins by making a standard distinction between conditional and marginal distributions for inputs and outputs (Jiang and Zhai 2007). Here we are clearly limited to considering biased inputs, because there is only an unlabeled sample from the test domain. One strategy for exploiting this is to apply the semi-supervised methods from the previous chapter, which will work as-is if the mismatch between training and test domains is not too great. Otherwise, we can downweight instances in the labeled training set whose inputs are not close to those in the test-domain data; techniques for measuring distance include LMs and KL divergence. A similar approach can be applied to features, by comparing the training-data values of a fea"
J14-2011,C12-2114,0,0.0139775,"be robust to domain shifts in the absence of any prior information about the test domain. A common effect is the out-of-vocabulary (OOV) problem, in which features do not appear in a new test domain. If these have high weights, test-domain performance can degrade badly, to the point where it would have been better to have left them out of training in the first place, allowing other features to take up the slack. (Note that for some versions of the OOV problem—such as in MT—things aren’t this easy.) An interesting technique for countering feature OOVs, recently introduced to NLP by the author (Søgaard and Johannsen 2012), is adversarial learning, in which random subsets of features are removed during training. The chapter ends with a discussion of more traditional ensemble-based methods (voting, product of experts, stacking, meta-learning) that combine predictions from a set of diverse base classifiers in order to decrease variance. The final chapter gives a short treatment of the problem of how to predict the performance of systems on new domains. The main, and most interesting, suggestion is metaanalysis, a technique widely used in fields such as medicine and psychology. The idea is to extrapolate from many"
langlais-etal-2000-evaluation,C90-2045,0,\N,Missing
langlais-etal-2000-evaluation,J93-2003,0,\N,Missing
langlais-etal-2000-evaluation,C90-3008,0,\N,Missing
langlais-etal-2000-evaluation,P98-2158,0,\N,Missing
langlais-etal-2000-evaluation,C98-2153,0,\N,Missing
langlais-etal-2000-evaluation,A00-1019,1,\N,Missing
langlais-etal-2000-evaluation,P97-1037,0,\N,Missing
langlais-etal-2000-evaluation,C86-1077,0,\N,Missing
langlais-etal-2000-evaluation,P97-1047,0,\N,Missing
N06-1004,P05-1066,0,0.485406,"nal Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they assign a probability to each possible phrase reordering. These “segment choice” models (SCMs) can be trained on “segmen"
N06-1004,koen-2004-pharaoh,0,0.574965,"al., 2003; Och and Ney, 2004). Distortion in phrase-based MT occurs when the order of phrases in the source-language sentence changes during translation, so the order of corresponding phrases in the target-language translation is different. Some MT systems allow arbiOur model assumes that the source sentence is completely segmented prior to distortion. This simplifying assumption requires generation of hypotheses about the segmentation of the complete source sentence during decoding. The model also assumes that each translation hypothesis grows in a predetermined order. E.g., Koehn’s decoder (Koehn 2004) builds each new hypothesis by adding phrases to it left-to-right (order is deterministic for the target hypothesis). Our model doesn’t require this order of operation – it would support right-to-left or inwards-outwards hypothesis construction – but it does require a predictable order. One can keep track of how segments in the source sentence have been rearranged during decoding for a given hypothesis, using what we call a “distorted source-language hypothesis” (DSH). A similar concept appears in (Collins et al., 2005) (this paper’s preoccupations strongly resemble 25 Proceedings of the Human"
N06-1004,N03-1017,0,0.00873664,"bles consists of all parallel text available for the NIST MT05 Chinese-English evaluation, except the Xinhua corpora and part 3 of LDC's “MultipleTranslation Chinese Corpus” (MTCCp3). The English language model was trained on the same corpora, plus 250M words from Gigaword. The DTbased SCM was trained and tuned on a subset of this same training corpus (above). The dev corpus for optimizing component weights is MTCCp3. The experimental results below were obtained by testing on the evaluation set for MTeval NIST04. Phrase tables were learned from the training corpus using the “diag-and” method (Koehn et al., 2003), and using IBM model 2 to produce initial word alignments (these authors found this worked as well as IBM4). Phrase probabilities were based on unsmoothed relative frequencies. The model used by the decoder was a log-linear combination of a phrase translation model (only in the P(source|target) direction), trigram language model, word penalty (lexical weighting), an optional segmentation model (in the form of a phrase penalty) and distortion model. Weights on the components were assigned using the (Och, 2003) method for max-BLEU training on the development set. The decoder uses a dynamicprogr"
N06-1004,P03-1021,0,0.0267581,"Missing"
N06-1004,P02-1040,0,0.0721342,"se it is the leftmost RS: the “leftmost” predictor. Or, the last phrase in the DSH will be followed by the phrase that originally followed it, [8 9]: the “following” predictor. Or, perhaps positions in the source and target should be close, so since the next DSH position to be filled is 4, phrase [4] should be favoured: the “parallel” predictor. original: [0 1] [2 3] [4] [5] [6] [7] [8 9] DSH: [0 1] [5] [7], RS: [2 3], [4], [6], [8 9] Defining Disperp The ultimate reason for choosing one SCM over another will be the performance of an MT system containing it, as measured by a metric like BLEU (Papineni et al., 2002). However, training and 26 Figure 2. Segment choice prediction example Model B will be based on the “leftmost” predictor, giving the leftmost segment in the RS twice the probability of the other segments, and giving the others uniform probabilities. Model C will be based on the “following” predictor, doubling the probability for the segment in the RS whose first word was the closest to the last word in the DSH, and otherwise assigning uniform probabilities. Finally, Model D combines “leftmost” and “following”: where the leftmost and following segments are different, both are assigned double th"
N06-1004,P05-1069,0,0.0636859,"d Kuhn, Denis Yuen, Michel Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson Institute for Information Technology, National Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that"
N06-1004,N04-4026,0,0.426085,"Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson Institute for Information Technology, National Research Council of Canada Gatineau, Québec, CANADA Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com trary reordering of phrases, but impose a distortion penalty proportional to the difference between the new and the original phrase order (Koehn, 2004). Some interesting recent research focuses on reordering within a narrow window of phrases (Kumar and Byrne, 2005; Tillmann and Zhang, 2005; Tillmann, 2004). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. A recent paper (Collins et al., 2005) shows that major gains can be obtained by constructing a parse tree for the source sentence and then applying handcrafted reordering rules to rewrite the source in target-language-like word order prior to MT. Abstract This paper presents a new approach to distortion (phrase reordering) in phrasebased machine translation (MT). Distortion is modeled as a sequence of choices during translation. The approach yields trainable, probabilistic distortion models that are global: they"
N06-1004,J93-2003,0,\N,Missing
N06-1004,C96-2141,0,\N,Missing
N06-1004,H05-1021,0,\N,Missing
N06-1004,J04-4002,0,\N,Missing
N06-1004,W06-3118,1,\N,Missing
N12-1047,W10-1756,0,0.0120603,"ed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter λ, and do not require special purpose code for hope and fear lattice decoding. Batch 4 SVM training with interpolated BLEU outperformed add-1 BLEU in preliminary testing. A comparison of different BLEU approximations under different tuning objectives would be an interesting path for future work. 5 MR approaches that use lattices (Li and Eisner, 2009; Pauls et al., 2009; Rosti et al., 2011) or the complete search space (Arun et al., 2010) exist, but are not tested here. k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs French-English (Fr-En), English-French (En-Fr) and Chinese-English (Zh En) , across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments usin"
N12-1047,D08-1024,0,0.835224,"U, which we re-encode into a cost ∆i (e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and ∆, and in how they employ their loss functions to tune their weights. 1 This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: h  i `i (w) ~ = max ∆i (e) + w ~ · ~hi (e) − ~hi (e∗i ) e∈Ei (2) where e∗i is an oracle derivation, and cost is defined as ∆i (e) = BLEUi (e∗i ) − BLEUi (e), so that ∆i (e∗i ) = 0. The loss `i (w) ~ is 0 only if w ~ separates each e ∈ Ei from e∗i by a margin proportional to their BLEU differentials. MIRA is an instance of online learning, repeating the following steps: visit an example i, decode according to w, ~ and update w ~ to reduce `i (w). ~ Each update makes the smallest change to w ~ (subject to a s"
N12-1047,N09-1025,0,0.167068,"Missing"
N12-1047,P11-2031,0,0.556511,"et al.’s (2008) hope-fear MIRA that use k-best or lattice-approximated search spaces, producing a Batch MIRA that outperforms a popular mechanism for parallelizing online learners. We also investigate the direct optimization of hinge loss on k-best lists, through the use of a Structured SVM (Tsochantaridis et al., 2004). We review and organize the existing tuning literature, providing sentence-level loss functions for minimum risk, online and pairwise training. Finally, since randomization plays a different role in each tuner, we also suggest a new method for testing an optimizer’s stability (Clark et al., 2011), which sub-samples the tuning set instead of varying a random seed. 2 Background We begin by establishing some notation. We view our training set as a list of triples [f, R, E]ni=1 , where f is a source-language sentence, R is a set of targetlanguage reference sentences, and E is the set of all reachable hypotheses; that is, each e ∈ Ei is a target-language derivation that can be decoded from fi . The function ~hi (e) describes e’s relationship to its source fi using features that decompose into the decoder. A linear model w ~ scores derivations according to their features, meaning that the d"
N12-1047,N12-1023,0,0.261828,"t term provides regularization, weighted by λ. Throughout this paper, (4) is optimized with respect to a fixed approximation of the decoder’s true search space, represented as a collection of k-best lists. The various methods differ in their definition of loss and in how they optimize their objective. Without the complications added by hope decoding and a time-dependent cost function, unmodified MIRA can be shown to be carrying out dual coordinate descent for an SVM training objective (Martins et al., 2010). However, exactly what objective hopefear MIRA is optimizing remains an open question. Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA. 2.3 Pairwise Ranking Optimization Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. That is, PRO employs a growing approximation of Ei by aggregating the k-best hypotheses from a series of increasingly refined models. This architecture is desirable, as most groups have infrastructure to k-best decode their tuning sets in parallel. For a given approximate E˜i , PRO creates a sample Si of (eg , eb )"
N12-1047,D11-1125,0,0.625231,"ver, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation. Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s established outer loop. This i"
N12-1047,N03-1017,0,0.222749,"09) summarized in table 1.6 The dev and test sets were chosen randomly from among the most recent 5 days of Hansard transcripts. The system for Zh-En was trained on data from the NIST 2009 Chinese MT evaluation, summarized in table 2. The dev set was taken from the NIST 05 evaluation set, augmented with some material reserved from other NIST corpora. The NIST 04, 06, and 08 evaluation sets were used for testing. 4.2 SMT Features For all language pairs, phrases were extracted with a length limit of 7 from separate word alignments performed by IBM2 and HMM models and symmetrized using diag-and (Koehn et al., 2003). Conditional phrase probabilities in both directions were 6 This corpus will be distributed on request. template tgt unal count bin word pair length bin total max 50 11 6724 63 6848 fren 50 11 1298 63 1422 enfr 50 11 1291 63 1415 zhen 31 11 1664 63 1769 Table 3: Sparse feature templates used in Big. estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). Language models were estimated with Kneser-Ney smoothing using SRILM. Six-feature lexicalized distortion models were estimated and applied as in Moses. For each language pair, we defined roughly equivalent sy"
N12-1047,P07-2045,0,0.0854368,"the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list. 4 Experimental Design We evaluated the six tuning strategies described in this paper, along with two MERT baselines, on three language pairs French-English (Fr-En), English-French (En-Fr) and Chinese-English (Zh En) , across three different feature-set sizes. Each setting was run five times over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007). All tuning methods that use an approximate E˜ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, λ was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay γ = 0.999 and C = 0"
N12-1047,D09-1005,0,0.0136064,"arge uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gra"
N12-1047,P06-1096,0,0.181544,"Missing"
N12-1047,C04-1072,0,0.227698,"resist standard mechanisms of regularization that aim to keep ||w|| ~ small. The problems with MERT can be addressed through the use of surrogate loss functions. In this paper, we focus on linear losses that decompose over training examples. Using Ri and Ei , each loss `i (w) ~ th indicates how poorly w ~ performs on the i training example. This requires a sentence-level approximation of BLEU, which we re-encode into a cost ∆i (e) on derivations, where a high cost indicates that e receives a low BLEU score. Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). The learners differ in their definition of ` and ∆, and in how they employ their loss functions to tune their weights. 1 This is true of any evaluation metric that considers only the ranking of hypotheses and not their model scores; ie, it is true of all common MT metrics. 2.1 Margin Infused Relaxed Algorithm First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al. (2008; 2009), the Margin Infused Relaxed Algorithm (MIRA) employs a structured hinge loss: h  i `i (w) ~ = max ∆i (e) + w ~ · ~hi (e) − ~hi (e∗i ) e∈Ei (2) where e∗i is an oracle derivation, and cost is defi"
N12-1047,D08-1076,0,0.0782686,"over randomized variants to improve reliability. To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007). All tuning methods that use an approximate E˜ perform 15 iterations of the outer loop and return the weights that achieve the best development BLEU score. When present, λ was coarsely tuned (trying 3 values differing by magnitudes of 10) in our largefeature Chinese-English setting. • kb-mert : k-best MERT with 20 random restarts. All k-best methods use k = 100. • lb-mert : Lattice MERT (Machery et al., 2008) using unpruned lattices and aggregating only those paths on the line search’s upper envelope. • mira : Online MIRA (§2.1). All MIRA variants use a pseudo-corpus decay γ = 0.999 and C = 0.01. Online parallelization follows McDonald et al. (2010), using 8 shards. We tested 20, 15, 10, 8 and 5 shards during development. • lb-mira : Batch Lattice MIRA (§3.1). • kb-mira : Batch k-best MIRA (§3.1). • pro : PRO (§2.3) follows Hopkins and May (2011), footnote 6, implementing a logistic sigmoid sampler with both the initial and maximum sample size set to 100 pairs. For classification, we employ an in-"
N12-1047,N10-1069,0,0.114766,"gin tuners that explore these trade-offs. 3.1 Batch MIRA Online training makes it possible to learn with the decoder in the loop, forgoing the need to approximate the search space, but it is not necessarily convenient to do so. Online algorithms are notoriously difficult to parallelize, as they assume each example is visited in sequence. Parallelization is important for efficient SMT tuning, as decoding is still relatively expensive. The parallel online updates suggested by Chiang et al. (2008) involve substantial inter-process communication, which may not be easily supported by all clusters. McDonald et al. (2010) suggest a simpler distributed strategy that is amenable to map-reduce-like frameworks, which interleaves online training on shards with weight averaging across shards. This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation. However, online training using the decoder may not be necessary for good performance. The success of MERT, PRO and MR indicates that their shared search approximation is actually quite reasonable. Therefore, we propose Batch MIRA, which sits exactly where MERT sits in the standard tuning architecture, greatly si"
N12-1047,P02-1038,0,0.548994,"evel loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 1 Introduction The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang"
N12-1047,P03-1021,0,0.826491,"s. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 1 Introduction The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al.,"
N12-1047,P02-1040,0,0.114886,"notation. We view our training set as a list of triples [f, R, E]ni=1 , where f is a source-language sentence, R is a set of targetlanguage reference sentences, and E is the set of all reachable hypotheses; that is, each e ∈ Ei is a target-language derivation that can be decoded from fi . The function ~hi (e) describes e’s relationship to its source fi using features that decompose into the decoder. A linear model w ~ scores derivations according to their features, meaning that the decoder solves: ei (w) ~ = arg max w ~ · ~hi (e) (1) e∈Ei Assuming we wish to optimize our decoder’s BLEU score (Papineni et al., 2002), the natural objective of learning would be to find a w ~ such that BLEU([e(w), ~ R]n1 ) is maximal. In most machine learning papers, this would be the point where we would say, “unfortunately, this objective is unfeasible.” But in SMT, we have been happily optimizing exactly this objective for years using MERT. However, it is now acknowledged that the MERT approach is not feasible for more than 30 or so features. This is due to two main factors: 1. MERT’s parameter search slows and becomes less effective as the number of features rises, stopping it from finding good training scores. 2. BLEU"
N12-1047,D09-1147,0,0.137005,"ze with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architectur"
N12-1047,W11-2119,0,0.0952467,"f classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010). Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture. The expectations n"
N12-1047,N04-1023,0,0.0579498,"T (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance the approximation. Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s establ"
N12-1047,P06-2101,0,0.320437,"a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002). The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approx"
N12-1047,D07-1080,0,0.249835,"o optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003). However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces. These methods fall into a number of broad categories. Minimum risk approaches (Och, 2003; Smith and Eisner, 2006) have been quietly capable of handling many features for some time, but have yet to see widespread adoption. Online methods (Liang et al., 2006; Watanabe et al., 2007), are recognized to be effective, but require substantial implementation efforts due to difficulties with parallelization. Pairwise ranking (Shen et al., 2004; Hopkins and May, 2011) recasts tuning as classification, and can be very easy to implement, as it fits nicely into the established MERT infrastructure. The MERT algorithm optimizes linear weights relative to a collection of k-best lists or lattices, which provide an approximation to the true search space. This optimization is wrapped in an outer loop that iterates between optimizing weights and re-decoding with those weights to enhance"
N12-1047,P10-1049,0,0.017593,"mprises 4 TM features, one LM, and length and distortion features. For the Chinese system, the LM is a 5-gram trained on the NIST09 Gigaword corpus; for English/French, it is a 4-gram trained on the target half of the parallel Hansard. The Medium set is a more competitive 18-feature system. It adds 4 TM features, one LM, and 6 lexicalized distortion features. For Zh-En, Small’s TM (trained on both train1 and train2 in table 2) is replaced by 2 separate TMs from these sub-corpora; for En/Fr, the extra TM (4 features) comes from a forced-decoding alignment of the training corpus, as proposed by Wuebker et al. (2010). For Zh-En, the extra LM is a 4-gram trained on the target half of the parallel corpus; for En/Fr, it is a 4-gram trained on 5m sentences of similar parliamentary data. The Big set adds sparse Boolean features to Medium, for a maximum of 6,848 features. We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011): tgt unal picks out each of the 50 most frequent target words to appear unaligned in the phrase table; count bin uniquely bins joint phrase pair counts with upper bounds 1,2,4,8,16,32,64,128,1k,10k,∞; word pair fires when each of the 80 mo"
N12-1047,N04-1033,0,0.0275206,"ts were used for testing. 4.2 SMT Features For all language pairs, phrases were extracted with a length limit of 7 from separate word alignments performed by IBM2 and HMM models and symmetrized using diag-and (Koehn et al., 2003). Conditional phrase probabilities in both directions were 6 This corpus will be distributed on request. template tgt unal count bin word pair length bin total max 50 11 6724 63 6848 fren 50 11 1298 63 1422 enfr 50 11 1291 63 1415 zhen 31 11 1664 63 1769 Table 3: Sparse feature templates used in Big. estimated from relative frequencies, and from lexical probabilities (Zens and Ney, 2004). Language models were estimated with Kneser-Ney smoothing using SRILM. Six-feature lexicalized distortion models were estimated and applied as in Moses. For each language pair, we defined roughly equivalent systems (exactly equivalent for En-Fr and FrEn, which are mirror images) for each of three nested feature sets: Small, Medium, and Big. The Small set defines a minimal 7-feature system intended to be within easy reach of all tuning strategies. It comprises 4 TM features, one LM, and length and distortion features. For the Chinese system, the LM is a 5-gram trained on the NIST09 Gigaword co"
N12-1047,D07-1055,0,0.0451808,"which performs a large uniform sample and then selects a subset of pairs with large BLEU differentials. The PRO loss uses a sum over pairs in place of MIRA’s max, which allows PRO to bypass oracle selection, and to optimize with off-the-shelf classifiers. This sum is potentially a weakness, as PRO receives credit for each correctly ordered pair in its sample, and these pairs are not equally relevant to the final BLEU score. 2.4 Minimum Risk Training Minimum risk training (MR) interprets w ~ as a probabilistic model, and optimizes expected BLEU. We focus on expected sentence costs (Och, 2003; Zens et al., 2007; Li and Eisner, 2009), as this risk is simple to optimize and fits nicely into our mathematical framework. Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011). We again assume a MERT-like tuning architecture. Let ∆i (e) = −BLEUi (e) and let h i P ~hi (e))∆i (e) exp( w ~ · ˜ e∈Ei `i (w) ~ = EPw~ [∆i (e)] = P ~ · ~hi (e0 )) e0 ∈E˜i exp(w This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores. This smooth, non-convex objective can be solved to a l"
N13-1114,D11-1033,0,0.194028,"nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set w"
N13-1114,W09-0432,0,0.0574768,"approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the senten"
N13-1114,P08-2040,1,0.901088,"7), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on"
N13-1114,2011.mtsummit-papers.30,1,0.81693,"Missing"
N13-1114,N12-1047,1,0.88809,"Missing"
N13-1114,W12-3125,0,0.0337185,"Missing"
N13-1114,W07-0717,1,0.955665,"adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield significant performance improvements. 1 In offline domain adaptation, the system is provided with a sample of translated sentences from the test domain prior to deployment. In a popular variant of offline adaptation, linear mixture model adaptation, each training corpus is used to generate a separate model component that forms part of a linear combination, and the sample is used to assign a weight to each component (Foster and Kuhn, 2007). If the sample resembles some of the corpora more than others, those corpora will receive higher weights in the combination. Introduction A phrase-based statistical machine translation (SMT) system typically has three main components: a translation model (TM) that contains information about how to translate word sequences (phrases) from the source language to the target language, a language model (LM) that contains information Previous research on domain adaptation for SMT has focused on the TM and the LM. Such research is easily motivated: translations across domains are unreliable. For exam"
N13-1114,D10-1044,1,0.922726,"Missing"
N13-1114,D08-1089,0,0.298034,"ically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase"
N13-1114,C10-1056,0,0.020848,"al sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of"
N13-1114,W07-0733,0,0.0860924,"model (TM) and language model (LM) adaptation. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and phrase sense disambiguation. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data"
N13-1114,P07-2045,0,0.0214996,"Missing"
N13-1114,koen-2004-pharaoh,0,0.452653,"ve not done so. Finally, the paper analyzes reordering to see why RM adaptation works. There seem to be two factors at work. First, the reordering behaviour of words and phrases often differs dramatically from one bilingual corpus to another. Second, there are corpora (for instance, comparable corpora and bilingual lexicons) which may contain very valuable information for the TM, but which are poor sources of RM information; RM adaptation downweights information from these corpora significantly, and thus improves the overall quality of the RM. 2 Reordering Model In early SMT systems, such as (Koehn, 2004), changes in word order when a sentence is translated were modeled by means of a penalty that is in939 curred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a"
N13-1114,D07-1036,0,0.0862795,"Missing"
N13-1114,D09-1074,0,0.206693,"omain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system"
N13-1114,P10-2041,0,0.135061,"veral different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far fro"
N13-1114,P02-1040,0,0.0876064,"2.1* 33.0** 32.8** Arabic 46.8 47.0 47.5** 48.2** Table 5: Comparison of LM, TM, and RM adaptation. Table 3: Results for variants of RM adaptation. system LM+TM adaptation +RMA+dev-smoothing+DF Chinese 33.2 33.5 Arabic 47.7 48.4** Table 4: RM adaptation improves over a baseline containing adapted LMs and TMs. tem was tuned with batch lattice MIRA (Cherry and Foster, 2012). 4.3 Results For our main baseline, we simply concatenate all training data. We also tried augmenting this with separate log-linear features corresponding to subcorpus-specific RMs. Our metric is case-insensitvie IBM BLEU-4 (Papineni et al., 2002); we report BLEU scores averaged across both test sets. Following (Koehn, 2004), we use the bootstrap-resampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. Table 3 shows that reordering model adaptation helps in both data settings. Adding either documentfrequency weighting (equation 4) or dev-set smoothing makes the improvement significant in both settings. Using both techniques together yields highly significant improvements. Our second experiment measures the improvement from RM adaptat"
N13-1114,2011.mtsummit-papers.2,0,0.0248563,"hen used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on"
N13-1114,2008.iwslt-papers.6,0,0.0382513,"hree possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lexicalized: the estimated probabilities of M, S and D depend on the sourcelanguage and target-language words in both the previous phrase pair and the newly chosen one. Galley and Manning (2008) proposed a “hierarchical” lexicalized RM in which the orientation (M, S, or D) is determined not by individual phrase pairs, but by blocks. A block is the largest contiguous sequence of phrase pairs that satisfies the phrase pair consistency requirement of having no external links. Thus, classification of the orientation of a newly chosen phrase as M, S, or D is carried out as if the decoder always chose the longest possible source phrase in the past, and will choose the longest possible source phrase in the future. The RM used in this paper is hierarchical and lexicalized. For a given phrase"
N13-1114,2012.eamt-1.43,0,0.0613514,"ining data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For instance, a sentence from a corpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this corpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 4 proposed phrase sense disambiguation (PSD) for translation model adaptation. In this approach, the context of a phrase helps the system to find the appropriate translation. All of the above work focuses on either TM or LM d"
N13-1114,P05-1069,0,0.0313324,"ntence is translated were modeled by means of a penalty that is in939 curred when the decoder chooses, as the next source phrase to be translated, a phrase that does not immediately follow the previously translated source sentence. Thus, the system penalizes deviations from monotone order, with the magnitude of the penalty being proportional to distance in the source sentence between the end of the previously translated source phrase and the start of the newly chosen source phrase. Many SMT systems, including our own, still use this distance-based penalty as a feature. However, starting with (Tillmann and Zhang, 2005; Koehn et al., 2005), a more sophisticated type of reordering model has often been adopted as well, and has yielded consistent performance gains. This type of RM typically identifies three possible orientations for a newly chosen source phrase: monotone (M), swap (S), and discontinuous (D). The M orientation occurs when the newly chosen phrase is immediately to the right of the previously translated phrase in the source sentence, the S orientation occurs when the new phrase is immediately to the left of the previous phrase, and the D orientation covers all other cases.1 This type of RM is lex"
N13-1114,P07-1004,0,0.0230579,"(Foster and Kuhn, 2007), which concluded that the best approach was to combine sub-models of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log-linearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the in-domain “dev” data, then add them to the training data. The selection criteria are typically related to the TM, though the newly found data will be used for training not only the TM but also the LM and RM. 945 Data weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) use a rich featu"
N13-1114,C04-1059,0,0.124106,"Missing"
N19-1208,W17-3205,1,0.71787,"gy from the reference. This result opens the door to learning more sophisticated curricula that exploit multiple data at2054 Proceedings of NAACL-HLT 2019, pages 2054–2061 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Figure 1: The agent’s interface with the NMT system. Figure 2: Linearly-decaying -greedy exploration. tributes and work with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimizat"
N19-1208,P18-1008,1,0.893687,"Missing"
N19-1208,P13-2119,0,0.0479278,"earn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula. 1 Introduction Machine Translation training data is typically heterogeneous: it may vary in characteristics such as domain, translation quality, and degree of difficulty. Many approaches have been proposed to cope with heterogeneity, such as filtering (Duh et al., 2013) or down-weighting (Wang et al., 2017) examples that are likely to be noisy or out of domain. A powerful technique is to control the curriculum—the order in which examples are presented to the system—as is done in fine-tuning (Freitag and Al-Onaizan, 2016), where training occurs first on general data, and then on more valuable in-domain data. Curriculum based approaches generalize data filtering and weighting1 by allowing examples to be visited multiple times 1 Assuming integer weights. or not at all; and they additionally potentially enable steering the training trajectory toward a better glo"
N19-1208,W18-6300,0,0.195937,"Missing"
N19-1208,D17-1147,0,0.171788,"Missing"
N19-1208,kocmi-bojar-2017-curriculum,0,0.0667793,"caying -greedy exploration. tributes and work with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a"
N19-1208,N18-1113,0,0.154268,"matter of extensive trial and error. Automating this process with meta-learning is thus an attractive proposition. However, it comes with many potential pitfalls such as failing to match a human-designed curriculum, or significantly increasing training time. In this paper we present an initial study on meta-learning an NMT curriculum. Starting from scratch, we attempt to match the performance of a successful non-trivial reference curriculum proposed by Wang et al. (2018), in which training gradually focuses on increasingly cleaner data, as measured by an external scoring function. Inspired by Wu et al. (2018), we use a reinforcement-learning (RL) approach involving a learned agent whose task is to choose a corpus bin, representing a given noise level, at each NMT training step. A challenging aspect of this task is that choosing only the cleanest bin is sub-optimal; the reference curriculum uses all the data in the early stages of training, and only gradually anneals toward the cleanest. Furthermore, we impose the condition that the agent must learn its curriculum in the course of a single NMT training run. We demonstrate that our RL agent can learn a curriculum that works as well as the reference,"
N19-1208,K18-1033,0,0.0304824,"re similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a co-trained classifier (Wu et al., 2018). Finally, Liu et al. (2018) apply imitation learning to actively select monolingual training sentences for labeling in NMT, and show that the learned strategy can be transferred to a related language pair. 3 Methods The attribute we choose to learn a curriculum over is noise. To determine a per-sentence noise score, we use the contrastive data selection (CDS) method defined in Wang et al. (2018). Given the parameters θn of an NMT model trained on a noisy corpus, and parameters θc of the same model finetuned on a very small trusted corpus, the score s(e, f ) = log pθc (f |e) − log pθn (f |e) (1) Wang et al. (2018) show t"
N19-1208,N19-1189,1,0.732527,"Missing"
N19-1208,N19-1119,0,0.0585249,"ork with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a co-trained classifier (Wu et al., 2018). Fin"
N19-1208,E17-2045,0,0.0897074,"xploit multiple data at2054 Proceedings of NAACL-HLT 2019, pages 2054–2061 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Figure 1: The agent’s interface with the NMT system. Figure 2: Linearly-decaying -greedy exploration. tributes and work with arbitrary corpora. 2 Related Work s(e, f ) for a translation pair e, f is defined as: Among the very extensive work on handling heterogeneous data in NMT, the closest to ours are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a"
N19-1208,P16-1013,0,0.0955844,"s are techniques that re-weight (Chen et al., 2017) or re-order examples to deal with domain mismatch (van der Wees et al., 2017; Sajjad et al., 2017) or noise (Wang et al., 2018). The idea of a curriculum was popularized by Bengio et al. (2009), who viewed it as a way to improve convergence by presenting heuristicallyidentified easy examples first. Several recent papers (Kocmi and Bojar, 2017; Zhang et al., 2019; Platanios et al., 2019) explore similar ideas for NMT, and verify that this strategy can reduce training time and improve quality. Work on meta-learning a curriculum originated with Tsvetkov et al. (2016), who used Bayesian optimization to learn a linear model for ranking examples in a word-embedding task. This approach requires a large number of complete training runs, and is thus impractical for NMT. More recent work has explored bandit optimization for scheduling tasks in a multi-task problem (Graves et al., 2017), and reinforcement learning for selecting examples in a co-trained classifier (Wu et al., 2018). Finally, Liu et al. (2018) apply imitation learning to actively select monolingual training sentences for labeling in NMT, and show that the learned strategy can be transferred to a re"
N19-1208,D17-1155,0,0.0306993,"the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula. 1 Introduction Machine Translation training data is typically heterogeneous: it may vary in characteristics such as domain, translation quality, and degree of difficulty. Many approaches have been proposed to cope with heterogeneity, such as filtering (Duh et al., 2013) or down-weighting (Wang et al., 2017) examples that are likely to be noisy or out of domain. A powerful technique is to control the curriculum—the order in which examples are presented to the system—as is done in fine-tuning (Freitag and Al-Onaizan, 2016), where training occurs first on general data, and then on more valuable in-domain data. Curriculum based approaches generalize data filtering and weighting1 by allowing examples to be visited multiple times 1 Assuming integer weights. or not at all; and they additionally potentially enable steering the training trajectory toward a better global optimum than might be attainable w"
N19-1208,W18-6314,0,0.44856,"a and its attributes. Although powerful heuristics like fine-tuning are helpful, setting hyper-parameters to specify a curriculum is usually a matter of extensive trial and error. Automating this process with meta-learning is thus an attractive proposition. However, it comes with many potential pitfalls such as failing to match a human-designed curriculum, or significantly increasing training time. In this paper we present an initial study on meta-learning an NMT curriculum. Starting from scratch, we attempt to match the performance of a successful non-trivial reference curriculum proposed by Wang et al. (2018), in which training gradually focuses on increasingly cleaner data, as measured by an external scoring function. Inspired by Wu et al. (2018), we use a reinforcement-learning (RL) approach involving a learned agent whose task is to choose a corpus bin, representing a given noise level, at each NMT training step. A challenging aspect of this task is that choosing only the cleanest bin is sub-optimal; the reference curriculum uses all the data in the early stages of training, and only gradually anneals toward the cleanest. Furthermore, we impose the condition that the agent must learn its curric"
P00-1006,W99-0604,0,\N,Missing
P00-1006,J93-2003,0,\N,Missing
P00-1006,J96-1002,0,\N,Missing
P00-1006,P98-2158,0,\N,Missing
P00-1006,C98-2153,0,\N,Missing
P00-1006,W00-0707,1,\N,Missing
P10-1086,J93-2003,0,0.0245623,"MI (r , c ) is defined as: F ( r, c ) N w(r , c) = MI (r , c) = F (r ) F (c ) × log log N N log (3) F ( r , c) + k R ∑ ( F ( r , c) + k ) = F (r , c ) + k (5) F (c) + kR i i =1 where k is a tunable global smoothing constant, and R is the number of rules. 4 Similarity Functions There are many possibilities for calculating similarities between bags-of-words in different languages. We consider IBM model 1 probabilities and cosine distance similarity functions. 4.1 IBM Model 1 Probabilities For the IBM model 1 similarity function, we take the geometric mean of symmetrized conditional IBM model 1 (Brown et al., 1993) bag probabilities, as in Equation (6). (6) sim(α , γ ) = sqrt( P( B f |Be ) ⋅ P( Be |B f )) To compute P ( B f |Be ) , IBM model 1 as(2) where w f i and we j are values for each source 3.3 where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F (r ) . Thus (3) simplifies to: sumes that all source words are conditionally independent, so that: I P( B f |Be ) = ∏ p ( f i |Be ) (7) i =1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model"
P10-1086,D07-1007,0,0.0353197,"nings. Although this bias is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source"
P10-1086,W09-2404,0,0.0273299,"Missing"
P10-1086,P07-1005,0,0.0426806,"is helpful on its own, possibly due to the mechanism we outline in section 6.1, it appears to have a synergistic effect when used along with the bilingual similarity feature. 3) Finally, we note that many of the features we use for capturing similarity, such as the context “the, of” for instantiations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target con"
P10-1086,P05-1033,0,0.813888,"s between the source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: X → α,γ , ~ where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. α ( γ ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in α and γ . 1 There has been a lot of work (more details in Section 7) on applying word sense"
P10-1086,J07-2003,0,0.730767,"source and target sides of a translation rule to improve statistical machine translation performance. This work attempts to measure directly the sense similarity for units from different languages by comparing their contexts1. Our contribution includes proposing new bilingual sense similarity algorithms and applying them to machine translation. We chose a hierarchical phrase-based SMT system as our baseline; thus, the units involved in computation of sense similarities are hierarchical rules. 2 Hierarchical phrase-based MT system The hierarchical phrase-based translation method (Chiang, 2005; Chiang, 2007) is a formal syntaxbased translation modeling method; its translation model is a weighted synchronous context free grammar (SCFG). No explicit linguistic syntactic information appears in the model. An SCFG rule has the following form: X → α,γ , ~ where X is a non-terminal symbol shared by all the rules; each rule has at most two nonterminals. α ( γ ) is a source (target) string consisting of terminal and non-terminal symbols. ~ defines a one-to-one correspondence between non-terminals in α and γ . 1 There has been a lot of work (more details in Section 7) on applying word sense disambiguation"
P10-1086,N09-1025,0,0.020523,"the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similar"
P10-1086,J90-1003,0,0.0978788,"Missing"
P10-1086,W08-0302,0,0.0122429,"iations of X in the unit the X of, are arguably more syntactic than semantic. Thus, like other “semantic” approaches, ours can be seen as blending syntactic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilin"
P10-1086,C08-1041,0,0.0792135,"work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another wo"
P10-1086,P90-1034,0,0.286648,"ranslation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity functi"
P10-1086,N03-1017,0,0.0120534,"t 2 Rule 3 Context 3 Rule 4 Context 4 会议 他, 出席, 了 他 X1 会议 出席, 了 出席 了 他,会议 the meeting he, attended he X1 the meeting attended attended he, the, meeting Figure 1: example of hierarchical rule pairs and their context features. Rule frequencies are counted during rule extraction over word-aligned sentence pairs, and they are normalized to estimate features on rules. Following (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P (γ |α ) and P (α |γ ) are direct and inverse rule-based conditional probabilities; • Pw (γ |α ) and Pw (α |γ ) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a cont"
P10-1086,W09-0424,0,0.0124616,"llowing (Chiang, 2005; Chiang, 2007), 4 features are computed for each rule: • P (γ |α ) and P (α |γ ) are direct and inverse rule-based conditional probabilities; • Pw (γ |α ) and Pw (α |γ ) are direct and inverse lexical weights (Koehn et al., 2003). Empirically, this method has yielded better performance on language pairs such as ChineseEnglish than the phrase-based method because it permits phrases with gaps; it generalizes the normal phrase-based models in a way that allows long-distance reordering (Chiang, 2005; Chiang, 2007). We use the Joshua implementation of the method for decoding (Li et al., 2009). 3 Bag-of-Words Vector Space Model To compute the sense similarity via VSM, we follow the previous work (Lin, 1998) and represent the source and target side of a rule by feature vectors. In our work, each feature corresponds to a context word which co-occurs with the translation rule. 3.1 Context Features In the hierarchical phrase-based translation method, the translation rules are extracted by abstracting some words from an initial phrase pair (Chiang, 2005). Consider a rule with nonterminals on the source and target side; for a given instance of the rule (a particular phrase pair in the tr"
P10-1086,P98-2127,0,0.260401,"improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton"
P10-1086,D08-1010,0,0.12289,"s into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to cho"
P10-1086,D09-1022,0,0.0330004,"tic and semantic information. 7 Related Work There has been extensive work on incorporating semantics into SMT. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or targ"
P10-1086,P03-1021,0,0.00622246,"on 3 from the full training data for α and for γ , recooc spectively. C f v v f = {w f1 , w f 2 ,..., w f I } v va = {waf1 , waf 2 ,..., waf I } 4.3 Cecooc ⊆ Cefull . Therefore, the original similarity functions are to compare the two context vectors built on full training data directly, as shown in Equation (13). sim(α , γ ) = sim(C ffull , Cefull ) (13) Then, we propose a new similarity function as follows: sim(α , γ ) = sim(C ffull , C cooc )λ1 ⋅ sim(C cooc , Cecooc )λ2 ⋅ sim(Cefull , Cecooc )λ3 (14) f f where the parameters λi (i=1,2,3) can be tuned via minimal error rate training (MERT) (Och, 2003). (11) α The standard cosine distance is defined as the v v inner product of the two vectors v f and va norγ malized by their norms. Based on Equation (10) and (11), it is easy to derive the similarity as follows: v v v f ⋅ va v v sim(α , γ ) = cos( v f , v a ) = v v |v f |⋅ |va | (12) J ∑∑ w = i =1 j =1 I fi Pr( f i |e j )we j I 2 sqrt (∑ w 2fi )sqrt (∑ wafi ) I =1 i =1 are the contexts for they satisfy the constraints: C cooc ⊆ C ffull and f Naïve Cosine Distance Similarity I cooc and C e α and γ when α and γ co-occur. Obviously, where p( f i |e j ) is a lexical probability (we use IBM model"
P10-1086,J07-2002,0,0.0108003,"rt hierarchical phrase-based machine translation system. 1 Introduction The sense of a term can generally be inferred from its context. The underlying idea is that a term is characterized by the contexts it co-occurs with. This is also well known as the Distributional Hypothesis (Harris, 1954): terms occurring in similar contexts tend to have similar meanings. There has been a lot of work to compute the sense similarity between terms based on their distribution in a corpus, such as (Hindle, 1990; Lund and Burgess, 1996; Landauer and Dumais, 1997; Lin, 1998; Turney, 2001; Pantel and Lin, 2002; Pado and Lapata, 2007). In the work just cited, a common procedure is followed. Given two terms to be compared, one first extracts various features for each term from their contexts in a corpus and forms a vector space model (VSM); then, one computes their similarity by using similarity functions. The features include words within a surface window of a fixed size (Lund and Burgess, 1996), grammatical dependencies (Lin, 1998; Pantel and Lin 2002; Pado and Lapata, 2007), etc. The similarity function which has been most widely used is cosine distance (Salton and McGill, 1983); other similarity functions include Euclid"
P10-1086,P02-1040,0,0.0849689,"sk. For German-to-English tasks, we used WMT 2006 4 data sets. The parallel training data contains 21 million target words; both the dev set and test set contain 2000 sentences; one reference is provided for each source input sentence. Only the target-language half of the parallel training data are used to train the language model in this task. 5.2 Results For the baseline, we train the translation model by following (Chiang, 2005; Chiang, 2007) and our decoder is Joshua 5 , an open-source hierarchical phrase-based machine translation system written in Java. Our evaluation metric is IBM BLEU (Papineni et al., 2002), which performs case-insensitive matching of n-grams up to n = 4. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. By observing the results on dev set in the additional experiments, we first set the smoothing constant k in Equation (5) to 0.5. Then, we need to set the sizes of the vectors to balance the computing time and translation accu4 5 838 http://www.statmt.org/wmt06/ http://www.cs.jhu.edu/~ccb/joshua/index.html racy, i.e., we keep only the top N context words with the highest feature value for each side of a rule 6 . In the following, we use “Alg"
P10-1086,P99-1067,0,0.234566,", City Block distance (Bullinaria and Levy; 2007), and Dice and Jaccard coefficients (Frakes and Baeza-Yates, 1992), etc. Measures of monolingual sense similarity have been widely used in many applications, such as synonym recognizing (Landauer and Dumais, 1997), word clustering (Pantel and Lin 2002), word sense disambiguation (Yuret and Yatbaz 2009), etc. Use of the vector space model to compute sense similarity has also been adapted to the multilingual condition, based on the assumption that two terms with similar meanings often occur in comparable contexts across languages. Fung (1998) and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora. The vectors in different languages are first mapped to a common space using an initial bilingual dictionary, and then compared. However, there is no previous work that uses the VSM to compute sense similarity for terms from parallel corpora. The sense similarities, i.e. the translation probabilities in a translation model, for units from parallel corpora are mainly based on the co-occurrence counts of the two units. Therefore, questions emerge: how good is the sense similarity computed v"
P10-1086,N09-2004,0,0.0398442,"ord-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT. More recent work has aimed at incorporating richer disambiguating features into the SMT log-linear model (Gimpel and Smith, 2008; Chiang et al, 2009); predicting coherent sets of target words rather than individual phrase translations (Bangalore et al, 2009; Mauser et al, 2009); and selecting applicable rules in hierarchical (He et al, 2008) and syntactic (Liu et al, 2008) translation, relying on source as well as target context. Work by Wu and Fung (2009) breaks new ground in attempting to match semantic roles derived from a semantic parser across source and target languages. Our work is different from all the above approaches in that we attempt to discriminate among hierarchical rules based on: 1) the degree of bilingual semantic similarity between source and target translation units; and 2) the monolingual semantic similarity between occurrences of source or target units as part of the given rule, and in general. In another words, WSD explicitly tries to choose a translation given the current source context, while our work rates rule pairs i"
P10-1086,N04-1033,0,0.0655189,"on (6). (6) sim(α , γ ) = sqrt( P( B f |Be ) ⋅ P( Be |B f )) To compute P ( B f |Be ) , IBM model 1 as(2) where w f i and we j are values for each source 3.3 where N is the total frequency counts of all rules and their context words. Since we are using this value as a weight, following (Turney, 2001), we drop log, N and F (r ) . Thus (3) simplifies to: sumes that all source words are conditionally independent, so that: I P( B f |Be ) = ∏ p ( f i |Be ) (7) i =1 To compute, we use a “Noisy-OR” combination which has shown better performance than standard IBM model 1 probability, as described in (Zens and Ney, 2004): p( fi |Be ) = 1 − p( f i |Be ) (8) J p( fi |Be ) ≈ 1 − ∏ (1 − p ( f i |e j )) (9) j =1 where p( f i |Be ) is the probability that f i is not in the translation of Be , and is the IBM model 1 probability. 4.2 Vector Space Mapping A common way to calculate semantic similarity is by vector space cosine distance; we will also 836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. There"
P10-1086,W04-3227,0,0.0214075,"836 use this similarity function in our algorithm. However, the two vectors in Equation (2) cannot be directly compared because the axes of their spaces represent different words in different languages, and also their dimensions I and J are not assured to be the same. Therefore, we need to first map a vector into the space of the other vector, so that the similarity can be calculated. Fung (1998) and Rapp (1999) map the vector onedimension-to-one-dimension (a context word is a dimension in each vector space) from one language to another language via an initial bilingual dictionary. We follow (Zhao et al., 2004) to do vector space mapping. Our goal is – given a source pattern – to distinguish between the senses of its associated target patterns. Therefore, we map all vectors in target language into the vector space in the source language. What we want is a representav tion va in the source language space of the target v v vector ve . To get va , we can let waf i , the weight of the ith source feature, be a linear combination over target features. That is to say, given a source feature weight for fi, each target feature weight is linked to it with some probability. So that we can calculate a transform"
P10-1086,J10-1004,0,\N,Missing
P10-1086,P07-1020,0,\N,Missing
P10-1086,C98-2122,0,\N,Missing
P10-1086,W04-3250,0,\N,Missing
P12-1099,W09-0432,0,0.123035,"tabolic diseases . IN naglazyme treatment should be supervis´e by a doctor the with in the management of patients with mps vi or other hereditary metabolic disease . OUT naglazyme ’s treatment must be supervised by a doctor with the experience of the care of patients with mps vi. or another disease hereditary metabolic . ENSEMBLE naglazyme treatment should be supervised by a physician experienced in the management of patients with mps vi or other hereditary metabolic disease . Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a f"
P12-1099,D08-1024,0,0.0137368,"ith the other two state-of-the-art SMT systems. Secondly, our combining method uses the union option, but instead of preserving the features of all phrase-tables, it only combines their scores using various mixture operations. This enables us to experiment with a number of different operations as opposed to sticking to only one combination method. Finally, by avoiding increasing the number of features we can add as many translation models as we need without serious performance drop. In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al., 2008). Our approach differs from the model combination approach of DeNero et al. (2010), a generalization of consensus or minimum Bayes risk decoding where the search space consists of those of multiple systems, in that model combination uses forest of derivations of all component models to do the combination. In other words, it requires all component models to fully decode each sentence, compute n-gram expectations from each component model and calculate posterior probabilities over translation derivations. While, in our approach we only use partial hypotheses from component models and the derivat"
P12-1099,P05-1033,0,0.540776,"se a novel system combination approach called ensemble decoding in order to combine two or more translation models with the goal of constructing a system that outperforms all the component models. The strength of this system combination method is that the systems are combined in the decoder. This enables the decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics as well as the linear mixture model of (Foster et al., 2010) for conditional phrase-p"
P12-1099,W07-0722,0,0.0287266,"es. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; SOURCE REF IN OUT ENSEMBLE SOURCE REF am´enorrh´ee , menstruations irr´eguli`eres amenorrhoea , irregular menstruation amenorrhoea , menstruations irr´eguli`eres am´enorrh´ee , irregular menstruation amenorrhoea , irregular menstruation le traitement par naglazyme doit eˆ tre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou d’ une autre maladie m"
P12-1099,P11-2031,0,0.0183841,"OUT LOGLIN LINMIX PBS 31.84 24.08 31.75 32.21 33.81 Hiero 33.69 25.32 33.76 – 35.57 Table 2: The results of various baselines implemented in a phrase-based (PBS) and a Hiero SMT on EMEA. Table 3 shows the results of ensemble decoding with different mixture operations and model weight settings. Each mixture operation has been evaluated on the test-set by setting the component weights uniformly (denoted by uniform) and by tuning the weights using C ONDOR (denoted by tuned) on a held-out set. The tuned scores (3rd column in Table 3) are averages of three runs with different initial points as in Clark et al. (2011). We also reported the BLEU scores when we applied the span-wise normalization heuristic. All of these mixture operations were able to significantly improve over the concatenation baseline. In particular, Switching:Max could gain up to 2.2 BLEU points over the concatenation baseline and 0.39 BLEU points over the best performing baseline (i.e. linear mixture model implemented in Hiero) which is statistically significant based on Clark et al. (2011) (p = 0.02). Prod when using with uniform weights gets the Mixture Operation W MAX W SUM S WITCHING :M AX S WITCHING :S UM P ROD Uniform 35.39 35.35"
P12-1099,eck-etal-2004-language,0,0.209087,"on Majid Razmara1 1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada {razmara,baskaran,anoop}@sfu.ca 2 National Research Council Canada, 283 Alexandre-Tach´e Blvd, Gatineau, QC, Canada george.foster@nrc.gc.ca Abstract translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain. As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation. It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation. In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments. We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation. Statistical machine translation is often faced with the problem of combining training data fro"
P12-1099,W07-0717,1,0.427919,"he decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics as well as the linear mixture model of (Foster et al., 2010) for conditional phrase-pair probabilities over IN and OUT. Furthermore, within the framework of ensemble decoding, we study and evaluate various methods for combining translation tables. 2 Baselines The natural baseline for model adaption is to concatenate the IN and OUT data into a single parallel corpus and train a model on it. In addit"
P12-1099,D10-1044,1,0.742619,"Missing"
P12-1099,W09-0406,0,0.0440285,"r other hereditary metabolic disease . Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a feature of the factored translation model framework in Moses SMT system (Koehn and Schroeder, 2007) to use multiple alternative decoding paths. Two decoding paths, one for each translation table (IN and OUT), were used during decoding. The weights are set with minimum error rate training (Och, 2003). Our work is closely related to Koehn and Schroeder (2007) but uses a different approach to deal with multiple translation tables. The Moses S"
P12-1099,2005.eamt-1.19,0,0.0283502,"T models to construct a better translation than both of them. In the first example, there are two OOVs one for each of the IN and OUT models. Our approach is able to resolve the OOV issues by taking advantage of the other model’s presence. Similarly, the second example shows how ensemble decoding improves lexical choices as well as word re-orderings. 945 5 5.1 Related Work Domain Adaptation Early approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instance"
P12-1099,P07-1034,0,0.0251815,"approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instances. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work"
P12-1099,W07-0733,0,0.780622,"instances than OUT instances. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; SOURCE REF IN OUT ENSEMBLE SOURCE REF am´enorrh´ee , menstruations irr´eguli`eres amenorrhoea , irregular menstruation amenorrhoea , menstruations irr´eguli`eres am´enorrh´ee , irregular menstruation amenorrhoea , irregular menstruation le traitement par naglazyme doit eˆ tre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou"
P12-1099,N03-1017,0,0.049812,"¯ e|f¯) and pm (f¯|¯ e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 ∗ 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm (¯ e|f¯) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: p(¯ e|f¯) = M X λm pm (¯ e|f¯) m Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p˜(¯ e, f¯) from the development set using standard techniques (HMM word alignment with grow-diag-and symmeterization (Koehn et al., 2003)). We then find the set ˆ that minimize the cross-entropy of the of weights λ mixture p(¯ e|f¯) with respect to p˜(¯ e, f¯): 941 ˆ = argmax λ λ X p˜(¯ e, f¯) log e¯,f¯ M X λm pm (¯ e|f¯) m For efficiency and stability, we use the EM algoˆ rather than L-BFGS as in (Foster et rithm to find λ, al., 2010). Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm (¯ e|f¯) to 0; pairs in p˜(¯ e, f¯) that do not appear in at least one component table are discarded. We learn separate linear mixtures for relative-frequency and lexical estimates for both p(¯ e|f¯)"
P12-1099,P09-1065,0,0.0178941,"ed by the ensemble model. A major difference is that in the model combination approach the component search spaces are conjoined and they are not intermingled as opposed to our approach where these search spaces are intermixed on spans. This enables us to generate new sentences that cannot be generated by component models. Furthermore, various combination methods can be explored in our approach. Finally, main techniques used in this work are orthogonal to our approach such as Minimum Bayes Risk decoding, using n-gram features and tuning using MERT. Finally, our work is most similar to that of Liu et al. (2009) where max-derivation and maxtranslation decoding have been used. Maxderivation finds a derivation with highest score and max-translation finds the highest scoring translation by summing the score of all derivations with the same yield. The combination can be done in two levels: translation-level and derivation-level. Their derivation-level max-translation decoding is similar to our ensemble decoding with wsum as the mixture operation. We did not restrict ourself to this particular mixture operation and experimented with a 947 number of different mixing techniques and as Table 3 shows we could"
P12-1099,P00-1056,0,0.0614986,"e corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1 2 guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to compare various translation model combination techniques. www.statmt.org/europarl Please contact the authors to access the data-sets. 944 Results Table 2 shows the results of the baselines. The first group are the baseline results on the phrase-based system discussed"
P12-1099,P03-1021,0,0.148516,"baseline for model adaption is to concatenate the IN and OUT data into a single parallel corpus and train a model on it. In addition to this baseline, we have experimented with two more sophisticated baselines which are based on mixture techniques. 2.1 Log-Linear Mixture Log-linear translation model (TM) mixtures are of the form: X  M ¯ ¯ p(¯ e|f ) ∝ exp λm log pm (¯ e|f ) m where m ranges over IN and OUT, pm (¯ e|f¯) is an estimate from a component phrase table, and each λm is a weight in the top-level log-linear model, set so as to maximize dev-set BLEU using minimum error rate training (Och, 2003). We learn separate weights for relative-frequency and lexical estimates for both pm (¯ e|f¯) and pm (f¯|¯ e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 ∗ 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm (¯ e|f¯) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: p(¯ e|f¯) = M X λm pm (¯ e|f¯) m Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p˜(¯ e, f¯) from the development"
P12-1099,W05-0822,1,0.808324,"1 Experimental Setup 4.2 We carried out translation experiments using the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus1 as OUT, for French to English translation. The dev and test sets were randomly chosen from the EMEA corpus.2 The details of datasets used are summarized in Table 1. Dataset Sents EMEA Europarl Dev Test 11770 1.3M 1533 1522 Words French English 168K 144K 40M 37M 29K 25K 29K 25K Table 1: Training, dev and test sets for EMEA. For the mixture baselines, we used a standard one-pass phrase-based system (Koehn et al., 2003), Portage (Sadat et al., 2005), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count. The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sanka"
P12-1099,P05-1003,0,0.0331143,"f¯) ∝ exp M X λm (wm · φm )  In Section 4.2, we compare the BLEU scores of different mixture operations on a French-English experimental setup. 3.2 Normalization m Product models have been used in combining LMs and TMs in SMT as well as some other NLP tasks such as ensemble parsing (Petrov, 2010). Each of these mixture operations has a specific property that makes it work in specific domain adaptation or system combination scenarios. For instance, LOPs may not be optimal for domain adaptation in the setting where there are two or more models trained on heterogeneous corpora. As discussed in (Smith et al., 2005), LOPs work best when all the models accuracies are high and close to each other with some degree of diversity. LOPs give veto power to any of the component models and this perfectly works for settings such as the one in (Petrov, 2010) where a number of parsers are trained by changing the randomization seeds but having the same base parser and using the same training set. They noticed that parsers trained using different randomization seeds have high accuracies but there are some diversities among them and they used product models for their advantage to get an even better parser. We assume tha"
P12-1099,P07-1004,1,0.757959,"es the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; SOURCE REF IN OUT ENSEMBLE SOURCE REF am´enorrh´ee , menstruations irr´eguli`eres amenorrhoea , irregular menstruation amenorrhoea , menstruations irr´eguli`eres am´enorrh´ee , irregular menstruation amenorrhoea , irregular menstruation le traitement par naglazyme doit eˆ tre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou d’ une autre maladie m´etabolique h´er´editaire . naglazyme treatment should be supervised by a physician experienced in the management of patients with mps vi or other inherited metabolic diseases . IN naglazyme tre"
P12-1099,N10-1141,0,\N,Missing
P12-1099,N10-1003,0,\N,Missing
P12-1099,D08-1076,0,\N,Missing
P13-1126,D11-1033,0,0.368647,"models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a s"
P13-1126,W09-0432,0,0.103531,"st approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically us"
P13-1126,P08-2040,1,0.823159,"2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Associati"
P13-1126,2011.mtsummit-papers.30,1,0.887165,"Missing"
P13-1126,N12-1047,1,0.101994,"ing IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with KN smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). 3.3 Results For the baseline, we simply concatenate all training data. We have also compared our approach to two widely used TM domain adaptation ap1288 proaches. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. The other is a linear combination of TMs trained on each subcorpus, with the weights of each model learned with an EM algorithm to maximize the likelihood of joint empirical phrase pair counts for in-domain dev data. For details, refer to (Foster and Kuhn, 200"
P13-1126,W07-0717,1,0.91413,"omain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen"
P13-1126,D10-1044,1,0.928725,"Missing"
P13-1126,D08-1089,0,0.0852581,"Missing"
P13-1126,2005.eamt-1.19,0,0.0733146,"different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the"
P13-1126,P90-1034,0,0.285011,"independent of the subcorpora. One can think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =< w1 (f, e), ...wi (f, e), ..., wC (f, e) &gt;, (1) where wi (f, e) is a standard tf · idf weight, i.e. wi (f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci (f, e) in the corpus si by dividing"
P13-1126,C10-1056,0,0.0231682,"ve learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop o"
P13-1126,P10-1026,0,0.0348231,"Missing"
P13-1126,W07-0733,0,0.55098,"cently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add th"
P13-1126,P07-2045,0,0.0039338,"Missing"
P13-1126,W04-3250,0,0.184155,"nglish and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: λ was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with λ and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both COS and BC yield statistically significant improvements over the baseline, with BC performing better than COS by a further statistically significant margin. The Bhattacharyya coefficient"
P13-1126,E12-1055,0,0.385818,"in data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT 1 proposed phras"
P13-1126,P07-1004,0,0.0242508,"in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2"
P13-1126,C04-1059,0,0.0646896,"Missing"
P13-1126,P98-2127,0,0.0289323,"think of several other ways of defining the vector space that might yield even better results than those reported here. Thus, VSM adaptation is not limited to the variant of it that we tested in our experiments. 2 Vector space model adaptation Vector space models (VSMs) have been widely applied in many information retrieval and natural language processing applications. For instance, to compute the sense similarity between terms, many researchers extract features for each term from its context in a corpus, define a VSM and then apply similarity functions (Hindle, 1990; Lund and Burgess, 1996; Lin, 1998; Turney, 2001). In our experiments, we exploited the fact that the training data come from a set of subcorpora. For instance, the Chinese-English training data are made up of 14 subcorpora (see section 3 below). Suppose we have C subcorpora. The domain vector for a phrase-pair (f, e) is defined as V (f, e) =< w1 (f, e), ...wi (f, e), ..., wC (f, e) &gt;, (1) where wi (f, e) is a standard tf · idf weight, i.e. wi (f, e) = tfi (f, e) · idf (f, e) . (2) To avoid a bias towards longer corpora, we normalize the raw joint count ci (f, e) in the corpus si by dividing by the maximum raw count of any phr"
P13-1126,D07-1036,0,0.352527,"Missing"
P13-1126,D09-1074,0,0.415681,"n the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might rec"
P13-1126,P10-2041,0,0.137255,"nearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For examp"
P13-1126,P02-1040,0,0.103578,"For details, refer to (Foster and Kuhn, 2007). The value of λ and α (see Eq 4 and Section 2.1) are determined by the performance on the dev set of the Arabic-to-English system. For both Arabic-to-English and Chinese-to-English experiment, these values obtained on Arabic dev were used to obtain the results below: λ was set to 8, and α was set to 0.01. (Later, we ran an experiment on Chinese-to-English with λ and α tuned specifically for that language pair, but the performance for the Chinese-English system only improved by a tiny, insignificant amount). Our metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores averaged across both test sets NIST06 and NIST08 for Chinese; NIST08 and NIST09 for Arabic. Following (Koehn, 2004), we use the bootstrapresampling test to do significance testing. In tables 3 to 5, * and ** denote significant gains over the baseline at p < 0.05 and p < 0.01 levels, respectively. We first compare the performance of different similarity functions: cosine (COS), JensenShannon divergence (JSD) and Bhattacharyya coefficient (BC). The results are shown in Table 3. All three functions obtained improvements. Both"
P13-1126,2011.mtsummit-papers.2,0,0.0163856,"em trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics typically use a rich feature set to decide on weights for the training data, at the sentence or phrase pair level. For example, a sentence from a subcorpus whose domain is far from that of the dev set would typically receive a low weight, but sentences in this subcorpus that appear to be of a general nature might receive higher weights. The 2012 JHU workshop on Domain Adaptation for MT"
P13-1126,2008.iwslt-papers.6,0,0.0772445,"ded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Ueffing et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; L¨u et al., 2007; Moore and Lewis, 2010; Axelrod et al., 2011) search for bilingual sentence pairs that are similar to the indomain “dev” data, then add them to the training data. Instance weighting approaches (Matsoukas et al., 2009; Foster et al., 2010; Huang and Xiang, 2010; Phillips and Brown, 2011; Sennrich, 2012) 1285 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1285–1293, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computat"
P13-1126,C98-2122,0,\N,Missing
P18-1008,W17-3203,0,0.0178033,"ems based on the encoder-decoder paradigm. In the first architectures that surpassed ∗ Equal contribution. 76 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 76–86 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We quickly note two prior works that provided empirical solutions to the difficulty of training NMT architectures (specifically RNMT). In (Britz et al., 2017) the authors systematically explore which elements of NMT architectures have a significant impact on translation quality. In (Denkowski and Neubig, 2017) the authors recommend three specific techniques for strengthening NMT systems and empirically demonstrated how incorporating those techniques improves the reliability of the experimental results. and how much can be attributed to the associated training and inference techniques. In some cases, these new techniques may be broadly applicable to other architectures and thus constitute a major, though implicit, contribution of an architecture paper. Clearly, they need to be considered in order to ensure a fair comparison across different model architectures. In this paper, we therefore take a ste"
P18-1008,D17-1300,0,0.0176911,"tigate the combination of transformer layers with RNMT+ layers in the same encoder block to build even richer feature representations. We exclusively use RNMT+ decoders in the following architectures since stateful decoders show better performance according to Table 5. We study two mixing schemes in the encoder (see Fig. 2): (1) Cascaded Encoder: The cascaded encoder aims at combining the representational power of RNNs and self-attention. The idea is to enrich a set of stateful representations by cascading a feature extractor with a focus on vertical mapping, similar to (Pascanu et al., 2013; Devlin, 2017). Our best performing cascaded encoder involves fine tuning transformer layers stacked on top of a pre-trained frozen RNMT+ encoder. Using a pre-trained encoder avoids optimization difficulties while significantly enhancing encoder capacity. As shown in Table 6, the cascaded encoder improves over the Transformer encoder by more than 0.5 BLEU points on the WMT’14 En→Fr task. This suggests that the Transformer encoder is able to extract richer representations if the input is augmented with sequential context. (2) Multi-Column Encoder: As illustrated in Fig. 2b, a multi-column encoder merges the"
P18-1008,D17-1151,0,0.0161321,"; Cho et al., 2014) has revolutionized the field of MT by replacing traditional phrasebased approaches with neural machine translation (NMT) systems based on the encoder-decoder paradigm. In the first architectures that surpassed ∗ Equal contribution. 76 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 76–86 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We quickly note two prior works that provided empirical solutions to the difficulty of training NMT architectures (specifically RNMT). In (Britz et al., 2017) the authors systematically explore which elements of NMT architectures have a significant impact on translation quality. In (Denkowski and Neubig, 2017) the authors recommend three specific techniques for strengthening NMT systems and empirically demonstrated how incorporating those techniques improves the reliability of the experimental results. and how much can be attributed to the associated training and inference techniques. In some cases, these new techniques may be broadly applicable to other architectures and thus constitute a major, though implicit, contribution of an architecture pap"
P18-1008,D16-1244,1,0.841999,"Missing"
P18-1008,P15-1001,0,0.0274561,"rage dev-set BLEU score over 21 consecutive evaluations. We report the mean test score and standard deviation over the selected window. This allows us to compare model architectures based on their mean performance after convergence rather than individual checkpoint evaluations, as the latter can be quite noisy for some models. To enable a fair comparison of architectures, we use the same pre-processing and evaluation methodology for all our experiments. We refrain from using checkpoint averaging (exponential moving averages of parameters) (JunczysDowmunt et al., 2016) or checkpoint ensembles (Jean et al., 2015; Chen et al., 2017) to focus on evaluating the performance of individual models. 4 RNMT+ 4.1 Model Architecture of RNMT+ The newly proposed RNMT+ model architecture is shown in Figure 1. Here we highlight the key architectural choices that are different between the RNMT+ model and the GNMT model. There are 6 bidirectional LSTM layers in the encoder instead of 1 bidirectional LSTM layer followed by 7 unidirectional layers as in GNMT. For each bidirectional layer, the outputs of the forward layer and the backward layer are concatenated before being fed into the next layer. The decoder network c"
P18-1008,W16-2316,0,0.0599679,"Missing"
P18-1008,D13-1176,0,0.0714055,"techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT’14 English→French and English→German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets. 1 Introduction In recent years, the emergence of seq2seq models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has revolutionized the field of MT by replacing traditional phrasebased approaches with neural machine translation (NMT) systems based on the encoder-decoder paradigm. In the first architectures that surpassed ∗ Equal contribution. 76 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 76–86 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We quickly note two prior works that provided empirical solutions to the difficulty of training NMT architectures ("
P18-1008,Q16-1027,0,0.0309818,"on Wolfgang Macherey Noam Shazeer Ashish Vaswani George Foster Ankur Bapna ∗ Llion Jones Jakob Uszkoreit Niki Parmar Lukasz Kaiser Mike Schuster Zhifeng Chen Yonghui Wu Macduff Hughes miachen,orhanf,ankurbpn,yonghui@google.com Google AI Abstract the quality of phrase-based MT, both the encoder and decoder were implemented as Recurrent Neural Networks (RNNs), interacting via a soft-attention mechanism (Bahdanau et al., 2015). The RNN-based NMT approach, or RNMT, was quickly established as the de-facto standard for NMT, and gained rapid adoption into large-scale systems in industry, e.g. Baidu (Zhou et al., 2016), Google (Wu et al., 2016), and Systran (Crego et al., 2016). Following RNMT, convolutional neural network based approaches (LeCun and Bengio, 1998) to NMT have recently drawn research attention due to their ability to fully parallelize training to take advantage of modern fast computing devices. such as GPUs and Tensor Processing Units (TPUs) (Jouppi et al., 2017). Well known examples are ByteNet (Kalchbrenner et al., 2016) and ConvS2S (Gehring et al., 2017). The ConvS2S model was shown to outperform the original RNMT architecture in terms of quality, while also providing greater training spe"
W00-0507,C90-3008,0,0.247146,"Missing"
W00-0507,J93-2003,0,0.00567536,"xpensive to compute than p(t[s) when using IBM-style translation models. Since speed is crucial for our application, we chose to forego it in the work described here. Our linear combination model is fully described in (Langlais and Foster, 2000) but can be seen as follows: 2.2.1 T h e e v a l u a t o r The evaluator is a function p(t[t&apos;, s) which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t&apos; which precede t in the current translation of s. Our approach to modeling this distribution is based to a large extent on that of the IBM group (Brown et al., 1993), but it diflhrs in one significant aspect: whereas the IBM model involves a &quot;noisy channel&quot; decomposition, we 48 p(tlt&apos;,s ) = p(tlt&apos; ) A(O(t&apos;,s)), (1) language + p(tls)[1-~(O(t&apos;,s))! translation where .~(O(t&apos;,s)) e [0,1] are contextdependent interpolation coefficients. O(t~,s) stands for any function which maps t~,s into a set of equivalence classes. Intuitively, ),(O(t r, s)) should be high when s is more informative than t r and low otherwise. For example, the translation model could have a higher weight at the start of sentence but the contribution of the language model can become more imp"
W00-0507,langlais-etal-2000-evaluation,1,0.713413,"Missing"
W00-0507,A00-1018,0,0.0195985,"Missing"
W00-0507,A00-1019,1,0.86616,"Missing"
W00-0507,C90-2045,0,0.0899252,"eract with the user and how to find appropriate multi-word units for suggestions that can be computed in real time. 1 Introduction TRANSTYPE is a project set up to explore an appealing solution to the problem of using Interactive Machine Translation (IMT) as a tool for professional or other highly-skilled translators. IMT first appeared as part of Kay&apos;s MIND system (Kay, 1973), where the user&apos;s role was to help the computer analyze the source text by answering questions about word sense, ellipsis, phrasal attachments, etc. Most later work on IMT, eg (Blanchon, 1991; Brown and Nirenburg, 1990; Maruyama and Watanabe, 1990; Whitelock et al., 1986), has followed in this vein, concentrating on improving the question/answer process by having less questions, more friendly ones, etc. Despite progress in these endeavors, systems of this sort are generally unsuitable as tools for skilled trans][ators because the user serves only as an advisor, with the MT components keeping overall control over the translation process. TRANSTYPE originated from the conviction that a better approach to IMT for competent translators would be to shift the focus of interaction from the meaning of the source text to the form of the target"
W00-0507,C86-1077,0,0.503703,"o find appropriate multi-word units for suggestions that can be computed in real time. 1 Introduction TRANSTYPE is a project set up to explore an appealing solution to the problem of using Interactive Machine Translation (IMT) as a tool for professional or other highly-skilled translators. IMT first appeared as part of Kay&apos;s MIND system (Kay, 1973), where the user&apos;s role was to help the computer analyze the source text by answering questions about word sense, ellipsis, phrasal attachments, etc. Most later work on IMT, eg (Blanchon, 1991; Brown and Nirenburg, 1990; Maruyama and Watanabe, 1990; Whitelock et al., 1986), has followed in this vein, concentrating on improving the question/answer process by having less questions, more friendly ones, etc. Despite progress in these endeavors, systems of this sort are generally unsuitable as tools for skilled trans][ators because the user serves only as an advisor, with the MT components keeping overall control over the translation process. TRANSTYPE originated from the conviction that a better approach to IMT for competent translators would be to shift the focus of interaction from the meaning of the source text to the form of the target text. This would relieve"
W00-0707,A00-1019,1,0.897551,"Missing"
W00-0707,P98-2158,0,0.15547,"Missing"
W00-0707,W99-0604,0,0.0774749,"Missing"
W00-0707,1992.tmi-1.7,1,0.793335,"Missing"
W00-0707,J96-1002,0,0.202252,"the &quot;decoding&quot; problem of finding the most likely target text. In particular, if hi is known, finding the best word at the current position requires only a straightforward search through the target p(w[hi, s) -- Ap(w[hi) + (1 - A)p(w[i, s). (2) where p(w[hi) is a language model, p(wli , s) is a translation model, and A E [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model 1This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using a stop token or a prior distribution over lengths. 37 significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters. Like model 1, its translation component is base"
W00-0707,J93-2003,0,0.030445,"d search through the target p(w[hi, s) -- Ap(w[hi) + (1 - A)p(w[i, s). (2) where p(w[hi) is a language model, p(wli , s) is a translation model, and A E [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for p(w[hi, s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model 1This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using a stop token or a prior distribution over lengths. 37 significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters. Like model 1, its translation component is based only on the occurrences in s of words which are potential translations for w, and does not take into account the positions of these words relative to w. An obvious enh"
W00-0707,1997.mtsummit-papers.1,0,0.0573792,"Missing"
W00-0707,C98-2153,0,\N,Missing
W00-0707,P00-1006,1,\N,Missing
W02-1020,J93-2003,0,0.00791075,"Missing"
W02-1020,W97-0504,0,0.109042,"Missing"
W02-1020,1997.mtsummit-papers.1,0,0.685544,"Missing"
W02-1020,W00-0707,1,0.889221,"Missing"
W02-1020,J99-4005,0,0.0733578,"Missing"
W02-1020,A00-1019,1,0.904482,"Missing"
W02-1020,P98-2158,0,0.0401272,"Missing"
W02-1020,C00-2123,0,0.0311209,"Missing"
W02-1020,C98-2153,0,\N,Missing
W02-1020,langlais-etal-2002-translators,1,\N,Missing
W02-1020,P00-1006,1,\N,Missing
W03-0413,J93-2003,0,0.023989,"Missing"
W03-0413,W02-1020,1,0.878672,"uggestions may range in length from 0 characters to the end of the target sentence; it is up to the system to decide how much text to predict in a given context, balancing the greater potential benefit of longer predictions against a greater likelihood of being wrong, and a higher cost to the user (in terms of distraction and editing) if they are wrong or only partially right. Our solution to the problem of how much text to predict is based on a decision-theoretic framework in which we attempt to find the prediction that maximizes the expected benefit to the translator in the current context (Foster et al., 2002b). Formally, we seek: x ˆ = argmax B(x|h, s), x (1) where x is a prediction about what will follow h in the translation of a source sentence s, and B(x|h, s) is the expected benefit in terms of typing time saved. As described in (Foster et al., 2002b), B(ˆ xm |h, s) = Pl k=0 p(k|x, h, s)B(x|h, s, k) depends on two main quantities: the probability p(k|x, h, s) that exactly k characters from the beginning of x are correct, and the benefit B(x|h, s, k) to the translator if this is the case. B(x|h, s, k) is estimated from a model of user behaviour—based on data collected in user trials of the too"
W03-0413,W00-0707,1,0.836689,"user model to determine his or her responses to predictions and to estimate the resulting benefit. Further details are given in (Foster et al., 2002b). 2.1 Translation Models We experimented with three different translation models for p(w|h, s). All have the property of being fast enough to support real-time searches for predictions of up to 5 words. The first model, referred to as Maxent1 below, is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the IBM translation model 2 (Brown et al., 1993). This model is described in (Foster, 2000). Its major weakness is that it does not keep track of which words in the current source sentence have already been translated, and hence it is prone to repeating previous suggestions. The second model, called Maxent2 below, is similar to Maxent1 but with the addition of extra parameters to limit this behaviour (Foster et al., 2002a). The final model, called Bayes below, is also described in (Foster et al., 2002a). It is a noisy-channel combination of a trigram language model and an IBM model 2 for the source text given target text. This model has roughly the same theoretical predictive capabi"
W03-0413,foster-etal-2002-text,1,\N,Missing
W03-0413,P98-1004,0,\N,Missing
W03-0413,C98-1004,0,\N,Missing
W04-3225,J96-1002,0,0.0140255,"ed within our IMT prototype. Section 3 describes the cache-based adaptation we performed on the target language model. In section 4, we present the different types of adaptations we performed on the translation model. Section 5 then puts the results in the context of our IMT application. Section 6 discusses the implications of our experiments and suggests some improvements that could be made to the system. 2 Current IMT models The word-based translation model embedded within the IMT system has been designed by Foster (2000). It is a Maximum Entropy/Minimum Divergence (MEMD) translation model (Berger et al., 1996), which mimics the parameters of the IBM model 2 (Brown et al., 1993) within a log-linear setting. The resulting model (named MDI2B) is of the following form, where h is the current target text, s the source sentence being translated, s a particular word in s and w the next word to be predicted: P q(w|h) exp( s∈s αsw + βAB ) p(w|h, s) = (1) Z(h, s) The q distribution represents the prior knowledge that we have about the true distribution and is modeled by an interpolated trigram in this study. The α coefficients are the familiar transfer or lexical parameters, and the β ones can be understood"
W04-3225,C88-1016,0,0.0304285,"as their position dependent correction. Z is a normalizing factor, the sum of the numerator for every w in the target vocabulary. Our baseline model used an interpolated trigram of the following form as the q distribution: p(w|h) = + + + λ1 (wi−2 wi−1 ) × ptri (wi |wi−2 wi−1 ) λ2 (wi−2 wi−1 ) × pbi (wi |wi−1 ) λ3 (wi−2 wi−1 ) × puni (wi ) λ4 (wi−2 wi−1 ) × |V 1|+1 where λ1 (wi−2 wi−1 ) + λ2 (wi−2 wi−1 ) + λ3 (wi−2 wi−1 ) + λ4 (wi−2 wi−1 ) = 1 and |V |+ 1 is the size of the event space (including a special unknown word). As mentioned above, the MDI2B model is closely related to the IBM2 model (Brown et al., 1988). It contains two classes of features: word pair features and positional features. The word pair feature functions are defined as follows:  1 if s ∈ s and t = w fst (w, h, s) = 0 otherwise This function is on if the predicted word is t and s is in the current source sentence. Each feature fst has a corresponding weight αst (for brevity, this is defined to be 0 in equation 1 if the pair s, t is not included in the model). The positional feature functions are defined as follows: fA,B (w, i, s) = J X δ[(i, j, J) ∈ A ∧ (sj , w) ∈ B ∧ j = ˆsj ] j=1 where δ[X] is 1 if X is true, otherwise 0; and"
W04-3225,J93-2003,0,0.00945756,"ation we performed on the target language model. In section 4, we present the different types of adaptations we performed on the translation model. Section 5 then puts the results in the context of our IMT application. Section 6 discusses the implications of our experiments and suggests some improvements that could be made to the system. 2 Current IMT models The word-based translation model embedded within the IMT system has been designed by Foster (2000). It is a Maximum Entropy/Minimum Divergence (MEMD) translation model (Berger et al., 1996), which mimics the parameters of the IBM model 2 (Brown et al., 1993) within a log-linear setting. The resulting model (named MDI2B) is of the following form, where h is the current target text, s the source sentence being translated, s a particular word in s and w the next word to be predicted: P q(w|h) exp( s∈s αsw + βAB ) p(w|h, s) = (1) Z(h, s) The q distribution represents the prior knowledge that we have about the true distribution and is modeled by an interpolated trigram in this study. The α coefficients are the familiar transfer or lexical parameters, and the β ones can be understood as their position dependent correction. Z is a normalizing factor, th"
W04-3225,W02-1020,1,0.903063,"Missing"
W04-3225,W03-0302,0,0.0102562,"ith documents such as the sniper corpus, we believe that this could be a key improvement for a dynamic adaptive model. Better alignment As mentioned before, the ultimate goal for our cache is that it contains only the pairs present in the perfect alignment. Better performance from the alignment would lead to pairs in the cache closer to this ideal. In this study we computed Viterbi alignments from an IBM model 2, because it is very efficient to compute and also because for training MDI2B, we do use the IBM model 2. We could consider also more advanced word alignment models (Och and Ney, 2000; Lin and Cherry, 2003; Moore, 2001). To keep the alignment model simple, we could still use an IBM model 2, but with the compositionality constraint that has been shown to give better word alignment than the Viterbi one (Simard and Langlais, 2003). Feature weights We implemented two versions of our model: one with only one feature weight and another with one feature weight for each word pair. The second model suffered from poor data representation and our training algorithm wasn’t able to estimate good cache feature weights. We think that creating classes of word pairs, such as it was done for positional alignment"
W04-3225,W03-0301,0,0.0457849,"Missing"
W04-3225,W01-1411,0,0.0113039,"the sniper corpus, we believe that this could be a key improvement for a dynamic adaptive model. Better alignment As mentioned before, the ultimate goal for our cache is that it contains only the pairs present in the perfect alignment. Better performance from the alignment would lead to pairs in the cache closer to this ideal. In this study we computed Viterbi alignments from an IBM model 2, because it is very efficient to compute and also because for training MDI2B, we do use the IBM model 2. We could consider also more advanced word alignment models (Och and Ney, 2000; Lin and Cherry, 2003; Moore, 2001). To keep the alignment model simple, we could still use an IBM model 2, but with the compositionality constraint that has been shown to give better word alignment than the Viterbi one (Simard and Langlais, 2003). Feature weights We implemented two versions of our model: one with only one feature weight and another with one feature weight for each word pair. The second model suffered from poor data representation and our training algorithm wasn’t able to estimate good cache feature weights. We think that creating classes of word pairs, such as it was done for positional alignment features, wou"
W04-3225,P00-1056,0,0.0456813,"ition. Especially with documents such as the sniper corpus, we believe that this could be a key improvement for a dynamic adaptive model. Better alignment As mentioned before, the ultimate goal for our cache is that it contains only the pairs present in the perfect alignment. Better performance from the alignment would lead to pairs in the cache closer to this ideal. In this study we computed Viterbi alignments from an IBM model 2, because it is very efficient to compute and also because for training MDI2B, we do use the IBM model 2. We could consider also more advanced word alignment models (Och and Ney, 2000; Lin and Cherry, 2003; Moore, 2001). To keep the alignment model simple, we could still use an IBM model 2, but with the compositionality constraint that has been shown to give better word alignment than the Viterbi one (Simard and Langlais, 2003). Feature weights We implemented two versions of our model: one with only one feature weight and another with one feature weight for each word pair. The second model suffered from poor data representation and our training algorithm wasn’t able to estimate good cache feature weights. We think that creating classes of word pairs, such as it was done fo"
W04-3225,E03-1032,0,0.0244729,"Missing"
W04-3225,W03-0304,1,0.806705,"irs present in the perfect alignment. Better performance from the alignment would lead to pairs in the cache closer to this ideal. In this study we computed Viterbi alignments from an IBM model 2, because it is very efficient to compute and also because for training MDI2B, we do use the IBM model 2. We could consider also more advanced word alignment models (Och and Ney, 2000; Lin and Cherry, 2003; Moore, 2001). To keep the alignment model simple, we could still use an IBM model 2, but with the compositionality constraint that has been shown to give better word alignment than the Viterbi one (Simard and Langlais, 2003). Feature weights We implemented two versions of our model: one with only one feature weight and another with one feature weight for each word pair. The second model suffered from poor data representation and our training algorithm wasn’t able to estimate good cache feature weights. We think that creating classes of word pairs, such as it was done for positional alignment features, would lead to better results. It would enable the model to take into account the tendency that a pair has to repeat itself in a document. Relative weighting Another key improvement is that changes to word-pair weigh"
W04-3225,P00-1006,1,\N,Missing
W05-0822,P00-1056,0,0.225418,"Missing"
W05-0822,2003.mtsummit-papers.15,1,0.86682,"s generated by rules; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. (A fourth postprocessing phase was not needed for the shared task.) http://www.statmt.org/wpt05/mt-shared-task/ 129 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129–132, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 2.1 Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003). For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ’ an into l’ an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore’s algorithm (Moore, 2002), segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of"
W05-0822,P02-1040,0,0.0814507,"lies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. 130 To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered during the iterations of Och’s algorithm (FrenchEnglish), and a grid search (all other languages). To perform the actual translation, we used our decoder, Canoe, which implements a dynamicprogramming beam search algorithm based on that of Pharaoh (Koehn, 2004). Canoe is input-output compatible with Pharaoh, with the except"
W05-0822,moore-2002-fast,0,0.0342615,"s, 2005 2.1 Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003). For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ’ an into l’ an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore’s algorithm (Moore, 2002), segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of a word to efficiently train a translation system. In a language like German, new words can be formed by compounding (writing two or more words together without a space or a hyphen in between). Segmentation is a crucial step in preprocessing languages such as German and Finnish texts. In addition to these simple operations, we also developed a rule-based component to detect numbers and dates in t"
W05-0822,P02-1038,0,0.179126,"Missing"
W05-0822,P03-1021,0,0.0391459,"ain components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002). The phrase-based translation model is similar to the one described in (Koehn, 2004), and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. 130 To set weights on the components of the loglinear model, we implemented Och’s algorithm (Och, 2003). This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell’s algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered dur"
W05-0822,koen-2004-pharaoh,0,\N,Missing
W06-1607,J93-2003,0,0.0496053,"that the phrases s˜k specified by a are conditionally independent, and depend only on their aligned phrases t˜k . The “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 2 Phrase-based Statistical MT Given a source sentence s, our phrase-based SMT system tries to find the target sentence ˆt that is the most likely translation of s. To make search more efficient, we use the Viterbi approximation and seek the most likely combination of t and its alignment a with s, rather than just the most likely t: 3 Smoothing Techniques Smoothing involves some recipe for modifying conditional distributions away from pure relativefrequency estimates made from joint counts, in order to compensate for data sparsity. In the spirit of ((Hastie et al., 2001), figure 2.11, pg. 38)"
W06-1607,2005.mtsummit-posters.19,0,0.0161648,"zero probabilities during phrase induction is reported in (Marcu and Wong, 2002), but no details are given. As described above, (Zens and i∈Aj where Aj is a set of likely alignment connections for sj . In our implementation of this method, ˜ ie the set of we assumed that Aj = {1, . . . , I}, all connections, and used IBM1 probabilities for p(s|t). 57 Ney, 2004) and (Koehn et al., 2005) use two different variants of glass-box smoothing (which they call “lexical smoothing”) over the phrasetable, and combine the resulting estimates with pure relativefrequency ones in a loglinear model. Finally, (Cettollo et al., 2005) describes the use of Witten-Bell smoothing (a black-box technique) for phrasetable counts, but does not give a comparison to other methods. As Witten-Bell is reported by (Chen and Goodman, 1998) to be significantly worse than Kneser-Ney smoothing, we have not yet tested this method. sets for tuning loglinear parameters, and tested on the 3064-sentence test sets. Results are shown in table 1 for relativefrequency (RF), Good-Turing (GT), Kneser-Ney with 1 (KN1) and 3 (KN3) discount coefficients; and loglinear combinations of both RF and KN3 phrasetables with Zens-Ney-IBM1 (ZN-IBM1) smoothed phr"
W06-1607,W95-0103,0,0.0108567,"proach assumes that all source words are conditionally independent, so that: p(˜ s|t˜) = J˜ Y where: c∗i (˜ s, t˜) = X c(˜ s, t1 . . . ti . . . tI˜). ti One might also consider progressively replacing the least informative remaining word in the target phrase (using tf-idf or a similar measure). The same idea could be applied in reverse, by replacing particular source (conditioned) words with wildcards. We have not yet implemented this new glass-box smoothing technique, but it has considerable appeal. The idea is similar in spirit to Collins’ backoff method for prepositional phrase attachment (Collins and Brooks, 1995). p(sj |t˜) j=1 We implemented two variants for p(sj |t˜) that are described in previous work. (Zens and Ney, 2004) describe a “noisy-or” combination: p(sj |t˜) = 1 − p(¯ sj |t˜) ≈ 1− c∗ (˜ s, t˜) Pi ∗ /I˜ ˜ c (˜ s , t ) s˜ i ˜ X I˜ Y (1 − p(sj |ti )) i=1 4 Related Work where s¯j is the probability that sj is not in the translation of t˜, and p(sj |ti ) is a lexical probability. (Zens and Ney, 2004) obtain p(sj |ti ) from smoothed relative-frequency estimates in a wordaligned corpus. Our implementation simply uses IBM1 probabilities, which obviate further smoothing. The noisy-or combination st"
W06-1607,P98-2158,0,0.0668697,"Missing"
W06-1607,N03-1017,0,0.192975,"techniques in ngram LM smoothing is to combine estimates made using the previous n − 1 words with those using only the previous n−i words, for i = 2 . . . n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing. where each fi (s, t, a) is a feature function, and weights λi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model probabilities are features of the form: • The predicted objects are word sequences (in another language). This contrasts to LM smoothing where they are single words, and are thus less amenable to decomposition for smoothing purposes. log p(s|t, a) ≈ We propose various ways of dealing with these special features of the phrasetable smoothing problem, and give evaluations of their performance within a"
W06-1607,W05-0824,0,0.0192822,"Missing"
W06-1607,W02-1018,0,0.0414401,"r statistical MT. For the IBM models, alignment probabilities need to be smoothed for combinations of sentence lengths and positions not encountered in training data (Garc´ıa-Varea et al., 1998). Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. Langlais (2005) reports negative results for synonym-based smoothing of IBM2 lexical probabilities prior to extracting phrases for phrasebased SMT. For phrase-based SMT, the use of smoothing to avoid zero probabilities during phrase induction is reported in (Marcu and Wong, 2002), but no details are given. As described above, (Zens and i∈Aj where Aj is a set of likely alignment connections for sj . In our implementation of this method, ˜ ie the set of we assumed that Aj = {1, . . . , I}, all connections, and used IBM1 probabilities for p(s|t). 57 Ney, 2004) and (Koehn et al., 2005) use two different variants of glass-box smoothing (which they call “lexical smoothing”) over the phrasetable, and combine the resulting estimates with pure relativefrequency ones in a loglinear model. Finally, (Cettollo et al., 2005) describes the use of Witten-Bell smoothing (a black-box t"
W06-1607,P03-1021,0,0.228399,"mass is subtracted from the seen translations. To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp λi fi (s, t, a) i • There is no obvious lower-order distribution for backoff. One of the most important techniques in ngram LM smoothing is to combine estimates made using the previous n − 1 words with those using only the previous n−i words, for i = 2 . . . n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing. where each fi (s, t, a) is a feature function, and weights λi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model probabilities are features of the form: • The predicted objects are word sequences (in another language). This contrasts to LM smoothing where they are single wo"
W06-1607,2001.mtsummit-papers.68,0,0.0338788,"To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp λi fi (s, t, a) i • There is no obvious lower-order distribution for backoff. One of the most important techniques in ngram LM smoothing is to combine estimates made using the previous n − 1 words with those using only the previous n−i words, for i = 2 . . . n. This relies on the fact that closer words are more informative, which has no direct analog in phrasetable smoothing. where each fi (s, t, a) is a feature function, and weights λi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model probabilities are features of the form: • The predicted objects are word sequences (in another language). This contrasts to LM smoothing where they are single words, and are thus less amenable to decomposition for smoothi"
W06-1607,N04-1033,0,0.587067,"smoothing distribution to modify p˜(˜ s|t˜) for higher-frequency events, whereas the latter uses it only for low-frequency events (most often 0frequency events). Since for phrasetable smoothing, better prediction of unseen (zero-count) events has no direct impact—only seen events are represented in the phrasetable, and thus hypothesized during decoding—interpolation seemed a more suitable approach. For combining relative-frequency estimates with glass-box smoothing distributions, we employed loglinear interpolation. This is the traditional approach for glass-box smoothing (Koehn et al., 2003; Zens and Ney, 2004). To illustrate the difference between linear and loglinear interpolation, consider combining two Bernoulli distributions p1 (x) and p2 (x) using each method: plinear (x) = αp1 (x) + (1 − α)p2 (x) ploglin (x) = p1 (x)α p2 (x) p1 (x)α p2 (x) + q1 (x)α q2 (x) where qi (x) = 1 − pi (x). Setting p2 (x) = 0.5 to simulate uniform smoothing gives ploglin (x) = p1 (x)α /(p1 (x)α + q1 (x)α ). This is actually less smooth than the original distribution p1 (x): it preserves extreme values 0 and 1, and makes intermediate values more extreme. On the other hand, plinear (x) = αp1 (x) + (1 − α)/2, which has"
W06-1607,P02-1040,0,\N,Missing
W06-1607,P04-1066,0,\N,Missing
W06-3118,N06-1004,1,0.829818,"s c(s, t) where D = n1 /(n1 + 2n2 ), n1+ (∗, t) is the number of distinct phrases s with which t co-occurs, and P pk (s) = n1+ (s, ∗)/ s n1+ (s, ∗), with n1+ (s, ∗) analogous to n1+ (∗, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SCMs differ from the conventional penalty-based distortion, which always favours less rather than more distortion. We developed a particular kind of SCM based on decision trees (DTs) containing both questions of a positional type (e.g., questions about the distance of a given phrase from the beginning of the source sentence or from the previ"
W06-3118,W05-0822,1,0.831685,"le data set and explore the benefits of a number of recently added features. Section 2 describes the changes that have been made to Portage in the past year that affect the participation in the 2006 shared task. Section 3 outlines the methods employed for this task and extensions of it. In Section 4 the results are summarized in tabular form. Following these, there is a conclusions section that highlights what can be gleaned of value from these results. 2 Portage Because this is the second participation of Portage in such a shared task, a description of the base system can be found elsewhere (Sadat et al, 2005). Briefly, Portage is a research vehicle and development prototype system exploiting the state-of-the-art in statistical machine translation (SMT). It uses a custom Phrase-Table Smoothing Phrase-based SMT relies on conditional distributions p(s|t) and p(t|s) that are derived from the joint frequencies c(s, t) of source/target phrase pairs observed in an aligned parallel corpus. Traditionally, relative-frequency estimation is used to derive conP ditional distributions, ie p(s|t) = c(s, t)/ s c(s, t). However, relative-frequency estimation has the well-known problem of favouring rare events. For"
W06-3118,W05-0800,0,0.0869259,"Missing"
W06-3118,2006.jeptalnrecital-poster.23,1,0.726586,"n to the training resources used in WPT 2005 for the French-English task, i.e. Europarl and Hansard, we used a bilingual dictionary, Le Grand Dictionnaire Terminologique (GDT) 2 to train translation models and the English side of the UN parallel corpus (LDC2004E13) to train an English language model. Integrating terminological lexicons into a statistical machine translation engine is not a straightforward operation, since we cannot expect them to come with attached probabilities. The approach we took consists on viewing all translation candidates of each source term or phrase as equiprobable (Sadat et al, 2006). In total, the data used in this second part of our contribution to WMT 2006 is described as follows: (1) A set of 688,031 sentences in French and English extracted from the Europarl parallel corpus (2) A set of 6,056,014 sentences in French and English extracted from the Hansard parallel corpus, the official record of Canada’s parliamentary debates. (3) A set of 701,709 sentences in French and English extracted from the bilingual dictionary GDT. (4) Language models were trained on the French and English parts of the Europarl and Hansard. We used the provided Europarl corpus while omitting da"
W06-3118,N04-1033,0,0.0248659,"s phrase. The resulting estimates are: cg (s, t) , s cg (s, t) + p(t)n1 pg (s|t) = P P where p(t) = c(t)/ t c(t). The estimates for pg (t|s) are analogous. The second strategy is Kneser-Ney smoothing (Kneser and Ney, 1995), using the interpolated variant described in (Chen and Goodman., 1998):1 pk (s|t) = c(s, t) − D + D n1+ (∗, t) pk (s) P s c(s, t) where D = n1 /(n1 + 2n2 ), n1+ (∗, t) is the number of distinct phrases s with which t co-occurs, and P pk (s) = n1+ (s, ∗)/ s n1+ (s, ∗), with n1+ (s, ∗) analogous to n1+ (∗, t). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. We believe the two approaches are complementary, so a combination of both would be worth exploring in future work. 2.2 Feature-Rich DT-based distortion In a recent paper (Kuhn et al, 2006), we presented a new class of probabilistic ”Segment Choice Models” (SCMs) for distortion in phrase-based systems. In some situations, SCMs will assign a better distortion score to a drastic reordering of the source sentence than to no reordering; in this, SC"
W07-0703,J93-2003,0,0.00932986,"Missing"
W07-0703,P06-1010,0,0.044303,"ts a correct transliteration that the decoder outputs correctly, but which fails to receive credit from the BLEU metric because this transliteration is not found in the references. As an example, for the name “”ﺳﻮﯾﺮﯾﻮس, four references came up with four different interpretations: swerios, swiriyus, severius, sweires. A quick query in Google showed us another four acceptable interpretations (severios, sewerios, sweirios, sawerios). Machine transliteration has been an active research field for quite a while (Al-Onaizan and Knight, 2002; AbdulJaleel and Larkey, 2003; Klementiev and Roth, 2006; Sproat et al, 2006) but to our knowledge there is little published work on evaluating transliteration within a real MT system. The closest work to ours is described in (Hassan and Sorensen, 2005) where they have a list of names in Arabic and feed this list as the input text to their MT system. They evaluate their system in three different cases: as a word-based NE translation, phrase-based NE translation and in presence of a transliteration module. Then, they report the BLEU score on the final output. Since their text is comprised of only NEs, the BLEU increase is quite high. Combining all three models, they get"
W07-0703,N06-1011,0,\N,Missing
W07-0703,W02-0505,0,\N,Missing
W07-0703,P02-1040,0,\N,Missing
W07-0703,W05-0712,0,\N,Missing
W07-0703,N03-1017,0,\N,Missing
W07-0703,D08-1076,0,\N,Missing
W07-0717,J93-2003,0,0.0101062,"p(˜ s|t˜): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Mixture-Model Adaptation 2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system (Koehn et al., 2003). Given a source sentence s, this tries to find the target sentence ˆt that is the most likely translation of s, using the Viterbi approximation: 1. Split the corpus into different components, according to some criterion. 2. Train a model on each corpus component. ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following general algorithm: t,a where alignment a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK );"
W07-0717,P99-1022,0,0.0257994,"slight advantage. granularity baseline file genre document dev nist04mix 31.9 32.4 32.5 32.9 nist05 30.4 30.8 31.1 30.9 test nist06nist 27.6 28.6 28.9 28.6 nist06gale 12.9 13.4 13.2 13.4 Table 8: The effects of source granularity on dynamic adaptation. 5 Related Work Mixture modeling is a standard technique in machine learning (Hastie et al., 2001). It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990). Most previous work on adaptive SMT focuses on the use of IR techniques to identify a relevant subset of the training corpus from which an adapted model can be learned. Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments. Hildebrand et al (1995) describe a similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation. They rely on a perplexit"
W07-0717,W06-1607,1,0.748963,"and loglinear mixing frameworks, with uniform weights used in the linear mixture. Both types of mixture model are better than the baseline, but the linear mixture is slightly better than the loglinear mixture. This is quite surprising, because these results are on the development set: the loglinear model tunes its component weights on this set, whereas the linear model only adjusts global LM and TM weights. We speculated that this may have been due to non-smooth component models, and tried various 132 smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components. None helped, however, and we conclude that the problem is most likely that Och’s algorithm is unable to find a good maximimum in this setting. Due to this result, all experiments we describe below involve linear mixtures only. combination baseline loglinear mixture uniform linear mixture adapted model LM TM LM+TM 30.2 30.2 30.2 30.9 31.2 31.4 31.2 31.1 31.8 Table 2: Linear versus loglinear combinations on NIST04-nw. 4.2 Distance Metrics for Weighting Table 3 compares the performance of all distance metrics descr"
W07-0717,P03-1021,0,0.053334,"model on each corpus component. ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following general algorithm: t,a where alignment a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK ); ˜ ˜ tk are target phrases such that t = t1 . . . t˜K ; s˜k are source phrases such that s = s˜j1 . . . s˜jK ; and s˜k is the translation of the kth target phrase t˜k . To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp αi fi (s, t, a) (1) i where each fi (s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and 129 3. Weight each model according to its fit with the test domain: • For cross-domain adaptation, set parameters using a development corpus drawn from the test domain, and use for all future documents. • For dynamic adaptation, set global parameters using a development corpus drawn from several different domain"
W07-0717,2001.mtsummit-papers.68,0,0.0190075,"s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following general algorithm: t,a where alignment a = (˜ s1 , t˜1 , j1 ), ..., (˜ sK , t˜K , jK ); ˜ ˜ tk are target phrases such that t = t1 . . . t˜K ; s˜k are source phrases such that s = s˜j1 . . . s˜jK ; and s˜k is the translation of the kth target phrase t˜k . To model p(t, a|s), we use a standard loglinear approach: "" # X p(t, a|s) ∝ exp αi fi (s, t, a) (1) i where each fi (s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and 129 3. Weight each model according to its fit with the test domain: • For cross-domain adaptation, set parameters using a development corpus drawn from the test domain, and use for all future documents. • For dynamic adaptation, set global parameters using a development corpus drawn from several different domains. Set mixture weights as a function of the distances from c"
W07-0717,P06-1091,0,0.00665594,"chnique of mixture modeling (Hastie et al., 2001). This involves dividing the training corpus into different components, training a model on each part, then weighting each model appropriately for the current context. Mixture modeling is a simple framework that encompasses many different variants, as described below. It is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases. This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (Tillmann and Zhang, 2006; Liang et al., 2006). Techniques for assigning mixture weights depend on the setting. In cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly. In dynamic adaptation, training poses a problem because no reference text is available. Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample). We do not learn mixture weights directly with this method, because there is little hope 128 Proceedings of th"
W07-0717,N04-1033,0,0.0400093,"r is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes. 4-gram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit. Phrase translation model probabilities PK are features of the form: log p(s|t, a) ≈ sk |t˜k ). k=1 log p(˜ We use two different estimates for the conditional probabilities p(t˜|˜ s) and p(˜ s|t˜): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Mixture-Model Adaptation 2 Phrase-based Statistical MT Our baseline is a s"
W07-0717,2006.iwslt-evaluation.12,0,0.025729,"language corpus. This approach has the advantage of not limiting LM adaptation to a parallel corpus, but the disadvantage of requiring two translation passes (one to generate the nbest lists, and another to translate with the adapted model). Ueffing (2006) describes a self-training approach that also uses a two-pass algorithm. A baseline system generates translations that, after confidence filtering, are used to construct a parallel corpus based on the test set. Standard phrase-extraction tech134 niques are then applied to extract an adapted phrase table from the system’s own output. Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters. Each source sentence is then decoded using the language model trained on the cluster that assigns highest likelihood to that sentence. The work we present here is complementary to both the IR approaches and Ueffing’s method because it provides a way of exploiting a preestablished corpus division. This has the potential to allow sentences having little surface similarity to the current source text to contribute"
W07-0717,C04-1059,0,0.84709,"Missing"
W07-0717,N03-1017,0,0.110712,"p(˜ We use two different estimates for the conditional probabilities p(t˜|˜ s) and p(˜ s|t˜): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004). In both cases, the “forward” phrase probabilities p(t˜|˜ s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s˜ that matches some ngram in s, only the 30 top-ranked translations t˜ according to p(t˜|˜ s) are retained. To derive the joint counts c(˜ s, t˜) from which ˜ ˜ p(˜ s|t) and p(t|˜ s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Mixture-Model Adaptation 2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system (Koehn et al., 2003). Given a source sentence s, this tries to find the target sentence ˆt that is the most likely translation of s, using the Viterbi approximation: 1. Split the corpus into different components, according to some criterion. 2. Train a model on each corpus component. ˆt = argmax p(t|s) ≈ argmax p(t, a|s), t Our approach to mixture-model adaptation can be summarized by the following gener"
W07-0717,P06-1096,0,0.0772821,"Missing"
W07-0717,P02-1040,0,\N,Missing
W07-0717,D08-1076,0,\N,Missing
W07-0717,2005.eamt-1.19,0,\N,Missing
W09-0439,N04-1033,0,0.0174667,"roach by between 0.6 and 2.5 BLEU points. The convergence behaviour of the lattice variant was also much smoother than that of the n-best variant. It would be interesting to apply some of the insights of the current paper to the lattice variant of Och’s procedure. Previous Work One possible approach to estimating log-linear weights on features is to dispense with the n-best lists employed by Och’s procedure and, instead, to optimize weights by directly accessing the decoder. The disadvantage of this approach is that far more iterations of decoding of the full development set are required. In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. However, each iteration is unusually fast, because only monotone decoding is permitted (i.e., the order of phrases in the target language mirrors that in the source language). Similarly, Cettolo and Federico (2004) apply the simplex method to optimize weights directly using the decoder. In their experiments on NIST 2003 Chinese-English data, they found about 100 iterations of decoding were required. Although they obtained consistent and stable performance gains for MT, the"
W09-0439,W08-0304,0,0.698034,"ve operation. Moreover, because this function is not differentiable, efficient gradient-based optimization algorithms cannot be used. Och’s procedure is the most widely-used version of MERT for SMT (Och, 2003). To reduce Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242–249, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 242 the test set. We analyze the causes of each of these problems, and propose solutions for improving the stability of the overall procedure. 2 pruning the set of n-best hypotheses at each iteration. Cer et al (2008) also aim at improving Och’s MERT. They focus on the search for the best set of weights for an n-best list that follows choice of a starting point. They propose a modified version of Powell’s in which “diagonal” directions are chosen at random. They also modify the objective function used by Powell’s to reflect the width of the optima found. They are able to show that their modified version of MERT outperforms both a version using Powell’s, and a more heuristic search algorithm devised by Philipp Koehn that they call Koehn Coordinate Descent, as measured on the development set and two test dat"
W09-0439,D07-1055,0,0.154387,"Missing"
W09-0439,2004.iwslt-papers.2,0,0.037137,"ghts on features is to dispense with the n-best lists employed by Och’s procedure and, instead, to optimize weights by directly accessing the decoder. The disadvantage of this approach is that far more iterations of decoding of the full development set are required. In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. However, each iteration is unusually fast, because only monotone decoding is permitted (i.e., the order of phrases in the target language mirrors that in the source language). Similarly, Cettolo and Federico (2004) apply the simplex method to optimize weights directly using the decoder. In their experiments on NIST 2003 Chinese-English data, they found about 100 iterations of decoding were required. Although they obtained consistent and stable performance gains for MT, these were inferior to the gains yielded by Och’s procedure in (Och, 2003). Taking Och’s MERT procedure as a baseline, (Zens et al., 2007) experiment with different training criteria for SMT and obtain the best results for a criterion they call “expected BLEU score”. Moore and Quirk (2008) share the goal underlying our own research: impro"
W09-0439,P05-1033,0,0.00472174,"ve to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och’s procedure by supplying different random seeds to a core component of the procedure (Powell’s algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data. 1 Introduction Most recent approaches in SMT, eg (Koehn et al., 2003; Chiang, 2005), use a log-linear model to combine probabilistic features. Minimum ErrorRate Training (MERT) aims to find the set of loglinear weights that yields the best translation performance on a development corpus according to some metric such as BLEU. This is an essential step in SMT training that can significantly improve performance on a test corpus compared to setting weights by hand. MERT is a difficult problem, however, because calculating BLEU as a function of log-linear weights requires decoding, which is an expensive operation. Moreover, because this function is not differentiable, efficient g"
W09-0439,P08-2010,0,0.061885,"at improving Och’s MERT. They focus on the search for the best set of weights for an n-best list that follows choice of a starting point. They propose a modified version of Powell’s in which “diagonal” directions are chosen at random. They also modify the objective function used by Powell’s to reflect the width of the optima found. They are able to show that their modified version of MERT outperforms both a version using Powell’s, and a more heuristic search algorithm devised by Philipp Koehn that they call Koehn Coordinate Descent, as measured on the development set and two test data sets. (Duh and Kirchhoff, 2008) ingeniously uses MERT as a weak learner in a boosting algorithm that is applied to the n-best reranking task, with good results (a gain of about 0.8 BLEU on the test set). Recently, some interesting work has been done on what might be considered a generalization of Och’s procedure (Macherey et al., 2008). In this generalization, candidate hypotheses in each iteration of the procedure are represented as lattices, rather than as n-best lists. This makes it possible for a far greater proportion of the search space to be represented: a graph density of 40 arcs per phrase was used, which correspon"
W09-0439,P07-1019,0,0.0113459,"this does not guarantee that Powell’s algorithm will find a global maximum, and so Powell’s is typically run with many different randomly-chosen initial weights in order to try to find a good maximum. 4 num sents 1506 2080 1788 1664 num Chinese toks 38,312 55,159 53,446 41,798 Table 1: Development and test corpora. et al., 2003) employing a log-linear combination of feature functions. HMM and IBM2 models were used to perform separate word alignments, which were symmetrized by the usual “diag-and” algorithm prior to phrase extraction. Decoding used beam search with the cube pruning algorithm (Huang and Chiang, 2007). We used two separate log-linear models for MERT: • large: 16 phrase-table features, 2 4-gram language model features, 1 distortion feature, and 1 word-count feature (20 features in total). • small: 2 phrase-table features, 1 4-gram language model feature, 1 distortion feature, and 1 word-count feature (5 features in total). The phrase-table features for the large model were derived as follows. Globally-trained HMM and IBM2 models were each used to extract phrases from UN and non-UN portions of the training corpora (see below). This produced four separate phrase tables, each of which was used"
W09-0439,N03-1017,0,0.00262285,"tends to be sensitive to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Och’s procedure by supplying different random seeds to a core component of the procedure (Powell’s algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data. 1 Introduction Most recent approaches in SMT, eg (Koehn et al., 2003; Chiang, 2005), use a log-linear model to combine probabilistic features. Minimum ErrorRate Training (MERT) aims to find the set of loglinear weights that yields the best translation performance on a development corpus according to some metric such as BLEU. This is an essential step in SMT training that can significantly improve performance on a test corpus compared to setting weights by hand. MERT is a difficult problem, however, because calculating BLEU as a function of log-linear weights requires decoding, which is an expensive operation. Moreover, because this function is not differentiab"
W09-0439,D08-1076,0,0.0216455,"width of the optima found. They are able to show that their modified version of MERT outperforms both a version using Powell’s, and a more heuristic search algorithm devised by Philipp Koehn that they call Koehn Coordinate Descent, as measured on the development set and two test data sets. (Duh and Kirchhoff, 2008) ingeniously uses MERT as a weak learner in a boosting algorithm that is applied to the n-best reranking task, with good results (a gain of about 0.8 BLEU on the test set). Recently, some interesting work has been done on what might be considered a generalization of Och’s procedure (Macherey et al., 2008). In this generalization, candidate hypotheses in each iteration of the procedure are represented as lattices, rather than as n-best lists. This makes it possible for a far greater proportion of the search space to be represented: a graph density of 40 arcs per phrase was used, which corresponds to an n-best size of more than two octillion (2 ∗ 1027 ) entries. Experimental results for three NIST 2008 tasks were very encouraging: though BLEU scores for the lattice variant of Och’s procedure did not typically exceed those for the n-best variant on development data, on test data the lattice varia"
W09-0439,C08-1074,0,0.586396,"ors that in the source language). Similarly, Cettolo and Federico (2004) apply the simplex method to optimize weights directly using the decoder. In their experiments on NIST 2003 Chinese-English data, they found about 100 iterations of decoding were required. Although they obtained consistent and stable performance gains for MT, these were inferior to the gains yielded by Och’s procedure in (Och, 2003). Taking Och’s MERT procedure as a baseline, (Zens et al., 2007) experiment with different training criteria for SMT and obtain the best results for a criterion they call “expected BLEU score”. Moore and Quirk (2008) share the goal underlying our own research: improving, rather than replacing, Och’s MERT procedure. They focus on the step in the procedure where the set of feature weights optimizing BLEU (or some other MT metric) for an n-best list is estimated. Typically, several different starting points are tried for this set of weights; often, one of the starting points is the best set of weights found for the previous set of n-best hypotheses. The other starting points are often chosen randomly. In this paper, Moore and Quirk look at the best way of generating the random starting points; they find that"
W09-0439,P03-1021,0,0.167088,"oglinear weights that yields the best translation performance on a development corpus according to some metric such as BLEU. This is an essential step in SMT training that can significantly improve performance on a test corpus compared to setting weights by hand. MERT is a difficult problem, however, because calculating BLEU as a function of log-linear weights requires decoding, which is an expensive operation. Moreover, because this function is not differentiable, efficient gradient-based optimization algorithms cannot be used. Och’s procedure is the most widely-used version of MERT for SMT (Och, 2003). To reduce Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 242–249, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 242 the test set. We analyze the causes of each of these problems, and propose solutions for improving the stability of the overall procedure. 2 pruning the set of n-best hypotheses at each iteration. Cer et al (2008) also aim at improving Och’s MERT. They focus on the search for the best set of weights for an n-best list that follows choice of a starting point. They propose a modified version of Powell’s i"
W10-1702,P07-2045,0,0.00952282,"sh. Chinese-to-English tasks are based on training data for the NIST 1 2009 evaluation Chinese-toEnglish track. All the allowed bilingual corpora have been used for estimating the translation model. We trained two language models: the first one is a 5-gram LM which is estimated on the target side of the parallel data. The second is a 5- 1 1,506 1,664 1,357 - Table 1: Statistics of training, dev, and test sets for Chinese-to-English task. We carried out experiments based on translation N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 |S| |W| |S| |S| |S| |S| Eng 10.1M 270.0M 279.1M 2 http://www.nist.gov/speech/tests/mt 14 http://www.st"
W10-1702,N04-1022,0,0.317546,"ments across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 P"
W10-1702,D08-1076,0,0.0202448,"N-best lists generated by a state-of-the-art phrase-based statistical machine translation system, similar to (Koehn et al., 2007). In detail, the phrase table is derived from merged counts of symmetrized IBM2 and HMM alignments; the system has both lexicalized and distance-based distortion components (there is a 7-word distortion limit) and employs cube pruning (Huang and Chiang, 2007). The baseline is a log-linear feature combination that includes language models, the distortion components, translation model, phrase and word penalties. Weights on feature functions are found by lattice MERT (Macherey et al., 2008). 3.1 |S| |W| |S| |S| |S| |S| Eng 10.1M 270.0M 279.1M 2 http://www.nist.gov/speech/tests/mt 14 http://www.statmt.org/wmt06/ 20 additional features. In this experiment, the former is about 10 times faster than the latter in terms of processing time, as shown in Table 3. testset NIST’06 NIST’08 baseline 35.70 28.60 rescoring 36.01 28.97 three-pass 35.98 28.99 FCD 36.00 29.10 Fwd. 36.13 29.19 Bwd. 36.11 29.20 Bid. 36.20 29.28 In our second experiment, we set the size of N-best list N equal to 10,000 for both Chinese-toEnglish and German-to-English tasks. The results are reported in Table 4. The s"
W10-1702,P03-1021,0,0.0144611,"Missing"
W10-1702,N04-1021,0,0.030639,"performance. Experimental results show consistent improvements across language pairs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Theref"
W10-1702,P02-1040,0,0.101853,"Missing"
W10-1702,D08-1065,0,0.239694,"of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 Proceedings of the Joint 5th Workshop on Statist"
W10-1702,P08-1025,0,0.021642,"airs, and an improvement of up to 0.72 BLEU is obtained over a competitive single-pass baseline on the Chinese-toEnglish NIST task. 1 Introduction State-of-the-art statistical machine translation (SMT) systems are often described as a two-pass process. In the first pass, decoding algorithms are applied to generate either a translation N-best list or a translation forest. Then in the second pass, various re-ranking algorithms are adopted to compute the final translation. The re-ranking algorithms include rescoring (Och et al., 2004) and Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004; Zhang and Gildea, 2008; Tromble et al., 2008). Rescoring uses more sophisticated additional feature functions to score the hypotheses. MBR decoding directly incorporates the evaluation metrics (i.e., loss function), into the decision criterion, so it is effective in tuning the MT performance for a specific loss function. In particular, sentence-level BLEU loss function gives gains on BLEU (Kumar and Byrne, 2004). The naïve MBR algorithm computes the loss function between every pair of k hypotheses, needing O(k2) comparisons. Therefore, only small number k is applicable. Very recently, De11 Proceedings of the Joint"
W10-1702,2007.mtsummit-papers.15,1,0.906463,"am n-gram one week's expansion new partial hyp. one week's work about one week's work new about week's work hypotheses one weeks' work . one week's work . one week's work . Fast Consensus Hypothesis Regeneration Since the three hypothesis regeneration methods with n-gram expansion, confusion network decoding and re-decoding produce very similar performance (Chen et al., 2008), we consider only n-gram expansion method in this paper. N-gram expansion can (almost) fully exploit the search space of target strings which can be generated by an n-gram language model trained on the N-best hypotheses (Chen et al., 2007). Figure 1: Example of original hypotheses; bi-grams collected from them; backward expanding a partial hypothesis via an overlapped n-1-gram; and new hypotheses generated through backward n-gram expansion. 2.1 2.2 Hypothesis regeneration with bidirectional n-gram expansion Feature-based scoring functions To speed up the search, the partial hypotheses are pruned via beam-search in each expanding step. Therefore, the scoring functions applied with the beam-search algorithm are very important. In (Chen et al., 2008), more than 10 additional global features are computed to rank the partial hypothe"
W10-1702,C08-1014,1,0.891638,"-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7 {Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca Nero et al. (2009) proposed a fast consensus decoding (FCD) algorithm in which the similarity scores are computed based on the feature expectations over the translation N-best list or translation forest. It is equivalent to MBR decoding when using a linear similarity function, such as unigram precision. Re-ranking approaches improve performance on an N-best list whose contents are fixed. A complementary strategy is to augment the contents of an N-best list in order to broaden the search space. Chen et al (2008) have proposed a three-pass SMT process, in which a hypothesis regeneration pass is added between the decoding and rescoring passes. New hypotheses are generated based on the original N-best hypotheses through n-gram expansion, confusion-network decoding or re-decoding. All three hypothesis regeneration methods obtained decent and comparable improvements in conjunction with the same rescoring model. However, since the final translation candidates in this approach are produced from different methods, local feature functions (such as translation models and reordering models) of each hypothesis a"
W10-1702,P09-1064,0,\N,Missing
W10-1702,W07-0724,0,\N,Missing
W10-1702,P07-1019,0,\N,Missing
W10-1717,W08-0304,0,0.0612136,"Missing"
W10-1717,W09-0439,1,0.800854,"alf of GigaFrEn; 7. Dynamic LM composed of 4 LMs, each trained on the French half of a parallel corpus (5-gram LM trained on “domain”, 4-gram LM on GigaFrEn, 5-gram LM on news-commentary and 5-gram LM on UN). The F-E system is a mirror image of the E-F system. 3 Details of lattice MERT (LMERT) Our system’s implementation of LMERT (Macherey et al., 2008) is the most notable recent change in our system. As more and more features are included in the loglinear model, especially if they are correlated, N-best MERT (Och, 2003) shows more and more instability, because of convergence to local optima (Foster and Kuhn, 2009). We had been looking for methods that promise more stability and better convergence. LMERT seemed to fit the bill. It optimizes over the complete lattice of candidate translations after a decoding run. This avoids some of the problems of N-best lists, which lack variety, leading to poor local optima and the need for many decoder runs. Though the algorithm is straightforward and is highly parallelizable, attention must be paid to space and time resource issues during implementation. Lattices output by our decoder were large and needed to be shrunk dramatically for the algorithm to function wel"
W10-1717,W07-0717,1,0.856161,"kward conditional probabilities. The lexicalized distortion probabilities are also obtained by adding IBM2 and HMM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, cont"
W10-1717,P07-1019,0,0.047578,"MM counts. They involve 6 features (monotone, swap and discontinuous features for following and preceding phrase) and are conditioned on phrase pairs in a model similar to that of Moses (Koehn et al., 2005); a MAP-based backoff smoothing scheme is used to combat data sparseness when estimating these probabilities. Dynamic mixture LMs are linear mixtures of ngram models trained on parallel sub-corpora with weights set to minimize perplexity of the current source text as described in (Foster and Kuhn, 2007); henceforth, we’ll call them “dynamic LMs”. Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. Contrary to the usual implementation of distortion limits, we allow a new phrase to end 127 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127–132, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics more than 7 words past the first non-covered word, as long as the new phrase starts within 7 words from the first non-covered word. Notwithstanding the distortion limit, contiguous phrases can always be swapped. Out-of-vocabulary (OOV) source words are passed through unchanged to the ta"
W10-1717,2005.iwslt-1.8,0,\N,Missing
W10-1717,N04-1033,0,\N,Missing
W10-1717,D08-1076,0,\N,Missing
W10-1717,P03-1021,0,\N,Missing
W12-3104,W08-0309,0,0.0671104,"Missing"
W12-3104,P12-1098,1,0.790951,"0 and p20 = 0 . Let Then we obtain nd +1+1+9 ) Its ρ = 1 - 6(14(16 −1) = −0.2 ; however, its th P1: 1 2 3 4 5 6 (the 2 word “the”, 4 word “of” and 6th word “,” in the reference are not aligned to any word in the hypothesis. Thus, their positions are not in P1, so the positions of the matching words “in winter 2010 I visited Paris” are normalized to 1 2 3 4 5 6) P2: 4 5 6 1 3 2 (the word “’s” was unaligned). 60 v2 = 1 − DIST2 ( P1 , P2 ) n2 −1 (7) As with v1, v2 is also from 0 to 1, and larger values indicate more similar permutations. The ordering measure vs is the harmonic mean of v1 and v2 (Chen et al., 2012): (8) vs = 2 / (1/v1 + 1/v2 ) . In (Chen et al., 2012) we found this to be slightly more effective than the geometric mean. vs in (8) is computed at segment level. We compute document level ordering vD with a weighted arithmetic mean: l vD ∑ v × len ( R) = ∑ len ( R) s =1 s l s =1 s (9) s where l is the number of segments of the document, and len(R) is the length of the reference after text preprocessing. vs is the segment-level ordering penalty. Recall that the penalty part of AMBER is the weighted product of several component penalties. In the original version of AMBER, there were 10 compone"
W12-3104,W11-2105,1,0.899965,"o AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 1 2 Introduction AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011). It is designed to have the advantages of BLEU (Papineni et al., 2002), such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment. According to the paper just cited: “It can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality”. Many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to BLEU, such as external lexical and syntactic resources. Unlike these and like BLEU, AMBER relies"
W12-3104,D10-1092,0,0.01535,"s. The first is absolute permutation distance: n DIST1 ( P1 , P2 ) = ∑ |p1i − p2i | (4) i =1 Let DIST1 ( P1 , P2 ) n( n + 1) / 2 ν1 = 1 − (5) v1 ranges from 0 to 1; a larger value means more similarity between the two permutations. This metric is similar to Spearman’s ρ (Spearman, 1904). However, we have found that ρ punishes long-distance reordering too heavily. For instance, ν 1 is more tolerant than ρ of the movement of “recently” in this example: Ref: Recently , I visited Paris Hyp: I visited Paris recently P1: 1 2 3 4 P2: 2 3 4 1 3.1 Ordering penalty v We use a simple matching algorithm (Isozaki et al., 2010) to do 1-1 word alignment between the hypothesis and the reference. After word alignment, represent the reference by a list of normalized positions of those of its words that were aligned with words in the hypothesis, and represent the hypothesis by a list of positions for the corresponding words in the reference. For both lists, unaligned words are ignored. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Suppose we have +1+1+ 3 v1 = 1 - 14(4 +1)/2 = 0.4 . Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure"
W12-3104,P02-1040,0,0.0936783,"e second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 1 2 Introduction AMBER is a machine translation evaluation metric first described in (Chen and Kuhn, 2011). It is designed to have the advantages of BLEU (Papineni et al., 2002), such as nearly complete language independence and rapid computability, while attaining even higher correlation with human judgment. According to the paper just cited: “It can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality”. Many recently defined machine translation metrics seek to exploit deeper sources of knowledge than are available to BLEU, such as external lexical and syntactic resources. Unlike these and like BLEU, AMBER relies entirely on matching surface forms in tokens in the hypothesis and ref"
W12-3104,C96-2141,0,0.429494,"simple matching algorithm (Isozaki et al., 2010) to do 1-1 word alignment between the hypothesis and the reference. After word alignment, represent the reference by a list of normalized positions of those of its words that were aligned with words in the hypothesis, and represent the hypothesis by a list of positions for the corresponding words in the reference. For both lists, unaligned words are ignored. E.g., let P1 = reference, P2 = hypothesis: P1: p11 p12 p13 p14 … p1i … p1n P2: p12 p22 p23 p24 … p2i … p2n Suppose we have +1+1+ 3 v1 = 1 - 14(4 +1)/2 = 0.4 . Inspired by HMM word alignment (Vogel et al., 1996), our second distance measure is based on jump width. This punishes only once a sequence of words that moves a long distance with the internal word order conserved, rather than on every word. In the following, only two groups of words have moved, so the jump width punishment is light: Ref: In the winter of 2010, I visited Paris Hyp: I visited Paris in the winter of 2010 The second distance measure is n DIST2 ( P1 , P2 ) = ∑ |( p1i − p1i −1 ) − ( p2i − p2i −1 ) |(6) Ref: in the winter of 2010 , I visited Paris Hyp: I visited Paris in 2010 ’s winter i =1 0 1 where we set p = 0 and p20 = 0 . Let"
W12-3104,W10-1703,0,\N,Missing
W14-3363,D11-1033,0,0.0376008,"ain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilit"
W14-3363,2011.iwslt-evaluation.18,0,0.0374739,"data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encoura"
W14-3363,2011.mtsummit-papers.30,1,0.896794,"that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect information (Section 4). Previous work provides empirical evidence supporting this. For instance, Foster et al. (2010) found that linear mixtures outperform log linear mixtures when adapting a French-English system to the medical domain, as well as on a ChineseEnglish NIST translation task. 2.4 Estimating Conditional Translation Probabilities Within each mixture component, we extract all phrase-pairs, compute relative frequencies, and use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk (t|s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequencies collected on the entire training set. 3 3.2 We propose to use automatic text clustering techniques to organize basic elements into homogeneous clusters that are seen as sub-domains. In our experiments, we apply clustering algorithms to the target (English) side of the corpus only. Each corpus element is transformed into a vector-space format by constructing"
W14-3363,2012.amta-papers.4,0,0.0728245,"main at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encourage us to rethink the use of mixture models, and opens up new ways of conceptualizing"
W14-3363,E12-1055,0,0.377042,"ranslation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks. 1 • Should mixture component capture domain information? Previous work assumes that training data should be organized into domains. When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions (Sennrich, 2012a). However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components. • Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain. Is this intuition still valid in our more complex heterogeneous training conditions? If not, how do mixture models affect translation probability estimates? Introduction While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems hav"
W14-3363,W07-0717,1,0.740538,"ng bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequ"
W14-3363,D10-1044,1,0.94741,"iform mixtures where all components are weighted equally: Linear Mixtures for Translation Models K X p˜(s, t) log We use the Expectation Maximization algorithm to solve this maximization problem. Does domain knowledge yield better translation quality when learning linear mixture weights for the translation model of a phrase-based MT system? We leave the study of linear mixtures for language and reordering models for future work. 2.1 X (1) k=1 where pk (t|s) is a conditional translation probability learned on subset k of the training corpus. 500 pean parliament proceedings and movie subtitles. Foster et al. (2010) work with a slightly different setting when defining mixture components for the NIST Chinese-English translation task: while there is no single obvious “in-domain” component in the NIST training set, homogeneous domains can still be defined in a straightforward fashion based on the provenance of the data (e.g., Hong Kong Hansards vs. Hong Kong Law vs. News articles from FBIS, etc.). We take a similar approach in our experiments. However, we will see that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect informatio"
W14-3363,2012.amta-papers.18,0,0.0177864,"an global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2"
W14-3363,2013.mtsummit-papers.23,1,0.626478,"ceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain distinctions can improve translation quality. It would be interesting to see whether our conclusion holds in these more artificial training settings, and whether sentence-level corpus organization could help translation quality in our settings. Finally, recent work shows that linear mixture weights can be optimized for BLEU, either directly (Haddow, 2013), or by simulating discriminative training (Foster et al., 2013). In this paper, we limited our studies to maximum likelihood and uniform mixtures, however, the various mixture component definitions proposed here can also be applied when maximizing BLEU. 8 Acknowledgments This research was supported in part by DARPA contract HR0011-12-C-0014 under subcontract to Raytheon BBN Technologies. The authors would like to thank the reviewers and the PORTAGE group at the National Research Council. Conclusion We have presented an extensive study of linear mixtures for training translation models on very heterogeneous data on Arabic-English and Chinese-English transl"
W14-3363,D07-1054,0,0.0324476,"rgences for very frequent source phrases. Overall, the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain dat"
W14-3363,D08-1089,0,0.0542947,"● ● 0 20 40 60 corpus component 80 #lines #words 100 4.4 Figure 1: Sizes of the 82 Arabic-English (top) and 101 Chinese-English (bottom) corpus components. 4.2 We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., 2011)2 • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and a word-displacement distortion penalty • a Good-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5-gram model trained on monolingual webforum data. Weights for these features are learned using a batch version of the MIRA algorithm (Chiang, 2012). Phrase pairs are extracted from several word alignments of the training set: HMM, IBM2, and IBM4. Word alignments are kept constant across all experiments. We apply our linear mixture models to both transl"
W14-3363,W12-3154,0,0.0153077,"er at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encourage us to rethink the use"
W14-3363,N13-1035,0,0.464864,"see in Section 4. We consider four very different ways of defining mixture components by grouping the basic corpus elements: (1) manual partition of the training corpus into domains, (2) automatically learning homogeneous domains using text clustering algorithms, (3) random partitioning, (4) sampling with replacement. 3.1 Induced Domains Using Automatic Clustering Algorithms Manually Defined Domains Heterogeneous training data is usually grouped into domains manually using provenance information. In most previous work, such domain distinctions are very clear and easy to define. For instance, Haddow (2013) uses European parliament proceedings to improve translation of text in the movie subtitles and News Commentary domains; Sennrich (2012a) aims to translate Alpine Club reports using components trained on Euro3.3 Random Partitioning We consider random partitions of the training corpus. They are generated by using a random number generator to assign each basic element to one of K clusters. Resulting components therefore do not capture any domain information. Each com501 Arabic-English Training Conditions segs src en train 8.5M 262M 207M Test Domain 1: Webforum segs src en dev (tune) 4.1k 66k 72k"
W14-3363,2005.eamt-1.19,0,0.044844,"inctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small numb"
W14-3363,W07-0733,0,0.0538177,"uster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with"
W14-3363,P07-2045,0,0.0073128,"● ● ●● ● ● ●● ● ●●●●●●● ●●● ● ● ●●●●●●●●●●●● ●●●●●● ● ●●●●●●●●●●●●●●● ● ● ●●●●●●●●●● ● ● ● ● ● ● ● ●●●●● ● ● 0 20 40 60 corpus component #lines #words 80 #lines / #words 1e+03 1e+05 1e+07 Chinese corpus components ● ●●●●● ● ●●●● ● ● ● ● ● ● ●●●●● ●●●●●●●●● ● ●●● ●● ●●● ●●●● ● ●● ●● ● ● ●● ●●●●●●● ● ●● ● ●●●●●●●●●●●●● ● ●●●● ●● ●●●●●●●●●●● ● ●●● ● ● ● 0 20 40 60 corpus component 80 #lines #words 100 4.4 Figure 1: Sizes of the 82 Arabic-English (top) and 101 Chinese-English (bottom) corpus components. 4.2 We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., 2011)2 • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and a word-displacement distortion penalty • a Good-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5"
W14-3363,2012.eamt-1.43,0,0.555765,"ranslation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks. 1 • Should mixture component capture domain information? Previous work assumes that training data should be organized into domains. When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions (Sennrich, 2012a). However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components. • Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain. Is this intuition still valid in our more complex heterogeneous training conditions? If not, how do mixture models affect translation probability estimates? Introduction While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems hav"
W14-3363,2010.iwslt-papers.5,0,\N,Missing
W16-2317,N12-1047,1,0.854573,"ierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a r"
W16-2317,N13-1003,1,0.836258,"). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translation"
W16-2317,P14-1129,0,0.161205,"Missing"
W16-2317,E14-4029,0,0.0478782,"Missing"
W16-2317,D11-1125,0,0.0353707,"mentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase l"
W16-2317,P07-1019,0,0.0247693,"ian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 8"
W16-2317,W10-1717,1,0.823355,"ces) and the monolingual English corpus available for the constrained news translation task, which is a combination of the Europarl v7 corpus, the NewsCommentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs a"
W16-2317,N04-1021,0,0.137306,"the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 82 features: 27 decoder features and 55 additional rescoring features. The rescoring model was tuned using nbest MIRA. Of the rescoring features, 51 consisted of various IBM features for word- and lemmaaligned IBM1, IBM2, IBM4 and HMM models, as well as various other standard length, n-gram, and n-best features. The final four features used NNJMs for rescoring, two Russian-word NNJM rescoring features and two Russian-lemma ones. Following Devlin et al. (2014), one NNJM feature rescored the 1000best list using a English-to-Russian NNJM, where the roles of th"
W16-2317,E99-1010,0,0.130019,"(Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural network joint model Distortion and sparse feature models Similar to the translation model, our hierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters o"
W16-2317,P12-1049,0,0.0701105,"Missing"
W16-2317,2014.amta-researchers.3,1,0.793782,"12 units for the single hidden layer. We train our models with mini-batch stochastic gradient descent, with a batch size of 128 words, and an initial learning rate of 0.3. We check our training objective on the development set every 20K batches, and if it fails to improve for two consecutive checks, the learning rate is halved. Training stops after 5 consecutive failed checks or after 90 checks. To enable efficient decoding, our models are self-normalized with a squared penalty on the Language models Our system consists of three n-gram language models (LMs) and two word class language models (Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural"
W16-2317,D13-1140,0,0.0283969,"xtracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. Our internal development experiments indicated that using lemma alignments improved the translation quality of a baseline phrase-based system by roughly 0.2 BLEU, and also benefited the perplexity of the bilingual neural language models described in Section 2.5 and 3.1. 2.3 2.5 We employ two neural network joint models, or NNJMs (Vaswani et al., 2013; Devlin et al., 2014). The NNJM is a feed-forward neural network language model that assumes access to a source sentence f and an aligned source index ai , which points to the most influential source word for the translation of the target word ei . The NNJM calculates the language modeling proba+m i−1 bility p(ei |ei−n+1 , faaii−m ), which accounts for the n−1 preceding target words, and for 2m+1 words of source context, centered around fai . Following Devlin et al. (2014), we use n = 4 and m = 5, resulting in 3 words of target context and 11 words of source context, effectively a 15-gram lan"
W17-3205,D11-1033,0,0.559108,"or author’s or publication’s style (Chen et al., 2013). Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches in40 Proceedings of the First Workshop on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weig"
W17-3205,W09-0432,0,0.0213964,"on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes wo"
W17-3205,P13-1141,0,0.0625807,"data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model."
W17-3205,P13-1126,1,0.917929,"Missing"
W17-3205,P17-2061,0,0.160124,"Missing"
W17-3205,P10-2041,0,0.180906,"ional origin, dialect, or author’s or publication’s style (Chen et al., 2013). Training data varies significantly across domains, and cross-domain translations are unreliable, so performance can often be improved by adapting the MT system to the test domain. Domain adaptation (DA) techniques for SMT systems have been widely studied. Approaches in40 Proceedings of the First Workshop on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data sel"
W17-3205,P12-2023,0,0.0191037,"synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after fir"
W17-3205,P02-1040,0,0.115578,"Missing"
W17-3205,W07-0717,1,0.366807,"on Neural Machine Translation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequenc"
W17-3205,2008.iwslt-papers.6,0,0.0546254,"NMT sub-models on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper"
W17-3205,D10-1044,1,0.472945,"Missing"
W17-3205,W16-2323,0,0.0410984,"being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the cross-entropy differenc"
W17-3205,P16-1009,0,0.0550095,"being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the cross-entropy differenc"
W17-3205,D11-1084,0,0.0213312,"nrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when com"
W17-3205,2012.iwslt-papers.17,0,0.0170511,"t can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after first presenting the NMT"
W17-3205,E12-1055,0,0.0524815,"slation, pages 40–46, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence tra"
W17-3205,P13-2122,0,0.0372216,"formance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description to section 4, after first presenting the NMT approach used in our experim"
W17-3205,P07-1066,0,0.0237287,"sing the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and applies them when computing the cost of each mini-batch during NMT training. We defer a detailed description"
W17-3205,W10-2602,0,0.0122925,"raining data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenario in this paper. Context-based DA includes word sense disambiguation for adaptation (Carpuat et al., 2013), which employs local context to distinguish the translations for different domains. The cachebased method (Tiedemann, 2010; Gong et al., 2011) uses local or document-level context. Work on topic-based DA includes (Tam et al., 2007; Eidelman et al., 2012; Hasler et al., 2012; Hewavitharana et al., 2013), and employs a topic 41 through an alignment model αti , which is a feedforward neural network to model the probability that yt is aligned to xi . plexity (Foster and Kuhn, 2007; Sennrich, 2012); and 4) take a weighted average of the parameters in the sub-models to form our final adapted model. Sentence-level weighting Our new method for weighting individual sentence pairs uses a classifier to assign weights, and a"
W17-3205,J07-1003,0,0.0120236,"orpora, we first train NMT sub-models on them, then combine these in a weighted fashion. Specifically, we: 1) train an NMT model on the large combined general-domain corpus; 2) initialize with the previous model, and train several new models on sub-corpora; 3) weight each sub-corpus according to its proximity to the in-domain data (dev set), using target-side language model perSMT adaptation techniques Most SMT domain adaptation (DA) techniques can be classified into one of five categories: selftraining, context-based DA, topic-based DA, data selection, and data weighting. With self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009), an MT system trained on general domain data is used to translate large in-domain monolingual data. The resulting bilingual sentence pairs are then used as additional training data. Sennrich (2016b) has shown that back-translating a large amount of target-language text and using the resulting synthetic parallel text can improve NMT performance significantly. We can expect greater improvement if the monolingual data are in-domain. This method assumes the availability of large amounts of in-domain monolingual data, which is not the adaptation scenari"
W17-3205,W04-3250,0,0.0729876,"Missing"
W17-3205,2015.iwslt-evaluation.11,0,0.218972,"e additional advantage of being integrated with sequence-to-sequence training. 2 Applying SMT adaptation techniques to NMT 2.2 There are several adaptation scenarios for MT, of which the most common is: 1) the training material is heterogeneous, with some parts that are not too far from the test domain; 2) a bilingual development set drawn from the test domain is available. In this paper, we study adaptation techniques for this scenario. 2.1 Application to NMT In this paper, we apply data selection, corpus weighting, and sentence weighting strategies to NMT. Data selection Some previous work (Luong and Manning, 2015; Sennrich et al., 2016b) has shown that the performance of NMT systems is highly sensitive to data size. Therefore, we follow the solution in (Luong and Manning, 2015): we first train an NMT system on all available training data, then further train on the selected in-domain data. We adopt two data selection methods in this paper. The first one is based on bilingual language model cross-entropy difference (Axelrod et al., 2011). For both the source and target language, two language models are trained on in-domain and out-of-domain data respectively; then, a sentence pair is evaluated with the"
W17-3205,D09-1074,0,0.0117488,"couver, Canada, August 4, 2017. 2017 Association for Computational Linguistics model to distinguish the translations for different topics. Data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Chen and Huang, 2016) search for data that are similar to the in-domain data according to some criterion, then use the results for training, either alone or in combination with existing data. Data weighting approaches weight each data item according to its proximity to the in-domain data. This can be applied at corpus (Foster and Kuhn, 2007; Sennrich, 2012), sentence (Matsoukas et al., 2009), or phrase level (Foster et al., 2010; Chen et al., 2013). We also study the application of existing SMT domain adaptation techniques to NMT, specifically data selection and corpus weighting methods. Experiments on Chinese-to-English NIST and English-to-French WMT tasks show that: 1) data selection and corpus weighting methods yield significant improvement over the non-adapted baseline; and 2) the new cost weighting method obtains the biggest improvement. The cost weighting scheme has the additional advantage of being integrated with sequence-to-sequence training. 2 Applying SMT adaptation te"
W17-3205,P13-2119,0,\N,Missing
W17-3205,K16-1031,1,\N,Missing
W17-4732,D11-1033,0,0.0310836,"omain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarity function for identifying sentence pairs from general-domain training corpora that are close to the target domain. We built four language models using the input and output sides of the training corpora and the development set respectively to select 3 million sentence pairs from the training corpora that are close to the news domain. However, the development set, which consists of only 1k sentence pairs, is too tiny to be a suitable corpus for building the in-domain language models that will enable the bilingual LM"
W17-4732,W16-2317,1,0.894593,"Missing"
W17-4732,W17-3205,1,0.837791,"cted by bilingual LM cross-entropy difference (xent), c) further trained with synthetic data, d) further trained with cost weighting, e) further trained with in-domain data selected by semi-supervised convolutional neural network classifier (sscnn), f) greedy model averaging and g) optimized against sentence-level BLEU on the intersection of the subsets of data selected by xent and sscnn using MRT. 3.2 Data selection and domain adaptation lion sentence pairs from the training corpora that are close to the news domain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarit"
W17-4732,K16-1031,1,0.92652,"-English (out of twenty participants) in WMT 2017 human evaluation. 1 George Foster∗ Work performed while at NRC. 330 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 330–337 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics the one described in Section 2.1,1 and then employed the machine-translated Russian and perfect English sentence pairs as additional data to train the Russian-English MT system. To select sentences for back-translation, we used a semi-supervised convolutional neural network classifier (Chen and Huang, 2016). We sampled two million sentences from the English monolingual News Crawl 2015 & 2016 corpora according to their classifier scores, which reflect their similarity to the the English half of our development set. formance (third place in both language pairs) in the preliminary automatic evaluation of WMT 2017. In this paper, we discuss the lessons learned in building large-scale state-of-the-art NMT systems. 2 Russian-English news translation We used all the Russian-English parallel corpora available for the constrained news translation task. They include the CommonCrawl corpus, the NewsComment"
W17-4732,N04-1021,0,0.0967458,"lt is rather disappointing by comparison with the exciting improvement reported in Sennrich et al. (2016a), i.e. 3-4 BLEU. Another disappointing result is that model averaging does not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take"
W17-4732,N12-1047,1,0.741262,"s not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take advantage of the rescoring framework to have our NNJMs view each candidate translation from 332 Figure 1: Russian-English learning curve on development set in cased BLEU of selected model"
W17-4732,P14-1129,0,0.0771307,"Missing"
W17-4732,E17-3017,0,0.0554434,"Missing"
W17-4732,P16-1009,0,0.450161,"inflections, since they play an important role in disambiguating the meaning of sentences. Chinese does not have clear word boundaries. The number of Chinese word types created by automatic word segmentation software is high, while naive character segmentation would result in a skewed Chinese to English sentence length ratio. These characteristics make it difficult for machine translation systems to learn the correct association between words in Chinese and English. Since this was the first time we deployed NMT models in an evaluation, we first tried to replicate the results of previous work (Sennrich et al., 2016a). Our NMT systems are based on Nematus (Sennrich et al., 2017). We used automatic back-translation (Sennrich et al., 2016b) of a subselected monolingual News corpus as additional training data, and all the training data is segmented into subword units using BPE (Sennrich et al., 2016c). We also experimented with pervasive dropout as implemented in Nematus. For Russian-English, our WMT16 PBMT system scored higher than all the NMT systems we built this year. We therefore experimented with using the NMT systems as features for rescoring the 1000-best output from our WMT16 PBMT system. This stra"
W17-4732,W16-2316,0,0.0118976,"and embedding layers to 0.15. For the hidden layers, we set the dropout probability to 0.3. NMT baseline system Our NMT baseline system is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A"
W17-4732,P16-1162,0,0.103844,"Missing"
W17-4732,P16-1159,0,0.0343062,"ystem is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A common practice for avoiding overfitting to the training data is ensembling the last few models saved as checkpoints. Rec"
W17-4732,W03-1730,0,0.0742719,"We used all the Chinese-English parallel corpora available for the constrained news translation task. They include the UN corpus, the NewsCommentary v12 corpus and the CWMT corpus. In total, 25 million parallel Chinese-English sentences were used to train the baseline system. We used half of the WMT 17 news translation development set as our development set and the other half as internal test set. The English texts in the training/development/test corpora were tokenized and lowercased while the Chinese texts in the training/development/test corpora were segmented using the ICTCLAS segmenter (Zhang et al., 2003). Then the Chinese and English text were combined to train a BPE model with vocabulary size of 90k. Although in figure 1 we see that none of the NMT systems manage to beat our WMT16 PBMT submission, the more interesting result is that there is more than 1.8 BLEU gain on the development set and 1.1 BLEU gain on the test set by rescoring the PBMT 1000-best list using just one of our NMT systems and no other features, as in line (g). The final rescoring with weighted collections of NMT systems, language model features, NNJM features and n-best features shows 1.8 BLEU improvement over the WMT 16 s"
W98-1103,J82-2005,0,0.207858,"ocument search mechanisms capable of matching a natural language query with documents written in a different language. Recently, we conducted several experiments aimed at comparing various methods of incorporating a cross-linguistic capability to existing information retrieval (IR) systems. Our results indicate that translating queries with off-theshelf machine translation systems can result in relatively good performance. But the results also indicate that other methods can perfonn even better. More specifically, we tested a probabilistic translation model of the kind proposed by Brown & al. [2]. The parameters of that system had been estimated automatically on a different, unrelated, corpus of parallel texts. After we augmented it with a small bilingual dictionary, this probabilistic translation model outperformed machine translation systems on our cross-language IR task. 1. I n t r o d u c t i o n Adequate text processing systems have become widely available for most natural languages. While English remains the dominant language on the Intemet, the relative share of other languages now appears to be on the rise. The network has become truly multilingual. This situation has created"
W98-1103,1997.mtsummit-papers.1,0,0.0479523,"translator. In recent years it has been shown that existing MT techniques can produce useful results when they are applied to tasks that amount to somewhat less than translation proper. In previous work, we have shown that probabilistic translation models such as those of Brown et al. [2] could be used as the key component of various translation support tools. Specifically, our work on the TransTalk project [1, 4] has established that such models could become instrumental in improving the process of automatically transcribing a spoken translation. And our ongoing work on the TransType project [5] indicates that models of the same kind can drive typing aids for translators. A key feature of such applications is that they do not expect the machine to volunteer a fullfledged translation on its own. Rather, the machine is only expected to restrict the range of possible translations so as to make it easier to guess what the intentions of the human translator are. • For example, in certain incarnations of the TransTalk system, the translation model is used as a means of answering the following question: given a source language sentence e, what is the likelihood of observing the word f in an"
W98-1103,J93-1004,0,0.242452,"al substance) or as ""mgdicament"" (a legal medicine) depending on the context. There is most often no explicit clue in the query that would allow one to choose the appropriate meaning. Yet another approach is to determine translational equivalence automatically, on the basis of a corpus of parallel texts (that is, a corpus made up of source texts and their translations). One way of doing this is to start by establishing translation correspondences between units larger than words, typically sentences. There are now well-known methods for aligning the sentences of parallel corpora (Gale & Church [6], Simard, Foster 8~ Isabelle [10]). Then, the translational equivalence of a given pair of words can be estimated by their degree of co-occurrence in parallel sentences. Compared to the previous approaches, this has the following advantages: - There is no need to acquire or to compile a bilingual dictionary or a complete MT system. - Word translations are made sensitive to the domain, as embodied by the training corpus. - As we will see below, it is relatively easy to obtain a suitable degree of query expansion based on translational ambiguity. In the next section, we describe the structure of"
W98-1103,1998.amta-tutorials.5,0,0.0822942,"Missing"
W98-1103,P91-1022,0,0.105012,"(a legal medicine) depending on the context. There is most often no explicit clue in the query that would allow one to choose the appropriate meaning. Yet another approach is to determine translational equivalence automatically, on the basis of a corpus of parallel texts (that is, a corpus made up of source texts and their translations). One way of doing this is to start by establishing translation correspondences between units larger than words, typically sentences. There are now well-known methods for aligning the sentences of parallel corpora (Gale & Church [6], Simard, Foster 8~ Isabelle [10]). Then, the translational equivalence of a given pair of words can be estimated by their degree of co-occurrence in parallel sentences. Compared to the previous approaches, this has the following advantages: - There is no need to acquire or to compile a bilingual dictionary or a complete MT system. - Word translations are made sensitive to the domain, as embodied by the training corpus. - As we will see below, it is relatively easy to obtain a suitable degree of query expansion based on translational ambiguity. In the next section, we describe the structure of a probabilistic translation mode"
