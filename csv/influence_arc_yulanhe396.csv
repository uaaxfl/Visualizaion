2020.acl-main.32,P18-1001,0,0.0144052,"yer) as shown in the right panel of Figure 2. It employs real distribution pair p~r and fake distribution pair p~f as input and then outputs Dout to identify the input sources (fake or real). Concretely, a higher value of Dout represents that D is more prone to predict the input as real and vice versa. 3.2 BAT with Gaussian (Gaussian-BAT) In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings (Mikolov et al., 2013a,b; Pennington et al., 2014; Joulin et al., 2017; Athiwaratkun et al., 2018) into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in Figure 3. V-dim K-dim N (~ 1 ; §1 ) µ~f » Dir(~ µf j~ ®) Document-topic distribution Word Embedding ~1 Á ... ... N (~ 2 ; §2 ) ~2 Á ... ... N (~ K ; §K ) ~K Á Gaussian distributions Topic-word distributions d~f p~f Documentfake distribution pair word distribution Generator Network (G) Figure 3: The generator of Gaussian-BAT. Concretely, Gaussian-BAT employs the multivariate Gaussian N (~ µk , Σk ) to model the k-th topic. Here, µ ~ k and"
2020.acl-main.32,E17-2068,0,0.0146325,"ayer and an output layer) as shown in the right panel of Figure 2. It employs real distribution pair p~r and fake distribution pair p~f as input and then outputs Dout to identify the input sources (fake or real). Concretely, a higher value of Dout represents that D is more prone to predict the input as real and vice versa. 3.2 BAT with Gaussian (Gaussian-BAT) In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings (Mikolov et al., 2013a,b; Pennington et al., 2014; Joulin et al., 2017; Athiwaratkun et al., 2018) into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in Figure 3. V-dim K-dim N (~ 1 ; §1 ) µ~f » Dir(~ µf j~ ®) Document-topic distribution Word Embedding ~1 Á ... ... N (~ 2 ; §2 ) ~2 Á ... ... N (~ K ; §K ) ~K Á Gaussian distributions Topic-word distributions d~f p~f Documentfake distribution pair word distribution Generator Network (G) Figure 3: The generator of Gaussian-BAT. Concretely, Gaussian-BAT employs the multivariate Gaussian N (~ µk , Σk ) to model the"
2020.acl-main.32,D14-1162,0,0.0834996,"ensional representation layer and an output layer) as shown in the right panel of Figure 2. It employs real distribution pair p~r and fake distribution pair p~f as input and then outputs Dout to identify the input sources (fake or real). Concretely, a higher value of Dout represents that D is more prone to predict the input as real and vice versa. 3.2 BAT with Gaussian (Gaussian-BAT) In BAT, the generator models topics based on the bag-of-words assumption as in most other neural topic models. To incorporate the word relatedness information captured in word embeddings (Mikolov et al., 2013a,b; Pennington et al., 2014; Joulin et al., 2017; Athiwaratkun et al., 2018) into the inference process, we modify the generator of BAT and propose Gaussian-BAT, in which G models each topic with a multivariate Gaussian as shown in Figure 3. V-dim K-dim N (~ 1 ; §1 ) µ~f » Dir(~ µf j~ ®) Document-topic distribution Word Embedding ~1 Á ... ... N (~ 2 ; §2 ) ~2 Á ... ... N (~ K ; §K ) ~K Á Gaussian distributions Topic-word distributions d~f p~f Documentfake distribution pair word distribution Generator Network (G) Figure 3: The generator of Gaussian-BAT. Concretely, Gaussian-BAT employs the multivariate Gaussian N (~ µ"
2020.acl-main.32,D19-1027,1,0.852245,"roximate the Dirichlet distribution, they are not exactly the same. An illustration of these two distributions is shown in Figure 1 in which the Logistic-Normal distribution does not exhibit multiple peaks at the vertices of the simplex as that in the Dirichlet distribution and as such, it is less capable to capture 340 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 340–350 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the multi-modality which is crucial in topic modeling (Wallach et al., 2009). To deal with the limitation, Wang et al. (2019a) proposed the Adversarialneural Topic Model (ATM) based on adversarial training, it uses a generator network to capture the semantic patterns lying behind the documents. However, given a document, ATM is not able to infer the document-topic distribution which is useful for downstream applications, such as text clustering. Moreover, ATM take the bag-of-words assumption and do not utilize any word relatedness information captured in word embeddings which have been proved to be crucial for better performance in many NLP tasks (Liu et al., 2018; Lei et al., 2018). To address these limitations, w"
2020.acl-main.32,P14-2114,1,0.857538,"Missing"
2020.acl-main.352,D19-1178,0,0.0163872,"hows promising results on geolocation prediction (Du et al., 2016). Benefiting from the flexibility and scalability of neural networks, several work has been done in this vein including event sequence prediction (Mei and Eisner, 2017) and failure prediction (Xiao et al., 2017). Our work is partly inspired by RMTPP, but departs from the previous work by jointly considering users’ social relations and topical attentions for stance prediction on social media. 5 Related Work The prediction of real-time stances on social media is challenging, partly caused by the diversity and fickleness of users (Andrews and Bishop, 2019). A line of work mitigated the problem by taking into account the homophily that users are similar to their friends (McPherson et al., 2001; Halberstam and Knight, 2016). For example, Chen et al. (2016) gauged a user’s opinion as an aggregated stance of their neighborhood users. Linmei et al. (2019) took a step further by exploiting the extracted topics, which discern a user’s focus on neighborhood tweets. Recent advances in this strand also include the application of GCNs, with which the social relationships are leveraged to enrich the user representations (Li and Goldwasser, 2019; Del Tredic"
2020.acl-main.352,S17-2126,0,0.0182978,"opic zi . The output of attention module is concatenated with tweet representation, time interval τi , user representation u, and topic representation zi , which is encoded from xbi via a Variational Autoencoder (VAE). Finally, the combined representation is sent to a GRU cell, whose hidden state participates in computing the intensity function and the softmax function, for the prediction of the posting time interval and the stance label of the next tweet. In the following, we elaborate the model in more details: Tweet representation: Words in tweets are mapped to pre-trained word embeddings (Baziotis et al., 2017)1 , which is specially trained for tweets. Then Bi-LSTM is used to generate the tweet representation. Topic extraction: The topic representation zi in Figure 1 captures the topic focus of the ith tweet. It is learned by VAE (Kingma and Welling, 2014), which approximates the intractable true posterior 1 https://github.com/cbaziotis/ datastories-semeval2017-task4 by optimising the reconstruction error between the generated tweet and the original tweet. Specifically, we convert each tweet to the bag-of-word format weighted by term frequency, xbi , and feed it to two inference neural networks defi"
2020.acl-main.352,C16-1208,0,0.132652,"opinions online towards various subject matters. Despite much progress in sentiment analysis in social media, the prediction of opinions, however, remains challenging. Opinion formation is a complex process. An individual’s opinion could be influenced by their own prior belief, their social circles and external factors. Existing studies often assume that socially connected users hold similar opinions. Social network information is integrated with user representations via weighted links and encoded using neural networks with attentions or more recently Graphical Convolutional Networks (GCNs) (Chen et al., 2016; Li and Goldwasser, 2019). This strand of work, including (Chen et al., 2018; Zhu et al., 2020; Del Tredici et al., 2019), leverages both the chronological tweet sequence and social networks to predict users’ opinions. The majority of previous work requires a manual segmentation of a tweet sequence into equallyspaced intervals based on either tweet counts or ∗ Corresponding author time duration. Models trained on the current interval are used to predict users’ opinions in the next interval. However, we argue that such a manual segmentation may not be appropriate since users post tweets at dif"
2020.acl-main.352,W14-4012,0,0.0560547,"Missing"
2020.acl-main.352,D19-1477,0,0.0610508,"Missing"
2020.acl-main.352,P19-1247,0,0.234189,"wards various subject matters. Despite much progress in sentiment analysis in social media, the prediction of opinions, however, remains challenging. Opinion formation is a complex process. An individual’s opinion could be influenced by their own prior belief, their social circles and external factors. Existing studies often assume that socially connected users hold similar opinions. Social network information is integrated with user representations via weighted links and encoded using neural networks with attentions or more recently Graphical Convolutional Networks (GCNs) (Chen et al., 2016; Li and Goldwasser, 2019). This strand of work, including (Chen et al., 2018; Zhu et al., 2020; Del Tredici et al., 2019), leverages both the chronological tweet sequence and social networks to predict users’ opinions. The majority of previous work requires a manual segmentation of a tweet sequence into equallyspaced intervals based on either tweet counts or ∗ Corresponding author time duration. Models trained on the current interval are used to predict users’ opinions in the next interval. However, we argue that such a manual segmentation may not be appropriate since users post tweets at different frequency. Also, th"
2020.acl-main.352,D19-1488,0,0.0172919,"d by RMTPP, but departs from the previous work by jointly considering users’ social relations and topical attentions for stance prediction on social media. 5 Related Work The prediction of real-time stances on social media is challenging, partly caused by the diversity and fickleness of users (Andrews and Bishop, 2019). A line of work mitigated the problem by taking into account the homophily that users are similar to their friends (McPherson et al., 2001; Halberstam and Knight, 2016). For example, Chen et al. (2016) gauged a user’s opinion as an aggregated stance of their neighborhood users. Linmei et al. (2019) took a step further by exploiting the extracted topics, which discern a user’s focus on neighborhood tweets. Recent advances in this strand also include the application of GCNs, with which the social relationships are leveraged to enrich the user representations (Li and Goldwasser, 2019; Del Tredici et al., 2019). On the other hand, several work has utilized the chronological order of tweets. Chen et al. (2018) presented an opinion tracker that predicts a stance every time a user publishes a tweet, whereas (Zhu et al., 2020) extended the previous work by introducing a topic-dependent attentio"
2020.acl-main.352,P16-2064,0,0.0150674,"osed protectionist market we pay than we ever… d1 x4 T1 T2 T3 Topic T1 immigration Top wordsp Words immigration, stop, free, work, change, countries, immigrants, migrants, migration, open T2 Boris Johnson Boris, live, Johnson, politics, sturgeon, TV, Nicola, morning, takebackcontrol, guy T3 vote remain voteremain, strongerin, Cameron, eureferendum, David, inorout, pm, eudebate, osborne, positive Figure 3: Distribution over 3 topics and attention signals on 3 neighborhood tweets, respectively in 2 time steps. Topics are labelled based on the top 10 words. 4 jith et al., 2017), rumor detection (Lukasik et al., 2016; Zubiaga et al., 2016; Alvari and Shakarian, 2019) and retweet prediction (Kobayashi and Lambiotte, 2016). A combination of the Hawkes process with recurrent neural networks, called Recurrent Marked Temporal Pointed Process (RMTPP), was proposed to automatically capture the influence of the past events on future events, which shows promising results on geolocation prediction (Du et al., 2016). Benefiting from the flexibility and scalability of neural networks, several work has been done in this vein including event sequence prediction (Mei and Eisner, 2017) and failure prediction (Xiao et al."
2020.acl-main.352,C16-1230,0,0.0193383,"ket we pay than we ever… d1 x4 T1 T2 T3 Topic T1 immigration Top wordsp Words immigration, stop, free, work, change, countries, immigrants, migrants, migration, open T2 Boris Johnson Boris, live, Johnson, politics, sturgeon, TV, Nicola, morning, takebackcontrol, guy T3 vote remain voteremain, strongerin, Cameron, eureferendum, David, inorout, pm, eudebate, osborne, positive Figure 3: Distribution over 3 topics and attention signals on 3 neighborhood tweets, respectively in 2 time steps. Topics are labelled based on the top 10 words. 4 jith et al., 2017), rumor detection (Lukasik et al., 2016; Zubiaga et al., 2016; Alvari and Shakarian, 2019) and retweet prediction (Kobayashi and Lambiotte, 2016). A combination of the Hawkes process with recurrent neural networks, called Recurrent Marked Temporal Pointed Process (RMTPP), was proposed to automatically capture the influence of the past events on future events, which shows promising results on geolocation prediction (Du et al., 2016). Benefiting from the flexibility and scalability of neural networks, several work has been done in this vein including event sequence prediction (Mei and Eisner, 2017) and failure prediction (Xiao et al., 2017). Our work is p"
2020.coling-main.229,W04-1013,0,0.0524384,"Missing"
2020.coling-main.229,W04-3252,0,0.0836809,"iven a question, select a random sentence from paired reviews as an answer. • Sentence Retrieval. First, convert each question and each sentence of its paired reviews into sentence embeddings using BERT, then retrieve the sentences with the highest cosine similarity with the question as the selective answer. The sentence length of both heuristic baselines is 120. • BERT+summary. Directly using BERT (Devlin et al., 2018) for generative QA is difficult since it is memory demanding to deal with multiple reviews in one go. We instead first generate an extractive summary of reviews using Textrank (Mihalcea and Tarau, 2004), then feed a question and its associated review summary into BERT for answer generation. • XLNet+summary. Although XLnet is theoretically capable of dealing with the text of unlimited length as it adopts the segmentation mechanism from Transformer XL (Dai et al., 2019), and could potentially process at once the concatenation of all the passages paired with a question, the computational requirements easily became rather prohibitive, and in practice is often not feasible to simultaneously deal with multiple long reviews with limited computational resources. Therefore, we take a similar summary-"
2020.coling-main.229,P19-1220,0,0.0169502,"re are mainly two types of methods for multi-passage QA. One is to use retrieval-based methods to first identify text passages that are most likely to contain answer information, and then perform QA on the extracted text passages which are essentially considered as a single passage. The other one is to separately run single-passage QA over each passage, obtaining multiple answer candidates, and then determine the best answer through mutual verification among the answers. Examples in the first type of methods include S-NET (Tan et al., 2018), Multi-passage BERT (Wang et al., 2019), and Masque (Nishida et al., 2019). These models require supporting text passages to be explicitly annotated. S-NET (Tan et al., 2018) follows an extraction-then-synthesis framework. First, relevant passages are extracted from context using a variant of R-NET (Wang et al., 2017), which learns to rank passages and extract the most possible evidence span from the selective passage; then, the evidencenotated selective passage is used for the GRU decoder synthesizing answers. In Multi-passage BERT (Wang et al., 2019), two independent BERTs were used to perform multi-passage QA. One BERT takes the question and a text passage as inp"
2020.coling-main.229,P02-1040,0,0.106112,"Missing"
2020.coling-main.229,D19-1250,0,0.0355587,"Missing"
2020.coling-main.229,D16-1264,0,0.318611,"f-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the interpretability introduced by the memory module. 1 Introduction With the development of large-scale pre-trained Language Models (LMs) such as BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019), tremendous progress has been made in Question Answering (QA). Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016). Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks. However, in many more realistic situations such as online communities, people tend to ask ‘descriptive’ questions (e.g., ‘How to improve the sound quality of echo dot?’). Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-for"
2020.coling-main.229,2020.acl-main.704,0,0.027136,"Missing"
2020.coling-main.229,P17-1018,0,0.0298035,"r to gain task-relevant but out-of-domain knowledge. Gupta et al. (2019) created a subset from the Amazon QA product review dataset (McAuley and Yang, 2016), consisting of 923k questions with 3.6M answers and 14M reviews on 156k Amazon products. They trained an answerability classifier from 3,297 question-context pairs labeled by Mechanical Turk and used it to classify answerability for the whole dataset. They then converted the dataset into a span-based format by heuristically creating an answer span from reviews that best answers a question based on users’ actual answers, and trained R-Net (Wang et al., 2017), which uses a gated self-attention mechanism and pointer networks, to predict answer boundaries. There are few studies using generative models to deal with opinion/review-based QA. Multi-passage QA There are mainly two types of methods for multi-passage QA. One is to use retrieval-based methods to first identify text passages that are most likely to contain answer information, and then perform QA on the extracted text passages which are essentially considered as a single passage. The other one is to separately run single-passage QA over each passage, obtaining multiple answer candidates, and"
2020.coling-main.229,P18-1178,0,0.0333748,"Missing"
2020.coling-main.229,D19-1599,0,0.0145762,"w-based QA. Multi-passage QA There are mainly two types of methods for multi-passage QA. One is to use retrieval-based methods to first identify text passages that are most likely to contain answer information, and then perform QA on the extracted text passages which are essentially considered as a single passage. The other one is to separately run single-passage QA over each passage, obtaining multiple answer candidates, and then determine the best answer through mutual verification among the answers. Examples in the first type of methods include S-NET (Tan et al., 2018), Multi-passage BERT (Wang et al., 2019), and Masque (Nishida et al., 2019). These models require supporting text passages to be explicitly annotated. S-NET (Tan et al., 2018) follows an extraction-then-synthesis framework. First, relevant passages are extracted from context using a variant of R-NET (Wang et al., 2017), which learns to rank passages and extract the most possible evidence span from the selective passage; then, the evidencenotated selective passage is used for the GRU decoder synthesizing answers. In Multi-passage BERT (Wang et al., 2019), two independent BERTs were used to perform multi-passage QA. One BERT takes the"
2020.coling-main.229,N19-1242,0,0.102808,"from alternatives in open-ended ’descriptive’ questions. More recently, Yu and Lam (2018) only focused on the yes/no questions in the Amazon QA dataset (McAuley and Yang, 2016) and trained a binary answer prediction model by leveraging latent aspect-specific representations of both questions and reviews learned by an autoencoder. Gao et al. (2019) focused on factual QA in e-commerce and proposed a Product-Aware Answer Generator that combines reviews and product attributes for answer generation, and uses a discriminator to determine whether the generated answer contains question-related facts. Xu et al. (2019a) proposed an extractive review-based QA task and manually created just over 2,500 questions and annotated the corresponding answer spans in less than 1,000 reviews relating to laptops and restaurants from the review data of SemEval 2016 Task 51 . They first jointly fine-tuned BERT for answer span detection, aspect extraction and aspect sentiment classification on the SemEval 2016 Task 5 data, and then post-trained BERT on over 3 million unlabelled Amazon and Yelp reviews in order to fuse domain knowledge, and also on SQuAD 1.1 (Rajpurkar et al., 2016) in order to gain task-relevant but out-o"
2020.coling-main.229,N19-1301,0,0.146038,"from alternatives in open-ended ’descriptive’ questions. More recently, Yu and Lam (2018) only focused on the yes/no questions in the Amazon QA dataset (McAuley and Yang, 2016) and trained a binary answer prediction model by leveraging latent aspect-specific representations of both questions and reviews learned by an autoencoder. Gao et al. (2019) focused on factual QA in e-commerce and proposed a Product-Aware Answer Generator that combines reviews and product attributes for answer generation, and uses a discriminator to determine whether the generated answer contains question-related facts. Xu et al. (2019a) proposed an extractive review-based QA task and manually created just over 2,500 questions and annotated the corresponding answer spans in less than 1,000 reviews relating to laptops and restaurants from the review data of SemEval 2016 Task 51 . They first jointly fine-tuned BERT for answer span detection, aspect extraction and aspect sentiment classification on the SemEval 2016 Task 5 data, and then post-trained BERT on over 3 million unlabelled Amazon and Yelp reviews in order to fuse domain knowledge, and also on SQuAD 1.1 (Rajpurkar et al., 2016) in order to gain task-relevant but out-o"
2020.coling-main.229,D18-1259,0,0.0411636,"Missing"
2020.tacl-1.31,P18-1031,0,0.0282302,"Missing"
2020.tacl-1.31,C18-1151,0,0.0374956,"Missing"
2020.tacl-1.31,P19-1165,0,0.0369493,"Missing"
2020.tacl-1.31,N19-1110,0,0.019001,"among all documents and learned from data. Also, whereas BSG only models the generation of context words given a pivot word, JTW explicitly models the generation of both the pivot word and the context words with different generative routes. also fine-grained interpretation of local topics, both of which are informed by word embeddings. Instead of using word embeddings for topic modeling, Liu et al. (2015) proposed the Topical Word Embedding model, which incorporates the topical information derived from standard topic models into word embedding learning by treating each topic as a pseudo-word. Briakou et al. (2019) followed this route and proposed a four-stage model in which topics were first extracted from a corpus by LDA and then the topic-based word embeddings are mapped to a shared space using anchor words that were retrieved from the WordNet. There are also approaches proposed to jointly learn topics and word embeddings built on SkipGram models. Shi et al. (2017) developed a SkipGram Topical word Embedding (STE) model built on PLSA where each word is associated with two matrices—one matrix used when the word is a pivot word and another used when the word is considered as a context word. Expectation"
2020.tacl-1.31,P19-1321,0,0.0747769,"ted to two lines of research: Skip-Gram approaches for word embedding learning. The Skip-Gram, also known as WORD2VEC (Mikolov et al., 2013b), maximizes the probability of the context words wn given a centroid word xn . Pennington et al. (2014) pointed out that Skip-Gram neglects the global word cooccurrence statistics. They thus formulated the Skip-Gram as a non-negative matrix factorization (NMF) with the cross-entropy loss switched to the least square error. Another NMF-based method was proposed by Xu et al. (2018), in which the Euclidean distance was substituted with Wasserstein distance. Jameel and Schockaert (2019) rewrote the NMF objective as a cumulative product of normal distributions, in which each factor is multiplied by a von Mises-Fisher (vMF) distribution of context word vectors, to hopefully cluster the context words since the vMF density retains the cosine similarity. Although the Skip-Gram-based methods attracted extensive attention, they were criticized for their inability to capture polysemy (Pilehvar and Collier, 2016). A pioneered solution to this problem is the Multiple-Sense Skip-Gram model (Neelakantan et al., 2014), where word vectors in a context are first averaged then clustered wit"
2020.tacl-1.31,P15-1077,0,0.168986,"BOWs) representation of a given document as the input to the VAE and aim to learn hidden topics that can be used to reconstruct the original document. They do not learn word embeddings concurrently. Other topic modeling approaches explore the pre-trained word embeddings for the extraction of more semantically coherent topics since word embeddings capture syntactic and semantic regularities by encoding the local context of word co-occurrence patterns. For example, the topicword generation process in the traditional topic models can be replaced by generating word embeddings given latent topics (Das et al., 2015) or by a two-component mixture of a Dirichlet multinomial component and a word embedding component (Nguyen et al., 2015). Alternatively, the information derived from word embeddings can be used to promote semantically related words in the Polya Urn sampling process of topic models (Li et al., 2017) or generate topic hierarchies (Zhao et al., 2018). However, all these models use pretrained word embeddings and do not learn word embeddings jointly with topics. Word embeddings could improve the topic modeling results, but conversely, the topic information could also benefit word embedding learning"
2020.tacl-1.31,N19-1423,0,0.0198927,"in deep contextualized word representation learning have generated significant impact in natural language processing. Different from traditional word embedding learning methods such as Word2Vec or GloVe, where each word is mapped to a single vector representation, deep contextualized word representation learning methods are typically trained by language modeling and generate a different word vector for each word depending on the context in which it is used. A notable work is ELMo (Peters et al., 2018), which is commonly regarded as the pioneer for deriving deep contextualized word embeddings (Devlin et al., 2019). ELMo calculates the weighed sum of different layers of a multi-layered BiLSTM-based language model, using the normalized vector as a representation for the corresponding word. More recently, in contrast to ELMo, BERT (Devlin et al., 2019) was proposed and the function is monotone to the similarity between the input ELMo/BERT embeddings and the reconstructed output embeddings, which 11 Model JTW ELMo BERT JTW-ELMo JTW-BERT Precision 0.5713±.021 0.6091±.005 0.6293±.014 0.6286±.008 0.6354±.014 Criteria Recall Macro-F1 0.5639±.014 0.5599±.016 0.6053±.001 0.6056±.002 0.5952±.006 0.6041±.012 0.611"
2020.tacl-1.31,D14-1162,0,0.109366,"alternatives in both word similarity evaluation and word sense disambiguation tasks, and can extract semantically more coherent topics from data; • We also show that JTW can be easily integrated with existing deep contextualized word embedding learning models to further improve the performance of downstream tasks such as sentiment classification. 2 Related Work Our work is related to two lines of research: Skip-Gram approaches for word embedding learning. The Skip-Gram, also known as WORD2VEC (Mikolov et al., 2013b), maximizes the probability of the context words wn given a centroid word xn . Pennington et al. (2014) pointed out that Skip-Gram neglects the global word cooccurrence statistics. They thus formulated the Skip-Gram as a non-negative matrix factorization (NMF) with the cross-entropy loss switched to the least square error. Another NMF-based method was proposed by Xu et al. (2018), in which the Euclidean distance was substituted with Wasserstein distance. Jameel and Schockaert (2019) rewrote the NMF objective as a cumulative product of normal distributions, in which each factor is multiplied by a von Mises-Fisher (vMF) distribution of context word vectors, to hopefully cluster the context words"
2020.tacl-1.31,S07-1009,0,0.0489293,"Missing"
2020.tacl-1.31,N18-1202,0,0.0395538,"trigonometric functions, which forms a probability distribution by Z π 1 θ cos dθ = 1, (12) 2 0 2 Recent advances in deep contextualized word representation learning have generated significant impact in natural language processing. Different from traditional word embedding learning methods such as Word2Vec or GloVe, where each word is mapped to a single vector representation, deep contextualized word representation learning methods are typically trained by language modeling and generate a different word vector for each word depending on the context in which it is used. A notable work is ELMo (Peters et al., 2018), which is commonly regarded as the pioneer for deriving deep contextualized word embeddings (Devlin et al., 2019). ELMo calculates the weighed sum of different layers of a multi-layered BiLSTM-based language model, using the normalized vector as a representation for the corresponding word. More recently, in contrast to ELMo, BERT (Devlin et al., 2019) was proposed and the function is monotone to the similarity between the input ELMo/BERT embeddings and the reconstructed output embeddings, which 11 Model JTW ELMo BERT JTW-ELMo JTW-BERT Precision 0.5713±.021 0.6091±.005 0.6293±.014 0.6286±.008"
2020.tacl-1.31,W15-1501,0,0.0193312,"r et al., 2011), which was designed to evaluate the word-embedding learning methods regarding their ability to disambiguate word senses. The lexical substitution task can be described by the following scenario: Given a sentence and one of its member words, find the most related replacement from a list of candidate words. As stated in Thater et al. (2011), a good lexical substitution should not only capture the relatedness between the candidate word and the original word, but also imply the correctness with respect to the context. Following Braˇzinskas et al. (2018), we derive the setting from Melamud et al. (2015) to ensure a fair comparison between the context-free word embedding methods and the context-dependent ones. In detail, for JTW and BSG, we capture the context of a given word using the BOW representation, and derive the representation of each candidate word taken into account of the context. For CvMF and STE, the similarity score is computed using The results are reported in Table 1. It can be observed that among the baselines, BSG achieves the lowest score on average, followed by MMSG. Although JTW clearly beats all the other models on SimLex-999 only, it only performs slightly worse than th"
2020.tacl-1.31,D16-1174,0,0.0180171,"ss switched to the least square error. Another NMF-based method was proposed by Xu et al. (2018), in which the Euclidean distance was substituted with Wasserstein distance. Jameel and Schockaert (2019) rewrote the NMF objective as a cumulative product of normal distributions, in which each factor is multiplied by a von Mises-Fisher (vMF) distribution of context word vectors, to hopefully cluster the context words since the vMF density retains the cosine similarity. Although the Skip-Gram-based methods attracted extensive attention, they were criticized for their inability to capture polysemy (Pilehvar and Collier, 2016). A pioneered solution to this problem is the Multiple-Sense Skip-Gram model (Neelakantan et al., 2014), where word vectors in a context are first averaged then clustered with other contexts to obtain a sense representation for the pivot word. In the same vein, Iacobacci and 1 Our source code is made available at http:// github.com/somethingx02/topical_wordvec_ models. 2 Navigli (2019) leveraged sense tags annotated by BabelNet (Navigli and Ponzetto, 2012) to jointly learn word and sense representations in the SkipGram manner that the context words are parameterized via a shared look-up table"
2020.tacl-1.31,N18-1092,0,0.0534498,"Missing"
2020.tacl-1.31,D14-1113,0,0.0393971,"h the Euclidean distance was substituted with Wasserstein distance. Jameel and Schockaert (2019) rewrote the NMF objective as a cumulative product of normal distributions, in which each factor is multiplied by a von Mises-Fisher (vMF) distribution of context word vectors, to hopefully cluster the context words since the vMF density retains the cosine similarity. Although the Skip-Gram-based methods attracted extensive attention, they were criticized for their inability to capture polysemy (Pilehvar and Collier, 2016). A pioneered solution to this problem is the Multiple-Sense Skip-Gram model (Neelakantan et al., 2014), where word vectors in a context are first averaged then clustered with other contexts to obtain a sense representation for the pivot word. In the same vein, Iacobacci and 1 Our source code is made available at http:// github.com/somethingx02/topical_wordvec_ models. 2 Navigli (2019) leveraged sense tags annotated by BabelNet (Navigli and Ponzetto, 2012) to jointly learn word and sense representations in the SkipGram manner that the context words are parameterized via a shared look-up table and sent to a BiLSTM to match the pivot word vector. There have also been Bayesian extensions of the Sk"
2020.tacl-1.31,I11-1127,0,0.0835906,"Missing"
2020.tacl-1.31,S07-1044,0,0.0562565,"xtualized word vectors. Among the topicdependent word embeddings, JTW built on VAE appears to be more effective than the PLSAbased STE and the mixed membership model MMSG, achieving the best overall score when averaging the evaluation results across all the seven benchmarking datasets. The small standard deviation of JTW indicates that the performance is consistent across multiple runs. 5.2 Lexical Substitution While the word similarity tasks focus more on the general meaning of a word (since word pairs are presented without context), in this section, we turn to the lexical substitution task (Yuret, 2007; Thater et al., 2011), which was designed to evaluate the word-embedding learning methods regarding their ability to disambiguate word senses. The lexical substitution task can be described by the following scenario: Given a sentence and one of its member words, find the most related replacement from a list of candidate words. As stated in Thater et al. (2011), a good lexical substitution should not only capture the relatedness between the candidate word and the original word, but also imply the correctness with respect to the context. Following Braˇzinskas et al. (2018), we derive the settin"
2021.acl-long.125,P19-1470,0,0.026599,"019) built ATOMIC, a knowledge graph centered on events rather than entities. Owing to the expressiveness of events and ameliorated relation types, using ATOMIC achieved competitive results against human evaluation in the task of If-Then reasoning. Alongside the development of knowledge bases, recent years have witnessed the thrive of new methods for training language models from large-scale text corpora as implicit knowledge base. As has been shown in (Petroni et al., 2019), pre-trained language models perform well in recalling relational knowledge involving triplet relations about entities. Bosselut et al. (2019) proposed COMmonsEnse Transformers (C OMET) which learns to generate commonsense descriptions in natural language by fine-tuning pre-trained language models on existing commonsense knowledge bases such as ATOMIC. Compared with extractive methods, language models fine-tuned on knowledge bases have a distinctive advantage of being able to generate knowledge for unseen events, which is of great importance for tasks which require the incorporation of commonsense knowledge such as emotion detection in dialogues. 3 3.1 Methodology Problem Setup A dialogue is defined as a sequence of utterances {x1 ,"
2021.acl-long.125,K16-1002,0,0.0173343,"ne tuned on dialogues, and the knowledgeaware transformer for emotion label sequence prediction for a given dialogue. In what follows, we will describe each of the components in turn. 3.2 Topic Representation Learning We propose to insert a topic layer into an existing language model and fine-tune the pre-trained language model on the conversational text for topic representation learning. Topic models, often formulated as latent variable models, play a vital role in dialogue modeling (Serban et al., 2017) due to the explicit modeling of ‘high-level syntactic features such as style and topic’ (Bowman et al., 2016). Despite the tremendous success of applying topic modeling in dialogue generation (Sohn et al., 2015; Shen et al., 2018; Gao et al., 2019), there is scarce work exploiting latent variable models for dialogue emotion detection. To this end, we borrow the architecture from VHRED (Serban et al., 2017) for topic discovery, with the key modification that both the encoder RNN and decoder RNN are replaced by layers of a pre-trained language model. Furthermore, we use a transformer multi-head attention in replacement of the LSTM to model the dependence between the latent topic vectors. Unlike VHRED,"
2021.acl-long.125,P19-1563,0,0.0256549,"ur. Other utterances are labeled as ‘Neutral’. In (a), utterances discussing food and restaurant are more likely carrying positive sentiment. In (b), the similar utterance, ‘He was doing so well’, expressed different emotions depending on its associated topic. Introduction The abundance in dialogues extracted from online conversations and TV series provides unprecedented opportunity to train models for automatic emotion detection, which are important for the development of empathetic conversational agents or chat bots for psychotherapy (Hsu and Ku, 2018; Jiao et al., 2019; Zhang et al., 2019; Cao et al., 2019). However, it is challenging to capture the contextual semantics of personal experience described in one’s utterance. For example, the emotion of the sentence “I just passed the exam” can be either happy or sad depending on the expectation of the subject. There are strands of works utilizing the dialogue context to enhance the utterance representation (Jiao et al., 2019; Zhang et al., 2019; Majumder et al., 2019), where influences from historical utterances were handled by recurrent units, and attention signals were further introduced to intensify the positional order of the utterances. Despit"
2021.acl-long.125,D19-1198,0,0.146466,"neural variational inference form named Variational Autoencoder (VAE) (Kingma and Welling, 2014), has been studied extensively to learn thematic representations of individual documents (Miao et al., 2016; Srivastava and Sutton, 2017; Rezaee and Ferraro, 2020). They have been successfully employed for dialogue generation to model thematic characteristics over dynamically evolving conversations. This line of work, which inlcudes approaches based on hierarchical recurrent VAEs (Serban et al., 2017; Park et al., 2018; Zeng et al., 2019) and conditional VAEs (Sohn et al., 2015; Shen et al., 2018; Gao et al., 2019), encodes each utterance with historical latent codes and autoregressively reconstructs the input sequence. On the other hand, pre-trained language models are used as embedding inputs to VAE-based mod1572 els (Peinelt et al., 2020; Asgari-Chenaghlu et al., 2020). Recent work by Li et al. (2020a) employs BERT and GPT-2 as the encoder-decoder structure of VAE. However, these models have to be either trained from scratch or built upon pre-trained embeddings. They therefore cannot be directly applied to the low-resource setting of dialogue emotion detection. 2014), in which the model consecutively"
2021.acl-long.125,2020.findings-emnlp.224,0,0.120366,"onvolutional Network (GCN). Meanwhile, Ghosal et al. (2019) extended the prior work (Majumder et al., 2019) by taking into account the intra-speaker dependency and relative position of the target and context within dialogues. Memory networks have been explored in (Jiao et al., 2020) to allow bidirectional influence between utterances. A similar idea has been explored by Li et al. (2020b). While the majority of works have been focusing on textual conversations, Zhong et al. (2019) enriched utterances with concept representations extracted from the ConceptNet (Speer et al., 2017). Ghosal et al. (2020) developed COSMIC which exploited ATOMIC (Sap et al., 2019) for the acquisition of commonsense knowledge. Different from existing approaches, we propose a topic-driven and knowledge-aware model built on a Transformer Encoder-Decoder structure for dialogue emotion detection. Latent Variable Models for Dialogue Context Modelling Latent variable models, normally described in their neural variational inference form named Variational Autoencoder (VAE) (Kingma and Welling, 2014), has been studied extensively to learn thematic representations of individual documents (Miao et al., 2016; Srivastava and"
2021.acl-long.125,D19-1015,0,0.0518611,"Missing"
2021.acl-long.125,N19-1037,0,0.342698,"face) emotions are highlighted in colour. Other utterances are labeled as ‘Neutral’. In (a), utterances discussing food and restaurant are more likely carrying positive sentiment. In (b), the similar utterance, ‘He was doing so well’, expressed different emotions depending on its associated topic. Introduction The abundance in dialogues extracted from online conversations and TV series provides unprecedented opportunity to train models for automatic emotion detection, which are important for the development of empathetic conversational agents or chat bots for psychotherapy (Hsu and Ku, 2018; Jiao et al., 2019; Zhang et al., 2019; Cao et al., 2019). However, it is challenging to capture the contextual semantics of personal experience described in one’s utterance. For example, the emotion of the sentence “I just passed the exam” can be either happy or sad depending on the expectation of the subject. There are strands of works utilizing the dialogue context to enhance the utterance representation (Jiao et al., 2019; Zhang et al., 2019; Majumder et al., 2019), where influences from historical utterances were handled by recurrent units, and attention signals were further introduced to intensify the pos"
2021.acl-long.125,2020.emnlp-main.378,0,0.150933,"ted a hierarchical neural network model that comprises two GRUs for the modelling of tokens and utterances respectively. Zhang et al. (2019) explicitly modelled the emotional dependencies on context and speakers using a Graph Convolutional Network (GCN). Meanwhile, Ghosal et al. (2019) extended the prior work (Majumder et al., 2019) by taking into account the intra-speaker dependency and relative position of the target and context within dialogues. Memory networks have been explored in (Jiao et al., 2020) to allow bidirectional influence between utterances. A similar idea has been explored by Li et al. (2020b). While the majority of works have been focusing on textual conversations, Zhong et al. (2019) enriched utterances with concept representations extracted from the ConceptNet (Speer et al., 2017). Ghosal et al. (2020) developed COSMIC which exploited ATOMIC (Sap et al., 2019) for the acquisition of commonsense knowledge. Different from existing approaches, we propose a topic-driven and knowledge-aware model built on a Transformer Encoder-Decoder structure for dialogue emotion detection. Latent Variable Models for Dialogue Context Modelling Latent variable models, normally described in their n"
2021.acl-long.125,I17-1099,0,0.0294016,"the subsequent utterances are unseen when predicting an emotion of the current utterance. As for the decoder, the output of the previous decoder block is input as a query to the self-attention layer. The training loss for the classifier is the negative log-likelihood expressed as: L=− N X log pθ (yn |u≤n , y<n ), n=1 where θ denotes the trainable parameters. 4 Experimental Setup In this section, we present the details of the datasets used, the methods for comparison, and the implementation details of our models. Datasets We use the following datasets for experimental evaluation: DailyDialog (Li et al., 2017) is collected from daily communications. It takes the Ekman’s six emotion types (Ekman, 1993) as the annotation protocol, that is, it annotates an utterance with one of the six basic emotions: anger, disgust, fear, happiness, sadness, or surprise. Those showing ambiguous emotions are annotated as neutral. MELD (Poria et al., 2019) is constructed from scripts of ‘Friends’, a TV series on urban life. Same as DailyDialog, the emotion label falls into Ekman’s six emotion types, or neutral. IEMOCAP (Busso et al., 2008) is built with subtitles from improvised videos. Its emotion labels are happy, sa"
2021.acl-long.125,2021.ccl-1.108,0,0.0951876,"Missing"
2021.acl-long.125,N18-1162,0,0.0504565,"Missing"
2021.acl-long.125,2020.acl-main.630,0,0.0398743,", 2017; Rezaee and Ferraro, 2020). They have been successfully employed for dialogue generation to model thematic characteristics over dynamically evolving conversations. This line of work, which inlcudes approaches based on hierarchical recurrent VAEs (Serban et al., 2017; Park et al., 2018; Zeng et al., 2019) and conditional VAEs (Sohn et al., 2015; Shen et al., 2018; Gao et al., 2019), encodes each utterance with historical latent codes and autoregressively reconstructs the input sequence. On the other hand, pre-trained language models are used as embedding inputs to VAE-based mod1572 els (Peinelt et al., 2020; Asgari-Chenaghlu et al., 2020). Recent work by Li et al. (2020a) employs BERT and GPT-2 as the encoder-decoder structure of VAE. However, these models have to be either trained from scratch or built upon pre-trained embeddings. They therefore cannot be directly applied to the low-resource setting of dialogue emotion detection. 2014), in which the model consecutively consumes an utterance xn and predicts the emotion label yn based on the earlier utterances and their associated predicted emotion labels. The joint probability of emotion labels for a dialogue is: Pθ (y1:N |x1:N ) = N Y Pθ (yn |x"
2021.acl-long.125,D19-1250,0,0.0655997,"Missing"
2021.acl-long.125,P19-1050,0,0.0283503,"enotes the trainable parameters. 4 Experimental Setup In this section, we present the details of the datasets used, the methods for comparison, and the implementation details of our models. Datasets We use the following datasets for experimental evaluation: DailyDialog (Li et al., 2017) is collected from daily communications. It takes the Ekman’s six emotion types (Ekman, 1993) as the annotation protocol, that is, it annotates an utterance with one of the six basic emotions: anger, disgust, fear, happiness, sadness, or surprise. Those showing ambiguous emotions are annotated as neutral. MELD (Poria et al., 2019) is constructed from scripts of ‘Friends’, a TV series on urban life. Same as DailyDialog, the emotion label falls into Ekman’s six emotion types, or neutral. IEMOCAP (Busso et al., 2008) is built with subtitles from improvised videos. Its emotion labels are happy, sad, neutral, angry, excited and frustrated. EmoryNLP (Zahiri and Choi, 2018)5 is also built with conversations from ‘Friends’ TV series, but with a slightly different annotation scheme in which disgust, anger and surprise become peaceful, mad and powerful, respectively. Following Zhong et al. (2019) and Ghosal et al. (2020), the ‘n"
2021.acl-long.125,D19-1410,0,0.0148651,"another. ATOMIC thus encodes triples such as hevent, relation type, eventi. There are a total of nine relation types, of which three are used: xIntent, the intention of the subject (e.g., ‘to get a raise’), xReact, the reaction of the subject (e.g., ‘be tired’), and oReact, the reaction of the object (e.g., ‘be worried’), since they are defined as the mental states of an event (Sap et al., 2019). Given an utterance xn , we can compare it with every node in the knowledge graph, and retrieve the most similar one. The method for computing the similarity between an utterance and events is SBERT (Reimers and Gurevych, 2019). We extract the top-K events, and obtain their intentions and reactions, which are denoted as sR oR {esI n,k , en,k , en,k }, k = 1, . . . , K. On the other hand, there is a knowledge gen2 https://homes.cs.washington.edu/ msap/atomic/ ˜ eration model, called C OMET3 , which is trained on ATOMIC. It can take xn as input and generate the knowledge with the desired event relation types specified (e.g., xIntent, xReact or oReact). The generated knowledge can be unseen in ATOMIC since C OMET is essentially a finetuned language model. We use C OMET to generate the K most likely events, each with re"
2021.acl-long.125,D19-1124,0,0.0212259,"Dialogue Context Modelling Latent variable models, normally described in their neural variational inference form named Variational Autoencoder (VAE) (Kingma and Welling, 2014), has been studied extensively to learn thematic representations of individual documents (Miao et al., 2016; Srivastava and Sutton, 2017; Rezaee and Ferraro, 2020). They have been successfully employed for dialogue generation to model thematic characteristics over dynamically evolving conversations. This line of work, which inlcudes approaches based on hierarchical recurrent VAEs (Serban et al., 2017; Park et al., 2018; Zeng et al., 2019) and conditional VAEs (Sohn et al., 2015; Shen et al., 2018; Gao et al., 2019), encodes each utterance with historical latent codes and autoregressively reconstructs the input sequence. On the other hand, pre-trained language models are used as embedding inputs to VAE-based mod1572 els (Peinelt et al., 2020; Asgari-Chenaghlu et al., 2020). Recent work by Li et al. (2020a) employs BERT and GPT-2 as the encoder-decoder structure of VAE. However, these models have to be either trained from scratch or built upon pre-trained embeddings. They therefore cannot be directly applied to the low-resource"
2021.acl-long.125,D19-1016,0,0.0267054,"Missing"
2021.acl-long.128,D14-1059,0,0.0325533,"19; Zhou ∗ corresponding author et al., 2019b). Moreover, as shown in Vosoughi et al. (2018), compared with truth, misinformation diffuses significantly farther, faster, and deeper in all genres. Therefore, there is an urgent need for quickly identifying the misinformation spread on the web. To solve this problem, we focus on the fact verification task (Thorne et al., 2018), which aims to automatically evaluate the veracity of a given claim based on the textual evidence retrieved from external sources. Recent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2"
2021.acl-long.128,2020.emnlp-main.200,0,0.0295103,"pieces of evidence (T Cee ); 2) topical consistency between the claim and each evidence (T Cce ). Specifically, to incorporate the topical coherence among multiple pieces of evidence into our model, we disregard the order of evidence and treat each evidence independently. Then we utilize the multihead attention (Vaswani et al., 2017) without position embedding to generate the new topic representation of evidence tˆe based on the sentence-level topic representation te ∈ RN ×K of the retrieved evidence for a given claim. tˆe = multihead(te ) (4) Moreover, we utilize the co-attention mechanism (Chen and Li, 2020) to weigh each evidence based on the topic consistency between the claim and the evidence. Given the sentence-level topic representation tc for claim and te for the corresponding evidence, the co-attention attends to the claim and the evidence simultaneously. We first compute the proximity matrix F ∈ RN , Coherence-based Topic Attention Based on the observation as illustrated in Figure 1, we as1615 F = tanh(tc Wl tTe ), (5) where Wl ∈ RK×K is the learnable weight matrix. The proximity matrix can be viewed as a transformation from the claim attention space to the evidence attention space. Then"
2021.acl-long.128,P17-1152,0,0.034673,"into the document retrieval phase and the evidence selection phase to shrink the search space of evidence (Thorne et al., 2018). In the document retrieval phase, researchers typically reuse the top performing approaches in the FEVER1.0 challenge to extract the documents with high relevance for a given claim (Hanselowski et al., 2018b; Yoneda et al., 2018; Nie et al., 2019a). In the evidence selection phase, to select relevant sentences, researchers generally train the classification models or rank models based on the similarity between the claim and each sentence from the retrieved documents (Chen et al., 2017; Stammbach and Neu1613 mann, 2019; Soleimani et al., 2020; Wadden et al., 2020; Zhong et al., 2020; Zhou et al., 2019a). Many fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018). Typically, these approaches contain the representation learning process and evidence aggregation process. Hanselowski et al. (2018b) and Nie et al. (2019a) concatenate all pieces of evidence as input and use the max pooling to aggregate the information for claim verification via"
2021.acl-long.128,N19-1423,0,0.0577162,"ng multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining the semantic representation of each evidence-claim pair through pre-trained language models (Devlin et al., 2019) or graph-based models (Velickovic et al., 2018), which largely overlooked the topical consistency between claim and evidence. For example in Figure 1, given the claim “A high school student named Cole Withrow was 1612 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1612–1622 August 1–6, 2021. ©2021 Association for Computational Linguistics Verdict: SUPPORTS Claim A high school student named Cole Withrow was charged for leaving an unloaded shotgun in his vehicle while parki"
2021.acl-long.128,N18-1132,0,0.039665,"Missing"
2021.acl-long.128,C18-1158,0,0.372571,"e et al., 2018), which aims to automatically evaluate the veracity of a given claim based on the textual evidence retrieved from external sources. Recent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evide"
2021.acl-long.128,K19-1046,0,0.0133883,"merXH (Zhao et al., 2020), KGAT (Liu et al., 2020) and DREAM (Zhong et al., 2020)). Implementation Details We describe our implementation details in this section. Document retrieval takes a claim along with a collection of documents as the input, then returns N most relevant documents. For the FEVER dataset, following Hanselowski et al. (2018a), we adopt the entity linking method since the title of a Wikipedia page can be viewed as an entity and can be linked easily with the extracted entities from 1 https://github.com/sheffieldnlp/fever-scorer the claim. For the UKP Snopes dataset, following Hanselowski et al. (2019), we adopt the tf-idf method where the tf-idf similarity between claim and concatenation of all sentences of each Snopes page is computed, and then the 5 highest ranked documents are taken as retrieved documents. Evidence selection retrieves the related sentences from retrieved documents in ranking setting. For the FEVER dataset, we follow the previous method from Zhao et al. (2020). Taking the concatenation of claim and each sentence as input, the [CLS] token representation is learned through BERT which is then used to learn a ranking score through a linear layer. The hinge loss is used to op"
2021.acl-long.128,W18-5516,0,0.340883,"e et al., 2018), which aims to automatically evaluate the veracity of a given claim based on the textual evidence retrieved from external sources. Recent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evide"
2021.acl-long.128,2020.acl-main.655,0,0.197187,"a given claim based on the textual evidence retrieved from external sources. Recent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining t"
2021.acl-long.128,W18-5526,0,0.0426342,"Missing"
2021.acl-long.128,P19-1244,0,0.19413,"n Vosoughi et al. (2018), compared with truth, misinformation diffuses significantly farther, faster, and deeper in all genres. Therefore, there is an urgent need for quickly identifying the misinformation spread on the web. To solve this problem, we focus on the fact verification task (Thorne et al., 2018), which aims to automatically evaluate the veracity of a given claim based on the textual evidence retrieved from external sources. Recent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-leve"
2021.acl-long.128,D19-1258,0,0.0515258,"Missing"
2021.acl-long.128,D16-1244,0,0.0953107,"Missing"
2021.acl-long.128,D19-6616,0,0.0422258,"Missing"
2021.acl-long.128,2020.emnlp-main.627,0,0.415201,"xtual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining the semantic representation of each evidence-claim pair through pre-trained language models (Devlin et al., 2019) or graph-based models (Velickovic et al., 2018), which largely overlooked the topical consistenc"
2021.acl-long.128,N18-1074,0,0.0487413,"Missing"
2021.acl-long.128,2020.emnlp-main.609,0,0.0416355,"Missing"
2021.acl-long.128,D18-1350,0,0.0243525,"j|i · oj 6: end for 7: Return o ∈ RM ×do , ρj = |oj |j=1:M 4 To model the implicit stances of evidence toward claim, we incorporate the capsule network (Sabour et al., 2017) into our model. As illustrated in Figure 2, we concatenate both the semantic representation S and the topical representation A to form the low-level evidence capsules ui = [ai ; si ]|N i=1 ∈ do denote the high-level class Rde . Let oj |M ∈ R j=1 capsules, where M denotes the number of classes. The capsule network models the relationship between the evidence capsules and the class capsules by the dynamic routing mechanism (Yang et al., 2018), which can be viewed as the implicit stances of each evidence toward three classes. (9) where Wj,i ∈ Rdo ×de denotes the transformation matrix from the evidence capsule ui to the class capsule oj . Each class capsule aggregates all of the evidence capsules by a weighted summation over all corresponding predicted vectors: (7) where w ∈ R1×l is the learnable weight, αe ∈ RN is the attention score of each piece of evidence for the claim. Eventually, the topic representation A ∈ RN ×K can be computed as follows, A = αe tˆe , Formally, let uj|i be the predicted vector from the evidence capsule ui"
2021.acl-long.128,D18-1010,0,0.0173069,"t al., 2020; Zhou et al., 2019a). Many fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018). Typically, these approaches contain the representation learning process and evidence aggregation process. Hanselowski et al. (2018b) and Nie et al. (2019a) concatenate all pieces of evidence as input and use the max pooling to aggregate the information for claim verification via the enhanced sequential inference model (ESIM) (Chen et al., 2017). In a similar vein, Yin and Roth (2018) incorporate the identification of evidence to further improve claim verification using ESIM with different granularity levels. Ma et al. (2019) leverage the co-attention mechanism between claim and evidence to generate claim-specific evidence representations which are used to infer the claim. Benefiting from the development of pre-trained language models, Zhou et al. (2019a) are the first to learn evidence representations by BERT (Devlin et al., 2019), which are subsequently used in a constructed evidence graph for claim inference by aggregating all claim-evidence pairs. Zhong et al. (2020) f"
2021.acl-long.128,W18-5515,0,0.0774096,"In general, fact verification is a task to assess the authenticity of a claim backed by a validated corpus of documents, which can be divided into two stages: fact extraction and claim verification (Zhou and Zafarani, 2020). Fact extraction can be further split into the document retrieval phase and the evidence selection phase to shrink the search space of evidence (Thorne et al., 2018). In the document retrieval phase, researchers typically reuse the top performing approaches in the FEVER1.0 challenge to extract the documents with high relevance for a given claim (Hanselowski et al., 2018b; Yoneda et al., 2018; Nie et al., 2019a). In the evidence selection phase, to select relevant sentences, researchers generally train the classification models or rank models based on the similarity between the claim and each sentence from the retrieved documents (Chen et al., 2017; Stammbach and Neu1613 mann, 2019; Soleimani et al., 2020; Wadden et al., 2020; Zhong et al., 2020; Zhou et al., 2019a). Many fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018). Typically, these a"
2021.acl-long.128,2020.acl-main.549,0,0.348819,"here the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining the semantic representation of each evidence-claim pair through pre-trained language models (Devlin et al., 2019) or graph-based models (Velickovic et al., 2018), which largely overlooked the topical consistency between claim and evidence. For example in Figu"
2021.acl-long.128,P19-1085,0,0.354917,"Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently. However, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining the semantic representation of each evidence-claim pair through pre-trained language models (Devlin et al., 2019) or graph-based models (Velickovic et al., 2018), which largely over"
2021.acl-long.261,D18-1066,0,0.0214395,"ntations of textual units relying on rule-based systems (Lee et al., 2010) or incorporated commonsense knowledge bases (Gao et al., 2015) for emotion cause extraction. Machine learning methods leveraged text features (Gui et al., 2017) and combined them with multi-kernel Support Vector Machine (SVM) (Xu et al., 2017). More recent works developed neural architectures to generate effective semantic features. Cheng et al. (2017b) employed LSTM models, Gui et al. (2017) made use of memory networks, while Li et al. (2018) devised a Convolutional Neural Network (CNN) with a co-attention mechanism. (Chen et al., 2018) used the emotion classification task to enhance cause extraction results. Position-aware Models. More recent methodologies have started to explicitly leverage the positions of cause clauses with respect to the emotion clause. A common strategy is to concatenate the clause relative position embedding with the candidate clause representation (Ding et al., 2019; Xia et al., 2019; Li et al., 2019). The Relative Position Augmented with Dynamic Global Labels (PAE-DGL) (Ding et al., 2019) reordered clauses based on their distances from the target emotion clause, and propagated the information of sur"
2021.acl-long.261,D17-1167,1,0.887786,"ith the existing state-of-the-art methods on the original ECE dataset, and is more robust when evaluating on the adversarial examples. 2 Related Work The presented work is closely related to two lines of research in emotion cause extraction: positioninsensitive and position-aware models. Position-insensitive Models. A more traditional line of research exploited structural representations of textual units relying on rule-based systems (Lee et al., 2010) or incorporated commonsense knowledge bases (Gao et al., 2015) for emotion cause extraction. Machine learning methods leveraged text features (Gui et al., 2017) and combined them with multi-kernel Support Vector Machine (SVM) (Xu et al., 2017). More recent works developed neural architectures to generate effective semantic features. Cheng et al. (2017b) employed LSTM models, Gui et al. (2017) made use of memory networks, while Li et al. (2018) devised a Convolutional Neural Network (CNN) with a co-attention mechanism. (Chen et al., 2018) used the emotion classification task to enhance cause extraction results. Position-aware Models. More recent methodologies have started to explicitly leverage the positions of cause clauses with respect to the emotio"
2021.acl-long.261,D16-1170,1,0.805876,"Pos RB EMOCause Ngrams+SVM Multi-Kernel CNN CANN Memnet 67.47 26.72 42.00 65.88 62.15 77.21 70.76 42.87 71.30 43.75 69.27 59.44 68.91 68.38 52.43 38.87 42.85 67.52 60.76 72.66 69.55 W. Pos HCS MANN LambdaMART PAE-DGL RTHN 73.88 78.43 77.20 76.19 76.97 71.54 75.87 74.99 69.08 76.62 72.69 77.06 76.08 72.42 76.77 Our KAG : w/o R-GCNs : w/o K-Edge : w/o S-Edge 79.12 73.68 75.67 76.34 75.81 72.76 72.63 75.46 77.43 73.14 74.12 75.88 4.1 Table 1: Results of different models on the ECE dataset. Our model achieves the best Precision and F1 score. Dataset and Evaluation Metrics The evaluation dataset (Gui et al., 2016) consists of 2,105 documents from SINA city news. As the dataset size is not large, we perform 10-fold cross-validation and report results on three standard metrics, i.e. Precision (P), Recall (R), and F1-Measure, all evaluated at the clause level. Baselines We compare our model with the position-insensitive and position-aware baselines: RB (Lee et al., 2010) and EMOCause (Russo et al., 2011) are rules-based methods. Multi-Kernel (Gui et al., 2016) and Ngrams+SVM (Xu et al., 2017) leverage Support Vector Machines via different textual feature to train emotion cause classifiers. CNN (Kim, 2014)"
2021.acl-long.261,C18-1114,0,0.0429829,"Missing"
2021.acl-long.261,D14-1181,0,0.00354723,"al., 2016) consists of 2,105 documents from SINA city news. As the dataset size is not large, we perform 10-fold cross-validation and report results on three standard metrics, i.e. Precision (P), Recall (R), and F1-Measure, all evaluated at the clause level. Baselines We compare our model with the position-insensitive and position-aware baselines: RB (Lee et al., 2010) and EMOCause (Russo et al., 2011) are rules-based methods. Multi-Kernel (Gui et al., 2016) and Ngrams+SVM (Xu et al., 2017) leverage Support Vector Machines via different textual feature to train emotion cause classifiers. CNN (Kim, 2014) and CANN (Li et al., 2018) are vanilla or attention-enhanced approaches. Memnet (Gui et al., 2017) uses a deep memory network to re-frame ECE as a question-answering task. Position-aware models use the relative position embedding to enhance the semantic features. HCS (Yu et al., 2019) uses separate hierarchical and attention module to obtain context and information. Besides that, PAE-DGL (Ding et al., 2019) and RTHN (Xia et al., 2019) use similar Global Prediction Embedding (GPE) to twist the clauses’ first-round predictions. MANN (Li et al., 2019) performs multi-head attention in CNN to join"
2021.acl-long.261,W10-0206,0,0.0816473,"base between clauses. Node representations are updated using the extended Relation-GCN. • Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust when evaluating on the adversarial examples. 2 Related Work The presented work is closely related to two lines of research in emotion cause extraction: positioninsensitive and position-aware models. Position-insensitive Models. A more traditional line of research exploited structural representations of textual units relying on rule-based systems (Lee et al., 2010) or incorporated commonsense knowledge bases (Gao et al., 2015) for emotion cause extraction. Machine learning methods leveraged text features (Gui et al., 2017) and combined them with multi-kernel Support Vector Machine (SVM) (Xu et al., 2017). More recent works developed neural architectures to generate effective semantic features. Cheng et al. (2017b) employed LSTM models, Gui et al. (2017) made use of memory networks, while Li et al. (2018) devised a Convolutional Neural Network (CNN) with a co-attention mechanism. (Chen et al., 2018) used the emotion classification task to enhance cause e"
2021.acl-long.261,D19-1563,1,0.88203,"ent representation D. (c) Clause Graph Update. A clause graph is built with the clause representations Cˆi used to ˆi and the emotion clause C ˆE are initialise the graph nodes. The K-Edge weight eiE between a candidate clause C measured by their distance along their path si . (d) Classification. Node representation hi of a candidate clause Ci is concatenated with the emotion node representation hE , and then fed to a softmax layer to yield the clause ˆi. classification result y The generated representations are fed to a CNN layer for emotion cause extraction. The Hierarchical Neural Network (Fan et al., 2019) aimed at narrowing the gap between the prediction distribution p and the true distribution of the cause clause relative positions. 3 Knowledge-Aware Graph (KAG) Model for Emotion Cause Extraction We first define the Emotion Cause Extraction (ECE) task here. A document D contains N clauses D = {Ci }N i=1 , one of which is annotated as an emotion clause CE with a pre-defined emotion class label, Ew . The ECE task is to identify one or more cause clauses, Ct , 1 ≤ t ≤ N , that trigger the emotion expressed in CE . Note that the emotion clause itself can be a cause clause. We propose a Knowledge-"
2021.acl-long.261,D18-1506,0,0.0464947,"Position-insensitive Models. A more traditional line of research exploited structural representations of textual units relying on rule-based systems (Lee et al., 2010) or incorporated commonsense knowledge bases (Gao et al., 2015) for emotion cause extraction. Machine learning methods leveraged text features (Gui et al., 2017) and combined them with multi-kernel Support Vector Machine (SVM) (Xu et al., 2017). More recent works developed neural architectures to generate effective semantic features. Cheng et al. (2017b) employed LSTM models, Gui et al. (2017) made use of memory networks, while Li et al. (2018) devised a Convolutional Neural Network (CNN) with a co-attention mechanism. (Chen et al., 2018) used the emotion classification task to enhance cause extraction results. Position-aware Models. More recent methodologies have started to explicitly leverage the positions of cause clauses with respect to the emotion clause. A common strategy is to concatenate the clause relative position embedding with the candidate clause representation (Ding et al., 2019; Xia et al., 2019; Li et al., 2019). The Relative Position Augmented with Dynamic Global Labels (PAE-DGL) (Ding et al., 2019) reordered clause"
2021.acl-long.261,D19-1282,0,0.0277055,"he annotated emotion word or the emotion class label, Ew , in the emotion clause. More concretely, for a candidate clause, we first perform word segmentation using the Chinese segmentation tool, Jieba2 , and then extract the top three keywords ranked by Text-Rank3 . Based on the findings in (Fan et al., 2019) that sentiment descriptions can be relevant to the emotion cause, we also include adjectives in the keywords set. We regard each keyword in a candidate clause as a head entity, eh , and the emotion word or the emotion class label in the emotion clause as the tail entity, et . Similar to (Lin et al., 2019), we apply networkx4 to perform a depth-first search on the ConceptNet to identify the paths which start from eh and end at et , and only keep the paths which contain less than two intermediate entities. This is because shorter paths are more likely to offer reliable reasoning evidence (Xiong et al., 2017). Since not all relations in ConceptNet are related to or indicative of causal relations, we further remove the paths which contain any of these four relations: ‘antonym’, ‘distinct from’, ‘not desires’, and ‘not capable of ’. Finally, we order paths by their lengths in an ascending order and"
2021.acl-long.261,D15-1166,0,0.00791143,"on index r1 can then be sampled from the Gaussian distribution. As the sampled value is continuous, we round the value to its nearest integer: r1 ← bge, g v Gaussian(µ, σ 2 ). (7) To locate the least likely cause clause, we propose to choose the value for r2 according to the attention score between a candidate clause and the emotion clause. Our intuition is that if the emotion clause has a lower score attended to a candidate clause, then it is less likely to be the cause clause. We use an existing emotion cause extraction model to generate contextual representations and use the Dot-Attention (Luong et al., 2015) to measure the similarity between each candidate clause and the emotion clause. We then select the index i which gives the lowest attention score and assign it to r2 : r2 = arg min{λi }N i=1 , i ˆi , CˆE ), λi = Dot-Att.(C (8) ˆi is the representation of the i-th candidate where C clause, CˆE is the representation of the emotion clause, and N denotes a total of N clauses in a document. Here, we use existing ECE models as different discriminators to generate different adversarial samples.8 The desirable adversarial samples will fool the discriminator to predict the inverse label. We use leave-"
2021.acl-long.261,2020.starsem-1.7,0,0.0612517,"Missing"
2021.acl-long.261,P18-1213,0,0.0309593,"Missing"
2021.acl-long.261,D17-1060,0,0.020533,"2019) that sentiment descriptions can be relevant to the emotion cause, we also include adjectives in the keywords set. We regard each keyword in a candidate clause as a head entity, eh , and the emotion word or the emotion class label in the emotion clause as the tail entity, et . Similar to (Lin et al., 2019), we apply networkx4 to perform a depth-first search on the ConceptNet to identify the paths which start from eh and end at et , and only keep the paths which contain less than two intermediate entities. This is because shorter paths are more likely to offer reliable reasoning evidence (Xiong et al., 2017). Since not all relations in ConceptNet are related to or indicative of causal relations, we further remove the paths which contain any of these four relations: ‘antonym’, ‘distinct from’, ‘not desires’, and ‘not capable of ’. Finally, we order paths by their lengths in an ascending order and choose the top K paths as the result for each candidateemotion clause pair5 . An example is shown in Figure 3. The 5-th clause is annotated as the emotion clause and the emotion class label is ‘happiness’. For the keyword, ‘adopted’, in the first clause, we show two example paths extracted from ConceptNet"
2021.acl-long.261,W11-1720,0,0.0798584,"Missing"
2021.acl-long.261,D19-1221,0,0.0633962,"Missing"
2021.acl-long.261,P19-1096,0,0.0353849,"Missing"
2021.eacl-main.169,D19-1005,0,0.0494712,"Missing"
2021.eacl-main.199,C16-1091,0,0.0262601,"ics generated by a vanilla Poisson factorisation model can be considered as parent topics, while polarity-bearing subtopics generated by BTM can be considered as child topics. Ideally, we would like the parent topics to be either neutral or carrying a mixed sentiment which would facilitate the learning of polarised sub-topics better. In cases when parent topics carry either strongly positive or strongly negative sentiment signals, BTM would fail to produce polarity-varying subtopics. One possible way is to employ earlier filtering of topics with strong polarities. For example, topic labeling (Bhatia et al., 2016) could be employed to obtain a rough estimate of initial topic polarities; these labels would be in turn used for filtering out topics carrying strong sentiment polarities. Although the adversarial mechanism tends to be robust with respect to class imbalance, the disproportion of available reviews with different polarities could hinder the model performance. One promising approach suitable for the BTM adversarial mechanism would consist in decoupling the representation learning and the classification, as suggested in Kang et al. (2020), preserving the original data distribution used by the mod"
2021.eacl-main.199,P18-1189,0,0.0606179,"Missing"
2021.eacl-main.199,P17-1036,0,0.0282912,"robability under the corresponding topic, summing over all topics. Polarity-bearing Topics Models Early approaches to polarity-bearing topics extraction were built on LDA in which a word is assumed to be generated from a corpus-wide sentiment-topicword distributions (Lin and He, 2009). In order to be able to separate topics bearing different polarities, word prior polarity knowledge needs to be 2342 incorporated into model learning. In recent years, the neural network based topic models have been proposed for many NLP tasks, such as information retrieval (Xie et al., 2015), aspect extraction (He, 2017) and sentiment classification (He et al., 2018). Most of them are built upon Variational Autoencode (VAE) (Kingma and Welling, 2014) which constructs a neural network to approximate the topic-word distribution in probabilistic topic models (Srivastava and Sutton, 2017; Sønderby et al., 2016; Bouchacourt et al., 2018). Intuitively, training the VAE-based supervised neural topic models with class labels (Chaidaroon and Fang, 2017; Huang et al., 2018; Gui et al., 2020) can introduce sentiment information into topic modelling, which may generate better features for sentiment classification. ing se"
2021.eacl-main.199,C18-1096,0,0.0227153,", summing over all topics. Polarity-bearing Topics Models Early approaches to polarity-bearing topics extraction were built on LDA in which a word is assumed to be generated from a corpus-wide sentiment-topicword distributions (Lin and He, 2009). In order to be able to separate topics bearing different polarities, word prior polarity knowledge needs to be 2342 incorporated into model learning. In recent years, the neural network based topic models have been proposed for many NLP tasks, such as information retrieval (Xie et al., 2015), aspect extraction (He, 2017) and sentiment classification (He et al., 2018). Most of them are built upon Variational Autoencode (VAE) (Kingma and Welling, 2014) which constructs a neural network to approximate the topic-word distribution in probabilistic topic models (Srivastava and Sutton, 2017; Sønderby et al., 2016; Bouchacourt et al., 2018). Intuitively, training the VAE-based supervised neural topic models with class labels (Chaidaroon and Fang, 2017; Huang et al., 2018; Gui et al., 2020) can introduce sentiment information into topic modelling, which may generate better features for sentiment classification. ing set. (Wang et al., 2020) further extended the ATM"
2021.eacl-main.199,2020.emnlp-main.725,0,0.0424161,"17; Sønderby et al., 2016; Bouchacourt et al., 2018). Intuitively, training the VAE-based supervised neural topic models with class labels (Chaidaroon and Fang, 2017; Huang et al., 2018; Gui et al., 2020) can introduce sentiment information into topic modelling, which may generate better features for sentiment classification. ing set. (Wang et al., 2020) further extended the ATM model with a Bidirectional Adversarial Topic (BAT) model, using a bidirectional adversarial training to incorporate a Dirichlet distribution as prior and exploit the information encoded in word embeddings. Similarly, (Hu et al., 2020) builds on the aforementioned adversarial approach adding cycle-consistent constraints. Although the previous methods make use of adversarial mechanisms to approximate the posterior distribution of topics, to the best of our knowledge, none of them has so far used adversarial learning to lead the generation of topics based on their sentiment polarity and they do not provide any mechanism for smooth transitions between topics, as introduced in the presented Brand-Topic Model. 3 Market/Brand Topic Analysis The classic LDA can also be used to analyse market segmentation and brand reputation in va"
2021.eacl-main.199,D18-1494,0,0.0208474,"cent years, the neural network based topic models have been proposed for many NLP tasks, such as information retrieval (Xie et al., 2015), aspect extraction (He, 2017) and sentiment classification (He et al., 2018). Most of them are built upon Variational Autoencode (VAE) (Kingma and Welling, 2014) which constructs a neural network to approximate the topic-word distribution in probabilistic topic models (Srivastava and Sutton, 2017; Sønderby et al., 2016; Bouchacourt et al., 2018). Intuitively, training the VAE-based supervised neural topic models with class labels (Chaidaroon and Fang, 2017; Huang et al., 2018; Gui et al., 2020) can introduce sentiment information into topic modelling, which may generate better features for sentiment classification. ing set. (Wang et al., 2020) further extended the ATM model with a Bidirectional Adversarial Topic (BAT) model, using a bidirectional adversarial training to incorporate a Dirichlet distribution as prior and exploit the information encoded in word embeddings. Similarly, (Hu et al., 2020) builds on the aforementioned adversarial approach adding cycle-consistent constraints. Although the previous methods make use of adversarial mechanisms to approximate t"
2021.eacl-main.199,P19-1041,0,0.0203312,"pics, Gao et al. (2017) dynamically modelled users’ interested items for recommendation. Zhang et al. (2015) focused on brand topic tracking. They built a dynamic topic model to analyse texts and images posted on Twitter and track competitions in the luxury market among given brands, in which topic words were used to identify recent hot topics in the market (e.g. Rolex watch) and brands over topics were used to identify the market share of each brand. Adversarial Learning Several studies have explored the application of adversarial learning mechanics to text processing for style transferring (John et al., 2019), disentangling representations (John et al., 2019) and topic modelling (Masada and Takasu, 2018). In particular, Wang et al. (2019) has proposed an Adversarial-neural Topic Model (ATM) based on the Generative Adversarial Network (GAN) (Goodfellow et al., 2014), that employees an adversarial approach to train a generator network producing word distributions indistinguishable from topic distributions in the trainBrand-Topic Model (BTM) We propose a probabilistic model for monitoring the assessment of various brands in the beauty market from Amazon reviews. We extend the TextBased Ideal Point (T"
2021.emnlp-main.253,Q19-1013,0,0.0169005,"oaches attempted to incorporate external information, which can be further classified into two categories, using the document metadata To tackle the above limitations, in this paper, we and using label information. For approaches utilizpropose a novel neural network based approach ing metadata, Tang et al. (2015) proposed a neural for multi-label document classification using two network approach for sentiment analysis which inheterogeneous graphs. Specifically, a metadata het- corporates user and product meta information by erogeneous graph with four node types and five a vector space model. Kim et al. (2019) employed edge types is constructed to capture the metadata categorical metadata signals as additional features information and a label heterogeneous graph with to train a deep neural network classifier. (Zhang two edge types is constructed to capture both the et al., 2021) developed an approach called MATCH, label hierarchy and labels’ statistical dependencies. which pre-trained the embeddings of text and metaBoth graphs are learned using the heterogeneous data in the same space, and used the Transformer graph transformer. Moreover, to fully utilize the to capture the relationship between the"
2021.emnlp-main.253,Q19-1009,0,0.054481,"Missing"
2021.emnlp-main.253,D14-1162,0,0.0858002,"cation method with metadata-aware Transformer and label hierarchy. Evaluation Metrics Two widely used metrics, precision at top k (P @k) and Normalized Discounted Cumulative Gains at top k (nDCG@k), are used to evaluate the model performance 4 . P @k = k 1X yd,rank(i) . k i=1 k X yd,rank(i) DCG@k = , log(i + 1) (14) i=1 DCG@k nDCG@k = Pmin(k,||y ||) d 0 i=1 1 log(i+1) . Here, yd 2 {0, 1}|L |is the ground truth label vector of the document d, rank(i) is the index of the i-th highest predicted label . Parameter Setting For all methods, the embedding dimension k is set to 100, and GloVe.6B.100d (Pennington et al., 2014) is used to initialize word embeddings. For our method, we set the hidden vector dimension = 100, the number of the text 3 https://github.com/yuzhimanhua/MATCH https://github.com/yuzhimanhua/MATCH/ blob/master/deepxml/evaluation.py 3168 4 (a) MAG-CS (b) PubMed Figure 4: Ablation analysis of Comprehensive Label Information. encode layers L = 4, the threshold of label dependency = 0.3, and the number of attention heads 2. The training is performed using Adam (Kingma and Ba, 2014) with a batch size of 200 and a learning rate of 1e-3, and the maximum training epochs is 20. The compared methods use"
2021.emnlp-main.253,P15-1098,0,0.0281658,"ut text for each label. Chang et al. (2020) employed a pre-trained Transformer (Vaswani et al., 2017) to capture textual information for text classification. However, such methods ignore the information beyond text, which we believe is crucial for accurate multi-label document classification. Other approaches attempted to incorporate external information, which can be further classified into two categories, using the document metadata To tackle the above limitations, in this paper, we and using label information. For approaches utilizpropose a novel neural network based approach ing metadata, Tang et al. (2015) proposed a neural for multi-label document classification using two network approach for sentiment analysis which inheterogeneous graphs. Specifically, a metadata het- corporates user and product meta information by erogeneous graph with four node types and five a vector space model. Kim et al. (2019) employed edge types is constructed to capture the metadata categorical metadata signals as additional features information and a label heterogeneous graph with to train a deep neural network classifier. (Zhang two edge types is constructed to capture both the et al., 2021) developed an approach"
2021.emnlp-main.253,D19-1044,0,0.0711643,"antic encoder. Recently, Chang et al. (2020) proposed the X-Transformer model, a deep transformer model fine-tuned for MLDC. Different from the aforementioned approaches, 1 Introduction there have been attempts exploring information beyond text for MLDC. On the one hand, the informaWith the rapid growth of scientific documents, it tion associated with labels such as label semantics is difficult to track related literature manually. For and the relationships between labels are employed. example, there are more than 200,000 publications related to COVID-19 by April 2021. Therefore, For example, Xiao et al. (2019) generated labelspecific document representation using the label it is crucial to automatically assign publications semantic information. You et al. (2018) improved with their corresponding categories. Multi-label the classification performance by constructing a document classification (MLDC), associating one document instance with a set of most relevant la- hierarchical label tree. To model label dependency, bels, is attracting more and more research attention. MLDC is cast as a seq2seq task (Yang et al., 2018). On the other hand, the document metadata is inTo tackle the problem, early work f"
2021.emnlp-main.253,C18-1330,0,0.0248178,"an 200,000 publications related to COVID-19 by April 2021. Therefore, For example, Xiao et al. (2019) generated labelspecific document representation using the label it is crucial to automatically assign publications semantic information. You et al. (2018) improved with their corresponding categories. Multi-label the classification performance by constructing a document classification (MLDC), associating one document instance with a set of most relevant la- hierarchical label tree. To model label dependency, bels, is attracting more and more research attention. MLDC is cast as a seq2seq task (Yang et al., 2018). On the other hand, the document metadata is inTo tackle the problem, early work focused on corporated. For example, to employ the metadata learning semantic representations of the input text information, the representation of the document using some text encoders. For example, Liu et al. and its metadata are learned in the same embedding ⇤ Corresponding author. space (Zhang et al., 2021). The label hierarchy is 3162 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3162–3171 c November 7–11, 2021. 2021 Association for Computational Linguistics also"
2021.emnlp-main.551,N09-1057,0,0.0407787,"ntiment dicas events or indirectly model events with sotionary and some manually designed rules (Zhang phisticated models. Since events often trigger sentiments in sentences, we argue that this task and Liu, 2011), while event-centric approaches aswould benefit from explicit modeling of events sume sentiment is triggered by events described and event representation learning. To this end, in sentences. Examples include an early approach we represent an event as the combination of its which regarded noun phrases or entities in senevent type and the event triplet <subject, preditences as events (Greene and Resnik, 2009), and cate, object>. Based on such event representamore recent approaches which indirectly model tion, we further propose a novel model with events by capturing contextual information using hierarchical tensor-based composition mechgraph convolutional networks (Zuo et al., 2020) anism to detect sentiment in text. In addition, we present a dataset1 for event-centric or attention mechanism (Wei et al., 2020). We implicit sentiment analysis where each senargue that in the former, the event representation tence is labeled with the event representation is oversimplified and consequently event-relat"
2021.emnlp-main.551,W11-0705,0,0.0485734,"Missing"
2021.emnlp-main.551,P17-1044,0,0.0192789,"iment Analysis, and SemEval17 Task4 (Rosenthal et al., 2017). The SemEval-2017 Task 4 Subtask A aims to identify the overall sentiment of a tweet. The data was first cleaned by removing the hashtags, user mentions, irregular phrases and abbreviations. The original dataset does not have the annotations of event triplets. Existing event extraction models are mainly trained on the ACE 2005 dataset (Grishman et al., 2005), where triggers and arguments of news events are quite different from that of personal social events mentioned in the dataset. Therefore, we used a Semantic Role Labeling model (He et al., 2017) to extract a predicate from each sentence and mark its subject and object manually. We manually checked more than 20,000 tweets in this dataset, and retained 7,117 tweets which contain the full event triplets <subject, predicate, object>. Category Dataset EveSA SemEval17 Task4 Positive Neutral Negative 1205 624 2152 3247 2001 1869 Total 3981 7117 Table 2: Statistics of the datasets. The statistics of the two corpora used in our experiments are shown in Table 2. It should be noted that since we use the SRL model instead of event extraction model to annotate the SemEval 17 Task 4 dataset, there"
2021.emnlp-main.551,P98-1013,0,0.450663,"tered dataset for implicit sentiment analysis. We tive emotional tendencies (Zhang and Liu, 2011). first identify event types from FrameNet (Baker In event-centric approaches, events mentioned in et al., 1998) and then crawl tweets which contain text may imply positive or negative sentiments. Bal- the triggering words of the corresponding event ahur et al. (2011) presented an approach for detect- types. ing event-triggered sentiment based on common3.1 Construction of an Event Type Library sense knowledge, EmotiNet, a knowledge base of concepts with associated affective value. Greene FrameNet (Baker et al., 1998) is an English vocaband Resnik (2009) used grammatical structures to ulary knowledge base that contains more than 1200 mine language features related to implicit senti- semantic frames (each frame can be regarded as an ments, and used similarity calculations to improve event type) and lexical units (each lexical units can the performance of sentiment classification. Zuo be regarded as a predicate), and more than 200,000 6885 labeled sentences under all frames, which can be used for learning models for NLP tasks such as information extraction and event detection. With the FrameNet knowledge bas"
2021.emnlp-main.551,2020.coling-main.11,0,0.0223331,"it sentiment analysis. • Our model outperforms several competitive baselines on both EveSA and SemEval17Task4 Subtask A (Rosenthal et al., 2017). 2 Related Work et al. (2020) proposed a context-specific heterogeneous graph convolutional network to address the problem of the absence of sentiment words. Event-related Sentiment Analysis In recent years, researchers also pay attention to the importance of event information in sentiment analysis. Deng and Wiebe (2015) encoded a set of sentiment inference rules in a probabilistic soft logic framework for entity/event-level sentiment analysis tasks. Hofmann et al. (2020) encoded properties of events as latent variables following theories of cognitive appraisal of events to improve emotion classification performance. Gaonkar et al. (2020) tracked label-label correlations through label embeddings in sentiment classification tasks to maintain the consistency of emotion caused by the same type of event. Ding and Riloff (2016) first defined affective events as triples <subjective, verb, objective>, and created a dataset containing affective events with sentiment polarity labels. Ding and Riloff (2018b) expanded affective events as <subjective, predicate, objective"
2021.emnlp-main.551,W11-1707,0,0.0281091,"Missing"
2021.emnlp-main.551,D15-1018,0,0.0171885,"classification. • We present a dataset, called EveSA, with annotated event triplets, event types, and sentence-level sentiment polarity labels for implicit sentiment analysis. • Our model outperforms several competitive baselines on both EveSA and SemEval17Task4 Subtask A (Rosenthal et al., 2017). 2 Related Work et al. (2020) proposed a context-specific heterogeneous graph convolutional network to address the problem of the absence of sentiment words. Event-related Sentiment Analysis In recent years, researchers also pay attention to the importance of event information in sentiment analysis. Deng and Wiebe (2015) encoded a set of sentiment inference rules in a probabilistic soft logic framework for entity/event-level sentiment analysis tasks. Hofmann et al. (2020) encoded properties of events as latent variables following theories of cognitive appraisal of events to improve emotion classification performance. Gaonkar et al. (2020) tracked label-label correlations through label embeddings in sentiment classification tasks to maintain the consistency of emotion caused by the same type of event. Ding and Riloff (2016) first defined affective events as triples <subjective, verb, objective>, and created a"
2021.emnlp-main.551,D14-1162,0,0.0875646,"e multi-task learning module is skipped in the experiments on this dataset. For both EveSA and SemEval17 Task4, we employ accuracy and weighted-F1 as evaluation metrics. The weighted-F1 is computed by: F 1weighted = PN i=1 weighti ⇥ F 1i N (10) 5.3 Implementation Details Compared Methods Since our model takes both event and sentence as input, we compare the proposed model with baselines taking three kinds of inputs: Sentence as input Models in this category take a tweet (i.e., a sentence) as input: BiLTSM (Schuster and Paliwal, 1997): A bidirectional LSTM neural network with GloVe embeddings (Pennington et al., 2014) for word sequence. BERT (Devlin et al., 2018): The state-of-the-art model for sentiment classification. We limit the sentence length to the last 40 tokens to allow a larger batch size. We use BERT-base due to the memory limit of our GPU. BiLSTM+ orthogonalAtt (Wei et al., 2020): A recently proposed implicit sentiment analysis model with bidirectional LSTM neural network with BERT encoder and orthogonal attention. Event triplet as input Models in the category take the event triplet in the form of <subject, predicate, object> as input: The first two baselines, BiLSTM and BERT, take the concaten"
2021.emnlp-main.551,S17-2088,0,0.0533652,"Missing"
2021.emnlp-main.551,C16-1311,0,0.0586472,"Missing"
2021.emnlp-main.551,P15-1098,0,0.0674323,"Missing"
2021.emnlp-main.551,P14-1146,0,0.025666,"ective in detecting sentiments triggered by approach. events. 1 Introduction To overcome the limitations of existing eventcentric approaches, we propose to construct Sentiment analysis aims at automatically detecting the sentiment of given text. Explicit sentiment anal- a corpus in which event triplets in the form of <subject, predicate, object> and ysis methods detect the sentiment mainly based their corresponding types are annotated in senon the occurrence of sentiment-related words and have been extensively explored (Agarwal et al., tences. In addition, each sentence is also assigned 2011; Tang et al., 2014, 2015a). However, senti- with a sentiment class label. An example sentence ment could also be implicitly expressed. For exam- and its annotation are shown below: ple, the sentence ‘I won the first place in the speech ‘You abandon me for a week to go off on holiday with daddy, come back and barely 2 days later you contest’ does not contain any sentiment words, but go off out with him again.’ the event of ‘winning the first place’ reflects the Event triplet: <you, abandon, me> positive sentiment. Implicit sentiment analysis was Event type: abandonment ⇤ Corresponding author. 1 Sentiment: negati"
2021.emnlp-main.551,D19-1017,1,0.818142,"used by the same type of event. Ding and Riloff (2016) first defined affective events as triples <subjective, verb, objective>, and created a dataset containing affective events with sentiment polarity labels. Ding and Riloff (2018b) expanded affective events as <subjective, predicate, objective, prepositional phrase>, and introduced a weakly supervised method for affective events classification. Ding and Riloff (2018a) categorized affective events into physiological, health, leisure, social, financial, cognition, and freedom, based upon human need related to people’s motivations and desires. Yang et al. (2019) regarded the topic distribution of a sentence as an implicit event, and proposed an event-driven attention with topics of sentences for emotion ranking. Zhuang et al. (2020) presented a discourse-enhanced selftraining method that iteratively improved the classifier with unlabeled affective events. Implicit Sentiment Analysis Liu (2012) first classifies sentiment analysis into explicit and implicit sentiment analysis. Generally speaking, im3 Dataset: EveSA plicit sentiment analysis can be further classified into metaphor-based and event-centric approaches. Since the existing sentiment analysis"
2021.emnlp-main.551,P11-2101,0,0.0436099,"st classifies sentiment analysis into explicit and implicit sentiment analysis. Generally speaking, im3 Dataset: EveSA plicit sentiment analysis can be further classified into metaphor-based and event-centric approaches. Since the existing sentiment analysis datasets only In metaphor-based approaches, sentences contain- contain text and sentiment polarity labels, but no ing keywords found in a metaphor dictionary are event-related annotations, we construct an eventconsidered implicitly expressing positive or nega- centered dataset for implicit sentiment analysis. We tive emotional tendencies (Zhang and Liu, 2011). first identify event types from FrameNet (Baker In event-centric approaches, events mentioned in et al., 1998) and then crawl tweets which contain text may imply positive or negative sentiments. Bal- the triggering words of the corresponding event ahur et al. (2011) presented an approach for detect- types. ing event-triggered sentiment based on common3.1 Construction of an Event Type Library sense knowledge, EmotiNet, a knowledge base of concepts with associated affective value. Greene FrameNet (Baker et al., 1998) is an English vocaband Resnik (2009) used grammatical structures to ulary kno"
2021.emnlp-main.551,2020.emnlp-main.452,0,0.034603,"vents with sentiment polarity labels. Ding and Riloff (2018b) expanded affective events as <subjective, predicate, objective, prepositional phrase>, and introduced a weakly supervised method for affective events classification. Ding and Riloff (2018a) categorized affective events into physiological, health, leisure, social, financial, cognition, and freedom, based upon human need related to people’s motivations and desires. Yang et al. (2019) regarded the topic distribution of a sentence as an implicit event, and proposed an event-driven attention with topics of sentences for emotion ranking. Zhuang et al. (2020) presented a discourse-enhanced selftraining method that iteratively improved the classifier with unlabeled affective events. Implicit Sentiment Analysis Liu (2012) first classifies sentiment analysis into explicit and implicit sentiment analysis. Generally speaking, im3 Dataset: EveSA plicit sentiment analysis can be further classified into metaphor-based and event-centric approaches. Since the existing sentiment analysis datasets only In metaphor-based approaches, sentences contain- contain text and sentiment polarity labels, but no ing keywords found in a metaphor dictionary are event-relat"
2021.emnlp-main.636,P07-2044,0,0.056367,"this has prompted the Successful understanding of natural language de- recent development of neural architectures for aupends, among other factors, on the capability to tomatic feature extraction (Ning et al., 2019; Wang accurately detect events and their evolution through et al., 2020; Han et al., 2019b), which achieves time. This has recently led to increasing interest better generalization and avoids costly design of in research for temporal relation extraction (Cham- statistical methods leveraging hand-crafted features bers et al., 2014; Wang et al., 2020) with the aim (Mani et al., 2006; Chambers et al., 2007; Verhagen of understanding events and their temporal orders. and Pustejovsky, 2008), the inherent complexity of Temporal reasoning has been proven beneficial, for temporal relations still hinders approaches that just example, in understanding narratives (Cheng et al., rely on the scarce availability of annotated data. 2013), answering questions (Ning et al., 2020), or Some of the intrinsic limitations of the mensummarizing events (Wang et al., 2018). tioned approaches are due to the adopted embed1 ding space. Existing approaches to temporal relaSource code is available at https://github.com/"
2021.emnlp-main.636,D19-1405,0,0.0229817,"e BiLSTM extracting the pair of events and the SSVM incorporating structural linguistic constraints across them2 . Wang et al. (2020) proposed a constrained learning framework, where event pairs are encoded via a BiLSTM, enhanced with common-sense knowledge from ConceptNet (Speer et al., 2017) and T EM P ROB (Ning et al., 2018b), while enforcing a set of logical constraints at training time. The aim is to train the model to detect and extract the event relations while regularizing towards consistency on logic converted into differentiable objective functions, similarly to what was proposed in Li et al. (2019). Hyperbolic Neural Models The aforementioned models are all designed to process data representations in the Euclidean space. However, several studies (Nickel et al., 2014; Bouchard et al., 2015) have shown the inherent limitations of the Euclidean space in terms of representing asymmetric relations and tree-like graphs (Nickel et al., 2014; Bouchard et al., 2015). Hyperbolic spaces, instead, are promising alternatives that have a natural hierarchical structure and can be thought of as continuous versions of trees. This makes them 2 They compute the metrics differently and use an old version o"
2021.emnlp-main.636,2021.ccl-1.108,0,0.0625175,"Missing"
2021.emnlp-main.636,2020.findings-emnlp.42,0,0.0371585,"l. (2018b) introduced a framework of hyperbolic neural networks composed of neural units learning and optimizing parameters in hyperbolic spaces. Their experiments show that, without increasing the number of parameters of the models, hyperbolic neural networks outperform their Euclidean counterparts on natural language inference and detection of noisy prefixes tasks. There have been a few attempts in revisiting NLP tasks in the hyperbolic space framework by generalizing hyperbolic neural activation functions for machine translation (Gulcehre et al., 2018), to detect hierarchical entity types (López and Strube, 2020), and for document classification (Zhang and Gao, 2020). Compared to the above works, our model is the first attempt in devising an end-to-end hyperbolic architecture showing the benefit of addressing event TempRel extraction in hyperbolic spaces. 3 Preliminaries In this section, we give a brief introduction of hyperbolic geometry and hyperbolic neural networks. Hyperbolic geometry. A hyperbolic space is a non-Euclidean space that has the same negative sectional curvature at every point (i.e., a constant negative curvature). Intuitively, that a space has constant curvature implies that it keep"
2021.emnlp-main.636,P06-1095,0,0.0472638,"easoning. Although this has prompted the Successful understanding of natural language de- recent development of neural architectures for aupends, among other factors, on the capability to tomatic feature extraction (Ning et al., 2019; Wang accurately detect events and their evolution through et al., 2020; Han et al., 2019b), which achieves time. This has recently led to increasing interest better generalization and avoids costly design of in research for temporal relation extraction (Cham- statistical methods leveraging hand-crafted features bers et al., 2014; Wang et al., 2020) with the aim (Mani et al., 2006; Chambers et al., 2007; Verhagen of understanding events and their temporal orders. and Pustejovsky, 2008), the inherent complexity of Temporal reasoning has been proven beneficial, for temporal relations still hinders approaches that just example, in understanding narratives (Cheng et al., rely on the scarce availability of annotated data. 2013), answering questions (Ning et al., 2020), or Some of the intrinsic limitations of the mensummarizing events (Wang et al., 2018). tioned approaches are due to the adopted embed1 ding space. Existing approaches to temporal relaSource code is available"
2021.emnlp-main.636,P18-1212,0,0.0343891,"Missing"
2021.emnlp-main.636,D19-1642,0,0.0351263,"Missing"
2021.emnlp-main.636,2020.emnlp-main.88,0,0.0128765,"est better generalization and avoids costly design of in research for temporal relation extraction (Cham- statistical methods leveraging hand-crafted features bers et al., 2014; Wang et al., 2020) with the aim (Mani et al., 2006; Chambers et al., 2007; Verhagen of understanding events and their temporal orders. and Pustejovsky, 2008), the inherent complexity of Temporal reasoning has been proven beneficial, for temporal relations still hinders approaches that just example, in understanding narratives (Cheng et al., rely on the scarce availability of annotated data. 2013), answering questions (Ning et al., 2020), or Some of the intrinsic limitations of the mensummarizing events (Wang et al., 2018). tioned approaches are due to the adopted embed1 ding space. Existing approaches to temporal relaSource code is available at https://github.com/ Xingwei-Warwick/hyper-event-TempRel. tion extraction typically operate in the Euclidean 8065 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8065–8077 c November 7–11, 2021. 2021 Association for Computational Linguistics space, in which an event embedding is represented as a point. Although such embeddings exhibit a lin"
2021.emnlp-main.636,N18-1077,0,0.253615,"ll shares (e1: tendered) by shareholders on the Paris Stock Exchange at the same price from today through Nov. 17. Figure 1: Events annotated with temporal relations from a document excerpt. Arrow lines represent the Before relations, while red dashed lines the Vague ones. However, events that occurred in text are not just simple and standalone predicates, they rather form complex and hierarchical structures with different granularity levels (Fig. 1), a characteristic that still challenges existing models and restricts their performance on real-world datasets for temporal relation extraction (Ning et al., 2018a,c). Addressing such challenges requires models to not only recognize accurately the events and their hierarchical and chronological properties but also encode them in appropriate representations enabling effective tem1 Introduction poral reasoning. Although this has prompted the Successful understanding of natural language de- recent development of neural architectures for aupends, among other factors, on the capability to tomatic feature extraction (Ning et al., 2019; Wang accurately detect events and their evolution through et al., 2020; Han et al., 2019b), which achieves time. This has re"
2021.emnlp-main.636,P18-1122,0,0.305371,"ll shares (e1: tendered) by shareholders on the Paris Stock Exchange at the same price from today through Nov. 17. Figure 1: Events annotated with temporal relations from a document excerpt. Arrow lines represent the Before relations, while red dashed lines the Vague ones. However, events that occurred in text are not just simple and standalone predicates, they rather form complex and hierarchical structures with different granularity levels (Fig. 1), a characteristic that still challenges existing models and restricts their performance on real-world datasets for temporal relation extraction (Ning et al., 2018a,c). Addressing such challenges requires models to not only recognize accurately the events and their hierarchical and chronological properties but also encode them in appropriate representations enabling effective tem1 Introduction poral reasoning. Although this has prompted the Successful understanding of natural language de- recent development of neural architectures for aupends, among other factors, on the capability to tomatic feature extraction (Ning et al., 2019; Wang accurately detect events and their evolution through et al., 2020; Han et al., 2019b), which achieves time. This has re"
2021.emnlp-main.636,D18-2013,0,0.25686,"ll shares (e1: tendered) by shareholders on the Paris Stock Exchange at the same price from today through Nov. 17. Figure 1: Events annotated with temporal relations from a document excerpt. Arrow lines represent the Before relations, while red dashed lines the Vague ones. However, events that occurred in text are not just simple and standalone predicates, they rather form complex and hierarchical structures with different granularity levels (Fig. 1), a characteristic that still challenges existing models and restricts their performance on real-world datasets for temporal relation extraction (Ning et al., 2018a,c). Addressing such challenges requires models to not only recognize accurately the events and their hierarchical and chronological properties but also encode them in appropriate representations enabling effective tem1 Introduction poral reasoning. Although this has prompted the Successful understanding of natural language de- recent development of neural architectures for aupends, among other factors, on the capability to tomatic feature extraction (Ning et al., 2019; Wang accurately detect events and their evolution through et al., 2020; Han et al., 2019b), which achieves time. This has re"
2021.emnlp-main.636,W16-5706,0,0.0648677,"Missing"
2021.emnlp-main.636,N18-1202,0,0.353908,"Kiela, 2017; Ganea et al., 2018a), we propose events since A FTER and B EFORE are reciprocal. to learn event embeddings based on the Poincaré In addition to the first loss term, we introduce model to capture their temporal relations. a second novel loss term to enforce an angular For a given text sequence containing an event pair (u, v), we first extract the contextualized em- property. As the example shown in Figure 2, we want to make ∠θ1 of a positive event pair (u, v) beddings of the event tokens3 , eu and ev , from smaller. Based on preliminary tests, the angular their, for example, ELMo (Peters et al., 2018) or RoBERTa (Liu et al., 2019) sequence encodings, loss can further enforce the first loss term which is driving the norm of u to be larger than the norm of v and use the exponential mapping function to map and thus increases the performance. Moreover, this 3 The contextualized embeddings capture the context informaangular property can help to distinguish VAGUE tion around the event triggers through pre-trained language pairs by using a threshold on the ∠θ2 to determine models. Thus, the resulting event embedding essentially also encodes the information about its subject and object. whether to"
2021.emnlp-main.636,C08-3012,0,0.0772806,"Missing"
2021.emnlp-main.636,2020.emnlp-main.51,0,0.295671,"fective tem1 Introduction poral reasoning. Although this has prompted the Successful understanding of natural language de- recent development of neural architectures for aupends, among other factors, on the capability to tomatic feature extraction (Ning et al., 2019; Wang accurately detect events and their evolution through et al., 2020; Han et al., 2019b), which achieves time. This has recently led to increasing interest better generalization and avoids costly design of in research for temporal relation extraction (Cham- statistical methods leveraging hand-crafted features bers et al., 2014; Wang et al., 2020) with the aim (Mani et al., 2006; Chambers et al., 2007; Verhagen of understanding events and their temporal orders. and Pustejovsky, 2008), the inherent complexity of Temporal reasoning has been proven beneficial, for temporal relations still hinders approaches that just example, in understanding narratives (Cheng et al., rely on the scarce availability of annotated data. 2013), answering questions (Ning et al., 2020), or Some of the intrinsic limitations of the mensummarizing events (Wang et al., 2018). tioned approaches are due to the adopted embed1 ding space. Existing approaches to tempor"
2021.emnlp-main.636,2020.emnlp-demos.6,0,0.0651785,"Missing"
2021.findings-acl.154,P16-1035,0,0.0306799,"rs which is problematic as mentioned earlier. 3 where n is the number of retrieved documents and T F (w|di ) is the term frequency of word w in document di . KL-Divergence based extraction (KLD) The second one is inspired by the query expansion technique (Carpineto et al., 2001). By intuition, words relevant to the input query have a high probability in the retrieved sub-corpus but a low probability in the whole corpus. The score can be defined as: Score(w) = PR (w)log Relevance model with word embedding (REL) This approach extracts concept words from a wordembedding enhanced relevance model (Diaz et al., 2016). The probability assigned to word w by the relevance model (Lavrenko and Croft, 2017) is: p(w|RM ) = X p(w|d)p(d|q) (4) d∈R where R is the retrieved documents set, p(w|d) is the probability of word w in document d and p(d|q) is d’s query likelihood from equation (1). We integrate this model with word embeddings: Score(w) = λp(w|RM ) + (1 − λ)sim(w, q) 3.1 (3) where PR (w) is the probability of word w in the retrieved sub-corpus and PC (w) is the probability of word w in the whole corpus. We extract words with high scores as our concept words. Proposed Framework In outline, our model expands a"
2021.findings-acl.154,2020.tacl-1.29,0,0.0986546,"Missing"
2021.findings-acl.341,N19-1423,0,0.0108314,"., 2018), which consists of Twitter rumours around 5 high profile real-world events. Statistics regarding the dataset can be found in the Appendix. The PHEME dataset was chosen as it is a particularly challenging dataset due to class imbalance and a leave-one-event out cross validation setting, reflecting a real-world evaluation scenario. Baseline Models We perform comparison of the proposed model SAVED with existing state-of-theart models (Kochkina et al., 2018; Li et al., 2019b; Cheng et al., 2020) and several strong baselines, described in this section. BERT We use the pretrained BERTBASE (Devlin et al., 2019), uncased, which consists of 12 selfattention layers, and returns a 768-dimension vector representation of a sentence. We generate BERT representations for each tweet in the conversation before feeding them into the Veracity Module. VAED is a version of SAVED, where the Topic Learning Module is reduced to only its VAE component, without the stance classifiers from Section 2.1. The loss is Lc + Lx + γLM I . VAED Without Disentanglement (VAE) is a simplified version of VAED, where the Topic Learning Module is reduced to only using loss from the context-enriched latent factor without the target m"
2021.findings-acl.341,C18-1284,0,0.0217335,"D model alone (with disentanglement, without any other modifications or stance/veracity classifiers) scores 0.363, showcasing the efficacy of disentanglement per se. The BERT-based model only outperformed VAE. The proposed SAVED model outperforms those of prior work on overall accuracy and the True and Unverified classes. However, results for the False class are rather low - which is in fact the case for most of the models in Table 1, with only Cheng et al. (2020) being an exception. Results of VAED+V are lower than that of SAVED, in line with the knowledge that stance is related to veracity (Dungs et al., 2018). This suggests that stance is also a worthwhile intermediate classification target. Event False True Unverified MacroF1 Charlie Hebdo Ferguson Germanwings Crash Ottawa Shooting Sydney Siege 0.223 0.129 0.033 0.058 0.157 0.505 0.080 0.520 0.735 0.700 0.324 0.906 0.289 0.119 0.140 0.351 0.372 0.281 0.304 0.332 Overall 0.164 0.642 0.531 0.434 Table 2: Per-fold evaluation results of SAVED. Per-fold Results Table 2 shows the per-fold results in our leave-one-event-out setting. Interestingly, the model tends to perform best on rumors of True veracity and worst on those which are False. Performance"
2021.findings-acl.341,Q19-1017,1,0.822952,"ssment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet representations based on the word representations learned in the latent stance-dependent topic space. Our results show that using such tweet representations for rumour veracity classification achieves superior performance on the PHEME dataset"
2021.findings-acl.341,S19-2147,1,0.828716,"re is increasing need for machine learning algorithms to assist with rumour veracity assessment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet representations based on the word representations learned in the latent stance-dependent topic space. Our results show that using such tweet representations"
2021.findings-acl.341,C18-1288,1,0.760705,"1 Department of Computer Science, University of Warwick, UK 2 Queen-Mary University of London, UK 3 Alan Turing Institute, UK {j.Dougrez-Lewis,yulan.he}@warwick.ac.uk {m.liakata,e.kochkina}@qmul.ac.uk Abstract rumour to assist in veracity classification, which has previously been shown to be predictive of rumour veracity (Dungs et al., 2018). A number of approaches (Kochkina et al., 2018; Li et al., 2019b) also showed benefits of using stance classification as an auxiliary task in a multitask learning setup. Some recent approaches exploit the structure of the conversation discussing a rumour. Kochkina et al. (2018) used LSTM to model linear branches extracted from the conversation tree, while Ma et al. (2018) and Bian et al. (2020) modelled a tree structure to capture information from responses. With the rapid growth of social media in the past decade, the news are no longer controlled by just a few mainstream sources. Users themselves create large numbers of potentially fictitious rumours, necessitating automated veracity classification systems. Here we present a novel approach towards automatically classifying rumours circulating on Twitter with respect to their veracity. We use a model built on Varia"
2021.findings-acl.341,S19-2148,0,0.0813606,"rumours online with the potential to influence and pose as news. Since it is impossible to manually check the vast volume of circulating tweets, there is increasing need for machine learning algorithms to assist with rumour veracity assessment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet r"
2021.findings-acl.341,P19-1113,0,0.143205,"rumours online with the potential to influence and pose as news. Since it is impossible to manually check the vast volume of circulating tweets, there is increasing need for machine learning algorithms to assist with rumour veracity assessment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet r"
2021.findings-acl.341,P18-1184,0,0.0290631,"an Turing Institute, UK {j.Dougrez-Lewis,yulan.he}@warwick.ac.uk {m.liakata,e.kochkina}@qmul.ac.uk Abstract rumour to assist in veracity classification, which has previously been shown to be predictive of rumour veracity (Dungs et al., 2018). A number of approaches (Kochkina et al., 2018; Li et al., 2019b) also showed benefits of using stance classification as an auxiliary task in a multitask learning setup. Some recent approaches exploit the structure of the conversation discussing a rumour. Kochkina et al. (2018) used LSTM to model linear branches extracted from the conversation tree, while Ma et al. (2018) and Bian et al. (2020) modelled a tree structure to capture information from responses. With the rapid growth of social media in the past decade, the news are no longer controlled by just a few mainstream sources. Users themselves create large numbers of potentially fictitious rumours, necessitating automated veracity classification systems. Here we present a novel approach towards automatically classifying rumours circulating on Twitter with respect to their veracity. We use a model built on Variational Autoencoder which disentangles the informational content of a tweet from the manner in wh"
2021.findings-emnlp.404,D13-1160,0,0.425537,"pproaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation dynamically. Recently, some edge Base Question Answering (KBQA) sys- researchers attempt to model prediction uncertaintem (Berant et al., 2013; Bordes et al., 2015; Yin ties in the simple question answering task with et al., 2016; Hao et al., 2018) is attracting more and Bayesian neural network (Zhang et al., 2021). Genmore attention. The KBQA system often contains erally, most of existing methods focus on detecting two core components: (1) entity linking, which one optimal relation path, considering the task a identifies the topic entity mentioned in the ques- single-label learning problem. tion; (2) relation detection, which detects the reHowever, for some question, there may exist lation paths starting from the topic entity to th"
2021.findings-emnlp.404,N19-1031,0,0.0149474,"amples of ground-truth relation paths corresponding to the given questions. 2018) rely on measuring the semantic similarity of questions and candidate relations. He and Golub (2016) proposed an encoder-decoder based generative framework for single-relation extraction. For multi-relation detection, some approaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation dynamically. Recently, some edge Base Question Answering (KBQA) sys- researchers attempt to model prediction uncertaintem (Berant et al., 2013; Bordes et al., 2015; Yin ties in the simple question answering task with et al., 2016; Hao et al., 2018) is attracting more and Bayesian neural network (Zhang et al., 2021). Genmore attention. The KBQA system often contains erally, most of existing methods focus on detecting two core compone"
2021.findings-emnlp.404,C18-1277,0,0.0172433,"plying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation dynamically. Recently, some edge Base Question Answering (KBQA) sys- researchers attempt to model prediction uncertaintem (Berant et al., 2013; Bordes et al., 2015; Yin ties in the simple question answering task with et al., 2016; Hao et al., 2018) is attracting more and Bayesian neural network (Zhang et al., 2021). Genmore attention. The KBQA system often contains erally, most of existing methods focus on detecting two core components: (1) entity linking, which one optimal relation path, considering the task a identifies the topic entity mentioned in the ques- single-label learning problem. tion; (2) relation detection, which detects the reHowever, for some question, there may exist lation paths starting from the topic entity to the multiple relation paths to the correct answer. For answer node. example, as shown in the upper part of F"
2021.findings-emnlp.404,D16-1166,0,0.0215709,"sequence generation task. A relation-aware sequence relation generation model is proposed to solve the problem in an end-to-end manner. Experimental results show the effectiveness of the proposed method for relation detection and KBQA. Which city hosted the 1900 summer Olympics? olympic/olympic_games/host_city time/time/event film/actor/film film/film_actor/portrayed_in_film What was Pierce Brosnan’s first outing as 007? Figure 1: Examples of ground-truth relation paths corresponding to the given questions. 2018) rely on measuring the semantic similarity of questions and candidate relations. He and Golub (2016) proposed an encoder-decoder based generative framework for single-relation extraction. For multi-relation detection, some approaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation"
2021.findings-emnlp.404,D16-1137,0,0.047957,"Missing"
2021.findings-emnlp.404,D17-1060,0,0.0223577,"time/time/event film/actor/film film/film_actor/portrayed_in_film What was Pierce Brosnan’s first outing as 007? Figure 1: Examples of ground-truth relation paths corresponding to the given questions. 2018) rely on measuring the semantic similarity of questions and candidate relations. He and Golub (2016) proposed an encoder-decoder based generative framework for single-relation extraction. For multi-relation detection, some approaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation dynamically. Recently, some edge Base Question Answering (KBQA) sys- researchers attempt to model prediction uncertaintem (Berant et al., 2013; Bordes et al., 2015; Yin ties in the simple question answering task with et al., 2016; Hao et al., 2018) is attracting more and Bayesian neural network (Zhang et al.,"
2021.findings-emnlp.404,P17-1114,0,0.029431,"etection as a multi-label learning problem. To solve the challenge of multi-label multi-hop relation detection, we cast it as a sequence generation problem. A relation-aware sequence relation generation model is proposed to learn the problem in an end-to-end manner. Experimental results show that our approach not only achieves better relation detection performance, but also improves the results of the state-of-the-art KBQA system. The experimental results of KBQA end-task are shown in Table 4. The FOFE-net (Jiang et al., 2019) Acknowledgements is a pipeline KBQA system built based on FOFEnet (Xu et al., 2017) , which achieves outstanding We would like to thank anonymous reviewers for results on SimpleQuestions and WebQSP datasets. their valuable comments and helpful suggestions. The RSGM result is obtained by performing en- This work was funded by the National Natural Scitity linking and relation detection with proposed ence Foundation of China (61772132), and the EPmodel.Based on the multiple relation paths gen- SRC (grant no. EP/T017112/1, EP/V048597/1). erated by RSGM, a majority vote strategy is em- YH is supported by a Turing AI Fellowship funded ployed to get the final answer. The results sh"
2021.findings-emnlp.404,C18-1330,0,0.080732,"ik = vaT tanh(Wa [si−1 ; hw ]) knowledge base to reach the correct answer node. j Compared with the existing well-known KBQA 2.3 Training and Inference datasets SimpleQuestions (Bordes et al., 2015) and The proposed model is trained under regular WebQuestion (Berant et al., 2013), it has the folsequence-to-sequence loss by maximizing the like- lowing characteristics: (1) for the give question, it lihood of the ground-truth token sequence. At the provides multiple annotated relation paths to the 4715 Methods CNN-multichannel (Kim, 2014) MLKNN (Zhang and Zhou, 2014) HAN (Yang et al., 2016) SGM (Yang et al., 2018) SGM-BERT RSGM Presion(↑) Recall(↑) Micro F1(↑) HL(×10−4 )(↓) 0.5158 0.5327 0.4965 0.5039 0.5992 0.6795 0.3952 0.3287 0.4254 0.3976 0.4372 0.5285 0.4475 0.4066 0.4582 0.4445 0.5056 0.5945 1.4285 1.4049 1.4728 1.4549 1.2437 1.0552 Table 1: Performance comparision of the proposed approach with other approaches on FreebaseQA test set. “HL” represents the metric of Hamming Loss. Dateset FreebaseQA Train Dev Test 20358 3994 3996 • HAN (Yang et al., 2016): a hierarchical attention network is employed to obtain sentence representations and then generate document representations based on sentence repr"
2021.findings-emnlp.404,N16-1174,0,0.0196645,"s multiple hops in the eik = vaT tanh(Wa [si−1 ; hw ]) knowledge base to reach the correct answer node. j Compared with the existing well-known KBQA 2.3 Training and Inference datasets SimpleQuestions (Bordes et al., 2015) and The proposed model is trained under regular WebQuestion (Berant et al., 2013), it has the folsequence-to-sequence loss by maximizing the like- lowing characteristics: (1) for the give question, it lihood of the ground-truth token sequence. At the provides multiple annotated relation paths to the 4715 Methods CNN-multichannel (Kim, 2014) MLKNN (Zhang and Zhou, 2014) HAN (Yang et al., 2016) SGM (Yang et al., 2018) SGM-BERT RSGM Presion(↑) Recall(↑) Micro F1(↑) HL(×10−4 )(↓) 0.5158 0.5327 0.4965 0.5039 0.5992 0.6795 0.3952 0.3287 0.4254 0.3976 0.4372 0.5285 0.4475 0.4066 0.4582 0.4445 0.5056 0.5945 1.4285 1.4049 1.4728 1.4549 1.2437 1.0552 Table 1: Performance comparision of the proposed approach with other approaches on FreebaseQA test set. “HL” represents the metric of Hamming Loss. Dateset FreebaseQA Train Dev Test 20358 3994 3996 • HAN (Yang et al., 2016): a hierarchical attention network is employed to obtain sentence representations and then generate document representation"
2021.findings-emnlp.404,N16-1000,0,0.100977,"eration task. A relation-aware sequence relation generation model is proposed to solve the problem in an end-to-end manner. Experimental results show the effectiveness of the proposed method for relation detection and KBQA. Which city hosted the 1900 summer Olympics? olympic/olympic_games/host_city time/time/event film/actor/film film/film_actor/portrayed_in_film What was Pierce Brosnan’s first outing as 007? Figure 1: Examples of ground-truth relation paths corresponding to the given questions. 2018) rely on measuring the semantic similarity of questions and candidate relations. He and Golub (2016) proposed an encoder-decoder based generative framework for single-relation extraction. For multi-relation detection, some approaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation"
2021.findings-emnlp.404,N19-1028,0,0.0115879,"round-truth to guide model learning. At the testing stage, the beam search optimization approach (Wiseman and Rush, 2016) is used to mitigate the exposure bias problem. Additionally, to avoid the repetition problem, a constraint mechanism is added. Unlike text generation or machine translation tasks, the multiple paths to the same question are usually mutually exclusive. Therefore, when a relation path is generated, an infinite penalty is added for that relation path, in order to avoid it being generated again. 3 Experiments We conduct experiments on a large KBQA benchmark dataset FreebaseQA (Jiang et al., 2019) to evaluate the effectiveness of the proposed RSGM model. 3.1 Dataset FreebaseQA (Jiang et al., 2019) is a novel KBQA dataset generated by matching trivia-type questionanswer pairs with facts existed in FreeBase. In XN w particular, for each question in the dataset, there αij hj ci = j=1 may often exists multiple multi-hop relation paths exp(eij ) (9) that can give rise to the correct answer. Here multiαij = N Σk=1 exp(eik ) hop means it should takes multiple hops in the eik = vaT tanh(Wa [si−1 ; hw ]) knowledge base to reach the correct answer node. j Compared with the existing well-known KB"
2021.findings-emnlp.404,P15-1128,0,0.0367795,"lts show the effectiveness of the proposed method for relation detection and KBQA. Which city hosted the 1900 summer Olympics? olympic/olympic_games/host_city time/time/event film/actor/film film/film_actor/portrayed_in_film What was Pierce Brosnan’s first outing as 007? Figure 1: Examples of ground-truth relation paths corresponding to the given questions. 2018) rely on measuring the semantic similarity of questions and candidate relations. He and Golub (2016) proposed an encoder-decoder based generative framework for single-relation extraction. For multi-relation detection, some approaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation dynamically. Recently, some edge Base Question Answering (KBQA) sys- researchers attempt to model prediction uncertaintem (Berant et al., 2013; Bordes"
2021.findings-emnlp.404,D14-1181,0,0.0172098,"j = N Σk=1 exp(eik ) hop means it should takes multiple hops in the eik = vaT tanh(Wa [si−1 ; hw ]) knowledge base to reach the correct answer node. j Compared with the existing well-known KBQA 2.3 Training and Inference datasets SimpleQuestions (Bordes et al., 2015) and The proposed model is trained under regular WebQuestion (Berant et al., 2013), it has the folsequence-to-sequence loss by maximizing the like- lowing characteristics: (1) for the give question, it lihood of the ground-truth token sequence. At the provides multiple annotated relation paths to the 4715 Methods CNN-multichannel (Kim, 2014) MLKNN (Zhang and Zhou, 2014) HAN (Yang et al., 2016) SGM (Yang et al., 2018) SGM-BERT RSGM Presion(↑) Recall(↑) Micro F1(↑) HL(×10−4 )(↓) 0.5158 0.5327 0.4965 0.5039 0.5992 0.6795 0.3952 0.3287 0.4254 0.3976 0.4372 0.5285 0.4475 0.4066 0.4582 0.4445 0.5056 0.5945 1.4285 1.4049 1.4728 1.4549 1.2437 1.0552 Table 1: Performance comparision of the proposed approach with other approaches on FreebaseQA test set. “HL” represents the metric of Hamming Loss. Dateset FreebaseQA Train Dev Test 20358 3994 3996 • HAN (Yang et al., 2016): a hierarchical attention network is employed to obtain sentence repr"
2021.findings-emnlp.404,C16-1164,0,0.0193465,"arning problem. tion; (2) relation detection, which detects the reHowever, for some question, there may exist lation paths starting from the topic entity to the multiple relation paths to the correct answer. For answer node. example, as shown in the upper part of Figure 1, Relation detection in KBQA can be categorized there are two distinct relation paths time/time/event into singe-relation (one-hop) detection and multi- and olympic/olymipic_games/host_city with the relation (multi-hop) detection. Most existing single- same meaning, making the instance multi-label. relation detection methods (Yin et al., 2016; Yu Moreover, as shown in the lower part of Figure 1, et al., 2017; Lukovnikov et al., 2017; Yu et al., there are two relation paths starting from different ∗ Corresponding author. topic entities Pierce Brosnan and 007. A robust 4713 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4713–4719 November 7–11, 2021. ©2021 Association for Computational Linguistics KBQA system should be able to infer the final answer based on multiple relation paths. We therefore consider multi-label multi-hop relation detection in this paper. Nevertheless, it is challenging to perform m"
2021.findings-emnlp.404,P17-1053,0,0.0178257,"tiveness of the proposed method for relation detection and KBQA. Which city hosted the 1900 summer Olympics? olympic/olympic_games/host_city time/time/event film/actor/film film/film_actor/portrayed_in_film What was Pierce Brosnan’s first outing as 007? Figure 1: Examples of ground-truth relation paths corresponding to the given questions. 2018) rely on measuring the semantic similarity of questions and candidate relations. He and Golub (2016) proposed an encoder-decoder based generative framework for single-relation extraction. For multi-relation detection, some approaches (Yih et al., 2015; Yu et al., 2017, 2018) proposed to tackle two or three-relation detection by applying some constraint which makes the number of hops fixed. Xiong et al. (2017) and Das et al. (2017) modeled the relation reasoning problem as 1 Introduction a Markov decision process. Chen et al. (2019) With the development of Knowledge Bases (KBs) exploited a transition-based search framework to such as DBpedia, Freebase, and WikiData, Knowl- select the relation dynamically. Recently, some edge Base Question Answering (KBQA) sys- researchers attempt to model prediction uncertaintem (Berant et al., 2013; Bordes et al., 2015; Yi"
2021.naacl-main.228,K16-1002,0,0.0716474,"se belief networks lection of movie and book reviews paired with (Mnih et al., 2014), or enforce the Dirichlet prior their plots; on the document-topic distribution via Wasserstein Autoencoders (Nan et al., 2019). Others adopt • We conduct an experimental assessment of continuous representations to capture long-term deour model, highlighting more interpretable topics with better topic coherence and diver- pendencies or preserve word order via sequence-tosequence VAE (Dieng et al., 2017; Xu et al., 2017; sity scores compared to others state-of-the-art supervised topic models, and improved dis- Bowman et al., 2016; Yang et al., 2017) whose criminative power on sentiment classification, time complexity and difficulty of training, however, have limited their applications. Neural Variational and a consistent topic-disentanglement rate. Document Model (NVDM) (Miao et al., 2016) is 2 Related Work a direct extension of VAE used for topic detection in text. In NVDM, the prior of latent topics is asOur work is closely related to three lines of re- sumed to be a Gaussian distribution. This is not search: sentiment-topic models, neural topic mod- ideal since it cannot mimic the simplex in the latent els and lear"
2021.naacl-main.228,P18-1189,0,0.0537725,"Missing"
2021.naacl-main.228,N19-1423,0,0.0111697,"et al., 2018): a neural framework based on variational inference for the generation of topic while incorporating metadata information. Parameter Setting. We perform tokenization and sentence splitting with SpaCy6 . When available, we keep the default preprocessing, as it is the case for sLDA and S CHOLAR. Along with stopwords, we also remove tokens shorter than three characters and those with just digits or punctuation. We set the vocabulary to the 2,000 most common words as the best trade-off for each dataset. The 300dimensional word vectors are initialized with a pretrained BERT embedding (Devlin et al., 2019). Sentence embeddings are generated from the SentenceBERT using a pretrained BERT-large with meantokens pooling (Reimers and Gurevych, 2019). We use the predefined split of the MOBO dataset into training, development and test set in the proportion of 80/10/10 and average all the results over 5 runs.7 6 7 https://spacy.io/ Hyperparameter setting and training details are in Appendix. 2874 Datasets Models 25 IMDB GoodReads Amazon Topic Coherence / Topic Uniqueness 50 100 200 LDA sLDA JST 0.395 / 20.3 0.421 / 15.8 0.472 / 22.7 0.387 / 30.1 0.376 / 18.9 0.526 / 26.8 0.383 / 33.9 0.291 / 13.5 0.527"
2021.naacl-main.228,D19-1350,1,0.647634,"Sentiment Classifier ?? ?? (?|??, ??) ? Review Document ?ො Reconstr. review ?? ? Plot ?? ?? (?|?? ) Latent plot topic Parameter sharing Sentiment Classifier Uniform distribution Plot Classifier ?? PlotID Latent plot topic ?? (?? |?) ?? Sentiment label ?መ Reconstructed plot Figure 2: The DIATOM Architecture. neural framework for topic models with metadata incorporation (Card et al., 2018). When metadata are document labels, the model infers topics that are relevant to those labels. Although some studies have applied the adversarial approach (Goodfellow et al., 2014) and reinforcement learning (Gui et al., 2019) to topic models setting a Dirichlet prior on the generative network (Wang et al., 2019; Masada et al., 2018), it is still unexplored how to use this mechanism to disentangle opinion-bearing topics from plot or neutral topics. Representation Disentanglement. Among the slightly different versions of representation disentanglement proposed (Bengio et al., 2013; Higgins et al., 2018; Gao et al., 2019), the one achieved in DIATOM is analogous to Thomas et al. (2017) and Bengio et al. (2017), where they impose additional constraints to the representations controlled using a reinforcement learning m"
2021.naacl-main.228,D18-1096,0,0.11459,"Missing"
2021.naacl-main.228,P19-1041,0,0.0906347,"ot or neutral topics. Representation Disentanglement. Among the slightly different versions of representation disentanglement proposed (Bengio et al., 2013; Higgins et al., 2018; Gao et al., 2019), the one achieved in DIATOM is analogous to Thomas et al. (2017) and Bengio et al. (2017), where they impose additional constraints to the representations controlled using a reinforcement learning mechanism determining the disentangled factors. Alternatively, in DIATOM we make use of an adversarial approach over the available target labels. Application in text processing has shown promising results (John et al., 2019; Kumar et al., 2017; Hoang et al., 2019; Esmaeili et al., 2019), yet applications to topic modeling are still limited (Wilson et al., 2016) and to the best of our knowledge, there is no work in separating opinion-bearing topics from plot/neutral topics. 3 DIATOM architecture ments (zs ) and plots2 (za ), we aim to learn a model maximizing the joint data-label log-likelihood, log p(x, ys ): Z Z log p(x, ys ) = log p(x, ys , za , zs )dza dzs ≥ Eqφ (za |x),qψ (zs |x,ys ) [log pθ (x|za , zs )] + Eqφ (za |x),qψ (zs |x,ys ) [log pπ (ys |x)]  − KL qφ (za |x)||p(za )  − KL qψ (zs |x, ys )||p(zs ) ("
2021.naacl-main.228,L18-1274,0,0.026878,"ts about MOvie and BOok, associated to human-annotated sentences: while the pairs of reviews and plots are used to enhance the generation of plot topics, the human-annotated sentences provide the necessary ground-truth to automatically evaluate the topics’ polarity. Movie and book reviews were collected and paired from 3 public datasets: the Stanford’s IMDB movie reviews (Maas et al., 2011), the GoodReads (Wan et al., 2019) and the Amazon reviews dataset (McAuley et al., 2015). Among all the available reviews in the IMDB dataset, we keep the ones with a corresponding plot in the MPST dataset (Kar et al., 2018), a corpus of movie synopses. The Goodreads dataset comes already with books’ reviews paired with the related plots; while from the Amazon dataset, among all the product reviews, we keep only the ones related to movies available on the store and whose descriptions consist of the movie plots3 . With the help of 15 annotators we further labeled more than 18,000 reviews’ sentences (∼ 6000 per corpus), marking the sentence polarity (Positive, Negative), or whether a sentence describes its corresponding movie/book Plot, or none of the above (None)4 . We ensured that each sentence was labelled by at"
2021.naacl-main.228,P11-1015,0,0.510115,"Therefore, we propose to distinguish opinionbearing topics from plot/neutral ones combining a neural topic model architecture with adversarial training. In this study, we present the DIsentangled Adversarial TOpic Model (DIATOM)1 , aiming at disentangling information related to the target labels (i.e. the review score), from other distinct aspects yet possibly still polarised (e.g. plot descriptions). We also introduce a new dataset, namely the MOBO dataset1 , made up of movie and book reviews, paired with their related plots. The reviews come from different publicly available datasets: IMDB (Maas et al., 2011), GoodReads (Wan et al., 2019) and Amazon reviews (McAuley et al., 2015), and encompass a wide spectrum of domains and styles. We conduct an extensive experimental assessment of our model. First, we assess the topic quality in terms of topic coherence and diversity and compare DIATOM with other supervised topic models on the sentiment classification task; then, we analyse the disentangling rate of topics to quantitatively assess the degree of separation between actual opinion and plot/neutral topics. Our contributions are summarized below: • We propose a new model, DIATOM, which is able to gen"
2021.naacl-main.228,P19-1640,0,0.352469,"ing strictly aligned to the provided labels. Instead, DIATOM is able to generate opinion-bearing topics and plot topics which may still be polarized but not carrying any user’s opinion. Neural Topic Models. Neural models provide a more generic and extendable alternative to topic modeling, and therefore, have recently gained in• We introduce the MOBO dataset, a new col- creasing interest. Some of them use belief networks lection of movie and book reviews paired with (Mnih et al., 2014), or enforce the Dirichlet prior their plots; on the document-topic distribution via Wasserstein Autoencoders (Nan et al., 2019). Others adopt • We conduct an experimental assessment of continuous representations to capture long-term deour model, highlighting more interpretable topics with better topic coherence and diver- pendencies or preserve word order via sequence-tosequence VAE (Dieng et al., 2017; Xu et al., 2017; sity scores compared to others state-of-the-art supervised topic models, and improved dis- Bowman et al., 2016; Yang et al., 2017) whose criminative power on sentiment classification, time complexity and difficulty of training, however, have limited their applications. Neural Variational and a consiste"
2021.naacl-main.228,P04-1035,0,0.654792,"tion of this is shown in Figure 1 in which opinion topics are separated from plot topics. though these approaches have achieved significant results via the neural inference process, existing However, models relying solely on sentiment topic models when applied to user reviews may ex- information are easily misled and not suitable to tract topics with writers’ subjective opinions mixed disentangle opinion from plots, since even plot dewith those related to factual descriptions such as scriptions frequently make large use of sentiment plot summaries of movies and books (Lin et al., expressions (Pang and Lee, 2004a). Consider for 2012). Yet surprisingly very little work has been example the following sentence: “The ring holds a 2870 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2870–2883 June 6–11, 2021. ©2021 Association for Computational Linguistics dark power, and it soon begins to exert its evil influence on Bilbo”, an excerpt from a strong positive Amazon’s review. Therefore, we propose to distinguish opinionbearing topics from plot/neutral ones combining a neural topic model architecture with a"
2021.naacl-main.228,D19-1410,0,0.0119695,"n. Parameter Setting. We perform tokenization and sentence splitting with SpaCy6 . When available, we keep the default preprocessing, as it is the case for sLDA and S CHOLAR. Along with stopwords, we also remove tokens shorter than three characters and those with just digits or punctuation. We set the vocabulary to the 2,000 most common words as the best trade-off for each dataset. The 300dimensional word vectors are initialized with a pretrained BERT embedding (Devlin et al., 2019). Sentence embeddings are generated from the SentenceBERT using a pretrained BERT-large with meantokens pooling (Reimers and Gurevych, 2019). We use the predefined split of the MOBO dataset into training, development and test set in the proportion of 80/10/10 and average all the results over 5 runs.7 6 7 https://spacy.io/ Hyperparameter setting and training details are in Appendix. 2874 Datasets Models 25 IMDB GoodReads Amazon Topic Coherence / Topic Uniqueness 50 100 200 LDA sLDA JST 0.395 / 20.3 0.421 / 15.8 0.472 / 22.7 0.387 / 30.1 0.376 / 18.9 0.526 / 26.8 0.383 / 33.9 0.291 / 13.5 0.527 / 29.3 0.391 / 34.4 0.288 / 14.6 0.530 / 31.1 NVDM GSM NTM P ROD LDA S CHOLAR 0.281 / 15.8 0.384 / 22.4 0.423 / 28.8 0.502 / 31.1 0.550 / 28"
2021.naacl-main.228,P19-1248,0,0.0793056,"guish opinionbearing topics from plot/neutral ones combining a neural topic model architecture with adversarial training. In this study, we present the DIsentangled Adversarial TOpic Model (DIATOM)1 , aiming at disentangling information related to the target labels (i.e. the review score), from other distinct aspects yet possibly still polarised (e.g. plot descriptions). We also introduce a new dataset, namely the MOBO dataset1 , made up of movie and book reviews, paired with their related plots. The reviews come from different publicly available datasets: IMDB (Maas et al., 2011), GoodReads (Wan et al., 2019) and Amazon reviews (McAuley et al., 2015), and encompass a wide spectrum of domains and styles. We conduct an extensive experimental assessment of our model. First, we assess the topic quality in terms of topic coherence and diversity and compare DIATOM with other supervised topic models on the sentiment classification task; then, we analyse the disentangling rate of topics to quantitatively assess the degree of separation between actual opinion and plot/neutral topics. Our contributions are summarized below: • We propose a new model, DIATOM, which is able to generate disentangled topics thro"
2021.naacl-main.228,W16-5619,0,0.0286971,"engio et al., 2013; Higgins et al., 2018; Gao et al., 2019), the one achieved in DIATOM is analogous to Thomas et al. (2017) and Bengio et al. (2017), where they impose additional constraints to the representations controlled using a reinforcement learning mechanism determining the disentangled factors. Alternatively, in DIATOM we make use of an adversarial approach over the available target labels. Application in text processing has shown promising results (John et al., 2019; Kumar et al., 2017; Hoang et al., 2019; Esmaeili et al., 2019), yet applications to topic modeling are still limited (Wilson et al., 2016) and to the best of our knowledge, there is no work in separating opinion-bearing topics from plot/neutral topics. 3 DIATOM architecture ments (zs ) and plots2 (za ), we aim to learn a model maximizing the joint data-label log-likelihood, log p(x, ys ): Z Z log p(x, ys ) = log p(x, ys , za , zs )dza dzs ≥ Eqφ (za |x),qψ (zs |x,ys ) [log pθ (x|za , zs )] + Eqφ (za |x),qψ (zs |x,ys ) [log pπ (ys |x)]  − KL qφ (za |x)||p(za )  − KL qψ (zs |x, ys )||p(zs ) (1) Inspired by Miao et al. (2016) and Card et al. (2018), we assume the document-level topic distribution for plots can be approximated by a"
2021.newsum-1.7,D18-1443,0,0.0264201,"g it first as an extractive summariser and then as an abstractive one. The BertSumExtAbs pretrained model3 has been used. PG (Pointer-Generator with Coverage Penalty) (See et al., 2017)4 . This uses a 1-layer bidirectional LSTM encoder and a 1-layer unidirectional LSTM decoder with attention, with the possibility of switching between copying words or generating them (Pointer-Generator) and including a coverage mechanism adding up attention distributions of previous steps to minimise repetitions. The ’OpenNMT BRNN (2 layer, emb 256, hid 1024)’ pre-trained model4 has been used. CopyTransformer (Gehrmann et al., 2018)5 . This uses the transformer architecture, but one attention head defines the copy distribution. The ’OpenNMT Transformer’ pretrained model4 has been used. FastAbsRL (Chen and Bansal, 2018)6 . An extractor agent is used to select sentences (using LSTM layers to represent and copy sentences) and an abstractor network is used to compress and paraphrase the selected sentences. Both are trained separately and then the full model is trained with reinforcement learning by using A2C (Mnih et al., 2016). The reported Rouge scores of these models (Lin, 2004) are shown in Table 1. None of the pre-train"
2021.newsum-1.7,N19-1204,0,0.0268445,"ge Penalty) (See et al., 2017), CopyTransformer (Gehrmann et al., 2018), and FastAbsRL (Chen and Bansal, 2018). Those models are applied in combination with the machine translation system MarianMT (JunczysDowmunt et al., 2018) using the Opus-MT models (Tiedemann and Thottingal, 2020). We have evaluated the quality of the summaries for each model and their comparison. Early research on the problem of text summarisation in low resourced languages (although not focused on deliberation) Orˇasan and Chiorean (2008) demonstrated the limitations of machine translation systems at that time. Recently, Ouyang et al. (2019) revisited the problem of low quality translations in low resourced languages and successfully demonstrated the possibility of using abstractive summarisation by retraining their model on corpora that have gone through the same machine translation process. In this study, we complete the cycle, translating from the original language to English, summarising, and translating back to the original language, thus avoiding the need for retraining. Using other approaches, Yao et al. (2015) studied English-to-Chinese summarisation combining an extractive approach with a process of sentence compression"
2021.woah-1.16,N19-1423,0,0.0311404,"Missing"
2021.woah-1.16,P16-2096,0,0.049425,"Missing"
2021.woah-1.16,D19-1474,0,0.0455916,"Missing"
2021.woah-1.16,2020.alw-1.11,0,0.0967008,"Missing"
2021.woah-1.16,W18-5107,0,0.109399,"Missing"
2021.woah-1.16,N16-2013,0,0.0856881,"Missing"
2021.woah-1.16,N19-1144,0,0.0994314,"Missing"
C08-1140,P94-1016,0,0.0287705,"lightly annotated sentences. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Introduction Semantic parsing maps the natural language sentences to complete formal meaning representations. Traditionally, research in the field of semantic parsing can be divided into two categories: rule-based approaches and statistical approaches. Based on hand-crafted semantic grammar rules, rule-based approaches fill slots in semantic frames using word pattern and semantic tokens (Dowding et al., 1994; Ward and Issar, 1994). Such rulebased approaches are typically domain-specific and often fragile. Statistical approaches are generally based on stochastic models. They can be further categorized into three types: generative approaches, discriminative approaches and a hybrid of the two. Generative approaches learn the joint probability model, P (W, C), of input sentence W and its semantic tag sequence C, compute P (C|W ) using the Bayes rule, and then take the most probable tag sequence C. The hidden Morkov model (HMM), being a generative model, has been predominantly used in statistical sema"
C08-1140,H94-1039,0,0.0105406,"ences. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Introduction Semantic parsing maps the natural language sentences to complete formal meaning representations. Traditionally, research in the field of semantic parsing can be divided into two categories: rule-based approaches and statistical approaches. Based on hand-crafted semantic grammar rules, rule-based approaches fill slots in semantic frames using word pattern and semantic tokens (Dowding et al., 1994; Ward and Issar, 1994). Such rulebased approaches are typically domain-specific and often fragile. Statistical approaches are generally based on stochastic models. They can be further categorized into three types: generative approaches, discriminative approaches and a hybrid of the two. Generative approaches learn the joint probability model, P (W, C), of input sentence W and its semantic tag sequence C, compute P (C|W ) using the Bayes rule, and then take the most probable tag sequence C. The hidden Morkov model (HMM), being a generative model, has been predominantly used in statistical semantic parsing. It models"
C16-1084,P14-1062,0,0.0123749,"Missing"
C16-1084,D14-1181,0,0.0345424,"Missing"
C16-1084,W10-1915,0,0.16045,"Missing"
C16-1084,D14-1162,0,0.10844,"Missing"
C16-1084,D15-1044,0,0.0165996,"Missing"
C16-1084,N16-1174,0,0.0218862,"Missing"
C16-1084,W16-0103,0,0.0302208,"Missing"
C16-1084,Q16-1019,0,0.00706576,"Missing"
D15-1225,D13-1068,0,0.085261,"ly. Kawamae (2011) proposed a trend analysis model which used the difference between temporal words and other words in each document to detect topic evolution over time. Ahmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twi"
D15-1225,P13-2099,0,0.0227646,"the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twitter, one of the most widely adopted social media platforms, appears to cover nearly all newswire events (Petrovic et al., 2013). Therefore, approaches have also been proposed for storyline summarization on social media. Given a user input query of an ongoing event, Lin et al. (2012) extracted the storyline of an event by first obtaining relevant tweets and then generating storylines via graph optimization. In (Li and Li, 2013), an evolutionary hierarchical Dirichlet process was proposed to capture the topic evolution pattern in storyline summarization. However, most of the aforementioned approaches do not represent events in the form of structured representation. More importantly, they ignore the dependency of the hierarchical structures of events at different epochs in a storyline. In this paper, we propose a dynamic storyline detection model to overcome the above limitations. 1943 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1943–1948, c Lisbon, Portugal, 17-21 Sep"
D17-1165,W14-2107,0,0.0179905,"ations for stance detection on several hundred labelled examples. Nevertheless, labelled data are expensive to obtain and there is a lack of portability of classifiers trained on one domain to move to another domain. 2.2 Argument Recognition Closely related to stance detection is argument recognition which can be considered as a more fine-grained task that it aims to identify text segments that contain premises that are against or in support of a claim. Cabrio and Villata (2012) combined textual entailment with argumentation theory to automatically extract the arguˇ ments from online debates. Boltuzic and Snajder (2014) trained supervised classifiers for argument extraction from their manually annotated corpus by collecting comments from online discussions about two specific topics. Sardianos et al. (2015) proposed a supervised approach based on Conditional Random Fields for argument extraction from Greek news. Nguyen and Litman (2015) run an LDA model and post-processed the output, computing argument and domain weights for each of the topics, which were then used to extract argument and domain words. Their model outperformed traditional n-grams and lexical/syntactic rules on a collection of persuasive essay"
D17-1165,W15-0503,0,0.0159942,"as a more fine-grained task that it aims to identify text segments that contain premises that are against or in support of a claim. Cabrio and Villata (2012) combined textual entailment with argumentation theory to automatically extract the arguˇ ments from online debates. Boltuzic and Snajder (2014) trained supervised classifiers for argument extraction from their manually annotated corpus by collecting comments from online discussions about two specific topics. Sardianos et al. (2015) proposed a supervised approach based on Conditional Random Fields for argument extraction from Greek news. Nguyen and Litman (2015) run an LDA model and post-processed the output, computing argument and domain weights for each of the topics, which were then used to extract argument and domain words. Their model outperformed traditional n-grams and lexical/syntactic rules on a collection of persuasive essays. Lippi and Torroni (2016a) hypothesized that vocal features of speech can improve argument mining and proposed to train supervised classifiers by combining features from both text and speech for claim detection from annotated political debates. Apart from claim/evidence detection, there has also been work focusing on i"
D17-1165,P12-2041,0,0.0323199,"labelled data by first learning sentence representations via a hashtag prediction auxiliary task and then fine tuning these sentence representations for stance detection on several hundred labelled examples. Nevertheless, labelled data are expensive to obtain and there is a lack of portability of classifiers trained on one domain to move to another domain. 2.2 Argument Recognition Closely related to stance detection is argument recognition which can be considered as a more fine-grained task that it aims to identify text segments that contain premises that are against or in support of a claim. Cabrio and Villata (2012) combined textual entailment with argumentation theory to automatically extract the arguˇ ments from online debates. Boltuzic and Snajder (2014) trained supervised classifiers for argument extraction from their manually annotated corpus by collecting comments from online discussions about two specific topics. Sardianos et al. (2015) proposed a supervised approach based on Conditional Random Fields for argument extraction from Greek news. Nguyen and Litman (2015) run an LDA model and post-processed the output, computing argument and domain weights for each of the topics, which were then used to"
D17-1165,D15-1110,0,0.0274366,"ent and domain words. Their model outperformed traditional n-grams and lexical/syntactic rules on a collection of persuasive essays. Lippi and Torroni (2016a) hypothesized that vocal features of speech can improve argument mining and proposed to train supervised classifiers by combining features from both text and speech for claim detection from annotated political debates. Apart from claim/evidence detection, there has also been work focusing on identification of argument discourse structures such as the prediction of relations among arguments or argument components (Stab and Gurevych, 2014; Peldszus and Stede, 2015). A more recent survey of various machine learning approaches used for argumentation mining can be found in (Lippi and Torroni, 2016b). All these approaches have been largely domainspecific and rely on a small set of labelled data for supervised model learning. 2.3 Topic Modeling for Sentiment/Perspective Detection Topic models can be modified to detect sentiments or perspectives. Lin and He (2009) introduced a joint sentiment topic (JST) model, which simultaneously extracts topics and topic-associated sentiments from text. Trabelsi and Zaıane (2014) proposed a joint topic viewpoint (JTV) mode"
D17-1165,D14-1006,0,0.0333627,"hen used to extract argument and domain words. Their model outperformed traditional n-grams and lexical/syntactic rules on a collection of persuasive essays. Lippi and Torroni (2016a) hypothesized that vocal features of speech can improve argument mining and proposed to train supervised classifiers by combining features from both text and speech for claim detection from annotated political debates. Apart from claim/evidence detection, there has also been work focusing on identification of argument discourse structures such as the prediction of relations among arguments or argument components (Stab and Gurevych, 2014; Peldszus and Stede, 2015). A more recent survey of various machine learning approaches used for argumentation mining can be found in (Lippi and Torroni, 2016b). All these approaches have been largely domainspecific and rely on a small set of labelled data for supervised model learning. 2.3 Topic Modeling for Sentiment/Perspective Detection Topic models can be modified to detect sentiments or perspectives. Lin and He (2009) introduced a joint sentiment topic (JST) model, which simultaneously extracts topics and topic-associated sentiments from text. Trabelsi and Zaıane (2014) proposed a joint"
D17-1165,W14-1305,0,0.15784,"rgument components (Stab and Gurevych, 2014; Peldszus and Stede, 2015). A more recent survey of various machine learning approaches used for argumentation mining can be found in (Lippi and Torroni, 2016b). All these approaches have been largely domainspecific and rely on a small set of labelled data for supervised model learning. 2.3 Topic Modeling for Sentiment/Perspective Detection Topic models can be modified to detect sentiments or perspectives. Lin and He (2009) introduced a joint sentiment topic (JST) model, which simultaneously extracts topics and topic-associated sentiments from text. Trabelsi and Zaıane (2014) proposed a joint topic viewpoint (JTV) model for the detection of latent viewpoints under a certain topic. This is essentially equivalent to the reparameterized version of the JST model called R EVERSE - JST (Lin et al., 2012) in which sentiment label (or viewpoint) generation is dependent on topics, as opposed to JST where topic generation is conditioned on sentiment labels. Fang et al. (2012) proposed a Cross-Perspective Topic Model (CPT) in which the generative processes for topic words (nouns) and opinion words (adjectives, adverbs and verbs) are different, as the opinion words are sample"
D17-1165,S16-1074,0,0.0153681,"et. As previously reported in (Mohammad et al., 2016b), a person may express the same stance towards a target by using negative or positive language. Hence, stance detection is different from sentiment classification and sentiment features alone are not sufficient for stance detection. With the introduction of the shared task of stance detection in tweets in SemEval 2016 (Mohammad et al., 2016a), there have been increasing interests of developing various approaches for stance detection. But most of them focused on building supervised classifiers from labelled data. The best performing system (Zarrella and Marsh, 2016) made use of large unlabelled data by first learning sentence representations via a hashtag prediction auxiliary task and then fine tuning these sentence representations for stance detection on several hundred labelled examples. Nevertheless, labelled data are expensive to obtain and there is a lack of portability of classifiers trained on one domain to move to another domain. 2.2 Argument Recognition Closely related to stance detection is argument recognition which can be considered as a more fine-grained task that it aims to identify text segments that contain premises that are against or in"
D17-1165,S16-1003,0,0.0575975,"Missing"
D17-1167,D14-1190,0,0.0625937,"Missing"
D17-1167,P15-2127,0,0.0423877,"the taxonomy of emotions. Researchers have proposed a list of primary emotions (Plutchik, 1980; Ekman, 1984; Turner, 2000). In this study, we Existing work in emotion analysis mostly focuses on emotion classification (Li et al., 2013; Zhou et al., 2016) and emotion information extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rat"
D17-1167,C10-1021,0,0.529812,"joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based methods, Russo et al. (2011) proposed a crowdsourcing method to construct a common-sense knowledge base which is related to emotion causes. But it is challenging to extend the common-sense knowledge base automatically. Ghazi et al. (2015) used Conditional Random Fields (CRFs) to extract emotion causes. However, it requires emotion"
D17-1167,Y10-1071,0,0.18817,"ure. 1 Introduction With the rapid growth of social network platforms, more and more people tend to share their experiences and emotions online. Emotion analysis of online text becomes a new challenge in Natural Language Processing (NLP). In recent years, studies in emotion analysis largely focus on emotion classification including detection of writers’ emotions (Gao et al., 2013) as well as readers’ emotions (Chang et al., 2015). There are also some information extraction tasks defined in emotion analysis (Chen et al., 2016; Balahur et al., 2011), such as extracting the feeler of an emotion (Das and Bandyopadhyay, 2010). These methods † Corresponding Author: xuruifeng@hit.edu.cn assume that emotion expressions are already observed. Sometimes, however, we care more about the stimuli, or the cause of an emotion. For instance, Samsung wants to know why people love or hate Note 7 rather than the distribution of different emotions. Ex.1 我的手机昨天丢了，我现在很难过。 Ex.1 Because I lost my phone yesterday, I feel sad now. In an example shown above, “sad” is an emotion word, and the cause of “sad” is “I lost my phone”. The emotion cause extraction task aims to identify the reason behind an emotion expression. It is a more diffi"
D17-1167,P13-1095,0,0.0353223,"classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014;"
D17-1167,D14-1181,0,0.00503362,"t by an attention mechanism. Based on the learned attention result, the network maps the text into a low dimensional vector space. This vector is then used to generate an answer. Existing memory network based approaches to QA use weighted sum of attentions to jointly consider short text segments stored in memory. However, they do not explicitly model sequential information in the context. In this paper, we propose a new deep memory network architecture to model the context of each word simultaneously by multiple memory slots which capture sequential information using convolutional operations (Kim, 2014), and achieves the state-of-the-art performance compared to existing methods which use manual rules, common sense knowledge bases or other machine learning models. The rest of the paper is organized as follows. Section 2 gives a review of related works on emotion analysis. Section 3 presents our proposed deep memory network based model for emotion cause extraction. Section 4 discusses evaluation results. Finally, Section 5 concludes the work and outlines the future directions. 2 Related Work Identifying emotion categories in text is one of the key tasks in NLP (Liu, 2015). Going one step furth"
D17-1167,W10-0206,0,0.743034,"s to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based methods, Russo et al. (2011) proposed a crowdsourcing method to construct a common-sense knowledge base which is related to emotion causes. But it is challe"
D17-1167,P13-2091,0,0.0388326,"to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al.,"
D17-1167,D15-1297,0,0.0226715,"16) and emotion information extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method"
D17-1167,D14-1123,0,0.0222424,"(2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based"
D17-1167,D14-1127,0,0.0455892,"inese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) exten"
D17-1167,D09-1150,0,0.0406229,"2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some st"
D17-1167,W11-1720,0,0.272973,"Missing"
D17-1167,P14-2070,0,0.0182461,"al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based methods, Russo et al. (2011)"
D17-1167,D16-1021,0,0.0120791,"use extraction requires an understanding of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015). Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015). To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction. 3 Our Approach In this section, we will first define our task. Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented. 3.1 Task Definition The formal definition of emotion cause extraction is given in (Gui et al., 2016). In this task, a given document, which is a passage about"
D17-1167,D15-1166,0,0.00787256,"in their model learning. Since emotion cause extraction requires an understanding of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015). Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015). To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction. 3 Our Approach In this section, we will first define our task. Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented. 3.1 Task Definition The formal definition of emotion cause extraction is given in (Gui et al., 2016). In this task, a g"
D17-1167,D15-1044,0,0.0332273,"of a given piece of text in order to correctly identify the relation between the description of an event which causes an emotion and the expression of that emotion, it can essentially be considered as a QA task. In our work, we choose the memory network, which is designed to model the relation between a story and a query for QA systems (Weston et al., 2014; Sukhbaatar et al., 2015). Apart from its application in QA, memory network has also achieved great successes in other NLP tasks, such as machine translation (Luong et al., 2015), sentiment analysis (Tang et al., 2016) or summarization (M. Rush et al., 2015). To the best of our knowledge, this is the first work which uses memory network for emotion cause extraction. 3 Our Approach In this section, we will first define our task. Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented. 3.1 Task Definition The formal definition of emotion cause extraction is given in (Gui et al., 2016). In this task, a given document, which is a passage about an emotion event, contains an emotion wo"
D17-1167,P13-1097,0,0.0280841,"formation extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes"
D17-1167,P14-2069,0,0.014941,"classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo et al., 2015; Mohtarami et al., 2013) or emotions in tweets or blogs (Quan and Ren, 2009; Liu et al., 2013; Hasegawa et al., 2013; Qadir and Riloff, 2014; Ou et al., 2014), and emotion lexicon construction (Mohammad and Turney, 2013; Yang et al., 2014; Staiano and Guerini, 2014). However, the aforementioned work all focused on analysis of emotion expressions rather than emotion causes. Lee et al. (2010) first proposed a task on emotion cause extraction. They manually constructed a corpus from the Academia Sinica Balanced Chinese Corpus. Based on this corpus, Chen et al. (2010) proposed a rule based method to detect emotion causes based on manually define linguistic rules. Some studies (Gui et al., 2014; Li and Xu, 2014; Gao et al., 2015) extended the rule based method to informal text in Weibo text (Chinese tweets). Other than rule based m"
D17-1167,D16-1061,0,0.0378978,"categories in text is one of the key tasks in NLP (Liu, 2015). Going one step further, emotion cause extraction can reveal important information about what causes a certain emotion and why there is an emotion change. In this section, we introduce related work on emotion analysis including emotion cause extraction. In emotion analysis, we first need to determine the taxonomy of emotions. Researchers have proposed a list of primary emotions (Plutchik, 1980; Ekman, 1984; Turner, 2000). In this study, we Existing work in emotion analysis mostly focuses on emotion classification (Li et al., 2013; Zhou et al., 2016) and emotion information extraction (Balahur et al., 2013). Xu et al. (2012) used a coarse to fine method to classify emotions in Chinese blogs. Gao et al. (2013) proposed a joint model to co-train a polarity classifier and an emotion classifier. Beck et al. (2014) proposed a Multi-task Gaussian-process based method for emotion classification. Chang et al. (2015) used linguistic templates to predict reader’s emotions. Das and Bandyopadhyay (2010) used an unsupervised method to extract emotion feelers from Bengali blogs. There are other studies which focused on joint learning of sentiments (Luo"
D18-1354,K16-1002,0,0.244028,"al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables. 2.2 Variational Autoregressive Models Recently, some works attempted to combine VAE with autoregressive models to better process input sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to improve the inference of traditional VAEs. The most well-known model is Inverse Autoregressive Flow (IAF), which used a series of invertible transformations based on the au"
D18-1354,E17-2029,0,0.0370672,"eneric responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables. 2.2 Variational Autoregressive Models Recently, some works attempted to combine VAE with autoregressive models to better process input sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to imp"
D18-1354,N16-1014,0,0.0894923,"associating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augmenting the hidden states of a backward RNN. • A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only us"
D18-1354,P16-1094,0,0.131353,"associating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augmenting the hidden states of a backward RNN. • A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only us"
D18-1354,D17-1222,1,0.881935,"Missing"
D18-1354,D16-1230,0,0.071479,"Missing"
D18-1354,N15-1020,0,0.0998218,"Missing"
D18-1354,D17-1065,0,0.0351809,"e diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better"
D18-1354,P17-1061,0,0.404009,"te-of-the-art baselines. 1 Figure 1: Distributions of latent variable Introduction Recently, variational Bayesian models have shown attractive merits from both theoretical and practical perspectives (Kingma and Welling, 2013). As one of the most successful variational Bayesian models, Conditional Variational Auto-Encoder (CVAE) (Kingma et al., 2014) was proposed to improve upon the traditional Sequence-to-Sequence (Seq2Seq) dialogue models. The CVAE based models incorporate stochastic latent variables into decoders in order to generate more relevant and diverse responses (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017). However, existing CVAE ∗ Corresponding author As illustrated in Figure 1, the unimodal latent variable z used in the conventional VAE usually captures simple unimodal pattern of responses. However, in open-domain conversations, an utterance may have various responses which form complex multimodal distributions. To overcome this problem and improve the quality of generated responses, we propose a novel model, named Variational Autoregressive Decoder (VAD) to iteratively incorporate a series of latent variables into the autoregressive decoder. In particular, a distinct late"
D18-1379,stoyanov-cardie-2008-annotating,0,0.219225,"rds, therefore failed to effectively distinguish different emotions carried by the same word in different topical contexts. In this paper, we focus on relevant emotion ranking (RER) by differentiating relevant emotions from irrelevant ones and only learning the rankings of relevant emotions while ignoring the irrelevant ones. A neural network with a novel loss function is proposed to tackle the RER problem. A topic representing a real-world event, an abstract entity, or an object could indicate the subject or context of the emotion. Different topics might contain or invoke different emotions (Stoyanov and Cardie, 2008). Incorporating such latent topics is essential for discovering topic-associated emotions. Motivated by transfer learning, we incorporate hidden topics and the topic distributions generated from a topic model into a neural network for RER. The main contributions of the paper are summarized below: Corresponding author • A novel Interpretable Neural Network for Relevant Emotion Ranking (INN-RER) is proposed. A novel error function is employed to optimize the whole network for parameter estimation. To the best of our knowledge, it is the first neural network based approach for RER. 3423 Proceedin"
D18-1379,S07-1013,0,0.216687,"ed since few votes can not be considered as proper representation of social emotion. In total, 5,586 news articles published from January 2014 to July 2016 were kept, together with the readers’ emotion votes. Ren-CECps corpus (Blogs) (Quan and Ren, 2010) contains 1,487 blogs in Chinese. Each document is annotated with eight basic emotions from writer’s perspective, including anger, anxiety, expect, hate, joy, love, sorrow and surprise, together with their emotion scores indicating the level of emotion intensity in the range of [0, 1]. Higher scores represent higher emotion intensity. SemEval (Strapparava and Mihalcea, 2007) is an English data set containing 1,250 news headlines extracted from Google news, CNN, and many other portals. The news headlines are typically short. Each headline was manually scored in a fine-grained valence scale of 0 to 100 across 6 emotions (i.e., anger, disgust, fear, joy, sad and surprise). After pruning 4 items with the total scores equal to 0, 1246 headlines are got for the experiments. News Category Blogs #Votes Category Touching Shock Amusement Sadness Curiosity Anger 694,006 572,651 869,464 837,431 212,559 1,109,315 Joy Hate Love Sorrow Anxiety Surprise Anger Expect All 4,295,42"
D18-1379,N18-1052,1,0.891904,"enerative model based approaches (Bao et al., 2012; Rao et al., 2014a) usually build on topic models and assume texts are generated from emotions ∗ and hidden topics. While these models can extract emotion-associated topics, they perform less satisfactorily in emotion classification since they are not optimized directly to minimize the misclassification rate. Discriminative model based approaches consider each emotion category as a class label and typically cast emotion detection as a classification problem. Approaches to the prediction of both multiple emotions and their intensities include (Zhou et al., 2018, 2016; Wang and Pal, 2015). Those approaches usually assumed wordlevel representations and ignored the latent topical information behind words, therefore failed to effectively distinguish different emotions carried by the same word in different topical contexts. In this paper, we focus on relevant emotion ranking (RER) by differentiating relevant emotions from irrelevant ones and only learning the rankings of relevant emotions while ignoring the irrelevant ones. A neural network with a novel loss function is proposed to tackle the RER problem. A topic representing a real-world event, an abstr"
D18-1379,D16-1061,1,0.785498,"onal neural network to detect single social emotion from short texts. To predict multiple emotions simultaneously, emotion detection can be solved using multi-label classification. Bhowmick (2009) presented a method for classifying news sentences into multiple emotion categories using an ensemble based multi-label classification technique. Wang and Pal (2015) output multiple emotions with intensities using nonnegative matrix factorization with several novel constraints such as topic correlation and emotion bindings. To predict multiple emotions with different intensities in a single sentence, Zhou et al. (2016) proposed a novel approach based on emotion distribution learning. Following this way, a relevant label ranking framework for emotion detection was proposed for predict multiple relevant emotions as well as the ranking of emotions based on their intensities (Zhou et al., 2018). Our work is partly inspired by (Zhou et al., 2018) for relevant emotion ranking, but with the following differences: (1) our model takes into account latent topics in texts for emotion detection, which was ignored in the model proposed in (Zhou et al., 2018); (2) our model is built upon topic models and neural networks"
D19-1017,S07-1013,0,0.192708,"al., 2018) consists of 5,586 news articles collected from the Sina 181 news Society channel. Each document was kept together with the readers’ emotion votes of the six emotions including Funny, Moved, Angry, Sad, Strange, and Shocked. Ren-CECps corpus (Blogs) (Quan and Ren, 2010) is a Chinese data set containing 1,487 blogs annotated with eight basic emotions from writer’s perspective, including Anger, Anxiety, Expect, Hate, Joy, Love, Sorrow and Surprise. The emotions are represented by their emotion scores in the range of [0, 1]. Higher scores represent higher emotion intensities. SemEval (Strapparava and Mihalcea, 2007) contains 1,250 English news headlines extracted from Google news, CNN, and many other portals, which are manually annotated with a fine-grained valence scale of 0 to 100 across 6 emotions, including Anger, Disgust, Fear, Joy, Sad and Surprise. News Blogs #Votes Touching 694,006 Joy 349.2 anger Shock 572,651 Hate 174.2 disgust Amusement 869,464 Love 610.6 fear 20306 Sadness 837,431 Sorrow 408.4 joy 23613 Curiosity 212,559 Anxiety 422.6 sad 24039 1,109,315 Surprise surprise 21495 Anger Expect All 4,295,426 All PRO Loss #Scores 59.2 Category P et ∈Ri ∪{Θ} es ∈≺(et ) 1 normt,s lt,s Hamming Loss n"
D19-1017,D18-1379,1,0.745624,"relevant emotions from irrelevant ones and only predict the ranking results for the relevant emotion labels. Therefore, the task we need to perform is the relevant emotion ranking. Understanding and automatically ranking users’ emotional states would be potentially useful for downstream applications such as dialogue systems (Picard and Picard, 1997). Multiple emotion detection from texts has been previously addressed in (Zhou et al., 2016) which predicted multiple emotions with different intensities based on emotion distribution learning. A relevant emotion ranking framework was proposed in (Yang et al., 2018) to predict multiple relevant emotions as well as the rankings based on their intensities. However, existing emotion detection approaches do not model the events in texts Introduction The advent and prosperity of social media enable users to share their opinions, feelings and attitudes online. Apart from directly expressing their opinions on social media posts, users can also vote for their emotional states after reading an article online. An example of a news article crawled from Sina News Society Channel together with its associated emotion votes received from readers is illustrated in Figur"
D19-1017,N16-1174,0,0.0804822,"vide clues of how the emotions are evoked with interpretable results. To the best of our knowledge, it is the first deep event-driven neural approach for RER. • To consider event information comprehensively, corpus-level event embeddings are incorporated to consider global events in corpus and document-level event distributions are incorporated to learn document-specific event-related attention respectively. In recent years, deep neural network models have been widely used for text classification. In particular, the attention-based recurrent neural networks (RNNs) (Schuster and Paliwal, 2002; Yang et al., 2016) prevail in text classification. However, these approaches ignore the latent events in texts thus fail to attend on event-related parts. Moreover, they are lack of interpretation. • Experimental results on three different realworld corpora show that the proposed method performs better than the state-of-the-art emotion detection methods and multi-label learning methods. Moreover, the event-driven attention enables dynamically highlighting important event-related parts evoking the emotions in texts. 2 Our work is partly inspired by (Yang et al., 2018) for relevant emotion ranking, but with the f"
D19-1017,I17-1102,0,0.0417697,"Missing"
D19-1017,D14-1162,0,0.0859605,"evant emotions and their rankings can be obtained simultaneously according to the scores assigned by the learned ranking function g. The learning objective of relevant emotion ranking (RER) is to both discriminate relevant emotions from irrelevant ones and to rank relevant emotions according to their intensities. Therefore, to fulfil the requirements of RER, the global ob3.2 Input Embedding Layer The Input Embedding Layer contains word embeddings and event embeddings. Assuming a document di consisting of N words represented as 179 di = {w1 , w2 , ..., wN }, the pre-trained word vector, GloVe (Pennington et al., 2014), is used to obtain the fixed word embedding of each word and di can be represented as di = {x1 , x2 , ..., xN } as shown in Figure 2. Since nouns and verbs are more important than other word types in referring to specific events, they are utilized as inputs of topic model such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to generate events automatically. Therefore, the granularity of extracted events is controlled by the predefined K, the number of events. For the corpus D consisting of K events {e1 , e2 , ..., eK }, the event embedding of the kth event ek can be obtained from the"
D19-1017,P18-1030,0,0.0128124,"riven attentions. 3.4 Encoder Layer ?? ?1? ℎ1? ⋯ ℎ2? ⋯ ? ?? ℎ4? ℎ3? ⋯ ? ℎ? ℎ?? ?1 ?2 … 3.4.1 Word-Level Attention As for word-level attentions, since not all words contribute equally to the meaning of a document, we introduce an attention mechanism to extract words with greater importance and aggregate the representations of those informative words to form the document representation, which is shown in the left part of Figure 2. More concretely, ?? Figure 3: Word Encoder. The Encoder Layer contains both the word encoder and event encoder. As for the word encoder, an alternative RNN structure (Zhang et al., 2018) is used to encode texts into semantic representations since it has been shown to be more effective in encoding longer texts. For document di , formally, a state at time step t can be denoted by: t H =< ht1 , ..., htN , htq > Attention Layer ϕwi = tanh(Ww (hi + hq ) + bw ) exp(ϕ> wi uw ) P aw = i > i exp(ϕwi uw ) X rw = aw i hi (2) (3) i which consists of sub-states hti for the ith word wi in document di and a document-level sub-state htq as shown in Figure 3. The hidden states are independent of each other at the present recurrent step and are connected across recurrent steps, which can captu"
D19-1017,N18-1052,1,0.903981,"5; Rao, 2016). Lin et al. (2008) studied the readers’ emotion detection with various kinds of feature sets on news articles. Quan et al. (2015) detected emotions from texts with a logistic regression model introducing the intermediate hidden variables to model the latent structure of input text corpora. Zhou et al. (2016) predicted multiple emotions with intensities based on emotion distribution learning. A relevant label ranking framework for emotion detection was proposed to predict multiple relevant emotions as well as the rankings of emotions based on their intensities (Yang et al., 2018; Zhou et al., 2018). However, these approaches do not model the latent events in texts. • A novel interpretable relevant emotion ranking model with event-driven attention (IREREA) is proposed. The latent event information is incorporated into a deep learning architecture through event-driven attentions which can provide clues of how the emotions are evoked with interpretable results. To the best of our knowledge, it is the first deep event-driven neural approach for RER. • To consider event information comprehensively, corpus-level event embeddings are incorporated to consider global events in corpus and documen"
D19-1017,D16-1061,1,0.935147,"These emotion votes could be noises (e.g., readers accidentally clicked on a wrong emotion button) and hence can be considered as irrelevant emotions. We need to separate the relevant emotions from irrelevant ones and only predict the ranking results for the relevant emotion labels. Therefore, the task we need to perform is the relevant emotion ranking. Understanding and automatically ranking users’ emotional states would be potentially useful for downstream applications such as dialogue systems (Picard and Picard, 1997). Multiple emotion detection from texts has been previously addressed in (Zhou et al., 2016) which predicted multiple emotions with different intensities based on emotion distribution learning. A relevant emotion ranking framework was proposed in (Yang et al., 2018) to predict multiple relevant emotions as well as the rankings based on their intensities. However, existing emotion detection approaches do not model the events in texts Introduction The advent and prosperity of social media enable users to share their opinions, feelings and attitudes online. Apart from directly expressing their opinions on social media posts, users can also vote for their emotional states after reading a"
D19-1027,P11-1040,0,0.034788,"bute-value pairs and mapped them to manually generated schemes for extracting the natural disaster events. Similarly, to extract the city-traffic related event, Anantharam et al. (2015) viewed the task as a sequential tagging problem and proposed an approach based on the conditional random fields. Zhang (2018) proposed an event extraction approach based on imitation learning, especially on inverse reinforcement learning. Open-domain event extraction aims to extract events without limiting the specific types of events. To analyze individual messages and induce a canonical value for each event, Benson et al. (2011) proposed an approach based on a structured graphical model. By representing an event with a binary tuple which is constituted by a named entity and a date, Ritter et al. (2012) employed some statistic to measure the strength of associations between a named entity and a date. The proposed system relies on a supervised labeler trained on annotated data. In (Abdelhaq et al., 2013), Abdelhaq et al. developed a realtime event extraction system called EvenTweet, and each event is represented as a triple constituted by time, location and keywords. To extract more information, Wang el al. (2015) deve"
D19-1027,P18-1046,0,0.0568217,"Missing"
D19-1027,P14-2114,1,0.952857,"sociations between a named entity and a date. The proposed system relies on a supervised labeler trained on annotated data. In (Abdelhaq et al., 2013), Abdelhaq et al. developed a realtime event extraction system called EvenTweet, and each event is represented as a triple constituted by time, location and keywords. To extract more information, Wang el al. (2015) developed a system employing the links in tweets and combing tweets with linked articles to identify events. Xia el al. (2015) combined texts with the location information to detect the events with low spatial and temporal deviations. Zhou et al. (2014; 2017) represented event as a quadruple and proposed two Bayesian models to extract events from tweets. • We propose a novel Adversarial-neural Event Model (AEM), which is, to the best of our knowledge, the first attempt of using adversarial training for open-domain event extraction. • Unlike existing Bayesian graphical modeling approaches, AEM is able to extract events from different text sources (short and long). And a significant improvement on computational efficiency is also observed. • Experimental results on three datasets show that AEM outperforms the baselines in terms of accuracy, r"
D19-1027,E17-1076,1,0.515958,"default setting. The vertical axis represents methods/parameter settings, the horizontal axis denotes the corresponding performance value. All blue histograms with different intensity are those obtained by AEM. with the default configuration. Google dataset, we further divide the non-location named entities into two categories (‘person’ and ‘organization’) and employ a quadruple <organization, location, person, keyword&gt; to denote an event in news articles. We also remove common stopwords and only keep the recognized named entities and the tokens which are verbs, nouns or adjectives. • DPEMM (Zhou et al., 2017) is a nonparametric mixture model for event extraction. It addresses the limitation of LEM that the number of events should be known beforehand. We implement the model with the default configuration. 4.2 Experimental Results For social media text corpus (FSD and Twitter), a named entity tagger3 specifically built for Twitter is used to extract named entities including locations from tweets. A Twitter Part-of-Speech (POS) tagger (Gimpel et al., 2010) is used for POS tagging and only words tagged with nouns, verbs and adjectives are retained as keywords. For the Google dataset, we use the Stanfo"
D19-1027,P11-2008,0,\N,Missing
D19-1350,P18-1189,0,0.0473887,"Missing"
D19-1350,P17-1036,0,0.0177494,"lexity and topic coherence measure compared to state-of-the-art neural topic models. 1 Introduction Probabilistic topic models have been used widely in nature language processing (Li et al., 2016; Zeng et al., 2018). The fundamental principle is that words are assumed to be generated from latent topics which can be inferred from data based on word co-occurrence patterns (Neal, 1993; Andrieu et al., 2003). In recent years, Variational Autoencoder (VAE) has been proved more effective and efficient to approximating deep, complex and underestimated variance in integrals (Kingma and Welling, 2013; He et al., 2017). However, the VAE-based topic models focus on the construction of deep neural networks to approximate the § † The two authors contributed equally to this work. Corresponding author. intractable distribution between observed words and latent topics based on log-likelihood and the learning objective is to minimise the error of reconstructing the original documents based on the learned latent topic vectors rather than improving the quality of learned topics, for example, measured by coherence scores (Kingma and Welling, 2013; Sønderby et al., 2016; Miao et al., 2016; Card et al., 2017; Srivastav"
D19-1350,P16-1199,1,0.84341,"ls to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models. 1 Introduction Probabilistic topic models have been used widely in nature language processing (Li et al., 2016; Zeng et al., 2018). The fundamental principle is that words are assumed to be generated from latent topics which can be inferred from data based on word co-occurrence patterns (Neal, 1993; Andrieu et al., 2003). In recent years, Variational Autoencoder (VAE) has been proved more effective and efficient to approximating deep, complex and underestimated variance in integrals (Kingma and Welling, 2013; He et al., 2017). However, the VAE-based topic models focus on the construction of deep neural networks to approximate the § † The two authors contributed equally to this work. Corresponding auth"
D19-1350,P18-1091,0,0.0164164,"e the vocabulary size and achieve better topic extraction results. Word filtering is often done heuristically. Although there have been attempts to automatically distinguishing background words and topic words, existing approaches either require a switch variable defined at each word position to indicate whether the word is a background word, which makes the models cumbersome, or model each latent topic as the deviation in logfrequency from a constant background distribution (Eisenstein et al., 2011; Smith et al., 2018). In this paper, we propose a new framework to use reinforcement learning (Pan et al., 2018; Qin et al., 2018; Yin et al., 2018) to incorporate the topic coherence measures into the learning of a neural topic model and filter background words dynamically. More concretely, given an input document, its constituent words will first be sampled 3478 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3478–3483, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Neural topic model with reinforcement learning. by a weight vector w"
D19-1350,P18-1199,0,0.0214504,"ize and achieve better topic extraction results. Word filtering is often done heuristically. Although there have been attempts to automatically distinguishing background words and topic words, existing approaches either require a switch variable defined at each word position to indicate whether the word is a background word, which makes the models cumbersome, or model each latent topic as the deviation in logfrequency from a constant background distribution (Eisenstein et al., 2011; Smith et al., 2018). In this paper, we propose a new framework to use reinforcement learning (Pan et al., 2018; Qin et al., 2018; Yin et al., 2018) to incorporate the topic coherence measures into the learning of a neural topic model and filter background words dynamically. More concretely, given an input document, its constituent words will first be sampled 3478 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3478–3483, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Neural topic model with reinforcement learning. by a weight vector which assigns highe"
D19-1350,P17-1072,0,0.0418122,"Missing"
D19-1350,P18-1053,0,0.0267251,"tter topic extraction results. Word filtering is often done heuristically. Although there have been attempts to automatically distinguishing background words and topic words, existing approaches either require a switch variable defined at each word position to indicate whether the word is a background word, which makes the models cumbersome, or model each latent topic as the deviation in logfrequency from a constant background distribution (Eisenstein et al., 2011; Smith et al., 2018). In this paper, we propose a new framework to use reinforcement learning (Pan et al., 2018; Qin et al., 2018; Yin et al., 2018) to incorporate the topic coherence measures into the learning of a neural topic model and filter background words dynamically. More concretely, given an input document, its constituent words will first be sampled 3478 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3478–3483, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Neural topic model with reinforcement learning. by a weight vector which assigns higher weights to words"
D19-1350,N18-1035,0,0.0294293,"earning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models. 1 Introduction Probabilistic topic models have been used widely in nature language processing (Li et al., 2016; Zeng et al., 2018). The fundamental principle is that words are assumed to be generated from latent topics which can be inferred from data based on word co-occurrence patterns (Neal, 1993; Andrieu et al., 2003). In recent years, Variational Autoencoder (VAE) has been proved more effective and efficient to approximating deep, complex and underestimated variance in integrals (Kingma and Welling, 2013; He et al., 2017). However, the VAE-based topic models focus on the construction of deep neural networks to approximate the § † The two authors contributed equally to this work. Corresponding author. intractable dist"
E17-1076,P11-1040,0,0.563481,"Missing"
E17-1076,P11-2008,0,0.0812647,"Missing"
E17-1076,P14-2114,1,0.919633,"strength of association between each named entity y and date d is measured based on the number of co-occurring tweets in order to form a binary tuple hy, di to represent an event. However, TwiCal relies on a supervised sequence labeler trained on tweets annotated with event mentions for the identification of eventrelated phrases. Assuming that each tweet message m ∈ {1..M } is assigned to one event instance e, while e is modeled as a joint distribution over the named entities y, the date/time d when the event occurred, the location l where the event occurred and the event-related keywords k, Zhou et al. (2014; 2015) proposed an unsupervised Bayesian model called latent event model (LEM) for event extraction from Twitter. However, LEM requires the number of events to be known beforehand, which is not realistic in practical applications. To address this limitation, in this paper, a non-parametric mixture model for event extraction is proposed, in which the number of events is inferred automatically from data. Moreover, the lexical variation of the same named entity, for example, “Charles” and “The Prince of Wales”, if identified properly, could be exploited to help in detecting the same event descri"
he-etal-2012-quantising,W11-0705,0,\N,Missing
he-etal-2012-quantising,pak-paroubek-2010-twitter,0,\N,Missing
I11-1129,P07-1056,0,0.00823906,"bjective utterance from conversation data (Wilson and Raaijmakers, 2008; Raaijmakers et al., 2008; Murray and Carenini, 2009). However, the aforementioned line of work tackled subjectivity detection either as supervised or semi-supervised learning, requiring labelled data and extensive knowledge which are expensive to acquire. On the other hand, both subjectivity and sentiment are context sensitive and in general quite domain dependent (Pang and Lee, 2008), so that classifiers trained on one domain often fail to produce satisfactory performance when shifted to new domains (Gamon et al., 2005; Blitzer et al., 2007). Moreover, user generated content from web are often massive and evolve rapidly over time, which imposes more challenges to the subjectivity detection task. These observations have thus motivated us to develop a subjectivity detection algorithm that is relatively simple compared to existing methods (e.g., based on bootstrapping or n-gram features), and yet can easily be trans1153 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1153–1161, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ferred between domains through unsupervised or weakly-s"
I11-1129,W06-1641,0,0.00994916,"(Riloff and Wiebe, 2003). 2.2 Weakly-supervised Sentiment Classification In this section, we first review some work in sentiment analysis using generative models as it partly inspires our work of viewing subjectivity detection as generative model learning. We then discuss other weakly-supervised sentiment classification approaches which also use prior word knowledge. Intuitively, sentiment or subjectivity are context dependent. Therefore, modelling topic coupled with sentiment should serve a critical function in sentiment analysis. There has seen several lines of work pursuing this direction. Eguchi and Lavrenko (2006) considered the topic dependence of sentiment and combined sentiment mod1154 els with topic models for sentiment retrieval. Mei et al. (2007) proposed the topic-sentiment mixture (TSM) model for capturing mixture of topics and sentiment simultaneously on Weblogs. The multiaspect sentiment (MAS) model by Titov and McDonald (2008) focused on aggregating sentiment text for sentiment summary of rating aspects. The more recently proposed joint sentimenttopic (JST) model (Lin and He, 2009; Lin et al., 2010) holds the closest paradigm to the proposed subjLDA model. They targeted document-level sentim"
I11-1129,esuli-sebastiani-2006-sentiwordnet,0,0.00568589,"ith the proposed subjectivity detection LDA (subjLDA) model. In this model, the generative process involves: (1) three subjectivity labels for sentences (i.e., sentence expresses subjective opinions as being positive/negative, or states facts as being objective); (2) a sentiment label for each word in the sentence (either positive, negative, or neutral), and (3) the words in the sentences. We test the subjLDA model on the publicly available Multi-Perspective Question Answering (MPQA) dataset. Two lists of domain independent subjectivity lexicons, namely the subjClue and SentiWordNet lexicons (Esuli and Sebastiani, 2006), were incorporated as prior knowledge for subjLDA model learning. Preliminary results show that the weakly-supervised subjLDA model is able to significantly outperform baseline. Furthermore, it was found that while incorporating subjectivity clues bearing positive or negative polarity can achieve a significant performance gain, the prior lexical information from neutral words is less effective for improving the classification accuracy. The rest of the paper is organized as follows. Section 2 reviews the previous work on subjectivity classification. Section 3 presents the subjLDA model. Experi"
I11-1129,W10-2918,1,0.837799,"in sentiment analysis. There has seen several lines of work pursuing this direction. Eguchi and Lavrenko (2006) considered the topic dependence of sentiment and combined sentiment mod1154 els with topic models for sentiment retrieval. Mei et al. (2007) proposed the topic-sentiment mixture (TSM) model for capturing mixture of topics and sentiment simultaneously on Weblogs. The multiaspect sentiment (MAS) model by Titov and McDonald (2008) focused on aggregating sentiment text for sentiment summary of rating aspects. The more recently proposed joint sentimenttopic (JST) model (Lin and He, 2009; Lin et al., 2010) holds the closest paradigm to the proposed subjLDA model. They targeted document-level sentiment detection with weakly-supervised generative model learning, where the only knowledge being incorporated was from generic sentiment lexicons. In the JST model, topics are assumed to be generated dependent on sentiment distributions and then words are generated conditioned on sentiment-topic pairs. However, there are several intrinsic differences between JST and subjLDA: (1) JST mainly focused on document-level sentiment classification, while, in contrast, subjLDA has a different scope of targeting"
I11-1129,P07-1123,0,0.0737072,"he previous work on subjectivity classification. Section 3 presents the subjLDA model. Experimental setup and results on the MPQA dataset are discussed in Sections 4 and 5, respectively. Finally, Section 6 concludes the paper and outlines the future work. 2 2.1 Related Work Subjectivity Detection While sentiment classification and subjectivity detection are closely related to each other, it has been reported that separating subjective and objective instances from text is more difficult than sentiment classification, and the improvement of subjectivity detection can benefit the latter as well (Mihalcea et al., 2007). Early work by Riloff and Wiebe (2003) focused on a bootstrapping method for sentencelevel subjectivity detection. They started with high-precision subjectivity classifiers which automatically identified subjective and objective sentences in un-annotated texts. The subjective expression patterns were learned from syntactic structure output from the previously labelled high confidence texts. The learned patterns were used to automatically identify additional subjective sentences, which enlarged the training set, and the entire process was then iterated. Wiebe and Riloff (2005) used very simila"
I11-1129,D08-1049,0,0.135271,"Wiebe and Riloff, 2005; Pang and Lee, 2008). Work on sentence-level subjectivity detection is relatively sparse compared to document-level sentiment classification. Early work used a bootstrapping algorithm to learn subjective (Riloff and Wiebe, 2003) or both subjective and objective (Wiebe and Riloff, 2005) expressions for sentence-level subjectivity detection. In contrast to bootstrapping, there has been some recent attempts exploring various n-gram features and different level of lexical instantiation for detecting subjective utterance from conversation data (Wilson and Raaijmakers, 2008; Raaijmakers et al., 2008; Murray and Carenini, 2009). However, the aforementioned line of work tackled subjectivity detection either as supervised or semi-supervised learning, requiring labelled data and extensive knowledge which are expensive to acquire. On the other hand, both subjectivity and sentiment are context sensitive and in general quite domain dependent (Pang and Lee, 2008), so that classifiers trained on one domain often fail to produce satisfactory performance when shifted to new domains (Gamon et al., 2005; Blitzer et al., 2007). Moreover, user generated content from web are often massive and evolve rap"
I11-1129,D09-1026,0,0.00958357,"the prior sentiment label of w in a sentiment lexicon, i.e., positive, negative or neutral. For example, the word “excellent” with index wt has a positive sentiment polarity. The corresponding row vector λwt is [0.05, 0.9, 0.05] with its elements representing neutral, positive, and negative prior polarity. Multiplying β with λ, we can enforce that the word “excellent”has much higher probability of being drawn from the positive topic word distributions generated from a Dirichlet distribution with parameter βlpos wt . The previously proposed DiscLDA (LacosteJulien et al., 2008) and Labeled LDA (Ramage et al., 2009) also utilize a transformation matrix to modify Dirichlet priors by assuming the availability of document class labels. In contrast, we use word prior sentiment as supervised information to modify the topic-word Dirichlet priors. Model Inference The total probability of the model is Q P (w, l, s, θ, ϕ, π; α, β, γ) = Sj=1 P (ϕj ; λ × β)· QMd QD m=1 P (sd,m |πd )P (θd,m ; αsd,m )· d=1 P (πd ; γ) QNd,m (2) t=1 P (ld,m,t |θd,m )P (wd,m,t |ϕld,m,t ), where the bold-font variables denote vectors. We use Gibbs sampling to estimate the posterior of subjLDA by sequentially sampling each variable of int"
I11-1129,W03-1014,0,0.761206,"cuments are opinionated, and ideally only contain subjective statements. Document summarization systems need to summarize different perspectives and opinions. For question answering systems, extracting and presenting information of the appropriate type, i.e., opinions or facts, is imperative according to the specific question being asked (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005; Pang and Lee, 2008). Work on sentence-level subjectivity detection is relatively sparse compared to document-level sentiment classification. Early work used a bootstrapping algorithm to learn subjective (Riloff and Wiebe, 2003) or both subjective and objective (Wiebe and Riloff, 2005) expressions for sentence-level subjectivity detection. In contrast to bootstrapping, there has been some recent attempts exploring various n-gram features and different level of lexical instantiation for detecting subjective utterance from conversation data (Wilson and Raaijmakers, 2008; Raaijmakers et al., 2008; Murray and Carenini, 2009). However, the aforementioned line of work tackled subjectivity detection either as supervised or semi-supervised learning, requiring labelled data and extensive knowledge which are expensive to acqui"
I11-1129,P08-1036,0,0.0264483,"s which also use prior word knowledge. Intuitively, sentiment or subjectivity are context dependent. Therefore, modelling topic coupled with sentiment should serve a critical function in sentiment analysis. There has seen several lines of work pursuing this direction. Eguchi and Lavrenko (2006) considered the topic dependence of sentiment and combined sentiment mod1154 els with topic models for sentiment retrieval. Mei et al. (2007) proposed the topic-sentiment mixture (TSM) model for capturing mixture of topics and sentiment simultaneously on Weblogs. The multiaspect sentiment (MAS) model by Titov and McDonald (2008) focused on aggregating sentiment text for sentiment summary of rating aspects. The more recently proposed joint sentimenttopic (JST) model (Lin and He, 2009; Lin et al., 2010) holds the closest paradigm to the proposed subjLDA model. They targeted document-level sentiment detection with weakly-supervised generative model learning, where the only knowledge being incorporated was from generic sentiment lexicons. In the JST model, topics are assumed to be generated dependent on sentiment distributions and then words are generated conditioned on sentiment-topic pairs. However, there are several i"
I11-1129,W03-1017,0,0.0384197,"presses opinions (subjective) or reports facts (objective). Such a task of distinguishing subjective information from objective is useful for many natural language processing applications. For instance, sentiment classification often assumes that the input documents are opinionated, and ideally only contain subjective statements. Document summarization systems need to summarize different perspectives and opinions. For question answering systems, extracting and presenting information of the appropriate type, i.e., opinions or facts, is imperative according to the specific question being asked (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005; Pang and Lee, 2008). Work on sentence-level subjectivity detection is relatively sparse compared to document-level sentiment classification. Early work used a bootstrapping algorithm to learn subjective (Riloff and Wiebe, 2003) or both subjective and objective (Wiebe and Riloff, 2005) expressions for sentence-level subjectivity detection. In contrast to bootstrapping, there has been some recent attempts exploring various n-gram features and different level of lexical instantiation for detecting subjective utterance from conversation data (Wilson and Raaijmakers, 2008;"
I11-1129,C08-1135,0,0.0118687,"e topic-word Dirichlet priors and essentially create an informed prior distribution for the sentiment labels. Another common solution to weaklysupervised sentiment classification is to make use of prior word polarity knowledge, where one uses a small number of seed words with known polarity to infer the polarity of a large set of unidentified terms. Turney and Littman (2002) classified the sentiment orientation of other terms in the corpus through mutual information, based on a small set of positive/negative paradigm words. Starting with a single seed word meaning “good” and a negation check, Zagibalov and Carroll (2008) derived a classifier through iteratively retraining, and treated sentiment and subjectivity as a continuum rather than distinct classes. 3 The SubjLDA Model As shown in Figure 1(b), subjLDA is essentially a four-layer Bayesian model. In order to generate a word wd,m,t (i.e., the tth word token of sentence m within document d), one first chooses a subjectivity label1 sd,m ∈ [1, K] for each sentence in document d from the per-document subjectivity distribution πd . Following that, one chooses a sentiment label ld,m,t ∈ [1, S] for each word in the sentences from the per-sentence sentiment distri"
I11-1129,D09-1140,0,0.372477,"ang and Lee, 2008). Work on sentence-level subjectivity detection is relatively sparse compared to document-level sentiment classification. Early work used a bootstrapping algorithm to learn subjective (Riloff and Wiebe, 2003) or both subjective and objective (Wiebe and Riloff, 2005) expressions for sentence-level subjectivity detection. In contrast to bootstrapping, there has been some recent attempts exploring various n-gram features and different level of lexical instantiation for detecting subjective utterance from conversation data (Wilson and Raaijmakers, 2008; Raaijmakers et al., 2008; Murray and Carenini, 2009). However, the aforementioned line of work tackled subjectivity detection either as supervised or semi-supervised learning, requiring labelled data and extensive knowledge which are expensive to acquire. On the other hand, both subjectivity and sentiment are context sensitive and in general quite domain dependent (Pang and Lee, 2008), so that classifiers trained on one domain often fail to produce satisfactory performance when shifted to new domains (Gamon et al., 2005; Blitzer et al., 2007). Moreover, user generated content from web are often massive and evolve rapidly over time, which impose"
I13-1013,P12-1036,0,0.0238833,"that is a function of observed meta data of the document. Labeled LDA (Ramage et al., 2009) defines a one-toone correspondence between LDA’s latent topics and observed document labels and utilize a transformation matrix to modify Dirichlet priors. Partially Labeled LDA (PLDA) extends Labeled LDA to incorporate per-label latent topics (Ramage et al., 2011). The DF-LDA model (Andrzejewski et al., 2009) employs must-link and cannot-link constraints as Dirichlet Forest priors for LDA learning, but it suffers the scalability issue. Most recently, the aspect extraction model for sentiment analysis (Mukherjee and Liu, 2012) assumes that a seed set is given which consists of words together with their respective aspect category. Then depending on whether a word is a seed or non-seed word, a different route of multinomial distribution will be taken to emit the word. Our work was partially inspired by the previously proposed joint sentiment-topic model (JST) (Lin and He, 2009; Lin et al., 2012), which extracts topics grouped under different sentiments, relying only on domainindependent polarity word prior information. While the afore-mentioned approaches assume the existence of either document label information or w"
I13-1013,N10-1012,0,0.00462675,"xes the Moscow bombing event with the Egyptian protesters Museum attack event. When checking the topics produced by PLDA we can see that it fails to correctly characterise violen and no-violent topics, since PLDA T2 should have been clearly classified as non-violent and the non-violent PLDA T1 as violent. Moreover in the violent PLDA T1 topic which presents violent related words, we can empirically identify more than one event involved. In order to measure the semantic topical coherence of VDM and the proposed baselines, we made use of the Pointwise Mutual Information(PMI) metric proposed in (Newman et al., 2010). PMI is an automatic topic coherence evaluation which has been found to correspond well with human judgements on topic coherence. In particular, a coherent topic should only contain semantically related words and hence any pair of the top words from the same topic should have a large PMI value. For each topic, we compute its PMI by averaging over the PMI of all the word pairs extracted from the top 10 topic words. Figure 3 shows the PMI values of topics extracted under the violence and non-violence classes with the topic numbers varying between 5 and 30. It can be observed that JST and PLDA g"
I13-1013,D09-1026,0,0.0346638,"orporates word prior knowledge into model learning. Here, we also review existing approaches for the incorporation of supervised information into LDA model learning. The supervised LDA (sLDA) (Blei and McAuliffe, 2008) uses empirical topic frequencies as a covariant for http://dbpedia.org http://www.opencalais.com 3 110 http://wikipedia.org a regression on document labels such as movie ratings. The Dirichlet-multinomial regression (DMR) model (Mimno and McCallum, 2008) uses a loglinear prior on document-topic distributions that is a function of observed meta data of the document. Labeled LDA (Ramage et al., 2009) defines a one-toone correspondence between LDA’s latent topics and observed document labels and utilize a transformation matrix to modify Dirichlet priors. Partially Labeled LDA (PLDA) extends Labeled LDA to incorporate per-label latent topics (Ramage et al., 2011). The DF-LDA model (Andrzejewski et al., 2009) employs must-link and cannot-link constraints as Dirichlet Forest priors for LDA learning, but it suffers the scalability issue. Most recently, the aspect extraction model for sentiment analysis (Mukherjee and Liu, 2012) assumes that a seed set is given which consists of words together"
L18-1026,D13-1190,0,0.0563479,"Missing"
L18-1026,P11-2015,0,0.0573154,"Missing"
L18-1026,L16-1651,1,0.843589,"pproach (Udochukwu and He, 2015). In addition, we also perform polarity detection (positive and negative) using majority voting based on the lexicon matching results obtained with three sentiment lexicons, SentiWordNet (Esuli and Sebastiani, 2005), AFINN (Hansen et al., 2011) and the Subjectivity Lexicon (Wilson et al., 2005). We implement a contextual valence shifter as described in (Polanyi and Zaenen, 2006) to detect polarity change in context. Apart from emotion and polarity features, we also consider the expressions of blame and praise as additional features using the method proposed in (Orizu and He, 2016) 8 5 Bias Features http://www.mpi-sws.org/˜cristian/Biased_ language.html 9 https://en.wikipedia.org/wiki/Wikipedia: Manual_of_Style/Words_to_watch 10 http://www.sfu.ca/rst/01intro/ definitions.html 168 Feature Name Description Sentence Level Average Sentence Length Average length of the sentences in the document Average Unique Word Count Average # of unique words per sentence Average number of punctuations per sentence Average Punctuation Adjective Rate Rate of adjectives per sentence CC Rate Rate of coordinating conjunctions per sentence Pronouns Rate Rate of pronouns per sentence Word Count"
L18-1026,panicheva-etal-2010-personal,0,0.0272809,"will tend to increase the reader’s interest/regard for the subject matter; 4. Since the choice of words projects opinions and preferences, CoI articles likely contain more expressions of implicit or explicit emotions. In this section, we explore a rich set of features to test our hypotheses above and to train supervised classifiers for CoI detection. 3.1. Stylometric Features Stylometric features attempt to recognise patterns of style in text. These techniques have been traditionally applied to attribute authorship (Reddy et al., 2016; Stamatatos, 2009; Argamon et al., 2009), opinion mining (Panicheva et al., 2010), and forensic linguistics (Turell, 2010; Olsson and Luchjenbroers, 2013). We create a list of features selected from previous research work in vandalism and bias as mentioned in the Related Work section. Since not all features are relevant to our CoI detection task, We perform feature selection using the implementation of InfoGain and Chi-Square available in Weka5 to eliminate insignificant features. We also include the nine universal dependency groups6 , detection of which is done using the Stanford Dependency Parser7 . The final set of features is listed in Table 1. This set of features is"
L18-1026,P13-1162,0,0.0318592,"2010; Olsson and Luchjenbroers, 2013). We create a list of features selected from previous research work in vandalism and bias as mentioned in the Related Work section. Since not all features are relevant to our CoI detection task, We perform feature selection using the implementation of InfoGain and Chi-Square available in Weka5 to eliminate insignificant features. We also include the nine universal dependency groups6 , detection of which is done using the Stanford Dependency Parser7 . The final set of features is listed in Table 1. This set of features is relating to Hypothesis 1. 3.2. In (Recasens et al., 2013), two major classes of bias in Wikipedia edits have been discussed, framing bias and epistemological bias. The former is realised by subjective words or phrases linked with a particular point of view, while the latter is related to linguistic features that subtly focus on the believability of a proposition. We use the same classes of bias as discussed in (Recasens et al., 2013) and identify existence of the classes in a Wikipedia article based on a bias lexicon8 . We also consider other words/phrases which may introduce bias as illustrated in the Wikipedia’s manual of style/Words to Watch9 . T"
L18-1026,H05-1044,0,0.0150576,"sed explicitly by using “emotion-bearing words” or implicitly without such words. For explicit emotions, we use a simple lexicon-based approach with negation handling based on a modified version of the NRC lexicon (Mohammad and Turney, 2013); and for implicit emotions, we use the rule-based approach (Udochukwu and He, 2015). In addition, we also perform polarity detection (positive and negative) using majority voting based on the lexicon matching results obtained with three sentiment lexicons, SentiWordNet (Esuli and Sebastiani, 2005), AFINN (Hansen et al., 2011) and the Subjectivity Lexicon (Wilson et al., 2005). We implement a contextual valence shifter as described in (Polanyi and Zaenen, 2006) to detect polarity change in context. Apart from emotion and polarity features, we also consider the expressions of blame and praise as additional features using the method proposed in (Orizu and He, 2016) 8 5 Bias Features http://www.mpi-sws.org/˜cristian/Biased_ language.html 9 https://en.wikipedia.org/wiki/Wikipedia: Manual_of_Style/Words_to_watch 10 http://www.sfu.ca/rst/01intro/ definitions.html 168 Feature Name Description Sentence Level Average Sentence Length Average length of the sentences in the do"
L18-1273,W07-0734,0,0.0277804,"representation of the image caption. Finally, we use the generated vector to retrieve the most similar sentence from the original news article based on cosine similarity measurement as the caption of the given image. We also explore a number of variants of our proposed architecture and compare them with the previous work on the news image captioning task. Our experimental results on the BBC News Corpus show that our proposed strategies outperform traditional methods according to automatic evaluation metrics like BLEU scores (Papineni et al., 2002) and are comparable in terms of Meteor Scores (Lavie and Agarwal, 2007). Since automatic evaluation metrics are currently limited by their capability to measure the quality of caption generation models, a human evaluation experiment has also been conducted, where users were shown the news articles from our test dataset. Our evaluation results show that captions generated by our proposed approach were more favoured than captions generated by an existing model based on LDA. In what follows, we first discuss related work and then describe our proposed methodology, followed by experiments and results, and finally conclude the paper and outline future research directi"
L18-1273,P02-1040,0,0.117241,"t-Term Memory (LSTM) network (Sak et al., 2014) to generate a vector representation of the image caption. Finally, we use the generated vector to retrieve the most similar sentence from the original news article based on cosine similarity measurement as the caption of the given image. We also explore a number of variants of our proposed architecture and compare them with the previous work on the news image captioning task. Our experimental results on the BBC News Corpus show that our proposed strategies outperform traditional methods according to automatic evaluation metrics like BLEU scores (Papineni et al., 2002) and are comparable in terms of Meteor Scores (Lavie and Agarwal, 2007). Since automatic evaluation metrics are currently limited by their capability to measure the quality of caption generation models, a human evaluation experiment has also been conducted, where users were shown the news articles from our test dataset. Our evaluation results show that captions generated by our proposed approach were more favoured than captions generated by an existing model based on LDA. In what follows, we first discuss related work and then describe our proposed methodology, followed by experiments and resu"
L18-1273,D13-1170,0,0.00578313,"Missing"
N16-1166,W14-2107,0,0.0650195,"Missing"
N16-1166,P98-1013,0,0.464853,"her candidates. In this corpus, each debate transcript lists the speakers including moderator and candidates and questions asked during the debate. Each transcript also clearly delimits turns between speakers and moderators as well as mark-up occurrences of the audience’s reactions such as booing and laughter. 3.3 Semantic Frames We propose to make use of the persuasion essays corpus annotations to understand persuasive argumentation in political debates by means of the use of semantic frames. A semantic frame is a description of context in which a word sense is used. We make use of FrameNet (Baker et al., 1998), which consist of over 1000 patterns used in English (e.g., Leadership, Causality, Awareness, and Hostile encounter). In this work we extract such patterns using SEMAFOR (Das et al., 2010). Consider the sentence in Table 2 in which two semantic frames are detected. Each parsed semantic frame consists of {Frame, SemanticRole, label} providing a higher level characterisation of a text, highlighting the semantics of the discourse used in this text. If such semantic frames appear to be some of the most prominent features for a certain persuasive argumentation annotation scheme (e.g., “Claim”), th"
N16-1166,I13-1191,0,0.0133908,"t in influencing an audience in supporting their candidature. We model the influence index of each candidate based on their relative standings in the polls released prior to the debate and present a system which ranks speakers in terms of their relative influence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker’s influence rank, persuasive argumentation also affects such indices. 1 Introduction In recent years, researchers have studied political texts detecting ideological positions (Sim et al., 2013; Hasan and Ng, 2013), predicting voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011) and characterising power based on linguistic features (Prabhakaran et al., 2013). While there is a vast amount of theoretical research on the rhetoric of politicians, only recently there has been a growing interest in understanding the argumentation processes involved in political communication by means of computational linguistics (Hasan and ˇ Ng, 2013; Boltuˇzi´c and Snajder, 2014). Previous work (Rosenberg and Hirschberg, 2009) has analysed political speech transcripts identifying prosodic and lexical-syntactic cues"
N16-1166,I13-1042,0,0.408972,"polls released prior to the debate and present a system which ranks speakers in terms of their relative influence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker’s influence rank, persuasive argumentation also affects such indices. 1 Introduction In recent years, researchers have studied political texts detecting ideological positions (Sim et al., 2013; Hasan and Ng, 2013), predicting voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011) and characterising power based on linguistic features (Prabhakaran et al., 2013). While there is a vast amount of theoretical research on the rhetoric of politicians, only recently there has been a growing interest in understanding the argumentation processes involved in political communication by means of computational linguistics (Hasan and ˇ Ng, 2013; Boltuˇzi´c and Snajder, 2014). Previous work (Rosenberg and Hirschberg, 2009) has analysed political speech transcripts identifying prosodic and lexical-syntactic cues which correlate with political personalities. Prabhakaran et al. (2013) proposed interactions within political debates as predictors of a candidate’s relat"
N16-1166,D14-1157,0,0.113558,"Missing"
N16-1166,D13-1010,0,0.0161598,"le and their effect in influencing an audience in supporting their candidature. We model the influence index of each candidate based on their relative standings in the polls released prior to the debate and present a system which ranks speakers in terms of their relative influence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker’s influence rank, persuasive argumentation also affects such indices. 1 Introduction In recent years, researchers have studied political texts detecting ideological positions (Sim et al., 2013; Hasan and Ng, 2013), predicting voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011) and characterising power based on linguistic features (Prabhakaran et al., 2013). While there is a vast amount of theoretical research on the rhetoric of politicians, only recently there has been a growing interest in understanding the argumentation processes involved in political communication by means of computational linguistics (Hasan and ˇ Ng, 2013; Boltuˇzi´c and Snajder, 2014). Previous work (Rosenberg and Hirschberg, 2009) has analysed political speech transcripts identifying prosodic and le"
N16-1166,strapparava-etal-2010-predicting,0,0.0317219,"se the use of emotive language, we generated a list of emotionrelated semantic frames (e.g., emotion directed, emotions by stimulus, emotions by possibility)4 , then for each speaker u in each debate d, we generated an emotion-frame vector weighted by tf-idf. Once the features for each speaker have been generated, we followed a supervised learning approach for ranking speakers of a debate based on their influence Index, which can be used to denote how well a speakers participation on a debate has impacted the audience endorsement of his/her campaign. 4.1.3 External Emotion Cues Previous work (Strapparava et al., 2010) has shown that an audiences’ social signal reactions to an idea, such as booing or cheering, are good pre4 FrameNet’s frame index, http://tinyurl.com/ q2ytth9 dictors of hot-spots where persuasion attempts succeeded or at least such attempts were recognised by the audience. In this work, rather than recognising such persuasion hot-spots, we explore these audiences’ reaction cues (e.g applause) as potential predictors of a candidate success on a political debate, we refer to such cues as external emotion cues. For each speaker in a debate, we computed the number of i) applauses (APL); ii) booi"
N16-1166,W06-1639,0,0.0958466,"candidature. We model the influence index of each candidate based on their relative standings in the polls released prior to the debate and present a system which ranks speakers in terms of their relative influence using a combination of content and persuasive argumentation features. Our results show that although content alone is predictive of a speaker’s influence rank, persuasive argumentation also affects such indices. 1 Introduction In recent years, researchers have studied political texts detecting ideological positions (Sim et al., 2013; Hasan and Ng, 2013), predicting voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011) and characterising power based on linguistic features (Prabhakaran et al., 2013). While there is a vast amount of theoretical research on the rhetoric of politicians, only recently there has been a growing interest in understanding the argumentation processes involved in political communication by means of computational linguistics (Hasan and ˇ Ng, 2013; Boltuˇzi´c and Snajder, 2014). Previous work (Rosenberg and Hirschberg, 2009) has analysed political speech transcripts identifying prosodic and lexical-syntactic cues which correlate with political personalities. Pra"
N16-1166,C98-1013,0,\N,Missing
N18-1052,P82-1020,0,0.735422,"Missing"
N18-1052,D16-1061,1,0.623636,"rvised learning approaches consider each emotion category as a class label and emotion detection is cast as a classification problem. If only choosing the strongest emotion as the emotion label for a given text, emotion detection is essentially a single-label classification problem (Lin et al., 2008; Quan et al., 2015). To predict multiple emotions simultaneously, emotion detection can be solved in the multi-label classification framework (Bhowmick, 2009). Moreover, to predict both multiple emotions and their intensities, some approaches have been proposed using emotion distribution learning (Zhou et al., 2016). Some lexicon-based approaches such as (Wang and Pal, 2015) can also output multiple emotions with intensities using non-negative matrix factorization. In this paper, we are interested in exploring emotion ranking from either readers’ perspective or writers’ perspective in two different real-world corpora. In both cases, a given text is associated with multiple emotions. For example, Figure 1 illustrates an online news article crawled from Sina News Society Channel together with readers’ emotion votes. It can be observed that when reading the news article, readers expressed different emotions"
N18-1156,D13-1068,0,0.0178839,"nts is generated and propagated continuously on online news media sites. It is difficult for the public to digest such large volumes of information effectively. Storyline generation, aiming at summarizing the development of certain related events, has been intensively studied recently (Diao and Jiang, 2014). In general, storyline can be considered as an event cluster where event-related news articles are ordered and clustered depending on both content and temporal similarity. Different ways of calculating content and temporal similarity can be used to cluster related events (Yan et al., 2011; Huang and Huang, 2013). Bayesian nonparametric models could also be used to tackle this problem by describing the storyline generating process using probabilistic graphical models (Li and Cardie, 2014; Diao and Jiang, 2014). Nevertheless, most existing approaches extract events independently and link relevant events in a post-processing step. More recently, Zhou et al. (2016) proposed a non-parametric generative model to extract storylines which is combined with Chinese Restaurant Processes (CRPs) to determine the number of storylines automatically. However, the parameter inference procedure is too complex and the"
N18-1156,D11-1040,0,0.0339379,"about current events is generated and propagated continuously on online news media sites. It is difficult for the public to digest such large volumes of information effectively. Storyline generation, aiming at summarizing the development of certain related events, has been intensively studied recently (Diao and Jiang, 2014). In general, storyline can be considered as an event cluster where event-related news articles are ordered and clustered depending on both content and temporal similarity. Different ways of calculating content and temporal similarity can be used to cluster related events (Yan et al., 2011; Huang and Huang, 2013). Bayesian nonparametric models could also be used to tackle this problem by describing the storyline generating process using probabilistic graphical models (Li and Cardie, 2014; Diao and Jiang, 2014). Nevertheless, most existing approaches extract events independently and link relevant events in a post-processing step. More recently, Zhou et al. (2016) proposed a non-parametric generative model to extract storylines which is combined with Chinese Restaurant Processes (CRPs) to determine the number of storylines automatically. However, the parameter inference procedure"
N18-1156,D15-1225,1,0.84242,"from these news articles. We chose the following four methods as the baseline approaches. 1. DLDA (Blei and Lafferty, 2006): the dynamic LDA is based on the Markovian assumption that the topic-word distribution at the current time period is only influenced by the topic-word distribution in the previous time period. Moreover, topic-word distributions are linked across time periods by a Markovian chain. 2. RCRP (Ahmed et al., 2011a): it is a nonparametric model for evolutionary clustering based on RCRP, which assumes that the past story popularity is a good prior for current popularity. 3. SDM (Zhou et al., 2015): it assumes that the number of storylines is fixed and the storyline is modeled as a joint distribution over 1 https://nlp.stanford.edu/software/CRF-NER.html 4. DSEM (Zhou et al., 2016): this model is integrated with CRPs so that the number of storylines can be determined automatically without human intervention. Moreover, per-token Metropolis-Hastings sampler based on light LDA (Yuan et al., 2015) is used to reduce sampling complexity. For DLDA, SDM and our model NSEM, the storyline number is set to 100 on both Dataset II and III. In consideration of the dependency to the historical storylin"
P11-1013,P07-1056,0,0.874767,"Missing"
P11-1013,W04-3237,0,0.0147228,"Missing"
P11-1013,P07-1033,0,0.0396392,"Missing"
P11-1013,P07-1034,0,0.150538,"Missing"
P11-1013,P10-1043,0,0.169048,"Missing"
P11-1013,W10-2918,1,0.549494,"Missing"
P11-1013,P07-1055,0,0.0344422,"Missing"
P11-1013,P04-1035,0,0.0446219,"Missing"
P11-1013,W02-1011,0,0.0227519,"Missing"
P11-1013,N03-1027,0,0.0292067,"Missing"
P11-1013,P09-2080,0,0.0211733,"Missing"
P11-1013,C10-2152,0,0.0351914,"Missing"
P11-1013,P10-2062,0,0.0215578,"Missing"
P11-1013,D10-1102,0,0.0230438,"Missing"
P11-1013,D08-1013,0,0.0147944,"Missing"
P13-2011,J96-2004,0,0.0239954,"Missing"
P13-2011,W10-3001,0,0.664687,"Missing"
P13-2011,P09-2044,0,0.554532,"Missing"
P13-2011,W10-3004,0,0.0563479,"ork on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional 2 http://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical ("
P13-2011,W08-0607,0,0.0429344,"al media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise q"
P13-2011,D11-1147,0,0.0933427,"Missing"
P13-2011,J12-2004,0,0.617265,"Missing"
P13-2011,P08-1033,0,0.113025,"cally for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information"
P13-2011,W10-3002,0,0.449553,"gories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional 2 http://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (Kiefer, 2005). For Epistemic, there are two sub-classes Possible and Probable. For Hypothetical, th"
P13-2011,W04-3103,0,0.203939,"g time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain"
P13-2011,P07-1125,0,0.270428,"y has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to extern"
P13-2011,J12-2001,0,0.0854876,"ed as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain statements. But, neither of the uncertainty expressions can be represented based on the existing types of uncertainty defined in the literature. Therefore, a different uncertainty classification scheme is needed in social media context. In this pap"
P13-2011,W08-0606,0,\N,Missing
P14-2101,W13-0102,0,0.0313411,"Missing"
P14-2101,C10-2069,0,0.332195,"ing of the Association for Computational Linguistics (Short Papers), pages 618–624, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics relating to a topic; and - We show that summarisation algorithms, which are independent of extenal sources, can be used with success to label topics, presenting a higher perfomance than the top-n terms baseline. was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate labels while maximising the mutual information between these two word distributions. Lau et al. (2010) proposed to label topics by selecting top-n terms to label the overall topic based on different ranking mechanisms including pointwise mutual information and conditional probabilities. 2 Methodology We propose to approach the topic labelling problem as a multi-document summarisation task. The following describes our proposed framework to characterise documents relevant to a topic. Methods relying on external sources for automatic labelling of topics include the work by Magatti et al. (2009) which derived candidate topic labels for topics induced by LDA using the hierarchy obtained from the Go"
P14-2101,P11-1154,0,0.769254,"dels Learned from Twitter by Summarisation Amparo Elizabeth Cano Basave† Yulan He‡ Ruifeng Xu§ † Knowledge Media Institute, Open University, UK ‡ School of Engineering and Applied Science, Aston University, UK § Key Laboratory of Network Oriented Intelligent Computation Shenzhen Graduate School, Harbin Institute of Technology, China amparo.cano@open.ac.uk, y.he@cantab.net, xuruifeng@hitsz.edu.cn Abstract ence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using the marginal probabilities P (wi |tj ) associated with each word wi for a given topic tj . This task can be illustrated by consideri"
P14-2101,E12-1022,0,0.0130221,"opic pairs (ti − tj ) do j 8: Collect relevant news articles CN W of topic tj from the NW set. j 9: Extract the headlines of news articles from CN W and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for ROUGE-1 These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj ), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold War DisAc Edu LawCri TT 0.162 0.134 0.106 0.035 SB 0.184 0.194 0.240 0.159 TFIDF 0.192 0.160 0.187 0.149 MMR 0.154 0.132 0.104 0.034 TR 0.141 0.124 0.023 0.115 Table 1: Average ROUGE-1 for topic labels at x = {1..10}, generated from the TW dataset. The generated labels with summarisation at x = 5 are presented in Table 2, where GS re"
P14-2101,N06-1059,0,0.0446431,"Evaluation of automatic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous Maximal Marginal Relevance (MMR) This is a relevance based ranking algorithm (Carbonell and Goldstein, 1998), which avoids redundancy in the documents used for generating a summary. It measures the degree of dissimilarity between the documents considered and previously selected ones already in the ranked list. Text Rank (TR) This is a"
P14-2101,P12-1056,0,0.153845,"ia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (Lau et al., 2011; Magatti et al., 2009; Mei et al., 2007) or graphbased (Hulpus et al., 2013) algorithms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coher618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics relating to a topic; and - We show that summarisation algorithms, which are independent of extenal sources, can be used with success to label topics, presenting a higher perfo"
P14-2101,W04-1013,0,0.0259931,"eprocessing steps were performed on NW. Therefore, following a similarity alignment approach we performed the steps oulined in Algorithm 1 for generating the GS topic labels of a topic in TW. standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the word given the topic (p(w|k)) from the topic model. We evaluated these summarisation approaches with the ROUGE-1 method (Lin, 2004), a widely used summarisation evaluation metric that correlates well with human evaluation (Liu and Liu, 2008). This method measures the overlap of words between the generated summary and a reference, in our case the GS generated from the NW dataset. The evaluation was performed at x = {1, .., 10}. Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases. We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases. In particular, in bo"
P14-2101,W00-0408,0,0.0353846,"Missing"
P14-2101,P08-2051,0,0.018893,"med the steps oulined in Algorithm 1 for generating the GS topic labels of a topic in TW. standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the word given the topic (p(w|k)) from the topic model. We evaluated these summarisation approaches with the ROUGE-1 method (Lin, 2004), a widely used summarisation evaluation metric that correlates well with human evaluation (Liu and Liu, 2008). This method measures the overlap of words between the generated summary and a reference, in our case the GS generated from the NW dataset. The evaluation was performed at x = {1, .., 10}. Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases. We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases. In particular, in both the Education and Law & Crime categories, both SB and TFIDF outperforms TT and TR by a large margin. The ob"
P14-2101,D09-1032,0,0.0116069,"matic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous Maximal Marginal Relevance (MMR) This is a relevance based ranking algorithm (Carbonell and Goldstein, 1998), which avoids redundancy in the documents used for generating a summary. It measures the degree of dissimilarity between the documents considered and previously selected ones already in the ranked list. Text Rank (TR) This is a graph-based summariser m"
P14-2101,N09-1041,0,0.0214889,"case 0.7) 6: end for 7: for each of the extracted topic pairs (ti − tj ) do j 8: Collect relevant news articles CN W of topic tj from the NW set. j 9: Extract the headlines of news articles from CN W and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for ROUGE-1 These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj ), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold War DisAc Edu LawCri TT 0.162 0.134 0.106 0.035 SB 0.184 0.194 0.240 0.159 TFIDF 0.192 0.160 0.187 0.149 MMR 0.154 0.132 0.104 0.034 TR 0.141 0.124 0.023 0.115 Table 1: Average ROUGE-1 for topic labels at x = {1..10}, generated from the TW dataset. The generated labels with summarisatio"
P14-2101,J13-2002,0,0.0158926,"en relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous Maximal Marginal Relevance (MMR) This is a relevance based ranking algorithm (Carbonell and Goldstein, 1998), which avoids redundancy in the documents used for generating a summary. It measures the degree of dissimilarity between the documents considered and previously selected ones already in the ranked list. Text Rank (TR) This is a graph-based summariser method (Mihalcea and Tarau,"
P14-2101,N13-1135,0,0.0127601,"s on topics derived from well formatted and static documents. However in contrast to this type of content, the labelling of topics derived from tweets presents different challenges. In nature micropost content is sparse and present ill-formed words. Moreover, the use of Twitter as the “what’shappening-right now” tool, introduces new eventdependent relations between words which might not have a counter part in existing knowledge sources (e.g. Wikipedia). Our original interest in labelling topics stems from work in topic model based event extraction from social media, in particular from tweets (Shen et al., 2013; Diao et al., 2012). As opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counter part on existing external sources. Based on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic’s relevant documents. Our contributions are two-fold: - We propose a novel approach for topics labelling that relies on term relevance of documents 2.2 Automatic Lab"
P14-2101,P09-2075,0,0.013068,"of the extracted topic pairs (ti − tj ) do j 8: Collect relevant news articles CN W of topic tj from the NW set. j 9: Extract the headlines of news articles from CN W and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for ROUGE-1 These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj ), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold War DisAc Edu LawCri TT 0.162 0.134 0.106 0.035 SB 0.184 0.194 0.240 0.159 TFIDF 0.192 0.160 0.187 0.149 MMR 0.154 0.132 0.104 0.034 TR 0.141 0.124 0.023 0.115 Table 1: Average ROUGE-1 for topic labels at x = {1..10}, generated from the TW dataset. The generated labels with summarisation at x = 5 are pres"
P14-2101,D12-1134,0,0.0256048,"urces (e.g. Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (Lau et al., 2011; Magatti et al., 2009; Mei et al., 2007) or graphbased (Hulpus et al., 2013) algorithms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coher618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics relating to a topic; and - We show that summarisation algorithms, which are independent of extenal sources, can be used with success to label topics, prese"
P14-2101,D11-1024,0,0.075931,"Missing"
P14-2101,N10-1012,0,0.0824927,"Missing"
P14-2101,W04-3252,0,\N,Missing
P14-2114,P11-1040,0,0.0385764,"Missing"
P14-2114,D11-1141,0,0.220749,"Missing"
P14-2114,chang-manning-2012-sutime,0,0.0126879,"Missing"
P14-2114,P11-2008,0,0.1347,"Missing"
P14-2114,P00-1010,0,0.259102,"Missing"
P14-5007,I13-1013,1,0.713754,"Missing"
P14-5007,N10-1021,1,0.615233,"Missing"
P14-5007,P12-3005,0,\N,Missing
P14-5007,D13-1100,0,\N,Missing
P15-2003,D14-1110,0,0.187102,"Missing"
P15-2003,D14-1179,0,0.0363681,"Missing"
P15-2003,C14-1048,0,0.0187377,"resentations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clusterin"
P15-2003,P12-1092,0,0.678402,"eddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed rep"
P15-2003,P14-1002,0,0.00766999,"odels for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 15–20, c Beijing, China, Ju"
P15-2003,P14-1062,0,0.00623338,"ng, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on"
P15-2003,P10-1040,0,0.0320513,"ure work. the closest to vb − va + vc . WordRep is a benchmark collection for the research on learning distributed word representations, which expands the Mikolov et al.’s analogical reasoning questions. In our experiments, we use one evaluation set in WordRep, the WordNet collection which consists of 13 sub tasks. We use the precision p × 100 as metric for each sub task. Table 2 shows the results on the 13 sub tasks. The Word Pair column is the number of word pairs of each sub task. The results of C&W were obtained using the 50-dimensional word embeddings that were made publicly available by Turian et al. (2010).6 The CBOW results were previously reported in (Gao et al., 2014). It can be observed that among 13 subtasks, our model outperforms the others by a good margin in 6 subtasks, Attribute, Causes, Entails, IsA, MadeOf and RelatedTo. 3.3 Analogical Reasoning Task 3.4 Discussion The analogical reasoning task introduced by (Mikolov et al., 2013) consists of questions of the form “a is to b is as c is to ”, where (a, b) and (c, ) are two word pairs. The goal is to find a word d∗ in vocabulary V whose representation vector is Although our evaluation results on the word similarity task and the analogi"
P15-2003,D14-1181,0,0.062281,"ion of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan"
P15-2003,P14-1011,0,0.0245325,"oning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annua"
P15-2003,D14-1113,0,0.674777,"ac.uk 1 Abstract In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clustering algorithm which requires the setting of the sense number for each word. For example, Neelakantan et al. (2014) proposed two clustering based model: the Multi-Sense Skip-Gram (MSSG) model and Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) model. MSSG assumes each word has the same k-sense (e.g. k = 3), i.e., the same number of possible senses. However, the number of senses in WordNet (Miller, 1995) varies from 1 such as “ben” to 75 such as “break”. As such, fixing the number of senses for all words would result in poor representations. NP-MSSG can learn the number of senses for each word directly from data. But it requires a tuning of a hyperparameter λ which controls the creation of cluster centroids"
P15-2003,D13-1170,0,0.00141234,"and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna"
P15-2003,C14-1016,0,\N,Missing
P16-1026,P11-1040,0,0.0343194,"aim to extract events from a particular location or for emergency response during natural disasters. Anantharam et al. (2015) focused on extracting city events by solving a sequence labeling problem. Evaluation was carried out on a real-world dataset consisting of event reports and tweets collected over four months from San Francisco Bay Area. TSum4act (Nguyen et al., 2015) was designed for emergency response during disasters and was evaluated on a dataset containing 230,535 tweets. Most of open domain approaches focused on extracting a summary of events discussed in social media. For example Benson et al. (2011) proposed a structured graphical model which simultaneously analyzed individual messages, clustered, and induced a canonical value for each event. Capdevila et al. (2015) proposed a model named Tweet-SCAN based on the hierarchical Dirichlet process to detect events from geo-located tweets. To extract more information, a system called SEEFT (Wang et al., 2015) used links in tweets and combined tweets and linked articles to identify events. Zhou et al. (2014; 2015) proposed an unsupervised Bayesian model called latent event model (LEM) for event extraction from Twitter by assuming that each twee"
P16-1026,P14-2114,1,0.845292,"ntaining 230,535 tweets. Most of open domain approaches focused on extracting a summary of events discussed in social media. For example Benson et al. (2011) proposed a structured graphical model which simultaneously analyzed individual messages, clustered, and induced a canonical value for each event. Capdevila et al. (2015) proposed a model named Tweet-SCAN based on the hierarchical Dirichlet process to detect events from geo-located tweets. To extract more information, a system called SEEFT (Wang et al., 2015) used links in tweets and combined tweets and linked articles to identify events. Zhou et al. (2014; 2015) proposed an unsupervised Bayesian model called latent event model (LEM) for event extraction from Twitter by assuming that each tweet message is assigned to one event instance and each event is modeled as a joint distribution over named entities, a date/time, a location and the event-related keywords. Our proposed method is partly inspired by (Zhou et al., 2015). However, different from previous methods, our approach not only extracts the structured representation of events, but also learns the coordinates of events and tweets simultaneously. 3 Methodology We follow the same pre-proces"
P16-1026,P11-1163,0,0.0805881,"Missing"
P16-1199,D15-1259,1,0.51264,"topic such as [O] and [R7] or raise a new aspect (subtopic) of the previously discussed topics such as [R2] and [R10]. These messages are named as leaders, which contain salient content in topic description, e.g., the italic and underlined words in Figure 1. The remaining messages, named as followers, do not raise new issues but simply respond to their reposted or replied messages following what has been raised by the leaders and often contain non-topic words, e.g., OMG, OK, agree, etc. Conversation tree structures from microblogs have been previously shown helpful to microblog summarization (Li et al., 2015), but have never been explored for topic modeling. We follows Li et al. (2015) to detect leaders and followers across paths of conversation trees using Conditional Random Fields (CRF) trained on annotated data. The detected leader/follower information is then incorporated as prior knowledge into our proposed topic model. Our experimental results show that our model, which captures parent-child topic correlations in conversation trees and generates topics by considering messages being leaders or followers separately, is able to induce high-quality topics and outperforms a number of competitive"
P16-1199,D11-1024,0,0.170074,"Missing"
P16-1199,P13-4009,0,0.150509,"Missing"
P16-1199,N10-1020,0,0.204366,"Missing"
P16-1199,N15-1119,0,0.111395,"oorly when directly applied to short and colloquial microblog content due to severe sparsity in microblog messages (Wang and McCallum, 2006; Hong and Davison, 2010). A common way to deal with short text sparsity is to aggregate short messages into long pseudodocuments. Most of the studies heuristically aggregate messages based on authorship (Zhao et al., 2011; Hong and Davison, 2010), shared words (Weng et al., 2010), or hashtags (Ramage et al., 2010; Mehrotra et al., 2013). Some works directly take into account the word relations to alleviate document-level word sparseness (Yan et al., 2013; Sridhar, 2015). More recently, a self-aggregation-based topic model called SATM (Quan et al., 2015) was proposed to aggregate texts jointly with topic inference. However, we argue that the existing aggregation strategies are suboptimal for modeling topics in short texts. Microblogs allow users to share and comment on messages with friends through reposting or replying, similar to our everyday conversations. Intuitively, the conversation structures can not only enrich context, but also provide useful clues for identifying relevant topics. This is nonetheless ignored in previous approaches. Moreover, the occu"
P16-1199,P11-1153,0,0.167809,"e topics. Since leader messages subsume the content of their followers, the topic of a leader can be generated from the topic distribution of the entire tree. Consequently, the topic mixture of a conversation tree is determined by the topic assignments to the leader messages on it. The topics of followers, however, exhibit strong and explicit dependencies on the topics of their ancestors. So, their topics need to be generated in consideration of local constraints. Here, we mainly address how to model the topic dependencies of followers. Enlighten by the general Structural Topic Model (strTM) (Wang et al., 2011), which incorporates document structures into topic model by explicitly modeling topic dependencies between adjacent sentences, we exploit the topical transitions between parents and children in the trees for guiding topic assignments. Intuitively, the emergence of a leader results in potential topic shift. It tends to weaken the topic similarities between the emerging leaders and their predecessors. For example, [R7] in Figure 1 transfers the topic to a new focus, thus weakens the tie with its parent. We can simplify our case by assuming that followers are topically responsive just up to (hen"
Q19-1017,C18-1063,0,0.243907,"arning from the tree-structured data, previous studies have pointed out that, in practice, sequence models serve as a more simple yet robust alternative (Li et al., 2015). In this work, we follow the common practice in most conversation modeling research (Ritter et al., 2010; Joty et al., 2011; Zhao et al., 2018) to take a conversation as a sequence of turns. To this end, each conversation tree is flattened into root-to-leaf paths. Each one of such paths is hence considered as a conversation instance, and a message on the path corresponds to a conversation turn (Zarisheva and Scheffler, 2015; Cerisara et al., 2018; Jiao et al., 2018). The overall architecture of our model is shown in Figure 2. Formally, we formulate a conversation c as a sequence of messages (x1 , x2 , . . . , xMc ), where Mc denotes the number of messages in c. In the conversation, each message x, as the target message, is fed into our model sequentially. Here we process the target message x as the bag-of-words (BoW) term vector xBoW ∈ RV , following the bag-of-words assumption in most Conversation Discourse. Our work is also in the area of discourse analysis for conversations, ranging from the prediction of the shallow discourse role"
Q19-1017,D15-1109,0,0.0265474,"c denotes the number of messages in c. In the conversation, each message x, as the target message, is fed into our model sequentially. Here we process the target message x as the bag-of-words (BoW) term vector xBoW ∈ RV , following the bag-of-words assumption in most Conversation Discourse. Our work is also in the area of discourse analysis for conversations, ranging from the prediction of the shallow discourse roles on utterance level (Stolcke et al., 2000; Ji et al., 2016; Zhao et al., 2018) to the discourse parsing for a more complex conversation structure (Elsner and Charniak, 2008, 2010; Afantenos et al., 2015). In this area, most existing models heavily rely on the data annotated with discourse labels for learning (Zhao et al., 2017). Different from them, our model, in a fully unsupervised way, identifies distributional word clusters to represent latent discourse factors in conversations. Although such latent discourse variables have been studied in previous work (Ritter et al., 2010; Joty et al., 2011; Ji et al., 2016; Zhao et al., 2018), none of them explores the effects of latent discourse on the identification of conversation topic, which is a gap our work fills in. 3 Model Overview Our Neural"
Q19-1017,J18-4008,1,0.10441,"ay and How You Say it: Joint Modeling of Topics and Discourse in Microblog Conversations Jichuan Zeng1∗ Jing Li2 ∗ Yulan He3 Cuiyun Gao1 Michael R. Lyu1 Irwin King1 1 Department of Computer Science and Engineering The Chinese University of Hong Kong, HKSAR, China 2 Tencent AI Lab, Shenzhen, China 3 Department of Computer Science, University of Warwick, UK 1 {jczeng, cygao, lyu, king}@cse.cuhk.edu.hk 2 ameliajli@tencent.com, 3 yulan.he@warwick.ac.uk Toward key focus understanding of a conversation, previous work has shown the benefits of discourse structure (Li et al., 2016b; Qin et al., 2017; Li et al., 2018), which shapes how messages interact with each other, forming the discussion flow, and can usefully reflect salient topics raised in the discussion process. After all, the topical content of a message naturally occurs in context of the conversation discourse and hence should not be modeled in isolation. Conversely, the extracted topics can reveal the purpose of participants and further facilitate the understanding of their discourse behavior (Qin et al., 2017). Further, the joint effects of topics and discourse will contribute to better understanding of social media conversations, benefiting d"
Q19-1017,N16-1037,0,0.0256508,"re of our model is shown in Figure 2. Formally, we formulate a conversation c as a sequence of messages (x1 , x2 , . . . , xMc ), where Mc denotes the number of messages in c. In the conversation, each message x, as the target message, is fed into our model sequentially. Here we process the target message x as the bag-of-words (BoW) term vector xBoW ∈ RV , following the bag-of-words assumption in most Conversation Discourse. Our work is also in the area of discourse analysis for conversations, ranging from the prediction of the shallow discourse roles on utterance level (Stolcke et al., 2000; Ji et al., 2016; Zhao et al., 2018) to the discourse parsing for a more complex conversation structure (Elsner and Charniak, 2008, 2010; Afantenos et al., 2015). In this area, most existing models heavily rely on the data annotated with discourse labels for learning (Zhao et al., 2017). Different from them, our model, in a fully unsupervised way, identifies distributional word clusters to represent latent discourse factors in conversations. Although such latent discourse variables have been studied in previous work (Ritter et al., 2010; Joty et al., 2011; Ji et al., 2016; Zhao et al., 2018), none of them exp"
Q19-1017,D15-1278,0,0.0246574,"istributions (φT and φD ) as latent variables in a neural network and learns the parameters via back propagation. Before touching the details of our model, we first describe how we formulate the input. On microblogs, as a message might have multiple replies, messages in an entire conversation can be organized as a tree with replying relations (Li et al., 2016b, 2018). Though the recent progress in recursive models allows the representation learning from the tree-structured data, previous studies have pointed out that, in practice, sequence models serve as a more simple yet robust alternative (Li et al., 2015). In this work, we follow the common practice in most conversation modeling research (Ritter et al., 2010; Joty et al., 2011; Zhao et al., 2018) to take a conversation as a sequence of turns. To this end, each conversation tree is flattened into root-to-leaf paths. Each one of such paths is hence considered as a conversation instance, and a message on the path corresponds to a conversation turn (Zarisheva and Scheffler, 2015; Cerisara et al., 2018; Jiao et al., 2018). The overall architecture of our model is shown in Figure 2. Formally, we formulate a conversation c as a sequence of messages ("
Q19-1017,D14-1181,0,0.00256989,"276 Figure 4: (a) The impact of topic numbers. The horizontal axis shows the number of topics; the vertical axis shows the Cv topic coherence. (b) The impact of discourse numbers. The horizontal axis represents the number of discourse; the vertical axis represents the homogeneity measure. Figure 5: Visualization of the topic-discourse assignment of a twitter conversion from TWT16. The annotated blue words are prone to be discourse words, and the red are topic words. The shade indicates the confidence of the current assignment. joint training our model with convolutional neural network (CNN) (Kim, 2014), the widely used model on short text classification, can bring benefits to the classification performance. We set the embedding dimension to 200, with random initialization. The results are shown in Table 7, where we observe that joint training our model and the classifier can successfully boost the classification performance. relying on manually defined dialogue acts. We also notice that our model sometimes fails to identify discourse behaviors requiring more in-depth semantic understanding, such as sarcasm, irony, and humor. This is because our model detects latent discourse purely based on"
Q19-1017,Q15-1022,0,0.291853,"tensions (Blei et al., 2003; Rosen-Zvi et al., 2004), the applications of these models have been limited to formal and welledited documents, such as news reports (Blei et al., 2003) and scientific articles (Rosen-Zvi et al., 2004), attributed to their reliance on documentlevel word collocations. When processing short texts, such as the messages on microblogs, it is likely that the performance of these models will be inevitably compromised, due to the severe data sparsity issue. To deal with such an issue, many previous efforts incorporate the external representations, such as word embeddings (Nguyen et al., 2015; Li et al., 2 In this paper, the discourse role refers to a certain type of dialogue act (e.g., statement or question) for each message. And the discourse structure refers to some combination of discourse roles in a conversation. 268 and discourse in conversations. We first present an overview of our model in Section 3.1, followed by the model generative process and inference procedure in Section 3.2 and 3.3, respectively. 2016a; Shi et al., 2017) and knowledge (Song et al., 2011; Yang et al., 2015; Hu et al., 2016), pre-trained on large-scale high-quality resources. Different from them, our"
Q19-1017,D14-1162,0,0.0839168,"uyen et al., 2015), and NTM (Miao et al., 2017). In particular, BTM and LF-DMM are the state-of-the-art topic models for short texts. BTM explores the topics of all word pairs (biterms) in each message to alleviate data sparsity in short texts. LF-DMM incorporates word embeddings pre-trained on external data to expand semantic meanings of words, so does LF-LDA. In Nguyen et al. (2015), LF-DMM, based on one-topic-perdocument Dirichlet Multinomial Mixture (DMM) (Nigam et al., 2000), was reported to perform better than LF-LDA, based on LDA. For LF-LDA and LF-DMM, we use GloVe Twitter embeddings (Pennington et al., 2014) as the pre-trained word embeddings.9 For the discourse modeling experiments, we compare our results with LAED (Zhao et al., 2018), a VAE-based representation learning model for conversation discourse. In addition, for both topic and discourse evaluation, we compare with Li et al. (2018), a recently proposed model for microblog conversations, where topics and discourse are jointly explored with a non-neural framework. Besides the existing models from previous studies, we also compare with the variants of our model that only models topics (henceforth TOPIC ONLY) or discourse (henceforth DISC 10"
Q19-1017,D07-1043,0,0.151717,"Missing"
Q19-1017,P17-1090,0,0.0292997,"Missing"
Q19-1017,J00-3003,0,0.66529,"Missing"
Q19-1017,N10-1020,0,0.445962,"model’s ability to capture useful representations for microblog messages. Particularly, our model enables an easy combination with existing neural models for end-to-end training, such as convolutional neural networks, which is shown to perform better in classification than the pipeline approach without joint training. Figure 1: A Twitter conversation snippet about the gun control issue in U.S. Topic words reflecting the conversation focus are in boldface. The italic words in [ ] are our interpretations of the messages’ discourse roles. a statement, asking a question, and other dialogue acts (Ritter et al., 2010; Joty et al., 2011), which further shape the discourse structure of a conversation.2 To distinguish the above two components, we examine the conversation contexts and identify two types of words: topic words, indicating what a conversation focuses on, and discourse words, reflecting how the opinion is voiced in each message. For example, in Figure 1, the topic words ‘‘gun’’ and ‘‘control’’ indicate the conversation topic while the discourse word ‘‘what’’ and ‘‘?’’ signal the question in M3 . Concretely, we propose a neural framework built upon topic models, enabling the joint exploration of w"
Q19-1017,P18-1101,0,0.0371204,"Missing"
Q19-1017,D15-1037,0,0.0307574,"sue, many previous efforts incorporate the external representations, such as word embeddings (Nguyen et al., 2015; Li et al., 2 In this paper, the discourse role refers to a certain type of dialogue act (e.g., statement or question) for each message. And the discourse structure refers to some combination of discourse roles in a conversation. 268 and discourse in conversations. We first present an overview of our model in Section 3.1, followed by the model generative process and inference procedure in Section 3.2 and 3.3, respectively. 2016a; Shi et al., 2017) and knowledge (Song et al., 2011; Yang et al., 2015; Hu et al., 2016), pre-trained on large-scale high-quality resources. Different from them, our model learns topic and discourse representations only with the internal data and thus can be widely applied on scenarios where the specific external resource is unavailable. In another line of the research, most prior work focuses on how to enrich the context of short messages. To this end, biterm topic model (BTM) (Yan et al., 2013) extends a message into a biterm set with all combinations of any two distinct words appearing in the message. On the contrary, our model allows the richer context in a"
Q19-1017,P17-1061,0,0.051409,"Missing"
Q19-1017,W15-4614,0,0.0211977,"ls allows the representation learning from the tree-structured data, previous studies have pointed out that, in practice, sequence models serve as a more simple yet robust alternative (Li et al., 2015). In this work, we follow the common practice in most conversation modeling research (Ritter et al., 2010; Joty et al., 2011; Zhao et al., 2018) to take a conversation as a sequence of turns. To this end, each conversation tree is flattened into root-to-leaf paths. Each one of such paths is hence considered as a conversation instance, and a message on the path corresponds to a conversation turn (Zarisheva and Scheffler, 2015; Cerisara et al., 2018; Jiao et al., 2018). The overall architecture of our model is shown in Figure 2. Formally, we formulate a conversation c as a sequence of messages (x1 , x2 , . . . , xMc ), where Mc denotes the number of messages in c. In the conversation, each message x, as the target message, is fed into our model sequentially. Here we process the target message x as the bag-of-words (BoW) term vector xBoW ∈ RV , following the bag-of-words assumption in most Conversation Discourse. Our work is also in the area of discourse analysis for conversations, ranging from the prediction of the"
Q19-1017,D18-1351,1,0.879805,"essage naturally occurs in context of the conversation discourse and hence should not be modeled in isolation. Conversely, the extracted topics can reveal the purpose of participants and further facilitate the understanding of their discourse behavior (Qin et al., 2017). Further, the joint effects of topics and discourse will contribute to better understanding of social media conversations, benefiting downstream tasks such as the management of discussion topics and discourse behavior of social chatbots (Zhou et al., 2018) and the prediction of user engagements for conversation recommendation (Zeng et al., 2018b). To illustrate how the topics and discourse interplay in a conversation, Figure 1 displays a snippet of Twitter conversation. As can be seen, the content words reflecting the discussion topics (such as ‘‘supreme court’’ and ‘‘gun rights’’) appear in context of the discourse flow, where participants carry the conversation forward via making a statement, giving a comment, asking a question, and so forth. Motivated by such an observation, we assume that a microblog conversation can be decomposed into two crucially different components: one for topical content and the other for discourse behavi"
Q19-1017,N18-1035,1,0.825217,"essage naturally occurs in context of the conversation discourse and hence should not be modeled in isolation. Conversely, the extracted topics can reveal the purpose of participants and further facilitate the understanding of their discourse behavior (Qin et al., 2017). Further, the joint effects of topics and discourse will contribute to better understanding of social media conversations, benefiting downstream tasks such as the management of discussion topics and discourse behavior of social chatbots (Zhou et al., 2018) and the prediction of user engagements for conversation recommendation (Zeng et al., 2018b). To illustrate how the topics and discourse interplay in a conversation, Figure 1 displays a snippet of Twitter conversation. As can be seen, the content words reflecting the discussion topics (such as ‘‘supreme court’’ and ‘‘gun rights’’) appear in context of the discourse flow, where participants carry the conversation forward via making a statement, giving a comment, asking a question, and so forth. Motivated by such an observation, we assume that a microblog conversation can be decomposed into two crucially different components: one for topical content and the other for discourse behavi"
Q19-1017,J10-3004,0,\N,Missing
saif-etal-2014-stopwords,W11-2207,0,\N,Missing
saif-etal-2014-stopwords,W12-3704,0,\N,Missing
saif-etal-2014-stopwords,pak-paroubek-2010-twitter,0,\N,Missing
saif-etal-2014-stopwords,S13-2052,0,\N,Missing
W04-3007,H94-1010,0,0.050291,"of the goal. TAN networks relax this independence assumption by adding dependencies between concepts based on the conditional mutual information (CMI) between concepts given the goal. The goal prior probability P (Gu ) and the conditional probability of each semantic concept Ci given the goal Gu , P (Ci |Gu ) are learned from the training data. Dialogue act detection is done by picking the goal with the highest posterior probability of Gu given the particular instance of concepts C1 · · · Cn , P (Gu |C1 · · · Cn ). 3 Noise Robustness The ATIS corpus which contains air travel information data (Dahl et al., 1994) has been chosen for the SLU system development and evaluation. ATIS was developed in the DARPA sponsored spoken language understanding programme conducted from 1990 to 1995 and it provides a convenient and well-documented standard for measuring the end-to-end performance of an SLU system. However, since the ATIS corpus contains only clean speech, corrupted test data has been generated by adding samples of background noise to the clean test data at the waveform level. 3.1 Experimental Setup The experimental setup used to evaluate the SLU system was similar to that described in (He and Young, 2"
W04-3007,P94-1016,0,0.0359486,"ople use different words and sentence structures to convey the same meaning. Also, many utterances are grammaticallyincorrect or ill-formed. It thus remains an open issue as to how to provide robustness for large populations of nonexpert users in spoken dialogue systems. The key component of a spoken language understanding (SLU) system is the semantic parser, which translates the users’ utterances into semantic representations. Traditionally, most semantic parser systems have been built using hand-crafted semantic grammar rules and so-called robust parsing (Ward and Issar, 1996; Seneff, 1992; Dowding et al., 1994) is used to handle the ill-formed user input in which word patterns corresponding to semantic tokens are used to fill slots in different semantic frames in parallel. The frame with the highest score then yields the selected semantic representation. Formally speaking, the robustness of language (recognition, parsing, etc.) is a measure of the ability of human speakers to communicate despite incomplete information, ambiguity, and the constant element of surprise (Briscoe, 1996). In this paper, two aspects of SLU system performance are investigated: noise robustness and adaptability to different"
W04-3007,N03-1027,0,0.0303745,"iques are widely used to reduce the mismatch between training and test or to adapt a well-trained model to a novel domain. Commonly used techniques can be classified into two categories, Bayesian adaptation which uses a maximum a posteriori (MAP) probability criteria (Gauvain and Lee, 1994) and transformation-based approaches such as maximum likelihood linear regression (MLLR) (Gales and Woodland, 1996), which uses a maximum likelihood (ML) criteria. In recent years, MAP adaptation has been successfully applied to n-gram language models (Bacchiani and Roark, 2003) and lexicalized PCFG models (Roark and Bacchiani, 2003). Luo et al. have proposed transformation-based approaches based on the Markov transform (Luo et al., 1999) and the Householder transform (Luo, 2000), to adapt statistical parsers. However, the optimisation processes for the latter are complex and it is not clear how general they are. Since MAP adaptation is straightforward and has been applied successfully to PCFG parsers, it has been selected for investigation in this paper. Since one of the special forms of MAP adaptation is interpolation between the indomain and out-of-domain models, it is natural to also consider the use of non-linear int"
W04-3007,H94-1039,0,\N,Missing
W08-0617,W02-1002,0,0.0229161,"Missing"
W10-2918,P08-1034,0,0.0248087,"Missing"
W10-2918,P07-1056,0,0.892547,"iment expression can be quite different in different domains (Aue and Gamon, 2005). Moreover, aside from the diversity of genres and large-scale size of Web corpora, user-generated contents evolve rapidly over time, which demands much more efficient algorithms for sentiment analysis than the current approaches can offer. These observations have thus motivated the problem of using unsupervised approaches for domain-independent joint sentiment topic detection. Some recent research efforts have been made to adapt sentiment classifiers trained on one domain to another domain (Aue and Gamon, 2005; Blitzer et al., 2007; Li and Zong, 2008; Andreevskaia and Bergler, 2008). However, the adaption performance of these lines of work pretty much depends on the distribution similarity between the source and target domain, and considerable effort is still required to obtain labelled data for training. Intuitively, sentiment polarities are dependent on contextual information, such as topics or domains. In this regard, some recent work (Mei et al., 2007; Titov and McDonald, 2008a) has tried to model both sentiment and topics. However, these two models either require postprocessing to calculate the positive/negative co"
W10-2918,W02-1011,0,0.0407311,"of Engineering, Computing and Mathematics University of Exeter Exeter, EX4 4QF, UK. cl322@exeter.ac.uk Richard Everson Yulan He School of Engineering, Knowledge Media Institute Computing and Mathematics The Open University University of Exeter Milton Keynes Exeter, EX4 4QF, UK. MK7 6AA, UK Y.He@open.ac.uk R.E.Everson@exeter.ac.uk Abstract the use of sarcasm or incorporated with highly domain-specific information. Although the task of identifying the overall sentiment polarity of a document has been well studied, most of the work is highly domain dependent and favoured in supervised learning (Pang et al., 2002; Pang and Lee, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; McDonald et al., 2007), requiring annotated corpora for every possible domain of interest, which is impractical for real applications. Also, it is well-known that sentiment classifiers trained on one domain often fail to produce satisfactory results when shifted to another domain, since sentiment expression can be quite different in different domains (Aue and Gamon, 2005). Moreover, aside from the diversity of genres and large-scale size of Web corpora, user-generated contents evolve rapidly over time, which demands much mo"
W10-2918,D09-1061,0,0.143879,"to the traditional topic-based text classification, sentiment classification is deemed to be more challenging as sentiment is often embodied in subtle linguistic mechanisms such as 144 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 144–152, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics spectively. Finally, Section 6 concludes the paper and outlines the future work. quire some kind of supervised setting in which review text should contain ratings for aspects of interest (Titov and McDonald, 2008a). More recently, Dasgupta and Ng (2009) proposed an unsupervised sentiment classification algorithm by integrating user feedbacks into a spectral clustering algorithm. Features induced for each dimension of spectral clustering can be considered as sentimentoriented topics. Nevertheless, human judgement of identifying the most important dimensions during spectral clustering is required. Lin and He (2009) proposed a joint sentimenttopic (JST) model for unsupervised joint sentiment topic detection. They assumed that topics are generated dependent on sentiment distributions and then words are generated conditioned on sentiment-topic pa"
W10-2918,P08-1036,0,0.540733,"tection. Some recent research efforts have been made to adapt sentiment classifiers trained on one domain to another domain (Aue and Gamon, 2005; Blitzer et al., 2007; Li and Zong, 2008; Andreevskaia and Bergler, 2008). However, the adaption performance of these lines of work pretty much depends on the distribution similarity between the source and target domain, and considerable effort is still required to obtain labelled data for training. Intuitively, sentiment polarities are dependent on contextual information, such as topics or domains. In this regard, some recent work (Mei et al., 2007; Titov and McDonald, 2008a) has tried to model both sentiment and topics. However, these two models either require postprocessing to calculate the positive/negative coverage in a document for polarity identification (Mei et al., 2007) or reThis paper presents a comparative study of three closely related Bayesian models for unsupervised document level sentiment classification, namely, the latent sentiment model (LSM), the joint sentimenttopic (JST) model, and the Reverse-JST model. Extensive experiments have been conducted on two corpora, the movie review dataset and the multi-domain sentiment dataset. It has been foun"
W10-2918,P06-2059,0,0.0503688,"Missing"
W10-2918,P08-2065,0,0.0105644,"e quite different in different domains (Aue and Gamon, 2005). Moreover, aside from the diversity of genres and large-scale size of Web corpora, user-generated contents evolve rapidly over time, which demands much more efficient algorithms for sentiment analysis than the current approaches can offer. These observations have thus motivated the problem of using unsupervised approaches for domain-independent joint sentiment topic detection. Some recent research efforts have been made to adapt sentiment classifiers trained on one domain to another domain (Aue and Gamon, 2005; Blitzer et al., 2007; Li and Zong, 2008; Andreevskaia and Bergler, 2008). However, the adaption performance of these lines of work pretty much depends on the distribution similarity between the source and target domain, and considerable effort is still required to obtain labelled data for training. Intuitively, sentiment polarities are dependent on contextual information, such as topics or domains. In this regard, some recent work (Mei et al., 2007; Titov and McDonald, 2008a) has tried to model both sentiment and topics. However, these two models either require postprocessing to calculate the positive/negative coverage in a documen"
W10-2918,P09-1028,0,0.0610039,"Missing"
W10-2918,P07-1055,0,0.0456262,"22@exeter.ac.uk Richard Everson Yulan He School of Engineering, Knowledge Media Institute Computing and Mathematics The Open University University of Exeter Milton Keynes Exeter, EX4 4QF, UK. MK7 6AA, UK Y.He@open.ac.uk R.E.Everson@exeter.ac.uk Abstract the use of sarcasm or incorporated with highly domain-specific information. Although the task of identifying the overall sentiment polarity of a document has been well studied, most of the work is highly domain dependent and favoured in supervised learning (Pang et al., 2002; Pang and Lee, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; McDonald et al., 2007), requiring annotated corpora for every possible domain of interest, which is impractical for real applications. Also, it is well-known that sentiment classifiers trained on one domain often fail to produce satisfactory results when shifted to another domain, since sentiment expression can be quite different in different domains (Aue and Gamon, 2005). Moreover, aside from the diversity of genres and large-scale size of Web corpora, user-generated contents evolve rapidly over time, which demands much more efficient algorithms for sentiment analysis than the current approaches can offer. These o"
W10-2918,P04-1035,0,0.482354,"mputing and Mathematics University of Exeter Exeter, EX4 4QF, UK. cl322@exeter.ac.uk Richard Everson Yulan He School of Engineering, Knowledge Media Institute Computing and Mathematics The Open University University of Exeter Milton Keynes Exeter, EX4 4QF, UK. MK7 6AA, UK Y.He@open.ac.uk R.E.Everson@exeter.ac.uk Abstract the use of sarcasm or incorporated with highly domain-specific information. Although the task of identifying the overall sentiment polarity of a document has been well studied, most of the work is highly domain dependent and favoured in supervised learning (Pang et al., 2002; Pang and Lee, 2004; Whitelaw et al., 2005; Kennedy and Inkpen, 2006; McDonald et al., 2007), requiring annotated corpora for every possible domain of interest, which is impractical for real applications. Also, it is well-known that sentiment classifiers trained on one domain often fail to produce satisfactory results when shifted to another domain, since sentiment expression can be quite different in different domains (Aue and Gamon, 2005). Moreover, aside from the diversity of genres and large-scale size of Web corpora, user-generated contents evolve rapidly over time, which demands much more efficient algorit"
W10-4116,D08-1014,0,0.262824,"including blogs, discussion forums, tweets, etc. Research in sentiment analysis has mainly focused on the English language. There have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of a list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experi"
W10-4116,D08-1058,0,0.226282,"2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine translation. Unlike other cross-lingual sentiment classification methods which often require labeled corpora for training and therefore hinder their applicability for cross-domain sentiment analysis, the proposed approach does not require labeled documents. Moreover, as o"
W10-4116,P09-1027,0,0.375028,"s, etc. Research in sentiment analysis has mainly focused on the English language. There have been few studies in sentiment analysis in other languages due to the lack of resources, such as subjectivity lexicons consisting of a list of words marked with their respective polarity (positive, negative or neutral) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Neve"
W10-4116,esuli-sebastiani-2006-sentiwordnet,0,0.140388,"to express preferences on expectations of sentiment labels of those lexicon words. We leave the exploitation of other mechanisms of incorporating prior knowledge into model training as future work. The document sentiment is classified based on P (l|d), the probability of sentiment label given document, which can be directly obtained from the document-sentiment distribution. We define that a document d is classified as positive if P (lpos |d) &gt; P (lneg |d), and vice versa. timent lexicons in our experiments, namely the MPQA subjectivity lexicon, the appraisal lexicon5 , and the SentiWordNet6 (Esuli and Sebastiani, 2006). For all these lexicons, we only extracted words bearing positive or negative polarities and discarded words bearing neutral polarity. For SentiWordNet, as it consists of words marked with positive and negative orientation scores ranging from 0 to 1, we extracted a subset of 8,780 opinionated words, by selecting those whose orientation strength is above a threshold of 0.6. We used Google translator toolkit7 to translate these three English lexicons into Chinese. After translation, duplicate entries, words that failed to translate, and words with contradictory polarities were removed. For comp"
W10-4116,C08-1135,0,0.525431,"l) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine tran"
W10-4116,I08-1040,0,0.315097,"l) and manually labeled subjectivity corpora with documents labeled with their polarity. Pilot studies on cross-lingual sentiment analysis utilize machine translation to perform sentiment analysis on the English translation of foreign language text (Banea et al., 2008; Bautin et al., 2008; Wan, 2009). The major problem is that they cannot be generalized well when there is a domain mismatch between the source and target languages. There have also been increasing interests in exploiting bootstrappingstyle approaches for weakly-supervised sentiment classification in languages other than English (Zagibalov and Carroll, 2008b; Zagibalov and Carroll, 2008a; Qiu et al., 2009). Other approaches use ensemble techniques by either combining lexicon-based and corpus-based algorithms (Tan et al., 2008) or combining sentiment classification outputs from different experimental settings (Wan, 2008). Nevertheless, all these approaches are either complex or require careful tuning of domain and data specific parameters. This paper proposes a weakly-supervised approach for Chinese sentiment classification by incorporating language-specific lexical knowledge obtained from available English sentiment lexicons through machine tran"
W10-4116,P07-1123,0,\N,Missing
W10-4116,I05-3027,0,\N,Missing
W11-0150,W09-1402,0,0.0169002,"a finite stack size. State transitions are factored into separate stack pop and push operations constrained to give a tractable search space. The sequence of HVS stack states corresponding to the given parse tree is illustrated in Figure 1. The result is a model which is complex enough to capture hierarchical structure but which can be trained automatically from only lightly annotated data. In the HVS-based semantic parser, conventional grammar rules are replaced by three probability tables. Let each state at time t be denoted by a vector of Dt semantic concept labels (tags) ct = [ct [1], ct [2], ..ct [Dt ]] where ct [1] is the preterminal concept label and ct [Dt ] is the root concept label (SS in Figure 3). Given a word sequence W , concept vector sequence C and a sequence of stack pop operations N , the joint probability of P (W, C, N ) can be decomposed as P (W, C, N ) = T Y P (nt |ct−1 )P (ct [1]|ct [2 · · · Dt ])P (wt |ct ) t=1 396 (1) SS Positive_regulation Site Phosphorylation Dummy sent_start IFN-alpha SS enhanced tyrosine Dummy Positive_regulation Site SS SS Positive_regulation SS phosphorylation Dummy Protein SE of STAT1 sent_end Phosphorylation Dummy Protein Site Phosphor"
W11-0150,W09-1401,0,0.109326,"Missing"
