A92-1009,J86-3001,0,0.0576403,"Missing"
A92-1009,H89-1022,0,0.258816,"Missing"
A92-1009,P89-1025,0,0.354205,"Missing"
A92-1009,C92-1038,1,0.858264,"Missing"
A92-1009,P92-1034,1,0.877652,"Missing"
A92-1009,P83-1023,0,0.0472859,"Missing"
C12-1020,W10-1301,0,0.025265,"ngineers or nurses (Goldberg et al., 1994; Theune et al., 2001; Portet et al., 2009). These are capable of generating high quality texts; e.g., offshore oil rig workers preferred weather forecasts generated by the SumTime system to texts written by professional human forecasters (Sripada et al., 2003). There is some previous work on the use of data-to-text for lay audiences; e.g., generating narratives from sensor data for automotive (Reddington et al., 2011) and environmental (Molina et al., 2011) applications, generating personal narratives to help children with complex communication needs (Black et al., 2010), and summarising neonatal intensive care data for parents of premature babies (Mahamood et al., 2008). There are some notable examples of NLG systems that make use of structured textual records rather than numeric data. Peba-II (Milosavljevic, 1997) was an online animal encyclopedia that provided descriptions and comparison of animals using HTML pages. The Power (Daley et al., 1998) and ILEX (O’Donnell et al., 2001) systems in the virtual museum domain dynamically generated descriptions of museum objects based on the user’s discourse history and user model. Dial Your Disc (Van Deemter and Odi"
C12-1020,W09-0613,0,0.44043,"ssed as similar species then the similar species group will not be present at all. The schema loops through the entire message list and decides what groups are needed based on the type of messages present. 3.3 Microplanner The microplanner is the second stage of the generation architecture. Here, the document plan produced by the first stage, the document planner, is refined to produce a text specification. This includes phrase specifications and their aggregation into sentences. The phrase specification is structured in a way that it can be realised by the surface realiser. We use SIMPLENLG (Gatt and Reiter, 2009), and the design of these structures is therefore influenced by the functionalities of the SIMPLENLG library. To allow for the generation of more complex sentence structures and sentences that are easier to understand, the microplanner also carries out aggregation. The aggregation performed in this system focuses on the formation of sentences. For example, in the paragraph that deals with comparisons (see Fig. 2), the system knows that it will be comparing different features. This means that the phrase specifications can be aggregated through subordination, using conjunctions such as “whereas”"
C12-1020,karasimos-isard-2004-multi,0,0.183843,"m of generating engaging texts, attempting to keep its users amused by 314 (a) (b) Figure 4: Bumblebee Model: (a) Visual bumblebee features, (b) List of Thorax types focusing on the expression of unusual content. Our work shares commonalities with these systems, in that it targets non-expert audiences and has educational goals. The main mechanism employed by us in the current work is the use of comparisons for generating feedback. This builds on a body of previous work. Milosavljevic (1997) described comparisons as a useful tool for augmenting the user’s existing knowledge with new knowledge. Karasimos and Isard (2004) showed that texts which contained comparisons and aggregations helped readers retain more information and perform better on factual recall while also finding these texts more interesting and pleasant to read. Later, Marge et al. (2008) performed a similar experiment to isolate the effects of comparison from those of aggregation. These two experiments provide evidence that comparisons can help to improve the knowledge of users on a given domain, and are a basis for our work. 3 Implementation of the NLG module Our NLG system uses the architecture proposed by Reiter and Dale (2000) and is compat"
C12-1020,W08-1123,0,0.0612935,"with these systems, in that it targets non-expert audiences and has educational goals. The main mechanism employed by us in the current work is the use of comparisons for generating feedback. This builds on a body of previous work. Milosavljevic (1997) described comparisons as a useful tool for augmenting the user’s existing knowledge with new knowledge. Karasimos and Isard (2004) showed that texts which contained comparisons and aggregations helped readers retain more information and perform better on factual recall while also finding these texts more interesting and pleasant to read. Later, Marge et al. (2008) performed a similar experiment to isolate the effects of comparison from those of aggregation. These two experiments provide evidence that comparisons can help to improve the knowledge of users on a given domain, and are a basis for our work. 3 Implementation of the NLG module Our NLG system uses the architecture proposed by Reiter and Dale (2000) and is compatible with a wide range of work within the field. There are three main components in this architecture; a document planner, a micro planner and a surface realiser. Additionally, the document planning makes use of a domain model. We descr"
C94-2117,J86-3001,0,0.0425035,"Missing"
C94-2117,C92-1038,0,0.0255326,"Missing"
C94-2117,J94-2003,0,\N,Missing
C94-2117,C92-2114,0,\N,Missing
cahill-etal-2000-enabling,J98-3004,0,\N,Missing
cahill-etal-2000-enabling,W94-0319,0,\N,Missing
cahill-etal-2000-enabling,A00-1017,1,\N,Missing
cahill-etal-2000-enabling,H89-1022,0,\N,Missing
J06-1008,H89-1033,0,0.0624425,"Missing"
J09-1003,P04-1051,1,0.592181,"Missing"
J09-1003,P05-1018,0,0.280506,"ple in Strube and Hahn’s model. In addition to the variability caused by the numerous deﬁnitions of transitions and the introduction of the various principles, parameters such as “utterance,” “ranking,” and “realization” can also be speciﬁed in several ways giving rise to different instantiations of centering (Poesio et al. 2004). The following section discusses how these parameters were deﬁned in the corpora we deploy. 4. Experimental Data We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME corpus (Poesio et al. 2004), and the two corpora that Barzilay and Lapata (2005) experimented with. In this section, we discuss how the centering representations we utilize were derived from each corpus. 4.1 The MPIRO-CF Corpus Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the database of the MPIRO concept-to-text generation system (Isard et al. 2003), realized them as sentences, and organized them in sets. Each set consisted of six facts which were ordered by a domain expert. The orderings produced by this expert were shown to be very close to those produced by two other archeologists (Karamanis and Mellish 2005b). Our ﬁrst corpus, MPIRO-C"
J09-1003,N04-1015,0,0.208113,"Missing"
J09-1003,P06-1049,0,0.0283821,"Missing"
J09-1003,P87-1022,0,0.502533,"sentences of a comprehensible text. This process very often gives rise to documents that do not make sense although the information content is the same before and after the reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000). Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization. Since the early 1980s, when it was ﬁrst introduced, centering theory has been an inﬂuential framework for modelling entity coherence. Seminal papers on centering such as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and Weinstein (1995, page 215) suggest that centering may provide solutions for information ordering. Indeed, following the pioneering work of McKeown (1985), recent work on text generation exploits constraints on entity coherence to organize information (Mellish et al. 1998; Kibble and Power 2000, 2004; O’Donnell et al. 2001; Cheng 2002; Lapata ∗ Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK. Nikiforos.Karamanis@cl.cam.ac.uk. ∗∗ Department of Computing Science, King’s College, Aberdeen AB24 3UE, UK. † Department of Computer Science, Wivenhoe"
J09-1003,P97-1003,0,0.0149678,"... ... ... ... ... ... (e) [The government]S may ﬁle [a civil suit]O ruling that [conspiracy]S to curb [competition]O through [collusion]X is [a violation]O of [the Sherman Act]X . (f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X . Barzilay and Lapata automatically annotated their corpora for the grammatical function of the NPs in each sentence (denoted in the example by the subscripts S, O, and X for subject, object, and other, respectively) as well as their coreferential relations (which do not include bridging references). More speciﬁcally, they used a parser (Collins 1997) to determine the constituent structure of the sentences from which the grammatical function for each NP was derived.6 Coreferential NPs such as Microsoft Corp. and the company in (3a) were identiﬁed using the system of Ng and Cardie (2002). The entity grid is a two-dimensional array that captures the distribution of NP referents across sentences in the text using the aforementioned symbols for their grammatical role and the symbol “−” for a referent that does not occur in a sentence. Table 4 illustrates a fragment of the grid for the sentences in Example (3).7 Barzilay and Lapata use the grid"
J09-1003,W03-2304,0,0.120101,"sults of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to"
J09-1003,J95-2003,0,0.839789,"Missing"
J09-1003,P88-1020,0,0.534593,"rzilay and Lee 2004), that is, deciding in which sequence to present a set of preselected information-bearing items, has received much attention in recent work in automatic text generation. This is because text generation systems need to organize the content in a way that makes the output text coherent, that is, easy to read and understand. The easiest way to exemplify coherence is by arbitrarily reordering the sentences of a comprehensible text. This process very often gives rise to documents that do not make sense although the information content is the same before and after the reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000). Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization. Since the early 1980s, when it was ﬁrst introduced, centering theory has been an inﬂuential framework for modelling entity coherence. Seminal papers on centering such as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and Weinstein (1995, page 215) suggest that centering may provide solutions for information ordering. Indeed, following the pioneering work of McKeown"
J09-1003,W06-1662,0,0.0175532,"arious metrics of coherence suitable for information ordering. Then, Section 6 outlines a corpus-based methodology for choosing among these metrics. Section 7 reports on the results of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karam"
J09-1003,W02-2101,0,0.0518339,"Missing"
J09-1003,N06-2017,1,0.782977,"icut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to text-to-text generation applications is text. 30 Karamanis et al. Centering for Information Ordering centering and will serve as the premises of our investigation of centering in this article. Metrics of coherence are used in other work on text generation, too (Mellish et al. 1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and Power’s work, the features of entity coherence used in these metrics are informall"
J09-1003,P04-1050,1,0.762185,"2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to text-to-text generation applications is text. 30 Karamanis et al. Centering for Information Ordering centering and will serve as the premises of our investigation of centering in this article. Metrics of coherence are used in other work on text generation, too (Mellish et al. 1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and Power’s work, the features of entity coherence used in these"
J09-1003,P13-2092,0,0.0541466,"Missing"
J09-1003,W02-2111,1,0.916787,"ain contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a metric of coherence deﬁned using features derived from 1 Earlier versions of this work were presented in Karamanis et al. (2004) and Karamanis (2006). 2 Concept-to-text generation is concerned with the automatic generation of text from some underlying non-linguistic representation. By contrast, the input to text-to-text generation applications is text. 30 Karamanis et al. Centering for Information Ordering centering and will serve as the"
J09-1003,W05-1621,1,0.89359,"), and the two corpora that Barzilay and Lapata (2005) experimented with. In this section, we discuss how the centering representations we utilize were derived from each corpus. 4.1 The MPIRO-CF Corpus Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the database of the MPIRO concept-to-text generation system (Isard et al. 2003), realized them as sentences, and organized them in sets. Each set consisted of six facts which were ordered by a domain expert. The orderings produced by this expert were shown to be very close to those produced by two other archeologists (Karamanis and Mellish 2005b). Our ﬁrst corpus, MPIRO-CF, consists of 122 orderings that were made available to us by D&A. We computed a CF list for each fact in each ordering by applying the instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-totext generation. That is, we took each database fact to correspond to an “utterance” and speciﬁed the “realization” parameter using the arguments of each fact as the members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the centering transitions, and the violations of CHEAPNESS for the following example from MPIRO-CF: (1) (a) T"
J09-1003,J01-4007,0,0.333116,"that CF(Un ) should contain at least one member of CF(Un−1 ). This became known as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz, Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) deﬁne the additional transition NOCB to account for this case. Different types of NOCB transitions are introduced by Passoneau (1998) and Poesio et al. (2004), among others. Other researchers, however, consider the NOCB transition to be a type of ROUGH - SHIFT (Miltsakaki and Kukich 2004). Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and SALIENCE , which correspond to the identity checks used to deﬁne the transitions (see Table 1). To improve the way centering resolves pronominal anaphora, Strube and Hahn (1999) introduced a fourth principle called CHEAPNESS and deﬁned it as CB(Un )=CP(Un−1 ). They also redeﬁned Rule 2 to favor transition pairs which satisfy 3 “CB(Un−1 ) undef.” in Table 1 stands for the cases where Un−1 does not have a CB. Instead of classifying the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT is"
J09-1003,W00-1411,0,0.0673401,"Missing"
J09-1003,J04-4001,0,0.642398,"to have the same number of CONTINUEs, the sum of RETAINs is examined, and so forth for the other two types of centering transitions.12 M.KP, the metric deployed by Kibble and Power (2000) in their text generation system, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE, and SALIENCE, preferring the ordering with the lowest total cost. In addition to the violations of CONTINUITY and CHEAPNESS, the candidate ordering also violates SALIENCE once, so its score according to M.KP is 5. An alternative ordering with a lower score (if any) will be preferred by this metric. Although Kibble and Power (2004) introduced a weighted version of M.KP, the exact weighting of centering’s principles remains an open question, as argued by Kibble (2001). This is why we decided to experiment with M.KP instead of its weighted variant. In the remainder of the paper, we take forward the four metrics motivated in this section as the most appropriate starting point for experimentation. We would like to emphasize, however, that these are not the only possible options. Indeed, similarly to the various ways in which centering’s parameters can be speciﬁed, there exist many other ways of using centering to deﬁne metr"
J09-1003,P03-1069,0,0.187334,"ering data structures from existing corpora. Section 5 discusses how centering can be used to deﬁne various metrics of coherence suitable for information ordering. Then, Section 6 outlines a corpus-based methodology for choosing among these metrics. Section 7 reports on the results of our experiments and Section 8 discusses their implications. We conclude the paper with directions for future work and a summary of our main contributions.1 2. Information Ordering Information ordering has been investigated by substantial recent work in text-totext generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation. These approaches use a me"
J09-1003,J06-4002,0,0.112028,"easible, computational corpus-based experiments are 11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS. 12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for the deﬁnition of transitions in M.BFP. 38 Karamanis et al. Centering for Information Ordering often the most viable alternative (Poesio et al. 2004; Barzilay and Lee 2004). Corpusbased evaluation can be usefully employed during system development and may be later supplemented by less extended evaluation based on human judgments as suggested by Lapata (2006). The corpus-based methodology of Karamanis (2003) served as our experimental framework. This methodology is based on the premise that the original sentence order (OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it has to be penalized. Karamanis (2003) introduced a performance measure called the classiﬁcation error rate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2. Better(M,OSO) stands for the percentage of orderings that score better than the"
J09-1003,W07-2312,0,0.0225034,"Missing"
J09-1003,W98-1411,1,0.754978,"mplex interactions between different types of coherence which leave many other equally plausible combinations unexplored. Clearly, one would like to know what centering can achieve on its own before devising more complicated metrics. To address this question, we deﬁne metrics which are purely centering-based, placing any attempt to specify a more elaborate model of coherence beyond the scope of this article. This strategy is similar to most work on centering for text interpretation in which additional constraints on coherence are not taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic examples). This simpliﬁcation makes it possible to assess for the ﬁrst time how useful the employed centering features are for information ordering. Work on text generation which is solely based on rhetorical relations (Hovy 1988; Marcu 1997, among others) typically masks entity coherence under the ELABORATION relation. However, ELABORATION has been characterized as “the weakest of all rhetorical relations” (Scott and de Souza 1990, page 60). Knott et al. (2001) identiﬁed several theoretical problems all related to ELABORATION and suggested that this relation be replaced by"
J09-1003,P02-1014,0,0.0251601,"Missing"
J09-1003,J04-3003,1,0.272865,"Missing"
J09-1003,P06-2103,0,0.161845,"Missing"
J09-1003,J99-3001,0,0.154073,"discuss the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) deﬁne the additional transition NOCB to account for this case. Different types of NOCB transitions are introduced by Passoneau (1998) and Poesio et al. (2004), among others. Other researchers, however, consider the NOCB transition to be a type of ROUGH - SHIFT (Miltsakaki and Kukich 2004). Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and SALIENCE , which correspond to the identity checks used to deﬁne the transitions (see Table 1). To improve the way centering resolves pronominal anaphora, Strube and Hahn (1999) introduced a fourth principle called CHEAPNESS and deﬁned it as CB(Un )=CP(Un−1 ). They also redeﬁned Rule 2 to favor transition pairs which satisfy 3 “CB(Un−1 ) undef.” in Table 1 stands for the cases where Un−1 does not have a CB. Instead of classifying the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT is sometimes used (Kameyama 1998; Poesio et al. 2004). 32 Karamanis et al. Centering for Information Ordering CHEAPNESS over those which violate it. This means that CHEAPNESS is given priority over every other centering principle in Strube a"
J89-4002,P82-1030,0,0.639044,"Missing"
J89-4002,J89-4002,1,0.0512367,"Missing"
J89-4002,P84-1018,0,0.0542086,"ping from local message patterns to linguistic structures. It is similar to the system described in Chester (1976) in the local nature of its operation, but Chester builds sentences directly, rather than via structural representations. Mann et al. (1982) would call our system a &quot;direct translation&quot; system. A system built in this fashion has the advantage of a very simple control structure and has the potential of having its principles expressed in modular, independent rules. Our linguistic structural descriptions are similar to the functional descriptions used in Functional Grammar (Kay 1979; Kay 1984). F o r example, the following is a slightly simplified version of the rule used to realize the hypo_result construct above: 238 Natural Language Generation from Plans hp!oo.result (Agent, Act, State) --* [ ss~nesentence, conjn = [root = 'if], first = Is, agent = [ SAgent], pred = [active, m o r p h = pree, SAct, adv = + [ap, adv_word=[root=now]]]], rest = Is, pred = [vp, aux = [root = will], pred = [vp, $State, m o r p h = inf, adv = + [ap, adv_word= [ root = afterwards ] ] ] ] ] ]. In this rule, the left hand side of the ---> is a Prolog pattern to be matched with part of the message (symbol"
J89-4002,J82-2003,0,0.153869,"state (user, not (located (cab)))), prereqs ( user, start (engine), state (user, located (cab)))), neccbefore (user, start (engine), go_to (user, function (front, c a r ) ) ) ) 3.5 COMPOSITIONAL STRUCTURE BUILDING The next stage is to build a linguistic structure from this message expression. The structure-building component uses an ordered set of rules for mapping from local message patterns to linguistic structures. It is similar to the system described in Chester (1976) in the local nature of its operation, but Chester builds sentences directly, rather than via structural representations. Mann et al. (1982) would call our system a &quot;direct translation&quot; system. A system built in this fashion has the advantage of a very simple control structure and has the potential of having its principles expressed in modular, independent rules. Our linguistic structural descriptions are similar to the functional descriptions used in Functional Grammar (Kay 1979; Kay 1984). F o r example, the following is a slightly simplified version of the rule used to realize the hypo_result construct above: 238 Natural Language Generation from Plans hp!oo.result (Agent, Act, State) --* [ ss~nesentence, conjn = [root = 'if], f"
J89-4002,E89-1009,1,\N,Missing
J89-4002,J81-1002,0,\N,Missing
J97-1007,J86-3001,0,0.233552,"for different levels of users, we look only at the former group of work, which aims at generating descriptions for a subsequent reference to distinguish it from the set of entities with which it might be confused. The main data structure in these algorithms is a context set, which is the set of entities the hearer is currently assumed to be attending to, except the intended referent. Minimal distinguishing descriptions pursue efficiency in producing an adequate description that can identity the intended referent unambiguously with a given context set. Dale (1992) used the global focus space (Grosz and Sidner 1986), as the context set in his domain of small discourse. Following this idea, the context set grows as the discourse proceeds. Consider, for example, two nominal anaphora referring to the same entity occurring at different places in a discourse. According to the above algorithms, a single description would be produced for both anaphora if the context sets at both places contain the same elements. On the other hand, in general, a description with more distinguishing information is used for the second anaphor if distractors have entered into the context set. Two entities are said to be distractors"
J97-1007,C92-1038,0,0.0132513,"huangdao yige renJ, (he) bump-to a person &apos;(He) bumped into a person.&apos; c. ta i kanqing lena ren J de zhangxiang, he see-clear ASPECT that person GEN appearance &apos;He saw clearly that person&apos;s appearance.&apos; d. oi2 renchu na renJ shi shui. (he) recognize that person is who &apos;(He) recognized who that person is.&apos; This research starts with establishing possible rules for the generation of anaphora in Chinese. Previous work suggests obtaining these rules from consulting the results of linguistic study, including general principles, such as the Gricean maxims (Grice 1975) used in (Dale and Haddock 1991; Reiter and Dale 1992; Dale 1992) and focus theory, as used in (Dale 1992). A shortcoming of previous work is that it is unclear to what extent the resulting rules are effective in dealing with the generation of anaphora. In an attempt to overcome this, we adopt an empirical approach to obtaining rules based on observations of real texts. The basic methodology used is to start with a set of human-generated Chinese texts and the simplest possible anaphor generation rule (a rule that only considers the locality of anaphora). We then progressively add extra tests to the rule, based on independently motivated but simp"
J97-1007,C92-2114,0,0.0440765,"Missing"
J97-1007,J94-2003,0,0.0558452,"Missing"
N01-1001,W98-1426,0,\N,Missing
N01-1001,W98-1411,1,\N,Missing
N01-1001,W96-0405,0,\N,Missing
N01-1001,W02-2103,0,\N,Missing
N01-1001,J99-2004,0,\N,Missing
N01-1001,A00-2018,0,\N,Missing
N01-1001,W96-0404,1,\N,Missing
N01-1001,J93-1009,0,\N,Missing
N01-1001,W99-0707,0,\N,Missing
N01-1001,1999.tmi-1.7,0,\N,Missing
N01-1001,W98-1105,0,\N,Missing
N01-1001,A00-2030,0,\N,Missing
N01-1001,W96-0410,0,\N,Missing
N01-1001,A00-2023,0,\N,Missing
N01-1001,J99-3003,0,\N,Missing
N01-1001,1997.tmi-1.17,0,\N,Missing
N01-1001,W01-0510,0,\N,Missing
N01-1001,W02-2102,0,\N,Missing
N01-1001,E89-1032,0,\N,Missing
N01-1001,C92-2066,0,\N,Missing
N01-1001,C96-1030,0,\N,Missing
N01-1001,C92-2092,0,\N,Missing
N01-1001,W94-0319,0,\N,Missing
N01-1001,C90-3044,0,\N,Missing
N01-1001,A88-1006,0,\N,Missing
N01-1001,C00-1052,0,\N,Missing
N01-1001,C02-1064,0,\N,Missing
N01-1001,P00-1024,0,\N,Missing
N01-1001,C02-1005,1,\N,Missing
N01-1001,P00-1059,0,\N,Missing
N01-1001,C00-1007,0,\N,Missing
N01-1001,A00-1017,0,\N,Missing
N01-1001,C88-1029,0,\N,Missing
N01-1001,N01-1003,0,\N,Missing
N01-1001,P00-1012,0,\N,Missing
N01-1001,hockenmaier-steedman-2002-acquiring,0,\N,Missing
N01-1001,J03-4003,0,\N,Missing
N01-1001,H94-1020,0,\N,Missing
N01-1001,J96-1002,0,\N,Missing
N01-1001,P02-1040,0,\N,Missing
N01-1001,P95-1034,0,\N,Missing
N01-1001,C88-2128,0,\N,Missing
N01-1001,P98-1116,0,\N,Missing
N01-1001,C98-1112,0,\N,Missing
N01-1001,P02-1056,0,\N,Missing
N01-1001,W00-0708,0,\N,Missing
N01-1001,P99-1018,0,\N,Missing
N01-1001,P01-1056,0,\N,Missing
N01-1001,C92-2065,0,\N,Missing
N01-1001,P98-2227,0,\N,Missing
N01-1001,C98-2222,0,\N,Missing
N01-1001,P92-1034,1,\N,Missing
N01-1001,P83-1021,0,\N,Missing
N01-1001,P98-1061,0,\N,Missing
N01-1001,C98-1059,0,\N,Missing
N01-1001,P01-1023,0,\N,Missing
N01-1001,N01-1002,1,\N,Missing
N01-1001,A00-2026,0,\N,Missing
N01-1001,P96-1027,0,\N,Missing
N01-1001,H94-1028,0,\N,Missing
N01-1001,1997.iwpt-1.11,0,\N,Missing
N01-1001,W00-1401,0,\N,Missing
N01-1001,A97-1039,0,\N,Missing
N01-1002,W98-0607,1,\N,Missing
N01-1002,P00-1019,0,\N,Missing
N01-1002,W00-1425,1,\N,Missing
N01-1002,W00-1415,1,\N,Missing
N01-1002,J86-3001,0,\N,Missing
N01-1002,poesio-2000-annotating,1,\N,Missing
P04-1050,P87-1022,0,0.851057,"Missing"
P04-1050,W03-2304,0,0.140846,"fication rate of each metric. The gnome corpus contains texts from different genres, not all of which are of interest to us. In order to restrict the scope of the experiment to the text-type most relevant to our study, we selected 20 “museum labels”, i.e., short texts that describe a concrete artefact, which served as the input to seec together with the metrics in section 3.10 5.1 Permutation and search strategy In specifying the performance of the metrics we made use of a simple permutation heuristic exploiting a piece of domain-specific communication knowledge (Kittredge et al., 1991). Like Dimitromanolaki and Androutsopoulos (2003), we noticed that utterances like (a) in example (1), should always appear at the beginning of a felicitous museum label. Hence, we restricted the orderings considered by the seec 9 The Sign Test was chosen over its parametric alternatives to test significance because it does not carry specific assumptions about population distributions and variance. It is also more appropriate for small samples like the one used in this study. 10 Note that example (1) is characteristic of the genre, not the length, of the texts in our subcorpus. The number of CF lists that the BfCs consist of ranges from 4 to"
P04-1050,J95-2003,0,0.982899,"t structuring perspective. 1 Motivation Our research area is descriptive text generation (O’Donnell et al., 2001; Isard et al., 2003), i.e. the generation of descriptions of objects, typically museum artefacts, depicted in a picture. Text (1), from the gnome corpus (Poesio et al., 2004), is an example of short human-authored text from this genre: (1) (a) 144 is a torc. (b) Its present arrangement, twisted into three rings, may be a modern alteration; (c) it should probably be a single ring, worn around the neck. (d) The terminals are in the form of goats’ heads. According to Centering Theory (Grosz et al., 1995; Walker et al., 1998a), an important factor for the felicity of (1) is its entity coherence: the way centers (discourse entities), such as the referent of the NPs “144” in clause (a) and “its” in clause (b), are introduced and discussed in subsequent clauses. It is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring, in combination with other factors (see (Karamanis, 2003) for an overview). However, how successful Centering’s constraints are on their own in generating a felicitous t"
P04-1050,W02-2111,1,0.941744,"deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000). Our approach is further characterized by two key insights. The first distinguishing feature is that we assume a search-based approach to text structuring (Mellish et al., 1998; Kibble and Power, 2000; Karamanis and Manurung, 2002) in which many candidate orderings of clauses are evaluated according to scores assigned by a given metric, and the best-scoring ordering among the candidate solutions is chosen. The second novel aspect is that our approach is based on the position that the most straightforward way of using Centering for text structuring is by defining a Centering-based metric of coherence Karamanis (2003). Together, these two assumptions lead to a view of text planning in which the constraints of Centering act not as filters, but as ranking factors, and the text planner may be forced to choose a sub-optimal s"
P04-1050,W00-1411,0,0.722635,"ole of other factors is deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000). Our approach is further characterized by two key insights. The first distinguishing feature is that we assume a search-based approach to text structuring (Mellish et al., 1998; Kibble and Power, 2000; Karamanis and Manurung, 2002) in which many candidate orderings of clauses are evaluated according to scores assigned by a given metric, and the best-scoring ordering among the candidate solutions is chosen. The second novel aspect is that our approach is based on the position that the most straightforward way of using Centering for text structuring is by defining a Centering-based metric of coherence Karamanis (2003). Together, these two assumptions lead to a view of text planning in which the constraints of Centering act not as filters, but as ranking factors, and the text planner may be f"
P04-1050,J01-4007,0,0.339699,"Missing"
P04-1050,P03-1069,0,0.412162,"guide text structuring, in combination with other factors (see (Karamanis, 2003) for an overview). However, how successful Centering’s constraints are on their own in generating a felicitous text structure is an open question, already raised by the seminal papers of the theory (Brennan et al., 1987; Grosz et al., 1995). In this work, we explored this question by developing an approach to text structuring purely based on Centering, in which the role of other factors is deliberately ignored. In accordance with recent work in the emerging field of text-to-text generation (Barzilay et al., 2002; Lapata, 2003), we assume that the input to text structuring is a set of clauses. The output of text structuring is merely an ordering of these clauses, rather than the tree-like structure of database facts often used in traditional deep generation (Reiter and Dale, 2000). Our approach is further characterized by two key insights. The first distinguishing feature is that we assume a search-based approach to text structuring (Mellish et al., 1998; Kibble and Power, 2000; Karamanis and Manurung, 2002) in which many candidate orderings of clauses are evaluated according to scores assigned by a given metric, an"
P04-1050,W98-1411,1,0.776343,"Missing"
P04-1050,J02-3003,0,0.0184596,"ite=’finite-yes’ id=’u210’> <ne id=&quot;ne410&quot; gf=&quot;subj&quot;>144</ne> is <ne id=&quot;ne411&quot; gf=&quot;predicate&quot;> a torc</ne> </unit>. The ranking of the CFs other than the CP is defined according to the following preference on their gf (Brennan et al., 1987): obj>iobj>other. CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance. The second column of Table 1 shows how the utterances in example (1) are automatically translated by the scripts developed by Poesio et al. (2004) into a 1 For example, one could equate “utterance” with sentence (Strube and Hahn, 1999; Miltsakaki, 2002), use indirect realisation for the computation of the CF list (Grosz et al., 1995), rank the CFs according to their information status (Strube and Hahn, 1999), etc. 2 Our definition includes titles which are not always finite units, but excludes finite relative clauses, the second element of coordinated VPs and clause complements which are often taken as not having their own CF lists in the literature. 3 Or as a post-copular subject in a there-clause. U (a) (b) (c) (d) CF list: {CP, {de374, {de376, {de374, {de380, other CFs} de375} de374, de377} de379} de381, de382} CB n.a. de374 de374 - Trans"
P04-1050,J04-3003,1,0.846124,"Missing"
P04-1050,J99-3001,0,0.352512,"10 “144”. (2) <unit finite=’finite-yes’ id=’u210’> <ne id=&quot;ne410&quot; gf=&quot;subj&quot;>144</ne> is <ne id=&quot;ne411&quot; gf=&quot;predicate&quot;> a torc</ne> </unit>. The ranking of the CFs other than the CP is defined according to the following preference on their gf (Brennan et al., 1987): obj>iobj>other. CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance. The second column of Table 1 shows how the utterances in example (1) are automatically translated by the scripts developed by Poesio et al. (2004) into a 1 For example, one could equate “utterance” with sentence (Strube and Hahn, 1999; Miltsakaki, 2002), use indirect realisation for the computation of the CF list (Grosz et al., 1995), rank the CFs according to their information status (Strube and Hahn, 1999), etc. 2 Our definition includes titles which are not always finite units, but excludes finite relative clauses, the second element of coordinated VPs and clause complements which are often taken as not having their own CF lists in the literature. 3 Or as a post-copular subject in a there-clause. U (a) (b) (c) (d) CF list: {CP, {de374, {de376, {de374, {de380, other CFs} de375} de374, de377} de379} de381, de382} CB n.a."
P13-4029,C12-1020,1,0.699404,"Missing"
P13-4029,W12-1520,1,0.842727,"Missing"
P13-4029,W09-0613,0,0.0343657,"ging”) as an issue. Our focus will now be on improving the language, to address some of the readability and engagingness concerns. Surface realiser The role of the surface realiser is to convert the text specification received from the microplanner into text that the user can read and understand. This includes linguistic realisation (converting the sentence specifications into sentences) and structural realisation (structuring the sentences inside the document). Both the linguistic and structural realisations are performed by using functionalities provided by the S IMPLE NLG realiser library (Gatt and Reiter, 2009). 4 Utility of blogs in this domain Until recently, our partner charity was publishing hand-written blogs based on the journeys of these satellite tagged red kites. They have had to close down the site due to resource constraints: Such blogs are difficult, monotonous and time consuming to produce by hand. Tag2Blog will allow the charity to restart this form of public engagement. We have earlier studied the use of ecological blogs based on satellite tag data (Siddharthan et al., 2012). Using hand-written blogs in a toy domain, we found that readers were willing to anthromorphise the bird, and g"
P89-1013,P84-1075,0,0.009326,"le with VP found bottom-up) (by top-down rule) (by fundamental rule with NP found bottom-up) Figure 1: Focusing on an emx. In _~.~_ptingan ATN parser to compare partial parses, Weischedel and Sondheimer have already introduced machinery to represent several alternative partial parses simultaneously. From this, it is a relatively small step to introduce a well-formed substring table, or even an active chart, which allows for a glohal assessment of the state of the parser. If the grammar form~fi~m is also changed to a declarative formalism (e.g. CF-PSGs, DCGs (Pereira and Warren 1980), patr-ll (Shieber 1984)), then there is a possibility of constructing other partial parses that do not start at the beginning of the input. In this way, right context can play a role in the determination of the ~est&quot; parse. WHAT A CHART PARSER LEAVES BEHIND The information that an active chart parser leaves behind for consideration by a &quot;post mortem&quot; obviously depends on the parsing sWategy used (Kay 1980, Gazdar and Mellish 1989). Act/re edges are particularly important fx~n the point of view of diagnosing errors, as an unsatisfied active edge suggests a place where an input error may have occurred. So we might exp"
P89-1013,J83-3003,0,0.293247,"Missing"
P89-1013,J83-3001,0,\N,Missing
P89-1013,J83-3002,0,\N,Missing
P89-1013,A88-1029,0,\N,Missing
P92-1034,P90-1020,0,0.0659593,"Missing"
P92-1034,C90-3052,0,0.0496593,"Missing"
P92-1034,E89-1009,0,0.122749,"Missing"
P92-1034,H89-1022,0,0.125367,"Missing"
P92-1034,P83-1012,0,0.0531459,"Missing"
P92-1034,J81-1002,0,0.0840723,"Missing"
P92-1034,P89-1025,0,0.204415,"Missing"
P92-1034,C92-1038,1,0.877902,"Missing"
P92-1034,A92-1009,1,0.883591,"Missing"
P92-1034,C88-2128,0,0.0683055,"Missing"
W00-1410,H89-1022,0,0.0457245,"is built collaboraquery the knowledge base and arguments are knowltively by several modules. For instance, many sysedge base entities. tems have a referring expression generation module Semantic (Concrete) semantic representations whose task is to complete a semantic representation provide a complete notation for &quot;logical forms&quot; which lacks those structures which will be realised where there is no longer any reference to ,the knowlas NPs. Such a functionality cannot be described edge base. The representations are based on sysunless partially complete semantic representations tems such as SPL (Kasper, 1989) and DRT (Kamp can be communicated. and Reyle, 1993). In addition, mixed representations are possible, where (possibly partial) representations at several 4More details can be found in (Cahill et levels are combined with explicit links between the al., 1999) and at the RAGS project web site: h t t p : / / w w w . i t r i .b r i g h t o n , ac. u k / r a g s . elements. Many NLG modules have to be sensi70 tive to a number of levels at once (consider, for ..........instance, -aggregatiomxeferring,expmssion.,generation and lexicalisation, all of which need to take into account rhetorical, semanti"
W00-1410,A00-1017,0,0.280656,"te: h t t p : / / w w w . i t r i .b r i g h t o n , ac. u k / r a g s . elements. Many NLG modules have to be sensi70 tive to a number of levels at once (consider, for ..........instance, -aggregatiomxeferring,expmssion.,generation and lexicalisation, all of which need to take into account rhetorical, semantic and syntactic constraints). The input to most reusable realisation systems is also best viewed as a mixture of semantic and abstract syntactic information. The extra flexibility of having partial and mixed representations turned out to be vital in the reconstruction of the CGS system. (Mellish et al., 2000). 4 CGS architecture RAGS representations I II III IV V I II HI IV V •I II III IV &quot;V I II In IV v SAGE . The CGS system The Caption Generation System (CGS) generates explanatory captions of graphical presentations (2D charts and graphs). Its architecture is a pipeline with several modules, shown in the left hand part of Figure 1. An example of a diagram and its accompanying text are given in Figure 2. The propositions are numbered for ease of reference throughout the paper. The input to CGS is a picture representation (graphical elements and its mapping from the data set) generated by SAGE plu"
W00-1410,J98-3004,0,0.256422,"s of these definitions. The system chosen was In this exercise it was important to choose a systhe Caption Generation System. tem that had been developed by people outside the RAGS project. Equally, it was important to have 1 Introduction sufficient clear information about the system in the available literature, and/or by means of personal The RAGS project ~ aims to define a reference arcontact with the developers. The system chosen was chitecture for natural language generation systems. the Caption Generation System (Mittal et al., 1995; Currently the major part of this architecture consists Mittal et al., 1998) 3. This system was chosen beof a set of datatype definitions for specifying the cause, as well as fulfilling the criteria above, it apinput and output formats for modules within NLG peared to be a relatively simple pipeline, thus avoidsystems. The intention is that such representations ing complex control issues, with individual modules can be used to assist in reusability of components performing the varied linguistic tasks that the RAGS of NLG systems. System components that adhere data structures had been designed to handle. to these representations, or use a format that can be The reinter"
W00-1410,W94-0319,0,0.085954,"the system in Abstract Rhetorical Abstract Rhetorical RepreRAGS terms. Finally we discuss,, the :implications:. :..-._..sentations,are--tree-structures with,rhetorical .relafor RAGS of this exercise, tions at the internal nodes and Abstract Rhetorical trees or Abstract Semantic Representations at the leaves. 2 The RAGS datatypes Rhetorical Abstract Rhetorical Representations The RAGS project initially set out to develop a refare viewed as descriptions of sets of possible erence architecture based on the three-stage pipeline Rhetorical Representations. Each one may be transsuggested by Reiter (Reiter, 1994). However, a detailed analysis of existing applied NLG systems formed into some subset of the possible Rhetori(Cahill and Reape~_~l:998}:suggested~,that~ttch.an~ ar -~: ~&lt;.eaLReprese,ntations by,,means ~ofa,set..o_f~.petmitted chitecture was not specific enough and not closely transformations, e.g. reversing the order of nucleus and satellite or changing the rhetorical relation to enough adhered to by the majority of the systems one within a permitted set. surveyed for this to be used as the basis of the architecture. Abstract Document Document structure defines The abstract functionality of a"
W00-1410,J95-2003,0,\N,Missing
W00-1415,P98-2242,1,0.878357,"Missing"
W00-1415,W98-1411,1,\N,Missing
W00-1415,C98-2237,1,\N,Missing
W00-1415,J96-2004,0,\N,Missing
W00-1415,poesio-2000-annotating,0,\N,Missing
W00-1418,W90-0108,0,0.0444499,"ng ringset watch button dress-clip hat-pin)) Figure 4: Defining Taxonomic Knowledge 4.1 Taxonomic Organisation ILEX requires that the entities of the domain are organised under a domain taxonomy. The user defines a basic type (e.g., jewellery), and then defines the sub-types of the basic-type, and perhaps further subclassification. Figure 4 shows the lisp forms defining a basic type in the jewellery domain, and the subclassification of this type. The basic type is also mapped onto a type (or set of types) in the concept ontology used for sentence generation, a version of Penman&apos;s Upper Model (Bateman, 1990). This allows the sentence generator to reason about the objects it expresses. Taxonomic organisation is important for several reasons, including a m o n g others: 1. Expressing Entities: each type can be related to lexical i t e m s &apos; t o use,to-express t h a t type (e.g., linking the type brooch to a the lexical item for &quot;brooch&quot;. If no lexical item is defined for a type, a lexical item associated with some super-type can be used instead. Other aspects of the expression of entities may depend on the conceptual type, for instance pronominalisation, deixis (e.g., mass or count entities), etc."
W00-1418,W98-1404,1,0.83724,"e objects are also automatically generated. ILEX has been applied to other domains, including personnel (Nowson, 1999), and a sales catalogue for computer systems and peripherals (Anderson and Bradshaw, 1998). One of the advantages of using NLG for database browsing is that the system can keep track of what has already been said about objects, and not repeat that information on later pages. Appropriate referring expressions can also be selected on the basis of the discourse history. The object descriptions can be tailored to the informational interests of the user. See Knott et al. (1997) and Mellish et al. (1998) for more information on these aspects of ILEX. In section 2, we consider some systems related to the ILEX system. Section 3 describes the form of relational database that ILEX accepts as input. Section 4 outlines what additional information - domain semantics - needs to be provided for coherent text production from the database, while section 5 describes additional information which can be provided to improve the quality of the text produced. 2 Related Work It should be clear that the task we are discussing is very distinct from the task of response generation in a natural language interface"
W00-1418,W96-0405,1,0.82954,"(def-predicate Bib-Note .&lt;def~predicate Designer :argl jewellery :expression ( :adjunctl &quot;for further information&quot; :mood imperative :verb see-verb :voice active) . o . :importance ((expert lO)(default 6)(child 5)) :interest ((expert lO)(default 6)(child 4)) :assimilation ((expert O)(default O)(child 0)) :assim-rate ((expert l)(default l)(child 0.5)) ) Figure 10: Specifying User Parameters Figure 9: More Complex Fact Expression The expression form is used to construct a partial syntactic specification, which is then completed using the sentence generation module of the WAG sentence generator (O&apos;Donnell, 1996). With the level of d o m a i n semantics specified so far, ILEX is able to produce texts such as the two below, which provides an initial page describing database entity BUNDY01, a n d then a subsequent page when more information was requested (this from the Personnel domain (Nowson, 1999)): o P a g e 1: Alan B u n d y is located in room F1, which is in South Bridge. He lectures a course called Advanced Automated Reasoning and is in the Institute for Representation and Reasoning. He is the Head of Division and is a professor. * P a g e 2: As already mentioned, Alan Bundy lectures Advanced Aut"
W00-1425,P98-2242,1,0.890877,"a semantic relation, the conjunct should mantic hypotaxis concerns facts connected by be suppressed. For example, in 3 of Figure 1. nucleus-satellite relations (e.g. cause). Semanapart from other relations, there is an amplificatic parataxis and hypotaxis feature in relationtion relation signalled by indeed and a conjunct based coherence and they depend on the text between the last two propositions. Compared planner to put the related facts next to each with 3, 4 is less preferred because it misses tile other in order to perform a combination. amplification and the center transition from the (Cheng, 1998) describes interactions that necklace to an Arts and Crafts style jewel is not need to be taken into account in aggregaso smooth, whereas 3 expresses the amplification. Firstly, complex embedded components tion explicitly and the conjunct implicitly. like non-restrictive clauses may interrupt tile However, a semantic relation can only be used semantic connection or syntactic similarity beif the knowledge assumed to be shared by the tween a set of clauses. Secondly, the possibilities hearer is introduced in the previous discourse of other types of aggregation should be consid(Mellish et al.. 19"
W00-1425,J86-3001,0,0.107865,"e evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text. 1 Discourse coherence aggregation and hi NLG, theories based on domain-independent rhetorical relations, in particular, Rhetorical Structure Theory (Mann and Thompson, 1987), are often used in text planning, whose task is to select the relevant information to be expressed and organise it into a hierarchical structure which captures certain discourse preferences such as preferences for the use of rhetorical relations. In the theory of discourse structure developed by Grosz and Sidner (1986), each discourse segment exhibits two types of coherence: local coherence among utterances inside the segment, and global coherence between this segment and other discourse segments. Discourse segments are connected by either a dominaTzce relation or a satisfaction-precedence relation. There has been an effort to synthesise tile two accounts of discourse structure. Xloser and Moore (1996) argue that the two theories have 186 considerable common ground, which lies in the correspondence between the notion of dominance and nuclearity. It is possible to map between Grosz and Sidner&apos;s linguistic st"
W00-1425,J95-2003,0,0.352229,"ed as a part of text the description of the necklace to that of the destructuring. This requires better coordination signer, which is a side effect of embedding. between aggregation and other generation tasks Since the centers of sentences are normally as well as among different types of aggregation NPs and embedding adds non-restrictive comthan is present in current NLG systems. ponents into an NP, it could affect the way a Cb In this paper, we describe how to capture the is realised (e.g. preventing it from being a proabove interactions as preferences among related noun). As pointed out in (Grosz et al., 1995), features, and the implementation of the preferdifferent realisations (e.g. pronoun vs. definite ences in two very different generation architecdescription) are not equivalent with respect to tures to produce descriptions of museum objects their effect on coherence. Therefore, embedding on display. could influence local coherence by forcing a dif2 Preferences among coherence ferent realisation from that preferred by Centerfeatures ing Theory. There is an obvious need to balance the consideration for local coherence and stylisWe claim that it is the relative preferences tic preferences. among"
W00-1425,W98-1411,1,0.893564,"style jewels usually have an elaborate design. They tend to have floral motifs. For instance, this necklace has floral motifs. King was Scottish. She once lived in London. 3. The necklace is in the Arts and Crafts style. It is set with jewels in that it features cabuchon stones. Indeed, an Arts and Crafts style jewel usually uses cabuchon stones. It usually uses oval stones. 4. The necklace is in the Arts and Crafts style. It is set. with jewels in that it features cabuchon stones. An Arts and Crafts style jewel usually uses cabuchon stones and oval stones. Figure 1: Aggregation examples of (Mellish et al., 1998a) which uses a joint relation to connect every two text spans that do not have a semantic relation other t h a n objectattribute elaboration and conjunct/disjunct in between. Although joint is not preferred when other relations are present, it is better than missing presuppositions or embedding a conj u n c t relation inside a semantic relation. Therefore, we have the following heuristics, where "" A &gt; B "" means that A is preferred over B. H e u r i s t i c 1 Preferences among features for global coherence: a semantic relation &gt; Conjunct/Disjunct &gt; Joint &gt; presuppositions not met Joint &gt; Conju"
W00-1425,W98-1404,1,0.8867,"Missing"
W00-1425,J96-3006,0,0.0536338,"Missing"
W00-1425,W97-1301,0,0.0120914,"s always preferred, we have the following heuristic: H e u r i s t i c 3 Preferences among semantic relations and center transitions: a semantic relation &gt; Joint ÷ Continuation 2.4 P r e f e r e n c e s f o r e m b e d d i n g We distinguish between.a.-good,.rwrmal,and-bad embedding based on the features it bears. We do not claim that the set of features is complete. In a different context, more criteria might have to be considered. A good embedding is one satisfying all the following conditions: 1. The referring part is an indefinite, a demonstrative or a bridging description (as defined in (Poesio et al., 1997)). 2. The embedded part can be realised as an adjective or a prepositional phrase (Scott and de Souza, 1990). 3. In the resulting text, the embedded part does not lie between text spans connected by semantic parataxis and hypotaxis. 4. There is an available syntactic slot to hold the embedded part. A good embedding is highly preferred and should be performed whenever possible. A normal embedding is one satisfying condition 1, 3 and 4 and the e m b e d d e d part is a relative clause which provides additional information about the referent. Bad embeddings are all those left, for example, if the"
W00-1425,C98-2237,1,\N,Missing
W00-1425,P87-1022,0,\N,Missing
W00-1435,W90-0108,0,0.041837,"ly, taking into account the user's context of browsing. For more information on ILEX, see Knott et al. (1997) and Mellish et al. (1998). The demonstration will consist of generating a series of texts, in each case adding in additional components of the domain semantics. This short paper should be read in conjunction with the full paper elsewhere in this volume. Material silver enamel gold Domain Taxonomy: detailing the taxonomic organisation of the various classes of the entities. 3. Mapping Domain taxonomy onto Upper Model: ILEX uses an Upper Model (a domainindependent semantic taxonomy, see Bateman (1990)), which supports the grammatical expression of entities, e.g., selection of pronoun, differentiation between mass and count entities, between things and qualities, etc. We require that the basic types in the domain taxonomy are mapped onto the upper model, to allow the entities to be grammaticalised and lexicalised appropriately. 2 Generating from Bare Data We start initially with a relational database, as defined by a set of tab-delimited database files, plus some minimal semantics. As discussed in the paper, we use assume a relational database to consist of two types of files: With just thi"
W00-1435,W98-1404,1,0.834128,"tabases, and this demonstration will focus on this ability. We will also show how incremental extensions to the domain semantics improve the quality of the text produced. Figure 2: A Sample from a Link file column are, e.g., string, entity-id, domain type, etc. . 1 Introduction ILEX is a tool for dynamic browsing of databasedefined information: it allows a user to browse through the information in a database using hypertext. ILEX generates descriptions of a database object on the fly, taking into account the user's context of browsing. For more information on ILEX, see Knott et al. (1997) and Mellish et al. (1998). The demonstration will consist of generating a series of texts, in each case adding in additional components of the domain semantics. This short paper should be read in conjunction with the full paper elsewhere in this volume. Material silver enamel gold Domain Taxonomy: detailing the taxonomic organisation of the various classes of the entities. 3. Mapping Domain taxonomy onto Upper Model: ILEX uses an Upper Model (a domainindependent semantic taxonomy, see Bateman (1990)), which supports the grammatical expression of entities, e.g., selection of pronoun, differentiation between mass and co"
W05-1613,J03-1003,0,0.0688477,"Missing"
W05-1613,P98-1116,0,0.082283,"Missing"
W05-1613,W98-1404,1,0.861336,"Missing"
W05-1613,P95-1053,0,0.0944925,"Missing"
W05-1613,J93-1008,0,0.115269,"Missing"
W05-1613,N01-1001,1,0.901589,"Missing"
W05-1613,C98-1112,0,\N,Missing
W05-1621,N04-1015,0,0.12579,"Missing"
W05-1621,P87-1022,0,0.549936,"Missing"
W05-1621,W03-2304,0,0.444596,"Missing"
W05-1621,W02-2111,1,0.803445,"Missing"
W05-1621,W00-1411,0,0.293794,"Missing"
W05-1621,P03-1069,0,0.328288,"Missing"
W05-1621,W02-2113,0,0.0558959,"Missing"
W07-2310,W06-1414,0,0.348007,"is means they need an entry in the lexicon that maps them to a dependency tree (classes merely need a noun phrase). A straightforward way to obtain such entries is to let a system administrator create them when needed. However, this would cause considerable delays for the user and would look almost as restrictive as not allowing new property creation at all. Instead, we want to enable the system to create these lexicon entries immediately, so the user can use the new property that session. Using the property name that the user provides, the system should generate an appropriate lexicon entry. Hallett (2006) describes a portable WYSIWYM application that generates its lexicon automatically. It is unclear, however, how such a fully automated approach would work in practice. ONTOSUM (Bontcheva, 2005) depends on the user to provide 4.2 The Freedom of Folksonomies There are two types of property in OWL-Lite: object and datatype properties. The object properties have a class as their range, the datatype properties a data-type, such as ‘string’ or ‘date’. The implementation allows the user complete freedom in specifying the value of a ‘string’ datatype, using free text to enter them. Unfortunately this"
W07-2310,P83-1023,0,0.356116,"ural language applications are often domain specific and not very flexible. This makes the openendedness we need a great challenge. Existing elicitation approaches, such as using Controlled Languages, restrict in great measure what the user can and cannot say. We believe that to achieve the desired open-endedness and flexibility, the best ap2 Related Work Existing Semantic Grid tools that avoid the need to write RDF are often graphical (Handschuh et al., 2001). Natural language approaches include GINO (Bernstein and Kaufmann, 2006), an ontology editor with an approach reminiscent of NL-Menus (Tennant et al., 1983), and Controlled languages, e.g. PENG-D (Schwitter and Tilbrook, 2004). Natural language approaches tend to restrict expressivity to ensure that every entry can be parsed, 1 http://www.policygrid.org/ 2 http://www.flickr.com/ 3 http://www.data-archive.ac.uk/ 69 The tool presents the users with a text containing an expansion point (anchor) for each object that is mentioned, which has a menu with possible properties associated with that object. These objects and properties are defined by an underlying OWL-Lite ontology4 . The ontology we use for development is based on the UK Data Archive. This"
W07-2316,J97-1004,0,\N,Missing
W07-2325,W00-1429,1,0.87135,"Missing"
W07-2325,W07-2315,1,0.871054,"s how texts that communicate emotionally sensitive information should be worded. In this paper we focus on the usage of ’hedge phrases’ which communicate empathetic information, such as ""unfortunately."" An experiment in the domain of communicating exam results to students suggests that such emphathetic hedge phrases are appreciated by non-native English speakers, but disliked by native English speakers. 2 Motivation Some NLG systems produce texts that communicate emotionally sensitive information. In particular, the BT-Parent system, which is part of the BabyTalk project (Portet et al., 2007; Reiter, 2007), produces texts that summarise the condition of a baby in a neonatal intensive care unit, for the baby’s parents. Such texts must be worded in a way which minimises emotional distress, while of course still being truthful. The work here is an initial attempt to explore one aspect of how sensitive information should be communicated. Because of ethical considerations, we have conducted this initial experiment with a different group, students who are being told about exam results. 155 Hedge Phrases Hedging can be described as a strategy by which speakers mitigate and soften the force of their ut"
W08-1110,P00-1020,0,0.52669,"e. In all of these situations, the solution could be to use additional measuring methods (e.g. physiological methods), and to check if the results of such methods can strengthen the results of the questionnaires. Another option is to use an objective observer during the experiment (e.g. videotaping the participants and observing the duration of smiles or frowns) to judge whether the subject is affected. Yet another possibility would be only to measure emotional effects via performance on a task that is known to be facilitated by particular emotions. For instance, one could use the methods of (Carenini and Moore, 2000) to measure persuasiveness of different textual realisations that may induce emotions. This paper presented our efforts to measure differences in emotional effects invoked in readers. These efforts were based on our assumption that the wording used to present a particular proposition matters in how the message is received. Participants’ judgements of the negative or positive nature of a text (in both the text validation and in the full study) are in accord with our predictions. In terms of reflective analysis of the text, therefore, participants behave as we expected. Although we strongly emph"
W08-1118,W06-1414,0,0.012374,"l learning curve before the user knows which structures are allowed. In order to maintain full expressivity and to shorten the learning curve, we have elected to use WYSIWYM (What You See Is What You Meant) (Power et al., 1998). This is a natural language generation approach where the system generates a feed138 3 http://www.w3.org/TR/owl-features/ back text for the user that is based on a semantic representation. This representation is edited directly by the user by manipulating the feedback text. WYSIWYM has been used by a number of other projects, such as MILE (Piwek et al., 2000) and CLEF (Hallett, 2006). As evaluation results in both of these projects were very positive (Piwek, 2002; Hallett et al., 2007), we felt that WYSIWYM would be a suitable approach to use in our work. We have developed a metadata elicitation tool that enables users to create metadata in the shape of ontology instance data; the tool is driven by the ontologies that define those instances. We are currently implementing a WYSIWYM tool for querying, that uses the same interface as the metadata creation tool. We also aim to develop a tool for presenting the results of the query, and for browsing the descriptions in the dat"
W08-1118,W07-2310,1,0.77073,"divided into paragraphs which correspond to the individuals; each property of an individual is realised as (part of) a sentence in its paragraph. Each property in the ontology is associated with a linguistic specification, a Dependency Tree (Mel’cuk, 1988) that corresponds to a sentence. The specification has slots where the source and target of the property should be inserted, and is sufficiently detailed to support processes such as aggregation, through which the feedback text is made more fluent. For a more extensive description of the metadata cre139 ation tool and its implementation, see Hielkema et al. (2007b). In August 2007 we ran a pilot evaluation study (Hielkema et al., 2007a) on this tool. This study was heuristic in nature, with subjects discussing the interface with the experimenter while performing set tasks. It highlighted a number of aspects which we felt it was necessary to improve before embarking on the formal evaluation. Apart from there being standard usability considerations such as a need for better undo and help functions, it became evident that the underlying ontology was neither extensive enough nor sufficiently well-structured: subjects struggled to find the options they nee"
W08-1118,P83-1023,0,0.522382,"ng and browsing of metadata by users with no prior experience of such technologies. Existing tools that provide access to RDF metadata are often graphical, e.g. (Handschuh et al., 2001; Catarci et al., 2004). However, we believe that, for social scientists, natural language is the best medium to use, as the way they conduct their research and the structure of their documents and data indicate that they are more oriented towards text than graphics. Natural language approaches include GINO (Bernstein and Kaufmann, 2006), an ontology editor with an approach reminiscent of Natural Language Menus (Tennant et al., 1983), and using Controlled languages such as PENG-D (Schwitter and Tilbrook, 2004). Such natural language approaches tend to restrict expressivity to ensure that every entry can be parsed, limiting the language and often making it stilted, so that there is a small learning curve before the user knows which structures are allowed. In order to maintain full expressivity and to shorten the learning curve, we have elected to use WYSIWYM (What You See Is What You Meant) (Power et al., 1998). This is a natural language generation approach where the system generates a feed138 3 http://www.w3.org/TR/owl-f"
W08-1118,J07-1006,0,\N,Missing
W09-0625,P08-1020,0,0.0262269,"an important step towards the empirical evaluation of affective NLG systems. 1 Chris Mellish University of Aberdeen Aberdeen c.mellish@abdn.ac.uk Introduction This paper is about developing techniques for the empirical evaluation of affective natural language generation (NLG). Affective NLG has been defined as “NLG that relates to, arises from or deliberately influences emotions or other non-strictly rational aspects of the Hearer” (De Rosis and Grasso, 2000). It currently covers two main strands of work, the portrayal of non-rational aspects in an artificial speaker/writer (e.g. the work of Mairesse and Walker (2008) on projecting personality) and the use of NLG in ways sensitive to the non-rational aspects of the hearer/reader and calculated to achieve effects on these aspects (e.g. the work of De Rosis et al. (1999) on generating instructions in an emotionally charged situation and that of Moore et al. (2004) on producing appropriate tutorial feedback). Although there has been success in evaluating work of the first kind, it remains more problematic to evaluate whether work of the second type directly affects emotion or Proceedings of the 12th European Workshop on Natural Language Generation, pages 146–"
W09-0625,W08-1110,1,0.876792,"Missing"
W09-0625,P00-1020,0,\N,Missing
W09-0625,W02-2108,0,\N,Missing
W10-4208,W09-0613,0,0.0297638,"e of 0.279. Following the learned policy, the system also obtained an average of 0.279. The difference between the learned behaviour and random generation is significant (p =0.002) according to a t test. 7 Experiment 2: Text length control A challenging stylistic requirement for NLG is that of producing a text satisfying precise length requirements (Reiter 2000). For this experiment, we took the EleonPlus NLG system developed by Hien Nguyen. This combines the existing Eleon user interface for domain authoring (Bilidas et al 2007) with a new NLG system that incorporates the SimpleNLG realiser (Gatt and Reiter 2009). The system was used for a simple domain of texts about university buildings. The data used was the authored information about 7 university buildings and associated objects. We evaluated texts using a simple (character) length criterion, where the ideal text was 250 characters, with a steeply increasing penalty for texts longer than this and a slowly increasing penalty for texts that are shorter. The notion of “state” that was logged took account of the depth of the traversal of the domain data, the maximum number of facts per sentence and an aggregation decision. Following the previous succe"
W10-4208,P08-1020,0,0.0230508,"the values of the stylistic features achieved. From this data the system then learns correlations between these: SS i ≅ SS est i = x0 + ∑ x j .GD j j To actually generate a text given stylistic goals SSi, the system then uses an online control regime. At each choice point, it considers making GDj versus not GDj. For each of these two, it estimates all the SSi that will be obtained for the complete text, using the learned equations. It prefers the choice that minimises the sum of absolute differences between these and the goal SSi, but is prepared to backtrack if necessary (best-first search). Mairesse and Walker (2008) (henceforth MW) use a different method for tuning their NLG system (“PERSONAGE”), whose objective is to produce texts in the styles of writers with different personality types. In this case, the system performance depends on 67 parameters, e.g. REPETITIONS (whether to repeat existing propositions), PERIOD (leave two sentences connected just with “.”, rather MW, offline training involves having the program generate a set of outputs with random values for all the parameters. Human judges estimate values for the “big five” personality traits (e.g. extroversion, neuroticism) for each output. Mach"
W10-4208,P05-1008,0,0.0291357,"any NLG system written in Java and have carried out two short experiments with existing NLG systems. 2 Controlling an NLG System: Examples NLG systems are frequently required to produce output that conforms to particular stylistic guidelines. Often conformance can only be tested at the end of the NLG pipeline, when a whole number of complex strategic and tactical decisions have been made, resulting in a complete text. A number of recent pieces of work have begun to address the question of how to tune systems in order to make the decisions that lead to the most stylistically preferred outputs. Paiva and Evans (2005) (henceforth PE) investigate controlling generator decisions for achieving stylistic goals, e.g. choices between: The patient takes the two gram dose of the patient’s medicine twice a day. and The dose of the patient’s medicine is taken twice a day. It is two grams. In this case, a stylistic goal of the system is expressed as goal values for features SSi, where each SSi expresses something that can be measured in the output text, e.g. counting the number of pronouns or passives. The system learns to control the number of times specific binary generator decisions are made (GDj), where these dec"
W10-4208,P06-2085,0,0.0253172,"ty buildings. The data used was the authored information about 7 university buildings and associated objects. We evaluated texts using a simple (character) length criterion, where the ideal text was 250 characters, with a steeply increasing penalty for texts longer than this and a slowly increasing penalty for texts that are shorter. The notion of “state” that was logged took account of the depth of the traversal of the domain data, the maximum number of facts per sentence and an aggregation decision. Following the previous successful demonstration of reinforcement learning for NLG decisions (Rieser and Lemon 2006), we decided to use the SARSA approach (though without function approximation) for the training. This involves rewarding individual states for their (direct or indirect) influence on outcome quality as the system actually performs. The policy is a mixture of random exploration and the choosing of the currently most promising states, according to the value of a numerical parameter ε. Running the system on the 7 examples with 3 random generations for each produced an average text quality of -2514. We tried a SARSA training regime with 3000 random examples at ε=0.1, followed by 2000 random exampl"
W10-4208,E06-2020,0,0.0285272,"nel to support training of the system. It is important to note that for these systems the instrumenting was done by someone (the author) with limited knowledge of the underlying NLG system and with a notion of text quality different from that used by the original system. Also, in both cases the limited availability of example data meant that testing had to be performed on the training data (and so any positive results may be partly due to overfitting). 6 Experiment 1: Matching human texts For this experiment, we took an NLG system that produces pollen forecasts and was written by Ross Turner (Turner et al 2006). Turner collected 68 The notion of program “state” that the oracle logged took the form of the 6 input values, together with the values of 7 choices made by the system (relating to the inclusion of trend information, thresholds for the words “high” and “low”, whether to segment the data and whether to include hay fever information). The system was trained by generating about 10000 random texts (making random decisions for randomly selected examples). For each, the numerical outcome (Meteor score) and state information was recorded. The half of the resulting data with highest outcomes was extr"
W10-4208,J00-2005,0,\N,Missing
W10-4209,W02-1703,0,0.0210724,"logies. The trouble with applying existing annotation methods (e.g. the Text Encoding Initiative) to NLG is that they presuppose the existence of a linear text to start with, whereas in NLG one is forced to represent more abstract structures before coming up with the actual text. A recent proposal from Linguistics for a linguistic ontology for the semantic web (Farrar and Langendoen, 2003) is again based around making annotations to existing text. Research is only just beginning to escape from a “time-based” mode of annotation, for instance by using “stand-off” annotations to indicate layout (Bateman et al., 2002). In addition, most annotation schemes are partial (only describe certain aspects of the text) and non-structured (assign simple labels to portions of text). For NLG, one needs a way of representing all the information that is needed for generating a text, and this usually has complex internal structure. Linguistic ontologies are ontologies developed to describe linguistic concepts. Although ontologies are used in a number of NLP projects (e.g. (Estival et al., 2004)), the ontologies used are usually ontologies of the application domain rather than the linguistic structures of natural language"
W10-4209,W04-0609,0,0.0275845,"st beginning to escape from a “time-based” mode of annotation, for instance by using “stand-off” annotations to indicate layout (Bateman et al., 2002). In addition, most annotation schemes are partial (only describe certain aspects of the text) and non-structured (assign simple labels to portions of text). For NLG, one needs a way of representing all the information that is needed for generating a text, and this usually has complex internal structure. Linguistic ontologies are ontologies developed to describe linguistic concepts. Although ontologies are used in a number of NLP projects (e.g. (Estival et al., 2004)), the ontologies used are usually ontologies of the application domain rather than the linguistic structures of natural languages. The development of ontologies to describe aspects of natural languages is comparatively rare. The WordNet ontologies are a widely used resource describing the repertoire of word senses of natural languages, but these concentrate on individual words rather than larger linguistic structures. More relevant to NLG is work on various versions of the Generalised Upper Model (Bateman et al., 1995), which outlines aspects of meaning relevant to making NLG decisions. This"
W10-4209,W03-0805,0,0.0780898,"Missing"
W11-2820,J88-3006,0,0.490899,"f obligations, prohibitions and permissions, possibly arising from different sources, using semantic web ontologies. Combining information from multiple sources has been used in user 151 Although policies can be used to control a number of aspects of adaptation, here we concentrate on their use within Natural Language Generation (NLG), mainly for content determination. In general NLG is often conceived as being responsive to multiple goals or constraints (e.g. Hovy, 1990). In addition, the content and form of a generated text often needs to be tailored to at least certain aspects of the user (Paris, 1988; Bateman and Paris, 1989). However, not many general mechanisms have been presented for dynamically combining different aspects of the context for guiding NLG. Plan-based tailoring (Paris, 1988; Paris et al., 2004) might provide part of such a mechanism, but it assumes a topdown approach to text planning, which is not natural for applications that just have to express some of what happens to be there in the input data (Marcu, 1997). Requirements on style, syntax, content, etc. can all be expressed and combined in constraintbased NLG (Piwek and van Deemter, 2007), but existing implementations"
W11-2820,C10-2116,0,0.0135184,"roups. Provenance (also referred to as lineage or heritage) aims to provide additional documentation about the processes that led to the creation of an artifact. Within this environment, a short textual description of an artifact, person or project can be valuable to a user. We have developed an NLG service to generate text descriptions of those resources based on the RDF metadata held by the system. This service has to perform “ontology verbalisation” (i.e. translate ontology fragments into natural language), a topic on which there has been much previous research (e.g. Sun and Mellish, 2007; Power and Third, 2010). Our own approach builds on the system of Hielkema (2010). However, work on ontology verbalisation has not yet presented general mechanisms for content determination from semantic web data. This paper discusses how policies can be used to tailor the content selected for an NLG service like ours, so that it adapts according to the context of use. 2 Capturing Context Underpinning the VRE is a rich and pervasive RDF (Klyne and Carroll, 2004) metadata infrastructure built upon a series of OWL ontologies (McGuinness and van Harmelen, 2004) describing aspects 1 of the provenance of digital artifact"
W11-2820,P92-1034,1,0.398413,"ural for applications that just have to express some of what happens to be there in the input data (Marcu, 1997). Requirements on style, syntax, content, etc. can all be expressed and combined in constraintbased NLG (Piwek and van Deemter, 2007), but existing implementations only use general constraintsatisfaction mechanisms for particular parts of the generation problem. Generation based on Systemic Grammar (Bateman, 1997) provides a clear mechanism for decision-making and tailoring (Bateman and Paris, 1989) but is less clear on the representation of context. In generation by classification (Reiter and Mellish, 1992), contexts are complex objects classified into an ontology. Aspects relevant Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 151–157, c Nancy, France, September 2011. 2011 Association for Computational Linguistics to particular generation decisions are then inherited according to where the context has been classified. Although this is elegant in theory, in practice, such ideas are now used more as part of object-oriented programming approaches to NLG (White and Caldwell, 1998). It thus remains to be seen both to what extent declarative representation of c"
W11-2820,W07-2316,1,0.897832,"Missing"
W11-2820,W98-1428,0,0.0294967,"s less clear on the representation of context. In generation by classification (Reiter and Mellish, 1992), contexts are complex objects classified into an ontology. Aspects relevant Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 151–157, c Nancy, France, September 2011. 2011 Association for Computational Linguistics to particular generation decisions are then inherited according to where the context has been classified. Although this is elegant in theory, in practice, such ideas are now used more as part of object-oriented programming approaches to NLG (White and Caldwell, 1998). It thus remains to be seen both to what extent declarative representation of contexts and NLG decision making is possible, and also to what extent control of NLG can use similar mechanisms to other types of adaptation. The current work can be seen as further exploration of this territory. In this paper, we report on policy-driven control of NLG as we have integrated it in a Virtual Research Environment (VRE) called ourSpaces1 . This system has been developed to facilitate collaboration and interaction between researchers by enabling users to track the provenance of their digital artifacts an"
W12-1505,W08-1104,1,0.744411,"Underwood et al., 2008). However, in such long-term and wide ranging initiatives, maintaining volunteer engagement can be challenging and volunteers must get feedback on their contributions to remain motivated to participate (Silvertown, 2009). NLG may serve the function of supplying this feedback. 3 Related work We are particularly interested in summarizing raw geographical and temporal data whose semantics need to be computed at run time – so called spatiotemporal NLG. Such extended techniques are studied in data-to-text NLG (Molina and Stent, 2010; Portet et al., 2009; Reiter et al., 2005; Turner et al., 2008; Thomas et al., Published online 2010). Generating text from spatio-temporal data involves not just finding data abstractions, but also determining appropriate descriptors for them (Turner et al., 2008). Turner et. al (2008) present a case study in weather forecast generation where selection of spatial descriptors is partly based on domain specific (weather related) links between spatial descriptors 17 INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 17–21, c Utica, May 2012. 2012 Association for Computational Linguistics and weather phenomena. In t"
W12-1505,W11-2803,0,\N,Missing
W12-1505,W09-0625,1,\N,Missing
W12-1520,W10-1301,0,0.0308918,"ges 120–124, c Utica, May 2012. 2012 Association for Computational Linguistics (a) (b) Figure 1: Plot of (a) distance from nest as a function of time, and (b) clusters of visited locations. ically been used to generate summaries of technical data for professionals, such as engineers, nurses and oil rig workers. There is some work on the use of data-to-text for lay audiences; e.g., generating narratives from sensor data for automotive (Reddington et al., 2011) and environmental (Molina et al., 2011) applications, generating personal narratives to help children with complex communication needs (Black et al., 2010), and summarising neonatal intensive care data for parents (Mahamood et al., 2008). Our application differs from the above-mentioned data-to-text applications, in that we aim to generate inspiring as well as informative texts. It bears some resemblance to NLG systems that offer “infotainment”, such as Dial Your Disc (Van Deemter and Odijk, 1997) and Ilex (O’Donnell et al., 2001). In fact, Dial Your Disc, which generates spoken monologues about classical music, focused emphatically on generating engaging texts, and achieved linguistic variation through the use of recursive, syntactically struct"
W12-1527,H05-1042,0,0.176602,"user profile, discourse history, query, etc) needed for this task depend on the application domain. This often led in the past to templateor graph-based combined content selection and discourse structuring approaches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012),"
W12-1527,W11-2810,1,0.799661,"aches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However, although most of these works make a good attempt at realisation, the problem of content determination from Semantic Web data is relatively untouched. For these reasons, we believe that"
W12-1527,W03-1016,0,0.767573,"tory, query, etc) needed for this task depend on the application domain. This often led in the past to templateor graph-based combined content selection and discourse structuring approaches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However"
W12-1527,W09-0623,0,0.0155452,"e application domain. This often led in the past to templateor graph-based combined content selection and discourse structuring approaches operating on idiosyncratically encoded small sets of input data. Furthermore, in many NLG-applications, target texts and sometimes even empirical data are not available, which makes it difficult to employ empirical approaches to knowledge elicitation. Nonetheless, during the last decade, there has been a steady flow of new work on content selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However, although most of these works make a good att"
W12-1527,C10-2116,0,0.028387,"ontent selection that employed Machine learning (Barzilay and Lapata, 2005; Duboue and McKeown, 2003; Jordan and Walker, 2005; Kelly et al., 2009), heuristic search (O’Donnell et al., 2001; Demir et al., 2010; Mellish and Pan, 2008), or a combination thereof (Bouayad-Agha et al., 2011). All of these strategies can deal with large volumes of data. On the other side, there is a clear tendency in NLG towards the use of resources encoded in terms of standard Semantic Web representation formats such as OWL and RDF, e.g., (Wilcock and Jokinen, 2003; Bontcheva and Wilks, 2004; Mellish and Pan, 2008; Power and Third, 2010; Bouayad-Agha et al., 2011; Dannells et al., 2012), to name but a few. However, although most of these works make a good attempt at realisation, the problem of content determination from Semantic Web data is relatively untouched. For these reasons, we believe that the time has come to bring together researchers working on (or interested in working on) content selection to participate in a challenge for this task using standard freely available web data as input. The availability of open modular multi-domain multi-billion triple data and of open ontological resources (Bizer et al., 2009) prese"
W12-1527,W10-4202,0,\N,Missing
W12-1527,W07-2322,0,\N,Missing
W13-2112,W13-2133,1,0.692485,"Missing"
W13-2112,W13-2134,0,0.0651598,"Missing"
W13-2112,W12-1527,1,0.850892,"iversity of Illinois at Chicago (USA). Before the presentation of the baseline evaluation of the submitted systems and the discussion of the results (Section 4), we outline the two data preparation subtasks (Sections 2 and 3). In Section 5, we then sketch some conclusions with regard to the achievements and future of the content selection task challenge. More details about the data, annotation and resources described in this overview, as well as links for downloading the data and other materials (e.g., evaluation results, code, etc.) are available on the challenge’s website.2 Introduction In (Bouayad-Agha et al., 2012), we presented the NLG challenge of content selection from semantic web data. The task to perform was described as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reflected in the target text (i.e., a short biography about that celebrity). The task first required a data preparation stage that involved the following two subtasks: 1) data gathering and preparation, that is, deciding which data and texts to use, then downloading and pairing them, and 2) working dataset selection and annotation, that is, defining the criteria/guidelines for det"
W13-2112,W03-1016,0,0.0390448,"submitted systems was limited. Both of the presented systems were data-intensive in that they usedeither a pool of textual knowledge or the corpus of triple data provided by the challenge in order to select the most relevant data. Unlike several previous challenges that involve more traditional NLG tasks (e.g., surface realization, referring expression generation), content selection from large input semantic data is a relatively new research endeavour in the NLG community that coincides with the rising interest in statistical approaches to NLG and dates back, to the best of our knowledge, to (Duboue and McKeown, 2003). Furthermore, although we had initially planned to produce a training set for the task, the cost of manual annotation turned out to be prohibitive and the resulting corpus was only fit for development and baseline evaluation. Despite these setbacks, we believe that open semantic web data is a promising test-bed and application field for NLG-oriented content selection (Bouayad-Agha et al., 2013) and trust that this first challenge has prepared the ground for follow up challenges with a larger participation. We would also like to encourage researchers from NLG and Semantic Web research fields t"
W13-2119,bouayad-agha-etal-2002-pills,0,0.0465122,"olves the description of the “treatment and findings”, which describes the events that happen whilst the patient is being cared for and relevant parts of the sensor data (see Figure 1). For this section of the report, the document planning algorithm is based on that of (Portet et al., 2007), which identifies a number of key events and creates a paragraph for each key event. Events that are explicitly linked to the key event or events that happen at the same time are added to the relevant paragraph. This is based on the earlier work of (Hallett et al., 2006). STOP (Reiter et al., 2003), PILLS (Bouayad-Agha et al., 2002), MIGRANE (Buchanan et al., 1992), and Healthdoc (Hirst et al., 1997). Other systems, such as TOPAZ (Kahn et al., 1991) and Suregen (H¨uske-Kraus, 2003), aim to summarise information in order to support medical decision-making. In the case of MIME, the challenge is to summarise large amounts of sensor data, in the context of carer observations and actions, in a coherent way that supports quick decision making by the reader. The problem of describing the data relates to previous work on summarising time series data (e.g. (Yu et al., 2007)). In many ways, though, our problem is most similar to t"
W13-2119,E03-2008,0,0.0946256,"Missing"
W13-2133,W03-1016,0,\N,Missing
W13-2133,W12-1527,1,\N,Missing
W14-4417,P12-1039,0,0.0137754,"remain apparently inactive because they have nothing positive to report. So, even though there was a need for automatically generated feedback now, there was a real question of who the readers would be and how to select the content to include in the feedback. 4 Related Work A standard approach to establish user requirements for NLG is to assemble a corpus of human-authored texts and their associated inputs (Reiter & Dale, 2000). This can be the basis of deriving rules by hand, or one can attempt to replicate content selection rules from the corpus by machine learning (Duboue & McKeown, 2003; Konstas & Lapata, 2012). To produce a useful corpus, however, one has to know one’s users or have reliable expert authors. As first pointed out by Levine et al. (1991), an NLG system that produces hypertext, rather than straight text, can avoid some content selection decisions, as the user makes some of these decisions by selecting links to follow. A similar advantage applies to other adaptive hypertext systems (Brusilovsky, 2001). Another general possibility is to allow users to design aspects of the texts they receive. For instance, ICONOCLAST (Power, Scott, & Bouayad-Agha, 2003) allows users to make choices about"
W14-4417,W12-1505,1,0.897088,"Missing"
W14-4417,W03-1016,0,\N,Missing
W15-0402,W13-0212,1,0.831337,"Missing"
W94-0329,C92-3158,0,0.0185286,"production rules to filter the documents, is probably closest to what we are doing in CORECT. The above-mentioned systems all simply distributed complete messages. In CORECT, however, our intention is to go beyond this by extracting information relevant to a particular user from the common knowledge pool, and then presenting this to the user as a natural language document. Other NLG systems that extract and summarise information have been developed in other research, particularly by CoGenTex; their systems include, for example, FOG (Bourbeau et al., 1990), which produced weather reports; LFS (Iordanskaja et al., 1992), which summarised employment statistics; and Joyce (Rambow and Korelsky, 1992), which summarised software designs from a security perspective. The work on Joyce is particularly interesting because part of its justification was that natural language design summaries are useful to the designers themselves, as well as to people outside the design group. We expect that designers will find summaries even more useful in a multi-author design tool such as CORECT, since they will give them an overview of the progress of the design as a whole, and of what their colleagues have accomplished to date. Th"
W94-0329,A92-1006,0,0.177935,"oing in CORECT. The above-mentioned systems all simply distributed complete messages. In CORECT, however, our intention is to go beyond this by extracting information relevant to a particular user from the common knowledge pool, and then presenting this to the user as a natural language document. Other NLG systems that extract and summarise information have been developed in other research, particularly by CoGenTex; their systems include, for example, FOG (Bourbeau et al., 1990), which produced weather reports; LFS (Iordanskaja et al., 1992), which summarised employment statistics; and Joyce (Rambow and Korelsky, 1992), which summarised software designs from a security perspective. The work on Joyce is particularly interesting because part of its justification was that natural language design summaries are useful to the designers themselves, as well as to people outside the design group. We expect that designers will find summaries even more useful in a multi-author design tool such as CORECT, since they will give them an overview of the progress of the design as a whole, and of what their colleagues have accomplished to date. The proposed combination of CSCW for collecting and modifying the knowledge pool"
W94-0329,C92-1038,0,0.0245273,"Missing"
W94-0329,C90-3059,0,\N,Missing
W94-0329,C90-1021,0,\N,Missing
W94-0329,A92-1009,1,\N,Missing
W96-0404,C94-1078,0,0.0520418,"er of matching Our approach can be seen as a generalisanodes has been used to rate different matches, tion of semantic head-driven generation [20]-which is similar to finding maximal reductions in we deal with a non-hierarchical input and non[6]. Alternatively a notion of semantic distance concatenative grammars. The use of Lexicalised [5] might be employed. In PROTECTOR we will DTG means that the algorithm in effect looks use a much more sophisticated notion of what first for a syntactic head. This aspect is similar it is for a conceptual graph to match better the to syntax-driven generation [8]. initial semantics than another graph. This capThe algorithm has to be checked against more tures the intuition that the generator should try linguistic data and we intend to do more work on to express as much as possible from the input additional control mechanisms and also using alwhile adding as little as possible extra material. ternative generation strategies using knowledge We use instructions showing how the se- sources free from control information. To this mantics of a mother syntactic node is computed end we have explored aspects of a new semanticbecause we want to be able to correc"
W96-0404,P92-1007,0,0.0196745,"sing trees as monotonic. The two combination operations that DTG uses are subsertion and sisteradjunetion. D-Tree Grammars Our generator uses a particular syntactic t h e o r y - - D - T r e e G r a m m a r ( D I G ) which we briefly introduce because the generation strategy is influenced by the linguistic structures and the operations on them. D-Tree G r a m m a r (DTG) [16] is a new grammar formalism which arises from work on TreeAdjoining Grammars (TAG) [7]. In the context of generation, TAGS have been used in a number of systems MUMBLE [10], SPOKESMAN [11], Wm [27], the system reported in [9], the first version of PROTECTOR [12], and recently SPUD (by Stone & Doran). In the area of grammar development TAG has been the basis of one of the largest grammars developed for English [4]. Unlike TAGs, DTGs provide a uniform treatment of complementation and modification at the syntactic level. DTGs are seen as attractive for generation because a close match between semantic and syntactic operations leads to simplifications in the overall generation architecture. DTGS try to overcome the problems associated with T A G S while remaining faithful to what is seen as the key advantages of TAGs"
W96-0404,P85-1012,0,0.0465328,"2). D-trees allow us to view the operations for composing trees as monotonic. The two combination operations that DTG uses are subsertion and sisteradjunetion. D-Tree Grammars Our generator uses a particular syntactic t h e o r y - - D - T r e e G r a m m a r ( D I G ) which we briefly introduce because the generation strategy is influenced by the linguistic structures and the operations on them. D-Tree G r a m m a r (DTG) [16] is a new grammar formalism which arises from work on TreeAdjoining Grammars (TAG) [7]. In the context of generation, TAGS have been used in a number of systems MUMBLE [10], SPOKESMAN [11], Wm [27], the system reported in [9], the first version of PROTECTOR [12], and recently SPUD (by Stone & Doran). In the area of grammar development TAG has been the basis of one of the largest grammars developed for English [4]. Unlike TAGs, DTGs provide a uniform treatment of complementation and modification at the syntactic level. DTGs are seen as attractive for generation because a close match between semantic and syntactic operations leads to simplifications in the overall generation architecture. DTGS try to overcome the problems associated with T A G S while remaining fa"
W96-0404,P95-1021,0,0.0922095,"e processing strategies using the same knowledge sources can therefore be envisaged. 3 between nodes. Graphically we will use a dashed line to indicate a d-link (see Figure 2). D-trees allow us to view the operations for composing trees as monotonic. The two combination operations that DTG uses are subsertion and sisteradjunetion. D-Tree Grammars Our generator uses a particular syntactic t h e o r y - - D - T r e e G r a m m a r ( D I G ) which we briefly introduce because the generation strategy is influenced by the linguistic structures and the operations on them. D-Tree G r a m m a r (DTG) [16] is a new grammar formalism which arises from work on TreeAdjoining Grammars (TAG) [7]. In the context of generation, TAGS have been used in a number of systems MUMBLE [10], SPOKESMAN [11], Wm [27], the system reported in [9], the first version of PROTECTOR [12], and recently SPUD (by Stone & Doran). In the area of grammar development TAG has been the basis of one of the largest grammars developed for English [4]. Unlike TAGs, DTGs provide a uniform treatment of complementation and modification at the syntactic level. DTGs are seen as attractive for generation because a close match between sem"
W96-0404,J82-1002,0,0.0840159,"n of technical documentation [25]. This work improves on existing generation approaches in the following respects: (i) Unlike the majority of generators this one takes a nonhierarchical (logically well defined) semantic representation as its input. This allows us to look at a more general version of the realisation problem which in turn has direct ramifications for Figure 1: A simple conceptual graph the increased paraphrasing power and usability of the generator; (ii) Following Nogier & Zock The use of semantic networks in generation [14], we take the view that lexical choice is esis not new [21, 18]. Two main approaches have sentially (pattern) matching, but unlike them we been employed for generation from semantic net- assume that the meaning representation may not works: utterance path traversal and incremental be entirely consumed at the end of the gener2The tree-like semantics imposes some restrictions ation process. Our generator uses a notion of approximate matching and can happen to conwhich the language may not support. arguments are realised as complements of the main verb--thus the control information is to a large extent encoded in the tree-like semantic structure. Unfortunate"
W96-0404,W94-0331,0,0.0291734,"econd approach, that of incremental consumption, generation is done by gradually relating (consuming) pieces of the input semantics to linguistic structure [3, 13]. Such covering of the semantic structure avoids some of the limitations of the utterance path approach and is also the general mechanism we have adopted (we do not rely on the directionality of the conceptual relations per se--the primitive operation that we use when consuming pieces of the input semantics is maximal join which is akin to pattern matching). The borderline between the two paradigms is not clear-cut. Some researchers [22] are looking at finding an appropriate sequence of expansions of concepts and reductions of subparts of the semantic network until all concepts have realisations in the language. Others assume all concepts are expressible and try to substitute syntactic relations for conceptual relations [2]. Other work addressing surface realisation from semantic networks includes: generation using Meaning-Text Theory [6], generation using the SNePS representation formalism [19], generation from conceptual dependency graphs [26]. Among those that have looked at generation with conceptual graphs are: generatio"
W96-0404,W94-0332,0,0.0270564,"all concepts have realisations in the language. Others assume all concepts are expressible and try to substitute syntactic relations for conceptual relations [2]. Other work addressing surface realisation from semantic networks includes: generation using Meaning-Text Theory [6], generation using the SNePS representation formalism [19], generation from conceptual dependency graphs [26]. Among those that have looked at generation with conceptual graphs are: generation using Lexical Conceptual Grammar [15], and generating from CGs using categorial grammar in the domain of technical documentation [25]. This work improves on existing generation approaches in the following respects: (i) Unlike the majority of generators this one takes a nonhierarchical (logically well defined) semantic representation as its input. This allows us to look at a more general version of the realisation problem which in turn has direct ramifications for Figure 1: A simple conceptual graph the increased paraphrasing power and usability of the generator; (ii) Following Nogier & Zock The use of semantic networks in generation [14], we take the view that lexical choice is esis not new [21, 18]. Two main approaches hav"
W96-0404,C94-2149,0,\N,Missing
W96-0404,P89-1002,0,\N,Missing
W96-0416,J93-4004,0,\N,Missing
W98-1404,J86-3001,0,0.0890912,"Missing"
W98-1404,J81-1002,0,0.318175,"Missing"
W98-1404,W98-1411,1,0.866447,"Missing"
W98-1404,J93-4004,0,0.042291,"Missing"
W98-1404,W13-4001,0,0.0273921,"Missing"
W98-1411,W98-1404,1,0.848933,"Missing"
