2016.gwc-1.51,kipper-etal-2006-extending,0,0.112706,"O (Segers et al., 2015) is a newly developed domain ontology to enhance the extraction and linking of dynamic and static events and their implications in text. Explicit modeling of event implications allows for extracting sequences of states and changes over time regardless of if this information was directly expressed in text, or inferred by a reasoner. Figure 1 shows such a chain of expressions for dynamic (hire, starts at, fire, leave) and static events (works for, employs, is CEO) and their implied situations. Lexicons that define implications of events, e.g. VerbNet (Kipper et al., 2000; Kipper et al., 2006), are rare and usually focus on the meaning of verbs in isolation. However, lexical structures do not make explicit how the meaning of a verb needs to be combined with other event components, such as the participants and the temporal properties for the purpose of semantic parsing. We therefore follow an ontological approach to interpret situations on the basis of text interpretation of all the event components to make the implications explicit. Though some research on deductive reasoning over Frame annotated text (e.g. (Scheffczyk et al., 2006)) and defining pre and post situations of predicat"
2016.gwc-1.51,P98-1013,0,0.535899,"the car 600 dollar 600 dollar the car during situation the car hasValue 600 dollar Figure 3: Non-formal transcription of the mappings, assertions and instantiation for the ESO class FinancialTransaction from ESO roles (65) to FrameNet Frame Elements (131). The properties in this table pertain to those properties that are used in the situation rule assertions. 3 Predicate Matrix The PredicateMatrix (PM)(de Lacalle et al., 2014a; de Lacalle et al., 2014b) is an automatic extension of SemLink (Palmer, 2009) that merges several models of predicates such as VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). The PM also contains for each predicate features of the ontologies integrated in the Multilingual Central Repository (GonzalezAgirre et al., 2012) like SUMO (Niles and Pease, ´ 2001), Top Ontology (Alvez et al., 2008) or WordNet domains (Bentivogli et al., 2004). The mappings between such knowledge bases allow to take advantage from their individual strengths. For example, the coverage of PropBank or the semantic relations among events and participants of FrameNet. The semantic interoperability offered by the PM allows to translate"
2016.gwc-1.51,J05-1004,0,0.145031,"he car during situation the car hasValue 600 dollar Figure 3: Non-formal transcription of the mappings, assertions and instantiation for the ESO class FinancialTransaction from ESO roles (65) to FrameNet Frame Elements (131). The properties in this table pertain to those properties that are used in the situation rule assertions. 3 Predicate Matrix The PredicateMatrix (PM)(de Lacalle et al., 2014a; de Lacalle et al., 2014b) is an automatic extension of SemLink (Palmer, 2009) that merges several models of predicates such as VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). The PM also contains for each predicate features of the ontologies integrated in the Multilingual Central Repository (GonzalezAgirre et al., 2012) like SUMO (Niles and Pease, ´ 2001), Top Ontology (Alvez et al., 2008) or WordNet domains (Bentivogli et al., 2004). The mappings between such knowledge bases allow to take advantage from their individual strengths. For example, the coverage of PropBank or the semantic relations among events and participants of FrameNet. The semantic interoperability offered by the PM allows to translate the output of a SRL analysis to"
2016.gwc-1.51,W04-2214,0,0.537594,"re used in the situation rule assertions. 3 Predicate Matrix The PredicateMatrix (PM)(de Lacalle et al., 2014a; de Lacalle et al., 2014b) is an automatic extension of SemLink (Palmer, 2009) that merges several models of predicates such as VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). The PM also contains for each predicate features of the ontologies integrated in the Multilingual Central Repository (GonzalezAgirre et al., 2012) like SUMO (Niles and Pease, ´ 2001), Top Ontology (Alvez et al., 2008) or WordNet domains (Bentivogli et al., 2004). The mappings between such knowledge bases allow to take advantage from their individual strengths. For example, the coverage of PropBank or the semantic relations among events and participants of FrameNet. The semantic interoperability offered by the PM allows to translate the output of a SRL analysis to Component Event classes – DynamicEvent classes – StaticEvent classes SUMO class mappings FrameNet Frame mappings Situation rules Situation rule assertions – Pre situation rule assertions – Post situation rule assertions During situation rule assertions Properties – Unary properties – Binary"
2016.gwc-1.51,W14-0150,1,0.882019,"Missing"
2016.gwc-1.51,lopez-de-lacalle-etal-2014-predicate,1,0.908889,"Missing"
2016.gwc-1.51,gonzalez-agirre-etal-2012-multilingual,1,0.882799,"Missing"
2016.gwc-1.51,C98-1013,0,\N,Missing
2018.gwc-1.4,alvez-etal-2008-complete,1,0.85225,"not free of errors and inconsistencies. Unfortunatelly, improving, revising, and correcting such large knowledge bases is a never ending task that have been mainly carried out also manually. A few automatic approaches have been also applied focusing on checking certain structural properties on WordNet (e.g. (Daud´e et al., 2003), (Richens, 2008)) or using automated theorem provers on SUMO ´ (e.g. (Horrocks and Voronkov, 2006), (Alvez et al., 2012)). Just a few more have studied automatic ways to validate the knowledge content encoded in these resources by cross-checking them. For in´ stance, Alvez et al. (2008) exploit the EuroWordNet Top Ontology (Rodr´ıguez et al., 1998) and its mapping to WordNet for detecting many ontological conflicts and inconsistencies in the WordNet nominal hierarchy. ´ In Alvez et al. (2017), we propose a method for the automatic creation of competency questions (CQs) (Gr¨uninger and Fox, 1995), which enable to evaluate the competency of SUMO-based ontologies. Our proposal is based on several predefined question patterns (QPs) that are instantiated using German Rigau IXA Group University of the Basque Country UPV/EHU german.rigau@ehu.eus information from WordNet (Fellbaum,"
2018.gwc-1.4,C08-1092,0,0.605702,"n the basis of few question patterns. From our preliminary evaluation results, we report on some of the detected inconsistencies. 1 Introduction Despite being created manually, knowledge resources such as WordNet (Fellbaum, 1998) and SUMO (Niles and Pease, 2003) are not free of errors and inconsistencies. Unfortunatelly, improving, revising, and correcting such large knowledge bases is a never ending task that have been mainly carried out also manually. A few automatic approaches have been also applied focusing on checking certain structural properties on WordNet (e.g. (Daud´e et al., 2003), (Richens, 2008)) or using automated theorem provers on SUMO ´ (e.g. (Horrocks and Voronkov, 2006), (Alvez et al., 2012)). Just a few more have studied automatic ways to validate the knowledge content encoded in these resources by cross-checking them. For in´ stance, Alvez et al. (2008) exploit the EuroWordNet Top Ontology (Rodr´ıguez et al., 1998) and its mapping to WordNet for detecting many ontological conflicts and inconsistencies in the WordNet nominal hierarchy. ´ In Alvez et al. (2017), we propose a method for the automatic creation of competency questions (CQs) (Gr¨uninger and Fox, 1995), which enable"
2020.lrec-1.171,C18-1139,0,0.0567942,"s across languages were not particularly meaningful. In this section we will summarize the setup for the experiments performed on both datasets, TW-10 and our new Catalonia Independence Corpus. Apart from the data pre-processing described in Section 3.1., we experimented with four different system architectures: (i) TF-IDF vectorization with a SVM classifier; (ii) a SVM trained with fastText word embeddings (Grave et al., 2018) for the representation of tweets; (iii) the fastText text classification system (Joulin et al., 2017) with fastText word embeddings and, finally (iv) the Flair system (Akbik et al., 2018), which implements a Recurrent Neural Network (RNN) for text classification that can be combined with static and context-based string embeddings. In the following, we describe the pre-processing and each of the architectures tested in both the TW-10 and the Catalonia Independence Corpus (CIC). 3.1. Data Pre-processing Since each tweet in the TW-1O dataset is given in context, with the previous and the next tweet, we use them to obtain longer and richer texts for classification. Normalization: We believe that normalization helps to reduce the number of features for TF-IDF feature representation"
2020.lrec-1.171,N19-4010,0,0.0173184,"l.org/ 3.5. Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. While fastText generates static word embeddings, generating a unique vector-based representation for a given word independently of the context, contextual word embeddings aimed to generate different word representations depending on the context in which the word occurs. Examples of such contextual representations are ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018), which are built upon LSTMbased architectures and trained as language models. The Flair toolkit (Akbik et al., 2019) allows to train sequence labelling and text classification models based on neural networks. Flair provides a common interface to use and combine different word embeddings, including both Flair and fastText embeddings. For text classification the computed word embeddings are fed into a BiLSTM to produce a document level embedding which is then used in a linear layer to make the class prediction. For best results, we follow their advice of combining in a stack the contextual Flair embeddings for Spanish with the fastText embeddings (Akbik et al., 2018). Every result reported with Flair is the a"
2020.lrec-1.171,N18-2004,0,0.0597585,"Missing"
2020.lrec-1.171,S16-1072,0,0.0631423,"Missing"
2020.lrec-1.171,S17-2006,0,0.0177451,"s that agree, disagree or discuss the given document and, (ii) based on the previous step, it would be possible to build a “truth-labeling” system based on the weighted credibility of the various news organizations from which the stance has been retrieved. Automatic stance detection has been defined as the task of classifying the attitude expressed in a text towards a given target or claim. Most of the work on stance detection has been undertaken in English using the data provided by the Detecting Stance in Tweets shared task organized at SemEval 2016 (Mohammad et al., 2016), RumourEval 2017 (Derczynski et al., 2017) and the Fake News Challenge. The SemEval 2016 task was formulated as follows: given a 1 https://www.nytimes.com/2016/12/06/us/ fake-news-partisan-republican-democrat.html 2 http://www.fakenewschallenge.org/ tweet text and a target entity or topic, automatic natural language systems must determine whether the tweet expresses a stance in favor of the given target, against the given target, or whether none of those inferences are likely. For example, consider the following target−tweet pairs: Tweet: I still remember the days when I prayed God for strength.. then suddenly God gave me difficulties"
2020.lrec-1.171,L18-1550,0,0.144773,"ated by the experiments performed on the IberEval TW-10 data. The result of those experiments showed that, due to the highly imbalanced nature of the TW-10 corpus, any comparison of systems across languages were not particularly meaningful. In this section we will summarize the setup for the experiments performed on both datasets, TW-10 and our new Catalonia Independence Corpus. Apart from the data pre-processing described in Section 3.1., we experimented with four different system architectures: (i) TF-IDF vectorization with a SVM classifier; (ii) a SVM trained with fastText word embeddings (Grave et al., 2018) for the representation of tweets; (iii) the fastText text classification system (Joulin et al., 2017) with fastText word embeddings and, finally (iv) the Flair system (Akbik et al., 2018), which implements a Recurrent Neural Network (RNN) for text classification that can be combined with static and context-based string embeddings. In the following, we describe the pre-processing and each of the architectures tested in both the TW-10 and the Catalonia Independence Corpus (CIC). 3.1. Data Pre-processing Since each tweet in the TW-1O dataset is given in context, with the previous and the next tw"
2020.lrec-1.171,E17-2068,0,0.683042,"that, due to the highly imbalanced nature of the TW-10 corpus, any comparison of systems across languages were not particularly meaningful. In this section we will summarize the setup for the experiments performed on both datasets, TW-10 and our new Catalonia Independence Corpus. Apart from the data pre-processing described in Section 3.1., we experimented with four different system architectures: (i) TF-IDF vectorization with a SVM classifier; (ii) a SVM trained with fastText word embeddings (Grave et al., 2018) for the representation of tweets; (iii) the fastText text classification system (Joulin et al., 2017) with fastText word embeddings and, finally (iv) the Flair system (Akbik et al., 2018), which implements a Recurrent Neural Network (RNN) for text classification that can be combined with static and context-based string embeddings. In the following, we describe the pre-processing and each of the architectures tested in both the TW-10 and the Catalonia Independence Corpus (CIC). 3.1. Data Pre-processing Since each tweet in the TW-1O dataset is given in context, with the previous and the next tweet, we use them to obtain longer and richer texts for classification. Normalization: We believe that"
2020.lrec-1.171,P16-1089,0,0.0423352,"Missing"
2020.lrec-1.171,S16-1003,0,0.462821,"the top documents from other news sources that agree, disagree or discuss the given document and, (ii) based on the previous step, it would be possible to build a “truth-labeling” system based on the weighted credibility of the various news organizations from which the stance has been retrieved. Automatic stance detection has been defined as the task of classifying the attitude expressed in a text towards a given target or claim. Most of the work on stance detection has been undertaken in English using the data provided by the Detecting Stance in Tweets shared task organized at SemEval 2016 (Mohammad et al., 2016), RumourEval 2017 (Derczynski et al., 2017) and the Fake News Challenge. The SemEval 2016 task was formulated as follows: given a 1 https://www.nytimes.com/2016/12/06/us/ fake-news-partisan-republican-democrat.html 2 http://www.fakenewschallenge.org/ tweet text and a target entity or topic, automatic natural language systems must determine whether the tweet expresses a stance in favor of the given target, against the given target, or whether none of those inferences are likely. For example, consider the following target−tweet pairs: Tweet: I still remember the days when I prayed God for streng"
2020.lrec-1.171,D19-1452,0,0.0147949,"her interesting work is that of (Rajadesingan and Liu, 2014) who tried to determine stance at user level. Their 3 https://github.com/ixa-ehu/ catalonia-independence-corpus assumption was that if many users retweeted a particular pair of tweets in a short time, then it is likely that this pair of tweets had something in common and share the same opinion on the topic. As far as we know, most approaches to stance detection are developed for English, with the few exceptions that use the Catalan and Spanish data from IberEval 2017 and 2018 (Taul´e et al., 2017; Taul´e et al., 2018) or the work of (Mohtarami et al., 2019) using the Arabic corpus provided by (Baly et al., 2018). With respect to the “MultiModal Stance Detection in tweets on Catalan #1Oct Referendum” task at IberEval 2018 (MultiStanceCat), the best results for Spanish were obtained by the uc3m team (Segura-Bedmar, 2018). They presented a system based on bag-of-words with TF-IDF vectorization. They evaluated several of the most commonly used classifiers, obtaining a final 28.02 F1 macro score in the Spanish test data. The best result in Catalan subset was obtained by the CriCa team (Cuquerella and Rodr´ıguez, 2018). Their approach consisted of com"
2020.lrec-1.171,N18-1202,0,0.0161081,"as additional features with the aim of capturing word order information. 4 https://github.com/michmech/ lemmatization-lists 5 1370 http://commoncrawl.org/ 3.5. Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. While fastText generates static word embeddings, generating a unique vector-based representation for a given word independently of the context, contextual word embeddings aimed to generate different word representations depending on the context in which the word occurs. Examples of such contextual representations are ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018), which are built upon LSTMbased architectures and trained as language models. The Flair toolkit (Akbik et al., 2019) allows to train sequence labelling and text classification models based on neural networks. Flair provides a common interface to use and combine different word embeddings, including both Flair and fastText embeddings. For text classification the computed word embeddings are fed into a BiLSTM to produce a document level embedding which is then used in a linear layer to make the class prediction. For best results, we follow their advice of combining"
2020.lrec-1.171,C18-1203,0,0.0852159,"a Convolutional Neural Network (CNN) architecture combined with a voting scheme to guide the predictions instead of generating them based on the accuracy obtained in the validation set. The MITRE team (Zarrella and Marsh, 2016) employed two recurrent RNN classifiers: the first was trained to predict taskrelevant hashtags on a large unlabeled Twitter corpus which was then used to initialize a second RNN to be trained on the SemEval 2016 training set. (Du et al., 2017) proposed a neural network-based model to incorporate target-specific information by means of an attention mechanism. Finally, (Sun et al., 2018) proposed a hierarchical attention network to weigh the importance of various linguistic information, and learn the mutual attention between the document and the linguistic information. It should be said that neural network approaches have been more successful so far for the SemeEval 2016 Task B (weakly-supervised setting). Apart from the previously mentioned systems (Wei et al., 2016), (Augenstein et al., 2016) proposed a bidirectional Long-Short Term Memory (LSTM) encoding model. First, the target is encoded by a LSTM network and then a second LSTM is used to encode the tweet using the encod"
2020.lrec-1.171,S16-1062,0,0.0610681,"ity of results3 . 2. Related Work The state of the art is divided into two main approaches. First, those that rely on traditional machine learning models combined with hand-engineered features (Mohammad et al., 2017) or vector-based word representations (word embeddings) (Bøhler et al., 2016). In particular, (Mohammad et al., 2017) obtained the best results for the supervised setting of the SemEval 2016 task using a SVM classifier to learn word n-grams (1-, 2-, and 3-gram) and character ngrams (2-, 3-, 4-, and 5-gram) features, outperforming deep learning approaches (Zarrella and Marsh, 2016; Wei et al., 2016). Among the deep learning systems published, the pkudblab system (Wei et al., 2016) proposed a Convolutional Neural Network (CNN) architecture combined with a voting scheme to guide the predictions instead of generating them based on the accuracy obtained in the validation set. The MITRE team (Zarrella and Marsh, 2016) employed two recurrent RNN classifiers: the first was trained to predict taskrelevant hashtags on a large unlabeled Twitter corpus which was then used to initialize a second RNN to be trained on the SemEval 2016 training set. (Du et al., 2017) proposed a neural network-based mod"
2020.lrec-1.171,S16-1074,0,0.0891102,"research and reproducibility of results3 . 2. Related Work The state of the art is divided into two main approaches. First, those that rely on traditional machine learning models combined with hand-engineered features (Mohammad et al., 2017) or vector-based word representations (word embeddings) (Bøhler et al., 2016). In particular, (Mohammad et al., 2017) obtained the best results for the supervised setting of the SemEval 2016 task using a SVM classifier to learn word n-grams (1-, 2-, and 3-gram) and character ngrams (2-, 3-, 4-, and 5-gram) features, outperforming deep learning approaches (Zarrella and Marsh, 2016; Wei et al., 2016). Among the deep learning systems published, the pkudblab system (Wei et al., 2016) proposed a Convolutional Neural Network (CNN) architecture combined with a voting scheme to guide the predictions instead of generating them based on the accuracy obtained in the validation set. The MITRE team (Zarrella and Marsh, 2016) employed two recurrent RNN classifiers: the first was trained to predict taskrelevant hashtags on a large unlabeled Twitter corpus which was then used to initialize a second RNN to be trained on the SemEval 2016 training set. (Du et al., 2017) proposed a neura"
2020.lrec-1.708,J92-4003,0,0.135918,"ivos (negative, pl.) retirada de (withdrawal of ) niega (denies) suspender (withhold) 123 99 96 83 59 batch size (16), the learning rate (00.5) and learning rate decay (00.1). Several groups of input features at token level have been tested, namely: • form: affixes of 2 and 3 characters, and whether the token is a punctuation mark, a number or an alphabetic string. • morphsyn: the token’s lemma, its part-of-speech tag, the type of dependency relation, and the lemmas of the dependent children to the right and left, all extracted with spaCy’s es-core-news-md 2.2.0 model. • brown: Brown cluster (Brown et al., 1992) complete paths and paths pruned at lengths 16, 32, and 64. The clusters were learned with tan-clustering10 from the training set and the 11,278 sentences left out from the dataset split. • metadata: the speciality and section the sentence has been extracted from. • window: all the features of the neighbouring tokens in a ± 2 window. morphological negation afebril (afrebile) asintom´atico (asymptomatic, m.) asintom´atica (asymptomatic, f.) inespec´ıfico (non-specific) asintomatico (sic) 252 241 150 39 34 syntactic uncertainty vs o (or) versus vs. 13 4 1 1 lexical uncertainty probable posible ("
2020.lrec-1.708,W17-1808,0,0.117944,"Missing"
2020.lrec-1.708,C18-1078,0,0.0466125,"Missing"
2020.lrec-1.708,J12-2001,0,0.066218,"tion in Spanish and the first that also incorporates the annotation of speculation cues, scopes, and events. Keywords: negation, uncertainty, clinical texts, Spanish 1. Introduction The aim of Natural Language Understanding is to capture the intended meaning of texts or utterances. However, until recently, research has predominantly focused only on propositional aspects of meaning. Truly understanding language involves taking into account many linguistic aspects which are usually overlooked. These linguistic phenomena are sometimes referred to as Extra-Propositional Aspects of Meaning (EPAM) (Morante and Sporleder, 2012). Some examples of EPAM include factuality, uncertainty, opinions, beliefs, intentions or subjectivity. Documents enriched with this kind of information can be of utmost importance. For instance, in a domain such as the biomedical, the implicit meaning of a sentence can be crucial to differentiate whether a patient suffers from a disease or not, or whether they should be taking or not a given drug. One way to learn these nuances is through means of an annotated corpus. Unfortunately, there are not many corpora that cover these phenomena. Just a few consider negation, a key aspect of factuality"
2020.lrec-1.708,E12-2021,0,0.137924,"Missing"
2020.lrec-1.708,W08-0606,0,0.0432031,"Missing"
2020.lrec-1.708,C14-1174,0,0.0459638,"Missing"
2020.lrec-1.708,P18-4013,0,0.0356544,"Missing"
2020.lrec-1.708,W17-1807,0,\N,Missing
2020.mmw-1.1,peters-peters-2000-treatment,0,0.253191,"g, 1997; Henrich and Hinrichs, 2010) the cluster-approach is not followed: adjectives are hierarchically structured, as in the case of nouns and verbs, and, thus, the relation of indirect antonyms is eliminated. Moreover, adjectives are categorised into different semantic classes such as perceptional, spatial, or weatherrelated.2 Building on GermaNet adjectival classification, Tsvetkov et al. (2014) propose supersense (high-level semantic classes) taxonomy for English adjectives. They distinguish 11 classes such as motion, substance or weather3 . Regarding the ontologies, the SIMPLE Ontology (Peters and Peters, 2000) distinguishes the adjectives according to their predicative function: intensional adjectives and extensional adjectives. Intensional adjectives have the following subclasses: temporal, modal, emotive, manner, objectrelated, and emphasizer. The subclasses of the extensional adjectives are: psychological property, social property, physical property, temporal property, intensifying property, and relational property. The DOLCE family of ontologies relates qualities as individuals to regions, that belong to quality spaces (Gangemi et al., 2016) e.g. hasQuality(AmazonRiver,wide). The Suggested Uppe"
2020.mmw-1.1,tsvetkov-etal-2014-augmenting-english,0,0.013525,"es are regarded as a special case (Fellbaum et al., 1993). Furthermore, in the morphosemantic links (Fellbaum et al., 2007) adjectives are related to their derived/derivative nouns and verbs. In GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) the cluster-approach is not followed: adjectives are hierarchically structured, as in the case of nouns and verbs, and, thus, the relation of indirect antonyms is eliminated. Moreover, adjectives are categorised into different semantic classes such as perceptional, spatial, or weatherrelated.2 Building on GermaNet adjectival classification, Tsvetkov et al. (2014) propose supersense (high-level semantic classes) taxonomy for English adjectives. They distinguish 11 classes such as motion, substance or weather3 . Regarding the ontologies, the SIMPLE Ontology (Peters and Peters, 2000) distinguishes the adjectives according to their predicative function: intensional adjectives and extensional adjectives. Intensional adjectives have the following subclasses: temporal, modal, emotive, manner, objectrelated, and emphasizer. The subclasses of the extensional adjectives are: psychological property, social property, physical property, temporal property, intensif"
2020.mmw-1.1,agerri-garcia-serrano-2010-q,0,0.0305882,"Missing"
2020.mmw-1.1,2019.gwc-1.25,1,0.820081,"out properties and attributes in SUMO by exploiting the information encoded in WordNet adjectives and its mapping to SUMO. To that end, we considered clusters of semantically related groups of WordNet adjectival and nominal synsets. Based on these clusters, we propose a new semi-automatic model for SUMO attributes and their mapping to WordNet, which also includes polarity information. In this paper, as an exploratory approach, we focus on qualities. Keywords: Adjectives, WordNet, SUMO, Commonsense Reasoning 1. Introduction and Pease, 2003) lacks of an accurate and complete charac´ terization (Alvez et al., 2019a). For instance, many WordNet adjectives have been mapped to SUMO processes instead to SUMO attributes. A proper characterization of this type of knowledge is required to perform formal commonsense reasoning based on the attributes encoded in SUMO, for example, if we want to distinguish one concept from another based on their properties. In this framework, two main problems arise when reasoning with the SUMO knowledge related to WordNet adjectives and their antonymy relations. The first one is related to the SUMO mapping and the second one is related to an incomplete axiomatization. Regarding"
2020.mmw-1.1,W97-0800,0,0.51389,"um, 1998) are divided into descriptive and relational adjectives. The basic relation between descriptive adjectives is antonymy (direct or indirect). Moreover, by similarity they are linked to semantically comparable adjectives, which are called satellites. This way, bipolar cluster are formed as the one presented in Figure 1. Relational adjectives are also related to nouns and color adjectives are regarded as a special case (Fellbaum et al., 1993). Furthermore, in the morphosemantic links (Fellbaum et al., 2007) adjectives are related to their derived/derivative nouns and verbs. In GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) the cluster-approach is not followed: adjectives are hierarchically structured, as in the case of nouns and verbs, and, thus, the relation of indirect antonyms is eliminated. Moreover, adjectives are categorised into different semantic classes such as perceptional, spatial, or weatherrelated.2 Building on GermaNet adjectival classification, Tsvetkov et al. (2014) propose supersense (high-level semantic classes) taxonomy for English adjectives. They distinguish 11 classes such as motion, substance or weather3 . Regarding the ontologies, the SIMPLE Ontology (Peters"
2020.mmw-1.1,henrich-hinrichs-2010-gernedit,0,0.0213688,"to descriptive and relational adjectives. The basic relation between descriptive adjectives is antonymy (direct or indirect). Moreover, by similarity they are linked to semantically comparable adjectives, which are called satellites. This way, bipolar cluster are formed as the one presented in Figure 1. Relational adjectives are also related to nouns and color adjectives are regarded as a special case (Fellbaum et al., 1993). Furthermore, in the morphosemantic links (Fellbaum et al., 2007) adjectives are related to their derived/derivative nouns and verbs. In GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) the cluster-approach is not followed: adjectives are hierarchically structured, as in the case of nouns and verbs, and, thus, the relation of indirect antonyms is eliminated. Moreover, adjectives are categorised into different semantic classes such as perceptional, spatial, or weatherrelated.2 Building on GermaNet adjectival classification, Tsvetkov et al. (2014) propose supersense (high-level semantic classes) taxonomy for English adjectives. They distinguish 11 classes such as motion, substance or weather3 . Regarding the ontologies, the SIMPLE Ontology (Peters and Peters, 2000) distinguish"
2021.findings-emnlp.333,N13-1092,0,0.0909879,"Missing"
2021.findings-emnlp.333,L16-1046,0,0.0191789,"nal meta-embedding. An ablation study confirms that these steps increase the performance of the generated meta-embeddings in both intrinsic and extrinsic tasks. Another recent research line tries to dynamically generate meta-embeddings for specific tasks (He et al., 2020; Kiela et al., 2018; O’Neill and Bollegala, 2020). These methods extend already existing algorithms to generate meta-embeddings by learning task specific weights. Instead, the focus of our research is to generate the best general purpose meta-embedding that can be applied to any task. such as SVD (Yin and Schütze, 2016), PCA (Ghannay et al., 2016) or DRA (Raunak, 2017). In this line of work, Numberbatch (Speer et al., 2017) 3 Evaluation Framework claims to be the best meta-embedding model so far, by combining knowledge from a variety of As it has been earlier mentioned, several methods embeddings obtained from different corpora and to generate meta-embeddings have been previously knowledge bases such as ConceptNet. proposed and evaluated on many different benchMethods such as MUSE (Lample et al., 2018) marks, as shown by Table 1. Moreover, add-hoc and VecMap (Artetxe et al., 2018) project em- decisions (not always explicitly mentioned)"
2021.findings-emnlp.333,W07-1401,0,0.159454,"/kudkudak/wordembeddings-benchmarks Word Corpus 4 . Jiant5 provides a framework for extrinsic evaluation of word representations using GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a). We use the same bag-of-words configuration used in the GLUE leaderboard for the Cbow baseline 6 and we evaluate the embeddings in all GLUE tasks (CoLa (Warstadt et al., 2019) , SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017), QQP 7 , MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016; Wang et al., 2019b), RTE (Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), AX (Wang et al., 2019b)). 4 Our Method Our meta-embedding generation approach consists of two main steps: (i) pre-processing of the source embeddings and (ii) generation of the metaembedding by averaging. Our method can combine any number of word embeddings as long as there is some common vocabulary shared between them. The resulting meta-embedding vocabulary will be the union of the vocabularies of the source word embeddings used. 4.1 Word embeddings pre-processing Word embeddings generated with different sources or techniques can resu"
2021.findings-emnlp.333,N15-1165,0,0.041135,"Missing"
2021.findings-emnlp.333,2020.coling-main.106,0,0.129289,"ui et al., 2015) at the cost of increasing the dimensionality of the meta-embeddings. Coates and Bollegala (2018) prove that averaging is in some cases better than concatenation, with the additional benefit of a reduced dimensionality. The most popular approach to address the dimensionality problem is to apply dimensionality reduction algorithms 4. We report the largest meta-embedding extrinsic evaluation performed so far showing that 1 meta-embedding performance in these tasks https://github.com/ikergarcia1996/ has been overestimated by previous work. MetaVec 3958 Paper (Kiela et al., 2018) (He et al., 2020) (Bollegala and Bao, 2018) Intrinsic Tasks (O’Neill and Bollegala, 2020) (Jawanpuria et al., 2020) (Doval et al., 2018) Sim. (6), An. (3) Sim. (6), An. (2) Sim. (4), Bilingual dictionary induction (4), hypernym discovery (1) Sim. (6), An. (2), Relation Classification (1) Sim. (5), An. (1) Sim. (5), An. (1) (Bollegala et al., 2018) (Coates and Bollegala, 2018) (Yin and Schütze, 2016) (Li et al., 2020) (Chen et al., 2020) (Goikoetxea et al., 2016) (Speer et al., 2017) (García-Ferrero et al., 2020) This work Sim. (4), An. (3), Relation Classification (1) Sim. Sim. Sim. Sim. Sim. (5) (4) (5). SAT"
2021.findings-emnlp.333,J15-4004,0,0.124005,"Missing"
2021.findings-emnlp.333,2020.findings-emnlp.147,0,0.144304,"rd similarity (Collobert and larity (García-Ferrero et al., 2020). Furthermore, Weston, 2008; Turian et al., 2010; Socher et al., different evaluation methodologies have been ap2011), Semantic Textual Similarity (Shao, 2017), plied. For example, Yin and Schütze (2016) disor more recently, unsupervised machine translation card the words in the datasets which are not repre(Artetxe et al., 2019), inferring representations for sented in the meta-embedding model, while Speer rare words (Schick and Schütze, 2020), unsuper- and Lowry-Duda (2017) use various strategies to vised word alignment (Jalili Sabet et al., 2020) or minimize the number of out-of-vocabulary (OOV) knowledge base probes (Dufter et al., 2021). In words. To make things more complicated, previous these tasks, word embeddings perform similarly meta-embeddings approaches require some ad-hoc or better than transformer-based language models pre-processing to tune multiple filtering criteria and such as BERT (Devlin et al., 2019), while requir- parameters according to the source embeddings ing a comparatively tiny amount of resources for used (Bollegala et al., 2018; Bollegala and Bao, training and inference. 2018; Yin and Schütze, 2016), which"
2021.findings-emnlp.333,2020.repl4nlp-1.6,0,0.0240547,"ommon space by means of a bilingual dictionary results. Let us consider, for example, the problem (Mikolov et al., 2013b). This requires minimal of out-of-vocabulary (OOV) words. bilingual supervision while still leveraging large Two popular techniques are used to address OOV amounts of monolingual corpora with very com- words. Table 2 shows the accuracy of FastText empetitive results (Artetxe et al., 2016, 2018). These beddings 2 in the Google Analogy dataset using the techniques are used by Doval et al. (2018); García- two approaches. The first one uses the average of Ferrero et al. (2020); Jawanpuria et al. (2020); He all the embeddings as a representation for unknown et al. (2020) to generate meta-embeddings. This words (With OOV). The second approach simply usually involves mapping all the source embed- removes from the dataset the examples containing dings to a common vector space followed by aver- unknown words (Without OOV). Additionally, the aging. We extend this idea by proposing a multiple dataset is usually pre-processed. A common apstep algorithm that: (i) normalizes the source em- proach lowercase all the words and removes non beddings; (ii) maps them to the same vector space; 2 (iii) handle"
2021.findings-emnlp.333,S12-1047,0,0.0996581,"Missing"
2021.findings-emnlp.333,D18-1176,0,0.0612146,"s retrofitting (Faruqui et al., 2015) at the cost of increasing the dimensionality of the meta-embeddings. Coates and Bollegala (2018) prove that averaging is in some cases better than concatenation, with the additional benefit of a reduced dimensionality. The most popular approach to address the dimensionality problem is to apply dimensionality reduction algorithms 4. We report the largest meta-embedding extrinsic evaluation performed so far showing that 1 meta-embedding performance in these tasks https://github.com/ikergarcia1996/ has been overestimated by previous work. MetaVec 3958 Paper (Kiela et al., 2018) (He et al., 2020) (Bollegala and Bao, 2018) Intrinsic Tasks (O’Neill and Bollegala, 2020) (Jawanpuria et al., 2020) (Doval et al., 2018) Sim. (6), An. (3) Sim. (6), An. (2) Sim. (4), Bilingual dictionary induction (4), hypernym discovery (1) Sim. (6), An. (2), Relation Classification (1) Sim. (5), An. (1) Sim. (5), An. (1) (Bollegala et al., 2018) (Coates and Bollegala, 2018) (Yin and Schütze, 2016) (Li et al., 2020) (Chen et al., 2020) (Goikoetxea et al., 2016) (Speer et al., 2017) (García-Ferrero et al., 2020) This work Sim. (4), An. (3), Relation Classification (1) Sim. Sim. Sim. Sim. Sim."
2021.findings-emnlp.333,N16-1175,0,0.0299048,"Missing"
2021.findings-emnlp.333,L18-1008,0,0.53927,"valuating meta-embeddings, overestimated. there is no consensus on either evaluation tasks or methodology. Meta-embeddings are evaluated in a 1 Introduction wide range of tasks (Schnabel et al., 2015; Bakarov, Word embeddings successfully capture lexical se- 2018), ranging from intrinsic (i.e. word similarmantic information about words based on co- ity, word analogy) to extrinsic tasks such as short occurrence patterns extracted from large corpora text classification (Bollegala and Bao, 2018; Bol(Mikolov et al., 2013a; Pennington et al., 2014; legala et al., 2018), common-sense stories (Speer Mikolov et al., 2018) or knowledge bases (Bor- et al., 2017), Named Entity Recognition (O’Neill des et al., 2011), with excellent results on sev- and Bollegala, 2020) or Semantic Textual Simieral tasks, including word similarity (Collobert and larity (García-Ferrero et al., 2020). Furthermore, Weston, 2008; Turian et al., 2010; Socher et al., different evaluation methodologies have been ap2011), Semantic Textual Similarity (Shao, 2017), plied. For example, Yin and Schütze (2016) disor more recently, unsupervised machine translation card the words in the datasets which are not repre(Artetxe et al., 2019), inferring"
2021.findings-emnlp.333,N13-1090,0,0.383592,"us to conclude that previous extrinsic evaluations of meta-embeddings have been When it comes to evaluating meta-embeddings, overestimated. there is no consensus on either evaluation tasks or methodology. Meta-embeddings are evaluated in a 1 Introduction wide range of tasks (Schnabel et al., 2015; Bakarov, Word embeddings successfully capture lexical se- 2018), ranging from intrinsic (i.e. word similarmantic information about words based on co- ity, word analogy) to extrinsic tasks such as short occurrence patterns extracted from large corpora text classification (Bollegala and Bao, 2018; Bol(Mikolov et al., 2013a; Pennington et al., 2014; legala et al., 2018), common-sense stories (Speer Mikolov et al., 2018) or knowledge bases (Bor- et al., 2017), Named Entity Recognition (O’Neill des et al., 2011), with excellent results on sev- and Bollegala, 2020) or Semantic Textual Simieral tasks, including word similarity (Collobert and larity (García-Ferrero et al., 2020). Furthermore, Weston, 2008; Turian et al., 2010; Socher et al., different evaluation methodologies have been ap2011), Semantic Textual Similarity (Shao, 2017), plied. For example, Yin and Schütze (2016) disor more recently, unsupervised mach"
2021.findings-emnlp.333,Q17-1022,0,0.0499911,"Missing"
2021.findings-emnlp.333,D14-1162,0,0.104942,"vious extrinsic evaluations of meta-embeddings have been When it comes to evaluating meta-embeddings, overestimated. there is no consensus on either evaluation tasks or methodology. Meta-embeddings are evaluated in a 1 Introduction wide range of tasks (Schnabel et al., 2015; Bakarov, Word embeddings successfully capture lexical se- 2018), ranging from intrinsic (i.e. word similarmantic information about words based on co- ity, word analogy) to extrinsic tasks such as short occurrence patterns extracted from large corpora text classification (Bollegala and Bao, 2018; Bol(Mikolov et al., 2013a; Pennington et al., 2014; legala et al., 2018), common-sense stories (Speer Mikolov et al., 2018) or knowledge bases (Bor- et al., 2017), Named Entity Recognition (O’Neill des et al., 2011), with excellent results on sev- and Bollegala, 2020) or Semantic Textual Simieral tasks, including word similarity (Collobert and larity (García-Ferrero et al., 2020). Furthermore, Weston, 2008; Turian et al., 2010; Socher et al., different evaluation methodologies have been ap2011), Semantic Textual Similarity (Shao, 2017), plied. For example, Yin and Schütze (2016) disor more recently, unsupervised machine translation card the w"
2021.findings-emnlp.333,D11-1014,0,0.187161,"Missing"
2021.findings-emnlp.333,D18-1169,0,0.0403181,"Missing"
2021.findings-emnlp.333,D13-1170,0,0.00514298,"ion regardless of the number of words in the vocabulary, we trim the vocabulary of all the embeddings and meta-embeddings to the 200,000 most popular English words according to the Google’s Trillion 3 https://github.com/kudkudak/wordembeddings-benchmarks Word Corpus 4 . Jiant5 provides a framework for extrinsic evaluation of word representations using GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a). We use the same bag-of-words configuration used in the GLUE leaderboard for the Cbow baseline 6 and we evaluate the embeddings in all GLUE tasks (CoLa (Warstadt et al., 2019) , SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017), QQP 7 , MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016; Wang et al., 2019b), RTE (Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), AX (Wang et al., 2019b)). 4 Our Method Our meta-embedding generation approach consists of two main steps: (i) pre-processing of the source embeddings and (ii) generation of the metaembedding by averaging. Our method can combine any number of word embeddings as long as there is some common vocabulary shared between the"
2021.findings-emnlp.333,D16-1264,0,0.00940224,"he 200,000 most popular English words according to the Google’s Trillion 3 https://github.com/kudkudak/wordembeddings-benchmarks Word Corpus 4 . Jiant5 provides a framework for extrinsic evaluation of word representations using GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a). We use the same bag-of-words configuration used in the GLUE leaderboard for the Cbow baseline 6 and we evaluate the embeddings in all GLUE tasks (CoLa (Warstadt et al., 2019) , SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017), QQP 7 , MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016; Wang et al., 2019b), RTE (Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), AX (Wang et al., 2019b)). 4 Our Method Our meta-embedding generation approach consists of two main steps: (i) pre-processing of the source embeddings and (ii) generation of the metaembedding by averaging. Our method can combine any number of word embeddings as long as there is some common vocabulary shared between them. The resulting meta-embedding vocabulary will be the union of the vocabularies of the source word embeddings used. 4.1 Word em"
2021.findings-emnlp.333,S17-2008,0,0.0209966,"Missing"
2021.findings-emnlp.333,P16-2068,0,0.0203725,"017) combines knowledge encoded in Contively combines multiple source embeddings resultceptNet, Word2vec, GloVe and OpenSubtitles 2016 ing in representations as good as the ones generated by concatenation without increasing their dimen- using concatenation, dimensionality reduction and a variation of retrofitting. Numberbatch version sionality. 19.08 is used. We also tested other embeddings such as ExtVec 8 For computation efficiency we limit the maximum k to (Komninos and Manandhar, 2016), LexSub (Arora 50. In our experiments the optimal k is usually smaller than 20. et al., 2020) or LexVec (Salle et al., 2016) but 3961 Embedding FT GV W2V J UKB P AR N AVG 67.8 64.7 59.1 52.2 46.6 58.5 48.5 68.1 C 71.4 69.9 67.9 70.0 67.9 66.5 59.7 73.6 WS 73.6 70.3 65.6 65.2 61.8 70.2 63.6 75.2 A 58.5 54.0 43.9 21.4 10.2 38.9 22.2 55.4 Table 3: Source embedding intrinsic evaluation results. Text WN PPDB CN Text 67.9 66.3 68.5 69.1 WN 66.3 50.9 62.5 65.4 PPDB 68.5 62.5 60.2 67.8 CN 69.1 65.4 67.8 - Table 4: Comparison of the average performance in the intrinsic evaluation tasks for meta-embeddings generated using pairs of embeddings that encode knowledge form the same or different sources. WN stands for WordNet and"
2021.findings-emnlp.333,2020.acl-main.368,0,0.12452,"al., 2011), with excellent results on sev- and Bollegala, 2020) or Semantic Textual Simieral tasks, including word similarity (Collobert and larity (García-Ferrero et al., 2020). Furthermore, Weston, 2008; Turian et al., 2010; Socher et al., different evaluation methodologies have been ap2011), Semantic Textual Similarity (Shao, 2017), plied. For example, Yin and Schütze (2016) disor more recently, unsupervised machine translation card the words in the datasets which are not repre(Artetxe et al., 2019), inferring representations for sented in the meta-embedding model, while Speer rare words (Schick and Schütze, 2020), unsuper- and Lowry-Duda (2017) use various strategies to vised word alignment (Jalili Sabet et al., 2020) or minimize the number of out-of-vocabulary (OOV) knowledge base probes (Dufter et al., 2021). In words. To make things more complicated, previous these tasks, word embeddings perform similarly meta-embeddings approaches require some ad-hoc or better than transformer-based language models pre-processing to tune multiple filtering criteria and such as BERT (Devlin et al., 2019), while requir- parameters according to the source embeddings ing a comparatively tiny amount of resources for us"
2021.findings-emnlp.333,D15-1036,0,0.0176979,"t much complex methods based on linerate meta-embeddings, outperforming previear transformations and supervised neural models ous work on a large number of intrinsic evaluhave also been proposed (Bollegala et al., 2018; ation benchmarks. Our evaluation framework Bollegala and Bao, 2018; Yin and Schütze, 2016). also allows us to conclude that previous extrinsic evaluations of meta-embeddings have been When it comes to evaluating meta-embeddings, overestimated. there is no consensus on either evaluation tasks or methodology. Meta-embeddings are evaluated in a 1 Introduction wide range of tasks (Schnabel et al., 2015; Bakarov, Word embeddings successfully capture lexical se- 2018), ranging from intrinsic (i.e. word similarmantic information about words based on co- ity, word analogy) to extrinsic tasks such as short occurrence patterns extracted from large corpora text classification (Bollegala and Bao, 2018; Bol(Mikolov et al., 2013a; Pennington et al., 2014; legala et al., 2018), common-sense stories (Speer Mikolov et al., 2018) or knowledge bases (Bor- et al., 2017), Named Entity Recognition (O’Neill des et al., 2011), with excellent results on sev- and Bollegala, 2020) or Semantic Textual Simieral tas"
2021.findings-emnlp.333,S17-2016,0,0.0175247,"cted from large corpora text classification (Bollegala and Bao, 2018; Bol(Mikolov et al., 2013a; Pennington et al., 2014; legala et al., 2018), common-sense stories (Speer Mikolov et al., 2018) or knowledge bases (Bor- et al., 2017), Named Entity Recognition (O’Neill des et al., 2011), with excellent results on sev- and Bollegala, 2020) or Semantic Textual Simieral tasks, including word similarity (Collobert and larity (García-Ferrero et al., 2020). Furthermore, Weston, 2008; Turian et al., 2010; Socher et al., different evaluation methodologies have been ap2011), Semantic Textual Similarity (Shao, 2017), plied. For example, Yin and Schütze (2016) disor more recently, unsupervised machine translation card the words in the datasets which are not repre(Artetxe et al., 2019), inferring representations for sented in the meta-embedding model, while Speer rare words (Schick and Schütze, 2020), unsuper- and Lowry-Duda (2017) use various strategies to vised word alignment (Jalili Sabet et al., 2020) or minimize the number of out-of-vocabulary (OOV) knowledge base probes (Dufter et al., 2021). In words. To make things more complicated, previous these tasks, word embeddings perform similarly meta-embed"
2021.findings-emnlp.333,Q19-1040,0,0.0121767,"n order to ensure a fair evaluation regardless of the number of words in the vocabulary, we trim the vocabulary of all the embeddings and meta-embeddings to the 200,000 most popular English words according to the Google’s Trillion 3 https://github.com/kudkudak/wordembeddings-benchmarks Word Corpus 4 . Jiant5 provides a framework for extrinsic evaluation of word representations using GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a). We use the same bag-of-words configuration used in the GLUE leaderboard for the Cbow baseline 6 and we evaluate the embeddings in all GLUE tasks (CoLa (Warstadt et al., 2019) , SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017), QQP 7 , MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016; Wang et al., 2019b), RTE (Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), AX (Wang et al., 2019b)). 4 Our Method Our meta-embedding generation approach consists of two main steps: (i) pre-processing of the source embeddings and (ii) generation of the metaembedding by averaging. Our method can combine any number of word embeddings as long as there is some common"
2021.findings-emnlp.333,Q15-1025,0,0.0160328,"ed itkevitch et al., 2013), Attract Repel (AR) (Mrkši´c average of the selected k nearest neighbours in et al., 2017) improves word embeddings by injecttheir corresponding spaces. We use the cosine sim- ing synonymy and antonym constraints extracted ilarity from the nearest neighbors in E1 to W as from monolingual and cross-lingual lexical reweights. Finally, the selected representation of the sources. We used the English vocabulary from OOV word in E2 is the one corresponding to the the four-lingual (English, German, Italian, Rusclosest candidate to W in E1. sian) vector space. Paragram (P) (Wieting et al., 2015) are pre-trained word vectors learned using word paraphrase pairs from PPDB using a modi4.2 Meta-embedding generation fication of the skip-gram objective function. The hyper parameters were tuned using the wordsimWe combine the harmonized source embeddings by averaging them. In our experiments we demon- 353 dataset. The word embeddings of the default strate that, thanks to the pre-processing steps de- model are initialized with Glove word vectors. Using ConceptNet; Numberbatch (N) (Speer scribed above, averaging source embeddings effecet al., 2017) combines knowledge encoded in Contively combi"
2021.findings-emnlp.333,N18-1101,0,0.011484,"dings and meta-embeddings to the 200,000 most popular English words according to the Google’s Trillion 3 https://github.com/kudkudak/wordembeddings-benchmarks Word Corpus 4 . Jiant5 provides a framework for extrinsic evaluation of word representations using GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a). We use the same bag-of-words configuration used in the GLUE leaderboard for the Cbow baseline 6 and we evaluate the embeddings in all GLUE tasks (CoLa (Warstadt et al., 2019) , SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017), QQP 7 , MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016; Wang et al., 2019b), RTE (Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), AX (Wang et al., 2019b)). 4 Our Method Our meta-embedding generation approach consists of two main steps: (i) pre-processing of the source embeddings and (ii) generation of the metaembedding by averaging. Our method can combine any number of word embeddings as long as there is some common vocabulary shared between them. The resulting meta-embedding vocabulary will be the union of the vocabularies of the source wor"
2021.findings-emnlp.333,P16-1128,0,0.352104,"ed to deal with the task. Cona unified common framework, including both intrinsic and extrinsic tasks, for a fair and catenation (Goikoetxea et al., 2016) and averaging objective meta-embeddings evaluation. Fur(Coates and Bollegala, 2018) are two very strong thermore, we present a new method to genbaselines, but much complex methods based on linerate meta-embeddings, outperforming previear transformations and supervised neural models ous work on a large number of intrinsic evaluhave also been proposed (Bollegala et al., 2018; ation benchmarks. Our evaluation framework Bollegala and Bao, 2018; Yin and Schütze, 2016). also allows us to conclude that previous extrinsic evaluations of meta-embeddings have been When it comes to evaluating meta-embeddings, overestimated. there is no consensus on either evaluation tasks or methodology. Meta-embeddings are evaluated in a 1 Introduction wide range of tasks (Schnabel et al., 2015; Bakarov, Word embeddings successfully capture lexical se- 2018), ranging from intrinsic (i.e. word similarmantic information about words based on co- ity, word analogy) to extrinsic tasks such as short occurrence patterns extracted from large corpora text classification (Bollegala and B"
2021.gwc-1.6,W04-2214,0,0.17451,"pic information with a subset of its synsets. This topic labeling is achieved through pointers from a source synset to a target synset representing the topic. WordNet uses 440 topics and the most frequent one is &lt;law, jurisprudence&gt;. In order to reduce the manual effort required, a few semi-automatic and fully automatic methods have been applied for associating domain labels to synsets. For instance, WordNet Domains2 (WND) is a lexical resource where synsets have been semi-automatically annotated with one or more domain labels from a set of 165 hierarchically organized domains (Magnini, 2000; Bentivogli et al., 2004). The uses of WND include the possibility to reduce the polysemy degree of the words, grouping those senses that belong to the same domain (Magnini et al., 2002). But the semiautomatic method used to develop this resource was far from being perfect. For instance, the noun synset &lt;diver, frogman, underwater diver&gt; defined as some-one who works underwater has domain history because it inherits from its hypernym &lt;explorer, adventurer&gt; also labelled with history. Moreover, many synsets have been labelled as factotum meaning that the synset cannot be labelled with a particular domain. WND also prov"
2021.gwc-1.6,D15-1075,0,0.114286,"Missing"
2021.gwc-1.6,E17-2036,0,0.0162552,"2.14 F1 score (A2T+ descriptors ). Finally, we also include the results of both the fine-tuned student versions which still obtain very competitive results while drastically reducing the inference time of the original models. Method Precision Recall F1 Distributional BabelDomains 84.0 81.7 59.8 68.7 69.9 74.6 A2T A2T(&gt; 0.05) A2T+ descriptors 81.62 83.20 92.14 81.62 81.03 92.14 81.62 82.10 92.14 A2TFT-small A2TFT-xlingual 91.42 90.58 91.42 90.58 91.42 90.58 Table 4: Micro-averaged precision, recall and F1 for each of the systems. Distributional (Camacho-Collados et al., 2016) and BabelDomains (Camacho-Collados and Navigli, 2017) measures are the ones reported by them. 5.6 Error analysis Figure 4 presents the confusion matrix of our best system. The matrix is row wise normalized due Figure 3: Precision/Recall trade-off of A2T system. Annotations indicates the probability thresholds. to the imbalance of the dataset label distribution. Looking at the figure there are 4 classes that are misleading. The ”Animals” domain is confused with the related domains ”Biology” and ”Food and drink”. For instance, this is the case of the synset &lt;diet&gt; with the definition the usual food and drink consumed by an organism (person or anim"
2021.gwc-1.6,2020.acl-main.747,0,0.0696126,"Missing"
2021.gwc-1.6,N19-1423,0,0.444691,"f supervision. Furthermore, the system is not restricted to use a particular set of domain labels. We exploit the knowledge encoded within different off-theshelf pre-trained Language Models and task formulations to infer the domain label of a particular WordNet definition. The proposed zero-shot system achieves a new state-of-theart on the English dataset used in the evaluation. 1 Introduction The whole Natural Language Processing (NLP) research area have been accelerated with the advent of the unsupervised pre-trained Language Models. First with ELMo (Peters et al., 2018) and then with BERT (Devlin et al., 2019) the paradigm of using pre-trained Language Models for finetuning on a particular NLP task has became the new standard approach, replacing the more traditional knowledge-based and fully supervised approaches. Currently, as the size of the corpus and models increase, the research community has observed that the Transfer Learning approach has the capacity to work without any or with a very small fine-tuning. Some examples of the strength of this approach are GPT-2 (Radford et al., 2019) or more recently GPT-3 (Brown et al., 2020) that shows the ability of these huge pre-trained Language Models t"
2021.gwc-1.6,gonzalez-agirre-etal-2012-proposal,1,0.704969,"senses that belong to the same domain (Magnini et al., 2002). But the semiautomatic method used to develop this resource was far from being perfect. For instance, the noun synset &lt;diver, frogman, underwater diver&gt; defined as some-one who works underwater has domain history because it inherits from its hypernym &lt;explorer, adventurer&gt; also labelled with history. Moreover, many synsets have been labelled as factotum meaning that the synset cannot be labelled with a particular domain. WND also provides mappings to WordNet Topics and also to Wikipedia categories. eXtended WordNet Domains3 (XWND) (Gonzalez-Agirre et al., 2012; Gonz´alez et al., 2012) applied a graph-based method to propagate the WND labels through the WordNet structure. 1 https://github.com/osainz59/ Ask2Transformers 2 http://wndomains.fbk.eu/ 3 https://adimen.si.ehu.es/web/XWND Domain information is also available in other lexical resources. For instance, IATE4 , a European Union inter-institutional terminology database. The domain labels of IATE are based on the Eurovoc thesaurus5 and were introduced manually. More recently, BabelDomains6 (CamachoCollados and Navigli, 2017) propose an automatic method that propagates the knowledge categories fro"
2021.gwc-1.6,2021.ccl-1.108,0,0.0496846,"Missing"
2021.gwc-1.6,magnini-cavaglia-2000-integrating,0,0.509457,"s associated topic information with a subset of its synsets. This topic labeling is achieved through pointers from a source synset to a target synset representing the topic. WordNet uses 440 topics and the most frequent one is &lt;law, jurisprudence&gt;. In order to reduce the manual effort required, a few semi-automatic and fully automatic methods have been applied for associating domain labels to synsets. For instance, WordNet Domains2 (WND) is a lexical resource where synsets have been semi-automatically annotated with one or more domain labels from a set of 165 hierarchically organized domains (Magnini, 2000; Bentivogli et al., 2004). The uses of WND include the possibility to reduce the polysemy degree of the words, grouping those senses that belong to the same domain (Magnini et al., 2002). But the semiautomatic method used to develop this resource was far from being perfect. For instance, the noun synset &lt;diver, frogman, underwater diver&gt; defined as some-one who works underwater has domain history because it inherits from its hypernym &lt;explorer, adventurer&gt; also labelled with history. Moreover, many synsets have been labelled as factotum meaning that the synset cannot be labelled with a partic"
2021.gwc-1.6,N18-1202,0,0.0391173,"Missing"
2021.gwc-1.6,D19-1412,0,0.0129051,"is case, we just use the predictions of the entailment class. The predictions of the contradiction and neutral are not used. As in the previous case, no matter if any of the s2 hypothesis entails the premise s1 or not, the most probable entailment should be the correct domain label. For example, consider again the example presented in Table 1. 4 Experimental setting This section describes our experimental setup. We introduce the pre-trained Language Models and the dataset used. For the case of the Language Models, we have tested BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and BART (Wang et al., 2019). For the dataset, we have used the one released by Camacho-Collados et al. (2016) based on WordNet. 4.1 Pretrained models All the Language Models have been obtained from the Huggingface Transformers library (Wolf et al., 2019). MLM For the objective we have used robertalarge and roberta-base checkpoints. These models have obtained state-of-the-art results on many NLP tasks and benchmarks. NSP For this objective we use the BERT models as they are the only ones trained on that objective. For the sake of comparing the performance of more than one model of each objective we have selected the bert"
2021.gwc-1.6,N18-1101,0,0.366071,"re s1 encodes a WordNet gloss as a context and s2 is formed by a template and a domain-label. In order to make the classification, we run as many times as domain labels and then apply a softmax over the positive class outputs. We hypothesize that, no matter if any of the s2 can really follow the given s1 , the most probable one should be the s2 formed by the correct label. For instance, recall the hospital example shown in Table 1. 3.3 Natural Language Inference In this case, we use a pre-trained LM that has been fine-tuned for a general inference task which is the Natural Language Inference (Williams et al., 2018a). Given two sentences in the form of a premise s1 and an hypothesis s2 , the NLI task consists on redicting whether the s1 entails or contradicts s2 or if the relation between both is neutral. We also used the input pattern shown in the previous NSP approach to adapt the NLI models to the domain labelling task. In this case, we just use the predictions of the entailment class. The predictions of the contradiction and neutral are not used. As in the previous case, no matter if any of the s2 hypothesis entails the premise s1 or not, the most probable entailment should be the correct domain lab"
2021.gwc-1.6,D19-1404,0,0.0179631,"ticlass problem, where the output of the model is a class probability distribution. In our zero-shot experiments we did not modify any of the pre-trained models. We just reformulate the domain labelling task to match with the LMs training objective. 3.1 3.2 Next Sentence Prediction Along with the MLM the Next Sentence Prediction (NSP) is the training objective used by the BERT models. Given a pair of sentences s1 and s2 , this objective predicts whether s1 is followed by s2 or not. To adapt the BERT objective to the domain labelling task, we propose the next strategy inspired in the work from Yin et al. (2019). We use the following input pattern: s1 : [context] s2 : Domain or topic about [domainlabel] Masked Language Modeling The Masked Language Modeling (MLM) is a pretraining objective followed by models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). This objective works as follows. Given a sequence of tokens s = [t1 , t2 , ..., tn ], the sequence is first perturbed by replacing some of the tokens t with an special token [MASK]. Then, the model is trained to recover the original sequence s given the modified sequence sˆ. This denoising objective can be seen as an evolution for"
A92-1044,P80-1026,0,0.0871808,"Missing"
agerri-etal-2014-ixa,A00-2018,0,\N,Missing
agerri-etal-2014-ixa,W96-0213,0,\N,Missing
agerri-etal-2014-ixa,H05-1100,0,\N,Missing
agerri-etal-2014-ixa,W02-2004,0,\N,Missing
agerri-etal-2014-ixa,N03-1033,0,\N,Missing
agerri-etal-2014-ixa,W03-0424,0,\N,Missing
agerri-etal-2014-ixa,W09-1119,0,\N,Missing
agerri-etal-2014-ixa,W02-1001,0,\N,Missing
agerri-etal-2014-ixa,W03-0434,0,\N,Missing
agerri-etal-2014-ixa,J03-4003,0,\N,Missing
agerri-etal-2014-ixa,P03-1054,0,\N,Missing
agerri-etal-2014-ixa,W11-1902,0,\N,Missing
agerri-etal-2014-ixa,J13-4004,0,\N,Missing
agerri-etal-2014-ixa,gimenez-marquez-2004-svmtool,0,\N,Missing
agerri-etal-2014-ixa,P02-1022,0,\N,Missing
agerri-etal-2014-ixa,taule-etal-2008-ancora,0,\N,Missing
agerri-etal-2014-ixa,padro-stanilovsky-2012-freeling,0,\N,Missing
agerri-etal-2014-ixa,D10-1048,0,\N,Missing
agerri-etal-2014-ixa,P05-1045,0,\N,Missing
agirre-etal-2010-exploring,agirre-soroa-2008-using,1,\N,Missing
agirre-etal-2010-exploring,agirre-de-lacalle-2004-publicly,1,\N,Missing
agirre-etal-2010-exploring,E09-1005,1,\N,Missing
agirre-etal-2010-exploring,H93-1061,0,\N,Missing
agirre-etal-2010-exploring,R09-1039,1,\N,Missing
agirre-etal-2010-exploring,J06-1003,0,\N,Missing
agirre-etal-2010-exploring,D07-1061,0,\N,Missing
agirre-etal-2010-exploring,N09-1003,1,\N,Missing
agirre-etal-2010-exploring,P06-1127,0,\N,Missing
alvez-etal-2008-complete,atserias-etal-2004-towards,1,\N,Missing
alvez-etal-2008-complete,L08-1000,0,\N,Missing
atserias-etal-2004-cross,habash-dorr-2002-handling,0,\N,Missing
atserias-etal-2004-cross,briscoe-carroll-2002-robust,1,\N,Missing
atserias-etal-2004-cross,H92-1045,0,\N,Missing
atserias-etal-2004-cross,magnini-cavaglia-2000-integrating,1,\N,Missing
atserias-etal-2004-cross,agirre-etal-2004-exploring,1,\N,Missing
atserias-etal-2004-spanish,P95-1026,0,\N,Missing
atserias-etal-2004-spanish,magnini-cavaglia-2000-integrating,0,\N,Missing
atserias-etal-2004-towards,W02-1304,1,\N,Missing
atserias-etal-2004-towards,P97-1007,1,\N,Missing
atserias-etal-2004-towards,magnini-cavaglia-2000-integrating,0,\N,Missing
C08-1021,agirre-de-lacalle-2004-publicly,0,0.750896,"Missing"
C08-1021,W01-0703,0,0.130537,"ical power” and by explicit semantic relations to other synsets. Fortunately, during the last years the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can men161 1 Symmetric relations are counted only once. Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 161–168 Manchester, August 2008 tion eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de Lacalle, 2004) or knowledge about individuals from Wikipedia (Suchanek et al., 2007). Obviously, all these semantic resources have been acquired using a very different methods, tools and corpora. As expected, each semantic resource has different volume and accuracy figures when evaluated in a common and controlled framework (Cuadros and Rigau, 2006). However, not all these large-scale resources encode semantic relations"
C08-1021,W06-1663,1,0.323557,"2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de Lacalle, 2004) or knowledge about individuals from Wikipedia (Suchanek et al., 2007). Obviously, all these semantic resources have been acquired using a very different methods, tools and corpora. As expected, each semantic resource has different volume and accuracy figures when evaluated in a common and controlled framework (Cuadros and Rigau, 2006). However, not all these large-scale resources encode semantic relations between synsets. In some cases, only relations between synsets and words have been acquired. This is the case of the Topic Signatures acquired from the web (Agirre and de Lacalle, 2004). This is one of the largest semantic resources ever built with around one hundred million relations between synsets and semantically related words 2 . A knowledge net or KnowNet (KN), is an extensible, large and accurate knowledge base, which has been derived by semantically disambiguating small portions of the Topic Signatures acquired fr"
C08-1021,S07-1015,1,0.812139,"Missing"
C08-1021,J98-1006,0,0.0524077,"y, the acquisition of TS consists of: • acquiring the best possible corpus examples for a particular word sense (usually characterizing each word sense as a query and performing a search on the corpus for those examples that best match the queries) • building the TS by selecting the context words that best represent the word sense from the selected corpora. The Topic Signatures acquired from the web (hereinafter TSWEB) constitutes one of the largest semantic resource available with around 100 million relations (between synsets and words) (Agirre and de Lacalle, 2004). Inspired by the work of (Leacock et al., 1998), TSWEB was constructed using monosemous relatives from WN (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. Thus, TSWEB consist of large ordered lists of words with weights associated to the polysemous nouns of WN1.6. The number of constructed topic signatures is 35,250 with an average size per signature of 6,877 words. 162 tammany#n federalist#n whig#n missionary#j Democratic#n nazi#j republican#n constitutional#n"
C08-1021,C00-1072,0,0.0706153,"w KnowNet-5 New KnowNet-10 New KnowNet-15 New KnowNet-20 #relations 235,402 203,546 550,922 932,008 231,163 689,610 1,378,286 2,358,927 Table 1: Number of synset relations 3 presents the approach we followed for building highly dense and accurate knowledge bases from the Topic Signatures. In section 4, we present the evaluation framework used in this study. Section 5 describes the results when evaluating different versions of KnowNet and finally, section 6 presents some concluding remarks and future work. 2 Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from a large corpora. This study considers word senses as topics. Basically, the acquisition of TS consists of: • acquiring the best possible corpus examples for a particular word sense (usually characterizing each word sense as a query and performing a search on the corpus for those examples that best match the queries) • building the TS by selecting the context words that best represent the word sense from the selected corpora. The Topic Signatures acquired from the web (hereinafter TSWEB) constitutes one of the large"
C08-1021,magnini-cavaglia-2000-integrating,0,0.0754665,"Missing"
C94-1052,A92-1044,1,0.894032,"Missing"
C94-1052,A92-1012,0,\N,Missing
C96-1005,H92-1046,0,0.0813737,"Missing"
C96-1005,J92-1001,0,0.00808683,"Missing"
C96-1005,H93-1061,0,0.0630953,"Missing"
C96-1005,H94-1046,0,0.0211457,"Missing"
C96-1005,W95-0105,0,0.086135,"Missing"
C96-1005,C92-2070,0,0.673012,"Missing"
C96-1005,H91-1077,0,\N,Missing
C96-1005,E95-1016,0,\N,Missing
C96-1005,C92-1056,0,\N,Missing
C96-1005,H93-1052,0,\N,Missing
C98-2176,P97-1007,1,0.897256,"Missing"
C98-2176,W97-0313,0,0.0338297,"Missing"
C98-2176,C92-2070,0,0.0901388,"Missing"
C98-2176,W90-0108,0,\N,Missing
C98-2176,C92-4189,0,\N,Missing
C98-2176,P95-1026,0,\N,Missing
C98-2176,P81-1030,0,\N,Missing
cuadros-etal-2010-integrating,vossen-etal-2008-kyoto,1,\N,Missing
cuadros-etal-2010-integrating,P00-1064,0,\N,Missing
cuadros-etal-2010-integrating,E09-1005,0,\N,Missing
cuadros-etal-2010-integrating,R09-1039,1,\N,Missing
cuadros-etal-2012-highlighting,alvez-etal-2008-complete,1,\N,Missing
cuadros-etal-2012-highlighting,J98-1006,0,\N,Missing
cuadros-etal-2012-highlighting,agirre-de-lacalle-2004-publicly,0,\N,Missing
cuadros-etal-2012-highlighting,W06-1663,1,\N,Missing
cuadros-etal-2012-highlighting,C02-1144,0,\N,Missing
cuadros-etal-2012-highlighting,E09-1005,0,\N,Missing
cuadros-etal-2012-highlighting,C00-1072,0,\N,Missing
cuadros-etal-2012-highlighting,P05-1016,0,\N,Missing
cuadros-etal-2012-highlighting,P04-1036,0,\N,Missing
cuadros-etal-2012-highlighting,P06-1101,0,\N,Missing
cuadros-etal-2012-highlighting,magnini-cavaglia-2000-integrating,0,\N,Missing
E09-1045,alvez-etal-2008-complete,1,0.829727,"proach followed to build the class–based system is explained. Experiments and results are shown in section 4. Finally some conclusions are drawn in section 5. Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini and Cavagli`a, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts (Vossen et al., 1998), Top Concept Ontology labels (Alvez et al., 2008) or Basic Level Concepts (Izquierdo et al., 2007). Obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD. Possibly, their combination could improve the overall results since they offer different semantic perspectives of the data. Furthermore, to our knowledge, to date no comparative evaluation has been performed on SensEval data exploring different levels of abstraction. In fact, (Villarejo et al., 2005) studied the performance of class–based WSD comparing only SuperSenses and SUMO by 10–fold cro"
E09-1045,W06-1670,0,0.241154,"Spanish Government under the project Text-Mess (TIN2006-15265-C06-01) and KNOW (TIN2006-15049-C03-01) 1 http://www.senseval.org http://wordnet.princeton.edu 3 http://www.wikipedia.org 2 Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389–397, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 389 In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. empirically demonstrate that a) Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class– based WSD, b) that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c) that the class-based approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers. After this introduction, section 2 presents the sense-groupings used in this stu"
E09-1045,W03-1022,0,0.160464,"r the projects QALL-ME (FP6 IST-033860) and KYOTO (FP7 ICT-211423), and the Spanish Government under the project Text-Mess (TIN2006-15265-C06-01) and KNOW (TIN2006-15049-C03-01) 1 http://www.senseval.org http://wordnet.princeton.edu 3 http://www.wikipedia.org 2 Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389–397, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 389 In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. empirically demonstrate that a) Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class– based WSD, b) that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c) that the class-based approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers. After"
E09-1045,P05-1004,0,0.0815425,"T-211423), and the Spanish Government under the project Text-Mess (TIN2006-15265-C06-01) and KNOW (TIN2006-15049-C03-01) 1 http://www.senseval.org http://wordnet.princeton.edu 3 http://www.wikipedia.org 2 Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389–397, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 389 In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. empirically demonstrate that a) Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class– based WSD, b) that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c) that the class-based approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers. After this introduction, section 2 presents the"
E09-1045,W93-0106,0,0.277004,"Missing"
E09-1045,W97-0811,0,0.508693,"the European Union under the projects QALL-ME (FP6 IST-033860) and KYOTO (FP7 ICT-211423), and the Spanish Government under the project Text-Mess (TIN2006-15265-C06-01) and KNOW (TIN2006-15049-C03-01) 1 http://www.senseval.org http://wordnet.princeton.edu 3 http://www.wikipedia.org 2 Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389–397, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 389 In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. empirically demonstrate that a) Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class– based WSD, b) that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c) that the class-based approach to WSD reduces dramatically the required amount of training examples to obtain"
E09-1045,D07-1107,0,0.0828027,"Missing"
E09-1045,W04-0811,0,0.0366539,"is a subset of the Brown Corpus plus the novel The Red Badge of Courage, and it has been developed by the same group that created WordNet. It contains 253 texts and around 700,000 running words, and more than 200,000 are also lemmatized and sense-tagged according to Princeton WordNet 1.6. SensEval-27 English all-words corpus (hereinafter SE2) (Palmer et al., 2001) consists on 5,000 words of text from three WSJ articles representing different domains from the Penn TreeBank II. The sense inventory used for tagging is WordNet 1.7. Finally, SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from two WSJ articles and one excerpt from the Brown Corpus. Sense repository of WordNet 1.7.1 was used to tag 2,041 words with their proper senses. 7 8 Feature types 9 We have selected these set since they represent different levels of abstraction. Remember that 20 and 50 refer to the threshold of minimum number of synsets that a possible BLC must subsume to be considered as a proper BLC. These BLC sets were built using all kind of relations. 10 That is, the value of the feature, for example a feature type can be word-form, and a feature of that type can"
E09-1045,magnini-cavaglia-2000-integrating,0,0.302012,"Missing"
E09-1045,N07-1025,0,0.0828421,"Missing"
E09-1045,P94-1013,0,0.0682496,"ires the estimation of a parameter (C), that represent the trade-off allowed between training errors and the margin. We have set this value to 0.01, which has been proved as a good value for SVM in WSD tasks. When classifying an example, we obtain the value of the output function for each SVM classifier corresponding to each semantic class for the word example. Our system simply selects the class with the greater value. We have defined a set of features to represent the examples according to previous works in WSD and the nature of class-based WSD. Features widely used in the literature as in (Yarowsky, 1994) have been selected. These features are pieces of information that occur in the context of the target word, and can be organized as: Local features: bigrams and trigrams that contain the target word, including part-of-speech (PoS), lemmas or word-forms. Topical features: word–forms or lemmas appearing in windows around the target word. In particular, our systems use the following basic features: Word–forms and lemmas in a window of 10 words around the target word PoS: the concatenation of the preceding/following three/five PoS Bigrams and trigrams formed by lemmas and word-forms and obtained i"
E09-1045,H93-1061,0,0.279109,"n terms of the frequency of f. For each class c, and for each feature f of that class, we calculate the frequency of the feature within the class (the number of times that it occurs in examples 3.2 Corpora Three semantic annotated corpora have been used for training and testing. SemCor has been used for training while the corpora from the English all-words tasks of SensEval-2 and SensEval-3 has been used for testing. We also considered SemEval-2007 coarse–grained task corpus for testing, but this dataset was discarded because this corpus is also annotated with clusters of word senses. SemCor (Miller et al., 1993) is a subset of the Brown Corpus plus the novel The Red Badge of Courage, and it has been developed by the same group that created WordNet. It contains 253 texts and around 700,000 running words, and more than 200,000 are also lemmatized and sense-tagged according to Princeton WordNet 1.6. SensEval-27 English all-words corpus (hereinafter SE2) (Palmer et al., 2001) consists on 5,000 words of text from three WSJ articles representing different domains from the Penn TreeBank II. The sense inventory used for tagging is WordNet 1.7. Finally, SensEval-38 English all-words corpus (hereinafter SE3) ("
E09-1045,P06-1014,0,0.0871533,"Missing"
E09-1045,S01-1005,0,\N,Missing
E14-1010,agerri-garcia-serrano-2010-q,1,0.76862,"Missing"
E14-1010,P97-1023,0,0.060624,"rds as seeds which are then used to perform some iterative propagation on the LKB (Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue arising from corpus methods is that for a given domain the same word can be positive in one context but negative in another. This is also a problem shared by manual and dictionary-based methods, and that is why qwn-ppv also produces synset-based lexicons for approaches on Sentiment Analysis at sense level. This paper presents a simple, robust and (almost) unsupervised dictionary-based method, QWordNet-PPV (QWordNet by Personalized PageRank Vector) to automatically generate polarit"
E14-1010,E09-1005,0,0.0324595,"tations they convey. ‘Beautiful’, ‘wonderful’, and ‘amazing’ are examples of positive words whereas ‘bad’, ‘awful’, and ‘poor’ are examples of negatives. The creation of lists of sentiment words has generally been performed by means of manual-, dictionary- and corpus-based methods. Manually collecting such lists of polarity annotated words is 88 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 88–97, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 PageRank algorithm (Agirre et al., 2014; Agirre and Soroa, 2009) over a LKB projected into a graph. We see qwn-ppv as an effective methodology to easily create polarity lexicons for any language for which a WordNet is available. Related Work There is a large amount of work on Sentiment Analysis and Opinion Mining, and good comprehensive overviews are already available (Pang and Lee, 2008; Liu, 2012), so we will review the most representative and closest to the present work. This means that we will not be reviewing corpus-based approaches but rather those constructed manually or upon a dictionary or LKB. We will in turn use the approaches here reviewed for"
E14-1010,C04-1200,0,0.0688353,"final check to correct mistakes. However, there are well known lexicons which have been fully (Stone et al., 1966; Taboada et al., 2010) or at least partially manually created (Hu and Liu, 2004; Riloff and Wiebe, 2003). Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that contain synonyms and antonyms for each word. A simple technique in this approach is to start with some sentiment words as seeds which are then used to perform some iterative propagation on the LKB (Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue ari"
E14-1010,P07-1123,0,0.0133062,"apparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue arising from corpus methods is that for a given domain the same word can be positive in one context but negative in another. This is also a problem shared by manual and dictionary-based methods, and that is why qwn-ppv also produces synset-based lexicons for approaches on Sentiment Analysis at sense level. This paper presents a simple, robust and (almost) unsupervised dictionary-based method, QWordNet-PPV (QWordNet by Personalized PageRank Vector) to automatically generate polarity lexicons based on propagating some automatically created seeds using a Personalized This p"
E14-1010,baccianella-etal-2010-sentiwordnet,0,0.593516,"al., 2010) or at least partially manually created (Hu and Liu, 2004; Riloff and Wiebe, 2003). Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that contain synonyms and antonyms for each word. A simple technique in this approach is to start with some sentiment words as seeds which are then used to perform some iterative propagation on the LKB (Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue arising from corpus methods is that for a given domain the same word can be positive in one context but negative in another. This is als"
E14-1010,D09-1063,0,0.129667,"ons which have been fully (Stone et al., 1966; Taboada et al., 2010) or at least partially manually created (Hu and Liu, 2004; Riloff and Wiebe, 2003). Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that contain synonyms and antonyms for each word. A simple technique in this approach is to start with some sentiment words as seeds which are then used to perform some iterative propagation on the LKB (Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue arising from corpus methods is that for a given domain the same word can be"
E14-1010,P13-1133,0,0.0165222,"create qwn-ppv(s) for other languages, and we demonstrate it here by creating many polarity lexicons not only for English but also for Spanish; (vi) the method works at both word and sense levels and it only requires the availability of a LKB or dictionary; finally, (vii) a dictionarybased method like qwn-ppv allows to easily create quality polarity lexicons whenever no domainbased annotated reviews are available for a given language. After all, there usually is available a dictionary for a given language; for example, the Open Multilingual WordNet site lists WordNets for up to 57 languages (Bond and Foster, 2013). Although there has been previous work using graph methods for obtaining lexicons via propagation, the qwn-ppv method to combine the seed generation and the Personalized PageRank propagation is novel. Furthermore, it is considerable simpler and obtains better and easier to reproduce results than previous automatic approaches (Esuli and Sebastiani, 2007; Mohammad et al., 2009; Rao and Ravichandran, 2009). Next section reviews previous related work, taking special interest on those that are currently available for evaluation purposes. Section 3 describes the qwn-ppv method to automatically gene"
E14-1010,padro-stanilovsky-2012-freeling,0,0.0246014,"Missing"
E14-1010,perez-rosas-etal-2012-learning,0,0.184873,"Missing"
E14-1010,D09-1062,0,0.0495774,"(Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue arising from corpus methods is that for a given domain the same word can be positive in one context but negative in another. This is also a problem shared by manual and dictionary-based methods, and that is why qwn-ppv also produces synset-based lexicons for approaches on Sentiment Analysis at sense level. This paper presents a simple, robust and (almost) unsupervised dictionary-based method, QWordNet-PPV (QWordNet by Personalized PageRank Vector) to automatically generate polarity lexicons based on propagating some automatically created seeds usi"
E14-1010,E09-1077,0,0.0660411,"s are available for a given language. After all, there usually is available a dictionary for a given language; for example, the Open Multilingual WordNet site lists WordNets for up to 57 languages (Bond and Foster, 2013). Although there has been previous work using graph methods for obtaining lexicons via propagation, the qwn-ppv method to combine the seed generation and the Personalized PageRank propagation is novel. Furthermore, it is considerable simpler and obtains better and easier to reproduce results than previous automatic approaches (Esuli and Sebastiani, 2007; Mohammad et al., 2009; Rao and Ravichandran, 2009). Next section reviews previous related work, taking special interest on those that are currently available for evaluation purposes. Section 3 describes the qwn-ppv method to automatically generate lexicons. The resulting lexical resources are evaluated in section 4. We finish with some concluding remarks and future work in section 5. 89 derived-from, pertains-to and also-see. and Littman, 2003). These seeds are then iteratively extended following the construction of WordNet-Affect (Strapparava and Valitutti, 2004). (ii) They train 7 supervised classifiers with the synsets’ glosses which are u"
E14-1010,W03-1014,0,0.0262761,"or LKB. We will in turn use the approaches here reviewed for comparison with qwn-ppv in section 4. The most popular manually-built polarity lexicon is part of the General Inquirer (Stone et al., 1966), and consists of 1915 words labelled as “positive” and 2291 as “negative”. Taboada et al. (2010) manually created their lexicons annotating the polarity of 6232 words on a scale of 5 to -5. Liu et al., starting with Hu and Liu (2004), have along the years collected a manually corrected polarity lexicon which is formed by 4818 negative and 2041 positive words. Another manually corrected lexicon (Riloff and Wiebe, 2003) is the one used by the Opinion Finder system (Wilson et al., 2005) and contains 4903 negatively and 2718 positively annotated words respectively. Among the automatically built lexicons, Turney and Littman (2003) proposed a minimally supervised algorithm to calculate the polarity of a word depending on whether it co-ocurred more with a previously collected small set of positive words rather than with a set of negative ones. Agerri and Garc´ıa Serrano presented a very simple method to extract the polarity information starting from the quality synset in WordNet (Agerri and Garc´ıaSerrano, 2010)."
E14-1010,P07-1054,0,0.0139694,"y lexicons whenever no domainbased annotated reviews are available for a given language. After all, there usually is available a dictionary for a given language; for example, the Open Multilingual WordNet site lists WordNets for up to 57 languages (Bond and Foster, 2013). Although there has been previous work using graph methods for obtaining lexicons via propagation, the qwn-ppv method to combine the seed generation and the Personalized PageRank propagation is novel. Furthermore, it is considerable simpler and obtains better and easier to reproduce results than previous automatic approaches (Esuli and Sebastiani, 2007; Mohammad et al., 2009; Rao and Ravichandran, 2009). Next section reviews previous related work, taking special interest on those that are currently available for evaluation purposes. Section 3 describes the qwn-ppv method to automatically generate lexicons. The resulting lexical resources are evaluated in section 4. We finish with some concluding remarks and future work in section 5. 89 derived-from, pertains-to and also-see. and Littman, 2003). These seeds are then iteratively extended following the construction of WordNet-Affect (Strapparava and Valitutti, 2004). (ii) They train 7 supervis"
E14-1010,strapparava-valitutti-2004-wordnet,0,0.316704,"with automated approaches as the final check to correct mistakes. However, there are well known lexicons which have been fully (Stone et al., 1966; Taboada et al., 2010) or at least partially manually created (Hu and Liu, 2004; Riloff and Wiebe, 2003). Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that contain synonyms and antonyms for each word. A simple technique in this approach is to start with some sentiment words as seeds which are then used to perform some iterative propagation on the LKB (Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One"
E14-1010,P05-1017,0,0.0145136,"ect mistakes. However, there are well known lexicons which have been fully (Stone et al., 1966; Taboada et al., 2010) or at least partially manually created (Hu and Liu, 2004; Riloff and Wiebe, 2003). Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that contain synonyms and antonyms for each word. A simple technique in this approach is to start with some sentiment words as seeds which are then used to perform some iterative propagation on the LKB (Hu and Liu, 2004; Strapparava and Valitutti, 2004; Kim and Hovy, 2004; Takamura et al., 2005; Turney and Littman, 2003; Mohammad et al., 2009; Agerri and Garc´ıa-Serrano, 2010; Baccianella et al., 2010). Corpus-based methods have usually been applied to obtain domain-specific polarity lexicons: they have been created by either starting from a seed list of known words and trying to find other related words in a corpus or by attempting to directly adapt a given lexicon to a new one using a domain-specific corpus (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Ding et al., 2008; Choi and Cardie, 2009; Mihalcea et al., 2007). One particular issue arising from corpus method"
E14-1010,P02-1053,0,0.047164,"Missing"
E14-1010,H05-1044,0,0.296351,"on with qwn-ppv in section 4. The most popular manually-built polarity lexicon is part of the General Inquirer (Stone et al., 1966), and consists of 1915 words labelled as “positive” and 2291 as “negative”. Taboada et al. (2010) manually created their lexicons annotating the polarity of 6232 words on a scale of 5 to -5. Liu et al., starting with Hu and Liu (2004), have along the years collected a manually corrected polarity lexicon which is formed by 4818 negative and 2041 positive words. Another manually corrected lexicon (Riloff and Wiebe, 2003) is the one used by the Opinion Finder system (Wilson et al., 2005) and contains 4903 negatively and 2718 positively annotated words respectively. Among the automatically built lexicons, Turney and Littman (2003) proposed a minimally supervised algorithm to calculate the polarity of a word depending on whether it co-ocurred more with a previously collected small set of positive words rather than with a set of negative ones. Agerri and Garc´ıa Serrano presented a very simple method to extract the polarity information starting from the quality synset in WordNet (Agerri and Garc´ıaSerrano, 2010). Mohammad et al. (2009) developed a method in which they first iden"
E14-1010,J11-2001,0,\N,Missing
E14-1010,J14-1003,0,\N,Missing
E14-2002,W02-1001,0,0.111486,"Missing"
E14-2002,J03-4003,0,0.0166699,"Missing"
E14-2002,H05-1100,0,0.0150641,"valent plain text dictionary; and (iii) We also provide lemmatization by lookup in WordNet-3.0 (Fellbaum and Miller, 1998) via the JWNL API12 . Note that this method is only available for English. 3.3 as defined in his PhD thesis (1999), and another one based on Stanford’s parser Semantic Head Rules15 . The latter are a modification of Collins’ head rules according to lexical and semantic criteria. These head rules are particularly useful for the Coreference resolution module and for projecting the constituents into dependency graphs. As far as we know, and although previous approaches exist (Cowan and Collins, 2005), ixa-pipeparse provides the first publicly available statistical parser for Spanish. 3.5 ixa-pipe-nerc The module of coreference resolution included in the IXA pipeline is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The module takes every linguistic information it requires from the KAF layers annotated by all the previously described modules. The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves ar"
E14-2002,P05-1045,0,0.0133025,"Missing"
E14-2002,gimenez-marquez-2004-svmtool,0,0.104864,"Missing"
E14-2002,J13-4004,0,0.0301652,"Missing"
E14-2002,padro-stanilovsky-2012-freeling,0,0.0626778,"Missing"
E14-2002,W09-1119,0,0.0497547,"Missing"
E14-2002,W02-2004,0,0.111589,"Missing"
E14-2002,taule-etal-2008-ancora,0,0.0761671,"Missing"
E14-2002,W03-0424,0,0.131663,"Missing"
E14-2002,N03-1033,0,0.0240618,"Missing"
fernandez-etal-2004-automatic,J98-1006,0,\N,Missing
fernandez-etal-2004-automatic,P03-1018,0,\N,Missing
fernandez-etal-2004-automatic,W00-1702,0,\N,Missing
gonzalez-agirre-etal-2012-multilingual,alvez-etal-2008-complete,1,\N,Missing
gonzalez-agirre-etal-2012-multilingual,pociello-etal-2008-wnterm,1,\N,Missing
gonzalez-agirre-etal-2012-multilingual,W02-1304,1,\N,Missing
gonzalez-agirre-etal-2012-multilingual,P00-1064,0,\N,Missing
gonzalez-agirre-etal-2012-multilingual,magnini-cavaglia-2000-integrating,0,\N,Missing
gonzalez-agirre-etal-2012-multilingual,atserias-etal-2004-spanish,1,\N,Missing
gonzalez-agirre-etal-2012-proposal,E09-1005,0,\N,Missing
gonzalez-agirre-etal-2012-proposal,W04-2214,0,\N,Missing
gonzalez-agirre-etal-2012-proposal,P10-1023,0,\N,Missing
gonzalez-agirre-etal-2012-proposal,agirre-etal-2010-exploring,1,\N,Missing
gonzalez-agirre-etal-2012-proposal,magnini-cavaglia-2000-integrating,0,\N,Missing
L16-1009,baccianella-etal-2010-sentiwordnet,0,0.215025,"ood coverage and difficult to maintain when the vocabulary evolves or a new language or domain must be analyzed. 54 Therefore it is necessary to devise a method to automate the process as much as possible. Some systems employ existing lexical resources like WordNet (Fellbaum, 1998) to bootstrap a list of positive and negative words via different methods. In (Esuli and Sebastiani, 2006) the authors employ the glosses that accompany each WordNet synset1 to perform a semi-supervised synset classification. The result consists of three scores per synset: positivity, negativity and objectivity. In (Baccianella et al., 2010) version 3.0 of SentiWordNet is introduced with improvements like a random walk approach in the WordNet graph to calculate the SO of the synsets. In (Agerri and Garcia, 2009) another system is introduced, Q-WordNet, which expands the polarities of the WordNet synsets using lexical relations like synonymy. In (Guerini et al., 2013) the authors propose and compare different approaches based SentiWordNet to improve the polarity determination of the synsets. Other authors try different bootstrapping approaches and evaluate them on WordNet of different languages (Maks et al., 2014; Vicente et al.,"
L16-1009,N10-1122,0,0.0363268,"ph based on some intuitions like that two adjectives joined by ”and” will tend to share the same orientation while two adjectives joined by ”but” will have opposite orientations. On the other hand, in (Turney, 2002) the SO is obtained calculating the Pointwise Mutual Information (PMI) between each word and a very positive word (like ”excellent”) and a very negative word (like ”poor”) in a corpus. The result is a continuous numeric value between -1 and +1. These ideas of bootstrapping SO from a corpus have been further explored and sophisticated in more recent works (Popescu and Etzioni, 2005; Brody and Elhadad, 2010; Qiu et al., 2011) 2.1. et al., 2010; Huang et al., 2012; Mikolov et al., 2013c). Word embeddings show interesting semantic properties to find related concepts, word analogies, or to use them as features to conventional machine learning algorithms (Socher et al., 2013; Tang et al., 2014b; Pavlopoulos and Androutsopoulos, 2014). Word embeddings are also explored in tasks such as deriving adjectival scales (Kim, 2013). 3. Lexicons and methods Our aim is to compare different existing sentiment lexicons and methods to find out if continuous word embeddings can be used to easily compute accurate s"
L16-1009,esuli-sebastiani-2006-sentiwordnet,0,0.30347,"SO is known as sentiment lexicon. Sentiment lexicons can be constructed manually, by human experts that estimate the corresponding SO value to each word of interest. Obviously, this approach is usually too time consuming for obtaining a good coverage and difficult to maintain when the vocabulary evolves or a new language or domain must be analyzed. 54 Therefore it is necessary to devise a method to automate the process as much as possible. Some systems employ existing lexical resources like WordNet (Fellbaum, 1998) to bootstrap a list of positive and negative words via different methods. In (Esuli and Sebastiani, 2006) the authors employ the glosses that accompany each WordNet synset1 to perform a semi-supervised synset classification. The result consists of three scores per synset: positivity, negativity and objectivity. In (Baccianella et al., 2010) version 3.0 of SentiWordNet is introduced with improvements like a random walk approach in the WordNet graph to calculate the SO of the synsets. In (Agerri and Garcia, 2009) another system is introduced, Q-WordNet, which expands the polarities of the WordNet synsets using lexical relations like synonymy. In (Guerini et al., 2013) the authors propose and compar"
L16-1009,D13-1125,0,0.0196391,"a different methods. In (Esuli and Sebastiani, 2006) the authors employ the glosses that accompany each WordNet synset1 to perform a semi-supervised synset classification. The result consists of three scores per synset: positivity, negativity and objectivity. In (Baccianella et al., 2010) version 3.0 of SentiWordNet is introduced with improvements like a random walk approach in the WordNet graph to calculate the SO of the synsets. In (Agerri and Garcia, 2009) another system is introduced, Q-WordNet, which expands the polarities of the WordNet synsets using lexical relations like synonymy. In (Guerini et al., 2013) the authors propose and compare different approaches based SentiWordNet to improve the polarity determination of the synsets. Other authors try different bootstrapping approaches and evaluate them on WordNet of different languages (Maks et al., 2014; Vicente et al., 2014). A problem with the approaches based on resources like WordNet is that they rely on the availability and quality of those resources for a new languages. Being a general resource, WordNet also fails to capture domain dependent semantic orientations. Likewise other approaches using common dictionaries do not take into account"
L16-1009,P97-1023,0,0.212745,"ets. Other authors try different bootstrapping approaches and evaluate them on WordNet of different languages (Maks et al., 2014; Vicente et al., 2014). A problem with the approaches based on resources like WordNet is that they rely on the availability and quality of those resources for a new languages. Being a general resource, WordNet also fails to capture domain dependent semantic orientations. Likewise other approaches using common dictionaries do not take into account the shifts between domains (Ramos and Marques, 2005). Other methods calculate the SO of the words directly from text. In (Hatzivassiloglou and McKeown, 1997) the authors model the corpus as a graph of adjectives joined by conjunctions. Then, they generate partitions on the graph based on some intuitions like that two adjectives joined by ”and” will tend to share the same orientation while two adjectives joined by ”but” will have opposite orientations. On the other hand, in (Turney, 2002) the SO is obtained calculating the Pointwise Mutual Information (PMI) between each word and a very positive word (like ”excellent”) and a very negative word (like ”poor”) in a corpus. The result is a continuous numeric value between -1 and +1. These ideas of boots"
L16-1009,W14-1306,0,0.031266,"positive word (like ”excellent”) and a very negative word (like ”poor”) in a corpus. The result is a continuous numeric value between -1 and +1. These ideas of bootstrapping SO from a corpus have been further explored and sophisticated in more recent works (Popescu and Etzioni, 2005; Brody and Elhadad, 2010; Qiu et al., 2011) 2.1. et al., 2010; Huang et al., 2012; Mikolov et al., 2013c). Word embeddings show interesting semantic properties to find related concepts, word analogies, or to use them as features to conventional machine learning algorithms (Socher et al., 2013; Tang et al., 2014b; Pavlopoulos and Androutsopoulos, 2014). Word embeddings are also explored in tasks such as deriving adjectival scales (Kim, 2013). 3. Lexicons and methods Our aim is to compare different existing sentiment lexicons and methods to find out if continuous word embeddings can be used to easily compute accurate sentiment polarity over the words of a domain, and under which conditions. The experiments are carried on two specific domains, in particular restaurants and laptops reviews. 3.1. General lexicons The General Inquirer (GI) (Stone et al., 1966) is a very well-known manually crafted lexicon that includes the polarity of many Engli"
L16-1009,J11-1002,0,0.0249174,"ns like that two adjectives joined by ”and” will tend to share the same orientation while two adjectives joined by ”but” will have opposite orientations. On the other hand, in (Turney, 2002) the SO is obtained calculating the Pointwise Mutual Information (PMI) between each word and a very positive word (like ”excellent”) and a very negative word (like ”poor”) in a corpus. The result is a continuous numeric value between -1 and +1. These ideas of bootstrapping SO from a corpus have been further explored and sophisticated in more recent works (Popescu and Etzioni, 2005; Brody and Elhadad, 2010; Qiu et al., 2011) 2.1. et al., 2010; Huang et al., 2012; Mikolov et al., 2013c). Word embeddings show interesting semantic properties to find related concepts, word analogies, or to use them as features to conventional machine learning algorithms (Socher et al., 2013; Tang et al., 2014b; Pavlopoulos and Androutsopoulos, 2014). Word embeddings are also explored in tasks such as deriving adjectival scales (Kim, 2013). 3. Lexicons and methods Our aim is to compare different existing sentiment lexicons and methods to find out if continuous word embeddings can be used to easily compute accurate sentiment polarity o"
L16-1009,N16-1091,0,0.04906,"Missing"
L16-1009,D13-1170,0,0.0170002,"texts, like the English Wikipedia. One of the best known systems to obtain a dense continuous representation of words is Word2Vec (Mikolov et al., 2013c). But Word2Vec is not the only one, and in fact there are already a lot of variants and many researchers working on different kinds of word embeddings (Le and Mikolov, 2014; Iacobacci et al., 2015; Ji et al., 2015; Hill et al., 2014; Schwartz et al., 2014). With regard to Sentiment Analysis, word embeddings are used as features to more complex supervised classification systems to obtain very precise sentiment classifiers (Tang et al., 2014a; Socher et al., 2013). In this paper we compare a set of existing static sentiment lexicons and dynamic sentiment lexicon generation techniques. We also show a simple but competitive technique to calculate a word polarity value for each word in a domain using continuous word embeddings. Our objective is to see if word embeddings calculated on an in-domain corpus can be directly used to obtain a polarity measure of the domain vocabulary with no additional supervision. Further, we want to see to which extent word embeddings calcu2. Related Work Sentiment analysis refers to the use of NLP techniques to identify and e"
L16-1009,J11-2001,0,0.0424661,"views several works related to the generation of sentiment lexicons, providing the context for the rest of the paper. Section 3. describes the lexicons and methods that will be used to make the comparison, focusing on the ones using continuous word representations. Section 4. presents the datasets used to generate some of the lexicons. Section 5. describes the experiments to compare the different approaches and discusses them. Finally the last section shows the conclusions. A key point in Sentiment Analysis is to determine the polarity of the sentiment implied by a certain word or expression (Taboada et al., 2011). In basic Sentiment Analysis systems this sentiment polarity of the words is accounted and weighted in different ways to provide a degree of positivity/negativity of, for example, a customer review. In more sophisticated systems, word polarity is employed as an additional feature for machine learning algorithms. This polarity value can be a categorical value (e.g. positive/neutral/negative) or a real value within a range (e.g. from -1.0 to +1.0), and can be plugged in supervised classification algorithms together with other lexical and semantic features to help discriminating the overall pola"
L16-1009,P14-1146,0,0.440209,"very big corpora of texts, like the English Wikipedia. One of the best known systems to obtain a dense continuous representation of words is Word2Vec (Mikolov et al., 2013c). But Word2Vec is not the only one, and in fact there are already a lot of variants and many researchers working on different kinds of word embeddings (Le and Mikolov, 2014; Iacobacci et al., 2015; Ji et al., 2015; Hill et al., 2014; Schwartz et al., 2014). With regard to Sentiment Analysis, word embeddings are used as features to more complex supervised classification systems to obtain very precise sentiment classifiers (Tang et al., 2014a; Socher et al., 2013). In this paper we compare a set of existing static sentiment lexicons and dynamic sentiment lexicon generation techniques. We also show a simple but competitive technique to calculate a word polarity value for each word in a domain using continuous word embeddings. Our objective is to see if word embeddings calculated on an in-domain corpus can be directly used to obtain a polarity measure of the domain vocabulary with no additional supervision. Further, we want to see to which extent word embeddings calcu2. Related Work Sentiment analysis refers to the use of NLP techn"
L16-1009,P10-1040,0,0.12272,"Missing"
L16-1009,P02-1053,0,0.0463871,"ure domain dependent semantic orientations. Likewise other approaches using common dictionaries do not take into account the shifts between domains (Ramos and Marques, 2005). Other methods calculate the SO of the words directly from text. In (Hatzivassiloglou and McKeown, 1997) the authors model the corpus as a graph of adjectives joined by conjunctions. Then, they generate partitions on the graph based on some intuitions like that two adjectives joined by ”and” will tend to share the same orientation while two adjectives joined by ”but” will have opposite orientations. On the other hand, in (Turney, 2002) the SO is obtained calculating the Pointwise Mutual Information (PMI) between each word and a very positive word (like ”excellent”) and a very negative word (like ”poor”) in a corpus. The result is a continuous numeric value between -1 and +1. These ideas of bootstrapping SO from a corpus have been further explored and sophisticated in more recent works (Popescu and Etzioni, 2005; Brody and Elhadad, 2010; Qiu et al., 2011) 2.1. et al., 2010; Huang et al., 2012; Mikolov et al., 2013c). Word embeddings show interesting semantic properties to find related concepts, word analogies, or to use them"
L16-1009,P12-1092,0,0.0540384,"”and” will tend to share the same orientation while two adjectives joined by ”but” will have opposite orientations. On the other hand, in (Turney, 2002) the SO is obtained calculating the Pointwise Mutual Information (PMI) between each word and a very positive word (like ”excellent”) and a very negative word (like ”poor”) in a corpus. The result is a continuous numeric value between -1 and +1. These ideas of bootstrapping SO from a corpus have been further explored and sophisticated in more recent works (Popescu and Etzioni, 2005; Brody and Elhadad, 2010; Qiu et al., 2011) 2.1. et al., 2010; Huang et al., 2012; Mikolov et al., 2013c). Word embeddings show interesting semantic properties to find related concepts, word analogies, or to use them as features to conventional machine learning algorithms (Socher et al., 2013; Tang et al., 2014b; Pavlopoulos and Androutsopoulos, 2014). Word embeddings are also explored in tasks such as deriving adjectival scales (Kim, 2013). 3. Lexicons and methods Our aim is to compare different existing sentiment lexicons and methods to find out if continuous word embeddings can be used to easily compute accurate sentiment polarity over the words of a domain, and under w"
L16-1009,P15-1010,0,0.0253418,"iscriminating the overall polarity of an expression or a sentence. Currently words are also modelled as continuous dense vectors, known as word embeddings, which seem to encode interesting semantic knowledge. The word vectors are usually computed using very big corpora of texts, like the English Wikipedia. One of the best known systems to obtain a dense continuous representation of words is Word2Vec (Mikolov et al., 2013c). But Word2Vec is not the only one, and in fact there are already a lot of variants and many researchers working on different kinds of word embeddings (Le and Mikolov, 2014; Iacobacci et al., 2015; Ji et al., 2015; Hill et al., 2014; Schwartz et al., 2014). With regard to Sentiment Analysis, word embeddings are used as features to more complex supervised classification systems to obtain very precise sentiment classifiers (Tang et al., 2014a; Socher et al., 2013). In this paper we compare a set of existing static sentiment lexicons and dynamic sentiment lexicon generation techniques. We also show a simple but competitive technique to calculate a word polarity value for each word in a domain using continuous word embeddings. Our objective is to see if word embeddings calculated on an in-"
L16-1009,S14-2105,0,0.0201127,"t in Sentiment Analysis is to determine the polarity of the sentiment implied by a certain word or expression (Taboada et al., 2011). Usually this polarity is also known as Semantic Orientation (SO). SO indicates whether a word or an expression states a positive or a negative sentiment, and can be a continuous value in a range from very positive to very negative, or a categorical value (like the common 5star rating used to rate products). Further, the SO of a word is a useful feature to be used within a more complex Sentiment Analysis system like machine learning algorithms (Lin et al., 2009; Jaggi et al., 2014; Tang et al., 2014a). A collection of words and their respective SO is known as sentiment lexicon. Sentiment lexicons can be constructed manually, by human experts that estimate the corresponding SO value to each word of interest. Obviously, this approach is usually too time consuming for obtaining a good coverage and difficult to maintain when the vocabulary evolves or a new language or domain must be analyzed. 54 Therefore it is necessary to devise a method to automate the process as much as possible. Some systems employ existing lexical resources like WordNet (Fellbaum, 1998) to bootstrap"
L16-1009,D13-1169,0,0.0186973,"c value between -1 and +1. These ideas of bootstrapping SO from a corpus have been further explored and sophisticated in more recent works (Popescu and Etzioni, 2005; Brody and Elhadad, 2010; Qiu et al., 2011) 2.1. et al., 2010; Huang et al., 2012; Mikolov et al., 2013c). Word embeddings show interesting semantic properties to find related concepts, word analogies, or to use them as features to conventional machine learning algorithms (Socher et al., 2013; Tang et al., 2014b; Pavlopoulos and Androutsopoulos, 2014). Word embeddings are also explored in tasks such as deriving adjectival scales (Kim, 2013). 3. Lexicons and methods Our aim is to compare different existing sentiment lexicons and methods to find out if continuous word embeddings can be used to easily compute accurate sentiment polarity over the words of a domain, and under which conditions. The experiments are carried on two specific domains, in particular restaurants and laptops reviews. 3.1. General lexicons The General Inquirer (GI) (Stone et al., 1966) is a very well-known manually crafted lexicon that includes the polarity of many English words. GI contains about 2000 positive and negative words. It has been used in many diff"
L16-1009,S14-2076,0,0.0332623,"xing (Dumais et al., 1995) and Latent Semantic Analysis (Dumais, 2004). Currently it is becoming very common in the literature to employ Neural Networks and the so-called Deep Learning to compute word embeddings (Bengio et al., 2003; Turian SO(w) = PMI(w, P OS) − PMI(w, N EG) PMI(w1, w2) = log p(w1, w2) p(w1) × p(w2) (1) (2) 2 https://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#lexicon 3 A WordNet synset in a set of synonym words that denote the same concept 1 A WordNet synset in a set of synonym words that denote the same concept 55 Firstly, we have borrowed the lexicon generated in (Kiritchenko et al., 2014) (named NRC CANADA in the experiment tables), which was generated computing the PMI between each word and positive reviews(4 or 5 stars in a 5-star rating) and negative reviews (1 or 2 stars), for both restaurants and laptops review datasets. Because it uses the user ratings, this approach is supervised. As a counterpart we have calculated another PMI based lexicon, in which we employ the co-occurrence of words within a five word window with the word excellent (analogously with the word terrible for negative) to calculate the PMI score. This is potentially less accurate but requires no supervi"
L16-1009,D15-1200,0,0.0153777,"eneral domain dataset is a 700MB raw text file after cleaning it, while restaurants and laptop dataset only weight 28 and 40 MB respectively. General domain datasets, like the whole Wikipedia data or News dataset from online newspapers, capture very well general syntactic and semantic regularities. However, to capture in-domain word polarities smaller domain focused dataset might work better (Garc´ıa-Pablos et al., 2015). Also notice that at the time of writing this paper, there are appearing a lot of different techniques to calculate word embeddings that could work better than plain Word2Vec(Li and Jurafsky, 2015; Rothe et al., 2016), but due to their recent apparition are not employed in these experiments. slow outstanding fantastic amazing exceptional awesome top notch great superb incredible wonderful terrible awful sucked horrid poor sucks atrocious lousy horrific yuck spotty inattentive uncaring painfully neglectful lax slower inconsistent uneven iffy Obtained enwik9.zip from outstanding exceptional awesome incredible excelent amazing excellant fantastic terrific superb terrible deplorable awful abysmal poor horrid lousy whining horrendous unprofessional counterintuitive painfully unstable sluggi"
L16-1009,maks-etal-2014-generating,0,0.0185762,"vity. In (Baccianella et al., 2010) version 3.0 of SentiWordNet is introduced with improvements like a random walk approach in the WordNet graph to calculate the SO of the synsets. In (Agerri and Garcia, 2009) another system is introduced, Q-WordNet, which expands the polarities of the WordNet synsets using lexical relations like synonymy. In (Guerini et al., 2013) the authors propose and compare different approaches based SentiWordNet to improve the polarity determination of the synsets. Other authors try different bootstrapping approaches and evaluate them on WordNet of different languages (Maks et al., 2014; Vicente et al., 2014). A problem with the approaches based on resources like WordNet is that they rely on the availability and quality of those resources for a new languages. Being a general resource, WordNet also fails to capture domain dependent semantic orientations. Likewise other approaches using common dictionaries do not take into account the shifts between domains (Ramos and Marques, 2005). Other methods calculate the SO of the words directly from text. In (Hatzivassiloglou and McKeown, 1997) the authors model the corpus as a graph of adjectives joined by conjunctions. Then, they gen"
L16-1233,P98-1013,0,0.346679,"ecification of the concepts, but these axioms do not always provide the information relevant and specific for our domain. Also, SUMO needs a so´ phisticated reasoning system to be productive (Alvez et al., 2015). Furthermore, such ontologies need to be integrated with semantic parsing systems that deal with expressions on natural language. We therefore decided to develop a new ontology for modelling events and their implications that is tailored to a semantic parsing system for text. The Predicate Matrix (L´opez de Lacalle et al., 2014) integrates predicate and role information from FrameNet (Baker et al., 1998), VerbNet (Kipper et al., 2000), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and WordNet (Fellbaum, 1998). This resource is used to assign role and predicate annotations at sentence level. All classes and roles in ESO are fed back into the Predicate Matrix. As such the ontology provides an additional layer of annotations in text that allow for inferencing over events and implications. The remainder of this paper is organized as follows. Section 2. presents the ontological meta model and the content of ESO. Section 3. describes the Predicate Matrix and the integration with ESO"
L16-1233,W04-2214,0,0.142993,"Missing"
L16-1233,gonzalez-agirre-etal-2012-multilingual,1,0.890287,"Missing"
L16-1233,kipper-etal-2006-extending,0,0.0104722,"sent the implied situations of each event. Modeling of event implications allows for extracting sequences of states and changes over time regardless of this information being directly expressed in text, or inferred by a reasoner. The model targets interpretations of situations rather than the semantics of predicates per se. Events are interpreted as situations using RDF, taking all event components into account. Hence, the ontology and the linked resources need to be considered from the perspective of this interpretation model. Lexicons that define implications of events, for example VerbNet (Kipper et al., 2006), are rare and usually focus on the meaning of verbs in isolation. However, lexical structures do not make explicit how the meaning of a verb needs to be combined with other event components, such as the participants and the temporal properties for the purpose of semantic parsing. We therefore follow an ontological approach to interpret situations on the basis of the event components to make these implications explicit. Though some research on deductive reasoning over Frame annotated text (Scheffczyk et al., 2006) and defining pre and post situations of predicates exist (Im and Pustejovsky, 20"
L16-1233,lopez-de-lacalle-etal-2014-predicate,1,0.929586,"Missing"
L16-1233,L16-1423,1,0.854358,"Missing"
L16-1233,W04-2705,0,0.101321,"and specific for our domain. Also, SUMO needs a so´ phisticated reasoning system to be productive (Alvez et al., 2015). Furthermore, such ontologies need to be integrated with semantic parsing systems that deal with expressions on natural language. We therefore decided to develop a new ontology for modelling events and their implications that is tailored to a semantic parsing system for text. The Predicate Matrix (L´opez de Lacalle et al., 2014) integrates predicate and role information from FrameNet (Baker et al., 1998), VerbNet (Kipper et al., 2000), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and WordNet (Fellbaum, 1998). This resource is used to assign role and predicate annotations at sentence level. All classes and roles in ESO are fed back into the Predicate Matrix. As such the ontology provides an additional layer of annotations in text that allow for inferencing over events and implications. The remainder of this paper is organized as follows. Section 2. presents the ontological meta model and the content of ESO. Section 3. describes the Predicate Matrix and the integration with ESO. In Section 4. we provide an overview of the Predicate Matrix and ESO in our document collect"
L16-1233,L16-1699,1,0.881072,"Missing"
L16-1233,J05-1004,0,0.100043,"ovide the information relevant and specific for our domain. Also, SUMO needs a so´ phisticated reasoning system to be productive (Alvez et al., 2015). Furthermore, such ontologies need to be integrated with semantic parsing systems that deal with expressions on natural language. We therefore decided to develop a new ontology for modelling events and their implications that is tailored to a semantic parsing system for text. The Predicate Matrix (L´opez de Lacalle et al., 2014) integrates predicate and role information from FrameNet (Baker et al., 1998), VerbNet (Kipper et al., 2000), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and WordNet (Fellbaum, 1998). This resource is used to assign role and predicate annotations at sentence level. All classes and roles in ESO are fed back into the Predicate Matrix. As such the ontology provides an additional layer of annotations in text that allow for inferencing over events and implications. The remainder of this paper is organized as follows. Section 2. presents the ontological meta model and the content of ESO. Section 3. describes the Predicate Matrix and the integration with ESO. In Section 4. we provide an overview of the Predicate Matrix"
L16-1233,2016.gwc-1.51,1,0.502091,"ESO assertions are correct. Keywords: Ontology, Semantic Role Labeling, Text Mining, Semantic Web 1. Introduction In this paper, we present the Event and Implied Situation Ontology (ESO) Version 2, that is matched with the Predicate Matrix (PM). Both resources rely on Semantic Role Labeling (SRL) descriptions and are used to detect and abstract over events, their participants and event implications in a large document collection about ten years of global automotive industries, thus favoring the construction of large event-centric knowledge graphs (Rospocher et al., to appear). ESO Version 2 (Segers et al., 2016) is a newly developed domain ontology to enhance the extraction and linking of dynamic and static events and their implications in text. Such a chain of changes and states and their implied situations is presented in Figure 1. Here, the boxes represent various event expressions about John’s employment while the ovals represent the implied situations of each event. Modeling of event implications allows for extracting sequences of states and changes over time regardless of this information being directly expressed in text, or inferred by a reasoner. The model targets interpretations of situation"
L16-1268,S10-1013,1,0.943639,"Missing"
L16-1268,J14-1003,1,0.901281,"Missing"
L16-1268,D14-1110,0,0.091557,"Missing"
L16-1268,H93-1061,0,0.83636,"Missing"
L16-1268,S13-2040,0,0.221915,"Missing"
L16-1268,S01-1005,0,0.0195329,"Missing"
L16-1268,W97-0108,0,0.195731,"s. One of the reasons why IMS is performing so well is that it partly overcomes the knowledge bottleneck problem by making use of parallel data from two different languages and thus generating more training data for the LFS. Other supervised approaches focus on improving the performance of the mapping function by reducing the number of possible classes. The rationale behind this approach is to reduce the knowledge bottleneck problem by combining the training data from related senses. Good results have been reported for these approaches on WordNet Domains, Supersenses, and Base Level Concepts (Peh and Ng, 1997; Izquierdo et al., 2007). In this paper, we propose an approach to modify the output from a WSD system using a MFS classifier. We do not attempt to overcome the knowledge bottleneck problem, but we try to correct the systems for their MFS bias by reducing the mapping function to MFS and LFS. 3. System Description The starting point for our system is the output from a WSD system. We report the results for the UKB and the IMS systems. A feature set containing mostly static features, focusing predominantly on frequency and domain properties of lemmas, is combined with the WSD output and fed into"
L16-1268,S07-1016,0,0.0785243,"Missing"
L16-1268,P15-1173,0,0.0826637,"Missing"
L16-1268,W04-0811,0,0.0750197,"Missing"
L16-1268,steinberger-etal-2012-jrc,0,0.0299295,"B+C IMS IMS+C 65.9 66.1 60.6 59.4 65.9 66.1 60.6 59.4 25.3 36.3 20.9 31.5 sval2015 relation between the system sense entropy and the sense entropy in Semcor. In addition, we use mostly static features, which are the same in training, development, and test for a particular lemma. These features include TF-IDF, part of speech, number of senses and system sense entropy, as well as WordNet domains, and the WordNet Supersense. Finally, we use one feature that is dependent on the corpus used in training, development, and test. This feature makes use of the domain classifier JRC EuroVoc Indexer JEX (Steinberger et al., 2012). We compare the domain distribution of Semcor to the domain distribution of the instances of a lemma. Finally, the output from the MFS classifier and a WSD system are combined to obtain the final sense assignment. The algorithm is visualized in Algorithm 1. UKB UKB+C IMS IMS+C 68.5 69.5 67.1 64.8 67.1 68.1 65.8 63.6 20.8 27.3 17.3 23.5 Table 2: In this Table, the WSD results are presented for the competitions sval2013 and sval2015, respectively. Three measures are used to show the performance of UKB and IMS WSD systems with the MFS classifier (+C) and without. Precision (Pwsd) and Recall (Rws"
L16-1268,P10-4014,0,0.127711,"e initialized using the knowledge from the graph. Next, the node weights are updated with respect to the knowledge found in the local context of a target word, resulting in context-dependent PageRank. In general, unsupervised approaches do not suffer greatly from the knowledge bottleneck problem. However, recent work has shown that they also have a strong bias towards the MFS (Calvo and Gelbukh, 2015). Supervised approaches attempt to maximize the performance of the mapping function by training word and sense experts using (mostly) sense-labeled training data. The It makes sense (IMS) system (Zhong and Ng, 2010) is one of the best performing supervised approaches, which makes use of linear support vector machines with mostly local contextual features. The biggest challenge for supervised approaches is the reliance on manually sense-tagged training data, which is expensive and time-consuming to create, especially for high polysemous words. One of the reasons why IMS is performing so well is that it partly overcomes the knowledge bottleneck problem by making use of parallel data from two different languages and thus generating more training data for the LFS. Other supervised approaches focus on improvi"
L16-1268,S15-2049,0,\N,Missing
L16-1423,aparicio-etal-2008-ancora,0,0.0506835,"Missing"
L16-1423,E12-1059,0,0.0199728,"with an overview of the methods developed to build the resource. In Section 4., we explain how we have extended the Predicate Matrix with nominal predicates and for multiple languages, and we present the resulting version 1.3. We finalize with some conclusions and future work in Section 5.. 2. Related Work Several previous works have been focused on the integration of resources targeted at knowledge about nouns and named entities. Well known examples are YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2010) or UBY (Gurevych et al., 2012). However, less attention has been paid to the integration of existing models for verbs and predicates (Burchardt et al., 2005; Fellbaum and Baker, 2013). Regarding predicate information, SemLink (Palmer, 2009) has focused on mapping complementary lexical resources that associate semantic information to verbal predicates in a sentence. The resources integrated in SemLink vary in the detail and abstraction level of the encoded semantic information associated to each predicate. SemLink aims at unifying all these lexical resources. First by providing type-totype mappings between the lexical units"
L16-1423,lopez-de-lacalle-etal-2014-predicate,1,0.840949,"Missing"
L16-1423,W14-0150,1,0.820261,"Missing"
L16-1423,W04-2705,0,0.719954,"c resources that are part of the Predicate Matrix 1.2 only contain an English verbal lexicon. But, if any other semantic resource is linked to any of the resources included in the Predicate Matrix the integration of the new resource can be done straightforwardly. In this paper we demonstrate this feature by extending the Predicate Matrix to English nominal predicates and to Spanish, Basque and Catalan languages (cf. Section 4.) and we also present the Predicate Matrix 1.31 that includes nominalizations and multilingual predicates. In the case of the English nominal predicates, we use NomBank (Meyers et al., 2004). The projection to Spanish and Catalan is possible thanks to the AnCora (Taul´e et al., 2008a) corpus and the AnCoraVerb (Aparicio et al., 2008) and AnCora-Nom semantic resources. Finally, the Basque Verb Index (BVI) (Estarrona et al., 2015) corpus-based lexicon is used in the case of Basque. This paper is organized as follows. First, Section 2. includes a summary of the related work. Section 3. presents a brief description of the Predicate Matrix 1.2 with an overview of the methods developed to build the resource. In Section 4., we explain how we have extended the Predicate Matrix with nomin"
L16-1423,P10-1023,0,0.0261062,"ription of the Predicate Matrix 1.2 with an overview of the methods developed to build the resource. In Section 4., we explain how we have extended the Predicate Matrix with nominal predicates and for multiple languages, and we present the resulting version 1.3. We finalize with some conclusions and future work in Section 5.. 2. Related Work Several previous works have been focused on the integration of resources targeted at knowledge about nouns and named entities. Well known examples are YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), DBpedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2010) or UBY (Gurevych et al., 2012). However, less attention has been paid to the integration of existing models for verbs and predicates (Burchardt et al., 2005; Fellbaum and Baker, 2013). Regarding predicate information, SemLink (Palmer, 2009) has focused on mapping complementary lexical resources that associate semantic information to verbal predicates in a sentence. The resources integrated in SemLink vary in the detail and abstraction level of the encoded semantic information associated to each predicate. SemLink aims at unifying all these lexical resources. First by providing type-totype map"
L16-1423,J05-1004,0,0.358549,"ing from the integration of multiple sources of predicate information including FrameNet, VerbNet, PropBank and WordNet. This new version of the Predicate Matrix has been extended to cover nominal predicates by adding mappings to NomBank. Similarly, we have integrated resources in Spanish, Catalan and Basque. As a result, the Predicate Matrix 1.3 provides a multilingual lexicon to allow interoperable semantic analysis in multiple languages. Keywords: Lexicon, Semantics, Multilinguality 1. Introduction Predicate resources such as VerbNet (Kipper, 2005), FrameNet (Baker et al., 1997), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998) offer individually interesting characteristics not provided by their alternatives. Unfortunately, these semantic resources are developed independently and they are not fully integrated in a common framework. Thus, a common semantic infraestructure would allow the interoperability among all these lexicons. With this aim, we developed the Predicate Matrix (L´opez de Lacalle et al., 2014b; L´opez de Lacalle et al., 2014a; L´opez de Lacalle et al., 2016 fc), a lexical resource resulting from the automatic integration of multiple sources of predicate information includ"
L16-1423,L16-1233,1,0.801186,"ew multilingual version of the Predicate Matrix we can also obtain the same event representation for a Spanish translation of the English sentence Steve Jobs ofreci´o el lunes su conferenTable 13: Some examples of mappings in the Predicate Matrix 1.3 cia inaugural de la WWDC en el Moscone Center. That is, that Steve Jobs corresponds to the Communicator role of a Communication frame according to FrameNet. This same process was performed with Dutch and Italian role resources. Furthermore, in NewsReader the events and their participants are also aligned to the Event and Situation Ontology (ESO) (Segers et al., 2016). ESO formalizes with preconditions and post-conditions events and roles and reuses existing resources such as WordNet, SUMO (Niles and Pease, 2001) and FrameNet. 5. Conclusion Building large and rich predicate models takes a great deal of expensive manual effort. Furthermore, the same effort should be invested for each different language. Predicate resources such as VerbNet, FrameNet, PropBank and WordNet offer individually interesting characteristics not provided by their alternatives. Unfortunately, these semantic resources are developed independently and they are not fully integrated in a"
L16-1423,taule-etal-2008-ancora,0,0.0910173,"Missing"
L16-1423,vossen-etal-2014-newsreader,1,0.836728,"as a side effect the enrishment of the Spanish, Catalan and Basque wordnets integrated into the MCR. Table 14 presents the total number of new senses aligned to the different wordnets. Interestingly, some additional word senses are also created for the English WordNet. We plan to include this new word sense aligments in future releases of the MCR. Table 9: Examples of argument structures defined in NomBank for the predicate “sale.01”. 4.3.2. The Predicate Matrix in NewsReader The Predicate Matrix 1.3 is part of a multilingual event detection system implemented within the NewsReader project5 (Vossen et al., 2014). The NewsReader project develops advanced technology to process daily news streams in 4 languages (Agerri et al., 2016), extracting what happened, when and where it happened and who was involved. With this purpose, the event detection system is a pipeline which contains a set of modules to perform various NLP tasks. Among others, the system has a semantic role labeling module that automatically annotates semantic information based on PropBank. Thanks to the Predicate Matrix, our pipelines also obtain the equivalent annotations in 2665 5 http://www.newsreader-project.eu &lt;lexentry lemma=”venta”"
L16-1423,P98-1013,0,\N,Missing
L16-1423,C98-1013,0,\N,Missing
L18-1322,agerri-etal-2014-ixa,1,0.932168,"itate knowledge discovery, exchange, and reuse by finding relevant terms and semantic structure in those texts. This paper presents a preliminary application that enriches EHRs with links to the Unified Medical Language System (UMLS)1 , a multilingual repository of biomedical terminologies. The tool is multilingual and cross-lingual by design, but we first focus on Spanish EHR processing because there is no existing tool for this language and for this specific purpose. We propose a sequential pipeline that retrieves mapping candidates from an indexed UMLS Metathesaurus, uses the IXA pipeline (Agerri et al., 2014) for basic language processing and UKB (Agirre and Soroa, 2009) for word sense disambiguation (WSD). In addition to the pipeline itself, this paper also presents a demonstration interface for the tool that will be available on-line2 . 2. Related Work Biomedical term normalization is a long-established research field in English-speaking countries where terminological resources and basic-processing tools for the biomedical domain and this language have been available for decades. Thus, there already exist several mature applications that are being effectively exploited for different purposes and"
L18-1322,E09-1005,0,0.0248353,"levant terms and semantic structure in those texts. This paper presents a preliminary application that enriches EHRs with links to the Unified Medical Language System (UMLS)1 , a multilingual repository of biomedical terminologies. The tool is multilingual and cross-lingual by design, but we first focus on Spanish EHR processing because there is no existing tool for this language and for this specific purpose. We propose a sequential pipeline that retrieves mapping candidates from an indexed UMLS Metathesaurus, uses the IXA pipeline (Agerri et al., 2014) for basic language processing and UKB (Agirre and Soroa, 2009) for word sense disambiguation (WSD). In addition to the pipeline itself, this paper also presents a demonstration interface for the tool that will be available on-line2 . 2. Related Work Biomedical term normalization is a long-established research field in English-speaking countries where terminological resources and basic-processing tools for the biomedical domain and this language have been available for decades. Thus, there already exist several mature applications that are being effectively exploited for different purposes and by different organizations as of today. In what follows, we pr"
L18-1322,carreras-etal-2004-freeling,0,0.214082,"Missing"
L18-1322,L16-1470,0,0.0609779,"Missing"
L18-1367,agerri-etal-2014-ixa,1,0.854455,"those type of annotations not developed within its toolchain. Most notably, it includes integration of wikification and Named Entity Disambiguation (NED) via DBPedia Spotlight (Mendes et al., 2011) as well as graph-based Word Sense Disambiguation (Agirre et al., 2014). Summarizing, the new set of NLP tools for Galician implemented within IXA pipes consists of the following modules: • ixa-pipe-tok: A rule-based tokenizer and sentence segmenter. NLP Tools In order to develop new linguistic processors using the resources described in the previous section, we decided to try the IXA pipes tools7 (Agerri et al., 2014). The aim of IXA pipes is to provide a modular set of ready to use Natural Language Processing (NLP) tools. Apart from being easy to train and deploy, they are also a good fit for our current aim of providing new tools for Galician because every module but the tokenizer is machine learning based. In fact, IXA pipes tries to use the same approach across NLP tasks in order to create robust processors both across domains and languages. This strategy has proven to be very successful for Named Entity Recognition and Classification (NER) (Agerri and Rigau, 2016) and Opinion Target Extraction (OTE) ("
L18-1367,J14-1003,0,0.0607024,"Missing"
L18-1367,J92-4003,0,0.651221,"llow feature set, avoiding any linguistic motivated features, with the objective of removing any reliance on costly extra gold annotations apart from the target task (POS, lemmas, NER) and/or cascading errors if automatic language processors are used. IXA pipes modules consist of: (i) Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching. Specifically, IXA pipes implements, on top of the local features, a combination of three word representation features: (i) Brown (Brown et al., 1992) clusters, taking the 4th, 8th, • ukb-naf: UKB graph-based Word Sense Disambiguation. 3.2. Galician language already had a previous version of the DBpedia Spotlight10 that was implemented together with the official server of the Galician DBpedia (Solla Portela and G´omez Guinovart, 2016), but it used the Lucene version (Mendes et al., 2011). A new, better performing, generative model (Daiber et al., 2013) for DBpedia Spotlight has been created by configuring the Galician language in model-quickstarter11 in order to handle Wikification and Named Entity Disambiguation with IXA pipes, using the m"
L18-1367,E03-1009,0,0.106574,"lician (TALG Research Group, 2018) was manually annotated on a subsection of the CTG corpus consisting of 202,334 tokens in 8,137 sentences (from the news and ecology and environmental sciences domains). The CoNLL guidelines for annotation were followed (Tjong Kim Sang and De Meulder, 2003). This resulted in an inventory of 4 named entity classes distributed as follows: 1,293 persons (PER), 3,183 organizations (ORG), 2,616 locations (LOC) and 1,375 miscellaneous entities (MISC). From this corpus 162K tokens are used for training and 41K for test. 3. 12th and 20th node in the path; (ii) Clark (Clark, 2003) clusters and, (iii) Word2vec (Mikolov et al., 2013) clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm. The implementation of the clustering features looks for the cluster class of the incoming token in one or more of the clustering lexicons induced following the three methods listed above. If found, then we add the class as a feature. The Brown clusters only apply to the token related features, which are duplicated. The word representation features are combined and stacked from features induced over different data sources. For this work the new G"
L18-1367,W02-1001,0,0.213791,"e-pos); ixa-pipe-pos is complemented by the dictionaries described in the previous section for dictionary-based lemmatization and multiword detection. For efficiency, these dictionaries are deployed as finite state automata based on Morfologik9 . • ixa-pipe-nerc: A state of the art NER tagger. • ixa-pipe-wikify: Wikification tool based on DBpedia Spotlight. • ixa-pipe-ned: A NED tool based on DBpedia Spotlight. The NED uses the entities spotted by ixa-pipenerc as input to perform the disambiguation. Semi-supervised approach IXA pipes learns supervised models based on the Perceptron algorithm (Collins, 2002). To avoid duplication of efforts, IXA pipes uses the Apache OpenNLP project implementation8 customized with its own features. By design, IXA pipes tools aim to establish a simple and shallow feature set, avoiding any linguistic motivated features, with the objective of removing any reliance on costly extra gold annotations apart from the target task (POS, lemmas, NER) and/or cascading errors if automatic language processors are used. IXA pipes modules consist of: (i) Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types o"
L18-1367,padro-stanilovsky-2012-freeling,0,0.252358,"Missing"
L18-1367,S15-2127,1,0.811221,"e aim of IXA pipes is to provide a modular set of ready to use Natural Language Processing (NLP) tools. Apart from being easy to train and deploy, they are also a good fit for our current aim of providing new tools for Galician because every module but the tokenizer is machine learning based. In fact, IXA pipes tries to use the same approach across NLP tasks in order to create robust processors both across domains and languages. This strategy has proven to be very successful for Named Entity Recognition and Classification (NER) (Agerri and Rigau, 2016) and Opinion Target Extraction (OTE) (San Vicente et al., 2015) benchmarks, both in out-of-domain and in-domain evaluations. 3.1. • ixa-pipe-pos: A statistical lemmatizer and POS tagger (ixa-pipe-pos); ixa-pipe-pos is complemented by the dictionaries described in the previous section for dictionary-based lemmatization and multiword detection. For efficiency, these dictionaries are deployed as finite state automata based on Morfologik9 . • ixa-pipe-nerc: A state of the art NER tagger. • ixa-pipe-wikify: Wikification tool based on DBpedia Spotlight. • ixa-pipe-ned: A NED tool based on DBpedia Spotlight. The NED uses the entities spotted by ixa-pipenerc as i"
L18-1367,W03-0419,0,0.297813,"Missing"
L18-1557,W15-1007,1,0.838381,"as it can be seen Tables 6 and 7. en de it es Precision 70.00 68.40 67.03 55.66 Recall 60.34 39.24 62.45 59.69 F1 64.81 49.87 64.66 57.60 Table 6: Evaluating Gold-standard trained CoNLL and Evalita models on Europarl test. Still, and even though our first results are quite promising, we should note that the results of the automatically generated models are much lower than those established by the upper-bound. 5. Related Work Traditionally, there are many studies and works exploring the contribution of semantic information or features with the aim of improving Machine Translation (Koehn, 2010; Artetxe et al., 2015) but the reverse has been rather uncommon. Among previous works using parallel texts to automatically induce linguistic processors, most of them focus on inducing Part of Speech taggers (Yarowsky et al., 2001; Precision 70.30 78.87 75.14 80.29 Recall 68.01 63.94 53.41 53.42 F1 69.14 70.62 62.44 64.16 Table 7: Evaluating models trained on automatically projected data. Ganchev and Das, 2013; T¨ackstr¨om et al., 2012; Fossum and Abney, 2005) although a very few of them worked on semantic tasks such as Named Entity Recognition (NER) (Yarowsky et al., 2001; Zhang et al., 2016) and Semantic Role Lab"
L18-1557,P11-1061,0,0.075948,"target language. Therefore, to our knowledge, no previous approach aims at doing transfer of semantic annotations as we propose in this paper. These previous works based on projection of annotations have shown that the projected labels can result in a very noisy training set in the target language. Various methods have been applied to address this problem, including smoothing techniques (Yarowsky et al., 2001) and the combination of token-level and type-level constraints to recalculate the probability distribution of the labels in a CRF for Part of Speech tagging (T¨ackstr¨om et al., 2012). (Das and Petrov, 2011) use the projected labels as contraints in a Posterior Regularization framework and (Ganchev and Das, 2013) extend this work by training directly discriminative models via cross lingual projection with Posterior Regularization. Finally, instead of using total counts of labels of a class to enforce the constraints, (Wang and Manning, 2014) define expectation constraints at token level for NERC. Closest to our work, Zhang et al. (2016) generate a highconfidence annotation set using strict rules on parallel corpora in order to project the Named Entity information from the source to the target. Th"
L18-1557,I05-1075,0,0.13152,"Missing"
L18-1557,D13-1205,0,0.0950755,"d by the upper-bound. 5. Related Work Traditionally, there are many studies and works exploring the contribution of semantic information or features with the aim of improving Machine Translation (Koehn, 2010; Artetxe et al., 2015) but the reverse has been rather uncommon. Among previous works using parallel texts to automatically induce linguistic processors, most of them focus on inducing Part of Speech taggers (Yarowsky et al., 2001; Precision 70.30 78.87 75.14 80.29 Recall 68.01 63.94 53.41 53.42 F1 69.14 70.62 62.44 64.16 Table 7: Evaluating models trained on automatically projected data. Ganchev and Das, 2013; T¨ackstr¨om et al., 2012; Fossum and Abney, 2005) although a very few of them worked on semantic tasks such as Named Entity Recognition (NER) (Yarowsky et al., 2001; Zhang et al., 2016) and Semantic Role Labelling (SRL) (Pad´o and Lapata, 2009). Furthermore, almost every previous approach is based on one-to-one projections using only one language pair to induce the linguistic processors. As far as we know, there are only two exceptions: Yarowsky et al. (2001) use bridging between two languages to perform lemmatization in a third target language, and Fossum and Abney (2005) train multiple POS"
L18-1557,2005.mtsummit-papers.11,0,0.0784197,"c (Agerri and Rigau, 2016). It is designed to work robustly across languages and datasets and it obtains state of the art results for the languages used in this study. We also use the following corpora: 1. Gold standard data for training the initial ixa-pipe-nerc models for the source languages. CoNLL 2002 and 2003 for German, English and Spanish, and Evalita 2009 for Italian. Both CoNLL and Evalita annotate the three entity types (location, organization and person) that we will use to induce our training data. 2. The Europarl parallel corpus on which to perform the cross-lingual projections (Koehn, 2005), word-aligned using Giza++ (Och and Ney, 2000) and divided into a training and a test set. 3. The Europarl gold-standard test set is a new manuallyannotated evaluation set taken from the Europarl. The test set contains 800 sentences manually annotated using the three entity types and following the CoNLL 2002 and 2003 guidelines for the 4 languages used in this paper. 4. Back-off corpora to resolve ties in the projection step. The idea is to compute the most frequent tag of a token in a large NER annotated resource. Thus, in case of ties during the annotation projection the most frequent entit"
L18-1557,J10-4005,0,0.0340331,"for German, as it can be seen Tables 6 and 7. en de it es Precision 70.00 68.40 67.03 55.66 Recall 60.34 39.24 62.45 59.69 F1 64.81 49.87 64.66 57.60 Table 6: Evaluating Gold-standard trained CoNLL and Evalita models on Europarl test. Still, and even though our first results are quite promising, we should note that the results of the automatically generated models are much lower than those established by the upper-bound. 5. Related Work Traditionally, there are many studies and works exploring the contribution of semantic information or features with the aim of improving Machine Translation (Koehn, 2010; Artetxe et al., 2015) but the reverse has been rather uncommon. Among previous works using parallel texts to automatically induce linguistic processors, most of them focus on inducing Part of Speech taggers (Yarowsky et al., 2001; Precision 70.30 78.87 75.14 80.29 Recall 68.01 63.94 53.41 53.42 F1 69.14 70.62 62.44 64.16 Table 7: Evaluating models trained on automatically projected data. Ganchev and Das, 2013; T¨ackstr¨om et al., 2012; Fossum and Abney, 2005) although a very few of them worked on semantic tasks such as Named Entity Recognition (NER) (Yarowsky et al., 2001; Zhang et al., 2016"
L18-1557,N06-1014,0,0.0848064,"d is generally inefficiently slow and, in most cases, unaffordable in terms of human resources and economic costs. Instead, we would like to be able to use already available semantic processors and texts in other languages to get a good statistical model for a new target language. Our method leverages existing semantic processors and annotations to overcome the lack of annotation data for a given language. The intuition is to transfer or project semantic annotations, from multiple sources to a target language, by statistical word alignment methods applied to parallel texts (Och and Ney, 2000; Liang et al., 2006). The projected annotations could then be used to automatically generate semantic processors for the target language. In this way we would be able to provide semantic processors without training data for a given language. Thus, this means that the problem can be decomposed into two smaller inter-related ones: (i) How to project semantic annotations across languages via parallel texts with a sufficient acceptable quality to train semi- or weakly-supervised semantic processors and (ii) how to effectively leverage the (potentially noisy) projected annotations to induce robust statistical models t"
L18-1557,N12-1052,0,0.124453,"Missing"
L18-1557,W03-0419,0,0.410018,"Missing"
L18-1557,W02-2024,0,0.5864,"Missing"
L18-1557,Q14-1005,0,0.0174165,"address this problem, including smoothing techniques (Yarowsky et al., 2001) and the combination of token-level and type-level constraints to recalculate the probability distribution of the labels in a CRF for Part of Speech tagging (T¨ackstr¨om et al., 2012). (Das and Petrov, 2011) use the projected labels as contraints in a Posterior Regularization framework and (Ganchev and Das, 2013) extend this work by training directly discriminative models via cross lingual projection with Posterior Regularization. Finally, instead of using total counts of labels of a class to enforce the constraints, (Wang and Manning, 2014) define expectation constraints at token level for NERC. Closest to our work, Zhang et al. (2016) generate a highconfidence annotation set using strict rules on parallel corpora in order to project the Named Entity information from the source to the target. The resulting annotated bitext is then used to train a LSTM model. They evaluate their work with respect to a baseline consisting of the projected tags via automatic word alignments. The results show that the word alignment method is much worse than the bitext trained LSTM model. It should be noted that they do not explain how the annotatio"
L18-1557,H01-1035,0,0.59009,"Europarl test. Still, and even though our first results are quite promising, we should note that the results of the automatically generated models are much lower than those established by the upper-bound. 5. Related Work Traditionally, there are many studies and works exploring the contribution of semantic information or features with the aim of improving Machine Translation (Koehn, 2010; Artetxe et al., 2015) but the reverse has been rather uncommon. Among previous works using parallel texts to automatically induce linguistic processors, most of them focus on inducing Part of Speech taggers (Yarowsky et al., 2001; Precision 70.30 78.87 75.14 80.29 Recall 68.01 63.94 53.41 53.42 F1 69.14 70.62 62.44 64.16 Table 7: Evaluating models trained on automatically projected data. Ganchev and Das, 2013; T¨ackstr¨om et al., 2012; Fossum and Abney, 2005) although a very few of them worked on semantic tasks such as Named Entity Recognition (NER) (Yarowsky et al., 2001; Zhang et al., 2016) and Semantic Role Labelling (SRL) (Pad´o and Lapata, 2009). Furthermore, almost every previous approach is based on one-to-one projections using only one language pair to induce the linguistic processors. As far as we know, there"
L18-1557,C16-1045,0,0.0988997,"ation (Koehn, 2010; Artetxe et al., 2015) but the reverse has been rather uncommon. Among previous works using parallel texts to automatically induce linguistic processors, most of them focus on inducing Part of Speech taggers (Yarowsky et al., 2001; Precision 70.30 78.87 75.14 80.29 Recall 68.01 63.94 53.41 53.42 F1 69.14 70.62 62.44 64.16 Table 7: Evaluating models trained on automatically projected data. Ganchev and Das, 2013; T¨ackstr¨om et al., 2012; Fossum and Abney, 2005) although a very few of them worked on semantic tasks such as Named Entity Recognition (NER) (Yarowsky et al., 2001; Zhang et al., 2016) and Semantic Role Labelling (SRL) (Pad´o and Lapata, 2009). Furthermore, almost every previous approach is based on one-to-one projections using only one language pair to induce the linguistic processors. As far as we know, there are only two exceptions: Yarowsky et al. (2001) use bridging between two languages to perform lemmatization in a third target language, and Fossum and Abney (2005) train multiple POS taggers from monolingual source data and combine their annotations to project them to a given target language. Therefore, to our knowledge, no previous approach aims at doing transfer of"
L18-1723,2018.gwc-1.4,1,0.167964,"Missing"
L18-1723,alvez-etal-2008-complete,1,0.72548,"2001) are not free of errors and inconsistencies. Unfortunately, improving, revising and correcting such large knowledge bases is a never-ending task that has been mainly carried out manually. A few automatic approaches have also been applied focusing on checking certain structural properties on WordNet e.g. Daud´e et al. (2003) and Richens (2008) or using automated theorem provers on ´ SUMO e.g. Horrocks and Voronkov (2006) and Alvez et al. (2012). Just a few more have studied automatic ways to validate the knowledge content encoded in these resources ´ by cross-checking them. For instance, Alvez et al. (2008) exploit the EuroWordNet Top Ontology (Rodr´ıguez et al., 1998) and its mapping to WordNet for detecting many ontological conflicts and inconsistencies in the WordNet nominal hierarchy. ´ In Alvez et al. (2017), we proposed a method for the automatic creation of competency questions (CQs) (Gr¨uninger and Fox, 1995), which enabled to evaluate the competency of SUMO-based ontologies. Our proposal was based on several predefined question patterns (QPs) that were instantiated using information from WordNet and its mapping into SUMO (Niles and Pease, 2003). In addition, we described an application"
L18-1723,C08-1092,0,0.0338213,"ion of missing information or inconsistencies among these resources. Keywords: Meronymy, Knowledge validation, Automated Theorem Proving 1. Introduction Despite being created manually, knowledge resources such as WordNet (Fellbaum, 1998) and SUMO (Niles and Pease, 2001) are not free of errors and inconsistencies. Unfortunately, improving, revising and correcting such large knowledge bases is a never-ending task that has been mainly carried out manually. A few automatic approaches have also been applied focusing on checking certain structural properties on WordNet e.g. Daud´e et al. (2003) and Richens (2008) or using automated theorem provers on ´ SUMO e.g. Horrocks and Voronkov (2006) and Alvez et al. (2012). Just a few more have studied automatic ways to validate the knowledge content encoded in these resources ´ by cross-checking them. For instance, Alvez et al. (2008) exploit the EuroWordNet Top Ontology (Rodr´ıguez et al., 1998) and its mapping to WordNet for detecting many ontological conflicts and inconsistencies in the WordNet nominal hierarchy. ´ In Alvez et al. (2017), we proposed a method for the automatic creation of competency questions (CQs) (Gr¨uninger and Fox, 1995), which enabled"
laparra-etal-2012-mapping,cuadros-etal-2010-integrating,1,\N,Missing
laparra-etal-2012-mapping,alvez-etal-2008-complete,1,\N,Missing
laparra-etal-2012-mapping,W11-0129,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,E09-1005,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,D08-1048,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,W08-2208,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,P10-1023,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,P98-1013,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,C98-1013,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,R09-1039,1,\N,Missing
lopez-de-lacalle-etal-2014-predicate,E12-1059,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,P06-1117,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,J05-1004,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,gonzalez-agirre-etal-2012-multilingual,1,\N,Missing
lopez-de-lacalle-etal-2014-predicate,W14-0150,1,\N,Missing
lopez-de-lacalle-etal-2014-predicate,erk-pado-2004-powerful,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,taule-etal-2008-ancora,0,\N,Missing
lopez-de-lacalle-etal-2014-predicate,P13-1116,1,\N,Missing
P13-1116,P98-1013,0,0.04206,"r arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that exte"
P13-1116,W09-1206,0,0.0458694,"Missing"
P13-1116,W05-0620,0,0.127345,"Missing"
P13-1116,S10-1059,0,0.167107,"on challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These"
P13-1116,W06-1670,0,0.323912,"c consistency. To determine the semantic coherence between the potential candidates and a predicate argument argn , we have exploited the selectional preferences in the same way as in previous SRL and implicit argument resolution works. First, we have designed a list of very general semantic categories. Second, we have semi-automatically assigned one of them to every predicate argument argn in PropBank and NomBank. For this, we have used the semantic annotation provided by the training documents of the CoNLL-2008 dataset. This annotation was performed automatically using the SuperSenseTagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses4 . We have also defined a mapping between the semantic classes provided by the SuperSenseTagger and our seven semantic categories (see Table 1 for more details). Then, we have acquired the most common categories of each predicate argument argn . ImpAr algorithm also uses the SuperSenseTagger over the documents to be processed from BNB to check if the candidate belongs to the expected semantic category of the implicit argument to be filled. Following the example above, [Quest Medical Inc] is tagged as an ORGANIZATION by the SuperSenseTa"
P13-1116,P87-1019,0,0.252488,"enerally applicable tools. This problem has become a main concern for many NLP tasks. This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al., 2010). In this work, we study the coherence of the predicate and argument realization in discourse. In particular, we have followed a similar approach to 1180 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1180–1189, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics the one proposed by Dahl et al. (1987) who filled the arguments of anaphoric mentions of nominal predicates using previous mentions of the same predicate. We present an extension of this idea assuming that in a coherent document the different ocurrences of a predicate, including both verbal and nominal forms, tend to be mentions of the same event, and thus, they share the same argument fillers. Following this approach, we have developed a deterministic algorithm that obtains competitive results with respect to supervised methods. That is, our system can be applied to any predicate without training data. The main contributions of t"
P13-1116,J12-4003,0,0.503719,"ploited independently from the predicate. 3 Datasets In our experiments, we have focused on the dataset developed in Gerber and Chai (2010, 2012). This dataset (hereinafter BNB which stands for ”Beyond NomBank”) extends existing predicate annotations for NomBank and ProbBank. BNB presented the first annotation work of implicit arguments based on PropBank and NomBank frames. This annotation was an extension of the standard training, development and testing sections of Penn TreeBank that have been typically used for SRL evaluation and were already annotated with PropBank and NomBank predicate 2 Gerber and Chai (2012) includes a set of 81 different features. structures. The authors selected a limited set of predicates. These predicates are all nominalizations of other verbal predicates, without sense ambiguity, that appear frequently in the corpus and tend to have implicit arguments associated with their instances. These constraints allowed them to model enough occurrences of each implicit argument in order to cover adequately all the possible cases appearing in a test document. For each missing argument position they went over all the preceding sentences and annotated all mentions of the filler of that ar"
P13-1116,P10-1160,0,0.128632,"Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that were already annotated for PropBank (Palmer et al., 2005) and NomBank. This allowed them to avoid the sparseness problems and generalize properly from the training set. The r"
P13-1116,P00-1065,0,0.131399,"Missing"
P13-1116,J02-3001,0,0.142626,"Missing"
P13-1116,W13-0111,0,0.364445,"system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit"
P13-1116,W09-1201,0,0.0634236,"Missing"
P13-1116,W13-0114,1,0.793753,"li and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBa"
P13-1116,J94-4002,0,0.499104,"antecedent can change every time the algorithm finds a filler with a greater salience. A damping factor is applied to reduce the salience of distant predicates. 4.2 Filling arguments without explicit antecedents Filling the implicit arguments of a predicate has been identified as a particular case of coreference, very close to pronoun resolution (Silberer and Frank, 2012). Consequently, for those implicit arguments that have not explicit antecedents, we propose an adaptation of a classic algorithm for deterministic pronoun resolution. This component of our algorithm follows the RAP approach (Lappin and Leass, 1994). When our algorithm needs to fill an implicit predicate argument without an explicit antecedent it considers a set of candidates within a window formed by the sentence of the predicate and the two previous sentences. Then, the algorithm performs the following steps: 3 Note that the algorithm could also consider sequences of closely related predicates. 2. Select those candidates that are semantically consistent with the semantic category of the implicit argument. 3. Assign a salience score to each candidate. 4. Sort the candidates by their proximity to the predicate of the implicit argument. 5"
P13-1116,S12-1001,0,0.531941,"ole labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEv"
P13-1116,W04-2705,0,0.402864,"Missing"
P13-1116,W08-2121,0,0.0571591,"Missing"
P13-1116,W13-0211,0,0.0430038,"ly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that wer"
P13-1116,J05-1004,0,0.0670568,"cific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that were already annotated for PropBank (Palmer et al., 2005) and NomBank. This allowed them to avoid the sparseness problems and generalize properly from the training set. The results of this system were far better than those obtained by the systems that faced the SemEval-2010 dataset. This works represent the deepest study so far of the features that characterizes the implicit arguments 2 . However, many of the most important features are lexically dependent on the predicate and cannot been generalized. Thus, specific annotations are required for each new predicate to be analyzed. All the works presented in this section agree that implicit arguments m"
P13-1116,P86-1004,0,0.832885,"d systems. We release an open source prototype implementing this algorithm1 . The paper is structured as follows. Section 2 discusses the related work. Section 3 presents in detail the data used in our experiments. Section 4 describes our algorithm for implicit argument resolution. Section 5 presents some experiments we have carried out to test the algorithm. Section 6 discusses the results obtained. Finally, section 7 offers some concluding remarks and presents some future research lines. 2 Related Work The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al. (1986). This work applied selectional restrictions together with coreference chains, in a very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1 http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently,"
P13-1116,D10-1048,0,0.0341924,"Missing"
P13-1116,S10-1065,0,0.0451979,"oblem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two syste"
P13-1116,W11-0908,0,0.0668704,"ed a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (201"
P13-1116,P91-1003,0,0.418982,"k. Section 3 presents in detail the data used in our experiments. Section 4 describes our algorithm for implicit argument resolution. Section 5 presents some experiments we have carried out to test the algorithm. Section 6 discusses the results obtained. Finally, section 7 offers some concluding remarks and presents some future research lines. 2 Related Work The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al. (1986). This work applied selectional restrictions together with coreference chains, in a very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1 http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that includ"
P13-1116,R11-1046,0,0.0759057,"nd thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand,"
P13-1116,S10-1008,0,0.490376,"very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1 http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tu"
P13-1116,H86-1011,0,\N,Missing
P13-1116,C98-1013,0,\N,Missing
P15-2059,S13-2015,0,0.0157058,"em can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semantic and discourse features. (Laokulrat et al., 2013) obtained the best results on TempEval 2013 annotating sentences with predicate-role structures, while (Mirza and Tonelli, 2014) affirm that using a simple feature set results in better performances. However, recent works like (Chambers et al., 2014) have pointed out that these tasks cover just a part of all the temporal relations that can be inferred from the documents. Furthermore, time-anchoring is just a part of the works presented above. Our approach aims to extend these strategies and it is based on other research lines Introduction Temporal relation extraction has been the topic of diff"
P15-2059,P13-1116,1,0.851686,"uly 26-31, 2015. 2015 Association for Computational Linguistics involving the extraction of implicit information (Palmer et al., 1986; Whittemore et al., 1991; Tetreault, 2002). Particularly, we are inspired by recent works on Implicit Semantic Role Labelling (ISRL) (Gerber and Chai, 2012) and very specially on the work by (Blanco and Moldovan, 2014) who adapted the ideas about ISRL to focus on modifiers, including arguments of time, instead of core arguments or roles. As the SemEval 2015 task 4 does not include any training data we decided to develop a deterministic algorithm of the type of (Laparra and Rigau, 2013) for ISRL. 3 poral awareness of an annotation (UzZaman and Allen, 2011) based on temporal closure graphs. In order to calculate the precision, recall and F1 score, the TimeLines are first transformed into a graph representation. For that, the time anchors are represented as TIMEX3 and the events are related to the corresponding TIMEX3 by means of the SIMULTANEOUS relation type. In addition, BEFORE relation types are created to represent that one event happens before another one and SIMULTANEOUS relation types to refer to events happening at the same time. The official scores are based on the m"
P15-2059,agerri-etal-2014-ixa,1,0.83557,"we apply a pipeline of tools (cf. section 4.1) that provides annotations at different levels. 4.1 NLP processing Detecting mentions of events, entities and time expressions in text requires the combination of various Natural Language Processing (NLP) modules. We apply a generic pipeline of linguistic tools that includes Named-Entity Recognition (NER) and Disambiguation (NED), Co-reference Resolution (CR), Semantic Role Labelling (SRL), Time Expressions Identification (TEI) and Normalization (TEN), and Temporal Relation Extraction (TRE). The NLP processing is based on the NewsReader pipeline (Agerri et al., 2014a), version 2.1. Next, we present the different tools in our pipeline. Named-Entity Recognition (NER) and Disambiguation (NED): We perform NER using the ixa-pipe-nerc that is part of IXA pipes (Agerri et al., 2014b). The module provides very fast models with high performances, obtaining 84.53 in F1 on CoNLL tasks. Our NED module is based on DBpedia Spotlight (Daiber et al., 2013). We have created a NED client to query the DBpedia Spotlight server for the Named entities detected by the ixapipe-nerc module. Using the best parameter combination, the best results obtained by this module on the TAC"
P15-2059,W11-1902,0,0.031788,", obtaining 84.53 in F1 on CoNLL tasks. Our NED module is based on DBpedia Spotlight (Daiber et al., 2013). We have created a NED client to query the DBpedia Spotlight server for the Named entities detected by the ixapipe-nerc module. Using the best parameter combination, the best results obtained by this module on the TAC 2011 dataset were 79.77 in precision and 60.67 in recall. The best performance on the AIDA dataset is 79.67 in precision and 76.94 in recall. Coreference Resolution (CR): In this case, we use a coreference module that is loosely based on the Stanford Multi Sieve Pass sytem (Lee et al., 2011). The system consists of a number of rulebased sieves that are applied in a deterministic manner. The system scores 56.4 F1 on CoNLL 2011 task, around 3 points worse than the system by (Lee et al., 2011). Semantic Role Labelling (SRL): SRL is performed using the system included in the MATEtools (Bj¨orkelund et al., 2009). This system reported on the CoNLL 2009 Shared Task a labelled semantic F1 of 85.63 for English. Time Expression Identification (TEI) and 4.2 TimeLine extraction Our TimeLine extraction system uses the linguistic information provided by the pipeline. The process to extract the"
P15-2059,W09-1206,0,0.183334,"Missing"
P15-2059,E14-1016,0,0.0119706,"quires a quite complete time anchoring. This work focuses 358 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 358–364, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics involving the extraction of implicit information (Palmer et al., 1986; Whittemore et al., 1991; Tetreault, 2002). Particularly, we are inspired by recent works on Implicit Semantic Role Labelling (ISRL) (Gerber and Chai, 2012) and very specially on the work by (Blanco and Moldovan, 2014) who adapted the ideas about ISRL to focus on modifiers, including arguments of time, instead of core arguments or roles. As the SemEval 2015 task 4 does not include any training data we decided to develop a deterministic algorithm of the type of (Laparra and Rigau, 2013) for ISRL. 3 poral awareness of an annotation (UzZaman and Allen, 2011) based on temporal closure graphs. In order to calculate the precision, recall and F1 score, the TimeLines are first transformed into a graph representation. For that, the time anchors are represented as TIMEX3 and the events are related to the correspondin"
P15-2059,S15-2134,0,0.0687964,"aptures explicit time-anchors. The second one extends the baseline system by also capturing implicit time relations. We have evaluated both approaches in the SemEval 2015 task 4 TimeLine: CrossDocument Event Ordering. We empirically demonstrate that the document-based approach obtains a much more complete time anchoring. Moreover, this approach almost doubles the performance of the systems that participated in the task. 1 2 Related work The present work is closely related to previous approaches involved in TempEval campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015). In these works, the problem can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semanti"
P15-2059,S15-2133,0,0.0332862,"Missing"
P15-2059,P06-1095,0,0.0278306,"omplete time anchoring. Moreover, this approach almost doubles the performance of the systems that participated in the task. 1 2 Related work The present work is closely related to previous approaches involved in TempEval campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015). In these works, the problem can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semantic and discourse features. (Laokulrat et al., 2013) obtained the best results on TempEval 2013 annotating sentences with predicate-role structures, while (Mirza and Tonelli, 2014) affirm that using a simple feature set results in better performances. However, recent works like (Chambers et al.,"
P15-2059,P07-2044,0,0.025921,"The present work is closely related to previous approaches involved in TempEval campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015). In these works, the problem can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semantic and discourse features. (Laokulrat et al., 2013) obtained the best results on TempEval 2013 annotating sentences with predicate-role structures, while (Mirza and Tonelli, 2014) affirm that using a simple feature set results in better performances. However, recent works like (Chambers et al., 2014) have pointed out that these tasks cover just a part of all the temporal relations that can be inferred from the documents. Furthermore, time"
P15-2059,Q14-1022,0,0.156128,"Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semantic and discourse features. (Laokulrat et al., 2013) obtained the best results on TempEval 2013 annotating sentences with predicate-role structures, while (Mirza and Tonelli, 2014) affirm that using a simple feature set results in better performances. However, recent works like (Chambers et al., 2014) have pointed out that these tasks cover just a part of all the temporal relations that can be inferred from the documents. Furthermore, time-anchoring is just a part of the works presented above. Our approach aims to extend these strategies and it is based on other research lines Introduction Temporal relation extraction has been the topic of different SemEval tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015) and other challenges as the 6th i2b2 NLP Challenge (Sun et al., 2013). These tasks focused mainly on the temporal relations of the events w"
P15-2059,E14-1033,0,0.0147977,"nt and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semantic and discourse features. (Laokulrat et al., 2013) obtained the best results on TempEval 2013 annotating sentences with predicate-role structures, while (Mirza and Tonelli, 2014) affirm that using a simple feature set results in better performances. However, recent works like (Chambers et al., 2014) have pointed out that these tasks cover just a part of all the temporal relations that can be inferred from the documents. Furthermore, time-anchoring is just a part of the works presented above. Our approach aims to extend these strategies and it is based on other research lines Introduction Temporal relation extraction has been the topic of different SemEval tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015) and other challen"
P15-2059,N13-1112,0,0.0626529,"rhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015). In these works, the problem can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded rules with some semantic and discourse features. (Laokulrat et al., 2013) obtained the best results on TempEval 2013 annotating sentences with predicate-role structures, while (Mirza and Tonelli, 2014) affirm that using a simple feature set results in better performances. However, recent works like (Chambers et al., 2014) have pointed out that these tasks cover just a part of all the temporal relations that can be inferred from the documents. Furthermore, time-anchoring is just a part of the works presented above. Our approach aims to extend these strategies and it is base"
P15-2059,P86-1004,0,0.310802,"rmation must be recovered from different sources in a cross-document way. Second, the TimeLines are focused on the events involving just a given entity. Finally, unlike previous challenges, SemEval 2015 task 4 requires a quite complete time anchoring. This work focuses 358 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 358–364, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics involving the extraction of implicit information (Palmer et al., 1986; Whittemore et al., 1991; Tetreault, 2002). Particularly, we are inspired by recent works on Implicit Semantic Role Labelling (ISRL) (Gerber and Chai, 2012) and very specially on the work by (Blanco and Moldovan, 2014) who adapted the ideas about ISRL to focus on modifiers, including arguments of time, instead of core arguments or roles. As the SemEval 2015 task 4 does not include any training data we decided to develop a deterministic algorithm of the type of (Laparra and Rigau, 2013) for ISRL. 3 poral awareness of an annotation (UzZaman and Allen, 2011) based on temporal closure graphs. In"
P15-2059,J12-4003,0,0.0242018,"inally, unlike previous challenges, SemEval 2015 task 4 requires a quite complete time anchoring. This work focuses 358 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 358–364, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics involving the extraction of implicit information (Palmer et al., 1986; Whittemore et al., 1991; Tetreault, 2002). Particularly, we are inspired by recent works on Implicit Semantic Role Labelling (ISRL) (Gerber and Chai, 2012) and very specially on the work by (Blanco and Moldovan, 2014) who adapted the ideas about ISRL to focus on modifiers, including arguments of time, instead of core arguments or roles. As the SemEval 2015 task 4 does not include any training data we decided to develop a deterministic algorithm of the type of (Laparra and Rigau, 2013) for ISRL. 3 poral awareness of an annotation (UzZaman and Allen, 2011) based on temporal closure graphs. In order to calculate the precision, recall and F1 score, the TimeLines are first transformed into a graph representation. For that, the time anchors are repres"
P15-2059,J05-1004,0,0.0230452,"sentence but also in the previous and following sentences. TimeLine: Cross-Document Event Ordering In the SemEval task 4 TimeLine: Cross-Document Event Ordering (Minard et al., 2015), given a set of documents and a target entity, the aim is to build a TimeLine by detecting the events in which the entity is involved and anchoring these events to normalized times. Thus, a TimeLine is a collection of ordered events in time relevant for a particular entity. TimeLines contain relevant events in which the target entity participates as ARG0 (i.e agent) or ARG1 (i.e. patient) as defined in PropBank (Palmer et al., 2005).1 The target entities can be people, organization, product or financial entities and the annotation of time anchors is based on TimeML. For example, given the entity Steve Jobs, a TimeLine contains the events with the associated ordering in the TimeLine and the time anchor: 1 2 ... 4 2004 2005-06-05 18135-7-fighting 1664-2-keynote 2011-08-24 18315-2-step down The dataset used for the task is composed of articles from Wikinews. The trial data consists of 30 documents about “Apple Inc.” and gold standard TimeLines for six target entities. The test corpus consists of 3 sets of 30 documents aroun"
P15-2059,pianta-etal-2008-textpro,0,0.0567241,"Missing"
P15-2059,P11-2061,0,0.239231,"Missing"
P15-2059,S13-2001,0,0.428623,"baseline system that captures explicit time-anchors. The second one extends the baseline system by also capturing implicit time relations. We have evaluated both approaches in the SemEval 2015 task 4 TimeLine: CrossDocument Event Ordering. We empirically demonstrate that the document-based approach obtains a much more complete time anchoring. Moreover, this approach almost doubles the performance of the systems that participated in the task. 1 2 Related work The present work is closely related to previous approaches involved in TempEval campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015). In these works, the problem can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recently, (DSouza and Ng, 2013) combined hand-coded"
P15-2059,S07-1014,0,0.467242,"ted two different systems. The first one is a baseline system that captures explicit time-anchors. The second one extends the baseline system by also capturing implicit time relations. We have evaluated both approaches in the SemEval 2015 task 4 TimeLine: CrossDocument Event Ordering. We empirically demonstrate that the document-based approach obtains a much more complete time anchoring. Moreover, this approach almost doubles the performance of the systems that participated in the task. 1 2 Related work The present work is closely related to previous approaches involved in TempEval campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015). In these works, the problem can be seen as a classification task for deciding the type of the temporal link that connects two different events or an event and a temporal expression. For that reason, the task has been mainly addresed using supervised techniques. For example, (Mani et al., 2006; Mani et al., 2007) trained a MaxEnt classifier using training data which were bootstrapped by applying temporal closure. (Chambers et al., 2007) focused on event-event relations using previously ´ learned event attributes. More recentl"
P15-2059,P91-1003,0,0.088106,"ered from different sources in a cross-document way. Second, the TimeLines are focused on the events involving just a given entity. Finally, unlike previous challenges, SemEval 2015 task 4 requires a quite complete time anchoring. This work focuses 358 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 358–364, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics involving the extraction of implicit information (Palmer et al., 1986; Whittemore et al., 1991; Tetreault, 2002). Particularly, we are inspired by recent works on Implicit Semantic Role Labelling (ISRL) (Gerber and Chai, 2012) and very specially on the work by (Blanco and Moldovan, 2014) who adapted the ideas about ISRL to focus on modifiers, including arguments of time, instead of core arguments or roles. As the SemEval 2015 task 4 does not include any training data we decided to develop a deterministic algorithm of the type of (Laparra and Rigau, 2013) for ISRL. 3 poral awareness of an annotation (UzZaman and Allen, 2011) based on temporal closure graphs. In order to calculate the pr"
P15-2059,H86-1011,0,\N,Missing
P15-2059,S10-1010,0,\N,Missing
P15-2059,S15-2132,1,\N,Missing
P97-1007,A92-1044,1,0.878522,"Missing"
P97-1007,C96-1005,1,0.845135,"Missing"
P97-1007,P81-1030,0,0.0381477,"Missing"
P97-1007,H91-1077,0,0.0527603,"Missing"
P97-1007,H94-1046,0,0.0228879,"Missing"
P97-1007,H92-1046,0,0.0897578,"Missing"
P97-1007,W95-0105,0,0.0524789,"Missing"
P97-1007,J92-1001,0,0.205861,"Missing"
P97-1007,J90-1003,0,\N,Missing
P97-1007,C94-1042,0,\N,Missing
P97-1007,C92-1056,0,\N,Missing
P97-1007,C92-2070,0,\N,Missing
P97-1007,P95-1026,0,\N,Missing
P97-1007,P94-1013,0,\N,Missing
P97-1007,P92-1053,0,\N,Missing
P98-2181,P97-1007,1,0.89878,"Missing"
P98-2181,W97-0313,0,0.0356749,"Missing"
P98-2181,C92-2070,0,0.0484014,"Missing"
P98-2181,W90-0108,0,\N,Missing
P98-2181,C92-4189,0,\N,Missing
P98-2181,P95-1026,0,\N,Missing
P98-2181,P81-1030,0,\N,Missing
pociello-etal-2008-wnterm,magnini-cavaglia-2000-integrating,0,\N,Missing
pociello-etal-2008-wnterm,alegria-etal-2004-xml,0,\N,Missing
pociello-etal-2008-wnterm,agirre-etal-2006-methodology,1,\N,Missing
R09-1039,agirre-de-lacalle-2004-publicly,0,0.158111,"Missing"
R09-1039,E09-1005,0,0.108092,"Dijkstra requires a set of monosemous or already interpreted words, we have devised, developed and empirically tested four different versions of this algorithm to deal with sets having only polysemous words. The resulting new algorithms obtain improved results over state-of-the-art. As a result of this empirical study, we are currently developping a new version of the SSI-Dijkstra using FSI for nouns and adjectives, and FSP for verbs. We also plan to further extend the empirical evaluation with other available graph based algorithms that have been proved to be competitive in WSD such as UKB13 [2]. Finally, using the same automatic approach, we also plan to disambiguate the Lexical Elements of a given frame. 12 13 We did not remove unfrequent LUs 212 4 Conclusions and future work http://ixa2.si.ehu.es/ukb/ wn-mfs FSI ASI FSP ASP P 0.58 0,64 0,64 0,56 0,48 nouns R 0.58 0,55 0,55 0,48 0,41 F 0.58 0,59 0,59 0,52 0,44 P 0.48 0,50 0,53 0,59 0,53 verbs R 0.48 0,44 0,46 0,51 0,46 F 0.48 0,47 0,49 0,55 0,49 P 0.80 0,80 0,80 0,60 0,60 adjectives R F 0.80 0.80 0,80 0,80 0,80 0,80 0,60 0,60 0,60 0,60 P 0.54 0,58 0,59 0,58 0,52 all R 0.54 0,51 0,52 0,51 0,45 F 0.54 0,54 0,55 0,54 0,48 Table 5: Res"
R09-1039,S07-1018,0,0.0287283,"SSI-Dijkstra using FSI for nouns and adjectives, and FSP for verbs. of nouns, and a better disambiguation of nouns will also improve the disambiguation of verbs and adjectives. However, still remains unclear if the problem of frames having no monosemous LUs is because the lack of correctly disambiguated words in I, the small number of LUs per frame or its high polysemy degree. We expect to clarify this issue in future experiments and analysis. Although the experimental setting is different, [8] also present a direct evaluation of their integration of WN and FrameNet for the LU induction task [5]. They apply a combination of knowledge and distributional based methods to carry out the mapping process. In order to aliviate their data sparseness problem, they reduced the whole dataset in two ways. First, they neglected LUs occurring less than 50 times in the British National Corpus. Second, they excluded frames having less than 10 LUs. This leaves them with 220 frames, involving 4,380 LUs. They focused the study of the quality of their automatic mapping on four frames (i.e. KILLING, PEOPLE BY AGE, STATEMENT and CLOTHING) with 306 LUs. On this dataset, they report a precision of 0.80, a r"
R09-1039,W08-2208,0,0.461409,"ic processing takes a great deal of expensive manual effort involving large research groups during long periods of development. Thus, the coverage of currently available predicate-argument resources is still unsatisfactory. For example, [7] or [25] indicate the limited coverage of FrameNet as one of the main problems of this resource. In fact, FrameNet1.3 covers around 10,000 lexical-units while for instance, WordNet3.0 contains more than 150,000 words. Furthermore, the same effort should be invested for each different language [27]. Following the line of previous works [26], [7], [15], [24], [8], [29], we empirically study a novel approach to partially integrate FrameNet [6] and WordNet [12]. The method relies on the use of a knowledge-based Word Sense Disambiguation (WSD) algorithm that uses a large-scale graph of concepts derived from WordNet [12] and eXtented WordNet [19]. The WSD algorithm is applied to semantically coherent groupings of words belonging to the same frame. In that way we expect to extend the coverage of FrameNet (by including from WordNet closely related concepts), to enrich WordNet with frame semantic information (by porting frame information to WordNet) and poss"
R09-1039,C08-1021,1,0.924432,"t each step) to a set of synsets 4 (those word senses already interpreted in I), the original SSI uses an in-house knowledge base derived semiautomatically which integrates a variety of online resources [20]. This very rich knowledge-base is used to calculate graph distances between synsets. In order to avoid the exponential explosion of possibilities, not all paths are considered. They used a context-free grammar of relations trained on SemCor to filter-out inappropriate paths and to provide weights to the appropriate paths. Instead, we used a version of the SSI algorithm called SSI-Dijkstra [9] (see algorithm 1. SSI-Dijkstra uses the Dijkstra algorithm to obtain the shortest path distance between a node and some other nodes of the whole graph. The Dijkstra algorithm is a greedy algorithm that computes the shortest path distance between one node an the rest of nodes of a graph. BoostGraph5 library can be used to compute very efficiently the shortest distance between any two given nodes on very large graphs. As [9], we also use already available knowledge resources to build a very large connected graph with 99,635 nodes (synsets) and 636,077 edges (the set of direct relations between"
R09-1039,erk-pado-2004-powerful,0,0.0515284,"Missing"
R09-1039,P06-1117,0,0.0781772,"Missing"
R09-1039,magnini-cavaglia-2000-integrating,0,0.335609,"belonging to the same frame. In that way we expect to extend the coverage of FrameNet (by including from WordNet closely related concepts), to enrich WordNet with frame semantic information (by porting frame information to WordNet) and possibly to extend FrameNet to languages other than English (by exploiting local wordnets aligned to the English WordNet). WordNet1 [12] (hereinafter WN) is by far the most widely-used knowledge base. In fact, WN is being used world-wide for anchoring different types of semantic knowledge including wordnets for languages other than English [4], domain knowledge [17] or ontologies like SUMO [22] or the EuroWordNet Top Concept Ontology [3]. It contains manually coded information about English nouns, verbs, adjectives and adverbs and is organized around the notion of a synset. A synset is a set of words with the same part-of-speech that can be interchanged in a certain context. For example, <student, pupil, educatee> form a synset because they can be used to refer to the same concept. A synset is often further described by a gloss, in this case: ”a learner who is enrolled in an educational institution” and by explicit semantic relations to other synsets. Ea"
R09-1039,P04-1036,0,0.0273409,"s SSI-Dijkstra when W contains monosemous terms, but differently when W contains only polysemous words. In fact, FSI and ASI always provide an interpretation of W. While FSI includes in I the sense having minimal cumulated distance to the first senses of the rest of words in W, ASI includes in I the sense having minimal cumulated distance to the all the senses of the rest of words in W. The rationale behind the FSI algorithm is that the most frequent sense for a word, according to the WN sense ranking is very 210 competitive in WSD tasks, and it is extremely hard to improve upon even slightly [18]. Thus, this algorithm expects that the first sense in WN will be correct for most of the words in W. Regarding ASI, this algorithm expects that the words in W (corresponding to a very close semantic field) will establish many close path connections between different synsets of the same word (because of the fine-grained sense distinction of WN). At each step, both the original SSI and also the SSIDijkstra algorithms only consider the set I of already interpreted words to disambiguate the next word of P. That is, the remaining words of P are not used in the disambiguation process. In fact, the"
R09-1039,J05-1004,0,0.0170677,"Missing"
R09-1039,D08-1048,0,0.172319,"Missing"
R09-1039,D07-1002,0,0.0694,"Missing"
R09-1039,W09-3740,0,0.104112,"ersions consider the set I of already interpreted words of W and also the rest of words remaining in P. That is, at each step, the algorithm selects the word sense which is closer to the set I of already disambiguated words and the remaining words of P all together. While FSP selects the sense having minimal cumulated distance to I and the first senses of the words in P, ASP selects the sense having minimal cumulated distance to I and all the senses of the words in P. 3 Experiments We have evaluated the performance of the different versions of the SSI algorithm using the same data set used by [28] and [29]. This data set consists of a total of 372 LUs corresponding to 372 different frames from FrameNet1.3 (one LU per frame). Each LUs have been manually annotated with the corresponding WN 1.6 synset. This Gold Standard includes 9 frames (5 verbs and 4 nouns) with only one LU (the one that has been sense annotated). Obviously, for these cases, our approach will produce no results since no context words can be used to help the disambiguation process9 . Table 2 presents the main characteristics of the datasets we used in this work. In this table, FN stands for FrameNet10 , GS for the Gold-"
R09-1039,W09-1127,0,0.861917,"ocessing takes a great deal of expensive manual effort involving large research groups during long periods of development. Thus, the coverage of currently available predicate-argument resources is still unsatisfactory. For example, [7] or [25] indicate the limited coverage of FrameNet as one of the main problems of this resource. In fact, FrameNet1.3 covers around 10,000 lexical-units while for instance, WordNet3.0 contains more than 150,000 words. Furthermore, the same effort should be invested for each different language [27]. Following the line of previous works [26], [7], [15], [24], [8], [29], we empirically study a novel approach to partially integrate FrameNet [6] and WordNet [12]. The method relies on the use of a knowledge-based Word Sense Disambiguation (WSD) algorithm that uses a large-scale graph of concepts derived from WordNet [12] and eXtented WordNet [19]. The WSD algorithm is applied to semantically coherent groupings of words belonging to the same frame. In that way we expect to extend the coverage of FrameNet (by including from WordNet closely related concepts), to enrich WordNet with frame semantic information (by porting frame information to WordNet) and possibly t"
R09-1039,P98-1013,0,\N,Missing
R09-1039,C98-1013,0,\N,Missing
reese-etal-2010-wikicorpus,W07-0201,0,\N,Missing
reese-etal-2010-wikicorpus,agirre-de-lacalle-2004-publicly,0,\N,Missing
reese-etal-2010-wikicorpus,W08-2207,1,\N,Missing
reese-etal-2010-wikicorpus,widdows-ferraro-2008-semantic,0,\N,Missing
reese-etal-2010-wikicorpus,E09-1005,0,\N,Missing
reese-etal-2010-wikicorpus,S07-1015,1,\N,Missing
reese-etal-2010-wikicorpus,W01-0703,0,\N,Missing
reese-etal-2010-wikicorpus,atserias-etal-2006-freeling,0,\N,Missing
reese-etal-2010-wikicorpus,zesch-etal-2008-extracting,0,\N,Missing
reese-etal-2010-wikicorpus,atserias-etal-2008-semantically,0,\N,Missing
S01-1010,W00-0706,1,0.805521,"Missing"
S01-1017,W99-0606,0,0.0129816,"G. Rigau TALP Research Center Universitat Politecnica de Catalunya Jordi Girona Salgado, 1-3 Barcelona, Catalonia, Spain {escudero,lluism,g.rigau}@lsi.upc.es Abstract The particular algorithm used in our system to perform the classification of senses is the generalized AdaBoost.MH with confidence-rated predictions (Schapire and Singer, 1999). This algorithm is able to deal straightforwardly with multiclass multi-label problems, and has been previously applied, with significant success, to a number of NLP disambiguation tasks, including, among others: Part-of-speech tagging and PP-attachment (Abney et al., 1999), text categorization (Schapire and Singer, 2000), and shallow parsing (Carreras and Marquez, 2001). The weak hypotheses used in this work are decision stumps, which can be seen as extremely simple decision trees with one internal node testing the value of a single binary feature (e.g. &quot;the word dark appears in the context of the word to be disambiguated?&quot;) and two leaves that give the prediction of the senses based on the feature value. The &quot;Lazy&quot; Boosting, is a simple modification of the AdaBoost.MH algorithm, which consists of reducing the feature space that is explored when learning each w"
S01-1017,W01-0726,1,0.809627,"-3 Barcelona, Catalonia, Spain {escudero,lluism,g.rigau}@lsi.upc.es Abstract The particular algorithm used in our system to perform the classification of senses is the generalized AdaBoost.MH with confidence-rated predictions (Schapire and Singer, 1999). This algorithm is able to deal straightforwardly with multiclass multi-label problems, and has been previously applied, with significant success, to a number of NLP disambiguation tasks, including, among others: Part-of-speech tagging and PP-attachment (Abney et al., 1999), text categorization (Schapire and Singer, 2000), and shallow parsing (Carreras and Marquez, 2001). The weak hypotheses used in this work are decision stumps, which can be seen as extremely simple decision trees with one internal node testing the value of a single binary feature (e.g. &quot;the word dark appears in the context of the word to be disambiguated?&quot;) and two leaves that give the prediction of the senses based on the feature value. The &quot;Lazy&quot; Boosting, is a simple modification of the AdaBoost.MH algorithm, which consists of reducing the feature space that is explored when learning each weak classifier. More specifically, a small proportion of attributes are randomly selected and the b"
S01-1017,magnini-cavaglia-2000-integrating,0,0.0680963,"where the last three correspond to collocations of two consecutive words. The topical context is formed by c1, ... , Cm, which stand for the unordered set of open class words appearing in a medium-size 21-word window centered around the target word. The more innovative use of semantic domain information is detailed in the next section. 1.2.1 Domain Information We have enriched the basic set of features by adding semantic information in the form of domain labels. These domain labels are computed during a preprocessing step using the 164 domain labels linked to the nominal part of WordNet 1.6 (Magnini and Cavaglia, 2000). For each training example, a program gathers, from its context, all nouns and their synsets with the attached domain labels, and scores them according to a certain scoring function. The weights assigned by this function depend on the number of domain labels assigned to each noun and their relative frequencies in the whole WordNet. The result of this procedure is the set of domain labels that achieve a score higher than a certain experimentally set threshold, which are incorporated as regular features for describing the example. Two different simplifications have been carried out. Firstly, mu"
S01-1017,H93-1052,0,0.0707811,"Missing"
S07-1001,P00-1064,0,0.0127234,"ets the occurrence identifier, the sense tag (if in training), and the list of features that apply to the occurrence. 5 http://ixa2.si.ehu.es/semeval-clir/ 6 http://en.wikipedia.org/wiki/ Information retrieval 4 Allocation. Using topic-specific synset similarity measures, they create predictions for each word in each document using only word frequency information. The disambiguation process took aprox. 12 hours on a cluster of 48 machines (dual Xeons with 4GB of RAM). Note that contrary to the specifications, this team returned WordNet 2.1 senses, so we had to map automatically to 1.6 senses (Daude et al., 2000). UNIBA This team uses a a knowledge-based WSD system that attempts to disambiguate all words in a text by exploiting WordNet relations. The main assumption is that a specific strategy for each Part-Of-Speech (POS) is better than a single strategy. Nouns are disambiguated basically using hypernymy links. Verbs are disambiguated according to the nouns surrounding them, and adjectives and adverbs use glosses. ORGANIZERS In addition to the regular participants, and out of the competition, the organizers run a regular supervised WSD system trained on Semcor. The system is based on a single k-NN cl"
S07-1001,P97-1010,0,\N,Missing
S07-1001,D07-1007,0,\N,Missing
S07-1001,P07-1005,0,\N,Missing
S07-1001,W99-0624,0,\N,Missing
S07-1015,P06-1013,0,0.0505637,"Missing"
S07-1015,agirre-de-lacalle-2004-publicly,0,0.7258,"Missing"
S07-1015,W01-0703,0,0.631209,"rt advanced concept-based NLP applications directly. It seems that applications will not scale up to working in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means. Fortunately, during the last years, the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets (or between synsets and words). Many international research groups are working on knowledge-based WSD using a wide range of approaches (Mihalcea, 2006). However, less attention"
S07-1015,J98-1006,0,0.118912,"n the MCR. 3.1 Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from large corpora. In our case, we consider word senses as topics. For this study, we use two different large-scale Topic Signatures. The first constitutes one of the largest available semantic resource with around 100 million relations (between synsets and words) acquired from the web (Agirre and de la Calle, 2004). The second has been derived directly from SemCor. TSWEB2 : Inspired by the work of (Leacock et al., 1998), these Topic Signatures were constructed using monosemous relatives from WordNet (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the words with distinctive frequency using TFIDF. For these experiments, we used at maximum the first 700 words of each TS. TSSEM: These Topic Signatures have been constructed using the part of SemCor having all words tagged by PoS, lemmatized and sense tagged according to WN1.6 totalizing 192,639 words. For each word-sense appearing in SemCor,"
S07-1015,C00-1072,0,0.17892,"Missing"
S07-1032,W06-1670,0,0.0129326,"ms with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1 http://www.illc.uva.nl/EuroWordNet/ http://www.ceid.upatras.gr/Balkanet 3 http://www.lsi.upc.es/ nlp/meaning 2 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, c Prague, June 2007. 2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini"
S07-1032,W03-1022,0,0.0259971,"rd–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1 http://www.illc.uva.nl/EuroWordNet/ http://www.ceid.upatras.gr/Balkanet 3 http://www.lsi.upc.es/ nlp/meaning 2 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, c Prague, June 2007. 2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers f"
S07-1032,P05-1004,0,0.0133685,"automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1 http://www.illc.uva.nl/EuroWordNet/ http://www.ceid.upatras.gr/Balkanet 3 http://www.lsi.upc.es/ nlp/meaning 2 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, c Prague, June 2007. 2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such"
S07-1032,W93-0106,0,0.172297,"Missing"
S07-1032,W97-0811,0,0.0418682,"It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach. Thus, some research has been focused on deriving different sense groupings to overcome the fine– grained distinctions of WN (Hearst and Sch¨utze, 1993) (Peters et al., 1998) (Mihalcea and Moldovan, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006). However, most of the later approaches used the original Lexicographical Files of WN (more recently called Super1 http://www.illc.uva.nl/EuroWordNet/ http://www.ceid.upatras.gr/Balkanet 3 http://www.lsi.upc.es/ nlp/meaning 2 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, c Prague, June 2007. 2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions. However, not so much attention has been paid on lear"
S07-1032,magnini-cavaglia-2000-integrating,0,\N,Missing
S10-1090,P05-1005,0,0.186732,"t word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007). That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation. In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions. We suspect that selecting the appropriate level of abstraction could be on between both levels. Thus, we use the semantic classes modeled by the Basic Level Concepts1 (BLC) (Izquierdo et al., 2007). Our previous research using BLC empirically demonstrated that this automatically derived This paper summarizes ou"
S10-1090,W06-1670,0,0.0172075,"rcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007). That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation. In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions. We suspect that selecting the appropriate level of abstraction could be on between both levels. Thus, we use the semantic classes modeled by the Basic Level Concepts1 (BLC) (Izquierdo et al., 2007). Our previous research using BLC empirically demonstrated that this automatically derived This paper summarizes our participation in task #17 of S"
S10-1090,P06-1014,0,0.0230801,"ns for higher level applications like Machine Translation or Question & Answering. In fact, WSD at this level of granularity has resisted all attempts of inferring robust broad-coverage models. It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples. Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007). That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation. In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. Most of the later approaches used the original"
S10-1090,W03-1022,0,0.0351901,"tated examples. Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007). That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation. In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions. We suspect that selecting the appropriate level of abstraction could be on between both levels. Thus, we use the semantic classes modeled by the Basic Level Concepts1 (BLC) (Izquierdo et al., 2007). Our previous research using BLC empirically d"
S10-1090,P05-1004,0,0.0152242,"eriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007). That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation. In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions. We suspect that selecting the appropriate level of abstraction could be on between both levels. Thus, we use the semantic classes modeled by the Basic Level Concepts1 (BLC) (Izquierdo et al., 2007). Our previous research using BLC empirically demonstrated that this automatically derive"
S10-1090,W00-1322,1,0.819091,"Missing"
S10-1090,W97-0811,0,0.0522408,"umes of word–sense annotated examples. Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007). That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation. In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). That is, grouping senses of different words into the same explicit and comprehensive semantic class. Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions. We suspect that selecting the appropriate level of abstraction could be on between both levels. Thus, we use the semantic classes modeled by the Basic Level Concepts1 (BLC) (Izquierdo et al., 2007). Our previous r"
S10-1090,W93-0106,0,0.107849,"Missing"
S10-1090,D07-1107,0,0.0603286,"Missing"
S10-1090,E09-1045,1,0.809698,"Missing"
S10-1090,S10-1013,0,\N,Missing
S13-1018,E09-1005,1,0.849802,"se2 . • Subject and description: cosine similarity of TF.IDF vectors of respective fields. IDF values were calculated using a subset of Europeana items (the Culture Grid collection), available internally. These preliminary scores were im2 urlhttp://wordnetcode.princeton.edu/standofffiles/morphosemantic-links.xls 132 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 132–137, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proved using TF.IDF based on Wikipedia, UKB (Agirre and Soroa, 2009) and a more informed time similarity measure. We describe each of these processes in turn. 2.1 TF.IDF A common approach to computing document similarity is to represent documents as Bag-Of-Words (BOW). Each BOW is a vector consisting of the words contained in the document, where each dimension corresponds to a word, and the weight is the frequency in the corresponding document. The similarity between two documents can be computed as the cosine of the angle between their vectors. This is the approached use above. This approach can be improved giving more weight to words which occur in only a fe"
S13-1018,agirre-etal-2010-exploring,1,0.860547,"idf2w qP 2 2 w∈a (tfw,a × idfw ) × w∈b (tfw,b × idfw ) P qP w∈a,b where tfw,x is the frequency of the term w in x ∈ {a, b} and idfw is the inverted document frequency of the word w measured in Wikipedia. We substituted the preliminary general similarity score by the obtained using the TF.IDF presented in this section. 2.2 UKB The semantic disambiguation UKB3 algorithm (Agirre and Soroa, 2009) applies personalized PageRank on a graph generated from the English WordNet (Fellbaum, 1998), or alternatively, from Wikipedia. This algorithm has proven to be very competitive in word similarity tasks (Agirre et al., 2010). To compute similarity using UKB we represent WordNet as a graph G = (V, E) as follows: graph nodes represent WordNet concepts (synsets) and 3 http://ixa2.si.ehu.es/ukb/ 133 dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges. Our method is provided with a pair of vectors of words and a graph-based representation of WordNet. We first compute the personalized PageRank over WordNet separately for each of the vector of words, producing a probability distribution over WordNet synsets. We"
S13-1018,P05-1045,0,0.00503141,"y, using the training data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and descri"
S13-1018,N03-1033,0,0.0199478,"data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and description: cosine similarity"
S13-1018,L10-1000,0,\N,Missing
S13-1018,W12-1012,1,\N,Missing
S14-2010,S14-2085,0,0.0392393,"Missing"
S14-2010,S14-2069,0,0.0326403,"Missing"
S14-2010,S14-2128,0,0.0328229,"Missing"
S14-2010,P13-1024,1,0.0618966,"data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the news title, while the other one represents a Twitter comment on that particular news. They are evenly sampled from string similarity values between 0.5 and 1. Table 1 shows the explanations and values associated with each score between 5 and 0. As in prior years, we used Amazon Mechanical Turk (AMT)3 to crowdsource the annotation of the English pairs.4 Annotators are presented with the Table 2: English subtask: Summary of train (2012 and 2013) and test (2014) datasets. a DARPA sponsored workshop at Columbia University.1 In 2013, STS wa"
S14-2010,S14-2112,0,0.0317369,"Missing"
S14-2010,N06-2015,0,0.0715746,"ferent similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs"
S14-2010,S12-1051,1,0.623306,"r as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline map"
S14-2010,S14-2131,0,0.0336314,"Missing"
S14-2010,S14-2072,0,0.0817436,"Missing"
S14-2010,Q14-1018,0,0.0731,"Missing"
S14-2010,S14-2039,0,0.0993932,"Missing"
S14-2010,S14-2078,0,0.101731,"Missing"
S14-2010,S14-2022,0,0.0306687,"Missing"
S14-2010,S14-2046,0,0.022257,"Missing"
S14-2010,D13-1179,0,0.0175763,"Missing"
S14-2010,S12-1060,0,0.0199826,"Missing"
S14-2010,S14-2093,0,0.0281526,"Missing"
S14-2010,W10-0721,0,0.050451,"d 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the new"
S14-2010,W10-0707,0,\N,Missing
S14-2010,P94-1019,0,\N,Missing
S14-2010,Q14-1017,0,\N,Missing
S14-2010,S14-2138,0,\N,Missing
S14-2148,D09-1159,0,0.0213602,"t analysis, has gained a huge importance during the last decade due to the amount of review web sites, blogs and social networks producing everyday a massive amount of new content (Pang and Lee, 2008; Liu, 2012; Zhang and Liu, 2014). This content usually contains opinions about different entities, products or services. Trying to cope with this large amounts of textual data is unfeasible without the help of automatic Opinion Mining tools which try to detect, identify, classify, aggregate and summarize the opinions expressed about different topics (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al., 2010). In this framework, aspect based opinion mining systems aim to detect the sentiment at “aspect” level (i.e. the precise feature being opinionated in a clause or sentence). In this paper we describe our system presented in the SemEval 2014 task 41 Aspect Based Sentiment Analysis (Pontiki et al., 2014), which focuses on detecting opinionated aspect terms (e.g. wine 2 Our approach We have adapted the double-propagation technique described in (Qiu et al., 2009; Qiu et al., 2011). This method consists of using a minimal seed list of aspect terms and opinion words and propagate"
S14-2148,de-marneffe-etal-2006-generating,0,0.0703705,"Missing"
S14-2148,C10-2167,0,0.0388188,"Missing"
S14-2148,D13-1125,0,0.0227797,"pect terms of a given sentence into categories, we assign those categories to the sentence. If no category has been assigned, then we use the anecdotes/miscellaneous category as the default one. This approach is quite naive and it has many limitations. It works quite well for the category food, classifying ingredients and meals, but it fails when the category or the aspect terms are more vague or abstract. In addition, we do not perform any kind of word sense disambiguation or sense pruning, which probably would discard unrelated senses. For detecting the polarity we have used the SentiWords (Guerini et al., 2013; Warriner et al., 2013) as a polarity lexicon. Using direct dependency relations between aspect terms and polarity bearing words we assign the polarity value from the lexicon to the aspect term. We make a simple count of the polarities of the aspect terms classified under a certain category to assign the polarity of that category in a particular sentence. • If word N and word N+1 are nouns, and the combination is an entry in WordNet (or in Wikipedia, see below). E.g.: battery life • If word N is an adjective and word N+1 is a noun, and the combination is an entry in WordNet. E.g.: hot dog, ha"
S14-2148,S14-2004,0,0.022928,"s. Trying to cope with this large amounts of textual data is unfeasible without the help of automatic Opinion Mining tools which try to detect, identify, classify, aggregate and summarize the opinions expressed about different topics (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al., 2010). In this framework, aspect based opinion mining systems aim to detect the sentiment at “aspect” level (i.e. the precise feature being opinionated in a clause or sentence). In this paper we describe our system presented in the SemEval 2014 task 41 Aspect Based Sentiment Analysis (Pontiki et al., 2014), which focuses on detecting opinionated aspect terms (e.g. wine 2 Our approach We have adapted the double-propagation technique described in (Qiu et al., 2009; Qiu et al., 2011). This method consists of using a minimal seed list of aspect terms and opinion words and propagate them through an unlabelled domainrelated corpus using a set of propagation rules. The goal is to obtain an extended aspect term and opinion word lists. (Qiu et al., 2009) define opinion words as words that convey some positive or negative sentiment polarities. They only extract nouns as aspect terms and adjectives as opi"
S14-2148,J11-1002,0,0.0245955,"arize the opinions expressed about different topics (Hu and Liu, 2004) (Popescu and Etzioni, 2005) (Wu et al., 2009) (Zhang et al., 2010). In this framework, aspect based opinion mining systems aim to detect the sentiment at “aspect” level (i.e. the precise feature being opinionated in a clause or sentence). In this paper we describe our system presented in the SemEval 2014 task 41 Aspect Based Sentiment Analysis (Pontiki et al., 2014), which focuses on detecting opinionated aspect terms (e.g. wine 2 Our approach We have adapted the double-propagation technique described in (Qiu et al., 2009; Qiu et al., 2011). This method consists of using a minimal seed list of aspect terms and opinion words and propagate them through an unlabelled domainrelated corpus using a set of propagation rules. The goal is to obtain an extended aspect term and opinion word lists. (Qiu et al., 2009) define opinion words as words that convey some positive or negative sentiment polarities. They only extract nouns as aspect terms and adjectives as opinion words, and we assume the same restriction. The propagation rules have the form of dependency relations and some part-of-speech restrictions. Some rules extract new aspect te"
S14-2148,H05-2017,0,\N,Missing
S14-2148,H05-1043,0,\N,Missing
S14-2148,P94-1019,0,\N,Missing
S15-2032,agerri-etal-2014-ixa,1,0.840032,"tag and the similarity score. 3.1.1 Input Handling and Chunking Module We use the Stanford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy chunker. Nevertheless, the chunker’s output has been improved using simple regular expressions to fit to our task proposal. Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also develope"
S15-2032,C14-1068,0,0.053166,"Missing"
S15-2032,N13-1092,0,0.0929416,"Missing"
S15-2032,P03-1054,0,0.00490512,"fy segments over sentence pairs, and then, make alignments between them. First of all, the input handling and chunking module is responsible for linguistically processing the given input, and for creating the internal representation of the sentences. Once the input is processed the alignment module identifies related and unrelated segments among sentences. Finally, by using segment pair based features the classification module and the scoring module produce respectively the final relatedness tag and the similarity score. 3.1.1 Input Handling and Chunking Module We use the Stanford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy"
S15-2032,Q14-1018,0,0.0369857,". Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also developed some rules to unify nominal phrases separated by punctuations or conjunctions, or a combination of those. 3.1.2 Alignment Module The alignment module mainly focuses on the work done by the monolingual word aligner described in (Sultan et al., 2014), and HungarianMunkres algorithm. The monolingual word aligner is a simple and ready-to-use system that has demonstrated state-ofthe-art performance. To begin with we start by constructing the token to token link matrix in which each element at position (i,j) determines that there exists a link between token i (from sentence 1) and token j (from sentence 2). A link exists in the matrix if and 180 only if the monolingual word aligner has determined that both tokens are related. Then, the system uses token regions to group individual tokens into segments, and calculates the weight between every"
S15-2032,N03-1033,0,0.00552957,"rity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment. Note that some of the authors participated in the organization of the task. We scrupulously separated the tasks in such a way that the developers of the systems did not have access to the test sets, and that they only had access to the same training data as the rest of the participants. Building Cubes The first step is to produce parse trees for the sentences using the Stanford Parser (Toutanova et al., 2003). After parsing the sentences each pair of sentences can be represented by a NxM matrix, being N is the number of nodes of the parse tree of the first sentence, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations f"
S15-2032,S15-2045,1,\N,Missing
S15-2045,agerri-etal-2014-ixa,1,0.57453,"onal and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple regular expressions. Additionally, this step collects chunk regions coming either from gold standard or from the chunking done by ixa-pipes-chunk (Agerri et al., 2014). This is followed by a lowercased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus ena"
S15-2045,S12-1051,1,0.519741,"TS also differs from both TE and paraphrasing (in as far as both tasks have been defined to date in the literature) in that rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for many NLP tasks such as MT evaluation, information extraction, question answering, summarization. In 2012, we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success (Agirre et al., 2012). In addition, we 252 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity"
S15-2045,S14-2010,1,0.800447,"f the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity in a new language, namely Spanish (Agirre et al., 2014). This year we presented three subtasks: the English subtask, the Spanish subtask and the interpretable pilot subtask. The English subtask comprised pairs from headlines and image descriptions, and it also introduced new genres, including answer pairs from a tutorial dialogue system and from Q&A websites, and pairs from a dataset tagged with committed belief annotations. For the Spanish subtask, additional pairs from news and Wikipedia articles were selected. The annotations for both tasks leveraged crowdsourcing. Finally, with the interpretable STS pilot subtask, we wanted to start exploring"
S15-2045,P14-1023,0,0.0115737,"7070 0.7251 0.7311 0.7250 0.7422 0.6364 0.7775 0.7032 0.7130 0.7189 0.4616 0.7533 0.6111 0.5379 0.5424 0.5672 0.6558 0.4919 0.5912 0.6964 0.7114 0.6364 Table 3: Task 2a: English evaluation results in terms of Pearson correlation. 259 Rank 61 42 29 63 56 57 70 69 71 62 44 49 43 74 72 73 34 28 26 1 3 5 19 18 16 8 9 2 12 13 23 41 59 20 47 22 11 15 33 45 55 24 10 17 50 51 52 4 7 6 46 36 27 39 31 30 32 25 53 14 40 37 35 68 21 58 66 65 64 48 67 60 42 38 54 approach for the top three participants (DLS@CU, ExBThemis, Samsung). They use WordNet (Miller, 1995), Mikolov Embeddings (Mikolov et al., 2013; Baroni et al., 2014) and PPDB (Ganitkevitch et al., 2013). In general, generic NLP tools such as lemmatization, PoS tagging, distributional word embeddings, distributional and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple"
S15-2045,N13-1092,0,0.0796576,"the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,P13-2080,0,0.0179598,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,W10-0721,0,0.0157602,"ns student answers Q&A forum answers commited belief Table 2: English subtask: Summary of train (2012, 2013, 2014) and test (2015) datasets. lines come from a different EMM cluster. Then, we computed the string similarity between those pairs. Accordingly, we sampled 1000 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity as a metric. We sampled another 1000 pairs from the different EMM cluster in the same manner. The Images dataset is a subset of the PASCAL VOC-2008 dataset (Rashtchian et al., 2010), which consists of 1000 images with around 10 descriptions each, and has been used by a number of image description systems. It was also sampled using string similarity, discarding those that had been used in previous years. We organized two bins with 1000 pairs each: one with pairs of descriptions from the same image, and the other one with pairs of descriptions from different images. The source of the Answers-student pairs is the BEETLE corpus (Dzikovska et al., 2010), which is a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system."
S15-2045,W00-0726,0,0.313421,"Missing"
S15-2045,S12-1060,0,0.211502,"Missing"
S15-2045,W10-0707,0,\N,Missing
S15-2121,S15-2082,0,0.11286,"Missing"
S15-2121,D09-1159,0,\N,Missing
S15-2121,C10-2167,0,\N,Missing
S15-2132,S13-2002,0,0.0321759,"Missing"
S15-2132,girardi-etal-2014-cromer,1,0.731015,"types). Some examples of target entities are Steve Jobs (PERSON), Apple Inc. (ORGANISATION), Airbus A380 (PRODUCT), and Nasdaq (FINANCIAL). The annotation procedure for the creation of gold standard timelines for the target entities required one person month. It consisted of four steps, as described below. Entity annotation. All occurrences of the target entities in the four corpora were marked following (Tonelli et al., 2014). Cross-document co-reference was annotated according to the NewsReader crossdocument annotation guidelines (Speranza and Minard, 2014). For this task, we used CROMER3 (Girardi et al., 2014), a tool designed specifically for cross-document annotation. 2 3 http://en.wikinews.org. https://hlt.fbk.eu/technologies/cromer 780 Event and time anchor annotation. Using CROMER, the corpora were annotated with events following the NewsReader cross-document annotation guidelines (Speranza and Minard, 2014). The annotation of events as defined in (Tonelli et al., 2014) was restricted by limiting the annotation to events that could be placed on a timeline. Thus, we did not annotate adjectival events, cognitive events, counter-factual events (which certainly did not happen), uncertain events (w"
S15-2132,R09-1032,0,0.0178229,"he aim of the Cross-Document Event Ordering task is to build timelines from English news articles. To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically. The task focuses on cross-document event coreference resolution and cross-document temporal relation extraction. Additionally, it has also been the focus of the 6th i2b2 NLP Challenge for clinical records (Sun et al., 2013). The cross-document aspect, however, has not often been explored. One example is the work described in (Ji et al., 2009) using the ACE 2005 training corpora. Here the authors link pre-defined events involving the same centroid entities (i.e. entities frequently participating in events) on a timeline. Nominal coreference resolution has been the topic of SemEval 2010 Task on Coreference Resolution in Multiple Languages (Recasens et al., 2010). TimeLine is a pilot task that goes beyond the above-mentioned evaluation exercises by addressing coreference resolution for events and temporal relation extraction at a cross document level. This task was motivated by work done in the NewsReader project1 . The goal of the N"
S15-2132,S15-2132,1,0.106103,"Missing"
S15-2132,W09-2411,0,0.11861,"Missing"
S15-2132,S13-2003,0,0.0583595,"Missing"
S15-2132,P11-2061,0,0.189729,"m the other corpora. On the other hand, on average, Stock Market timelines contain events from a higher number of different documents, i.e. 9.1, versus 6.2 for Airbus and 5.7 for GM. 5 2004 2005-06-05 2011-01 2011-08-24 2011-10-06 fighting keynote leave step_down described BEFORE SIMULTANEOUS Explicit relations Evaluation Methodology Implicit relations The evaluation methodology of this task is based on the evaluation metric used for TempEval-3 (UzZaman et al., 2013) to evaluate relations in terms of recall, precision and F1 -score. The metric captures the temporal awareness of an annotation (UzZaman and Allen, 2011). Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. We calculate the Precision by checking the number of reduced system relations that can be verified from the reference annotation temporal closure graph, out of number of temporal relations in the reduced system relations. Similarly, we calculate the Recall by checking the number of reduced reference annotation relaFigure 2: Explicit and implicit relations resulting fro"
S15-2132,S13-2001,0,0.277531,"step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents. Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F1 -score of 7.85 in the main track (timeline creation from raw text). 1 • TempEval-1 (2007): Temporal Relation Identification (Verhagen et al., 2009) • TempEval-2 (2010): Evaluating Events, Time Expressions, and Temporal Relations (Verhagen et al., 2010) • TempEval-3 (2013): Temporal Annotation (UzZaman et al., 2013) Introduction In any domain, it is important that professionals have access to high quality knowledge for taking wellinformed decisions. As daily tasks of information professionals revolve around reconstructing a chain of previous events, an insightful way of presenting information to them is by means of timelines. The aim of the Cross-Document Event Ordering task is to build timelines from English news articles. To provide focus to the timeline creation, the task is presented as an ordering task in which events involving a particular target entity are to be ordered chronologically. The task f"
S15-2132,S10-1010,0,\N,Missing
S15-2132,S10-1001,0,\N,Missing
S16-1081,S16-1103,0,0.0432732,"n of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The nex"
S16-1081,S12-1051,1,0.454212,"for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translat"
S16-1081,S13-1004,1,0.536348,"n judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techni"
S16-1081,S14-2010,1,0.564132,"g the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fl"
S16-1081,S16-1101,1,0.859309,"Missing"
S16-1081,S16-1086,0,0.0343004,"Missing"
S16-1081,P98-1013,0,0.0704238,"E498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translation"
S16-1081,S16-1117,0,0.0317832,"Missing"
S16-1081,S16-1089,0,0.463664,"STS. The overall winner, Samsung Poland NLP Team, proposes a textual similarity model that is a novel hybrid of recursive auto-encoders from deep learning with penalty and reward signals extracted from WordNet (Rychalska et al., 2016). To obtain even better performance, this model is combined in an ensemble with a number of other similarity models including a version of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structure"
S16-1081,D15-1181,0,0.0283646,"nik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased approach for textual similarity that incorporates word alignment information (Itoh, 2016). 6"
S16-1081,S16-1170,0,0.023392,"s such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased"
S16-1081,N06-2015,0,0.0113871,"ual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations. We also introduce an evaluat"
S16-1081,S16-1106,0,0.0956972,"ween the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time systems without corrections are: ALL 0.68923; plagiarism 0.78949; answer-answer 0.48018; postediting 0.81241; headlines 0.76439; question-question 0.57140. Team Run ALL Ans.-Ans. HDL Plagiarism Postediting Ques.-Ques. Samsung Poland NLP Team UWB MayoNLPTeam Samsung Poland NLP Team NaCTeM ECNU UMD-TTIC"
S16-1081,P14-2124,0,0.0245172,"roximately 0.25 drop in correlation on the news data as compare to the multi-source setting; 2) systems performing evenly on both data sets. 6.5.1 Methods In terms of approaches, most runs rely on a monolingual framework. They automatically translate the Spanish member of a sentence pair into English and then compute monolingual semantic similarity using a system developed for English. In contrast, the CNRC team (Lo et al., 2016) provides a true crosslingual system that makes use of embedding space phrase similarity, the score from XMEANT, a crosslingual machine translation evaluation metric (Lo et al., 2014), and precision and recall features for material filling aligned cross-lingual semantic roles (e.g., action, agent, patient). The FBK HLT team (Ataman et al., 2016) proposes a model combining cross-lingual word embeddings with features from QuEst (Specia et al., 2013), a tool for machine translation quality estimation. The RTM system (Bic¸ici, 2016) also builds on methods developed for machine translation quality estimation and is applicable to both cross-lingual and monolingual similarity. The GWU NLP team (Aldarmaki and Diab, 2016) uses a shared cross-lingual vector space to directly assess"
S16-1081,S16-1102,0,0.0363344,"Missing"
S16-1081,P14-5010,0,0.0120217,"data sources we use for the evaluation sets. 3.1.1 Selection Heuristics Unless otherwise noted, pairs are heuristically selected using a combination of lexical surface form and word embedding similarity between a candidate pair of text snippets. The heuristics are used to find pairs sharing some minimal level of either surface or embedding space similarity. An approximately equal number of candidate sentence pairs are produced using our lexical surface form and word embedding selection heuristics. Both heuristics make use of a Penn Treebank style tokenization of the text provided by CoreNLP (Manning et al., 2014). 500 year 2016 2016 2016 dataset Trial News Multi-source pairs 103 301 294 source Sampled ≤ 2015 STS en-es news articles en news headlines, short-answer plag., MT postedits, Q&A forum answers, Q&A forum questions Table 3: Spanish-English subtask: Trial and test data sets. Surface Lexical Similarity Our surface form selection heuristic uses an information theoretic measure based on unigram overlap (Lin, 1998). As shown in equation (1), surface level lexical similarity between two snippets s1 and s2 is computed as a log probability weighted sum of the words common to both snippets divided by a"
S16-1081,D14-1162,0,0.109685,"Missing"
S16-1081,S16-1093,0,0.0144173,"ased model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engin"
S16-1081,S16-1091,0,0.0291873,"representations of the two snippets. 6.4 English Subtask The rankings for the English STS subtask are given in Tables 4 and 5. The baseline system ranked 100th. Table 6 provides the best and median scores for each of the individual evaluation sets as well as overall.15 The table also provides the difference between the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time sy"
S16-1081,P13-4014,0,0.0272301,"Missing"
S16-1081,2011.eamt-1.12,0,0.00782041,"swers. This corpus provides a collection of short answers to computer science questions that exhibit varying degrees of plagiarism from related Wikipedia articles.4 The short answers include text that was constructed by each of the following four strategies: 1) copying and pasting individual sentences from Wikipedia; 2) light revision of material copied from Wikipedia; 3) heavy revision of material from Wikipedia; 4) non-plagiarised answers produced without even looking at Wikipedia. This corpus is segmented into individual sentences using CoreNLP (Manning et al., 2014). 3.1.4 Postediting The Specia (2011) EAMT 2011 corpus provides machine translations of French news data using the Moses machine translation system (Koehn et al., 2007) paired with postedited corrections of those translations.5 The corrections were provided by human translators instructed to perform the minimum useful for finding semantically similar text snippets that differ in surface form. 4 Questions: A. What is inheritance in object orientated programming?, B. Explain the PageRank algorithm that is used by the Google search engine, C. Explain the Vector Space Model that is used for Information Retrieval., D. Explain Bayes Th"
S16-1081,S15-2027,0,0.0111195,"r to be significantly worse than the monolingual submissions even though the systems are being asked to perform the more challenging problem of evaluating crosslingual sentence pairs. While the correlations are not directly comparable, they do seem to motivate a more direct comparison between cross-lingual and monolingual STS systems. In terms of performance on the manually culled news data set, the highest overall rank is achieved by an unsupervised system submitted by team UWB (Brychcin and Svoboda, 2016). The unsupervised UWB system builds on the word alignment based STS method proposed by Sultan et al. (2015). However, when calculating the final similarity score, it weights both the aligned and unaligned words by their inverse document frequency. This system is able to attain a 0.912 correlation on the news data, while ranking second on the multi-source data set. For the multi-source test set, the highest scoring submission is a supervised system from the UWB team that combines multiple signals originating from lexical, syntactic and semantic similarity approaches in a regression-based model, achieving a 0.819 correlation. This is modestly better than the second place unsupervised approach that ac"
S16-1081,S16-1094,0,0.00995156,"th a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without"
S16-1081,C98-1013,0,\N,Missing
S16-1081,P07-2045,0,\N,Missing
S16-1082,agerri-etal-2014-ixa,1,0.844594,"each of the evaluation scenarios. The organizers provided a script to check if the run files are well formed. Nine teams participated on the gold chunks scenario, and out of them six teams also participated in the system chunks scenario. Regarding the datasets, all the teams gave their results for the three datasets, Baseline System The baseline system consists of a cascade concatenation of several procedures. First, input sentences are tokenized using simple regular expressions. Additionally, we collect chunks coming either from the gold standard or from the chunking done by ixapipes-chunk (Agerri et al., 2014). This is followed by a lower-cased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus en"
S16-1082,S15-2030,0,0.0872358,"• features, including WordNet and word embeddings. The system performs better in the system chunks scenario than in the gold chunks one. Therefore, there is no specific advantage of using chunked sentence pairs and their system is very powerful. The Answer-Students dataset has better performance than Headlines and Images. They obtain better results training a single system for the three datasets (compared to training a classifier separately for each dataset). Inspire (Kazmi and Sch¨uller, 2016): The authors propose a system based on logic programming which extends the basic ideas of NeroSim (Banjade et al., 2015). The rule based system makes use of several resources to prepare the input and uses Answer Set Programming to determine chunk boundaries. IISCNLP (Tekumalla and Sharmistha, 2016): The system uses an algorithm, iMATCH, for the alignment of multiple non-contiguous chunks based on Integer Linear Programming (ILP). Similarity type and score assignment for pairs of chunks is done using a supervised multiclass classification technique based on Random Forest Classifier. Vrep (Henry and Sands, 2016): features are extracted to create a learned rule-based classifier to assign a label. It uses semantic"
S16-1082,S16-1082,1,0.106867,"were lower for the system chunks. Both type and score are bounded by the alignment results and it is thus natural that alignment results are higher. Comparing type and score results, the type results are generally lower, possibly due to the harder task of guessing the correct label. The final results are bounded by both type and score, and the systems doing best in type are the ones doing best overall. From the results we can see that labeling the type was the most challenging. Regarding the overall test results for type and score (+TS) across datasets, UWB (Konop´ık et al., 2016) and DTSim (Banjade et al., 2016) obtained the best results for the gold chunks scenario, and DTSim and FBK-HLT-NLP (Magnolini et al., 2016) for the system chunks scenario. In addition, DTSim obtained the best overall results even though they have not good results for the Answer-Students dataset. 7 Systems, tools and resources Most of the teams reported input text processing such as lemmatization and part of speech tagging, and in some cases named-entity recognition and syntactic parsing. Additional resources such as WordNet, distributional embeddings, paraphrases from PPDB and global STS sentence scores were also used. Parti"
S16-1082,S13-2045,0,0.0371517,"ds for Headlines, and Student to Student-Answers. spend from three to five hours reading the material, building and observing circuits in the simulator and interacting with a dialogue-based tutor. They used the keyboard to interact with the system, and the computer tutor asked them questions and provided feedback via a text-based chat interface. The data from 73 undergraduate volunteer participants at south-eastern US university were recorded and annotated to form the BEETLE human-computer dialogue corpus (Dzikovska et al., 2010; Dzikovska et al., 2012), and later used in a SemEval 2015 task (Dzikovska et al., 2013). In the present corpus, we include sentence pairs composed of a student answer and the reference answer of a teacher. We have rejected those answers containing pronouns whose antecedent is not in the sentence (pronominal coreference), as the question is not included in the train data and, therefore, it is not possible to deduce which is the antecedent. There are also some dataset-specific details that are mentioned in the same section. The next pair sentences are an example of the AnswerStudents corpus. 2. Align chunks in order, from the clearest and strongest correspondences to the most uncl"
S16-1082,J07-3002,0,0.0101413,"alent as, in this dataset, X, Y, and Z always refer to switches X, Y, and Z. The same criteria is followed when annotating bulb c and C as equivalent, as A, B and C are always used to refer to bulb A, B and C. In the same way closed path and a path are equivalent, as paths are always considered to be closed. For further details related to such a corpus specific criteria refer to the annotation guidelines. 3 Evaluation Metrics The official evaluation is based on (Melamed, 1998), which uses the F1 of precision and recall of token alignments (in the context of alignment for Machine Translation). Fraser and Marcu (2007) argue that F1 is a better measure than other alternatives such as the Alignment Error Rate. The idea is that, for each pair of chunks that are aligned, we consider that any pairs of tokens in the chunks are also aligned with some weight. The weight of each token-token alignment is the inverse of the number of alignments of each token (so-called fan out factor, Melamed, 1998). Precision is measured as the ratio of token-token alignments that exist in both system and gold standard files, divided by the number of alignments in the system. Recall is measured similarly, as the ratio of token-token"
S16-1082,S16-1171,0,0.127777,"Missing"
S16-1082,S16-1124,0,0.063019,"Missing"
S16-1082,P13-2080,0,0.0414963,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. The SemEval Semantic Textual Similarity (STS) task in 2015 contained a subtask on Interpretable STS (Agirre et al., 2015), showing that the task is feasible, with high inter-annotator agreement and system scores well above baselines. The datasets comprised news headlines and image captions. For 2016, the pilot subtask has been updated into a standalone task. The restriction from the iSTS 2015 task to allow"
S16-1082,S16-1119,1,0.539962,"ures are extracted to create a learned rule-based classifier to assign a label. It uses semantic and syntactic (form of the chunks) relationship features. Rev (Ping Ping et al., 2016): The system consists of rules based on the analysis of the Headlines dataset considering lexical overlapping, part of speech tags and synonymy. Venseseval: This system is an adaptation of a pre-existing textual entailment system, VENSES, which first performs a semantic analysis of the text including argument structure and then looks for bridging information between chunks using several knowledge resources. iUBC (Lopez-Gazpio et al., 2016): A two layer architecture is used to produce the similarity type and score of pairs of chunks. The top layer consists of two models: a classifier and a regressor. The bottom layer consists of a recurrent neural network that processes input and feeds composed semantic feature vectors to the 519 top layer. Both layers are trained at the same time by propagating gradients. 8 Conclusions Last year, the Interpretable STS task was introduced as a pilot subtask of the STS task. At the present edition, it has been presented as an independent task that has attracted nine teams. In addition to the imag"
S16-1082,S16-1121,0,0.0814849,"s natural that alignment results are higher. Comparing type and score results, the type results are generally lower, possibly due to the harder task of guessing the correct label. The final results are bounded by both type and score, and the systems doing best in type are the ones doing best overall. From the results we can see that labeling the type was the most challenging. Regarding the overall test results for type and score (+TS) across datasets, UWB (Konop´ık et al., 2016) and DTSim (Banjade et al., 2016) obtained the best results for the gold chunks scenario, and DTSim and FBK-HLT-NLP (Magnolini et al., 2016) for the system chunks scenario. In addition, DTSim obtained the best overall results even though they have not good results for the Answer-Students dataset. 7 Systems, tools and resources Most of the teams reported input text processing such as lemmatization and part of speech tagging, and in some cases named-entity recognition and syntactic parsing. Additional resources such as WordNet, distributional embeddings, paraphrases from PPDB and global STS sentence scores were also used. Participants also revealed that most of their systems were built using some kind of distributional or knowledge-"
S16-1082,W10-0707,0,\N,Missing
S16-1082,W10-0721,0,\N,Missing
S16-1082,N12-1021,0,\N,Missing
S16-1082,S16-1122,0,\N,Missing
vossen-etal-2008-kyoto,W02-1304,1,\N,Missing
vossen-etal-2008-kyoto,W01-0703,1,\N,Missing
vossen-etal-2008-kyoto,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-kyoto,atserias-etal-2004-towards,1,\N,Missing
vossen-etal-2008-kyoto,soria-etal-2006-moving,1,\N,Missing
vossen-etal-2008-kyoto,chou-huang-2006-hantology,1,\N,Missing
vossen-etal-2008-kyoto,W06-1003,1,\N,Missing
vossen-etal-2014-newsreader,P98-1013,0,\N,Missing
vossen-etal-2014-newsreader,C98-1013,0,\N,Missing
vossen-etal-2014-newsreader,P10-1143,0,\N,Missing
vossen-etal-2014-newsreader,cattoni-etal-2012-knowledgestore,1,\N,Missing
vossen-etal-2014-newsreader,kipper-etal-2006-extending,0,\N,Missing
vossen-etal-2014-newsreader,W13-1202,1,\N,Missing
W00-0706,J98-1001,0,0.252615,"Missing"
W00-0706,kilgarriff-rosenzweig-2000-english,0,0.0194361,"ure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense. Thus, only the single most reliable piece of evidence is used to perform disambiguation. Regarding the details of implementation (smoothing, pruning of the decision list, etc.) we have followed (Agirre and Martinez, 2000). Decision Lists were one of the most successful systems on the 1st Senseval competition for WSD (Kilgarriff and Rosenzweig, 2000). E x e m p l a r - b a s e d Classifier (EB). In exemplar, instance, or memory-based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are simply stored in memory and the classification of new examples is based on the most similar stored exemplars. In our implementation, all examples are kept in memory and the classification is based on a k-NN (Nearest-Neighbours) algorithm using Hamming distance to measure closeness. For k's greater than 1, the resulting sense is the weighted majority sense of the k nearest neighbours --where each example"
W00-0706,J98-1006,0,0.0308183,"31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state-of-the-art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). are &quot;relevant&quot; for their class. When classifying a new example, SNo W is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNo W for WSD is explained in (Escudero et al., 2000c). S N o W is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context-sensitive spelling correction, POS tagging, P P - a t t a c h m e n t disambiguation, etc. D e c i s i o n L i s t s (DL). In this setting"
W00-0706,W96-0208,0,0.15912,"{escudero, lluism, g.rigau}@Isi.upc.es Abstract form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD - - i n the style of other studies devote"
W00-0706,P96-1006,0,0.0522569,"Missing"
W00-0706,W99-0502,0,0.0411649,"Missing"
W00-0706,W97-0323,0,0.175474,"a (UPC) Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia {escudero, lluism, g.rigau}@Isi.upc.es Abstract form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of th"
W00-0706,A97-1015,0,0.0261339,"revious methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997; Ratnaparkhi, 1999)-- is needed to assure the validity of the supervised approach, and to determine to which extent a tuning pre-process is necessary to make real WSD systems portable. In this direction, this work compares five different M L algorithms and explores their portability and tuning ability by training and testing them on different corpora. This paper describes a set of comparative experiments, including cross-corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNOW, Decision Lists, and"
W00-0706,P94-1013,0,0.231419,"a r d E s c u d e r o and L l u i s M h r q u e z and G e r m a n R i g a u TALP Research Center. LSI Department. Universitat Polit~cnica de C a t a l u n y a (UPC) Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia {escudero, lluism, g.rigau}@Isi.upc.es Abstract form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WS"
W00-0706,J99-2002,0,\N,Missing
W00-0706,J98-1005,0,\N,Missing
W00-0706,J98-4002,0,\N,Missing
W00-0706,J96-2004,0,\N,Missing
W00-0706,W00-1702,0,\N,Missing
W00-1322,J98-4002,0,0.0999223,"Missing"
W00-1322,P92-1032,0,0.0581055,"o the opinion of other authors (Ng, 1997b)): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to whi"
W00-1322,J98-1001,0,0.0568154,"Missing"
W00-1322,kilgarriff-rosenzweig-2000-english,0,0.0348502,"Missing"
W00-1322,J98-1006,0,0.218572,"ent corpora. Additionally, supervised methods suffer from the &quot;knowledge acquisition bottleneck&quot; (Gale et al., 1992a). (Ng, 1997b) estimates that the manual annotation effort necessary to build a broad coverage semantically annotated English corpus is about 16 personyears. This overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain. Due to this fact, recent works have focused on reducing the acquisition cost as well as the need for supervision in corpus-based methods. It is our belief that the research by (Leacock et al., 1998; Mihalcea and Moldovan, 1999) 2 provide enough evidence towards the &quot;opening&quot; of the bottleneck in the near future. For that reason, it is worth further investigating the robustness and portability of existing supervised ML methods to better resolve the WSD problem. It is important to note that the focus of this work will be on the empirical crosscorpus evaluation of several M L supervised algorithms. Other important issues, such as: selecting the best attribute set, discussing an appropriate definition of senses for the task, etc., are not addressed in this paper. 2 2.1 Learning Algorithms T"
W00-1322,W96-0208,0,0.0162853,"sical setting (Duda and Hart, 1973). T h a t is, assuming independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during training process using relative frequencies. To avoid the effect of zero counts when estimating probabilities, a very simple smoothing technique has been used, which was proposed in (Ng, 1997a). Despite its simplicity, Naive Bayes is claimed to obtain state-of-theart accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997a; Leacock et al., 1998). 2.2 E x e m p l a r - b a s e d Classifier (EB) In Exemplar-based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are stored in memory and the classification of new examples is based on the classes of the most similar stored examples. In our implementation, all examples are kept in memory and the classification of a new example is based on a k-NN (Nearest-Neighbours) algorithm using Hamming distance 3 to measure closeness (in doing so, all examples are examined). For k's greater t h a n 1, the resulting"
W00-1322,P96-1006,0,0.610856,"Ng, 1997b)): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is n"
W00-1322,W97-0323,0,0.188713,"e aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary t"
W00-1322,W97-0201,0,0.712724,"e aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary t"
W00-1322,A97-1015,0,0.0120727,"nally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary to make real WSD systems portable. In order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ML algorithms (previously applied to WSD) by training and testing them on different corpora. Additionally, supervised methods suffer from the &quot;knowledge acquisition bottleneck&quot; (Gale et al., 1992a). (Ng, 1997b) estimates that the manual annotation effort necessary to buil"
W00-1322,J98-1005,0,0.0162043,"Missing"
W00-1322,P94-1013,0,0.0919327,"Missing"
W00-1322,J99-2002,0,\N,Missing
W00-1322,W00-1702,0,\N,Missing
W01-1013,2000.iwpt-1.7,0,0.155609,"Missing"
W01-1013,P00-1064,0,0.0912682,"Missing"
W01-1013,O98-4002,1,0.865745,"Missing"
W01-1013,W98-0705,0,0.144557,"Missing"
W01-1013,M95-1017,0,\N,Missing
W02-1304,W02-1304,1,0.0511956,"Missing"
W02-1304,W00-1702,1,0.907606,"Missing"
W02-1304,W01-0703,1,0.824628,"Missing"
W02-1304,W99-0603,0,0.0765567,"Missing"
W02-1304,W00-1322,1,0.899983,"Missing"
W02-1304,J98-1001,0,0.0564481,"Missing"
W02-1304,J98-1006,0,0.0986902,"Missing"
W02-1304,magnini-cavaglia-2000-integrating,1,0.726135,"Missing"
W02-1304,W00-1326,1,0.886817,"Missing"
W02-1304,P98-2247,0,0.048344,"Missing"
W02-1304,S01-1029,1,0.792942,"Missing"
W02-1304,W97-0201,0,0.436936,"Missing"
W02-1304,S01-1017,1,\N,Missing
W02-1304,W00-1325,0,\N,Missing
W02-1304,P00-1064,0,\N,Missing
W02-1304,W00-0706,1,\N,Missing
W02-1304,P95-1026,0,\N,Missing
W02-1304,W02-0801,1,\N,Missing
W02-1304,C98-2242,0,\N,Missing
W04-0823,H93-1052,0,\N,Missing
W04-0823,J95-4004,0,\N,Missing
W04-0823,J92-1001,0,\N,Missing
W04-0823,P97-1007,1,\N,Missing
W04-0823,magnini-cavaglia-2000-integrating,0,\N,Missing
W04-0823,W99-0501,0,\N,Missing
W06-1663,agirre-de-lacalle-2004-publicly,0,0.781739,"Missing"
W06-1663,W01-0703,0,0.73024,"automatic means. For instance, in more than eight years of manual construction (from version 1.5 to 2.0), WordNet passed from 103,445 semantic relations to 204,074 semantic relations1 . That is, around twelve thousand semantic relations per year. However, during the last years the research community has devised a large set of innovative processes and tools for large-scale automatic acquisition of lexical knowledge from structured or unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001; Agirre and Martinez, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), largescale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or acquired from the BNC (Cuadros et al., 2005). Obviously, all these semantic resources have been acquired using a very different set of methods, tools and corpora, resulting on a different set of new semantic relations between synsets. In fact, each resource has different volume and accuracy figures. Although isolated evaluations have been performed by their developers in different experiThis paper prese"
W06-1663,J98-1006,0,0.228929,"Missing"
W06-1663,C00-1072,0,0.0857094,"and SemCor. • TSBNC: These Topic Signatures have been constructed using ExRetriever4 , a flexible tool to perform sense queries on large corpora. • MCR (Atserias et al., 2004): This knowledge resource uses the direct relations included in M CR. – This tool characterizes each sense of a word as a specific query using a declarative language. – This is automatically done by using a particular query construction strategy, defined a priori, and using information from a knowledge base. 2.2 Automatically retrieved Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from large volumes of text. In our case, we consider word senses as topics. Basically, the acquisition of TS consists of A) acquiring the best possible corpus examples for a particular word sense (usually characterizing each word sense as a query and performing a search on In this study, ExRetriever has been evaluated using the BNC, WN as a knowledge base and 3 4 536 http://ixa.si.ehu.es/Ixa/resources/sensecorpus http://www.lsi.upc.es/˜nlp/meaning/downloads.html TFIDF (as shown in formula 1) (Agirre and de la Calle, 200"
W06-1663,magnini-cavaglia-2000-integrating,0,0.25396,"Missing"
W08-2207,W06-1663,1,0.639518,"), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or knowledge about individuals from Wikipedia (Suchanek et al., 2007). Obviously, all these semantic resources have been acquired using a very different set of processes (Snow et al., 2006), tools and corpora. In fact, each semantic resource has different volume and accuracy figures when evaluated in a common and controlled framework (Cuadros and Rigau, 2006). However, not all available large-scale resources encode semantic relations between synsets. In some cases, only relations between synsets and words have been acquired. This is the case of the Topic Signatures (Agirre et al., 2000) acquired from the web (Agirre and de la Calle, 2004). This is one of the largest semantic resources ever built with around one hundred million relations between synsets and semantically related 1 Symmetric relations are counted only once. KnowNet: A Proposal for Building Knowledge Bases from the Web 73 words.2 A knowledge net or KnowNet, is an extensible, large and"
W08-2207,C08-1021,1,0.351528,"Missing"
W08-2207,E03-1020,0,0.0175059,"us, we plan to acquire by fully automatic means highly connected and dense knowledge bases from large corpora or the web by using the knowledge already available, increasing the total number of relations from less than one million (the current number of available relations) to millions. 4 http://www.lsi.upc.edu/~nlp/meaning KnowNet: A Proposal for Building Knowledge Bases from the Web 75 The current proposal consist of: • to follow Cuadros et al. (2005) and Cuadros and Rigau (2006) for acquiring highly accurate Topic Signatures for all monosemous words in WordNet (for instance, using InfoMap (Dorow and Widdows, 2003)). That is, to acquire word vectors closely related to a particular monosemous word (for instance, airport#n#1) from BNC or other large text collections like GigaWord, Wikipedia or the web. • to apply a very accurate knowledge–based all–words disambiguation algorithm to the Topic Signatures in order to obtain sense vectors instead of word vectors (for instance, using a version of Structural Semantic Interconnections algorithm (SSI) (Navigli and Velardi, 2005)). For instance, consider the first ten weighted words (with Part-of-Speech) appearing in the Topic Signature (TS) of the word sense airp"
W08-2207,J98-1006,0,0.00902896,"tal words) tammany#n alinement#n federalist#n whig#n missionary#j Democratic#n nazi#j republican#n constitutional#n organization#n 0.0319 0.0316 0.0315 0.0300 0.0229 0.0218 0.0202 0.0189 0.0186 0.0163 for those examples that best match the queries) • building the TS by deriving the context words that best represent the word sense from the selected corpora. The Topic Signatures acquired from the web (hereinafter TSWEB) constitutes one of the largest available semantic resources with around 100 million relations (between synsets and words) (Agirre and de la Calle, 2004). Inspired by the work of Leacock et al. (1998), TSWEB was constructed using monosemous relatives from WN (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. Thus, TSWEB consist of a large ordered list of words with weights associated to each of the senses of the polysemous nouns of WordNet 1.6. The number of constructed topic signatures is 35,250 with an average size per signature of 6,877 words. When evaluating TSWEB, we used at maximum the first 700 words while"
W08-2207,C00-1072,0,0.0623738,"c relations between synsets. After this introduction, Section 2 describes the Topic Signatures acquired from the web. Section 3 presents the approach we plan to follow for building highly dense and accurate knowledge bases. Section 4 describes the methods we followed for building KnowNet. In Section 5, we present the evaluation framework used in this study. Section 6 describes the results when evaluating different versions of KnowNet and finally, Section 7 presents some concluding remarks and future work. 2 Topic Signatures Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Topic Signatures are built by retrieving context words of a target topic from large corpora. In our case, we consider word senses as topics. Basically, the acquisition of TS consists of: • acquiring the best possible corpus examples for a particular word sense (usually characterising each word sense as a query and performing a search on the corpus 2 Available 3 These at http://ixa.si.ehu.es/Ixa/resources/sensecorpus KnowNet versions can be downloaded from http://adimen.si.ehu.es Cuadros and Rigau 74 Table 2: TS of party#n#1 (first 10 out of 12,890 total words) tammany#n alinement#n federalis"
W08-2207,magnini-cavaglia-2000-integrating,0,0.0131821,"sed NLP applications directly. It seems that applications will not scale up to working in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means. Obviously, this fact has severely hampered the state-of-the-art of advanced NLP applications. However, the Princeton WordNet is by far the most widely-used knowledge base (Fellbaum, 1998). In fact, WordNet is being used world-wide for anchoring different types of semantic knowledge including wordnets for languages other than English (Atserias et al., 2004), domain knowledge (Magnini and Cavaglià, 2000) or ontologies like SUMO (Niles and Pease, 2001) or the EuroWordNet Top Concept Ontology (Álvez et al., 2008). It contains manually coded information about nouns, verbs, adjectives and adverbs in English and is organised around the notion of a synset. A synset is a set of words with the same part-of-speech that can be interchanged in a certain context. For example, <party, political_party&gt; form a synset because they can be used to refer to the same concept. A synset is often further described by a gloss, in this case: ""an organisation to gain political power"" and by explicit semantic relations"
W08-2207,W02-1304,1,0.824418,"mum the first 20 words. For example, Table 2 present the first words (lemmas and part-of-speech) and weights of the Topic Signature acquired for party#n#1. 3 Building highly connected and dense knowledge bases It is our belief, that accurate semantic processing (such as WSD) would rely not only on sophisticated algorithms but on knowledge intensive approaches. In fact, the cycling arquitecture of the MEANING4 project demonstrated that acquiring better knowledge allow to perform better Word Sense Disambiguation (WSD) and that having improved WSD systems we are able to acquire better knowledge (Rigau et al., 2002). Thus, we plan to acquire by fully automatic means highly connected and dense knowledge bases from large corpora or the web by using the knowledge already available, increasing the total number of relations from less than one million (the current number of available relations) to millions. 4 http://www.lsi.upc.edu/~nlp/meaning KnowNet: A Proposal for Building Knowledge Bases from the Web 75 The current proposal consist of: • to follow Cuadros et al. (2005) and Cuadros and Rigau (2006) for acquiring highly accurate Topic Signatures for all monosemous words in WordNet (for instance, using InfoM"
W08-2207,P06-1101,0,0.0665202,"e-scale automatic acquisition of lexical knowledge from structured and unstructured corpora. Among others we can mention eXtended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic preferences acquired from SemCor (Agirre and Martinez, 2001, 2002) or acquired from British National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la Calle, 2004) or knowledge about individuals from Wikipedia (Suchanek et al., 2007). Obviously, all these semantic resources have been acquired using a very different set of processes (Snow et al., 2006), tools and corpora. In fact, each semantic resource has different volume and accuracy figures when evaluated in a common and controlled framework (Cuadros and Rigau, 2006). However, not all available large-scale resources encode semantic relations between synsets. In some cases, only relations between synsets and words have been acquired. This is the case of the Topic Signatures (Agirre et al., 2000) acquired from the web (Agirre and de la Calle, 2004). This is one of the largest semantic resources ever built with around one hundred million relations between synsets and semantically related 1"
W08-2207,agirre-de-lacalle-2004-publicly,0,\N,Missing
W08-2207,W01-0703,0,\N,Missing
W10-3301,alvez-etal-2008-complete,1,\N,Missing
W10-3301,E09-1005,1,\N,Missing
W11-1819,W11-1801,0,0.082567,"Missing"
W11-1819,W11-1802,0,\N,Missing
W11-1819,W09-1401,0,\N,Missing
W11-1819,W10-1901,0,\N,Missing
W13-0114,alvez-etal-2008-complete,1,0.735714,"of speech and semantic type agreement. Semantic Type: To extract the semantic type of the filler of a frame element, we first perform a very simple Word Sense Disambiguation (WSD) process assigning to each word, whenever possible, the most frequent sense of WordNet (Fellbaum, 1998). This heuristic has been used frequently as a baseline in the evaluation of WSD systems. As WordNet senses have been mapped to several ontologies, this disambiguation method allows us to label the documents with ontological features that can work as ´ semantic types. In this work we have used the Top Ontology (TO) (Alvez et al., 2008). We assign to each filler the ontological features of its syntactic head. In this way, we can learn from the training data and for each frame element a probability distribution of its semantic types. Table 2 contains some examples. Part of Speech: We also calculate the probability distribution of the part of speech (POS) of the head of the fillers similarly as for the semantic types. 4 Frame#FrameElement Expectation#Cognizer Residence#Location Attempt#Goal SemanticType Human Group Building Place Purpose UnboundEvent Object Part Probability 0.93 0.07 0.77 0.33 0.41 0.37 0.13 0.09 Table 2: Some"
W13-0114,P98-1013,0,0.113569,"Missing"
W13-0114,P87-1022,0,0.788088,"Missing"
W13-0114,P98-2241,0,0.0496556,"Missing"
W13-0114,S10-1059,0,0.112482,"that were really Null Instantiations, decided which of those NI were definite, and finally located the correct fillers of the DNIs. Two systems participated in the second sub-task: VENSES++ and SEMAFOR. VENSES++ (Tonelli and Delmonte, 2010) builds logical rules from syntactic parsing and uses handcrafted lexicons. They apply a rule based anaphora resolution procedure before employing semantic similarity between a NI and a potential filler using WordNet (Fellbaum, 1998). More recently, the same authors have tried to improve the performance of their system (Tonelli and Delmonte, 2011). SEMAFOR (Chen et al., 2010) is a supervised system that extends an existing semantic role labeller replacing the features defined for regular arguments with two new semantic features. First, their system checks if a potential filler in the context fills the null-instantiated role in one of the FrameNet sentences, and second, it calculates the distributional semantic similarity between the fillers and the roles. Although this system obtained the best performance in the task, data sparseness strongly affected the results. In a different approach, (Ruppenhofer et al., 2011) explore a number of linguistic strategies in orde"
W13-0114,P10-1160,0,0.0519113,"is simple. For filling DNIs they propose to use the semantic types specified for FEs in FrameNet. Following this line (Laparra and Rigau, 2012) presented a novel strategy for the DNI identification exploiting explicit Frame Elements annotations. Their approach gets the best results in the state of the art for DNI identification and showed its relevance in the DNI filling process. (Silberer and Frank, 2012) propose to solve the task adapting an entity-based coreference resolution model. In this work, the authors also extend automatically the training corpus to avoid data sparseness. Finally, (Gerber and Chai, 2010) define a closely related task characterizing the implicit arguments of some predicates appearing in NomBank (Meyers et al., 2004). They use a set of syntactic and semantic features to train a logistic regression classifier. The documents, obtained from the Wall Street Journal corpus, were already annotated with explicit arguments. Unlike SemEval-2010 task, the resulting dataset contains 1.253 predicate instances with an average of 1,8 roles annotated per instance. However just a set of ten different predicates is taken into account. 3 SEMEVAL-2010 dataset In the experiments reported in this p"
W13-0114,P00-1065,0,0.298081,"Missing"
W13-0114,J95-2003,0,0.782437,"Missing"
W13-0114,W04-2705,0,0.100225,"Rigau, 2012) presented a novel strategy for the DNI identification exploiting explicit Frame Elements annotations. Their approach gets the best results in the state of the art for DNI identification and showed its relevance in the DNI filling process. (Silberer and Frank, 2012) propose to solve the task adapting an entity-based coreference resolution model. In this work, the authors also extend automatically the training corpus to avoid data sparseness. Finally, (Gerber and Chai, 2010) define a closely related task characterizing the implicit arguments of some predicates appearing in NomBank (Meyers et al., 2004). They use a set of syntactic and semantic features to train a logistic regression classifier. The documents, obtained from the Wall Street Journal corpus, were already annotated with explicit arguments. Unlike SemEval-2010 task, the resulting dataset contains 1.253 predicate instances with an average of 1,8 roles annotated per instance. However just a set of ten different predicates is taken into account. 3 SEMEVAL-2010 dataset In the experiments reported in this paper, we have used the dataset distributed in SemEval-2010 for Task 10 “Linking Events and their Participants in Discourse”. The c"
W13-0114,P86-1004,0,0.886891,"Missing"
W13-0114,S12-1030,0,0.160955,"Missing"
W13-0114,D12-1016,0,0.229566,"Missing"
W13-0114,R11-1046,0,0.0975523,"e of their system (Tonelli and Delmonte, 2011). SEMAFOR (Chen et al., 2010) is a supervised system that extends an existing semantic role labeller replacing the features defined for regular arguments with two new semantic features. First, their system checks if a potential filler in the context fills the null-instantiated role in one of the FrameNet sentences, and second, it calculates the distributional semantic similarity between the fillers and the roles. Although this system obtained the best performance in the task, data sparseness strongly affected the results. In a different approach, (Ruppenhofer et al., 2011) explore a number of linguistic strategies in order to enhance the DNI identification. They conclude that a more sophisticated approach for DNI identification can improve significantly the performance of the whole pipeline, even if the method for the DNI filling is simple. For filling DNIs they propose to use the semantic types specified for FEs in FrameNet. Following this line (Laparra and Rigau, 2012) presented a novel strategy for the DNI identification exploiting explicit Frame Elements annotations. Their approach gets the best results in the state of the art for DNI identification and sho"
W13-0114,W09-2417,0,0.0914714,"ated work. Section 3 describes the SemEval-2010 task 10 dataset. Section 4 reviews a number of sources of evidence applied to the anaphora or coreference resolution tasks. We also propose how to adapt these features to select the appropriate fillers for the implicit arguments. Section 5 presents some experiments we have carried out to test these features. Section 6 discusses the initial results. Finally, section 7 offers some concluding remarks and presents some future researching. 2 Related Work Task 10 of SemEval-2010 focused on the evaluation of SRL systems based on the FrameNet paradigm1 (Ruppenhofer et al., 2009). This task was divided in two different sub-tasks: (i) Argument annotation in a traditional SRL manner. (ii) Filling null instantiatios over the document. The systems participating in the second subtask identified those missing Frame Elements that were really Null Instantiations, decided which of those NI were definite, and finally located the correct fillers of the DNIs. Two systems participated in the second sub-task: VENSES++ and SEMAFOR. VENSES++ (Tonelli and Delmonte, 2010) builds logical rules from syntactic parsing and uses handcrafted lexicons. They apply a rule based anaphora resolut"
W13-0114,T78-1012,0,0.509025,"Missing"
W13-0114,S12-1001,0,0.566892,"e correct filler for the DNI corresponding to FE Location, [the house], appears two sentences before: “Now, Mr. Holmes, with your permission, I will show you round the house.” The various bedrooms and sitting-rooms had yielded nothing to a careful search. Apparently [the tenantsResidence ]Resident had brought little or nothing with them. DNILocation Early studies on implicit arguments described this problem as a special case of anaphora or coreference resolution (Palmer et al., 1986; Whittemore et al., 1991; Tetreault, 2002). Also recent works cast this problem as an anaphora resolution task (Silberer and Frank, 2012). 1 In this work we present a detailed study of a set of features that have been traditionally used to model anaphora and coreference resolution tasks. We describe how these features manifest in a FrameNet based corpus for modeling implicit argument resolution, including an analysis of their benefits and drawbacks. The paper is structured as follows: section 2 discusses the related work. Section 3 describes the SemEval-2010 task 10 dataset. Section 4 reviews a number of sources of evidence applied to the anaphora or coreference resolution tasks. We also propose how to adapt these features to s"
W13-0114,S10-1065,0,0.635335,"ing. 2 Related Work Task 10 of SemEval-2010 focused on the evaluation of SRL systems based on the FrameNet paradigm1 (Ruppenhofer et al., 2009). This task was divided in two different sub-tasks: (i) Argument annotation in a traditional SRL manner. (ii) Filling null instantiatios over the document. The systems participating in the second subtask identified those missing Frame Elements that were really Null Instantiations, decided which of those NI were definite, and finally located the correct fillers of the DNIs. Two systems participated in the second sub-task: VENSES++ and SEMAFOR. VENSES++ (Tonelli and Delmonte, 2010) builds logical rules from syntactic parsing and uses handcrafted lexicons. They apply a rule based anaphora resolution procedure before employing semantic similarity between a NI and a potential filler using WordNet (Fellbaum, 1998). More recently, the same authors have tried to improve the performance of their system (Tonelli and Delmonte, 2011). SEMAFOR (Chen et al., 2010) is a supervised system that extends an existing semantic role labeller replacing the features defined for regular arguments with two new semantic features. First, their system checks if a potential filler in the context f"
W13-0114,W11-0908,0,0.809875,"entified those missing Frame Elements that were really Null Instantiations, decided which of those NI were definite, and finally located the correct fillers of the DNIs. Two systems participated in the second sub-task: VENSES++ and SEMAFOR. VENSES++ (Tonelli and Delmonte, 2010) builds logical rules from syntactic parsing and uses handcrafted lexicons. They apply a rule based anaphora resolution procedure before employing semantic similarity between a NI and a potential filler using WordNet (Fellbaum, 1998). More recently, the same authors have tried to improve the performance of their system (Tonelli and Delmonte, 2011). SEMAFOR (Chen et al., 2010) is a supervised system that extends an existing semantic role labeller replacing the features defined for regular arguments with two new semantic features. First, their system checks if a potential filler in the context fills the null-instantiated role in one of the FrameNet sentences, and second, it calculates the distributional semantic similarity between the fillers and the roles. Although this system obtained the best performance in the task, data sparseness strongly affected the results. In a different approach, (Ruppenhofer et al., 2011) explore a number of"
W13-0114,P91-1003,0,0.902819,"Missing"
W13-0114,H86-1011,0,\N,Missing
W13-0114,J78-3018,0,\N,Missing
W13-0114,C98-1013,0,\N,Missing
W13-0114,C98-2236,0,\N,Missing
W13-0114,J86-3001,0,\N,Missing
W14-0150,P06-1117,0,0.214154,"Missing"
W14-0150,gonzalez-agirre-etal-2012-multilingual,1,0.916695,"ish. VerbNet is organized into verb classes extending (Levin, 1993) classes through refinement and addition of subclasses to achieve syntactic and semantic coherence among members of a class. Each verb class in VerbNet is completely described by thematic-roles, selectional restrictions on the arguments, and frames consisting of a syntactic description and semantic predicates. WordNet 5 (Fellbaum, 1998) is by far the most widely-used knowledge base. In fact, WordNet is being used world-wide for anchoring different types of semantic knowledge including wordnets for languages other than English (Gonzalez-Agirre et al., 2012a). It contains manually coded information about English nouns, verbs, adjectives and adverbs and is organized around the notion of a synset. A synset is a set of words with the same part-of-speech that can be interchanged in a certain context. For example, &lt;learn, study, read, take&gt; form a synset because they can be used to refer to the same concept. A synset is often further described by a gloss, in this case: ”be a student of a certain subject” and by explicit semantic relations to other synsets. Each synset represents a concept that are related with an large number of semantic relations, i"
W14-0150,E12-1059,0,0.0604084,"et as one of the main problems of this resource. In fact, FrameNet1.5 covers around 10,000 lexical-units while for instance, WordNet3.0 contains more than 150,000 words. Furthermore, the same effort should be invested for each different language (Subirats and Petruck, 2003). Moreover, most previous research efforts on the integration of resources targeted at knowledge about nouns and named entities rather than predicate knowledge. Well known examples are YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), DBPedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2010) or UBY (Gurevych et al., 2012). Following the line of previous works (Shi and Mihalcea, 2005), (Burchardt et al., 2005), (Johansson and Nugues, 2007), (Pennacchiotti et al., 2008), (Cao et al., 2008), (Tonelli and Pianta, 2009), (Laparra et al., 2010), we will also focus on the integration of predicate information. We start from the basis of SemLink (Palmer, 2009). SemLink aimed to connect together different predicate resources such as FrameNet (Baker et al., 1997), VerbNet (Kipper, 2005), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). However, its coverage is still far from complete. The Predicate Matrix, th"
W14-0150,P13-1116,1,0.840557,"ledge allows to identify the underlying typical participants of a particular event independently of its realization in the text. Thus, using these models, different linguistic phenomena expressing the same event, such as active/passive transformations, verb alternations, nominalizations, implicit realizations can be harmonized into a common semantic representation. In fact, lately, several systems have been developed for shallow semantic parsing an explicit and implicit semantic role labeling using these resources (Erk and Pado, 2004), (Shi and Mihalcea, 2005), (Giuglea and Moschitti, 2006), (Laparra and Rigau, 2013). However, building large and rich enough predicate models for broad–coverage semantic processing takes a great deal of expensive manual effort involving large research groups during long periods of development. In fact, the coverage of currently available predicate-argument resources is still far from complete. For example, (Burchardt et al., 2005) or (Shen and Lapata, 2007) indicate the limited coverage of FrameNet as one of the main problems of this resource. In fact, FrameNet1.5 covers around 10,000 lexical-units while for instance, WordNet3.0 contains more than 150,000 words. Furthermore,"
W14-0150,P10-1023,0,0.0867785,"icate the limited coverage of FrameNet as one of the main problems of this resource. In fact, FrameNet1.5 covers around 10,000 lexical-units while for instance, WordNet3.0 contains more than 150,000 words. Furthermore, the same effort should be invested for each different language (Subirats and Petruck, 2003). Moreover, most previous research efforts on the integration of resources targeted at knowledge about nouns and named entities rather than predicate knowledge. Well known examples are YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), DBPedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2010) or UBY (Gurevych et al., 2012). Following the line of previous works (Shi and Mihalcea, 2005), (Burchardt et al., 2005), (Johansson and Nugues, 2007), (Pennacchiotti et al., 2008), (Cao et al., 2008), (Tonelli and Pianta, 2009), (Laparra et al., 2010), we will also focus on the integration of predicate information. We start from the basis of SemLink (Palmer, 2009). SemLink aimed to connect together different predicate resources such as FrameNet (Baker et al., 1997), VerbNet (Kipper, 2005), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). However, its coverage is still far from com"
W14-0150,J05-1004,0,0.900704,"e Predicate Matrix, we expect to provide a more robust interoperable lexicon by discovering and solving inherent inconsistencies among the resources. Moreover, we plan to extend the coverage of current predicate resources (by including from WordNet morphologically related nominal and verbal concepts), to enrich WordNet with predicate information, and possibly to extend predicate information to languages other than English (by exploiting the local wordnets aligned to the English WordNet). 1 Introduction Predicate models such as FrameNet (Baker et al., 1997), VerbNet (Kipper, 2005) or PropBank (Palmer et al., 2005) are core resources in most advanced NLP tasks, such as Question Answering, Textual Entailment or Information Extraction. Most of the systems with Natural Language Understanding capabilities require a large and precise amount of semantic knowledge at the predicateargument level. This type of knowledge allows to identify the underlying typical participants of a particular event independently of its realization in the text. Thus, using these models, different linguistic phenomena expressing the same event, such as active/passive transformations, verb alternations, nominalizations, implicit reali"
W14-0150,D08-1048,0,0.159497,"Missing"
W14-0150,D07-1002,0,0.0212981,"ely, several systems have been developed for shallow semantic parsing an explicit and implicit semantic role labeling using these resources (Erk and Pado, 2004), (Shi and Mihalcea, 2005), (Giuglea and Moschitti, 2006), (Laparra and Rigau, 2013). However, building large and rich enough predicate models for broad–coverage semantic processing takes a great deal of expensive manual effort involving large research groups during long periods of development. In fact, the coverage of currently available predicate-argument resources is still far from complete. For example, (Burchardt et al., 2005) or (Shen and Lapata, 2007) indicate the limited coverage of FrameNet as one of the main problems of this resource. In fact, FrameNet1.5 covers around 10,000 lexical-units while for instance, WordNet3.0 contains more than 150,000 words. Furthermore, the same effort should be invested for each different language (Subirats and Petruck, 2003). Moreover, most previous research efforts on the integration of resources targeted at knowledge about nouns and named entities rather than predicate knowledge. Well known examples are YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), DBPedia (Bizer et al., 2009), BabelN"
W14-0150,taule-etal-2008-ancora,0,0.255917,"Missing"
W14-0150,W09-3740,0,0.0145473,"fort should be invested for each different language (Subirats and Petruck, 2003). Moreover, most previous research efforts on the integration of resources targeted at knowledge about nouns and named entities rather than predicate knowledge. Well known examples are YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008), DBPedia (Bizer et al., 2009), BabelNet (Navigli and Ponzetto, 2010) or UBY (Gurevych et al., 2012). Following the line of previous works (Shi and Mihalcea, 2005), (Burchardt et al., 2005), (Johansson and Nugues, 2007), (Pennacchiotti et al., 2008), (Cao et al., 2008), (Tonelli and Pianta, 2009), (Laparra et al., 2010), we will also focus on the integration of predicate information. We start from the basis of SemLink (Palmer, 2009). SemLink aimed to connect together different predicate resources such as FrameNet (Baker et al., 1997), VerbNet (Kipper, 2005), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). However, its coverage is still far from complete. The Predicate Matrix, the resource resulting from the work presented in this paper, will allow to extend the coverage of current predicate resources (by including from WordNet closely related nominal and verbal concepts),"
W14-0150,W08-2208,0,\N,Missing
W14-0150,P98-1013,0,\N,Missing
W14-0150,C98-1013,0,\N,Missing
W14-0150,erk-pado-2004-powerful,0,\N,Missing
W15-0814,P98-1013,0,0.371456,"ressions, where the elements in the layers point to spans of terms. In the next examples, we show in NAF entities, a SR structure with a predicate and several of its roles, and a time expression for an English text. Each of the elements has a span element pointing to term identifiers that mark words and phrases in the text. We see in the first structure that the expression United States is detected as a named entity of the type LOCATION 111 and is disambiguated to a DBpedia entry.3 The SR element consists of a predicate and roles, where the predicate has references to various FrameNet frames (Baker et al., 1998) and WordNet synsets (Fellbaum, 1998) along with the predicate information included in the Predicate Matrix (Lacalle et al., 2014). The roles have a PropBank role (Palmer et al., 2005) and possibly one or more FrameNet elements.4 Finally, the time expression Monday has been normalised by reference to a particular date. <entity id=&quot;e3&quot; type=&quot;LOCATION&quot;> <!--United States--> <span><target id=&quot;t28&quot;/><target id=&quot;t29&quot;/></span> <externalReferences> <externalRef confidence=&quot;0.94&quot; reference=&quot;http://dbpedia.org/resource/United_States reftype=&quot;en&quot; resource=&quot;spotlight_v1&quot;/> </externalReferences> </entity>"
W15-0814,P10-1143,0,0.0188894,"tools, as shown by user evaluations (Hellmann et al., 2013). Because linguistic annotations are linked to strings it is furthermore not practical for representing hierarchical structures. (Fokkens et al., 2014) presents a more detailed discussion of the formal representations of linguistic annotations. Besides the formal representation of NLP output, our work relates to the representation of events and cross-document and cross-lingual event coreference. Cross-document event coreference so far has been addressed as a task, in which event markables are related to each other as coreference sets (Bejan and Harabagiu, 2010; Lee et al., 2012). For instance, the ECB corpus represents events and coreference relations using inline annotations in text and crossdocument identifiers with offset references. Representation and evaluation of cross-document eventcoreference is often done using scorers that use the CONLL-2011 format for expressing coreference (Pradhan et al., 2011). This format also exploits a simple token representation and identifiers. To the best of our knowledge, nobody really addressed the semantic representation of events as instances, exploiting interoperable semantic representations of event instan"
W15-0814,W13-1202,1,0.897189,"Missing"
W15-0814,lopez-de-lacalle-etal-2014-predicate,1,0.895255,"Missing"
W15-0814,D12-1045,0,0.0211208,"luations (Hellmann et al., 2013). Because linguistic annotations are linked to strings it is furthermore not practical for representing hierarchical structures. (Fokkens et al., 2014) presents a more detailed discussion of the formal representations of linguistic annotations. Besides the formal representation of NLP output, our work relates to the representation of events and cross-document and cross-lingual event coreference. Cross-document event coreference so far has been addressed as a task, in which event markables are related to each other as coreference sets (Bejan and Harabagiu, 2010; Lee et al., 2012). For instance, the ECB corpus represents events and coreference relations using inline annotations in text and crossdocument identifiers with offset references. Representation and evaluation of cross-document eventcoreference is often done using scorers that use the CONLL-2011 format for expressing coreference (Pradhan et al., 2011). This format also exploits a simple token representation and identifiers. To the best of our knowledge, nobody really addressed the semantic representation of events as instances, exploiting interoperable semantic representations of event instances and entity inst"
W15-0814,J05-1004,0,0.0331553,"expression for an English text. Each of the elements has a span element pointing to term identifiers that mark words and phrases in the text. We see in the first structure that the expression United States is detected as a named entity of the type LOCATION 111 and is disambiguated to a DBpedia entry.3 The SR element consists of a predicate and roles, where the predicate has references to various FrameNet frames (Baker et al., 1998) and WordNet synsets (Fellbaum, 1998) along with the predicate information included in the Predicate Matrix (Lacalle et al., 2014). The roles have a PropBank role (Palmer et al., 2005) and possibly one or more FrameNet elements.4 Finally, the time expression Monday has been normalised by reference to a particular date. <entity id=&quot;e3&quot; type=&quot;LOCATION&quot;> <!--United States--> <span><target id=&quot;t28&quot;/><target id=&quot;t29&quot;/></span> <externalReferences> <externalRef confidence=&quot;0.94&quot; reference=&quot;http://dbpedia.org/resource/United_States reftype=&quot;en&quot; resource=&quot;spotlight_v1&quot;/> </externalReferences> </entity> <predicate id=&quot;pr5&quot;> <!--flying--> <externalReferences> <externalRef reference=&quot;fn:Bringing&quot;, &quot;fn:Motion&quot;, &quot;fn:Operate_vehicle&quot;, &quot;fn:Ride_vehicle&quot;, &quot;fn:Self_motion&quot;, &quot;wn:ili-30-014518"
W15-0814,W11-1901,0,0.0568404,"Missing"
W15-4508,P99-1071,0,0.162011,"from scientists to intelligence analysts and web users. All of them are constantly struggling for synthesizing the relevant information from a particular topic. For instance, behind this overwhelmingly large collection of documents, it is often easy to miss the important details when trying to make sense of complex stories. To solve this problem various types of document processing systems have been recently proposed. For example, generic and queryfocused multi-document summarization systems aim to choose from the documents a subset of sentences that collectively conveys a query-related idea (Barzilay et al., 1999). News topic detection and tracking systems usually aim at grouping news articles into a cluster to present the events related to a certain topic (Allan, 2002). Timelines generation systems create summaries of relevant events in a topic by leveraging temporal information attached or appearing in the documents 1 http://alt.qcri.org/semeval2015/ task4/ 50 Proceedings of the First Workshop on Computing News Storylines, pages 50–55, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and The Asian Federation of Natural Language Processing Figure 1: Example of the Steve"
W15-4508,P09-1068,0,0.172441,"Missing"
W15-4508,D12-1062,0,0.0760122,"Missing"
W15-4508,P11-2061,0,0.0305647,"ome of them. We consider necessary to offer a metric which takes into account this type of outputs and also scores partial StoryLines. Obviously, a deeper study of the StoryLines casuistry will lead to a more complete and detailed evaluation metric. Line. Note that in real StoryLines all interacting entities should be annotated whereas now we only use those already selected by the TimeLines task. 3.2 Evaluation The evaluation methodology proposed in SemEval-2015 is based on the evaluation metric used for TempEval-3 (UzZaman et al., 2013) which captures the temporal awareness of an annotation (UzZaman and Allen, 2011). For that, they first transform the TimeLines into a set of temporal relations. More specifically, each time anchor is represented as a TIMEX3 so that each event is related to the corresponding TIMEX3 by means of the SIMULTANEOUS relation. In addition, SIMULTANEOUS and BEFORE relation types are used to connect the events. As a result, the TimeLine is represented as a graph and evaluated in terms of recall, precision and F1-score. As a first approach, the same graph representation can be used to characterize the StoryLines. Thus, for this trial we reuse the same evaluation metric as the one pr"
W15-4508,P15-2059,1,0.836145,"each StoryLine because the number of StoryLines is not set in advance. In addition, we also consider necessary to capture the cases in which having one gold standard StoryLine a system obtains 3.3 Example of a system-run In order to show that the dataset and evaluation strategy proposed are ready to be used on StoryLines, we follow the strategy described to build the gold annotations to implement an automatic system. This way, we create a simple system which merges automatically extracted TimeLines. To build the TimeLines, we use the system which currently obtains the best results in Track A (Laparra et al., 2015). The system follows a three step process to detect events, time-anchors and to sort the events according to their time-anchors. It captures explicit and implicit time-anchors and as a result, it obtains 14.31 F1-score. Thus, for each target entity, we first obtain the corresponding Timeline. Then, we check which TimeLines share the same events. In other words, which entities are co-participants of the same event and we build StoryLines from the TimeLines sharing events. This implies that more than two TimeLines can be merged into one single StoryLine. The system builds 2 StoryLines in the Air"
W15-4508,S13-2001,0,0.0256204,"system is not able to detect all the entities interacting in events but only some of them. We consider necessary to offer a metric which takes into account this type of outputs and also scores partial StoryLines. Obviously, a deeper study of the StoryLines casuistry will lead to a more complete and detailed evaluation metric. Line. Note that in real StoryLines all interacting entities should be annotated whereas now we only use those already selected by the TimeLines task. 3.2 Evaluation The evaluation methodology proposed in SemEval-2015 is based on the evaluation metric used for TempEval-3 (UzZaman et al., 2013) which captures the temporal awareness of an annotation (UzZaman and Allen, 2011). For that, they first transform the TimeLines into a set of temporal relations. More specifically, each time anchor is represented as a TIMEX3 so that each event is related to the corresponding TIMEX3 by means of the SIMULTANEOUS relation. In addition, SIMULTANEOUS and BEFORE relation types are used to connect the events. As a result, the TimeLine is represented as a graph and evaluated in terms of recall, precision and F1-score. As a first approach, the same graph representation can be used to characterize the S"
W15-4508,J05-1004,0,0.0515529,"ntity, by detecting the events in which the entity is involved and anchoring these events to normalized times. Thus, a TimeLine is a collection of ordered events in time relevant for a particular entity. Figure 1 shows the TimeLine extracted for the target entity Steve Jobs using information from 3 different documents. The events in bold form the TimeLine that can be placed on a TimeLine according to the task annotation guidelines (Minard et al., 2014). TimeLines contain relevant events in which the target entity participates as ARG0 (i.e agent) or ARG1 (i.e. patient) as defined in Prop-Bank (Palmer et al., 2005). Events such as adjectival events, cognitive events, counter-factual events, uncertain events and grammatical events 3 A Proposal for StoryLines In this section we present a first proposal for a novel evaluation task for StoryLines. We propose that a StoryLine can be built by merging the individual TimeLines of two or more different entities, provided that they are co-participants of at least one relevant event. In general, given a set of related documents, any entity appearing in the corpus is a candidate to take 2 A complete description of the annotation guidelines can be found at http://ww"
W15-4508,S15-2132,1,\N,Missing
W15-4508,P08-1090,0,\N,Missing
W98-0709,W97-0802,0,0.0413103,"Missing"
W98-0709,H94-1025,0,0.345534,"Missing"
W98-0709,P97-1007,1,0.884567,"Missing"
W98-0709,P98-2181,1,0.893481,"Missing"
W98-0709,P95-1026,0,0.0961408,"Missing"
W98-0709,W97-0805,0,0.0207339,"Missing"
