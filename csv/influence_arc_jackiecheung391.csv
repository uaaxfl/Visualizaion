2020.coling-main.106,E17-1005,0,0.130828,"Kim Sang and De Meulder, 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classes in total. We report the micro F1 score for the supersense tagging and the NER tagging tasks, discarding the O-tags in the predictions, as is standard (Alonso and Plank, 2017; Changpinyo et al., 2018). For the rest of the tasks, we report the accuracy on 4 Retrieved from the NLTK toolkit: http://www.nltk.org/nltk_data/. 1234 Model Average Concat DME Prism Average Concat DME Prism Facets FG FG FG FG All All All All Semcor 69.42 ± .1 72.23 ± .2 72.15 ± .2 73.51 ± .1 65.34 ± .3 73.95 ± .1 72.09 ± .1 73.82 ± .2 WSJ 96.76 ± .02 96.85 ± .04 96.81 ± .03 96.91 ± .01 96.63 ± .01 97.02 ± .01 96.89 ± .01 97.04 ± .01 Brown 98.44 ± .02 98.53 ± .02 98.53 ± .02 98.58 ± .02 98.21 ± .03 98.63 ± .01 98.58 ± .02 98.65 ± .01 NER 90.16 ± .2 90.49 ± .1 89.49 ± .2 90.70 ± .4 88.92 ± .3"
2020.coling-main.106,2020.tacl-1.21,1,0.748637,"to obtain a parsed corpus. We also experiment with the following off-the-shelf embeddings: GloVe (Pennington et al., 2014); trained on 840B tokens from the Common Crawl Corpus with 2.2M words in the vocabulary. FastText (Joulin et al., 2017); trained on 600B tokens from the Common Crawl Corpus with 2M words in the vocabulary. ConceptNet Numberbatch (Speer et al., 2017); retrofitted (Faruqui et al., 2015) on both Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014) with 516K words in the vocabulary; this facet allows us to incorporate information from knowledge graphs. LexSub (Arora et al., 2020); GloVe embeddings trained on 6B tokens from Wikipedia 2014 and the Gigaword 5 corpus (Parker et al., 2011), modified so that they can easily be projected into “lexical subspaces”, in which a word’s nearest neighbours reflect a particular lexical relation (e.g. synonymy, antonymy, hypernymy, meronymy). 4 Experiments Our experiments seek to determine if: (1) word prisms offer improvements over the other common meta-embedding methods; and, (2) if it is desirable to produce meta-embeddings with many different notions of context from the same corpus. For (1), we pursue a variety of experiments com"
2020.coling-main.106,D16-1250,0,0.126921,"a word’s surrounding context, but this provides little to no improvement on their downstream evaluations. By contrast, we propose a simpler attention mechanism by learning a single importance weight for each source embedding set, and we apply orthogonal transformations to source embeddings prior to linear combination. We also experiment with a larger selection of source embeddings, including embeddings trained with different notions of context. Orthogonal transformations have previously been employed in the context of mapping monolingual embeddings for different languages into a common space (Artetxe et al., 2016; Smith et al., 2017; Artetxe et al., 2018; Conneau et al., 2018; Doval et al., 2018). With these alignment transformations on monolingual space, one can obtain a better cross-lingual integration of the vector spaces. Recent work has also found that applying orthogonal transformations to source embeddings facilitates averaging (Garc´ıa et al., 2020; Jawanpuria et al., 2020). We expand on this work by incorporating orthogonal transformations in word prisms, which learn word meta-embeddings for specific downstream tasks. Additionally, we provide an analysis of source embeddings before and after"
2020.coling-main.106,P14-2131,0,0.113697,"amous word embedding techniques such as Word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclusive evidence that any single notion of context could be the best for solving NLP problems in general. Thus, many deep learning solutions for NLP have yet another hyperparameter to tune: what set of word embeddings should be selected for the input layer of the model. As NLP tasks become more and more complex, the practice of providing a deep model with only one notion of a word’s meaning becomes limiting. Word meta-embeddings address aspects of this problem by proposing techniques for combining multiple se"
2020.coling-main.106,C18-1140,0,0.0227944,"solid baselines on word similarity, analogy, and POS tagging tasks. They propose 1 TO N, which simultaneously learns meta-embeddings and projections from the meta-embedding space to each individual source embedding space. Ghannay et al. (2016) apply PCA and autoencoders after concatenating source embeddings. Zhang et al. (2016) apply a convolutional layer to each source embedding before concatenating the resulting feature maps. Bollegala et al. (2018) represent the meta-embedding for a word as a linear combination of the meta-embeddings for its nearest neighbours in each source embedding set. Bao and Bollegala (2018) produce meta-embeddings by either averaging or concatenating the outputs of encoders which take GloVe and CBOW (Mikolov et al., 2013a) embeddings as input. Coates and Bollegala (2018) demonstrate that, in certain settings, meta-embeddings produced by averaging can be as performant as concatenated ones. Kiela et al. (2018) propose dynamic meta-embeddings 1231 (DMEs), which perform attention over the linearly transformed source embeddings. The linear transformations applied to the source embeddings are not constrained to be orthogonal. Their model learns which source embedding sets are most use"
2020.coling-main.106,D15-1075,0,0.0286668,"are learned via backpropagation from supervised learning during the current task; the αf are obtained via a self-attention 0 mechanism on an additional learned parameter vector a ∈ Rd : αf = φ(a · (Pf wf + bf ) + b), where φ is the softmax function and b ∈ R is an additional learned bias parameter. 4.2 Datasets and downstream models We evaluate meta-embedding methods on a variety of downstream text classification and sequence labelling tasks. For text classification, we choose the Stanford Sentiment Treebank binary sentiment analysis dataset (SST2) (Socher et al., 2013) and the Stanford NLI (Bowman et al., 2015) (SNLI) benchmark. For sequence labelling, we select the CoNLL 2003 named entity recognition task (NER) (Tjong Kim Sang and De Meulder, 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classes in total. We report the micro F1 score fo"
2020.coling-main.106,C18-1251,0,0.111494,", 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classes in total. We report the micro F1 score for the supersense tagging and the NER tagging tasks, discarding the O-tags in the predictions, as is standard (Alonso and Plank, 2017; Changpinyo et al., 2018). For the rest of the tasks, we report the accuracy on 4 Retrieved from the NLTK toolkit: http://www.nltk.org/nltk_data/. 1234 Model Average Concat DME Prism Average Concat DME Prism Facets FG FG FG FG All All All All Semcor 69.42 ± .1 72.23 ± .2 72.15 ± .2 73.51 ± .1 65.34 ± .3 73.95 ± .1 72.09 ± .1 73.82 ± .2 WSJ 96.76 ± .02 96.85 ± .04 96.81 ± .03 96.91 ± .01 96.63 ± .01 97.02 ± .01 96.89 ± .01 97.04 ± .01 Brown 98.44 ± .02 98.53 ± .02 98.53 ± .02 98.58 ± .02 98.21 ± .03 98.63 ± .01 98.58 ± .02 98.65 ± .01 NER 90.16 ± .2 90.49 ± .1 89.49 ± .2 90.70 ± .4 88.92 ± .3 90.55 ± .1 89.36 ± .3 90.7"
2020.coling-main.106,W03-1022,0,0.08019,"parameter. 4.2 Datasets and downstream models We evaluate meta-embedding methods on a variety of downstream text classification and sequence labelling tasks. For text classification, we choose the Stanford Sentiment Treebank binary sentiment analysis dataset (SST2) (Socher et al., 2013) and the Stanford NLI (Bowman et al., 2015) (SNLI) benchmark. For sequence labelling, we select the CoNLL 2003 named entity recognition task (NER) (Tjong Kim Sang and De Meulder, 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classes in total. We report the micro F1 score for the supersense tagging and the NER tagging tasks, discarding the O-tags in the predictions, as is standard (Alonso and Plank, 2017; Changpinyo et al., 2018). For the rest of the tasks, we report the accuracy on 4 Retrieved from the NLTK toolkit: http://www.nltk.org/nltk_data/. 1234 Model Averag"
2020.coling-main.106,N18-2031,0,0.317485,"the original space of facet embeddings. It allows the combination of multiple source embeddings while preserving most information within each embedding set. To our knowledge, this work is the first to incorporate both explicit orthogonal transformations of source embeddings and importance weights for source embedding sets that are dynamically learned with the downstream tasks in the same meta-embedding method. Furthermore, it is the first to explore combining so many sets of source embeddings (thirteen). We compare the word prisms method to other standard meta-embedding algorithms (averaging (Coates and Bollegala, 2018), concatenation (Yin and Sch¨utze, 2016), and dynamic meta-embeddings, DMEs, (Kiela et al., 2018)). Word prisms overcome the shortcomings of each of these algorithms: (1) in averaging, performance deteriorates considerably when there are many facets — the orthogonal transformations in word prisms resolve this problem; (2) concatenation and DMEs are too expensive during inference when there are many facets — word prisms only need the final meta-embeddings at inference time, making them as efficient as averaging. Our results demonstrate that neural downstream models using word prisms generally o"
2020.coling-main.106,N19-1423,0,0.0186743,"sistent improvements over dynamic meta-embeddings (Kiela et al., 2018) and the averaging and concatenation baselines (Coates and Bollegala, 2018) in all six tasks. Analysis of the transformed embeddings suggests the “natural clustering” hypothesis for representation learning (Bengio et al., 2013) is important to consider for combining various source embeddings to create performant taskspecific meta-embeddings. Several future directions present themselves from this work. First, we believe that contextualized embedding models can benefit from prismatic representations of their input embeddings (Devlin et al., 2019), and that word prisms can benefit from including contextualized embeddings as facets. Second, we expect that word prisms can improve performance in other tasks such as automatic summarization, which often use a single set of word embeddings in their input layers (Dong et al., 2019). Third, we believe that meta-embeddings and the method behind word prisms can be generalized past word-based representations to sentence representations (Pagliardini et al., 2018) and may improve their quality, as was recently demonstrated by Poerner et al. (2019). Lastly, recent work has found simple word embeddin"
2020.coling-main.106,P19-1331,1,0.781268,"t al., 2013) is important to consider for combining various source embeddings to create performant taskspecific meta-embeddings. Several future directions present themselves from this work. First, we believe that contextualized embedding models can benefit from prismatic representations of their input embeddings (Devlin et al., 2019), and that word prisms can benefit from including contextualized embeddings as facets. Second, we expect that word prisms can improve performance in other tasks such as automatic summarization, which often use a single set of word embeddings in their input layers (Dong et al., 2019). Third, we believe that meta-embeddings and the method behind word prisms can be generalized past word-based representations to sentence representations (Pagliardini et al., 2018) and may improve their quality, as was recently demonstrated by Poerner et al. (2019). Lastly, recent work has found simple word embeddings to be useful for solving diverse problems from the medical domain (Zhang et al., 2019), to materials science (Tshitoyan et al., 2019), to law (Chalkidis and Kampas, 2019); we expect that word prisms and their motivations can further improve results in these applications. Acknowle"
2020.coling-main.106,D18-1027,0,0.0212369,"stream evaluations. By contrast, we propose a simpler attention mechanism by learning a single importance weight for each source embedding set, and we apply orthogonal transformations to source embeddings prior to linear combination. We also experiment with a larger selection of source embeddings, including embeddings trained with different notions of context. Orthogonal transformations have previously been employed in the context of mapping monolingual embeddings for different languages into a common space (Artetxe et al., 2016; Smith et al., 2017; Artetxe et al., 2018; Conneau et al., 2018; Doval et al., 2018). With these alignment transformations on monolingual space, one can obtain a better cross-lingual integration of the vector spaces. Recent work has also found that applying orthogonal transformations to source embeddings facilitates averaging (Garc´ıa et al., 2020; Jawanpuria et al., 2020). We expand on this work by incorporating orthogonal transformations in word prisms, which learn word meta-embeddings for specific downstream tasks. Additionally, we provide an analysis of source embeddings before and after orthogonal transformation, which leads to the insight that these mappings cause sourc"
2020.coling-main.106,N15-1184,0,0.0723821,"Missing"
2020.coling-main.106,L16-1046,0,0.163832,"al. (2016) combine embeddings trained with different notions of context via concatenation, as well as via SVD and CCA, leading to improved performance in multiple downstream tasks. However, they only combine two embedding sets at a time. Yin and Sch¨utze (2016) introduce the term “meta-embeddings” and demonstrate that concatenation and singular value decomposition (SVD) are solid baselines on word similarity, analogy, and POS tagging tasks. They propose 1 TO N, which simultaneously learns meta-embeddings and projections from the meta-embedding space to each individual source embedding space. Ghannay et al. (2016) apply PCA and autoencoders after concatenating source embeddings. Zhang et al. (2016) apply a convolutional layer to each source embedding before concatenating the resulting feature maps. Bollegala et al. (2018) represent the meta-embedding for a word as a linear combination of the meta-embeddings for its nearest neighbours in each source embedding set. Bao and Bollegala (2018) produce meta-embeddings by either averaging or concatenating the outputs of encoders which take GloVe and CBOW (Mikolov et al., 2013a) embeddings as input. Coates and Bollegala (2018) demonstrate that, in certain setti"
2020.coling-main.106,2020.repl4nlp-1.6,0,0.193143,"embeddings, including embeddings trained with different notions of context. Orthogonal transformations have previously been employed in the context of mapping monolingual embeddings for different languages into a common space (Artetxe et al., 2016; Smith et al., 2017; Artetxe et al., 2018; Conneau et al., 2018; Doval et al., 2018). With these alignment transformations on monolingual space, one can obtain a better cross-lingual integration of the vector spaces. Recent work has also found that applying orthogonal transformations to source embeddings facilitates averaging (Garc´ıa et al., 2020; Jawanpuria et al., 2020). We expand on this work by incorporating orthogonal transformations in word prisms, which learn word meta-embeddings for specific downstream tasks. Additionally, we provide an analysis of source embeddings before and after orthogonal transformation, which leads to the insight that these mappings cause source embedding sets to be more easily clusterable within the meta-embedding space. 3 Word Prisms and Meta-Embeddings In this section we introduce meta-embeddings, word prisms, and the source embeddings they are composed with in this work. A meta-embedding combines pre-trained embeddings from m"
2020.coling-main.106,E17-2068,0,0.288796,"d prisms (§3) during supersense tagging (§4). We display the nearest neighbor to the embedding for the word “apple” in five of the window-based facets (Figure 2, §3.2) used in the downstream model (§4). We include the ⊥ symbol on the learned transformation, Pf , to indicate its orthogonality. Levy et al., 2015; Newell et al., 2019), the gains to be found in diversifying at the level of algorithmic variation are likely to be minimal. With regard to vocabulary coverage, the out-of-vocabulary problem is at least partially addressed by character n-gram based embedding algorithms such as FastText (Joulin et al., 2017) and subword-based decomposition techniques that can be applied post-training (Zhao et al., 2018; Sasaki et al., 2019). Nonetheless, meta-embeddings have been shown to consistently outperform models that use only a single set of embeddings in their input layer. Our goal is to determine how to best combine many sets of input embeddings in order to obtain high quality results in downstream tasks. This work proposes word prisms as a simple and general way to produce and understand metaembeddings, visualized in Figure 1. Word prisms excel at combining many sets of source embeddings, which we call"
2020.coling-main.106,D18-1176,0,0.451667,"serving most information within each embedding set. To our knowledge, this work is the first to incorporate both explicit orthogonal transformations of source embeddings and importance weights for source embedding sets that are dynamically learned with the downstream tasks in the same meta-embedding method. Furthermore, it is the first to explore combining so many sets of source embeddings (thirteen). We compare the word prisms method to other standard meta-embedding algorithms (averaging (Coates and Bollegala, 2018), concatenation (Yin and Sch¨utze, 2016), and dynamic meta-embeddings, DMEs, (Kiela et al., 2018)). Word prisms overcome the shortcomings of each of these algorithms: (1) in averaging, performance deteriorates considerably when there are many facets — the orthogonal transformations in word prisms resolve this problem; (2) concatenation and DMEs are too expensive during inference when there are many facets — word prisms only need the final meta-embeddings at inference time, making them as efficient as averaging. Our results demonstrate that neural downstream models using word prisms generally obtain better results than the other algorithms across six downstream tasks, including supersense"
2020.coling-main.106,P14-2050,0,0.611621,"ntuition is the basis of famous word embedding techniques such as Word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclusive evidence that any single notion of context could be the best for solving NLP problems in general. Thus, many deep learning solutions for NLP have yet another hyperparameter to tune: what set of word embeddings should be selected for the input layer of the model. As NLP tasks become more and more complex, the practice of providing a deep model with only one notion of a word’s meaning becomes limiting. Word meta-embeddings address aspects of this problem by proposing techniques for"
2020.coling-main.106,Q15-1016,0,0.0389462,"work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 1229 Proceedings of the 28th International Conference on Computational Linguistics, pages 1229–1241 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: Word prisms (§3) during supersense tagging (§4). We display the nearest neighbor to the embedding for the word “apple” in five of the window-based facets (Figure 2, §3.2) used in the downstream model (§4). We include the ⊥ symbol on the learned transformation, Pf , to indicate its orthogonality. Levy et al., 2015; Newell et al., 2019), the gains to be found in diversifying at the level of algorithmic variation are likely to be minimal. With regard to vocabulary coverage, the out-of-vocabulary problem is at least partially addressed by character n-gram based embedding algorithms such as FastText (Joulin et al., 2017) and subword-based decomposition techniques that can be applied post-training (Zhao et al., 2018; Sasaki et al., 2019). Nonetheless, meta-embeddings have been shown to consistently outperform models that use only a single set of embeddings in their input layer. Our goal is to determine how"
2020.coling-main.106,N15-1144,0,0.0416496,"techniques such as Word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclusive evidence that any single notion of context could be the best for solving NLP problems in general. Thus, many deep learning solutions for NLP have yet another hyperparameter to tune: what set of word embeddings should be selected for the input layer of the model. As NLP tasks become more and more complex, the practice of providing a deep model with only one notion of a word’s meaning becomes limiting. Word meta-embeddings address aspects of this problem by proposing techniques for combining multiple sets of word embeddi"
2020.coling-main.106,W17-0239,0,0.0809226,"al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclusive evidence that any single notion of context could be the best for solving NLP problems in general. Thus, many deep learning solutions for NLP have yet another hyperparameter to tune: what set of word embeddings should be selected for the input layer of the model. As NLP tasks become more and more complex, the practice of providing a deep model with only one notion of a word’s meaning becomes limiting. Word meta-embeddings address aspects of this problem by proposing techniques for combining multiple sets of word embeddings before providing them into the input layer o"
2020.coling-main.106,P14-5010,0,0.00295917,"Figure 2. Letting W be the window size, we trained the following sets of embeddings: W1, W2, W5, W10, and W20. Furthermore, we trained embeddings using only a Left context of 5 words, and another set of embeddings with only a Right context of 5 words. Lastly, we trained a set of embeddings with only a Far context window, which only includes words between 20 and 30 words away, in order to create strong topic-based representations. We also trained a variant of dependency-based embeddings (Deps) (Levy and Goldberg, 2014a), where we defined a word’s context to be its governor. We ran the CoreNLP (Manning et al., 2014) dependency parser on Gigaword 3 to obtain a parsed corpus. We also experiment with the following off-the-shelf embeddings: GloVe (Pennington et al., 2014); trained on 840B tokens from the Common Crawl Corpus with 2.2M words in the vocabulary. FastText (Joulin et al., 2017); trained on 600B tokens from the Common Crawl Corpus with 2M words in the vocabulary. ConceptNet Numberbatch (Speer et al., 2017); retrofitted (Faruqui et al., 2015) on both Word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014) with 516K words in the vocabulary; this facet allows us to incorporate information"
2020.coling-main.106,J93-2004,0,0.0702773,"nction and b ∈ R is an additional learned bias parameter. 4.2 Datasets and downstream models We evaluate meta-embedding methods on a variety of downstream text classification and sequence labelling tasks. For text classification, we choose the Stanford Sentiment Treebank binary sentiment analysis dataset (SST2) (Socher et al., 2013) and the Stanford NLI (Bowman et al., 2015) (SNLI) benchmark. For sequence labelling, we select the CoNLL 2003 named entity recognition task (NER) (Tjong Kim Sang and De Meulder, 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classes in total. We report the micro F1 score for the supersense tagging and the NER tagging tasks, discarding the O-tags in the predictions, as is standard (Alonso and Plank, 2017; Changpinyo et al., 2018). For the rest of the tasks, we report the accuracy on 4 Retrieved from the NLTK tool"
2020.coling-main.106,N16-1118,0,0.0738532,"Word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclusive evidence that any single notion of context could be the best for solving NLP problems in general. Thus, many deep learning solutions for NLP have yet another hyperparameter to tune: what set of word embeddings should be selected for the input layer of the model. As NLP tasks become more and more complex, the practice of providing a deep model with only one notion of a word’s meaning becomes limiting. Word meta-embeddings address aspects of this problem by proposing techniques for combining multiple sets of word embeddings before providing t"
2020.coling-main.106,H93-1061,0,0.126252,"embedding methods on a variety of downstream text classification and sequence labelling tasks. For text classification, we choose the Stanford Sentiment Treebank binary sentiment analysis dataset (SST2) (Socher et al., 2013) and the Stanford NLI (Bowman et al., 2015) (SNLI) benchmark. For sequence labelling, we select the CoNLL 2003 named entity recognition task (NER) (Tjong Kim Sang and De Meulder, 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classes in total. We report the micro F1 score for the supersense tagging and the NER tagging tasks, discarding the O-tags in the predictions, as is standard (Alonso and Plank, 2017; Changpinyo et al., 2018). For the rest of the tasks, we report the accuracy on 4 Retrieved from the NLTK toolkit: http://www.nltk.org/nltk_data/. 1234 Model Average Concat DME Prism Average Concat DME Prism Facets FG FG"
2020.coling-main.106,N18-1049,0,0.0234106,"from this work. First, we believe that contextualized embedding models can benefit from prismatic representations of their input embeddings (Devlin et al., 2019), and that word prisms can benefit from including contextualized embeddings as facets. Second, we expect that word prisms can improve performance in other tasks such as automatic summarization, which often use a single set of word embeddings in their input layers (Dong et al., 2019). Third, we believe that meta-embeddings and the method behind word prisms can be generalized past word-based representations to sentence representations (Pagliardini et al., 2018) and may improve their quality, as was recently demonstrated by Poerner et al. (2019). Lastly, recent work has found simple word embeddings to be useful for solving diverse problems from the medical domain (Zhang et al., 2019), to materials science (Tshitoyan et al., 2019), to law (Chalkidis and Kampas, 2019); we expect that word prisms and their motivations can further improve results in these applications. Acknowledgments This work is supported by the Fonds de recherche du Qu´ebec – Nature et technologies, by the Natural Sciences and Engineering Research Council of Canada, and by Compute Can"
2020.coling-main.106,D14-1162,0,0.0998023,"ask at hand. Word prisms learn orthogonal transformations to linearly combine the input source embeddings, which allows them to be very efficient at inference time. We evaluate word prisms in comparison to other meta-embedding methods on six extrinsic evaluations and observe that word prisms offer improvements in performance on all tasks.1 1 Introduction A popular approach to representing word meaning in NLP is to characterize a word by “the company that it keeps” (Firth, 1957). This intuition is the basis of famous word embedding techniques such as Word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclu"
2020.coling-main.106,N19-1353,0,0.0198913,"five of the window-based facets (Figure 2, §3.2) used in the downstream model (§4). We include the ⊥ symbol on the learned transformation, Pf , to indicate its orthogonality. Levy et al., 2015; Newell et al., 2019), the gains to be found in diversifying at the level of algorithmic variation are likely to be minimal. With regard to vocabulary coverage, the out-of-vocabulary problem is at least partially addressed by character n-gram based embedding algorithms such as FastText (Joulin et al., 2017) and subword-based decomposition techniques that can be applied post-training (Zhao et al., 2018; Sasaki et al., 2019). Nonetheless, meta-embeddings have been shown to consistently outperform models that use only a single set of embeddings in their input layer. Our goal is to determine how to best combine many sets of input embeddings in order to obtain high quality results in downstream tasks. This work proposes word prisms as a simple and general way to produce and understand metaembeddings, visualized in Figure 1. Word prisms excel at combining many sets of source embeddings, which we call facets. They do so by learning task-specific orthogonal transformations to map embeddings from their facets to the com"
2020.coling-main.106,D13-1170,0,0.00847427,"t to 256 by Kiela et al. (2018)); Pf and bf are learned via backpropagation from supervised learning during the current task; the αf are obtained via a self-attention 0 mechanism on an additional learned parameter vector a ∈ Rd : αf = φ(a · (Pf wf + bf ) + b), where φ is the softmax function and b ∈ R is an additional learned bias parameter. 4.2 Datasets and downstream models We evaluate meta-embedding methods on a variety of downstream text classification and sequence labelling tasks. For text classification, we choose the Stanford Sentiment Treebank binary sentiment analysis dataset (SST2) (Socher et al., 2013) and the Stanford NLI (Bowman et al., 2015) (SNLI) benchmark. For sequence labelling, we select the CoNLL 2003 named entity recognition task (NER) (Tjong Kim Sang and De Meulder, 2003), POS tagging on the Brown corpus (Brown)4 , POS tagging on the WSJ corpus (WSJ) (Marcus et al., 1993), and Supersense tagging (Ciaramita and Johnson, 2003) on the Semcor 3.0 corpus (Semcor) (Miller et al., 1993). Supersense tagging is a problem situated between NER and word sense disambiguation. The task consists of 41 lexicographer class labels for nouns and verbs with IOB tags, producing 83 fine-grained classe"
2020.coling-main.106,D12-1086,0,0.0342545,"(Firth, 1957). This intuition is the basis of famous word embedding techniques such as Word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014). However, the question of what company a word keeps — i.e., what should define a word’s context — is open. A word’s context could be defined via a symmetric window of 1, 2, 5, 10, 20 words, the words that precede it, the words that follow it, the words with which it shares a dependency edge, etc. Determining the utility of such different notions of context for training word embeddings is a problem that has attracted considerable attention (Yatbaz et al., 2012; Levy and Goldberg, 2014a; Bansal et al., 2014; Lin et al., 2015; Melamud et al., 2016; Lison and Kutuzov, 2017) but there is no conclusive evidence that any single notion of context could be the best for solving NLP problems in general. Thus, many deep learning solutions for NLP have yet another hyperparameter to tune: what set of word embeddings should be selected for the input layer of the model. As NLP tasks become more and more complex, the practice of providing a deep model with only one notion of a word’s meaning becomes limiting. Word meta-embeddings address aspects of this problem by"
2020.coling-main.106,P16-1128,0,0.163387,"Missing"
2020.coling-main.106,N16-1178,0,0.0197336,"on, as well as via SVD and CCA, leading to improved performance in multiple downstream tasks. However, they only combine two embedding sets at a time. Yin and Sch¨utze (2016) introduce the term “meta-embeddings” and demonstrate that concatenation and singular value decomposition (SVD) are solid baselines on word similarity, analogy, and POS tagging tasks. They propose 1 TO N, which simultaneously learns meta-embeddings and projections from the meta-embedding space to each individual source embedding space. Ghannay et al. (2016) apply PCA and autoencoders after concatenating source embeddings. Zhang et al. (2016) apply a convolutional layer to each source embedding before concatenating the resulting feature maps. Bollegala et al. (2018) represent the meta-embedding for a word as a linear combination of the meta-embeddings for its nearest neighbours in each source embedding set. Bao and Bollegala (2018) produce meta-embeddings by either averaging or concatenating the outputs of encoders which take GloVe and CBOW (Mikolov et al., 2013a) embeddings as input. Coates and Bollegala (2018) demonstrate that, in certain settings, meta-embeddings produced by averaging can be as performant as concatenated ones."
2020.coling-main.106,D18-1059,0,0.0215222,"the word “apple” in five of the window-based facets (Figure 2, §3.2) used in the downstream model (§4). We include the ⊥ symbol on the learned transformation, Pf , to indicate its orthogonality. Levy et al., 2015; Newell et al., 2019), the gains to be found in diversifying at the level of algorithmic variation are likely to be minimal. With regard to vocabulary coverage, the out-of-vocabulary problem is at least partially addressed by character n-gram based embedding algorithms such as FastText (Joulin et al., 2017) and subword-based decomposition techniques that can be applied post-training (Zhao et al., 2018; Sasaki et al., 2019). Nonetheless, meta-embeddings have been shown to consistently outperform models that use only a single set of embeddings in their input layer. Our goal is to determine how to best combine many sets of input embeddings in order to obtain high quality results in downstream tasks. This work proposes word prisms as a simple and general way to produce and understand metaembeddings, visualized in Figure 1. Word prisms excel at combining many sets of source embeddings, which we call facets. They do so by learning task-specific orthogonal transformations to map embeddings from t"
2020.coling-main.515,2020.acl-main.679,0,0.299799,"ently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various works have lately re-examined the challenges of the WSC, leading to the proposal of more difficult, larger variants, data and model debiasing methods, and evaluation protocols that clarify which types of instances models excel on and which they struggle with (Trichelair et al., 2019; Emami et al., 2019; Sakaguchi et al., 2020; Abdou et al., 2020). However, little attention has been paid to studying the effects and influence of pretraining data points. While recent work has included some analysis on the effects of 13-gram overlaps between pretraining and test instances for the WSC (Brown et al., 2020), a deeper look into how the degree of overlap (and how this can be defined) affects language models’ performance is critical to revealing models’ reasoning This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 5855 Proceedings of the 28th Interna"
2020.coling-main.515,N19-1423,0,0.163507,"e he was so upset. (Answer: Jim) b. Jim comforted Kevin because he was so upset. (Answer: Kevin) For a number of years, models struggled to exceed chance-level performance (Kruengkrai et al., 2014; Sharma et al., 2015; Peng et al., 2015; Liu et al., 2016). The WSC task is carefully controlled, such that heuristics involving syntactic and semantic cues were ineffective, and the common-sense knowledge required to correctly resolve its test instances make it particularly difficult for statistical systems to model. More recently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various works have lately re-examined the challenges of the WSC, leading to the proposal of more difficult, larger variants, data and model debiasing methods, and evaluation protocols that clarify which types of instances models excel on and which they struggle with (Trichelair et al., 2019; Emami et al., 2019; Sakaguchi et al., 2020; Abdou et al., 2020). However, little attention has been paid to studying the effects and influe"
2020.coling-main.515,D18-1220,1,0.871611,"fying train-test overlaps consists of three main steps: (1) parsing a test instance into its core components, (2) formulating a query using a schema derived from the parse, and (3) quantifying the degree of overlap between a train-test pair using an overlap scoring mechanism. 3.1 Skeletal Representation We first perform a partial parse of each test instance into a general skeleton of each of the important semantic components, in the order that they appear. We use rules related to the syntactic parse of the sentence implemented by Stanford CoreNLP (Manning et al., 2014). We use the notation in Emami et al. (2018) to separate the components of WSC-like instances; that is, instances can be divided into a context clause, which introduces the two competing antecedents, and a query clause, which contains the target pronoun to be resolved: E1 , E2 the candidate antecedents P redC the context predicate + discourse connective P the target pronoun P redQ the query predicate E1 and E2 are noun phrases in the context clause. In the WSC, these two are specified without ambiguity. P redC is the context predicate composed of the verb phrase that relates both antecedents to some event. The context contains E1 , E2 ,"
2020.coling-main.515,P19-1386,1,0.937131,"t for statistical systems to model. More recently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various works have lately re-examined the challenges of the WSC, leading to the proposal of more difficult, larger variants, data and model debiasing methods, and evaluation protocols that clarify which types of instances models excel on and which they struggle with (Trichelair et al., 2019; Emami et al., 2019; Sakaguchi et al., 2020; Abdou et al., 2020). However, little attention has been paid to studying the effects and influence of pretraining data points. While recent work has included some analysis on the effects of 13-gram overlaps between pretraining and test instances for the WSC (Brown et al., 2020), a deeper look into how the degree of overlap (and how this can be defined) affects language models’ performance is critical to revealing models’ reasoning This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence"
2020.coling-main.515,N18-2017,0,0.0394827,"ted instances (e.g. The trophy does not fit in the suitcase because it is too large); that is, instances are often composed only of two clauses connected by a single causal discourse connective, like because. For example, during the crowdsourcing protocol for Winogrande, annotators are first primed by classical examples of WSC sentences that may influence their creative process. Test instances that are structurally similar to the original WSC instances seem more likely to overlap with training instances, and it is known that this kind of crowd-sourcing protocol engenders annotation artifacts (Gururangan et al., 2018) that are particularly problematic when they do not corresponding to real-word data (He et al., 2019). One can find more elaborate, real-word coreference examples that circumvent the above issues. For example, consider this sentence, taken as is from Reddit: (4) “Forbes wrote that Edison can’t be held accountable because his assistant willingly submitted to the trials and that the dangers of radiation poisoning were not well known.” Here, despite the instance being a valid pronoun disambiguation problem akin to the WSC instances, there are multiple discourse connectives and more than two claus"
2020.coling-main.515,D19-6115,0,0.0190722,"often composed only of two clauses connected by a single causal discourse connective, like because. For example, during the crowdsourcing protocol for Winogrande, annotators are first primed by classical examples of WSC sentences that may influence their creative process. Test instances that are structurally similar to the original WSC instances seem more likely to overlap with training instances, and it is known that this kind of crowd-sourcing protocol engenders annotation artifacts (Gururangan et al., 2018) that are particularly problematic when they do not corresponding to real-word data (He et al., 2019). One can find more elaborate, real-word coreference examples that circumvent the above issues. For example, consider this sentence, taken as is from Reddit: (4) “Forbes wrote that Edison can’t be held accountable because his assistant willingly submitted to the trials and that the dangers of radiation poisoning were not well known.” Here, despite the instance being a valid pronoun disambiguation problem akin to the WSC instances, there are multiple discourse connectives and more than two clauses, plus distractor content words that contribute variably to the correct resolution. All this render"
2020.coling-main.515,P19-1478,0,0.198962,"ruggled to exceed chance-level performance (Kruengkrai et al., 2014; Sharma et al., 2015; Peng et al., 2015; Liu et al., 2016). The WSC task is carefully controlled, such that heuristics involving syntactic and semantic cues were ineffective, and the common-sense knowledge required to correctly resolve its test instances make it particularly difficult for statistical systems to model. More recently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various works have lately re-examined the challenges of the WSC, leading to the proposal of more difficult, larger variants, data and model debiasing methods, and evaluation protocols that clarify which types of instances models excel on and which they struggle with (Trichelair et al., 2019; Emami et al., 2019; Sakaguchi et al., 2020; Abdou et al., 2020). However, little attention has been paid to studying the effects and influence of pretraining data points. While recent work has included some analysis on the effects of 13-gram overlaps between pretraini"
2020.coling-main.515,Y14-1042,0,0.0184618,"NOWREF -60 K is the largest corpus to date for WSC-style common-sense reasoning and exhibits a significantly lower proportion of overlaps with current pretraining corpora. 1 Introduction The original purpose of the Winograd Schema Challenge was to serve as an alternative Turing test to evaluate an automatic system’s capacity for common-sense inference (Levesque et al., 2011). As an example: (1) a. Jim yelled at Kevin because he was so upset. (Answer: Jim) b. Jim comforted Kevin because he was so upset. (Answer: Kevin) For a number of years, models struggled to exceed chance-level performance (Kruengkrai et al., 2014; Sharma et al., 2015; Peng et al., 2015; Liu et al., 2016). The WSC task is carefully controlled, such that heuristics involving syntactic and semantic cues were ineffective, and the common-sense knowledge required to correctly resolve its test instances make it particularly difficult for statistical systems to model. More recently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various"
2020.coling-main.515,P14-5010,0,0.00257442,"Hunting for Overlaps Our procedure for identifying train-test overlaps consists of three main steps: (1) parsing a test instance into its core components, (2) formulating a query using a schema derived from the parse, and (3) quantifying the degree of overlap between a train-test pair using an overlap scoring mechanism. 3.1 Skeletal Representation We first perform a partial parse of each test instance into a general skeleton of each of the important semantic components, in the order that they appear. We use rules related to the syntactic parse of the sentence implemented by Stanford CoreNLP (Manning et al., 2014). We use the notation in Emami et al. (2018) to separate the components of WSC-like instances; that is, instances can be divided into a context clause, which introduces the two competing antecedents, and a query clause, which contains the target pronoun to be resolved: E1 , E2 the candidate antecedents P redC the context predicate + discourse connective P the target pronoun P redQ the query predicate E1 and E2 are noun phrases in the context clause. In the WSC, these two are specified without ambiguity. P redC is the context predicate composed of the verb phrase that relates both antecedents t"
2020.coling-main.515,N15-1082,0,0.0243262,"WSC-style common-sense reasoning and exhibits a significantly lower proportion of overlaps with current pretraining corpora. 1 Introduction The original purpose of the Winograd Schema Challenge was to serve as an alternative Turing test to evaluate an automatic system’s capacity for common-sense inference (Levesque et al., 2011). As an example: (1) a. Jim yelled at Kevin because he was so upset. (Answer: Jim) b. Jim comforted Kevin because he was so upset. (Answer: Kevin) For a number of years, models struggled to exceed chance-level performance (Kruengkrai et al., 2014; Sharma et al., 2015; Peng et al., 2015; Liu et al., 2016). The WSC task is carefully controlled, such that heuristics involving syntactic and semantic cues were ineffective, and the common-sense knowledge required to correctly resolve its test instances make it particularly difficult for statistical systems to model. More recently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various works have lately re-examined the challe"
2020.coling-main.515,D12-1071,0,0.0746259,"Missing"
2020.coling-main.515,D19-1335,1,0.808252,"it particularly difficult for statistical systems to model. More recently, however, the advent of deep bidirectional transformers (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)) pretrained on massive amounts of data has led to near-human-level performance (Kocijan et al., 2019; Ye et al., 2019; Ruan et al., 2019). Various works have lately re-examined the challenges of the WSC, leading to the proposal of more difficult, larger variants, data and model debiasing methods, and evaluation protocols that clarify which types of instances models excel on and which they struggle with (Trichelair et al., 2019; Emami et al., 2019; Sakaguchi et al., 2020; Abdou et al., 2020). However, little attention has been paid to studying the effects and influence of pretraining data points. While recent work has included some analysis on the effects of 13-gram overlaps between pretraining and test instances for the WSC (Brown et al., 2020), a deeper look into how the degree of overlap (and how this can be defined) affects language models’ performance is critical to revealing models’ reasoning This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licens"
2020.coling-main.515,W18-5446,0,0.0306001,"he-art in Kocijan et al. (2019), and we include that corpus as an additional source for querying overlaps. RoBERTa RoBERTa (Liu et al., 2019) is an improved variant of BERT that adds more training data with larger batch sizes and longer training, as well as other refinements like dynamic masking. RoBERTa performs consistently better than BERT across many benchmarks. The pretraining corpora include those used for BERT and three more: CC-News (Nagel, 2016), Openwebtext (Gokaslan and Cohen, 2019), and the Stories Corpus (Trinh and Le, 2018). We fine-tune RoBERTa models on the WNLI-train dataset (Wang et al., 2018) for comparability with the state-of-the-art model in Liu et al. (2019), including the corpus as an additional source for querying potential overlaps. 4.3 Results In the following section, we report the performance of state-of-the-art models on subsets of the CSR test sets for which at least one overlapping instance was retrieved from the pretraining corpora – that is, where the BM25 score between a train-test sentence pair is &gt; 0. In addition, we investigate how performance changes as we increase the BM25 score cut-off, and use this to assess the relationship between each test set as a whole"
2020.coling-main.515,D18-1009,0,0.022203,"te-of-the-art transformer-based models (BERT and RoBERTa). Any such investigation must include formulating a more precise definition of contamination. Methods for purging easy instances from CSR benchmarks have been developed recently: for example, the algorithmic bias reduction of test sets proposed by (Sakaguchi et al., 2020) removes instances from the test set with exploitable annotation artifacts. These techniques depend on pre-computed neural network embeddings of a particular model, and so may be difficult for that model alone but not for previous or up-and-coming models. As the work of Zellers et al. (2018) and the follow-up by Zellers et al. (2019) have shown, adversarial filtering must be iteratively re-adapted to newer models that may be 1 The corpus, the code to scrape the sentences from the source texts, as well as the code to reproduce all experimental results will be publicly available at https://github.com/aemami1/KnowRef60k. 5856 immune to previous filtering. This may be costly. Adversarial filtering and related debiasing techniques also do not provide much insight on why certain test instances are filtered out. Our proposed method for data purging is interpretable and model-independent"
2020.coling-main.515,P19-1472,0,0.013411,"T and RoBERTa). Any such investigation must include formulating a more precise definition of contamination. Methods for purging easy instances from CSR benchmarks have been developed recently: for example, the algorithmic bias reduction of test sets proposed by (Sakaguchi et al., 2020) removes instances from the test set with exploitable annotation artifacts. These techniques depend on pre-computed neural network embeddings of a particular model, and so may be difficult for that model alone but not for previous or up-and-coming models. As the work of Zellers et al. (2018) and the follow-up by Zellers et al. (2019) have shown, adversarial filtering must be iteratively re-adapted to newer models that may be 1 The corpus, the code to scrape the sentences from the source texts, as well as the code to reproduce all experimental results will be publicly available at https://github.com/aemami1/KnowRef60k. 5856 immune to previous filtering. This may be costly. Adversarial filtering and related debiasing techniques also do not provide much insight on why certain test instances are filtered out. Our proposed method for data purging is interpretable and model-independent, and can be further supplemented with exis"
2020.emnlp-main.506,P18-1064,0,0.0238113,"18), and evaluation models for factual consistency in abstractive summarization (Goodrich et al., 2019; Falke et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020). Cao et al. (2018) proposed a dual attention module in an abstractive summarizer that attends to both the source document and to relation triples extracted from the document. Zhang et al. (2019b) propose to improve their abstractive summarization model by optimizing fact scores defined in radiology reports with reinforcement learning methods. Li et al. (2018) jointly train their model’s encoder on summarization and NLI tasks. Guo et al. (2018) train an abstractive summarization system with the auxiliary tasks of question and entailment generation and show that their generated summaries are less likely to produce extraneous facts. Kumar and Cheung (2019) show that neural abstractive summarizers often assign higher posterior likelihood to perturbed contrastive summaries that are inconsistent with the source text than to human-written gold-standard ones. Concurrently to our work, Zhu et al. (2020) recently proposed a fact-aware summarization model that uses a knowledge graph. They use a pre-trained corrector module to modify generated"
2020.emnlp-main.506,2020.acl-main.450,0,0.0553626,"Missing"
2020.emnlp-main.506,N19-1396,1,0.737543,"ention module in an abstractive summarizer that attends to both the source document and to relation triples extracted from the document. Zhang et al. (2019b) propose to improve their abstractive summarization model by optimizing fact scores defined in radiology reports with reinforcement learning methods. Li et al. (2018) jointly train their model’s encoder on summarization and NLI tasks. Guo et al. (2018) train an abstractive summarization system with the auxiliary tasks of question and entailment generation and show that their generated summaries are less likely to produce extraneous facts. Kumar and Cheung (2019) show that neural abstractive summarizers often assign higher posterior likelihood to perturbed contrastive summaries that are inconsistent with the source text than to human-written gold-standard ones. Concurrently to our work, Zhu et al. (2020) recently proposed a fact-aware summarization model that uses a knowledge graph. They use a pre-trained corrector module to modify generated summaries. Concurrent to our work, Dong et al. (2020) proposes factual correction models that leverages knowledge learned from question answering models via span selection. Their models employ single or multimaski"
2020.emnlp-main.506,2020.acl-main.458,0,0.155016,"Missing"
2020.emnlp-main.506,2020.acl-main.703,0,0.170329,"Missing"
2020.emnlp-main.506,C18-1121,0,0.067387,"e output of an abstractive summarizer, we find that our corrector is able to accurately correct errors in the generated summaries. However, the overall recall on correcting factual errors in real system summaries remains low, suggesting the errors introduced by heuristics have a different distribution than errors made by abstractive summarization systems. 2 Background and Related Work Previous work on factual consistency in abstractive summarization can be divided into two categories: abstractive summarization models tailored towards factual consistency (Cao et al., 2018; Zhang et al., 2019b; Li et al., 2018), and evaluation models for factual consistency in abstractive summarization (Goodrich et al., 2019; Falke et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020). Cao et al. (2018) proposed a dual attention module in an abstractive summarizer that attends to both the source document and to relation triples extracted from the document. Zhang et al. (2019b) propose to improve their abstractive summarization model by optimizing fact scores defined in radiology reports with reinforcement learning methods. Li et al. (2018) jointly train their model’s encoder on summarization and NLI tasks. Guo"
2020.emnlp-main.506,W04-1013,0,0.0983268,"Missing"
2020.emnlp-main.506,D19-1387,0,0.0347889,"t factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging1 . 1 Table 1: An example of an inconsistent systemgenerated summary and the output summary from our correction model. In this case, “France” is successfully corrected as “Israel”. Introduction Self-supervised methods have achieved success in a wide range of NLP tasks, and automatic summarization is no exception (Liu and Lapata, 2019; Lewis et al., 2019; Zhang et al., 2019a; Shi et al., 2019; Fabbri et al., 2019). These state-ofthe-art abstractive summarization models typically finetune pre-trained transformer-based models on a summarization dataset (Vaswani et al., 2017). Despite significant improvements over previous methods in terms of automatic evaluation scores such as ROUGE (Lin, 2004), ensuring factual consistency of the generated summary with respect to the source remains challenging. For example, Cao 1 Our data and code is available at https://github. com/mcao610/Factual-Error-Correction et al. (2018) claims that"
2020.emnlp-main.646,N19-1423,0,0.0167135,"k Times (NYT) Annotated Corpus (Sandhaus, 2008) to extract tuples of named entities and their document context. The NYT corpus contains high-quality metadata listing the salient named entities mentioned in each article. We form our tuples from entities tagged in the metadata for the same article. We refer to a tuple of entities and its associated information as an aggregatable instance. We Related work Abstractive summarizers have gained prominence with the popularization of RNNs (Sutskever et al., 2014; Nallapati et al., 2016), and more recently Transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019). Several abstractive models have achieved state-of-the-art performances on benchmark summarization datasets in terms of ROUGE, 3 8032 The TESA dataset Data collected aggregatable instances 2100 annotators 63 annotations 6299 Preprocessed dataset aggregatable instances 1718 42 annotators annotations 4675 P ERSON entities tuples 941 (801) L OCATION entities tuples 629 (412) O RGANIZATION entities tuples 148 (123) P ERSON aggregations 2900 (951) L OCATION aggregations 2041 (505) O RGANIZATION aggregations 456 (239) first describe the components of an aggregatable instance in more detail. Then, w"
2020.emnlp-main.646,C10-1039,0,0.245029,"ess only a proxy for abstraction in the broader sense which concerns semantic generalization. We argue that it is important to also focus explicitly on semantic abstraction, as this capability is required for more difficult types of summarization which are out of reach of current methods. For example, generating a plot summary of a novel might require describing sequences of events using one sentence. Writing a survey of a scientific field would require categorizing papers and ideas, and being able to refer to them as a whole. Outside of domain-specific settings such as opinion summarization (Ganesan et al., 2010; Gerani et al., 2014, inter alia), and tasks such as sentence fusion (Barzilay and McKeown, 2005), there has been little work focusing on semantic generalization and abstraction. 8031 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8031–8050, c November 16–20, 2020. 2020 Association for Computational Linguistics In this paper, we start to tackle this issue by focusing on the specific task of semantic aggregation of entities; i.e., how to refer to a tuple of named entities using a noun phrase instead of enumerating them (See Table 1 for an example)"
2020.emnlp-main.646,D18-1206,0,0.0645614,"Missing"
2020.emnlp-main.646,N19-4009,0,0.0127795,"ntage of this approach over the previous one is that it leverages the set-up of TESA as a ranking task, and the model is exposed to both correct and incorrect aggregations during training (which, on the other hand, makes it more computationally expensive). By contrast, generative BART only sees correct ones. We thus expect the discriminative model to produce higher performance. However, this comes at a cost, as this approach cannot generate freely an aggregation, but only retrieve one from a set of candidates. For all three versions above, we built upon code that is available through fairseq (Ott et al., 2019). We use the version of BART pre-trained on the CNN/DailyMail dataset. The choice of hyperparameters is described in Appendix D. 6 Results The results of the models on TESA’s test set are presented in Table 4. We see that most models outMethod Random baseline Frequency baseline Logistic regression Pre-trained BART Generative BART Discriminative BART MAP 0.222 0.570 0.700 0.389 0.701 0.895 R@10 0.442 0.655 0.863 0.682 0.903 0.991 MRR 0.289 0.761 0.840 0.505 0.840 0.954 Table 4: Results of the different models on the TESA test set. Method Generative BART (0.701) Discriminative BART (0.895) conte"
2020.emnlp-main.646,P16-1216,0,0.0509863,"Missing"
2020.emnlp-main.646,W18-5446,0,0.0803362,"Missing"
2020.emnlp-main.646,2020.findings-emnlp.217,0,0.0296911,"We present two ways of fine-tuning BART to TESA, either in a discriminative or in a generative fashion, and compare them against simpler statistical and frequency-based methods. The simple classifier achieves decent results on TESA. It is however outperformed by a wide margin by BART, when fine-tuned on our task in a discriminative manner. When fine-tuned as a generative model, BART yields similar performance as the simple classifier. Yet, the generative model is able to freely generate entity aggregations with diversity and quality, despite some factual inconsistencies. including ProphetNet (Yan et al., 2020), PEGASUS (Zhang et al., 2019) and BART (Lewis et al., 2019). Recent work has also focused on specific issues such as preventing inappropriate repetition (Kry´sci´nski et al., 2018), word-level rewriting, and evaluating factual consistency (Kry´sci´nski et al., 2019; Maynez et al., 2020). Abstraction is critical for certain domains and applications, but has not been thoroughly explored in many. For example, in scientific article summarization the particular structure and length of scientific articles make extractive techniques much easier to apply (Agarwal et al., 2011), therefore abstractive"
2020.emnlp-main.681,Q16-1028,0,0.23322,"he form of the loss terms fij , the kernel function ψij , and the association function φij . The derivative of fij with respect to ψij , which we call the characteristic gradient, helps compare models because it exhibits the action of the gradient yet is symmetric in the parameters. In the Appendix we show how this derivative relates to gradient descent. In the following subsections, we present the ∂f derivations of ∂ψijij , ψij , and φij for SVD (Levy and Goldberg, 2014; Levy et al., 2015), SGNS (Mikolov et al., 2013), FastText (Joulin et al., 2017), GloVe (Pennington et al., 2014), and LDS (Arora et al., 2016). The derivation for Swivel (Shazeer 8480 ∂fij ∂ψij Model φij hi|ji ≈ hi|ji PMI(i, j) PMI(i, j) hi|ji ln Nij− N PMI(i, j) − ln k hi|ji + bi + bj ln Nij PMI(i, j) khi |+ |ji |k2 ln Nij dPMI(i, j) − dγ PMI(i, j) PMI(i, j) PMI∗ (i, j) PMI∗ (i, j)   2 · ψij − φij SVD SGNS ψij   (Nij + Nij− ) · σ(ψij ) − σ(φij ) ij   2h(Nij ) · ψij − φij h i 4h(Nij ) · ψij − φij + C h i p Nij · ψij − φij   1 · σ ψij − φij GloVe LDS Swivel hi|ji ∂f Table 1: Comparison of low rank embedders. Final column shows the value of hi|ji at ∂ψijij = 0. GloVe and LDS set fij = 0 when Nij = 0; h(Nij ) is a weighting fun"
2020.emnlp-main.681,E17-2068,0,0.0710916,"n choices to be made: we must chose the embedding dimension d, the form of the loss terms fij , the kernel function ψij , and the association function φij . The derivative of fij with respect to ψij , which we call the characteristic gradient, helps compare models because it exhibits the action of the gradient yet is symmetric in the parameters. In the Appendix we show how this derivative relates to gradient descent. In the following subsections, we present the ∂f derivations of ∂ψijij , ψij , and φij for SVD (Levy and Goldberg, 2014; Levy et al., 2015), SGNS (Mikolov et al., 2013), FastText (Joulin et al., 2017), GloVe (Pennington et al., 2014), and LDS (Arora et al., 2016). The derivation for Swivel (Shazeer 8480 ∂fij ∂ψij Model φij hi|ji ≈ hi|ji PMI(i, j) PMI(i, j) hi|ji ln Nij− N PMI(i, j) − ln k hi|ji + bi + bj ln Nij PMI(i, j) khi |+ |ji |k2 ln Nij dPMI(i, j) − dγ PMI(i, j) PMI(i, j) PMI∗ (i, j) PMI∗ (i, j)   2 · ψij − φij SVD SGNS ψij   (Nij + Nij− ) · σ(ψij ) − σ(φij ) ij   2h(Nij ) · ψij − φij h i 4h(Nij ) · ψij − φij + C h i p Nij · ψij − φij   1 · σ ψij − φij GloVe LDS Swivel hi|ji ∂f Table 1: Comparison of low rank embedders. Final column shows the value of hi|ji at ∂ψijij = 0. Glo"
2020.emnlp-main.681,Q15-1016,0,0.365454,"a particular low rank embedder instance requires key design choices to be made: we must chose the embedding dimension d, the form of the loss terms fij , the kernel function ψij , and the association function φij . The derivative of fij with respect to ψij , which we call the characteristic gradient, helps compare models because it exhibits the action of the gradient yet is symmetric in the parameters. In the Appendix we show how this derivative relates to gradient descent. In the following subsections, we present the ∂f derivations of ∂ψijij , ψij , and φij for SVD (Levy and Goldberg, 2014; Levy et al., 2015), SGNS (Mikolov et al., 2013), FastText (Joulin et al., 2017), GloVe (Pennington et al., 2014), and LDS (Arora et al., 2016). The derivation for Swivel (Shazeer 8480 ∂fij ∂ψij Model φij hi|ji ≈ hi|ji PMI(i, j) PMI(i, j) hi|ji ln Nij− N PMI(i, j) − ln k hi|ji + bi + bj ln Nij PMI(i, j) khi |+ |ji |k2 ln Nij dPMI(i, j) − dγ PMI(i, j) PMI(i, j) PMI∗ (i, j) PMI∗ (i, j)   2 · ψij − φij SVD SGNS ψij   (Nij + Nij− ) · σ(ψij ) − σ(φij ) ij   2h(Nij ) · ψij − φij h i 4h(Nij ) · ψij − φij + C h i p Nij · ψij − φij   1 · σ ψij − φij GloVe LDS Swivel hi|ji ∂f Table 1: Comparison of low rank embedd"
2020.emnlp-main.681,P15-2031,0,0.0185013,"tion of SGNS’s solution is inspired by the work of Levy and Goldberg (2014), who proved that skip-gram with negative sampling (SGNS) (Mikolov et al., 2013) was implicitly factorizing the PMI − ln k matrix. However, they required additional assumptions for their derivation to hold. Li et al. (2015) explored relations between SGNS and matrix factorization, but their derivation diverges from Levy and Goldberg’s result and masks the connection between SGNS and other low rank embedders. Other works have also explored theoretical or empirical relationships between SGNS and GloVe (Shi and Liu, 2014; Suzuki and Nagata, 2015; Levy et al., 2015; Arora et al., 2016). 5 Discussion We observe common features between each of the ∂f algorithms (Table 1). In each case, ∂ψijij takes the form (multiplier) · (difference). The multiplier is always a “tempered” version of Nij (or Ni Nj ); that is, it increases sublinearly with Nij . For each algorithm, φij is equal to PMI or a scaled log of Nij . Yet, the choice of ψij in combination with φij provides that every model is optimized when hi|ji tends toward PMI(i, j) (with or without a constant shift or scaling). We demonstrated that the optimum for SGNS (and FastTest) is equiv"
2020.emnlp-main.681,N03-1032,0,0.0481384,"2020 Association for Computational Linguistics ciated to context words. In matrix notation, |ji corresponds to a column vector and hi |to a row vector. Their inner product is hi|ji. We later demonstrate that many word embedding algorithms, intentionally or not, learn a vector space where the inner product between a focal word j and context word i aims to approximate their PMI in the reference corpus: hi|ji ≈ PMI(i, j). Pointwise mutual information (PMI). PMI is a commonly used measure of association in computational linguistics, and has been shown to be consistent and reliable for many tasks (Terra and Clarke, 2003). It measures the deviation of the cooccurrence probability between two words i and j from the product of their marginal probabilities: PMI(i, j) := ln pij N Nij = ln , pi pj Ni Nj (1) where pij is the probability of word i and word j cooccurring (for some notion of cooccurrence), and where pi and pj are marginal probabilities of words i and j occurring. The empirical PMI can be found by replacing probabilities with corpus statistics. Words are typically considered to cooccur if they are separated by no more than w words; Nij is the number of counted cooccurrences between a context i and a ter"
2020.emnlp-main.749,N16-1012,0,0.0276652,"neralizable to any summarization system. (ii) We propose two methods to solve multi-fact correction problem with single or multi-span selection in an iterative or auto-regressive manner, respectively. (iii) Experimental results on multiple summarization benchmarks demonstrate that our approach can significantly improve multiple factuality measurements without a huge drop on ROUGE scores. 2 Related Work The general neural-based encoder-decoder structure for abstractive summarization is first proposed by Rush et al. (2015). Later work improves this structure with better encoders, such as LSTMs (Chopra et al., 2016) and GRUs (Nallapati et al., 2016), that are able to capture longrange dependencies, as well as with reinforcement learning methods that directly optimize summarization evaluation scores (Paulus et al., 2018). One drawback of the earlier neural-based summarization models is the inability to produce out-of9321 Figure 1: Training example created for the QA-span prediction model (upper right) and the auto-regressive fact correction model (bottom right). vocabulary words, as the model can only generate whole words based on a fixed vocabulary. See et al. (2017) proposes a pointer-generator framewor"
2020.emnlp-main.749,N19-1423,0,0.18317,"uncil is supporting the school to ensure its policies are appropriate... a muslim school has been accused of breaching the equality act by refusing to wear headscarves. a catholic school has been accused of breaching the equality act by refusing to wear headscarves. Table 1: Examples of factual error correction on different summarization datasets. Factual errors are marked in red. Corrections made by the proposed SpanFact models are marked in orange. Recently, with the advent of Transformer-based models (Vaswani et al., 2017) pre-trained using self-supervised objectives on large text corpora (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020), abstractive summarization models are surpassing extractive ones on automatic evaluation metrics such as ROUGE (Lin, 2004). However, several studies (Falke 9320 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9320–9331, c November 16–20, 2020. 2020 Association for Computational Linguistics et al., 2019; Goodrich et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020) observe that despite high ROUGE scores, system-generated abstractive summ"
2020.emnlp-main.749,2020.acl-main.454,0,0.46627,"dels (Vaswani et al., 2017) pre-trained using self-supervised objectives on large text corpora (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020), abstractive summarization models are surpassing extractive ones on automatic evaluation metrics such as ROUGE (Lin, 2004). However, several studies (Falke 9320 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9320–9331, c November 16–20, 2020. 2020 Association for Computational Linguistics et al., 2019; Goodrich et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020) observe that despite high ROUGE scores, system-generated abstractive summaries are often factually inconsistent with respect to the source text. Factual inconsistency is a well-known problem for conditional text generation, which requires models to generate readable text that is faithful to the input document. Consequently, sequence-to-sequence generation models need to learn to balance signals between the source for faithfulness and the learned language modeling prior for fluency (Kry´sci´nski et al., 2019). The dual objectives render abstractive summarization models hi"
2020.emnlp-main.749,P19-1102,0,0.0583151,"Missing"
2020.emnlp-main.749,P19-1213,0,0.516385,"Missing"
2020.emnlp-main.749,D18-1443,0,0.0359554,"rs these questions using either the source or the summary to obtain two sets of answers. The answers are compared against each other using an answer-similarity metric (token-level F1), and the averaged similarity metric over all questions is used as the QGQA score. Answers generated from a highly faithful system summary should be similar to those generated from the source. 4.3 Baselines We compare against the following abstractive summarization baselines. On CNNDM and XSum, we use BertSumAbs, BertSumExtAbs and TransformerAbs (Liu and Lapata, 2019). In addition, we also compare with Bottom-up (Gehrmann et al., 2018). On Gigaword, we use the pointergenerator (See et al., 2017), base and full GenParse models (Song et al., 2020) for comparison. For the factual correction baseline, we compare with the Two-encoder Pointer Generator6 (Split Encoder) (Shah et al., 2020), which employs a similar setting to ours for masking entities w.r.t. the source, and uses dual encoders to copy and generate from both the source and the masked query for fact update. Compared to our span selection models that can fill in the mask with any number of tokens, their models aim to regenerate the mask query based on the source. In ot"
2020.emnlp-main.749,K16-1028,0,0.0885291,"n system. (ii) We propose two methods to solve multi-fact correction problem with single or multi-span selection in an iterative or auto-regressive manner, respectively. (iii) Experimental results on multiple summarization benchmarks demonstrate that our approach can significantly improve multiple factuality measurements without a huge drop on ROUGE scores. 2 Related Work The general neural-based encoder-decoder structure for abstractive summarization is first proposed by Rush et al. (2015). Later work improves this structure with better encoders, such as LSTMs (Chopra et al., 2016) and GRUs (Nallapati et al., 2016), that are able to capture longrange dependencies, as well as with reinforcement learning methods that directly optimize summarization evaluation scores (Paulus et al., 2018). One drawback of the earlier neural-based summarization models is the inability to produce out-of9321 Figure 1: Training example created for the QA-span prediction model (upper right) and the auto-regressive fact correction model (bottom right). vocabulary words, as the model can only generate whole words based on a fixed vocabulary. See et al. (2017) proposes a pointer-generator framework that can copy words directly fro"
2020.emnlp-main.749,W17-3204,0,0.0708815,"Missing"
2020.emnlp-main.749,D18-1206,0,0.151743,"e the masks. Compared to the conventional Pointer Network (Vinyals et al., 2015; See et al., 2017) that only points to one token at a time, our sequential span selection decoder has the flexibility to replace a mask by any number of entity tokens, which is often required in summary factual correction. 4 Experiment In this section, we present our results on using SpanFact for multiple summarization datasets. 4.1 CNN DailyMail XSum Gigaword (9) Experimental Setup Training data for our fact correction models are generated as described in Section 3.2 on CNN/DailyMail (Hermann et al., 2015), XSum (Narayan et al., 2018) and Gigaword (Graff et al., 2003; Rush et al., 2015). The statistics of these three dataset are provided in Table 2. During training, if an entity does not have a corresponding span in the source, we point the answer span to the [CLS] token. During inference, if the answer span predicted is the [CLS] token, we replace back the original masked entity. for the answer span, and the softmax is used for computing the loss for back-propagation. # docs (train/val/test) doc len. summ. len. # mask 90,266/1,220/1093 196,961/12,148/10,397 204,045/11,332/11,334 3,803,957/189,651/1,951 760.50 653.33 431.0"
2020.emnlp-main.749,2020.acl-main.703,0,0.581252,"s policies are appropriate... a muslim school has been accused of breaching the equality act by refusing to wear headscarves. a catholic school has been accused of breaching the equality act by refusing to wear headscarves. Table 1: Examples of factual error correction on different summarization datasets. Factual errors are marked in red. Corrections made by the proposed SpanFact models are marked in orange. Recently, with the advent of Transformer-based models (Vaswani et al., 2017) pre-trained using self-supervised objectives on large text corpora (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020), abstractive summarization models are surpassing extractive ones on automatic evaluation metrics such as ROUGE (Lin, 2004). However, several studies (Falke 9320 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9320–9331, c November 16–20, 2020. 2020 Association for Computational Linguistics et al., 2019; Goodrich et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020) observe that despite high ROUGE scores, system-generated abstractive summaries are often factually inconsistent wit"
2020.emnlp-main.749,C18-1121,0,0.301284,"odrich et al., 2019), textual entailment predictions (Falke et al., 2019), adversarially pre-trained classifiers (Kry´sci´nski et al., 2019), to question answering (QA) systems (Wang et al., 2020; Durmus et al., 2020). It is worth noting that QA-based evaluation metrics show surprisingly high correlations with human judgment on factuality (Wang et al., 2020), indicating that QA models are robust in capturing facts that can benefit summarization tasks. On the other hand, some work focuses on model design to incorporate factual triples (Cao et al., 2018; Zhu et al., 2020) or textual entailment (Li et al., 2018; Falke et al., 2019) to boost factual consistency in generated summaries. Such models are efficient in boosting factual scores, but often at the expense of significantly lowering ROUGE scores of the generated summaries. This happens because the models struggle between generating pivotal content while retaining true facts, often with an eventual propensity to sacrificing informativeness for the sake of correctness of the summary. In addition, these models inherit the backbone of generative models that suffer from hallucination despite the regularization from complex knowledge graphs or text en"
2020.emnlp-main.749,W04-1013,0,0.076883,"accused of breaching the equality act by refusing to wear headscarves. Table 1: Examples of factual error correction on different summarization datasets. Factual errors are marked in red. Corrections made by the proposed SpanFact models are marked in orange. Recently, with the advent of Transformer-based models (Vaswani et al., 2017) pre-trained using self-supervised objectives on large text corpora (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020), abstractive summarization models are surpassing extractive ones on automatic evaluation metrics such as ROUGE (Lin, 2004). However, several studies (Falke 9320 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9320–9331, c November 16–20, 2020. 2020 Association for Computational Linguistics et al., 2019; Goodrich et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020) observe that despite high ROUGE scores, system-generated abstractive summaries are often factually inconsistent with respect to the source text. Factual inconsistency is a well-known problem for conditional text generation, which requires models to generate re"
2020.emnlp-main.749,D19-1387,0,0.0106183,"t of questions based on the system-generated summary, and then answers these questions using either the source or the summary to obtain two sets of answers. The answers are compared against each other using an answer-similarity metric (token-level F1), and the averaged similarity metric over all questions is used as the QGQA score. Answers generated from a highly faithful system summary should be similar to those generated from the source. 4.3 Baselines We compare against the following abstractive summarization baselines. On CNNDM and XSum, we use BertSumAbs, BertSumExtAbs and TransformerAbs (Liu and Lapata, 2019). In addition, we also compare with Bottom-up (Gehrmann et al., 2018). On Gigaword, we use the pointergenerator (See et al., 2017), base and full GenParse models (Song et al., 2020) for comparison. For the factual correction baseline, we compare with the Two-encoder Pointer Generator6 (Split Encoder) (Shah et al., 2020), which employs a similar setting to ours for masking entities w.r.t. the source, and uses dual encoders to copy and generate from both the source and the masked query for fact update. Compared to our span selection models that can fill in the mask with any number of tokens, the"
2020.emnlp-main.749,2020.acl-main.173,0,0.490527,"2017) pre-trained using self-supervised objectives on large text corpora (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020), abstractive summarization models are surpassing extractive ones on automatic evaluation metrics such as ROUGE (Lin, 2004). However, several studies (Falke 9320 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9320–9331, c November 16–20, 2020. 2020 Association for Computational Linguistics et al., 2019; Goodrich et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020) observe that despite high ROUGE scores, system-generated abstractive summaries are often factually inconsistent with respect to the source text. Factual inconsistency is a well-known problem for conditional text generation, which requires models to generate readable text that is faithful to the input document. Consequently, sequence-to-sequence generation models need to learn to balance signals between the source for faithfulness and the learned language modeling prior for fluency (Kry´sci´nski et al., 2019). The dual objectives render abstractive summarization models highly prone to hallucin"
2020.emnlp-main.749,2020.emnlp-main.506,1,0.826308,"Missing"
2020.emnlp-main.749,P18-2124,0,0.0261335,"or, which inherits the backbone of generative models that suffer from hallucination. 4.4 Experimental Results 5 We were not able to obtain any of the QA evaluation model or code from Wang et al. (2020); Durmus et al. (2020); Maynez et al. (2020) as the authors are still in the stage of making the code public. We used pre-trained UniLM model for question generation (QG) and BertForQuestionAnswering model for question answering (QA). The QG model is fine-tuned on NewsQA (Trischler et al., 2017) with entityanswer conditional task (Wang et al., 2020), and the QA model is pre-trained on SQuAD 2.0 (Rajpurkar et al., 2018). Tables 3, 4, and 5 summarize the results on the CNN/DailyMail, XSum and Gigaword datasets, respectively. Each block in the tables compares the original summarization model’s output with 6 https://github.com/darsh10/split_ encoder_pointer_summarizer 9326 Datasets QGQA FactCC sent 1 ROUGE 2 L GenParse (base) Split Encoders QA-Span Auto-regressive 52.63 63.60 66.47 64.77 46.07 48.22 52.17 48.95 35.23 34.32 34.38 33.97 17.11 17.01 16.50 16.08 32.88 31.98 32.07 31.70 GenParse (full) Split Encoders QA-Span Auto-regressive 55.47 65.88 67.12 66.48 48.44 52.11 54.59 52.18 36.61 35.01 35.66 35.04 18.8"
2020.emnlp-main.749,D15-1044,0,0.422515,"l correction framework that focuses on correcting erroneous facts in generated summaries, generalizable to any summarization system. (ii) We propose two methods to solve multi-fact correction problem with single or multi-span selection in an iterative or auto-regressive manner, respectively. (iii) Experimental results on multiple summarization benchmarks demonstrate that our approach can significantly improve multiple factuality measurements without a huge drop on ROUGE scores. 2 Related Work The general neural-based encoder-decoder structure for abstractive summarization is first proposed by Rush et al. (2015). Later work improves this structure with better encoders, such as LSTMs (Chopra et al., 2016) and GRUs (Nallapati et al., 2016), that are able to capture longrange dependencies, as well as with reinforcement learning methods that directly optimize summarization evaluation scores (Paulus et al., 2018). One drawback of the earlier neural-based summarization models is the inability to produce out-of9321 Figure 1: Training example created for the QA-span prediction model (upper right) and the auto-regressive fact correction model (bottom right). vocabulary words, as the model can only generate wh"
2020.emnlp-main.749,P17-1099,0,0.720848,"step of decoding. It is worth noting that although the argmax operations in Eqn. (9) and (10) are nondifferentiable, the model is trained based on the start and end positions of the ground-truth answer w.r.t. the start and end logits in Eqn. (4) and (5), which makes the gradient back-propagates to the encoder. Meanwhile, the encoder’s hidden states used to compose sent in Eqn. (11) also carry the i gradients. During inference, beam search is used to find the best sequence of predicted spans in the source to replace the masks. Compared to the conventional Pointer Network (Vinyals et al., 2015; See et al., 2017) that only points to one token at a time, our sequential span selection decoder has the flexibility to replace a mask by any number of entity tokens, which is often required in summary factual correction. 4 Experiment In this section, we present our results on using SpanFact for multiple summarization datasets. 4.1 CNN DailyMail XSum Gigaword (9) Experimental Setup Training data for our fact correction models are generated as described in Section 3.2 on CNN/DailyMail (Hermann et al., 2015), XSum (Narayan et al., 2018) and Gigaword (Graff et al., 2003; Rush et al., 2015). The statistics of thes"
2020.emnlp-main.749,N19-4012,0,0.0317995,"Missing"
2020.emnlp-main.749,W17-2623,0,0.0219104,"query based on the source. In other words, their decoder regenerates the whole sequence token by token with a pointer-generator, which inherits the backbone of generative models that suffer from hallucination. 4.4 Experimental Results 5 We were not able to obtain any of the QA evaluation model or code from Wang et al. (2020); Durmus et al. (2020); Maynez et al. (2020) as the authors are still in the stage of making the code public. We used pre-trained UniLM model for question generation (QG) and BertForQuestionAnswering model for question answering (QA). The QG model is fine-tuned on NewsQA (Trischler et al., 2017) with entityanswer conditional task (Wang et al., 2020), and the QA model is pre-trained on SQuAD 2.0 (Rajpurkar et al., 2018). Tables 3, 4, and 5 summarize the results on the CNN/DailyMail, XSum and Gigaword datasets, respectively. Each block in the tables compares the original summarization model’s output with 6 https://github.com/darsh10/split_ encoder_pointer_summarizer 9326 Datasets QGQA FactCC sent 1 ROUGE 2 L GenParse (base) Split Encoders QA-Span Auto-regressive 52.63 63.60 66.47 64.77 46.07 48.22 52.17 48.95 35.23 34.32 34.38 33.97 17.11 17.01 16.50 16.08 32.88 31.98 32.07 31.70 GenPa"
2020.emnlp-main.749,2020.acl-main.450,0,0.238868,"ransformer-based models (Vaswani et al., 2017) pre-trained using self-supervised objectives on large text corpora (Devlin et al., 2019; Radford et al., 2018; Lewis et al., 2020; Raffel et al., 2020), abstractive summarization models are surpassing extractive ones on automatic evaluation metrics such as ROUGE (Lin, 2004). However, several studies (Falke 9320 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9320–9331, c November 16–20, 2020. 2020 Association for Computational Linguistics et al., 2019; Goodrich et al., 2019; Kry´sci´nski et al., 2019; Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020) observe that despite high ROUGE scores, system-generated abstractive summaries are often factually inconsistent with respect to the source text. Factual inconsistency is a well-known problem for conditional text generation, which requires models to generate readable text that is faithful to the input document. Consequently, sequence-to-sequence generation models need to learn to balance signals between the source for faithfulness and the learned language modeling prior for fluency (Kry´sci´nski et al., 2019). The dual objectives render abstractive su"
2020.starsem-1.10,W09-2508,0,0.0431508,"RT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.1 1 A car is a _____ vehicle Plural Grammatical Number Probe trees cars are _____ Figure 1: Illustration of BERT’s inconsistent predictions on singular and plural hypernymy probes. NLP tasks, such as recognizing textual entailment (RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We v"
2020.starsem-1.10,E12-1004,0,0.0349348,"ed analysis, as ideally we would like AI agents to reason like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes d"
2020.starsem-1.10,C18-1152,0,0.015533,"distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentative perspective as treating PLMs as a tool to mine or store knowledge, like hypernymhyponym and other relations, from text (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopts an agentive perspective (Ettinger et al., 2018; Talmor et al., 2019), Introduction Hierarchical representations of concepts play a central role in reasoning and understanding natural language (Wellman and Gelman, 1992). They have long been studied as a core NLP objective in their own right through tasks requiring the identification of hypernyms (Hearst, 1992; Snow et al., 2005, 2006), and as components for use in downstream * Part of this work was done during an internship at Microsoft Research. 1 Diagnostic framework available at https: //github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Common"
2020.starsem-1.10,Q17-1010,0,0.0828281,"Missing"
2020.starsem-1.10,P16-1047,0,0.0620303,"Missing"
2020.starsem-1.10,P19-1470,0,0.092995,"ety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentative perspective as treating PLMs as a tool to mine or store knowledge, like hypernymhyponym and other relations, from text (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopts an agentive perspective (Ettinger et al., 2018; Talmor et al., 2019), Introduction Hierarchical representations of concepts play a central role in reasoning and understanding natural language (Wellman and Gelman, 1992). They have long been studied as a core NLP objective in their own right through tasks requiring the identification of hypernyms (Hearst, 1992; Snow et al., 2005, 2006),"
2020.starsem-1.10,W98-0707,0,0.0973099,"the NEG-136 diagnostic constructed by Ettinger (2020), selecting the affirmative contexts to test models’ use of hypernym information. Test items are drawn from a human study conducted by Fischler et al. (1983), wherein subject words are 18 concrete nouns and hypernyms belong to nine superordinate categories (Battig and Montague, 1969).6 The final diagnostic set consists of 18 prompts. LM D IAGNOSTIC E XTENDED: In this work, we additionally expand LM D IAGNOSTIC to construct a larger diagnostic set. For each superordinate category (Battig and Montague, 1969), we extract hyponyms from WordNet (Fellbaum, 1998a) such that they are nouns, not named entities, and 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et"
2020.starsem-1.10,N15-1098,0,0.0945249,"Missing"
2020.starsem-1.10,N18-2017,0,0.0663242,"Missing"
2020.starsem-1.10,C92-2082,0,0.427317,"oui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopts an agentive perspective (Ettinger et al., 2018; Talmor et al., 2019), Introduction Hierarchical representations of concepts play a central role in reasoning and understanding natural language (Wellman and Gelman, 1992). They have long been studied as a core NLP objective in their own right through tasks requiring the identification of hypernyms (Hearst, 1992; Snow et al., 2005, 2006), and as components for use in downstream * Part of this work was done during an internship at Microsoft Research. 1 Diagnostic framework available at https: //github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and"
2020.starsem-1.10,D19-1275,0,0.0263793,"age prompt designed to exercise a particular competence; for example, ‘A robin is a [MASK]’ to evaluate knowledge of hypernymy. The word assigned the highest probability at the masked position is considered the PLM’s answer. In this work, we design diagnostics to examine how systematically this “knowledge” generalizes. We consider two kinds of diagnostics—(1) Consistency: We evaluate a PLM’s ability to consistently answer queries reflecting the same conceptual understanding. We use a simple number consistency 3 Consistency tasks can be considered complementary to the control tasks proposed by Hewitt and Liang (2019). While control tasks test attribution, consistency tasks test validity. 4 Our study is based on probes in English. 5 This distinction is concerned with the axis of generalization of probes. In our syntagmatic generalization probes, we are concerned with different lexico-syntactic contexts where a model can demonstrate its knowledge of hypernymy. In the paradigmatic generalization probes, we are concerned with generalizing to novel hypernym/hyponym pairs. 2 We refer to such probes henceforth as zero-shot masked LM probes, since they require no training and use BERT’s masked-LM component to fil"
2020.starsem-1.10,N19-1112,0,0.0732591,"named entities, and 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their element-wise product w1 w2 to form representation ~x. Predict: We then apply a so"
2020.starsem-1.10,N19-1419,0,0.0246161,"t al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better than abstract ones (is strong). Wallace et al. (2019) Our work examines, in particular, hypernymy knowledg"
2020.starsem-1.10,marelli-etal-2014-sick,0,0.0142527,"sideration of design choices. 5 Closest to our work, Kassner and Sch¨utze (2020) find that PLMs do not differentiate between negated and non-negated statements. Negation is a notoriously hard phenomenon for neural NLP models (Morante and Sporleder, 2012; Fancellu et al., 2016; Naik et al., 2018); our work demonstrates that even affirmative factual knowledge that can be extracted from BERT does not systematically generalize. Our work is also closely related to recent challenge set construction efforts, which aim to serve as sanity checks on the knowledge and commonsense capabilities of models (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and s"
2020.starsem-1.10,2020.tacl-1.28,0,0.332855,"RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentative perspective as treating PLMs as a tool to mine or store knowledge, like hypernymhyponym and other relations, from text (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopt"
2020.starsem-1.10,P19-1334,0,0.0435905,"ot differentiate between negated and non-negated statements. Negation is a notoriously hard phenomenon for neural NLP models (Morante and Sporleder, 2012; Fancellu et al., 2016; Naik et al., 2018); our work demonstrates that even affirmative factual knowledge that can be extracted from BERT does not systematically generalize. Our work is also closely related to recent challenge set construction efforts, which aim to serve as sanity checks on the knowledge and commonsense capabilities of models (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019"
2020.starsem-1.10,2020.acl-main.698,0,0.0624841,"Missing"
2020.starsem-1.10,D18-1546,0,0.0542722,"Missing"
2020.starsem-1.10,P09-1089,0,0.0144145,"autionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.1 1 A car is a _____ vehicle Plural Grammatical Number Probe trees cars are _____ Figure 1: Illustration of BERT’s inconsistent predictions on singular and plural hypernymy probes. NLP tasks, such as recognizing textual entailment (RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrume"
2020.starsem-1.10,S19-1026,0,0.0482428,"Missing"
2020.starsem-1.10,W13-0904,0,0.0150334,"ng accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.1 1 A car is a _____ vehicle Plural Grammatical Number Probe trees cars are _____ Figure 1: Illustration of BERT’s inconsistent predictions on singular and plural hypernymy probes. NLP tasks, such as recognizing textual entailment (RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentati"
2020.starsem-1.10,J12-2001,0,0.0653652,"Missing"
2020.starsem-1.10,N16-1098,0,0.0316754,"ander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and analyzing their linguistic competencies and world knowledge, sometimes through tasks such as natural language inference (Williams et al., 2018; Wang et al., 2018) or story completion (Zellers et al., 2018, 2019; Mostafazadeh et al., 2016). In this work, we examine the agentive perspective, focusing specifically on the validity of conclusions drawn from probing studies. A popular approach to probing knowledge in pre-trained language models is the zero-shot masked-LM probing task. For example, given the statement ‘A robin is a [MASK]’, a PLM that produces the correct completion ‘bird’ is considered successful.2 Past work has studied this competency in BERT (Ettinger, 2020), offering BERT’s ability to correctly retrieve noun hypernyms in cloze tasks as evidence that it successfully encodes hypernymy information. But to what exten"
2020.starsem-1.10,E14-1054,0,0.0251545,"ly we would like AI agents to reason like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve"
2020.starsem-1.10,2020.tacl-1.54,0,0.0241367,"knowledge and commonsense capabilities of models (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better"
2020.starsem-1.10,C14-1097,0,0.0571385,"Missing"
2020.starsem-1.10,C18-1198,1,0.897181,"Missing"
2020.starsem-1.10,R11-2019,0,0.0184258,"but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlying PLMs. We suggest that future studies carefully evaluate the generalizability of their methods, and always be accompanied by consistency checks and controls to ensure that cla"
2020.starsem-1.10,L16-1722,0,0.0266883,"Missing"
2020.starsem-1.10,W15-4208,0,0.0164282,"terpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlying PLMs. We suggest that future studies carefully evaluate the generalizability of their methods, and always be accompanied by consistency checks and con"
2020.starsem-1.10,D14-1162,0,0.0954969,"Missing"
2020.starsem-1.10,D19-1250,0,0.100386,"Missing"
2020.starsem-1.10,D16-1159,0,0.0185649,"e extract hyponyms from WordNet (Fellbaum, 1998a) such that they are nouns, not named entities, and 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their el"
2020.starsem-1.10,S18-2023,0,0.0605095,"Missing"
2020.starsem-1.10,Q19-1027,0,0.0179213,"nd 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their element-wise product w1 w2 to form representation ~x. Predict: We then apply a softmax classifier over the"
2020.starsem-1.10,K15-1018,0,0.0153998,"is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlyin"
2020.starsem-1.10,P06-1101,0,0.195341,"Missing"
2020.starsem-1.10,L18-1239,0,0.0295604,"Missing"
2020.starsem-1.10,S18-2020,0,0.0137491,"the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their element-wise product w1 w2 to form representation ~x. Predict: We then apply a softmax classifier over the formed representation~o = sof tmax(W · ReLU (Dropout(h(~x)))) where h is a 300-dimensional hidden layer, dropout probability = 0.2, W ∈ Rn×300 , and n=2. 2.2.2 Syntagmatic Generalization Metrics Open vocabulary accuracy: We compute mean precision@k (Open Voc.) where for a given hyponym, the value is 1 if the hypernym is ranked in the top k results and 0 otherwise. We report results with both k = 1 and k"
2020.starsem-1.10,N18-1103,0,0.0274912,"Missing"
2020.starsem-1.10,S17-1004,0,0.0190867,"s on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlying PLMs. We suggest that future studies carefully evaluate the generalizability of their methods, and always be accompanied by consistency checks and controls to ensure that claims based on model behavi"
2020.starsem-1.10,D19-1534,0,0.0205141,"e grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better than abstract ones (is strong). Wallace et al. (2019) Our work examines, in particular, hypernymy knowledge encoded in BERT representations. The identification of hypernyms is studied extensively in cognitive science and philosophy. Some prominent theories include Rosch’s category theory (Rosch and Lloyd, 1978) and Tversky’s category resemblance approach (Tversky, 1977). This work 95 Dual Perspectives on PLMs: In this work, we characterize two perspectives on uncovering knowledge in PLMs: instrumentative and agent-based. We emphasize that while systematicity is a necessary requirement for agent-based analysis, as ideally we would like AI agents"
2020.starsem-1.10,W18-5446,0,0.0638054,"Missing"
2020.starsem-1.10,D19-1286,0,0.0189088,"s heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better than abstract ones (is strong). Wallace et al. (2019) Our work examines, in particular, hypernymy knowledge encoded in BERT representations. The identification of hypernyms is studied extensively in cognitive science and philosophy. Some promi"
2020.starsem-1.10,C14-1212,0,0.0185275,"n like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general compe"
2020.starsem-1.10,W03-1011,0,0.139551,"uirement for agent-based analysis, as ideally we would like AI agents to reason like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered"
2020.starsem-1.10,N18-1101,0,0.0190512,"Microsoft Research. 1 Diagnostic framework available at https: //github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and analyzing their linguistic competencies and world knowledge, sometimes through tasks such as natural language inference (Williams et al., 2018; Wang et al., 2018) or story completion (Zellers et al., 2018, 2019; Mostafazadeh et al., 2016). In this work, we examine the agentive perspective, focusing specifically on the validity of conclusions drawn from probing studies. A popular approach to probing knowledge in pre-trained language models is the zero-shot masked-LM probing task. For example, given the statement ‘A robin is a [MASK]’, a PLM that produces the correct completion ‘bird’ is considered successful.2 Past work has studied this competency in BERT (Ettinger, 2020), offering BERT’s ability to correctly retrieve noun hypernyms"
2020.starsem-1.10,D18-1009,0,0.0123462,"//github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and analyzing their linguistic competencies and world knowledge, sometimes through tasks such as natural language inference (Williams et al., 2018; Wang et al., 2018) or story completion (Zellers et al., 2018, 2019; Mostafazadeh et al., 2016). In this work, we examine the agentive perspective, focusing specifically on the validity of conclusions drawn from probing studies. A popular approach to probing knowledge in pre-trained language models is the zero-shot masked-LM probing task. For example, given the statement ‘A robin is a [MASK]’, a PLM that produces the correct completion ‘bird’ is considered successful.2 Past work has studied this competency in BERT (Ettinger, 2020), offering BERT’s ability to correctly retrieve noun hypernyms in cloze tasks as evidence that it successfully encodes hypern"
2020.starsem-1.10,P19-1472,0,0.0245132,"Missing"
2020.tacl-1.21,N09-1003,0,0.188619,"Missing"
2020.tacl-1.21,D19-1539,0,0.0431813,"Missing"
2020.tacl-1.21,Q17-1010,0,0.0673209,"space. Conversely, the main distributional vector space is not polluted by the need to model lexical relations in the same space, as is the case for previous models. Furthermore, the explicit linear projection that is learned ensures that a relation-specific subspace exists in the original distributional vector space, and can thus be discovered by a downstream model if the extrinsic task requires knowledge about lexical-semantic relations. Post-hoc Approaches. In the post-hoc approach, pre-trained word vectors such as GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013), FastText (Bojanowski et al., 2017), or Paragram (Wieting et al., 2015) are fine-tuned to endow them with lexical relational information (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016, 2017; Jo, 2018; Jo and Choi, 2018; Vuli´c and Mrkˇsi´c, 2017; Glavaˇs and Vuli´c, 2018). In this paper, we primarily discuss LEXSUB as a post-hoc model. This formulation of LEXSUB is similar to the other post-hoc approaches mentioned above with the significant difference that the lexical relations are enforced in a lexical subspace instead of the original distributional vector spa"
2020.tacl-1.21,D15-1075,0,0.0590354,"ond set of experiments where models for all the extrinsic tasks are trained in the original settings (i.e., without the changes mentioned above). In these experiments, we do not remove character embeddings from any model, nor do we put any restrictions on fine-tuning of the pretrained word embeddings. These results for both the experiments are reported in Table 4. Textual Entailment. For textual entailment experiments, we use the Decomposable Attention model by Parikh et al. (2016) for our experiments. We train and evaluate the models on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) using the standard train, test and validation split. 7 Results Question Answering. We use the SQUAD1.1 question answering dataset (Rajpurkar et al., 2016). The dataset contains 100k+ crowd-sourced question answer pairs. We use the BiDAF model (Seo et al., 2016) for the question answering task. We report the accuracy on the development set for SQuAD. We now report on the results of our comparisons of LEXSUB to Vanilla embeddings and baselines trained on the same lexical resource as LEXSUB. We use the main vector space embeddings in all our experiments except for hypernymy experiments, for whic"
2020.tacl-1.21,P98-1013,0,0.120046,"Missing"
2020.tacl-1.21,N18-1045,0,0.0314617,"ibutions. In summary, we propose LEXSUB, a framework for learning lexical linear subspaces within the distributional vector space. The proposed framework can model all major kinds of lexical-semantic relations, namely, attract-sym312 Other Approaches. Several approaches do not fall into either of the categories mentioned above. A subset of these approaches attempts to learn lexical relations, especially hypernymy, directly by embedding a lexical database, for example, Poincar´e Embeddings (Nickel and Kiela, 2017) or Order-Embeddings (Vendrov et al., 2015). Another set of approaches, like DIH (Chang et al., 2018) or Word2Gauss (Vilnis and McCallum, 2014; Athiwaratkun and Wilson, 2017) attempt to learn the hypernymy relation directly from the corpus without relying on any lexical database. The third set of approaches attempt to learn a scoring function over a sparse bag of words (SBOW) features. These approaches are summarized by Shwartz et al. (2017). the expense of distributional information. Unlike Rothe et al. (2016), our proposed method tries to retain the distributional information in the embeddings so that they can be used as a general-purpose initialization in any NLP pipeline. Embeddings from"
2020.tacl-1.21,P18-1004,0,0.0374749,"Missing"
2020.tacl-1.21,J15-4004,0,0.303789,"γ = 2, δmax = 1.5, syn hyp mer δmin = 0.5, δmin = 1, δmin = 1.0, and νsyn = 0.01, νhyp = 0.01, νmer = 0.001. We rely on the validation sets corresponding to our extrinsic tasks (Section 6.2) for choosing these hyperparameter values. We ran a grid search on the hyperparameter space and selected the final set of hyperparameters by first ranking validation (11) 3.4 Overall Loss Function The overall loss of LEXSUB is Ltotal = Ldist +Llex . 315 WordSim353 dataset (Agirre et al., 2009) to measure the ability of the embedding’s to retain the distributional information. We use the SimLex999 dataset (Hill et al., 2015) and SimVerb 3500 (Gerz et al., 2016) to evaluate the embedding’s ability to detect graded synonymy and antonymy relations. Both the relatedness and similarity tasks were evaluated in the main vector space for LEXSUB. results for each task in descending order, then calculating the mean rank across the tasks. We selected the hyperparameters that achieved the best (i.e., lowest) mean rank. 5 Baselines Vanilla. The Vanilla baselines refer to the original GloVe word embeddings without any lexical constraints. 6 Evaluations Hypernymy Tasks. Following Roller et al. (2018), we consider three tasks in"
2020.tacl-1.21,N15-1184,0,0.322267,"Missing"
2020.tacl-1.21,N15-1070,0,0.0260918,"previous models. Furthermore, the explicit linear projection that is learned ensures that a relation-specific subspace exists in the original distributional vector space, and can thus be discovered by a downstream model if the extrinsic task requires knowledge about lexical-semantic relations. Post-hoc Approaches. In the post-hoc approach, pre-trained word vectors such as GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013), FastText (Bojanowski et al., 2017), or Paragram (Wieting et al., 2015) are fine-tuned to endow them with lexical relational information (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016, 2017; Jo, 2018; Jo and Choi, 2018; Vuli´c and Mrkˇsi´c, 2017; Glavaˇs and Vuli´c, 2018). In this paper, we primarily discuss LEXSUB as a post-hoc model. This formulation of LEXSUB is similar to the other post-hoc approaches mentioned above with the significant difference that the lexical relations are enforced in a lexical subspace instead of the original distributional vector space. Rothe et al. (2016) explores the idea of learning specialized subspaces with to reduce the dimensionality of distributional space such that"
2020.tacl-1.21,W18-3003,0,0.0112774,"c subspace exists in the original distributional vector space, and can thus be discovered by a downstream model if the extrinsic task requires knowledge about lexical-semantic relations. Post-hoc Approaches. In the post-hoc approach, pre-trained word vectors such as GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013), FastText (Bojanowski et al., 2017), or Paragram (Wieting et al., 2015) are fine-tuned to endow them with lexical relational information (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016, 2017; Jo, 2018; Jo and Choi, 2018; Vuli´c and Mrkˇsi´c, 2017; Glavaˇs and Vuli´c, 2018). In this paper, we primarily discuss LEXSUB as a post-hoc model. This formulation of LEXSUB is similar to the other post-hoc approaches mentioned above with the significant difference that the lexical relations are enforced in a lexical subspace instead of the original distributional vector space. Rothe et al. (2016) explores the idea of learning specialized subspaces with to reduce the dimensionality of distributional space such that it maximally preserves relevant task-specific information at Contributions. In summary, we propose LEXSUB,"
2020.tacl-1.21,N13-1092,0,0.122137,"Missing"
2020.tacl-1.21,D16-1235,0,0.0334612,"Missing"
2020.tacl-1.21,D15-1242,0,0.0440283,"Missing"
2020.tacl-1.21,P15-2020,0,0.054239,"Missing"
2020.tacl-1.21,N16-1018,0,0.121588,"Missing"
2020.tacl-1.21,P15-1145,0,0.02494,"oc Approaches. The ad-hoc class of approaches add auxiliary lexical constraints to the distributional similarity loss function, usually, a language modeling objective like CBOW (Mikolov et al., 2013) or recurrent neural network language model (Mikolov et al., 2010; Sundermeyer et al., 2012). These constraints can either be viewed as a prior or as a regularizer to the distributional objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a; Fried and Duh, 2014). In other work, the original language modeling objective is modified to incorporate lexical constraints (Liu et al., 2015; Osborne et al., 2016; Bollegala et al., 2016; Ono et al., 2015; Nguyen et al., 2016, 2017; Tifrea et al., 2018). We discuss the ad-hoc formulation of LEXSUB in Appendix A. 3 Model 3.1 Task Definition An alternate axis along which to classify these approaches is by their ability to model different types of lexical relations. These types can be enumerated as symmetric-attract (synonymy), symmetric-repel (antonymy), and asymmetricattract (hypernymy, meronymy). Most approaches mentioned above can handle symmetric-attract type relations, but only a few of them can model other types of lexical rel"
2020.tacl-1.21,Q17-1022,0,0.0254542,"Missing"
2020.tacl-1.21,P19-1441,0,0.0207126,"srupt the distributional information, leading to poor extrinsic task performance. We expand on this point in Section 8.3. State-of-the-Art Results in Extrinsic Tasks. We have also added the current state-of-theart results for the respective extrinsic tasks in Table 4 (last row). The current state of the art for NER is Baevski et al. (2019). The authors also use the model proposed by Peters et al. (2018) but initialize the model with contextualized embeddings from a bi-directional transformer. Similarly, the current state of the art for SST-2 and QQP (ERNIE 2.0; Sun et al., 2019), SNLI (MTDNN; Liu et al., 2019), and SQuAD (XLNet; Yang et al., 2019) are all initialized with contextualized embeddings from a bidirectional transformerbased model trained on a data that is orders of magnitude larger than the GloVe variant used in our experiments. The contextualized embeddings, because of their ability to represent the word in the context of its usage, are considerably more powerful than GloVe, hence the models relying on them are not directly comparable to our model or the other baselines. 319 In this section, we perform several analyses to understand the behaviors of our model and the baselines better, f"
2020.tacl-1.21,D17-1022,0,0.0330397,"Missing"
2020.tacl-1.21,P16-2074,0,0.0174916,"the distributional similarity loss function, usually, a language modeling objective like CBOW (Mikolov et al., 2013) or recurrent neural network language model (Mikolov et al., 2010; Sundermeyer et al., 2012). These constraints can either be viewed as a prior or as a regularizer to the distributional objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a; Fried and Duh, 2014). In other work, the original language modeling objective is modified to incorporate lexical constraints (Liu et al., 2015; Osborne et al., 2016; Bollegala et al., 2016; Ono et al., 2015; Nguyen et al., 2016, 2017; Tifrea et al., 2018). We discuss the ad-hoc formulation of LEXSUB in Appendix A. 3 Model 3.1 Task Definition An alternate axis along which to classify these approaches is by their ability to model different types of lexical relations. These types can be enumerated as symmetric-attract (synonymy), symmetric-repel (antonymy), and asymmetricattract (hypernymy, meronymy). Most approaches mentioned above can handle symmetric-attract type relations, but only a few of them can model other types of lexical relations. For example, Ono et al. (2015) can exclusively model antonymy, Tifrea et al."
2020.tacl-1.21,N15-1100,0,0.0273326,"cal constraints to the distributional similarity loss function, usually, a language modeling objective like CBOW (Mikolov et al., 2013) or recurrent neural network language model (Mikolov et al., 2010; Sundermeyer et al., 2012). These constraints can either be viewed as a prior or as a regularizer to the distributional objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a; Fried and Duh, 2014). In other work, the original language modeling objective is modified to incorporate lexical constraints (Liu et al., 2015; Osborne et al., 2016; Bollegala et al., 2016; Ono et al., 2015; Nguyen et al., 2016, 2017; Tifrea et al., 2018). We discuss the ad-hoc formulation of LEXSUB in Appendix A. 3 Model 3.1 Task Definition An alternate axis along which to classify these approaches is by their ability to model different types of lexical relations. These types can be enumerated as symmetric-attract (synonymy), symmetric-repel (antonymy), and asymmetricattract (hypernymy, meronymy). Most approaches mentioned above can handle symmetric-attract type relations, but only a few of them can model other types of lexical relations. For example, Ono et al. (2015) can exclusively model ant"
2020.tacl-1.21,D16-1264,0,0.0282178,"se experiments, we do not remove character embeddings from any model, nor do we put any restrictions on fine-tuning of the pretrained word embeddings. These results for both the experiments are reported in Table 4. Textual Entailment. For textual entailment experiments, we use the Decomposable Attention model by Parikh et al. (2016) for our experiments. We train and evaluate the models on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) using the standard train, test and validation split. 7 Results Question Answering. We use the SQUAD1.1 question answering dataset (Rajpurkar et al., 2016). The dataset contains 100k+ crowd-sourced question answer pairs. We use the BiDAF model (Seo et al., 2016) for the question answering task. We report the accuracy on the development set for SQuAD. We now report on the results of our comparisons of LEXSUB to Vanilla embeddings and baselines trained on the same lexical resource as LEXSUB. We use the main vector space embeddings in all our experiments except for hypernymy experiments, for which we use the hypernymy space embeddings. Paraphrase Detection. For the paraphrase detection task, we use the BIMPM model by Wang et al. (2017) for our expe"
2020.tacl-1.21,P18-2057,0,0.0302463,"Missing"
2020.tacl-1.21,Q16-1030,0,0.037053,"Missing"
2020.tacl-1.21,D16-1244,0,0.0951689,"Missing"
2020.tacl-1.21,P15-2070,0,0.0602275,"Missing"
2020.tacl-1.21,D14-1162,0,0.100426,"lation without being polluted by information from the distributional space. Conversely, the main distributional vector space is not polluted by the need to model lexical relations in the same space, as is the case for previous models. Furthermore, the explicit linear projection that is learned ensures that a relation-specific subspace exists in the original distributional vector space, and can thus be discovered by a downstream model if the extrinsic task requires knowledge about lexical-semantic relations. Post-hoc Approaches. In the post-hoc approach, pre-trained word vectors such as GloVe (Pennington et al., 2014), Word2Vec (Mikolov et al., 2013), FastText (Bojanowski et al., 2017), or Paragram (Wieting et al., 2015) are fine-tuned to endow them with lexical relational information (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016, 2017; Jo, 2018; Jo and Choi, 2018; Vuli´c and Mrkˇsi´c, 2017; Glavaˇs and Vuli´c, 2018). In this paper, we primarily discuss LEXSUB as a post-hoc model. This formulation of LEXSUB is similar to the other post-hoc approaches mentioned above with the significant difference that the lexical relations are enforced in"
2020.tacl-1.21,N18-1202,0,0.336163,"ings and plug these post-hoc embeddings into the extrinsic task model. Second, we remove character embeddings from the input layer. Finally, we do not fine-tune the pretrained embeddings. cues. We do so by injecting our embeddings into recent high-performing models for those tasks. The tasks and models are: NER Classification. We use the CoNLL 2003 NER task (Tjong Kim Sang and De Meulder, 2003) for the Named Entity Recognition (NER) Task. The dataset consists of news stories from Reuters where the entities have been labeled into four classes (PER, LOC, ORG, MISC). We use the model proposed by Peters et al. (2018) for the NER task. Sentiment Classification. We use the BiAttentive Classification Network (BTN) by McCann et al. (2017) to train a sentiment classifier. We train all models for sentiment classification on the Stanford Sentiment Treebank (SST) (Socher et al., 2013). We use a two-class granularity where we remove the ‘‘neutral’’ class following McCann et al. (2017) and just use the ‘‘positive’’ and ‘‘negative’’ classes for classification. Setup 2: In order to demonstrate that we are not unfairly penalizing the base models, we also conduct a second set of experiments where models for all the ext"
2020.tacl-1.21,P15-1173,0,0.0548235,"Missing"
2020.tacl-1.21,E14-4008,0,0.0631433,"Missing"
2020.tacl-1.21,P16-1226,0,0.12806,"Missing"
2020.tacl-1.21,J17-4004,0,0.0297556,"Missing"
2020.tacl-1.21,E17-1007,0,0.0143455,"these approaches attempts to learn lexical relations, especially hypernymy, directly by embedding a lexical database, for example, Poincar´e Embeddings (Nickel and Kiela, 2017) or Order-Embeddings (Vendrov et al., 2015). Another set of approaches, like DIH (Chang et al., 2018) or Word2Gauss (Vilnis and McCallum, 2014; Athiwaratkun and Wilson, 2017) attempt to learn the hypernymy relation directly from the corpus without relying on any lexical database. The third set of approaches attempt to learn a scoring function over a sparse bag of words (SBOW) features. These approaches are summarized by Shwartz et al. (2017). the expense of distributional information. Unlike Rothe et al. (2016), our proposed method tries to retain the distributional information in the embeddings so that they can be used as a general-purpose initialization in any NLP pipeline. Embeddings from Rothe et al. (2016)’s method can only be used for the task on which they were trained. Ad-hoc Approaches. The ad-hoc class of approaches add auxiliary lexical constraints to the distributional similarity loss function, usually, a language modeling objective like CBOW (Mikolov et al., 2013) or recurrent neural network language model (Mikolov e"
2020.tacl-1.21,D13-1170,0,0.00748204,"for those tasks. The tasks and models are: NER Classification. We use the CoNLL 2003 NER task (Tjong Kim Sang and De Meulder, 2003) for the Named Entity Recognition (NER) Task. The dataset consists of news stories from Reuters where the entities have been labeled into four classes (PER, LOC, ORG, MISC). We use the model proposed by Peters et al. (2018) for the NER task. Sentiment Classification. We use the BiAttentive Classification Network (BTN) by McCann et al. (2017) to train a sentiment classifier. We train all models for sentiment classification on the Stanford Sentiment Treebank (SST) (Socher et al., 2013). We use a two-class granularity where we remove the ‘‘neutral’’ class following McCann et al. (2017) and just use the ‘‘positive’’ and ‘‘negative’’ classes for classification. Setup 2: In order to demonstrate that we are not unfairly penalizing the base models, we also conduct a second set of experiments where models for all the extrinsic tasks are trained in the original settings (i.e., without the changes mentioned above). In these experiments, we do not remove character embeddings from any model, nor do we put any restrictions on fine-tuning of the pretrained word embeddings. These results"
2020.tacl-1.21,C14-1212,0,0.199206,"Missing"
2020.tacl-1.21,W03-0419,0,0.584538,"Missing"
2021.acl-long.163,2020.emnlp-main.463,0,0.134706,"oorer generalization results are often observed (Keskar et al., 2016), especially when the dataset size is only several times larger than the batch size. Furthermore, many recent works noticed a performance gap in this training approach due to layer normalization 2089 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2089–2102 August 1–6, 2021. ©2021 Association for Computational Linguistics (Xu et al., 2019; Nguyen and Salazar, 2019; Zhang et al., 2019a; Wang et al., 2019b; Liu et al., 2020; Huang et al., 2020). Inspired by the recent T-Fixup by Huang et al. (2020), which eliminates the need for learning rate warm-up and layer normalization to train vanilla transformers, we derive a data-dependent initialization strategy by applying different analyses to address several key limitations of T-Fixup. We call our method the Data-dependent Transformer Fixed-update initialization scheme, DT-Fixup. In the mixed setup of additional yet-to-be-trained transformers on top of pre-trained models, DTFixup enables the training of significantly deeper transformers, and is generally applicable t"
2021.acl-long.163,2021.ccl-1.108,0,0.068026,"Missing"
2021.acl-long.553,W06-1805,0,0.06532,"an alleged criminal is not necessarily a criminal). Kamp and Partee (1995) further divided non-subsective adjectives in two categories: privative and plain. While when combined with nouns privative adjectives produce a disjoint set of entities from the original noun (e.g., former president does not fall under the class of presidents, making former a privative adjective), plain non-subsective adjectives do not guarantee this mutual exclusiveness (e.g., an alleged criminal may or may not be a criminal). This classification scheme has been adopted for many NLP applications including IE and RTE (Amoia and Gardent, 2006, 2007; McCrae 7118 et al., 2014). For RTE, inference rules were developed according to whether the adjective was non-subsective or not. For IE, non-subsective adjectives were treated as special cases for extracting open IE relations (Angeli et al., 2015). We show that there is also a relation between an adjective and the plausibility of the rest of the clause, even for subsective adjectives. This has direct implications for the extraction of generalizable abstract knowledge that can be extracted from a corpus. Aspects of this classification scheme have since been challenged, resulting in effo"
2021.acl-long.553,W07-1430,0,0.0739387,"notably different. The plausibility judgement of the event where a monkey turns on a light switch decreases when the adjectival modifier dead is added, while in the 1b or 1c examples, adding the same modifier leads to no change or an increase in event plausibility, respectively. This observation has important ramifications for many NLP applications like information extraction (IE) and recognizing textual entailment (RTE), where solutions have often relied on normative rules that group the effects of adjectives according to either the adjective or its taxonomic class (McNally and Boleda, 2004; Amoia and Gardent, 2007; McCrae et al., 2014). These taxonomies distinguish adjectives like false, dead, alleged (non-subsective) from others like red, large, or valid (subsective). Specifically, while the 1a example may influence systems to adopt the rule that adding a nonsubsective adjective like dead to a noun leads to a decrease in plausibility, the other examples suggest a conflicting rule. Distinguishing the effects of different adjectives (beyond just their denotation) may thus require common-sense and world knowledge. Powerful, massively pre-trained language models (LMs) have pushed the performance on variou"
2021.acl-long.553,P15-1034,0,0.0375407,"inal noun (e.g., former president does not fall under the class of presidents, making former a privative adjective), plain non-subsective adjectives do not guarantee this mutual exclusiveness (e.g., an alleged criminal may or may not be a criminal). This classification scheme has been adopted for many NLP applications including IE and RTE (Amoia and Gardent, 2006, 2007; McCrae 7118 et al., 2014). For RTE, inference rules were developed according to whether the adjective was non-subsective or not. For IE, non-subsective adjectives were treated as special cases for extracting open IE relations (Angeli et al., 2015). We show that there is also a relation between an adjective and the plausibility of the rest of the clause, even for subsective adjectives. This has direct implications for the extraction of generalizable abstract knowledge that can be extracted from a corpus. Aspects of this classification scheme have since been challenged, resulting in efforts to either expand on its definitions or abandon the taxonomy altogether. Del Pinal (2015) suggests that the meaning of certain nouns are only partially modified by non-subsective adjectives (e.g., only the functional features are modified), while Nayak"
2021.acl-long.553,S19-1028,0,0.0543264,"Missing"
2021.acl-long.553,D15-1075,0,0.0452398,"their taxonomical class) is on the interpretation of a sentence. For this, we frame our exploration in terms of changes in the plausibility of events, which we believe it can be seen as an extension to entailment. Recognizing Textual Entailment & Semantic Plausibility: The RTE Challenges were yearly sources of textual inference examples (Dagan et al., 2006) consisting of a three-way classification task with the inputs as sentence pairs {T , H} with labels for entailment, contradiction or unknown (meaning T neither contradicts nor entails H). Variations of this task are also described in SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The Johns Hopkins Ordinal Commonsense Inference (JOCI) task generalizes RTE to the problem of determining relative change in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or"
2021.acl-long.553,P06-1057,0,0.0475404,"e context words are just as instrumental in determining the effect of an adjective; resulting in a number of exceptions to taxonomically-based rules. Inspired by this, our work explores the broader question of how dependent the effect of any adjective (beyond their taxonomical class) is on the interpretation of a sentence. For this, we frame our exploration in terms of changes in the plausibility of events, which we believe it can be seen as an extension to entailment. Recognizing Textual Entailment & Semantic Plausibility: The RTE Challenges were yearly sources of textual inference examples (Dagan et al., 2006) consisting of a three-way classification task with the inputs as sentence pairs {T , H} with labels for entailment, contradiction or unknown (meaning T neither contradicts nor entails H). Variations of this task are also described in SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The Johns Hopkins Ordinal Commonsense Inference (JOCI) task generalizes RTE to the problem of determining relative change in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for t"
2021.acl-long.553,P15-2017,0,0.0277755,"strating the overall reliability of the labels we collected. 7122 Model 3-Class Dev. Accuracy 5-Class Dev. Accuracy Majority Prediction Normative Rule Human Human (no context) 66.4 70.1 90.04 75.04 66.4 63.6 85.04 71.04 BERT (no context) RoBERTa (no context) DeBERTa (no context) 72.0 72.4 72.1 69.4 69.1 68.6 BERT RoBERTa DeBERTa 72.3 73.1 73.9 69.8 70.8 69.7 Table 5: Performance of various models on the A DEPT development set. 5 5.1 Methods Neural Models We evaluate several transformer-based models on A DEPT. For fine-tuning, we adopt the standard practice for sentence-pair tasks described by Devlin et al. (2015). We concatenate the first and second sentence with [SEP], prepend the sequence with [CLS], and feed the input to the transformer model. The representation for [CLS] is fed into a softmax layer for a five-way classification. BERT (Devlin et al., 2015) is one of the first transformer-based architectures, featuring a pretrained neural language model with bidirectional paths and sentence representations in consecutive hidden layers. RoBERTa (Liu et al., 2019) is an improved variant of BERT that adds more training data with larger batch sizes and longer training, as well as other refinements like"
2021.acl-long.553,J03-3005,0,0.266874,"e in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or pre-trained LMs (Radford et al., 2019). Prior work has also collected human assessments of the plausibility of adjective-noun pairs (Lapata et al., 1999; Keller and Lapata, 2003; Zhang et al., 2019); however, this line of work specifically focuses on the plausibility of bi-grams without context, known as selectional preference. 3 The Task: A DEPT We develop A DEPT, a semantic plausibility task that features over 16 thousand instances consisting of two sentences, where the second sentence differs from the first only by the inclusion of an adjectival modifier. Examples of these instances are in Table 1, where the inserted modifier is bracketed. Formally, given the original sentence s and the modified sentence s0 , s0 is identical to s except for the addition of an adje"
2021.acl-long.553,P17-2049,0,0.0289551,". Our experiments suggest a persistent performance gap between human annotators and large language representation models, with the later exhibiting a lower sensitivity to context. Finally, our task provides deeper insight into the effects of various classes of adjectives on event plausibility, and suggests that rules based solely on the adjective or its denotation do not suffice in determining the correct plausibility readings of events. In the future, we wish to investigate how A DEPT could be used to improve performance on related natural language inference tasks (e.g. MNLI, SNLI & SciTail (Khot et al., 2017)). We also plan to develop new models on A DEPT and transfer them to other semantic plausibility tasks. Acknowledgements This work was supported by the Natural Sciences and Engineering Research Council of Canada and by Microsoft Research. Jackie Chi Kit Cheung is supported by the Canada CIFAR AI Chair program, and is also a consulting researcher for Microsoft Research. Ethical Considerations While our focus on examining what effects adjectives have on the plausibility of arbitrary events makes ascertaining the broader impact of our work challenging, this work is not void of possible adverse so"
2021.acl-long.553,E99-1005,0,0.491264,"mining relative change in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or pre-trained LMs (Radford et al., 2019). Prior work has also collected human assessments of the plausibility of adjective-noun pairs (Lapata et al., 1999; Keller and Lapata, 2003; Zhang et al., 2019); however, this line of work specifically focuses on the plausibility of bi-grams without context, known as selectional preference. 3 The Task: A DEPT We develop A DEPT, a semantic plausibility task that features over 16 thousand instances consisting of two sentences, where the second sentence differs from the first only by the inclusion of an adjectival modifier. Examples of these instances are in Table 1, where the inserted modifier is bracketed. Formally, given the original sentence s and the modified sentence s0 , s0 is identical to s except fo"
2021.acl-long.553,2021.ccl-1.108,0,0.0400641,"Missing"
2021.acl-long.553,W14-4724,0,0.0253873,"ausibility judgement of the event where a monkey turns on a light switch decreases when the adjectival modifier dead is added, while in the 1b or 1c examples, adding the same modifier leads to no change or an increase in event plausibility, respectively. This observation has important ramifications for many NLP applications like information extraction (IE) and recognizing textual entailment (RTE), where solutions have often relied on normative rules that group the effects of adjectives according to either the adjective or its taxonomic class (McNally and Boleda, 2004; Amoia and Gardent, 2007; McCrae et al., 2014). These taxonomies distinguish adjectives like false, dead, alleged (non-subsective) from others like red, large, or valid (subsective). Specifically, while the 1a example may influence systems to adopt the rule that adding a nonsubsective adjective like dead to a noun leads to a decrease in plausibility, the other examples suggest a conflicting rule. Distinguishing the effects of different adjectives (beyond just their denotation) may thus require common-sense and world knowledge. Powerful, massively pre-trained language models (LMs) have pushed the performance on various natural language und"
2021.acl-long.553,L18-1286,0,0.0128694,"DEPT. notate the data, we provide human annotators with labelling instructions, while implementing quality control measures as exclusion criteria for final dataset instances. 4.1 Data Collection We now detail the steps of our data collection process (see Figure 1 for an overview). Tables 2 and 3 provide examples of how each step contributes to the creation of an A DEPT instance. Noun-amod extraction: In order to extract adjectival modifier and noun pairs, we use two dependency-parsed corpora: English Wikipedia, which we parse using the Stanza pipeline (Qi et al., 2020), and a subset of DepCC (Panchenko et al., 2018), an automatic parse of the Common Crawl corpus. After a preliminary examination of the modifier-noun pairs’ quality, we kept only those pairs that occur at least 10 times in their respective corpus. This filtered out many pairs that appeared anomalous or atypical (e.g., unwieldy potato). We extracted 10 million pairs from English Wikipedia and 70 million pairs from Common Crawl. Noun filtering: Using these pairs, we created dictionary items consisting of nouns—that co-occur with at least four different adjectival modifiers—along with their adjectival modifiers. This threshhold (as opposed to"
2021.acl-long.553,S16-2014,0,0.0753464,"or their improvement. We show that the effect of adjectival modifiers on event plausibility is context dependent: We quantify the degree to which plausibility judgements vary for the same adjective and taxonomic class, finding that rules based only on the adjective or its denotation are insufficient when assessing the plausibility readings of events. For example, in our task, the non-subsective adjective like dead led to a decrease in events plausibility as frequently as it led to no change at all. Building on prior work showing that normative rules are often broken for subsective adjectives (Pavlick and Callison-Burch, 2016), we inves1 The corpus and the code to of our experimental results are https://github.com/aemami1/ADEPT. reproduce available all at tigate possible effects across all types of adjectives, beyond just the taxonomical categories. The scope of our analysis also goes beyond entailment effects, examining the effects on plausibility, which can be seen as both complimentary and even an extension to entailment tasks. 2 Background and Related Work Taxonomy of adjectives: The taxonomic classification of adjectives into subsective and nonsubsective categories originates from the works of Parsons (1970),"
2021.acl-long.553,2020.acl-demos.14,0,0.0160308,"iew of the data collection process for A DEPT. notate the data, we provide human annotators with labelling instructions, while implementing quality control measures as exclusion criteria for final dataset instances. 4.1 Data Collection We now detail the steps of our data collection process (see Figure 1 for an overview). Tables 2 and 3 provide examples of how each step contributes to the creation of an A DEPT instance. Noun-amod extraction: In order to extract adjectival modifier and noun pairs, we use two dependency-parsed corpora: English Wikipedia, which we parse using the Stanza pipeline (Qi et al., 2020), and a subset of DepCC (Panchenko et al., 2018), an automatic parse of the Common Crawl corpus. After a preliminary examination of the modifier-noun pairs’ quality, we kept only those pairs that occur at least 10 times in their respective corpus. This filtered out many pairs that appeared anomalous or atypical (e.g., unwieldy potato). We extracted 10 million pairs from English Wikipedia and 70 million pairs from Common Crawl. Noun filtering: Using these pairs, we created dictionary items consisting of nouns—that co-occur with at least four different adjectival modifiers—along with their adjec"
2021.acl-long.553,D19-1625,0,0.0207552,"lment, contradiction or unknown (meaning T neither contradicts nor entails H). Variations of this task are also described in SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The Johns Hopkins Ordinal Commonsense Inference (JOCI) task generalizes RTE to the problem of determining relative change in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or pre-trained LMs (Radford et al., 2019). Prior work has also collected human assessments of the plausibility of adjective-noun pairs (Lapata et al., 1999; Keller and Lapata, 2003; Zhang et al., 2019); however, this line of work specifically focuses on the plausibility of bi-grams without context, known as selectional preference. 3 The Task: A DEPT We develop A DEPT, a semantic plausibility task that features over 16 thousand instances consisting of two sentences, where"
2021.acl-long.553,W18-5446,0,0.0466871,"Missing"
2021.acl-long.553,N18-2049,0,0.0268834,"ication task with the inputs as sentence pairs {T , H} with labels for entailment, contradiction or unknown (meaning T neither contradicts nor entails H). Variations of this task are also described in SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The Johns Hopkins Ordinal Commonsense Inference (JOCI) task generalizes RTE to the problem of determining relative change in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or pre-trained LMs (Radford et al., 2019). Prior work has also collected human assessments of the plausibility of adjective-noun pairs (Lapata et al., 1999; Keller and Lapata, 2003; Zhang et al., 2019); however, this line of work specifically focuses on the plausibility of bi-grams without context, known as selectional preference. 3 The Task: A DEPT We develop A DEPT, a semantic plausibili"
2021.acl-long.553,N18-1101,0,0.0287002,"the interpretation of a sentence. For this, we frame our exploration in terms of changes in the plausibility of events, which we believe it can be seen as an extension to entailment. Recognizing Textual Entailment & Semantic Plausibility: The RTE Challenges were yearly sources of textual inference examples (Dagan et al., 2006) consisting of a three-way classification task with the inputs as sentence pairs {T , H} with labels for entailment, contradiction or unknown (meaning T neither contradicts nor entails H). Variations of this task are also described in SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). The Johns Hopkins Ordinal Commonsense Inference (JOCI) task generalizes RTE to the problem of determining relative change in semantic plausibility on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or pre-trained LMs (Radford et al.,"
2021.acl-long.553,P19-1071,0,0.0236046,"y on an ordinal 5-level Likert scale (from impossible to very likely) (Zhang et al., 2017). Other semantic plausibility datasets have collected judgments for the plausibility of single events (Wang et al., 2018b) and the plausibility of adjectives modifying a meronym (Mullenbach et al., 2019). Such plausibility tasks have often been solved using either data-driven methods (Huang and Luo, 2017; Sasaki et al., 2017) or pre-trained LMs (Radford et al., 2019). Prior work has also collected human assessments of the plausibility of adjective-noun pairs (Lapata et al., 1999; Keller and Lapata, 2003; Zhang et al., 2019); however, this line of work specifically focuses on the plausibility of bi-grams without context, known as selectional preference. 3 The Task: A DEPT We develop A DEPT, a semantic plausibility task that features over 16 thousand instances consisting of two sentences, where the second sentence differs from the first only by the inclusion of an adjectival modifier. Examples of these instances are in Table 1, where the inserted modifier is bracketed. Formally, given the original sentence s and the modified sentence s0 , s0 is identical to s except for the addition of an adjective a before the ro"
2021.eacl-main.93,D19-5402,0,0.0118149,"2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer models also provide better sentence representations for unsupervised summarization methods. For instance, PAC S UM (Zheng and Lapata, 2019), a directed graph-based unsuperv"
2021.eacl-main.93,P18-1063,0,0.046415,"Missing"
2021.eacl-main.93,P16-1046,0,0.394966,"king (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances o"
2021.eacl-main.93,N18-2097,0,0.352922,"significantly with each other (Xiao and Carenini, 2019). We implement this insight by injecting hierarchies into our model, introducing section-level representations as graph nodes in addition to sentence nodes. By doing so, we convert a flat graph into a hierarchical non-fully-connected graph, which has two advantages: 1) reduced computational cost and 2) pruning of distracting weak connections between sentences across different sections. We call our approach Hierarchical and Positional Ranking model (H IPO R ANK) and evaluate it on summarizing long scientific articles from PubMed and arXiv (Cohan et al., 2018). Empirical results show that our method significantly improves performance over previous unsupervised models (Zheng and Lapata, 2019; Erkan and Radev, 2004) in both automatic and human evaluation. In addition, our simple unsupervised approach achieves performance comparable to many expensive state-of-the-art supervised neural models that are trained on hundreds of thousands of examples of long document pairs (Xiao and Carenini, 2019; Subramanian et al., 2019). This suggests that patterns in the discourse structure are highly useful for determining sentence importance in long scientific articl"
2021.eacl-main.93,D15-1045,0,0.0214495,"aditional supervised machine learning algorithms with surface features as input (Xiao and Carenini, 2019). Surface features such as sentence position, sentence and document length, keyphrase score, and fine-grain rhetorical categories are often combined with Naive Bayes (Teufel and Moens, 2002), CRFs and SVMs (Liakata et al., 2013), LSTM and MLP (Collins et al., 2017) for extractive summarization over long scientific articles. To the best of our knowledge, the only unsupervised extractive summarization model for long scientific documents relies on citation networks (Qazvinian and Radev, 2008; Cohan and Goharian, 2015), by extracting citation-contexts from citing articles and ranking Figure 1: Example of a hierarchical document graph constructed by our approach on a toy document that contains two sections {T1 , T2 }, each containing three sentences for a total of six sentences {s1 , . . . , s6 }. Each double-headed arrow represents two edges with opposite directions. The solid and dashed arrows indicate intra-section and inter-section connections respectively. When compared to the flat fully-connected graph of traditional methods, our use of hierarchy effectively reduces the number of edges from 60 to 24 in"
2021.eacl-main.93,K17-1021,0,0.0159363,"he whole document, while Xiao and Carenini (2019) divided articles into sections and used non-auto-regressive approaches to model global and local information. Besides neural approaches, most previous scientific article summarization systems employ traditional supervised machine learning algorithms with surface features as input (Xiao and Carenini, 2019). Surface features such as sentence position, sentence and document length, keyphrase score, and fine-grain rhetorical categories are often combined with Naive Bayes (Teufel and Moens, 2002), CRFs and SVMs (Liakata et al., 2013), LSTM and MLP (Collins et al., 2017) for extractive summarization over long scientific articles. To the best of our knowledge, the only unsupervised extractive summarization model for long scientific documents relies on citation networks (Qazvinian and Radev, 2008; Cohan and Goharian, 2015), by extracting citation-contexts from citing articles and ranking Figure 1: Example of a hierarchical document graph constructed by our approach on a toy document that contains two sections {T1 , T2 }, each containing three sentences for a total of six sentences {s1 , . . . , s6 }. Each double-headed arrow represents two edges with opposite d"
2021.eacl-main.93,N19-1423,0,0.124867,"framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer models also provide better sentence representations for unsupervised summarization methods. For instance, PAC S UM (Zheng and Lapata, 2019), a directed graph-based unsupervised model that utilizes BERT-based sentence representations, achieved comparable performance to supervised models on the CNN/DailyMail and NYT datasets. 2.2 Extractive Summariz"
2021.eacl-main.93,D18-1409,1,0.935261,"Missing"
2021.eacl-main.93,W04-1006,0,0.11682,"ure are a strong signal for determining importance in scientific articles. 1 Single document summarization aims at shortening a text and preserving the most important ideas of the source document. While abstractive strategies generate summaries with novel words, extractive strategies select sentences from the source to form a summary (Nenkova et al., 2011). Despite recent advances in abstractive summarization, extractive models are still attractive in cases where faithfully preserving the original text is the priority. For example, legal arguments can hinge on the exact wording of a contract (Farzindar and Lapalme, 2004), and ensuring the factual correctness of a summary can be critical in the health or scientific domains, which is a known weakness of current abstractive methods (Kry´sci´nski et al., 2019). Supervised neural-based models have been the dominant paradigm in recent extractive systems, at least for short news summarization (Nallapati et al., ∗ Equal contribution. Link to our code: mirandrom/HipoRank. Introduction anxiety affects quality of life in those living with parkinson’s disease (pd) more so than overall cognitive status, motor deficits, apathy, and depression. Introduction although anxiety"
2021.eacl-main.93,D18-1443,0,0.0172016,"ge-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer models also provide better sentence representations"
2021.eacl-main.93,D19-1620,1,0.848852,"ROUGE-L consistently across different hyperparameters and embedding models we tested. Application to other genres While our work here is focused on long scientific document summarization, we believe that our approach is promising for other genres of text, provided that the right discourse-aware biases are given to the model. Indeed, one version of our model with our proposed boundary function can be seen as a generalization of PAC S UM, which achieves state-of-the-art performance on unsupervised summarization of news by exploiting the well known lead bias of news text (Zheng and Lapata, 2019; Grenander et al., 2019). We leave such explorations of adapting H IPO R ANK to other genres to future work. 7 Conclusion We presented an unsupervised graph-based model for long scientific document summarization. The proposed approach augments the measure of sentence centrality by inserting directionality and hierarchy in the graph with boundary positional functions and hierarchical topic information grounded in discourse structure. Our simple unsupervised approach with rich discourse modelling outperforms previous unsupervised graph-based summarization models by wide margins and achieves comparable performance to st"
2021.eacl-main.93,N18-1065,0,0.0397732,"Missing"
2021.eacl-main.93,D13-1158,0,0.0248722,"upervised neural models that are trained on hundreds of thousands of examples of long document pairs (Xiao and Carenini, 2019; Subramanian et al., 2019). This suggests that patterns in the discourse structure are highly useful for determining sentence importance in long scientific articles, and that explicitly building in biases inspired by this structure is a viable strategy for building summarization systems. 2 2.1 Related Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al.,"
2021.eacl-main.93,P18-1013,0,0.0181621,"m (Grusky et al., 2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer models also provide better sentence representations for unsupervised summarization methods. For instance, PAC S UM (Zheng and Lapata, 2019), a directed gr"
2021.eacl-main.93,2020.acl-main.703,0,0.0307678,"Missing"
2021.eacl-main.93,D13-1070,0,0.0264667,"introduction section as a proxy for the whole document, while Xiao and Carenini (2019) divided articles into sections and used non-auto-regressive approaches to model global and local information. Besides neural approaches, most previous scientific article summarization systems employ traditional supervised machine learning algorithms with surface features as input (Xiao and Carenini, 2019). Surface features such as sentence position, sentence and document length, keyphrase score, and fine-grain rhetorical categories are often combined with Naive Bayes (Teufel and Moens, 2002), CRFs and SVMs (Liakata et al., 2013), LSTM and MLP (Collins et al., 2017) for extractive summarization over long scientific articles. To the best of our knowledge, the only unsupervised extractive summarization model for long scientific documents relies on citation networks (Qazvinian and Radev, 2008; Cohan and Goharian, 2015), by extracting citation-contexts from citing articles and ranking Figure 1: Example of a hierarchical document graph constructed by our approach on a toy document that contains two sections {T1 , T2 }, each containing three sentences for a total of six sentences {s1 , . . . , s6 }. Each double-headed arrow"
2021.eacl-main.93,W04-1013,0,0.0388685,"s is model-dependent (details in Section 1093 6.2). We used the publicly released BERT model3 (Devlin et al., 2019), PAC S UM BERT model4 (Zheng and Lapata, 2019), SentBERT and SentRoBERTa5 (Reimers and Gurevych, 2019), and BioMed word2vec representations6 (Moen and Ananiadou, 2013). A section’s representation is calculated as the average of its sentences’ representations. The similarity between sentences or sections is defined to be the cosine similarity between the distributed representations. 4.3 Evaluation Methods We evaluate our method with automatic evaluation metrics - ROUGE F1 scores (Lin, 2004). ROUGE1 and ROUGE-2 compute unigram and bigram overlaps between system summaries and reference summaries, while ROUGE-L computes the longest common sub-sequence of the two. In addition, we design a human evaluation experiment (details in Section 5.2) to compare our model with the best unsupervised summarization model PAC S UM (Zheng and Lapata, 2019). As far as we know, we are the first to perform human evaluation 3 https://github.com/huggingface/transformers https://github.com/mswellhao/PACSUM 5 https://github.com/UKPLab/sentence-transformers 6 http://bio.nlplab.org/word-vectors 4 Lead Oracl"
2021.eacl-main.93,A97-1042,0,0.889764,"are semantically similar to other nodes are chosen to be included in the final summary. In other words, they determine node importance by defining a notion of centrality in the graph. In addition, we augment the document graph with directionality and hierarchy to reflect the rich discourse structure of long scientific documents. In particular, our method relies on two insights about the discourse structure of long scientific documents. The first is that important information typically occurs at the start and end of sections; i.e., they tend to appear near section boundaries (Baxendale, 1958; Lin and Hovy, 1997; Teufel, 1997). We implement this using an asymmetric edge weighting function in a directed graph which considers the distance of a sentence to a boundary. The second is that most sentences across section boundaries are unlikely to interact significantly with each other (Xiao and Carenini, 2019). We implement this insight by injecting hierarchies into our model, introducing section-level representations as graph nodes in addition to sentence nodes. By doing so, we convert a flat graph into a hierarchical non-fully-connected graph, which has two advantages: 1) reduced computational cost and 2)"
2021.eacl-main.93,P02-1058,0,0.155628,"nce comparable to many expensive state-of-the-art supervised neural models that are trained on hundreds of thousands of examples of long document pairs (Xiao and Carenini, 2019; Subramanian et al., 2019). This suggests that patterns in the discourse structure are highly useful for determining sentence importance in long scientific articles, and that explicitly building in biases inspired by this structure is a viable strategy for building summarization systems. 2 2.1 Related Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 201"
2021.eacl-main.93,D19-1387,0,0.120915,"play a key role in freezing of gait (fog), which is also related to attentional set-shifting. Future work s. future research should examine the link between anxiety, set-shifting, and fog, in order to determine whether treating anxiety might be a potential therapy for improving fog. Table 1: Example of a PubMed article’s summary produced by our model H IPO R ANK. The hierarchical and directed graph combined with discourse-aware edge weighting allow H IPO R ANK to generate summaries that cover topics from different sections of the scientific article. 2017; Dong et al., 2018; Zhou et al., 2018; Liu and Lapata, 2019; Narayan et al., 2018b; Zhang et al., 2019b). These models usually employ the encoderdecoder structure and have achieved promising performance on news datasets such as CNN/DailyMail (Hermann et al., 2015), and NYT (Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association fo"
2021.eacl-main.93,W04-3252,0,0.162192,"iases inspired by this structure is a viable strategy for building summarization systems. 2 2.1 Related Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and"
2021.eacl-main.93,D19-5407,0,0.0118374,"rayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer models also provide better sentence representations for unsupervised summarization methods. For instance, PAC S UM (Zheng and Lapata, 2019), a directed graph-based unsupervised model that utilizes"
2021.eacl-main.93,K16-1028,0,0.0235995,"r completeness. For unsupervised extractive summarization models, we compare with SumBasic (Vanderwende et al., 2007), LSA (Steinberger and Jezek, 2004), LexRank (Erkan and Radev, 2004) and PAC S UM (Zheng and Lapata, 2019). For supervised neural extractive summarization models, we compare with a vanilla encoder-decoder model (Cheng and Lapata, 2016), SummaRuNNer (Nallapati et al., 2017), GlobalLocalCont (Xiao and Carenini, 2019), SentCLF and Sent-PTR (Subramanian et al., 2019). We also compare with neural abstractive summarization models as reported in Xiao and Carenini (2019): Attn-Seq2Seq (Nallapati et al., 2016), PntrGen-Seq2Seq (See et al., 2017) and Discourseaware (Cohan et al., 2018). In addition, we report the lead baseline that selects the first k tokens as a summary (k = 203, = 220 for PubMed and arXiv respectively). Lastly, we report baselines for an Oracle summarizer (Nallapati et al., 2017). 4.4 Model 43.89 43.89 44.85 45.01 43.30 Unsupervised Extractive SumBasic (2007) LSA (2004) LexRank (2004) PAC S UM (2019) H IPO R ANK (ours) 37.15 33.89 39.19 39.79 43.58 11.36 9.93 13.89 14.00 17.00 33.43 29.70 34.59 36.09 39.31 Table 3: Test set results on PubMed (ROUGE F1). on the 2018 PubMed and arXi"
2021.eacl-main.93,D18-1206,0,0.100288,"ezing of gait (fog), which is also related to attentional set-shifting. Future work s. future research should examine the link between anxiety, set-shifting, and fog, in order to determine whether treating anxiety might be a potential therapy for improving fog. Table 1: Example of a PubMed article’s summary produced by our model H IPO R ANK. The hierarchical and directed graph combined with discourse-aware edge weighting allow H IPO R ANK to generate summaries that cover topics from different sections of the scientific article. 2017; Dong et al., 2018; Zhou et al., 2018; Liu and Lapata, 2019; Narayan et al., 2018b; Zhang et al., 2019b). These models usually employ the encoderdecoder structure and have achieved promising performance on news datasets such as CNN/DailyMail (Hermann et al., 2015), and NYT (Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association for Computational Lingui"
2021.eacl-main.93,N18-1158,0,0.0704391,"ezing of gait (fog), which is also related to attentional set-shifting. Future work s. future research should examine the link between anxiety, set-shifting, and fog, in order to determine whether treating anxiety might be a potential therapy for improving fog. Table 1: Example of a PubMed article’s summary produced by our model H IPO R ANK. The hierarchical and directed graph combined with discourse-aware edge weighting allow H IPO R ANK to generate summaries that cover topics from different sections of the scientific article. 2017; Dong et al., 2018; Zhou et al., 2018; Liu and Lapata, 2019; Narayan et al., 2018b; Zhang et al., 2019b). These models usually employ the encoderdecoder structure and have achieved promising performance on news datasets such as CNN/DailyMail (Hermann et al., 2015), and NYT (Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association for Computational Lingui"
2021.eacl-main.93,D15-1226,0,0.0260557,"els that are trained on hundreds of thousands of examples of long document pairs (Xiao and Carenini, 2019; Subramanian et al., 2019). This suggests that patterns in the discourse structure are highly useful for determining sentence importance in long scientific articles, and that explicitly building in biases inspired by this structure is a viable strategy for building summarization systems. 2 2.1 Related Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018) and XSum (Naraya"
2021.eacl-main.93,C08-1087,0,0.0541535,"arization systems employ traditional supervised machine learning algorithms with surface features as input (Xiao and Carenini, 2019). Surface features such as sentence position, sentence and document length, keyphrase score, and fine-grain rhetorical categories are often combined with Naive Bayes (Teufel and Moens, 2002), CRFs and SVMs (Liakata et al., 2013), LSTM and MLP (Collins et al., 2017) for extractive summarization over long scientific articles. To the best of our knowledge, the only unsupervised extractive summarization model for long scientific documents relies on citation networks (Qazvinian and Radev, 2008; Cohan and Goharian, 2015), by extracting citation-contexts from citing articles and ranking Figure 1: Example of a hierarchical document graph constructed by our approach on a toy document that contains two sections {T1 , T2 }, each containing three sentences for a total of six sentences {s1 , . . . , s6 }. Each double-headed arrow represents two edges with opposite directions. The solid and dashed arrows indicate intra-section and inter-section connections respectively. When compared to the flat fully-connected graph of traditional methods, our use of hierarchy effectively reduces the numbe"
2021.eacl-main.93,W00-0403,0,0.0679875,"ch achieves performance comparable to many expensive state-of-the-art supervised neural models that are trained on hundreds of thousands of examples of long document pairs (Xiao and Carenini, 2019; Subramanian et al., 2019). This suggests that patterns in the discourse structure are highly useful for determining sentence importance in long scientific articles, and that explicitly building in biases inspired by this structure is a viable strategy for building summarization systems. 2 2.1 Related Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail"
2021.eacl-main.93,D19-1410,0,0.0167903,"ser to the text boundary than j. α ∈ {0, 0.5, 0.8, 1.0, 1.2} controls the relative importance of the start or end of a section or document. µ1 ∈ {0.5, 1.0, 1.5} controls how much we weigh intra-section sentence importance vs. inter-section sectional importance. For each dataset, we experimented with different pretrained distributional sentence representation models. The dimension of sentence representations is model-dependent (details in Section 1093 6.2). We used the publicly released BERT model3 (Devlin et al., 2019), PAC S UM BERT model4 (Zheng and Lapata, 2019), SentBERT and SentRoBERTa5 (Reimers and Gurevych, 2019), and BioMed word2vec representations6 (Moen and Ananiadou, 2013). A section’s representation is calculated as the average of its sentences’ representations. The similarity between sentences or sections is defined to be the cosine similarity between the distributed representations. 4.3 Evaluation Methods We evaluate our method with automatic evaluation metrics - ROUGE F1 scores (Lin, 2004). ROUGE1 and ROUGE-2 compute unigram and bigram overlaps between system summaries and reference summaries, while ROUGE-L computes the longest common sub-sequence of the two. In addition, we design a human eva"
2021.eacl-main.93,P17-1099,0,0.443298,"ang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer m"
2021.eacl-main.93,P18-1062,0,0.0482558,"Missing"
2021.eacl-main.93,D17-1235,0,0.026547,"article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association for Computational Linguistics two significant limitations. First, they require large domain-specific training pairs of source documents and gold-standard summaries, which are often not available or feasible to create (Zheng and Lapata, 2019). Second, the typical setup of using a tokenlevel encoder-decoder with an attention mechanism does not scale well to longer documents (Shao et al., 2017), as the number of attention computations is quadratic with respect to the number of tokens in the input document. We instead explore unsupervised approaches to address these challenges on long document summarization. We show that a simple unsupervised graph-based ranking model combined with proper sophisticated modelling of discourse information as an inductive bias can achieve unreasonable effectiveness in selecting important sentences from long scientific documents. For the choice of unsupervised graph-based ranking model, we follow the paradigm of LexRank (Erkan and Radev, 2004) and PAC S"
2021.eacl-main.93,2020.emnlp-main.748,0,0.0305313,"Missing"
2021.eacl-main.93,W97-0710,0,0.79653,"milar to other nodes are chosen to be included in the final summary. In other words, they determine node importance by defining a notion of centrality in the graph. In addition, we augment the document graph with directionality and hierarchy to reflect the rich discourse structure of long scientific documents. In particular, our method relies on two insights about the discourse structure of long scientific documents. The first is that important information typically occurs at the start and end of sections; i.e., they tend to appear near section boundaries (Baxendale, 1958; Lin and Hovy, 1997; Teufel, 1997). We implement this using an asymmetric edge weighting function in a directed graph which considers the distance of a sentence to a boundary. The second is that most sentences across section boundaries are unlikely to interact significantly with each other (Xiao and Carenini, 2019). We implement this insight by injecting hierarchies into our model, introducing section-level representations as graph nodes in addition to sentence nodes. By doing so, we convert a flat graph into a hierarchical non-fully-connected graph, which has two advantages: 1) reduced computational cost and 2) pruning of dis"
2021.eacl-main.93,J02-4002,0,0.238076,"ues. Subramanian et al. (2019) used the introduction section as a proxy for the whole document, while Xiao and Carenini (2019) divided articles into sections and used non-auto-regressive approaches to model global and local information. Besides neural approaches, most previous scientific article summarization systems employ traditional supervised machine learning algorithms with surface features as input (Xiao and Carenini, 2019). Surface features such as sentence position, sentence and document length, keyphrase score, and fine-grain rhetorical categories are often combined with Naive Bayes (Teufel and Moens, 2002), CRFs and SVMs (Liakata et al., 2013), LSTM and MLP (Collins et al., 2017) for extractive summarization over long scientific articles. To the best of our knowledge, the only unsupervised extractive summarization model for long scientific documents relies on citation networks (Qazvinian and Radev, 2008; Cohan and Goharian, 2015), by extracting citation-contexts from citing articles and ranking Figure 1: Example of a hierarchical document graph constructed by our approach on a toy document that contains two sections {T1 , T2 }, each containing three sentences for a total of six sentences {s1 ,"
2021.eacl-main.93,W17-4507,0,0.0210243,"ated Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sandhaus, 2008), Newsroom (Grusky et al., 2018) and XSum (Narayan et al., 2018a), along with advancements in deep neural-based architectures, modern supervised neural network-based methods that employ encoderdecoder framework have become increasingly popular. These models have been proposed with extractive strategies (Cheng and Lapata, 2016; Nallapati et al., 2017; Wu and Hu, 2018; Dong et al., 2018; Zhou et al., 2018; Narayan et al., 2018b); abstractive strategi"
2021.eacl-main.93,D08-1079,0,0.0531244,"ny expensive state-of-the-art supervised neural models that are trained on hundreds of thousands of examples of long document pairs (Xiao and Carenini, 2019; Subramanian et al., 2019). This suggests that patterns in the discourse structure are highly useful for determining sentence importance in long scientific articles, and that explicitly building in biases inspired by this structure is a viable strategy for building summarization systems. 2 2.1 Related Work Extractive Summarization Traditional extractive summarization methods are mostly unsupervised (Radev et al., 2000; Lin and Hovy, 2002; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017; Zheng and Lapata, 2019), utilizing a notion of sentence importance based on n-gram overlap with other sentences and frequency information (Nenkova and Vanderwende, 2005), relying on graph-based methods for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004), or performing keyword extraction combined with submodular maximization (Tixier et al., 2017; Shang et al., 2018). With the development of large-scale summarization datasets such as CNN/DailyMail (Hermann et al., 2015), NYT (Sa"
2021.eacl-main.93,D19-1298,0,0.109387,"iscourse-aware edge weighting allow H IPO R ANK to generate summaries that cover topics from different sections of the scientific article. 2017; Dong et al., 2018; Zhou et al., 2018; Liu and Lapata, 2019; Narayan et al., 2018b; Zhang et al., 2019b). These models usually employ the encoderdecoder structure and have achieved promising performance on news datasets such as CNN/DailyMail (Hermann et al., 2015), and NYT (Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association for Computational Linguistics two significant limitations. First, they require large domain-specific training pairs of source documents and gold-standard summaries, which are often not available or feasible to create (Zheng and Lapata, 2019). Second, the typical setup of using a tokenlevel encoder-decoder with an attention mechanism does not scale well to longer documents (Shao et al., 2017), as the number of attention"
2021.eacl-main.93,P19-1499,0,0.0591565,"ich is also related to attentional set-shifting. Future work s. future research should examine the link between anxiety, set-shifting, and fog, in order to determine whether treating anxiety might be a potential therapy for improving fog. Table 1: Example of a PubMed article’s summary produced by our model H IPO R ANK. The hierarchical and directed graph combined with discourse-aware edge weighting allow H IPO R ANK to generate summaries that cover topics from different sections of the scientific article. 2017; Dong et al., 2018; Zhou et al., 2018; Liu and Lapata, 2019; Narayan et al., 2018b; Zhang et al., 2019b). These models usually employ the encoderdecoder structure and have achieved promising performance on news datasets such as CNN/DailyMail (Hermann et al., 2015), and NYT (Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association for Computational Linguistics two significant"
2021.eacl-main.93,P19-1628,0,0.0972028,"Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021. ©2021 Association for Computational Linguistics two significant limitations. First, they require large domain-specific training pairs of source documents and gold-standard summaries, which are often not available or feasible to create (Zheng and Lapata, 2019). Second, the typical setup of using a tokenlevel encoder-decoder with an attention mechanism does not scale well to longer documents (Shao et al., 2017), as the number of attention computations is quadratic with respect to the number of tokens in the input document. We instead explore unsupervised approaches to address these challenges on long document summarization. We show that a simple unsupervised graph-based ranking model combined with proper sophisticated modelling of discourse information as an inductive bias can achieve unreasonable effectiveness in selecting important sentences from"
2021.eacl-main.93,P19-1100,0,0.0124634,"n et al., 2018b); abstractive strategies (See et al., 2017; Chen and Bansal, 1090 2018; Gehrmann et al., 2018; Dong et al., 2019; Zhang et al., 2019a; Lewis et al., 2019); and hybrid strategies (Hsu et al., 2018; Bae et al., 2019; Moroshko et al., 2019). More recently, extractive approaches leveraging transformer architectures (Vaswani et al., 2017) and their pretrained counterparts (Devlin et al., 2019; Lewis et al., 2019; Zhang et al., 2019a; Dong et al., 2019) have achieved state-of-the-art performances on the CNN/DailyMail news benchmark dataset (Zhang et al., 2019b; Liu and Lapata, 2019; Zhong et al., 2019). Furthermore, pretrained transformer models also provide better sentence representations for unsupervised summarization methods. For instance, PAC S UM (Zheng and Lapata, 2019), a directed graph-based unsupervised model that utilizes BERT-based sentence representations, achieved comparable performance to supervised models on the CNN/DailyMail and NYT datasets. 2.2 Extractive Summarization of Long Scientific Papers Despite the success of deep neural-based models on news summarization, these approaches typically face challenges when applied to long documents such as scientific articles. Further"
2021.eacl-main.93,P18-1061,0,0.0961053,"been suggested to play a key role in freezing of gait (fog), which is also related to attentional set-shifting. Future work s. future research should examine the link between anxiety, set-shifting, and fog, in order to determine whether treating anxiety might be a potential therapy for improving fog. Table 1: Example of a PubMed article’s summary produced by our model H IPO R ANK. The hierarchical and directed graph combined with discourse-aware edge weighting allow H IPO R ANK to generate summaries that cover topics from different sections of the scientific article. 2017; Dong et al., 2018; Zhou et al., 2018; Liu and Lapata, 2019; Narayan et al., 2018b; Zhang et al., 2019b). These models usually employ the encoderdecoder structure and have achieved promising performance on news datasets such as CNN/DailyMail (Hermann et al., 2015), and NYT (Sandhaus, 2008). However, these models cannot easily be adapted to out-of-domain data that have greater length and fewer training examples such as scientific article summarization (Xiao and Carenini, 2019) due to 1089 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1089–1102 April 19 - 23, 2021"
2021.findings-acl.107,P18-1027,0,0.0647649,"Missing"
2021.findings-acl.107,2020.emnlp-main.574,0,0.127507,"019). In sequence transduction tasks, these learned characteristics, embedded in attention, make pre-trained Transformers a powerful language model (Radford et al., 2019). A final task-specific step is typically required for adapting a task-agnostic language model to perform the desired task3 . However, these taskspecific characteristics might not sufficiently coincide with general characteristics even after finetuning. For example, task-specific characteristics embedded in attention patterns – such as word alignments for machine translation – are often noisy and imperfect for generalization (Kobayashi et al., 2020). We show that insufficient learning of taskspecific characteristics, reflected in sentence-level attention patterns4 often being out of focus, may be associated with neural text degeneration (§3). Based on this observation, we propose a simple attention modulation framework that can dynamically redistribute sentence-level attention weights by injecting task-specific priors in Transformer blocks for different downstream tasks (§4). Remarkably, in long-range narrative story generation, abductive reasoning generation and constrained commonsense text generation, both automatic and human evaluatio"
2021.findings-acl.107,2020.findings-emnlp.165,1,0.935655,"d with the insufficient change of sentence-level attention. In §4 and §6, we show that generation quality can be vastly improved by injecting the prior – attention should look at the prompt differently when generating different sentences – through our proposed attention modulation. Lack of commonsense reasoning vs. attention Text generated by neural language models is also observed to be lacking commonsense reasoning (Mao et al., 2019). We check whether this type of neural degeneration is associated with attention patterns. A benchmark dataset for generative commonsense reasoning – CommonGen (Lin et al., 2020) – is used as our test bed. CommonGen is designed for constrained commonsense reasoning: given a set of common concepts (e.g., use, tool, piece, metal); the task is to generate a coherent and plausible sentence covering all these concepts (e.g., &quot;a piece of metal is used for making tools&quot;). Covering the concepts in generation requires relational reasoning with background commonsense knowledge. 1263 covered uncovered agg. max attn. SD # 0.434 0.376 0.0040 0.0068 4515 1473 for three different tasks: narrative story generation, abductive reasoning generation, and constrained commonsense reasoning"
2021.findings-acl.107,W04-1013,0,0.04101,"e generation is less dull if more unique tokens are generated. For repetition, we directly measure sentence-level repetition: two generated sentences are repeated if their strings are the same. For relevancy, we measure the percentage of generated tokens that appear in the prompt. Besides, we perform a human evaluation, where three annotators are asked to rate the generations based on fluency, interestingness, newness, relevancy, and repetition. On αNLG, we score the generated explanation with respect to the reference using the following automatic metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). In addition, we ask annotators to compare the generated explanations without and with attention modulation. Human judges are asked to decide which system provides a more plausible explanation of the observations. On CommonGen, we report SPICE (Anderson et al., 2016) – a measure that evaluates semantic propositional content, in addtion to BLEU, 7 These search-based decoding algorithms do not resolve the poorly generated token-level probabilities. Figure 3: Human evaluation results on the next 1 to 5 sentences generated with"
2021.findings-acl.107,D19-1615,0,0.0187153,"es. The sentence-level attention changes are vastly lower on all prompt sentences when generating repeated consecutive sentences. Thus, sentence-level repetition may be correlated with the insufficient change of sentence-level attention. In §4 and §6, we show that generation quality can be vastly improved by injecting the prior – attention should look at the prompt differently when generating different sentences – through our proposed attention modulation. Lack of commonsense reasoning vs. attention Text generated by neural language models is also observed to be lacking commonsense reasoning (Mao et al., 2019). We check whether this type of neural degeneration is associated with attention patterns. A benchmark dataset for generative commonsense reasoning – CommonGen (Lin et al., 2020) – is used as our test bed. CommonGen is designed for constrained commonsense reasoning: given a set of common concepts (e.g., use, tool, piece, metal); the task is to generate a coherent and plausible sentence covering all these concepts (e.g., &quot;a piece of metal is used for making tools&quot;). Covering the concepts in generation requires relational reasoning with background commonsense knowledge. 1263 covered uncovered ag"
2021.findings-acl.107,N16-1098,0,0.0306713,"number of tokens generated in the whole test corpus, which measures the number of new unique tokens generated. rel. represent relevancy, which measures the percentage of tokens generated appears in the prompt. rep. measures the sentence-level repetition – whether two sentences generated are identical. ers.7 We present the results with non-stochastic decoding algorithms (i.e. greedy decoding and beam search), as generations based on them truly reflect the token-level probabilities predicted by the model (Holtzman et al., 2018). Datasets We use three different generation datasets – ROCStories (Mostafazadeh et al., 2016), αNLG (Bhagavatula et al., 2020), and CommonGen (Lin et al., 2020). For ROCStories, we used the 2017 version and split the data into 75/10/15 for train/val/test. Evaluation On ROCStories, we measure dullness, relevancy and repetition similar to Welleck et al. (2019). We report the number of unique tokens generated, where the generation is less dull if more unique tokens are generated. For repetition, we directly measure sentence-level repetition: two generated sentences are repeated if their strings are the same. For relevancy, we measure the percentage of generated tokens that appear in the"
2021.findings-acl.107,D19-1002,0,0.161151,"rchitecture of stacked decoder. As GPT2 follows a multi-layer and multi-headed setting, αi,j is specific to a layer l and head h, noted as l,h αi,j . We use the GPT2-L model that has 36 layers with 20 heads per layer (762M total parameters). 3 We briefly discuss how vanilla attention works, as well as Transformer architecture used in this paper. (2) Neural text degeneration vs. attention As researchers have sought to understand the internal mechanisms of Transformers, the attention patterns exhibited by these heads have drawn considerable study (Vig and Belinkov, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). We perform sentence-level attention analysis to explore whether aggregated attention patterns are associated with neural text degeneration. 3.1 Sentence-level attention We first define the sentence-to-sentence attention of a language model M with L layers and H heads. Given two sentences p and g such that p precedes g, 1262 l,h l,h the mean α ¯ g,p and max α ˆ g,p sentence-to-sentence attentions from g to p for layer l and head h are: |g |P |p| P l,h α ¯ g,p = l,h = α ˆ g,p i=1 j=1 l,h αi,j (gi , pj ) |g |· |p| max i∈{1,...,|g|} j∈{1,...,|p|} l,h αi,j (gi , pj ). (5) (6) The aggregated sente"
2021.findings-acl.107,P02-1040,0,0.110759,"ique tokens generated, where the generation is less dull if more unique tokens are generated. For repetition, we directly measure sentence-level repetition: two generated sentences are repeated if their strings are the same. For relevancy, we measure the percentage of generated tokens that appear in the prompt. Besides, we perform a human evaluation, where three annotators are asked to rate the generations based on fluency, interestingness, newness, relevancy, and repetition. On αNLG, we score the generated explanation with respect to the reference using the following automatic metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). In addition, we ask annotators to compare the generated explanations without and with attention modulation. Human judges are asked to decide which system provides a more plausible explanation of the observations. On CommonGen, we report SPICE (Anderson et al., 2016) – a measure that evaluates semantic propositional content, in addtion to BLEU, 7 These search-based decoding algorithms do not resolve the poorly generated token-level probabilities. Figure 3: Human evaluation results on the next 1 to 5 sente"
2021.findings-acl.107,W19-4808,0,0.368146,"et al., 2020). It 2 attention ratios are normalized mean sentence-to-sentence attention from generation H to observations O1 and O2 1261 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1261–1274 August 1–6, 2021. ©2021 Association for Computational Linguistics learns the general characteristics of language processing through pre-training on large amounts of unlabeled data. For example, multiple analyses have suggested that attention patterns in pre-trained Transformers implicitly encode syntactic information (Raganato and Tiedemann, 2018; Michel et al., 2019; Vig and Belinkov, 2019). In sequence transduction tasks, these learned characteristics, embedded in attention, make pre-trained Transformers a powerful language model (Radford et al., 2019). A final task-specific step is typically required for adapting a task-agnostic language model to perform the desired task3 . However, these taskspecific characteristics might not sufficiently coincide with general characteristics even after finetuning. For example, task-specific characteristics embedded in attention patterns – such as word alignments for machine translation – are often noisy and imperfect for generalization (Koba"
2021.findings-emnlp.312,P99-1071,0,0.612763,"ee et al., 2017; Nallapati et al., 2017) and RL-based approaches (Paulus et al., 2018; Dong et al., 2018). Recently, extractive summarization models that are based on fine-tuning pre-trained transformers have We conduct two studies to understand different aspects of the problem using two English datasets, also shown strong performance (Liu and Lapata, 2019; Zhong et al., 2020). CNN/DailyMail and PubMed. First, we examine how often expert judges prefer summaries modified Extractive summarizers are known to suffer from by such a system over the original version of gen- issues such as verbosity (Barzilay et al., 1999), erated extractive summaries. For the second study, coreference issues (Steinberger et al., 2016) (e.g., we carry out an annotation study to obtain gold selecting a sentence with an anaphor that refers to standard annotations on the definiteness of noun an entity in a non-selected previous sentence) and phrases in sampled subsets of extractive summaries breaks in the pragmatic context (Hutchins, 1987) that are generated by different summarizers for (e.g., a selected sentence containing a presupposiboth CNN/DailyMail and PubMed. By comparing tion that is linked to an event/proposition appearin"
2021.findings-emnlp.312,P16-1046,0,0.0320642,"an pragmatically reasoning about the contexts. Overall, we show that our findings generalize over multiple combinations of datasets and summarizers, thus demonstrating further the efficacy of our method. 2 2.1 Related Work Extractive Summarization There exists a long line of work on extractive summarization beginning as early as the mid-1950s. For a comprehensive review, the reader is referred to Nenkova and McKeown (2011). More recent approaches are based on neural networks including sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014). These consist of MLEbased approaches (Cheng and Lapata, 2016; See et al., 2017; Nallapati et al., 2017) and RL-based approaches (Paulus et al., 2018; Dong et al., 2018). Recently, extractive summarization models that are based on fine-tuning pre-trained transformers have We conduct two studies to understand different aspects of the problem using two English datasets, also shown strong performance (Liu and Lapata, 2019; Zhong et al., 2020). CNN/DailyMail and PubMed. First, we examine how often expert judges prefer summaries modified Extractive summarizers are known to suffer from by such a system over the original version of gen- issues such as verbosit"
2021.findings-emnlp.312,D14-1179,0,0.0127955,"Missing"
2021.findings-emnlp.312,N18-2097,0,0.0236683,"and the extended context. A sample in the local context configuration is defined to be the set of tokens from the previous head noun of a noun phrase up to and including the head noun of the current noun phrase. For example, take the following passage (head nouns indicated in bold): Example 1 The newly elected mayor plans to actively fight corruption plaguing the city. total number of tokens per sample is reached. Similar to (Kabbara et al., 2016), we set that number to be 50. 4 4.1 Experimental Setup Datasets In our work, we use two datasets: CNN/DailyMail (Hermann et al., 2015) and PubMed (Cohan et al., 2018). CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. The dataset is a collection of 93K articles from CNN and 220K articles from Daily Mail. Approximately 90k documents and 197k documents are used for training, respectively, in the CNN and Daily Mail portions of the dataset. The PubMed dataset consists of long and structured scientific papers obtained from the PubMed repository of biomedical research papers. The abstracts are considered to be the summaries of the articles. The dataset consists of 133K articles of wh"
2021.findings-emnlp.312,N19-1423,0,0.00587114,"y the authors.123 3.2 Definiteness Prediction For the second step of predicting the definiteness of NPs, we adopt the methodology of Kabbara et al. (2016) in which they present an LSTMbased (Hochreiter and Schmidhuber, 1997) learning model for definiteness prediction. The learning task is a three-way classification where the labels represent one of three classes: “the"", “a"" (or “an"") and “none"". In order to explore the suitability and performance of different learning models on this task, we explore the use of a logistic regression classifier (De Felice, 2008), an LSTM model and a BERT-based (Devlin et al., 2019) neural model which has shown strong performance across a wide range of NLP tasks (Rogers et al., 2020). 3.2.1 Model Description The first model is a logistic regression classifier which learns the probabilities describing the possible outcomes of an input using a logistic function. The LSTM model is first fed a sequence of (onehot encoded) input tokens representing the sample. The tokens are then embedded using pre-trained word representations. The resulting embedded vectors are encoded by a number of stacked LSTM recurrent layers. We explore in Section 7 the effect of having a unidirectional"
2021.findings-emnlp.312,2021.eacl-main.93,1,0.746404,"ediction model g generates ˜ which we refer to a modified summary S 0 = g(S) as pseudo-extractive summary. The goal is thus to compare the final output to the original summary to understand whether such a post-editing step improves the coherence and readability of extractive summaries. Figure 2 depicts the proposed pipeline. 3.1 Extractive Summarization In our work, we experiment with three different summarizers: MatchSum (Zhong et al., 2020) casts the extractive summarization task as a semantic text matching problem and is currently state-of-theart on both CNN/DailyMail and PubMed. HipoRank (Dong et al., 2021) is a recent unsupervised graph-based ranking model for extractive summarization of long scientific documents with competitive performance on PubMed. Since it’s tailored for long scientific documents, we use HipoRank for PubMed only. Finally, to have another set of results for CNNDM, we use BanditSum (Dong et al., 2018) an RL-based neural extractive summarizer with near-SOTA performance on CNNDM (better than HipoRank). To generate summaries, we use the source code made public by the authors.123 3.2 Definiteness Prediction For the second step of predicting the definiteness of NPs, we adopt the"
2021.findings-emnlp.312,D18-1409,1,0.884382,"Missing"
2021.findings-emnlp.312,I08-1059,0,0.0615155,"ctness and consistency, which are a known issue of existing abstractive systems (Cao et al., 2018; Goodrich et al., 2019; Kry´sci´nski et al., 2019). 2.2 Definiteness Prediction The question of definiteness has been extensively covered in the areas of linguistics and philosophy of language with early work that studies the nature and properties of definiteness dating back as early as (Russell, 1905) and (Strawson, 1950). In the computational linguistics literature, several models for definiteness prediction were proposed such as (Knight and Chander, 1994; Minnen et al., 2000; Han et al., 2006; Gamon et al., 2008). De Felice (2008) presented a logistic regression classifier extracting a number of linguistically motivated features from the context of each head noun. The most recent work (Kabbara et al., 2016) presents an attention-based RNN that achieves state of the art on definiteness prediction and investigates, among other factors, the effect of having a local or wider context. In our work, we adopt this model as the basis for the proposed post-editing step. 3 Proposed Post-Editor Method Source Document Pre-trained Summarizer Extractive Summary Definiteness Prediction Modified Summary Figure 2: Diag"
2021.findings-emnlp.312,C16-1247,1,0.898522,"finiteness has been extensively covered in the areas of linguistics and philosophy of language with early work that studies the nature and properties of definiteness dating back as early as (Russell, 1905) and (Strawson, 1950). In the computational linguistics literature, several models for definiteness prediction were proposed such as (Knight and Chander, 1994; Minnen et al., 2000; Han et al., 2006; Gamon et al., 2008). De Felice (2008) presented a logistic regression classifier extracting a number of linguistically motivated features from the context of each head noun. The most recent work (Kabbara et al., 2016) presents an attention-based RNN that achieves state of the art on definiteness prediction and investigates, among other factors, the effect of having a local or wider context. In our work, we adopt this model as the basis for the proposed post-editing step. 3 Proposed Post-Editor Method Source Document Pre-trained Summarizer Extractive Summary Definiteness Prediction Modified Summary Figure 2: Diagram depicting our proposed method. The learning task can be stated as follows: Given a document D = {s1 , . . . , sn } with n sentences, a pre-trained extractive summarizer, f , generates a summary"
2021.findings-emnlp.312,D19-1387,0,0.0141162,"ve review, the reader is referred to Nenkova and McKeown (2011). More recent approaches are based on neural networks including sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014). These consist of MLEbased approaches (Cheng and Lapata, 2016; See et al., 2017; Nallapati et al., 2017) and RL-based approaches (Paulus et al., 2018; Dong et al., 2018). Recently, extractive summarization models that are based on fine-tuning pre-trained transformers have We conduct two studies to understand different aspects of the problem using two English datasets, also shown strong performance (Liu and Lapata, 2019; Zhong et al., 2020). CNN/DailyMail and PubMed. First, we examine how often expert judges prefer summaries modified Extractive summarizers are known to suffer from by such a system over the original version of gen- issues such as verbosity (Barzilay et al., 1999), erated extractive summaries. For the second study, coreference issues (Steinberger et al., 2016) (e.g., we carry out an annotation study to obtain gold selecting a sentence with an anaphor that refers to standard annotations on the definiteness of noun an entity in a non-selected previous sentence) and phrases in sampled subsets of"
2021.findings-emnlp.312,P14-5010,0,0.00250454,"ollection of 93K articles from CNN and 220K articles from Daily Mail. Approximately 90k documents and 197k documents are used for training, respectively, in the CNN and Daily Mail portions of the dataset. The PubMed dataset consists of long and structured scientific papers obtained from the PubMed repository of biomedical research papers. The abstracts are considered to be the summaries of the articles. The dataset consists of 133K articles of which 120K are used for training. To obtain the data for the second step (definiteness prediction), we first parse each dataset using Stanford CoreNLP (Manning et al., 2014) and then extract all of the NPs present in the parsed dataset whose head noun’s POS tag is one of NN, NNS, NNP, or NNPS. We do not lemmatize and ignore case and punctuation. As mentioned before, we remove all instances of the relevant articles (the, a, an) from all of the datasets. The numbers of training samples are as follows: For CNNDM, 48M samples from the stories and 3.9M samples from the summaries. For PM, 69M samples from the articles and 6M from the summaries. 4.2 Training Details The logistic regression classifier is implemented using the scikit-learn library (Pedregosa et al., 2011)"
2021.findings-emnlp.312,W00-0708,0,0.420615,"ly to lead to problems of factual correctness and consistency, which are a known issue of existing abstractive systems (Cao et al., 2018; Goodrich et al., 2019; Kry´sci´nski et al., 2019). 2.2 Definiteness Prediction The question of definiteness has been extensively covered in the areas of linguistics and philosophy of language with early work that studies the nature and properties of definiteness dating back as early as (Russell, 1905) and (Strawson, 1950). In the computational linguistics literature, several models for definiteness prediction were proposed such as (Knight and Chander, 1994; Minnen et al., 2000; Han et al., 2006; Gamon et al., 2008). De Felice (2008) presented a logistic regression classifier extracting a number of linguistically motivated features from the context of each head noun. The most recent work (Kabbara et al., 2016) presents an attention-based RNN that achieves state of the art on definiteness prediction and investigates, among other factors, the effect of having a local or wider context. In our work, we adopt this model as the basis for the proposed post-editing step. 3 Proposed Post-Editor Method Source Document Pre-trained Summarizer Extractive Summary Definiteness Pre"
2021.findings-emnlp.312,D14-1162,0,0.0881043,"tories and 3.9M samples from the summaries. For PM, 69M samples from the articles and 6M from the summaries. 4.2 Training Details The logistic regression classifier is implemented using the scikit-learn library (Pedregosa et al., 2011) Noting that all instances of the articles in ques- with all the corresponding default parameters. For tion (the, a/an) are removed from all the data the LSTM model, we use a vocabulary of size (training/validation/testing), the following samples 30,000 and we initialize the word embeddings with –relying on local context– are shown, with their la- GloVe vectors (Pennington et al., 2014) having bels: newly elected mayor – ‘the’, plans to actively 300-dimensions and trained on the 840 billion tofight corruption – ‘none’, plaguing city – ‘the’. ken version of the Common-Crawl corpus. For the Since Kabbara et al. (2016) provide evidence that LSTM model, unknown words are randomly inian extended context leads to a better performance tialized according to a normal distribution to the on their task of definiteness prediction, we explore same size as the GloVe embeddings. For BERT, we using the extended context which constructs the use the bert-base-uncased implementation by Hugsamp"
2021.findings-emnlp.312,2020.tacl-1.54,0,0.0169576,", we adopt the methodology of Kabbara et al. (2016) in which they present an LSTMbased (Hochreiter and Schmidhuber, 1997) learning model for definiteness prediction. The learning task is a three-way classification where the labels represent one of three classes: “the"", “a"" (or “an"") and “none"". In order to explore the suitability and performance of different learning models on this task, we explore the use of a logistic regression classifier (De Felice, 2008), an LSTM model and a BERT-based (Devlin et al., 2019) neural model which has shown strong performance across a wide range of NLP tasks (Rogers et al., 2020). 3.2.1 Model Description The first model is a logistic regression classifier which learns the probabilities describing the possible outcomes of an input using a logistic function. The LSTM model is first fed a sequence of (onehot encoded) input tokens representing the sample. The tokens are then embedded using pre-trained word representations. The resulting embedded vectors are encoded by a number of stacked LSTM recurrent layers. We explore in Section 7 the effect of having a unidirectional or bidirectional recurrent layer. The last hidden state is then fed to a linear layer followed by a so"
2021.findings-emnlp.312,P17-1099,0,0.0319541,"ng about the contexts. Overall, we show that our findings generalize over multiple combinations of datasets and summarizers, thus demonstrating further the efficacy of our method. 2 2.1 Related Work Extractive Summarization There exists a long line of work on extractive summarization beginning as early as the mid-1950s. For a comprehensive review, the reader is referred to Nenkova and McKeown (2011). More recent approaches are based on neural networks including sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014). These consist of MLEbased approaches (Cheng and Lapata, 2016; See et al., 2017; Nallapati et al., 2017) and RL-based approaches (Paulus et al., 2018; Dong et al., 2018). Recently, extractive summarization models that are based on fine-tuning pre-trained transformers have We conduct two studies to understand different aspects of the problem using two English datasets, also shown strong performance (Liu and Lapata, 2019; Zhong et al., 2020). CNN/DailyMail and PubMed. First, we examine how often expert judges prefer summaries modified Extractive summarizers are known to suffer from by such a system over the original version of gen- issues such as verbosity (Barzilay et al."
2021.findings-emnlp.312,P11-5003,0,0.154243,"Missing"
2021.findings-emnlp.312,2020.emnlp-demos.6,0,0.0180357,"Missing"
2021.findings-emnlp.312,2020.acl-main.552,0,0.0442543,"is referred to Nenkova and McKeown (2011). More recent approaches are based on neural networks including sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014). These consist of MLEbased approaches (Cheng and Lapata, 2016; See et al., 2017; Nallapati et al., 2017) and RL-based approaches (Paulus et al., 2018; Dong et al., 2018). Recently, extractive summarization models that are based on fine-tuning pre-trained transformers have We conduct two studies to understand different aspects of the problem using two English datasets, also shown strong performance (Liu and Lapata, 2019; Zhong et al., 2020). CNN/DailyMail and PubMed. First, we examine how often expert judges prefer summaries modified Extractive summarizers are known to suffer from by such a system over the original version of gen- issues such as verbosity (Barzilay et al., 1999), erated extractive summaries. For the second study, coreference issues (Steinberger et al., 2016) (e.g., we carry out an annotation study to obtain gold selecting a sentence with an anaphor that refers to standard annotations on the definiteness of noun an entity in a non-selected previous sentence) and phrases in sampled subsets of extractive summaries"
2021.findings-emnlp.351,D19-1598,0,0.160212,"ng which differs from reality. Clearly, demonstrating 4162 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4162–4172 November 7–11, 2021. ©2021 Association for Computational Linguistics social intelligence is a major barrier that needs to be crossed in our march towards the applied endgoal of creating NLP systems that blend seamlessly into the human world (Weston et al., 2016; Bisk et al., 2020). A recent line of work evaluates the theory-ofmind capability of memory-augmented neural models via a question answering task (Grant et al., 2017; Nematzadeh et al., 2018; Le et al., 2019). This task consists of stories with multiple entities that interact with each other in a synthetic environment, followed by a question about the entities’ beliefs. Memory augmented neural models like EntNet (Henaff et al., 2017), that are successful at solving reasoning tasks such as bAbi (Weston et al., 2016) by tracking world states, perform poorly at false belief tasks where the mental states of the entities do not match the world states. Moreover, these models are sensitive to distracting sentences, which decrease their accuracy. In this paper we propose a new model, that we call Textual"
2021.findings-emnlp.351,P19-1334,0,0.0232716,"d-suitcase containing the eggplant is in the office or the porch. Resolving its location requires the observation that if Jacob moved the eggplant, then its original container, i.e. the redsuitcase, should be at the same location as Jacob. This example, and others like it, are not exclusively testing the ToM capabilities of the model, as they require the model to understand spatial relationships, perform pragmatic reasoning and show common sense. 8 Conclusion could be more difficult. For example, issues such as recency or lexical overlap might result in Clever Hans phenomena as shown for NLI (McCoy et al., 2019). Demonstrating that our approach works for ToMi is a first step towards building models with complete ToM capabilities. Theory of Mind is a complex problem and we believe that we can make progress by gradually increasing the complexities of the ToM tasks in a controlled setting. This work is a part of a bottom-up process for solving ToM. To this end, our approach adds the missing piece of incorporating mental-state tracking along the time axis. With this prerequisite met and barrier crossed, we can move towards tackling other challenges in ToM in the future. Acknowledgements This work is supp"
2021.findings-emnlp.351,D18-1261,0,0.30254,"query by correctly deducing which differs from reality. Clearly, demonstrating 4162 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4162–4172 November 7–11, 2021. ©2021 Association for Computational Linguistics social intelligence is a major barrier that needs to be crossed in our march towards the applied endgoal of creating NLP systems that blend seamlessly into the human world (Weston et al., 2016; Bisk et al., 2020). A recent line of work evaluates the theory-ofmind capability of memory-augmented neural models via a question answering task (Grant et al., 2017; Nematzadeh et al., 2018; Le et al., 2019). This task consists of stories with multiple entities that interact with each other in a synthetic environment, followed by a question about the entities’ beliefs. Memory augmented neural models like EntNet (Henaff et al., 2017), that are successful at solving reasoning tasks such as bAbi (Weston et al., 2016) by tracking world states, perform poorly at false belief tasks where the mental states of the entities do not match the world states. Moreover, these models are sensitive to distracting sentences, which decrease their accuracy. In this paper we propose a new model, tha"
2021.findings-emnlp.351,D14-1162,0,0.086648,"t using the output module. 1. 2. 3. 4. 5. Jacob entered the patio. Jayden entered the patio. Isabella entered the patio. The pumpkin is in the red-box. Jayden moved the pumpkin to the red-container. 6. Jacob exited the patio. 7. What is the location of Jacob? U NKNOWN 8. What is the location of Jayden? M AIN L OCATION(patio) 9. What is the location of Isabella? M AIN L OCATION(patio) Table 3: The training data is updated with questions about the locations of the entities. Here, after Jacob exits the patio, we are not aware of his location, so we label it as U NKNOWN. 5 Experiments with GloVe (Pennington et al., 2014) pre-trained word embeddings did not improve performance. We use the Adam optimizer (Kingma and Ba, 2015) with a batchsize of 32, and clip the gradients at 40. We start with an initial learning rate of 0.01 and we halve the learning rate after every 25 epochs. We train the models on the corrected ToMi dataset for 200 epochs. 5.1 Evaluation measures and baseline We evaluate our model using the accuracy score. We report accuracy based on the belief of the entities in the question (either true belief or false belief) and on each question category; namely, memory, reality, first-order and second-o"
2021.findings-emnlp.359,N18-1202,0,0.00747209,"able on https:// malikaltakrori.github.io/ 2017, 2018) proposed a character- and word4243 level n-grams approach motivated by text distortion (Granados et al., 2012) for topic classification. In contrast to (Granados et al., 2012), Stamatatos kept the most frequent words and masked the rest of the text. Barlas and Stamatatos (2020, 2021) explored the widely used and massively pretrained transformer-based (Vaswani et al., 2017) language models for authorship attribution. Specifically, they trained a separate language model for each candidate author with a pretrained embeddings layer from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and ULMFit (Howard and Ruder, 2018). Each model was presented with words from the investigated document, and the most plausible author for that document is the one whose model has the lowest average perplexity. distribution is independent from that of the author. P (A, T, D) = P (A)P (T )P (D|A, T ) P (A = a|D) ∝ T X (1) [P (A = a)P (T = t) t P (D|T = t, A = a)] (2) During the attribution process, the model is used to predict an author given an anonymous document using Eq. 2, which follows from Eq. 1 after applying Bayes rule. The same"
2021.findings-emnlp.359,N15-1010,0,0.0670302,"Missing"
2021.findings-emnlp.359,P14-5010,0,0.00480996,"Missing"
2021.findings-emnlp.359,C14-1116,0,0.346069,"n goal is to prevent any external factors, such as the dataset imbalance, from affecting the attribution results. 2 Related Work The first work that used a computational approach is (Mosteller and Wallace, 1963), which used the Naïve Bayes algorithm with the frequency of function words to identify the authors of the Federalist papers (Juola, 2008). Research efforts have aimed at finding new sets of features for current domains/languages, adapting existing features to new languages or media, or using new classification techniques (Frantzeskou et al., 2007; Iqbal et al., 2013; Stamatatos, 2013; Sapkota et al., 2014, 2015; Ding et al., 2015; Altakrori et al., 2018). Recent attempts have been made to investigate • We propose topic confusion as a new evalua- authorship attribution in realistic scenarios, and tion setting in authorship attribution and use many studies have emerged where the constraints it to measure the effectiveness of features in differ from the training to the testing samples such the attribution process. as (Bogdanova and Lazaridou, 2014) on crosslanguage, (Goldstein-Stewart et al., 2009; Custó• Our evaluation shows that word-level n-grams dio and Paraboni, 2019) on cross-domain/genre,"
2021.findings-emnlp.359,E17-1107,0,0.333307,"sure the effectiveness of features in differ from the training to the testing samples such the attribution process. as (Bogdanova and Lazaridou, 2014) on crosslanguage, (Goldstein-Stewart et al., 2009; Custó• Our evaluation shows that word-level n-grams dio and Paraboni, 2019) on cross-domain/genre, can easily outperform pretrained embeddings and finally, (Sundararajan and Woodard, 2018; Stafrom BERT and RoBERTa models when used matatos, 2017, 2018; Barlas and Stamatatos, 2020, as features for cross-topic authorship attribu- 2021) on cross-topic. tion. The results also show that a combination Stamatatos (2017, 2018); Barlas and Stamatatos of n-grams on the part-of-speech (POS) tags (2020, 2021) achieved state-of-the-art results on 1 cross-topic authorship attribution. (Stamatatos, The code will be made available on https:// malikaltakrori.github.io/ 2017, 2018) proposed a character- and word4243 level n-grams approach motivated by text distortion (Granados et al., 2012) for topic classification. In contrast to (Granados et al., 2012), Stamatatos kept the most frequent words and masked the rest of the text. Barlas and Stamatatos (2020, 2021) explored the widely used and massively pretrained transfo"
2021.findings-emnlp.359,C18-1238,0,0.0347023,"Missing"
2021.findings-emnlp.359,2021.eacl-main.97,0,0.0617171,"Missing"
2021.findings-emnlp.359,2020.emnlp-demos.6,0,0.0225222,"Missing"
2021.findings-emnlp.359,D18-1294,0,0.01181,"Studies McGill University / Mila McGill University / Mila McGill University / Mila ben.fung@ jcheung@cs. malik.altakrori@mail. mcgill.ca mcgill.ca mcgill.ca Abstract tures for current domains/languages, adapting existing features to new languages or communication Authorship attribution is the problem of identidomains, or using new classification techniques, fying the most plausible author of an anonye.g. (Abbasi and Chen, 2006; Stamatatos, 2013; mous text from a set of candidate authors. Silva et al., 2011; Layton et al., 2012; Iqbal et al., Researchers have investigated same-topic and 2013; Zhang et al., 2018; Altakrori et al., 2018; Barcross-topic scenarios of authorship attribution, las and Stamatatos, 2020). Alternatively, motivated which differ according to whether new, unseen by the real-life applications of authorship attributopics are used in the testing phase. However, neither scenario allows us to explain whether tion different elements of and constraints on the errors are caused by a failure to capture auattribution process have been investigated (Houvarthorship writing style or by a topic shift. Modas and Stamatatos, 2006; Luyckx and Daelemans, tivated by this, we propose the topic conf"
2021.naacl-main.138,J02-2003,0,0.0388938,"n the context of defeasible reasoning (Rudinger et al., 2020): inferences that may be strengthened or weakened given additional evidence. The change in plausibility between an event and its abstraction can be formulated as a type of defeasible inference, and our findings may contribute to future work in this area. 2.1 plausible events (Wang et al., 2018). Closely related to our work are models of selectional preference that use the WordNet hierarchy to generalize co-occurrence probabilities over concepts. These include the work of Resnik (1993), related WordNet-based models (Li and Abe, 1998; Clark and Weir, 2002), and a more recent experiment by Ó Séaghdha and Korhonen (2012) to combine distributional models with WordNet. Notably, these methods make a discrete decision as to the right level of abstraction—if the most preferred subject of “breathe” is found to be “person,” for example, then all hyponyms of “person” will be assigned the same selectional preference score. 2.2 Conceptual Abstraction Our second proposed method can be thought of as finding the right level of abstraction at which to infer plausibility. This problem has been broadly explored by existing work. Van Durme et al. (2009) extract a"
2021.naacl-main.138,C90-3063,0,0.226767,"istinct from the (3) The thought breathes the car. likelihood of an event occurring in language. Third, This ability is required for understanding nat- plausibility reflects human intuition, and thus modeling plausibility at its extreme requires “the entire ural language: specifically, modeling selectional preference—the semantic plausibility of predicate- representational arsenal that people use in understanding language, ranging from social mores to argument structures—is known to be implicit in naive physics” (Resnik, 1996). discriminative tasks such as coreference resolution (Hobbs, 1978; Dagan and Itai, 1990; Zhang et al., A key property of plausibility is that the plau2019b), word sense disambiguation (Resnik, 1997; sibility of an event is generally consistent across McCarthy and Carroll, 2003), textual entailment some appropriate level of abstraction. For exam(Zanzotto et al., 2006; Pantel et al., 2007), and se- ple, events of the conceptual form “the [PERSON] mantic role labeling (Gildea and Jurafsky, 2002; breathes the [GAS]” are consistently plausible. Zapirain et al., 2013). Plausibility judgments follow this pattern because More broadly, modeling semantic plausibility people understand tha"
2021.naacl-main.138,P17-1191,0,0.0158984,"assed through a sigmoid to obtain the final output, f (e). We use the HuggingFace Transformers library PyTorch implementation of RoBERTa-base with 16-bit floating point precision (Wolf et al., 2020). 4.2 C ONCEPT I NJECT C ONCEPT I NJECT is an extension of the existing state-of-the-art plausibility models. This model takes as input, in addition to an event, the hypernym chains of the synsets corresponding to each argument in the event. We propose this model to explore how injecting simple awareness of a lexical hierarchy affects estimates. C ONCEPT I NJECT is similar in principle to OntoLSTM (Dasigi et al., 2017), which provides the entire hypernym chains of nouns as input to an LSTM for selectional preference, and also similar to K-BERT (Liu et al., 2020), which injects knowledge into BERT during fine-tuning by including relations as additional tokens in the input. K-BERT has demonstrated improved performance over Chinese BERT on several NLP tasks. The model extends our vanilla RoBERTa baseline (§4.1). We add an additional token embedding 2 Technically, RoBERTa’s [CLS] and [SEP] tokens are <s> and </s>. to RoBERTa for each synset c ∈ C. We initialize the embedding of c as the average embedding of the"
2021.naacl-main.138,J10-4007,0,0.0106541,"has high coverage of all nouns occurring in said position. Conceptual abstractions are captured to some extent in pre-trained language models’ representations (Ravichander et al., 2020; Weir et al., 2020). Selectional Preference 3 Problem Formulation Modeling the plausibility of single events is also studied in the context of selectional preference— Given a vocabulary of subjects S, verbs V, and the semantic preference of a predicate for taking objects O, let an event be represented by the s-v-o an argument as a particular dependency relation triple e ∈ S × V × O. (Evens, 1975; Resnik, 1993; Erk et al., 2010); e.g., We take g to be a ground-truth, total orderthe relative preference of the verb “breathe” for the ing of events expressed by the ordering function noun “dentist” as its nominal subject. g(e) > g(e0 ) iff e is more plausible than e0 . Our objective is to learn a model f : S × V × O → R Models of selectional preference are sometimes that is monotonic with respect to g, i.e., g(e) > evaluated by correlation with human judgements g(e0 ) =⇒ f (e) > f (e0 ). (Ó Séaghdha, 2010; Zhang et al., 2019a). The primary distinction between such evaluations and This simplification follows from previous"
2021.naacl-main.138,2020.blackboxnlp-1.16,0,0.0274815,"event plausibility (Zhang et al., 2020a). Our fine-tuned RoBERTa baseline follows this approach. Similar in spirit to our work, He et al. (2020) extend this baseline method by creating additional training data using the Probase taxonomy (Wu et al., 2012) in order to improve conceptual generalization; specifically, for each training example they swap the event’s arguments with its hypernym or hyponym, and they take this new, perturbed example to be an implausible event. There is also recent work focusing on monotonic inferences in semantic entailment (Yanaka et al., 2019; Goodwin et al., 2020; Geiger et al., 2020). Plausibility contrasts with entailment in that plausibility is not strictly monotonic with respect to hypernymy/hyponymy relations: the plausibility of an entity is not sufficient to infer the plausibility of its hyponyms (i.e., not downward entailing: it is plausible that a person gives birth but not that a man gives birth) nor hypernyms (i.e., not upward entailing: it is plausible that a baby fits inside a shoebox but not that a person does). Non-monotonic inferences have recently been explored in the context of defeasible reasoning (Rudinger et al., 2020): inferences that may be strengthe"
2021.naacl-main.138,J02-3001,0,0.612332,"nderstanding language, ranging from social mores to argument structures—is known to be implicit in naive physics” (Resnik, 1996). discriminative tasks such as coreference resolution (Hobbs, 1978; Dagan and Itai, 1990; Zhang et al., A key property of plausibility is that the plau2019b), word sense disambiguation (Resnik, 1997; sibility of an event is generally consistent across McCarthy and Carroll, 2003), textual entailment some appropriate level of abstraction. For exam(Zanzotto et al., 2006; Pantel et al., 2007), and se- ple, events of the conceptual form “the [PERSON] mantic role labeling (Gildea and Jurafsky, 2002; breathes the [GAS]” are consistently plausible. Zapirain et al., 2013). Plausibility judgments follow this pattern because More broadly, modeling semantic plausibility people understand that similar concept classes is a necessary component of generative inferences share similar affordances. 1732 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1732–1743 June 6–11, 2021. ©2021 Association for Computational Linguistics Furthermore, the change in plausibility between levels of abstraction is oft"
2021.naacl-main.138,2020.acl-main.177,0,0.0479917,"Missing"
2021.naacl-main.138,N18-2017,0,0.0489871,"Missing"
2021.naacl-main.138,P19-1478,0,0.0649354,"Missing"
2021.naacl-main.138,N15-1098,0,0.0310057,"event across all contexts 1734 and realizations. While meaning is sensitive to small linguistic perturbations, we are interested in cases where one event is more plausible than another marginalized over context. Consider that person-breathe-air is more plausible than thought-breathe-car regardless of the choice of determiners or tense of the verb. In practice, we would like to learn f without supervised training data, as collecting a sufficiently large dataset of human judgements is prohibitively expensive (Zhang et al., 2020b), and supervised models often learn dataset-specific correlations (Levy et al., 2015; Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019). Therefore, we train model f with distant supervision and evaluate by correlation with human ratings of plausibility which represent the ground-truth ordering g. intuition is that plausibility increases as we approach the most appropriate level of abstraction, then decreases beyond this level. A concave sequence is defined to be a sequence (a1 , a2 , a3 , ...) where ∀i, 2ai > ai−1 + ai+1 . Let ai−1 , ai , and ai+1 be the plausibility estimates for three sequential abstractions of an event. We define the divergence from concavi"
2021.naacl-main.138,J98-2002,0,0.530227,"ly been explored in the context of defeasible reasoning (Rudinger et al., 2020): inferences that may be strengthened or weakened given additional evidence. The change in plausibility between an event and its abstraction can be formulated as a type of defeasible inference, and our findings may contribute to future work in this area. 2.1 plausible events (Wang et al., 2018). Closely related to our work are models of selectional preference that use the WordNet hierarchy to generalize co-occurrence probabilities over concepts. These include the work of Resnik (1993), related WordNet-based models (Li and Abe, 1998; Clark and Weir, 2002), and a more recent experiment by Ó Séaghdha and Korhonen (2012) to combine distributional models with WordNet. Notably, these methods make a discrete decision as to the right level of abstraction—if the most preferred subject of “breathe” is found to be “person,” for example, then all hyponyms of “person” will be assigned the same selectional preference score. 2.2 Conceptual Abstraction Our second proposed method can be thought of as finding the right level of abstraction at which to infer plausibility. This problem has been broadly explored by existing work. Van Durme"
2021.naacl-main.138,J03-4004,0,0.0920876,"ition, and thus modeling plausibility at its extreme requires “the entire ural language: specifically, modeling selectional preference—the semantic plausibility of predicate- representational arsenal that people use in understanding language, ranging from social mores to argument structures—is known to be implicit in naive physics” (Resnik, 1996). discriminative tasks such as coreference resolution (Hobbs, 1978; Dagan and Itai, 1990; Zhang et al., A key property of plausibility is that the plau2019b), word sense disambiguation (Resnik, 1997; sibility of an event is generally consistent across McCarthy and Carroll, 2003), textual entailment some appropriate level of abstraction. For exam(Zanzotto et al., 2006; Pantel et al., 2007), and se- ple, events of the conceptual form “the [PERSON] mantic role labeling (Gildea and Jurafsky, 2002; breathes the [GAS]” are consistently plausible. Zapirain et al., 2013). Plausibility judgments follow this pattern because More broadly, modeling semantic plausibility people understand that similar concept classes is a necessary component of generative inferences share similar affordances. 1732 Proceedings of the 2021 Conference of the North American Chapter of the Association"
2021.naacl-main.138,P19-1334,0,0.0220927,"sensitive to small linguistic perturbations, we are interested in cases where one event is more plausible than another marginalized over context. Consider that person-breathe-air is more plausible than thought-breathe-car regardless of the choice of determiners or tense of the verb. In practice, we would like to learn f without supervised training data, as collecting a sufficiently large dataset of human judgements is prohibitively expensive (Zhang et al., 2020b), and supervised models often learn dataset-specific correlations (Levy et al., 2015; Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019). Therefore, we train model f with distant supervision and evaluate by correlation with human ratings of plausibility which represent the ground-truth ordering g. intuition is that plausibility increases as we approach the most appropriate level of abstraction, then decreases beyond this level. A concave sequence is defined to be a sequence (a1 , a2 , a3 , ...) where ∀i, 2ai > ai−1 + ai+1 . Let ai−1 , ai , and ai+1 be the plausibility estimates for three sequential abstractions of an event. We define the divergence from concavity to be ( 1 (ai−1 + ai+1 ) − ai 2ai < ai−1 + ai+1 δ= 2 0 otherwise"
2021.naacl-main.138,P10-1045,0,0.103551,"Missing"
2021.naacl-main.138,S12-1025,0,0.0249683,"may be strengthened or weakened given additional evidence. The change in plausibility between an event and its abstraction can be formulated as a type of defeasible inference, and our findings may contribute to future work in this area. 2.1 plausible events (Wang et al., 2018). Closely related to our work are models of selectional preference that use the WordNet hierarchy to generalize co-occurrence probabilities over concepts. These include the work of Resnik (1993), related WordNet-based models (Li and Abe, 1998; Clark and Weir, 2002), and a more recent experiment by Ó Séaghdha and Korhonen (2012) to combine distributional models with WordNet. Notably, these methods make a discrete decision as to the right level of abstraction—if the most preferred subject of “breathe” is found to be “person,” for example, then all hyponyms of “person” will be assigned the same selectional preference score. 2.2 Conceptual Abstraction Our second proposed method can be thought of as finding the right level of abstraction at which to infer plausibility. This problem has been broadly explored by existing work. Van Durme et al. (2009) extract abstracted commonsense knowledge from text using WordNet, obtaini"
2021.naacl-main.138,N07-1071,0,0.0693492,"onal preference—the semantic plausibility of predicate- representational arsenal that people use in understanding language, ranging from social mores to argument structures—is known to be implicit in naive physics” (Resnik, 1996). discriminative tasks such as coreference resolution (Hobbs, 1978; Dagan and Itai, 1990; Zhang et al., A key property of plausibility is that the plau2019b), word sense disambiguation (Resnik, 1997; sibility of an event is generally consistent across McCarthy and Carroll, 2003), textual entailment some appropriate level of abstraction. For exam(Zanzotto et al., 2006; Pantel et al., 2007), and se- ple, events of the conceptual form “the [PERSON] mantic role labeling (Gildea and Jurafsky, 2002; breathes the [GAS]” are consistently plausible. Zapirain et al., 2013). Plausibility judgments follow this pattern because More broadly, modeling semantic plausibility people understand that similar concept classes is a necessary component of generative inferences share similar affordances. 1732 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1732–1743 June 6–11, 2021. ©2021 Association"
2021.naacl-main.138,D14-1162,0,0.08827,"o|v). In the generalizations (Van Durme et al., 2009). We also case that a noun corresponds to multiple tokens, we filter out synsets whose corresponding lemma did mask all tokens and take the probability of the noun not appear in the training corpus. to be the geometric mean of its token probabilities. The WordNet models also require sense disambiguation. We use the raw triple as input to BERTGloVe+MLP The selectional preference model WSD (Yap et al., 2020) which outputs a probability of Van de Cruys (2014) initialized with GloVe emdistribution over senses. We take the argmax to be beddings (Pennington et al., 2014). the correct sense. We train all models with gradient descent using n-gram A simple baseline that estimates an Adam optimizer, a learning rate of 2e-5, and a P (s, o|v) by occurrence counts. We use a bigram batch size of 128. We train for two epochs over model as we found trigrams to correlate less with the entire training set of examples with a linear human judgments. warm-up of the learning rate over the first 10,000 Count(s, v) · Count(v, o) iterations. Fine-tuning RoBERTa takes five hours P (s, o|v) ≈ (1) Count(v)2 on a single Nvidia V100 32GB GPU. Fine-tuning C ONCEPT I NJECT takes 12 ho"
2021.naacl-main.138,S18-2023,0,0.056136,"Missing"
2021.naacl-main.138,D19-6015,1,0.841804,"arm-up of the learning rate over the first 10,000 Count(s, v) · Count(v, o) iterations. Fine-tuning RoBERTa takes five hours P (s, o|v) ≈ (1) Count(v)2 on a single Nvidia V100 32GB GPU. Fine-tuning C ONCEPT I NJECT takes 12 hours and C ONCEPT4 https://pypi.org/project/mlconjug/ M AX 24 hours. 1737 5.1 Training Data We use English Wikipedia to construct the selfsupervised training data. As a relatively clean, definitional corpus, plausibility models trained on Wikipedia have been shown to correlate with human judgements better than those trained on similarly sized corpora (Zhang et al., 2019a; Porada et al., 2019). We parse a dump of English Wikipedia using the Stanford neural dependency parser (Qi et al., 2018). For each sentence with a direct object, no indirect object, and noun arguments (that are not proper nouns), we extract a training example (s, v, o): we take s and o to be the lemma of the head of the respective relations (nsubj and obj), and v to be the lemma of the head of the root verb. This results in some false positives such as the sentence “The woman eats a hot dog.” being extracted to the triple woman-eat-dog (Table 1). We filter out triples that occur less than once and those where a w"
2021.naacl-main.138,K18-2016,0,0.157279,"RTa takes five hours P (s, o|v) ≈ (1) Count(v)2 on a single Nvidia V100 32GB GPU. Fine-tuning C ONCEPT I NJECT takes 12 hours and C ONCEPT4 https://pypi.org/project/mlconjug/ M AX 24 hours. 1737 5.1 Training Data We use English Wikipedia to construct the selfsupervised training data. As a relatively clean, definitional corpus, plausibility models trained on Wikipedia have been shown to correlate with human judgements better than those trained on similarly sized corpora (Zhang et al., 2019a; Porada et al., 2019). We parse a dump of English Wikipedia using the Stanford neural dependency parser (Qi et al., 2018). For each sentence with a direct object, no indirect object, and noun arguments (that are not proper nouns), we extract a training example (s, v, o): we take s and o to be the lemma of the head of the respective relations (nsubj and obj), and v to be the lemma of the head of the root verb. This results in some false positives such as the sentence “The woman eats a hot dog.” being extracted to the triple woman-eat-dog (Table 1). We filter out triples that occur less than once and those where a word occurred less than 1,000 times in its respective position. We do not extract the same triple mor"
2021.naacl-main.138,2020.starsem-1.10,1,0.77037,"nces such as “A [PERSON] can breathe.” They achieve this by first extracting factoids and then greedily taking the WordNet synset that dominates the occurrences of factoids to be the appropriate abstraction. Gong et al. (2016) similarly abstract a verb’s arguments into a set of prototypical concepts using Probase and a branch-and-bound algorithm. For a given verb and argument position, their algorithm finds a small set of concepts that has high coverage of all nouns occurring in said position. Conceptual abstractions are captured to some extent in pre-trained language models’ representations (Ravichander et al., 2020; Weir et al., 2020). Selectional Preference 3 Problem Formulation Modeling the plausibility of single events is also studied in the context of selectional preference— Given a vocabulary of subjects S, verbs V, and the semantic preference of a predicate for taking objects O, let an event be represented by the s-v-o an argument as a particular dependency relation triple e ∈ S × V × O. (Evens, 1975; Resnik, 1993; Erk et al., 2010); e.g., We take g to be a ground-truth, total orderthe relative preference of the verb “breathe” for the ing of events expressed by the ordering function noun “dentist”"
2021.naacl-main.138,2020.findings-emnlp.418,0,0.0390651,"Missing"
2021.naacl-main.138,2020.coling-main.605,0,0.0409022,"3). We therefore evaluate models by their ability to estimate the relative plausibility of events. • Plausibility describes non-surprisal conditioned on some context (Resnik, 1993; Gordon et al., 2011). For example, conditioned on the event “breathing,” it is less surprising to learn that the agent is “a dentist” than “a thought” and thus more plausible. • Plausibility is dictated by likelihood of occurrence in the world rather than text (Zhang et al., 2017; Wang et al., 2018). This discrepancy is due to reporting bias—the fact that people do not state the obvious (Gordon and Van Durme, 2013; Shwartz and Choi, 2020); e.g., “a person dying” is more likely to be attested than “a person breathing” (Figure 2). events plausible in the world attested events Figure 2: An attested event is necessarily plausible in the world, but not all plausible events are attested. By the world we refer to some possible world under consideration—in this sense plausibility is an epistemic modality. 1 Our implementation and data is available at https://github.com/ianporada/modeling_ event_plausibility Wang et al. (2018) present the problem formulation that we use in this work, and they show that 1733 static word embeddings lack"
2021.naacl-main.138,D14-1004,0,0.0477818,"Missing"
2021.naacl-main.138,E09-1092,0,0.0640803,"Missing"
2021.naacl-main.138,N18-2049,0,0.346416,"precisely, we adopt the following useful distinctions from the literature: • Plausibility is a matter of degree (Wilks, 1975; Resnik, 1993). We therefore evaluate models by their ability to estimate the relative plausibility of events. • Plausibility describes non-surprisal conditioned on some context (Resnik, 1993; Gordon et al., 2011). For example, conditioned on the event “breathing,” it is less surprising to learn that the agent is “a dentist” than “a thought” and thus more plausible. • Plausibility is dictated by likelihood of occurrence in the world rather than text (Zhang et al., 2017; Wang et al., 2018). This discrepancy is due to reporting bias—the fact that people do not state the obvious (Gordon and Van Durme, 2013; Shwartz and Choi, 2020); e.g., “a person dying” is more likely to be attested than “a person breathing” (Figure 2). events plausible in the world attested events Figure 2: An attested event is necessarily plausible in the world, but not all plausible events are attested. By the world we refer to some possible world under consideration—in this sense plausibility is an epistemic modality. 1 Our implementation and data is available at https://github.com/ianporada/modeling_ event_"
2021.naacl-main.138,2020.emnlp-demos.6,0,0.0921319,"Missing"
2021.naacl-main.138,P19-1071,0,0.0616957,"o an argument as a particular dependency relation triple e ∈ S × V × O. (Evens, 1975; Resnik, 1993; Erk et al., 2010); e.g., We take g to be a ground-truth, total orderthe relative preference of the verb “breathe” for the ing of events expressed by the ordering function noun “dentist” as its nominal subject. g(e) > g(e0 ) iff e is more plausible than e0 . Our objective is to learn a model f : S × V × O → R Models of selectional preference are sometimes that is monotonic with respect to g, i.e., g(e) > evaluated by correlation with human judgements g(e0 ) =⇒ f (e) > f (e0 ). (Ó Séaghdha, 2010; Zhang et al., 2019a). The primary distinction between such evaluations and This simplification follows from previous work those of semantic plausibility, as in our work, is (Wang et al., 2018), and the plausibility score for that evaluations of semantic plausibility emphasize a given triple can be considered the relative plauthe importance of correctly modeling atypical yet sibility of the respective event across all contexts 1734 and realizations. While meaning is sensitive to small linguistic perturbations, we are interested in cases where one event is more plausible than another marginalized over context. Co"
2021.naacl-main.138,P19-1083,0,0.116265,"o an argument as a particular dependency relation triple e ∈ S × V × O. (Evens, 1975; Resnik, 1993; Erk et al., 2010); e.g., We take g to be a ground-truth, total orderthe relative preference of the verb “breathe” for the ing of events expressed by the ordering function noun “dentist” as its nominal subject. g(e) > g(e0 ) iff e is more plausible than e0 . Our objective is to learn a model f : S × V × O → R Models of selectional preference are sometimes that is monotonic with respect to g, i.e., g(e) > evaluated by correlation with human judgements g(e0 ) =⇒ f (e) > f (e0 ). (Ó Séaghdha, 2010; Zhang et al., 2019a). The primary distinction between such evaluations and This simplification follows from previous work those of semantic plausibility, as in our work, is (Wang et al., 2018), and the plausibility score for that evaluations of semantic plausibility emphasize a given triple can be considered the relative plauthe importance of correctly modeling atypical yet sibility of the respective event across all contexts 1734 and realizations. While meaning is sensitive to small linguistic perturbations, we are interested in cases where one event is more plausible than another marginalized over context. Co"
2021.naacl-main.138,W19-4804,0,0.0220202,"istributional model as an approximation of event plausibility (Zhang et al., 2020a). Our fine-tuned RoBERTa baseline follows this approach. Similar in spirit to our work, He et al. (2020) extend this baseline method by creating additional training data using the Probase taxonomy (Wu et al., 2012) in order to improve conceptual generalization; specifically, for each training example they swap the event’s arguments with its hypernym or hyponym, and they take this new, perturbed example to be an implausible event. There is also recent work focusing on monotonic inferences in semantic entailment (Yanaka et al., 2019; Goodwin et al., 2020; Geiger et al., 2020). Plausibility contrasts with entailment in that plausibility is not strictly monotonic with respect to hypernymy/hyponymy relations: the plausibility of an entity is not sufficient to infer the plausibility of its hyponyms (i.e., not downward entailing: it is plausible that a person gives birth but not that a man gives birth) nor hypernyms (i.e., not upward entailing: it is plausible that a baby fits inside a shoebox but not that a person does). Non-monotonic inferences have recently been explored in the context of defeasible reasoning (Rudinger et"
2021.naacl-main.138,2020.findings-emnlp.4,0,0.152965,"2 Technically, RoBERTa’s [CLS] and [SEP] tokens are <s> and </s>. to RoBERTa for each synset c ∈ C. We initialize the embedding of c as the average embedding of the sub-tokens of c’s lemma.3 We refer to RoBERTa’s positional embedding matrix as the x-position and randomly initialize a second positional embedding matrix, the y-position. The model input format follows that used for RoBERTa (§4.1), with the critical distinction that we also include the tokens for the hypernyms of the subject and object as additional input. For the subject s, we first disambiguate the synset c of s using BERT-WSD (Yap et al., 2020). Then for each hypernym c(i) in the hypernym chain α(c), the token of c(i) is included in the model input: this token takes the same x-position as the first sub-token of s and takes its y-position to be i, the depth in the lexical hierarchy. Finally, the x-position, y-position, and token embedding are summed for each token to compute its initial representation (Figure 3). The hypernyms of the object are included by the same procedure. Non-synset tokens have a yposition of zero. C ONCEPT I NJECT thus sees an event and the full hypernym chains of the arguments when computing a plausibility scor"
2021.naacl-main.138,P06-1107,0,0.104658,"cally, modeling selectional preference—the semantic plausibility of predicate- representational arsenal that people use in understanding language, ranging from social mores to argument structures—is known to be implicit in naive physics” (Resnik, 1996). discriminative tasks such as coreference resolution (Hobbs, 1978; Dagan and Itai, 1990; Zhang et al., A key property of plausibility is that the plau2019b), word sense disambiguation (Resnik, 1997; sibility of an event is generally consistent across McCarthy and Carroll, 2003), textual entailment some appropriate level of abstraction. For exam(Zanzotto et al., 2006; Pantel et al., 2007), and se- ple, events of the conceptual form “the [PERSON] mantic role labeling (Gildea and Jurafsky, 2002; breathes the [GAS]” are consistently plausible. Zapirain et al., 2013). Plausibility judgments follow this pattern because More broadly, modeling semantic plausibility people understand that similar concept classes is a necessary component of generative inferences share similar affordances. 1732 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1732–1743 June 6–11, 20"
C16-1101,D10-1049,0,0.036245,"uses abstract domain-specific representations to produce linguistic representations. These representations are often used in domain-specific or user-specific generation systems (Mellish et al., 1998; Langkilde, 2000; Walker et al., 2002; Stent et al., 2002) and need to be adapted when changing domains (Stent et al., 2004; Walker et al., 2007). Previous work considered sentence planning to be a rigid succession of three distinct tasks: lexical choice, aggregation and referring expression generation (Reiter et al., 2000). However, more recent approaches have integrated them into a joint system (Angeli et al., 2010; Konstas and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because many microplanning decisions can be directly derived from the input text. For example, sentence fusion and sentence compression systems rely directly on surface text and make assumptions about the similarity of input sentences with limited rewording. Sentence compression, which consists of taking one sentence and removing redundant parts has been heavily studied (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeo"
C16-1101,J05-3002,0,0.357919,"k on text-totext generation, because many microplanning decisions can be directly derived from the input text. For example, sentence fusion and sentence compression systems rely directly on surface text and make assumptions about the similarity of input sentences with limited rewording. Sentence compression, which consists of taking one sentence and removing redundant parts has been heavily studied (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008). Similarly, a large body of work exists in sentence fusion (Barzilay et al., 1999; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013). However, relying so heavily on the input text has limited the scope of text-to-text systems. In terms of aggregation, most current systems in sentence fusion focus on fusing very similar input sentences, as determined by lexical overlap, though several recent approaches expand fusion to more disparate ones (Elsner and Santhanam, 2011; Cheung and Penn, 2014). Our work in this paper can be seen as a systematic investigation to determine what content is semantically compatible at a sentence-level, in order to generate inputs"
C16-1101,P99-1071,0,0.559556,"asets for this task from a corpus using an automatic extraction process. Based on the results of a user study, we develop two gold-standard clusterings and corresponding evaluation methods for each dataset. We present a hierarchical clustering framework for predicting aggregation decisions on this task, which outperforms several baselines and can serve as a reference in future work. 1 Introduction In text-to-text generation, existing sentence compression and sentence fusion systems assume that the input takes the form of one or more sentences, and the output is one suitably modified sentence (Barzilay et al., 1999; Knight and Marcu, 2000; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013, for example). This line of work has led to new, knowledge-lean methods for sentence generation for applications such as text simplification and automatic summarization. The next step in expanding the scope of text-to-text generation is to relax assumptions about the forms of the input and output, with the eventual goal of generating entire passages or documents in an abstractive manner. We focus in this paper on aggregation, the task of determining what input units belong in the same output sentence."
C16-1101,W11-2832,0,0.0318002,"the context of text-to-text generation outside of restricted syntactic contexts, such as entity-driven noun phrase rewriting (Nenkova, 2008). In this paper, we propose a new aggregation task suited for text-to-text generation. The goal of the task is to aggregate content into sentence-sized units by taking sentence fragments (i.e., meaningful bits of text) as input, and outputting clusters of these fragments. Figure 1 shows an example of such a clustering. It can be viewed as a preceding step for sentence fusion or other text-to-text generation methods. Inspired by surface realization tasks (Belz et al., 2011), we use pre-existing sentences as a source of data in order to cheaply and automatically generate datasets with different granularities. We conduct a user study to confirm the validity of these datasets and design two gold-standard clusterings. We use these gold standards to define two methods of evaluation, and introduce two baselines for the task. Then, we propose a simple clustering model for this task based on logistic regression and hierarchical clustering. All variants of the model are shown to outperform both baselines on our datasets, but an analysis of the results identifies shortcom"
C16-1101,D14-1085,1,0.792738,", 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008). Similarly, a large body of work exists in sentence fusion (Barzilay et al., 1999; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013). However, relying so heavily on the input text has limited the scope of text-to-text systems. In terms of aggregation, most current systems in sentence fusion focus on fusing very similar input sentences, as determined by lexical overlap, though several recent approaches expand fusion to more disparate ones (Elsner and Santhanam, 2011; Cheung and Penn, 2014). Our work in this paper can be seen as a systematic investigation to determine what content is semantically compatible at a sentence-level, in order to generate inputs to such systems. We focus on predicting in this paper on what content should be expressed in the same sentence. Other work has examined the issue of how the content should be expressed syntactically, such as by applying hand-crafted rules (Pan and Shaw, 2004). White and Howcroft (2015), for example, show that rules for aggregation for clause combination can automatically be learned. 3 3.1 Clustering task Task definition The tas"
C16-1101,C08-1018,0,0.0242429,"and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because many microplanning decisions can be directly derived from the input text. For example, sentence fusion and sentence compression systems rely directly on surface text and make assumptions about the similarity of input sentences with limited rewording. Sentence compression, which consists of taking one sentence and removing redundant parts has been heavily studied (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008). Similarly, a large body of work exists in sentence fusion (Barzilay et al., 1999; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013). However, relying so heavily on the input text has limited the scope of text-to-text systems. In terms of aggregation, most current systems in sentence fusion focus on fusing very similar input sentences, as determined by lexical overlap, though several recent approaches expand fusion to more disparate ones (Elsner and Santhanam, 2011; Cheung and Penn, 2014). Our work in this paper can be s"
C16-1101,W11-1607,0,0.0226084,"ht and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008). Similarly, a large body of work exists in sentence fusion (Barzilay et al., 1999; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013). However, relying so heavily on the input text has limited the scope of text-to-text systems. In terms of aggregation, most current systems in sentence fusion focus on fusing very similar input sentences, as determined by lexical overlap, though several recent approaches expand fusion to more disparate ones (Elsner and Santhanam, 2011; Cheung and Penn, 2014). Our work in this paper can be seen as a systematic investigation to determine what content is semantically compatible at a sentence-level, in order to generate inputs to such systems. We focus on predicting in this paper on what content should be expressed in the same sentence. Other work has examined the issue of how the content should be expressed syntactically, such as by applying hand-crafted rules (Pan and Shaw, 2004). White and Howcroft (2015), for example, show that rules for aggregation for clause combination can automatically be learned. 3 3.1 Clustering task"
C16-1101,C10-1037,0,0.607469,"Based on the results of a user study, we develop two gold-standard clusterings and corresponding evaluation methods for each dataset. We present a hierarchical clustering framework for predicting aggregation decisions on this task, which outperforms several baselines and can serve as a reference in future work. 1 Introduction In text-to-text generation, existing sentence compression and sentence fusion systems assume that the input takes the form of one or more sentences, and the output is one suitably modified sentence (Barzilay et al., 1999; Knight and Marcu, 2000; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013, for example). This line of work has led to new, knowledge-lean methods for sentence generation for applications such as text simplification and automatic summarization. The next step in expanding the scope of text-to-text generation is to relax assumptions about the forms of the input and output, with the eventual goal of generating entire passages or documents in an abstractive manner. We focus in this paper on aggregation, the task of determining what input units belong in the same output sentence. Aggregation is an important step in micro-planning in the traditi"
C16-1101,N07-1023,0,0.0819769,"Missing"
C16-1101,P13-1138,0,0.0261601,"o produce linguistic representations. These representations are often used in domain-specific or user-specific generation systems (Mellish et al., 1998; Langkilde, 2000; Walker et al., 2002; Stent et al., 2002) and need to be adapted when changing domains (Stent et al., 2004; Walker et al., 2007). Previous work considered sentence planning to be a rigid succession of three distinct tasks: lexical choice, aggregation and referring expression generation (Reiter et al., 2000). However, more recent approaches have integrated them into a joint system (Angeli et al., 2010; Konstas and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because many microplanning decisions can be directly derived from the input text. For example, sentence fusion and sentence compression systems rely directly on surface text and make assumptions about the similarity of input sentences with limited rewording. Sentence compression, which consists of taking one sentence and removing redundant parts has been heavily studied (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata,"
C16-1101,A00-2023,0,0.147785,"ve Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1061 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1061–1070, Osaka, Japan, December 11-17 2016. Figure 1: An example of clustering 3 sentence fragments into 2 clusters and Wanner, 1996). Traditionally, a microplanner uses abstract domain-specific representations to produce linguistic representations. These representations are often used in domain-specific or user-specific generation systems (Mellish et al., 1998; Langkilde, 2000; Walker et al., 2002; Stent et al., 2002) and need to be adapted when changing domains (Stent et al., 2004; Walker et al., 2007). Previous work considered sentence planning to be a rigid succession of three distinct tasks: lexical choice, aggregation and referring expression generation (Reiter et al., 2000). However, more recent approaches have integrated them into a joint system (Angeli et al., 2010; Konstas and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because many microplanning"
C16-1101,P14-5010,0,0.00825152,"Missing"
C16-1101,J93-2004,0,0.0533157,"snowboard” is split into they ski and they snowboard). This different mechanism and the imperfections in the dependency trees make splitting on conjunctions a complex task and can lead to fragments that are not consistent. As a result, we generate two datasets of different granularities from the original corpus: the first one does not involve any action on conjunctions while the second one contains fragments extracted from conjunctions. These datasets will be referred to as KeepConj and SplitConj, respectively. 4.2 Creation of the datasets We used the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). Manually-annotated constituent trees were converted into dependency trees using the the Stanford CoreNLP framework (Manning 1 Technically, in this example, the first extracted fragment should be post-processed from “Sam, my brother,” to “Sam is my brother” (and the corresponding dependency tree) to represent a new individual sentence. However, all features used in our models (see Section 6.1) are indifferent to these modifications so the extracted fragments are not modified. 1063 (a) KeepConj (b) SplitConj Figure 3: Distribution of the number of fragments per sentence et al., 2014). Two data"
C16-1101,W05-1612,0,0.220869,"matic extraction process. Based on the results of a user study, we develop two gold-standard clusterings and corresponding evaluation methods for each dataset. We present a hierarchical clustering framework for predicting aggregation decisions on this task, which outperforms several baselines and can serve as a reference in future work. 1 Introduction In text-to-text generation, existing sentence compression and sentence fusion systems assume that the input takes the form of one or more sentences, and the output is one suitably modified sentence (Barzilay et al., 1999; Knight and Marcu, 2000; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013, for example). This line of work has led to new, knowledge-lean methods for sentence generation for applications such as text simplification and automatic summarization. The next step in expanding the scope of text-to-text generation is to relax assumptions about the forms of the input and output, with the eventual goal of generating entire passages or documents in an abstractive manner. We focus in this paper on aggregation, the task of determining what input units belong in the same output sentence. Aggregation is an important step in micro-planni"
C16-1101,E06-1038,0,0.0134227,"oint system (Angeli et al., 2010; Konstas and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because many microplanning decisions can be directly derived from the input text. For example, sentence fusion and sentence compression systems rely directly on surface text and make assumptions about the similarity of input sentences with limited rewording. Sentence compression, which consists of taking one sentence and removing redundant parts has been heavily studied (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008). Similarly, a large body of work exists in sentence fusion (Barzilay et al., 1999; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013). However, relying so heavily on the input text has limited the scope of text-to-text systems. In terms of aggregation, most current systems in sentence fusion focus on fusing very similar input sentences, as determined by lexical overlap, though several recent approaches expand fusion to more disparate ones (Elsner and Santhanam, 2011; Cheung"
C16-1101,W98-1411,0,0.15154,"icenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1061 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1061–1070, Osaka, Japan, December 11-17 2016. Figure 1: An example of clustering 3 sentence fragments into 2 clusters and Wanner, 1996). Traditionally, a microplanner uses abstract domain-specific representations to produce linguistic representations. These representations are often used in domain-specific or user-specific generation systems (Mellish et al., 1998; Langkilde, 2000; Walker et al., 2002; Stent et al., 2002) and need to be adapted when changing domains (Stent et al., 2004; Walker et al., 2007). Previous work considered sentence planning to be a rigid succession of three distinct tasks: lexical choice, aggregation and referring expression generation (Reiter et al., 2000). However, more recent approaches have integrated them into a joint system (Angeli et al., 2010; Konstas and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because m"
C16-1101,I08-1016,0,0.0113764,"the scope of text-to-text generation is to relax assumptions about the forms of the input and output, with the eventual goal of generating entire passages or documents in an abstractive manner. We focus in this paper on aggregation, the task of determining what input units belong in the same output sentence. Aggregation is an important step in micro-planning in the traditional data-to-text NLG pipeline (Reiter et al., 2000). However, it has been little examined within the context of text-to-text generation outside of restricted syntactic contexts, such as entity-driven noun phrase rewriting (Nenkova, 2008). In this paper, we propose a new aggregation task suited for text-to-text generation. The goal of the task is to aggregate content into sentence-sized units by taking sentence fragments (i.e., meaningful bits of text) as input, and outputting clusters of these fragments. Figure 1 shows an example of such a clustering. It can be viewed as a preceding step for sentence fusion or other text-to-text generation methods. Inspired by surface realization tasks (Belz et al., 2011), we use pre-existing sentences as a source of data in order to cheaply and automatically generate datasets with different"
C16-1101,P04-1011,0,0.0264338,"http:// 1061 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1061–1070, Osaka, Japan, December 11-17 2016. Figure 1: An example of clustering 3 sentence fragments into 2 clusters and Wanner, 1996). Traditionally, a microplanner uses abstract domain-specific representations to produce linguistic representations. These representations are often used in domain-specific or user-specific generation systems (Mellish et al., 1998; Langkilde, 2000; Walker et al., 2002; Stent et al., 2002) and need to be adapted when changing domains (Stent et al., 2004; Walker et al., 2007). Previous work considered sentence planning to be a rigid succession of three distinct tasks: lexical choice, aggregation and referring expression generation (Reiter et al., 2000). However, more recent approaches have integrated them into a joint system (Angeli et al., 2010; Konstas and Lapata, 2013; Kondadadi et al., 2013). Microplanning has not been considered relevant as an independent step in most recent work on text-totext generation, because many microplanning decisions can be directly derived from the input text. For example, sentence fusion and sentence compressi"
C16-1101,I13-1198,0,0.667212,"ults of a user study, we develop two gold-standard clusterings and corresponding evaluation methods for each dataset. We present a hierarchical clustering framework for predicting aggregation decisions on this task, which outperforms several baselines and can serve as a reference in future work. 1 Introduction In text-to-text generation, existing sentence compression and sentence fusion systems assume that the input takes the form of one or more sentences, and the output is one suitably modified sentence (Barzilay et al., 1999; Knight and Marcu, 2000; Marsi and Krahmer, 2005; Filippova, 2010; Thadani and McKeown, 2013, for example). This line of work has led to new, knowledge-lean methods for sentence generation for applications such as text simplification and automatic summarization. The next step in expanding the scope of text-to-text generation is to relax assumptions about the forms of the input and output, with the eventual goal of generating entire passages or documents in an abstractive manner. We focus in this paper on aggregation, the task of determining what input units belong in the same output sentence. Aggregation is an important step in micro-planning in the traditional data-to-text NLG pipel"
C16-1101,W15-4704,0,0.0165866,"put sentences, as determined by lexical overlap, though several recent approaches expand fusion to more disparate ones (Elsner and Santhanam, 2011; Cheung and Penn, 2014). Our work in this paper can be seen as a systematic investigation to determine what content is semantically compatible at a sentence-level, in order to generate inputs to such systems. We focus on predicting in this paper on what content should be expressed in the same sentence. Other work has examined the issue of how the content should be expressed syntactically, such as by applying hand-crafted rules (Pan and Shaw, 2004). White and Howcroft (2015), for example, show that rules for aggregation for clause combination can automatically be learned. 3 3.1 Clustering task Task definition The task consists of clustering sentence fragments (see next subsection) into clusters of fragments that can be merged into a single sentence. Figure 1 shows an example of clustering. In this instance, cluster 1 fragments can be combined into the sentence “Exports in October stood at $5.29 billion while imports increased sharply to $5.39 billion.”. This particular resulting sentence is an example and others can be generated from the same cluster. The structu"
C16-1247,P15-1033,0,0.0248617,"indefinite article, then subsequently referred to by a definite article. On the other hand, non-context-dependent factors such as local syntactic and semantic restrictions may block the presence of an article. For example, demonstratives (e.g., this, that), certain quantifiers (e.g., no), and mass nouns (e.g., money) do not permit articles. In this work, we investigate Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), a subclass of recurrent neural networks (RNNs) which have been popular recently in a variety of NLP tasks (Sutskever et al., 2014; Mikolov et al., 2010; Dyer et al., 2015). A number of reasons are often cited for the impressive performance gains of RNNs (Goldberg, 2015). First, they are able to take advantage of the patterns inherent in the data to learn features and representations suitable for complex interpretation tasks. Second, they can learn connections between processing units in the same layer, allowing the network to capture relations and patterns over an unbounded number of timesteps. Third, they provide an easy and natural way to inject external semantic knowledge by initializing the parameters of the model in an informed way. For example, the word e"
C16-1247,I08-1059,0,0.264628,"Missing"
C16-1247,P98-1085,0,0.196569,"We also show how the attention mechanism contributes to the interpretability of the model’s effectiveness. 1 Introduction Correctly performing pragmatic reasoning is at the core of many NLP tasks such as information extraction, automatic summarization, and machine translation. We focus in this paper on definiteness prediction, the task of determining whether a noun phrase should be definite or indefinite. In English, one instantiation of this task is to predict whether to use a definite article (the), indefinite article (a(n)), or no article at all. It has applications in machine translation (Heine, 1998; Netzer and Elhadad, 1998), and in L2 grammatical error detection and correction (Han et al., 2006). Definiteness prediction is an interesting testbed for pragmatic reasoning, because both contextual and local cues are crucial to determining the acceptability of a particular choice of article. Consider the following example: (1) A/#the man entered the room. The/#a man turned on the TV. Factors such as discourse context, familiarity, and information status play a role in determining the choice of articles. Here, man is introduced into the discourse context by an indefinite article, then subseq"
C16-1247,P14-1002,0,0.05663,"Missing"
C16-1247,Q15-1024,0,0.022298,"Missing"
C16-1247,W13-3214,0,0.0653911,"Missing"
C16-1247,W09-3010,0,0.05505,"Missing"
C16-1247,D14-1220,0,0.0563759,"Missing"
C16-1247,P15-1107,0,0.0521306,"Missing"
C16-1247,J93-2004,0,0.0540955,"Mikolov et al. (2013) trained on the Google News corpus (about 100 billion words). • GloVe: The embedding is initialized by the global vectors Pennington et al. (2014) that are trained on the Common Crawl corpus (840 billion tokens). Both word2vec and GloVe word embeddings consist of 300 dimensions. To test whether compressing those embeddings would lead to a better prediction performance, we investigated the use of PCA to reduce the dimensionality of the word embeddings, but found that this did not influence performance on the development set. 4 Dataset We use the Penn Treebank (PTB) corpus (Marcus et al., 1993) with the standard section splits for training (01–23), development (00, 24) and testing (22, 23). We extract all of the noun phrases present in the parsed corpus whose head noun’s POS tag is one of NN, NNS, NNP, or NNPS. Also, we do not lemmatize, and ignore case and punctuation. Finally, we remove any occurrence of the relevant determiners (the, a, an) from all the data sets (training, development, test). The number of samples in the dataset is shown in Table 1. In our experiments, we adopt one of two different sample configurations: the first focusing on a local context and the second exten"
C16-1247,W00-0708,0,0.499236,"Missing"
C16-1247,W98-1012,0,0.0389648,"how the attention mechanism contributes to the interpretability of the model’s effectiveness. 1 Introduction Correctly performing pragmatic reasoning is at the core of many NLP tasks such as information extraction, automatic summarization, and machine translation. We focus in this paper on definiteness prediction, the task of determining whether a noun phrase should be definite or indefinite. In English, one instantiation of this task is to predict whether to use a definite article (the), indefinite article (a(n)), or no article at all. It has applications in machine translation (Heine, 1998; Netzer and Elhadad, 1998), and in L2 grammatical error detection and correction (Han et al., 2006). Definiteness prediction is an interesting testbed for pragmatic reasoning, because both contextual and local cues are crucial to determining the acceptability of a particular choice of article. Consider the following example: (1) A/#the man entered the room. The/#a man turned on the TV. Factors such as discourse context, familiarity, and information status play a role in determining the choice of articles. Here, man is introduced into the discourse context by an indefinite article, then subsequently referred to by a def"
C16-1247,D14-1162,0,0.0829231,"initializing it randomly, or with pre-trained vectors, which could inject knowledge derived from a large, external corpus. The intuition behind incorporating pre-trained vectors is that they might help the model recognize bridging references of those involving synonyms (e.g., a house ... the home). Thus, we compare the following options: • Random: The embedding is initialized randomly. • word2vec: The embedding is initialized by the SkipGram vectors of Mikolov et al. (2013) trained on the Google News corpus (about 100 billion words). • GloVe: The embedding is initialized by the global vectors Pennington et al. (2014) that are trained on the Common Crawl corpus (840 billion tokens). Both word2vec and GloVe word embeddings consist of 300 dimensions. To test whether compressing those embeddings would lead to a better prediction performance, we investigated the use of PCA to reduce the dimensionality of the word embeddings, but found that this did not influence performance on the development set. 4 Dataset We use the Penn Treebank (PTB) corpus (Marcus et al., 1993) with the standard section splits for training (01–23), development (00, 24) and testing (22, 23). We extract all of the noun phrases present in th"
C16-1247,J98-2001,0,0.167706,"Missing"
C16-1247,N07-2045,0,0.0551702,"Missing"
D10-1003,P05-1022,0,0.155039,"Missing"
D10-1003,C00-1027,0,0.0130438,"information to aid parsing, especially for structural dependencies between sentences, such as parallelism effects. tactic structure and the distance since its last occurrence, which indicates syntactic consistency. These studies, however, do not provide consistency results on subsets of production-types, such as by production LHS as our study does, so the implications that can be drawn from them for improving parsing are less apparent. We adopt the measure used by Dubey et al. (2005) to quantify syntactic consistency, adaptation probability. This measure originates in work on lexical priming (Church, 2000), and quantifies the probability of a target word or construction w appearing in a “primed” context. Specifically, four frequencies are calculated, based on whether the target construction appears in the previous context (the prime set), and whether the construction appears after this context (the target set): 2 Syntactic Consistency in the Penn Treebank WSJ Syntactic consistency has been examined by Dubey et al. (2005) for several English corpora, including the WSJ, Brown, and Switchboard corpora. They have provided evidence that syntactic consistency exists not only within coordinate structu"
D10-1003,W05-0622,0,0.0207686,"ge of candidate parses which share more productions with the previous parse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (χ2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T } given a sequence of observed output x = x{t=1...T } and weight vector θ = θ{k=1...K} is given as follows: T X X 1 θk fk (yt−1 , yt , x, t)) P (y|x) = exp( Z t=1 k where Z is the partition function. The feature functions fk (yt−1 , yt , x, t) can depend on two"
D10-1003,J05-1003,0,0.139605,"Missing"
D10-1003,H05-1104,0,0.101635,"use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies. 1 Introduction Recent corpus linguistics work has produced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (Gries, 2005; Dubey et al., 2005; Reitter, 2008). In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency1 . Both types of studies, however, have 1 Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations. For instance, Bock (1986) and Gries (2005) examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs, and Dubey et al."
D10-1003,P06-1053,0,0.0175089,"onsistent production-types have NP as the LHS, but overall, productions with many different LHS parents exhibit consistency. 3 A Context-Aware Reranker Having established evidence for widespread syntactic consistency in the WSJ corpus, we now investigate incorporating extra-sentential context into a statistical parser. The first decision to make is whether to incorporate the context into a generative or a discriminative parsing model. Employing a generative model would allow us to train the parser in one step, and one such parser which incorporates the previous context has been implemented by Dubey et al. (2006). They implement a PCFG, learning the production probabilities by a variant of standard PCFG-MLE probability estimation that conditions on whether a rule has recently occurred in the context or not: P (RHS|LHS, P rime) = c(LHS → RHS, P rime) c(LHS, P rime) LHS and RHS represent the left-hand side and 26 right-hand side of a production, respectively. Prime is a binary variable which is True if and only if the current production has occurred in the prime set (the previous sentence). c represents the frequency count. The drawback of such a system is that it doubles the state space of the model, a"
D10-1003,P08-1109,0,0.3334,"deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models would provide further imp"
D10-1003,P07-1086,0,0.0446182,"Missing"
D10-1003,E09-1047,0,0.0293061,"Missing"
D10-1003,N06-1020,0,0.0374066,"nd Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models wou"
D10-1003,N07-1051,0,0.0376031,"ment in phrasal verbs, and Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that comb"
D10-1003,N03-1028,0,0.0731368,"arse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (χ2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T } given a sequence of observed output x = x{t=1...T } and weight vector θ = θ{k=1...K} is given as follows: T X X 1 θk fk (yt−1 , yt , x, t)) P (y|x) = exp( Z t=1 k where Z is the partition function. The feature functions fk (yt−1 , yt , x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position o"
D10-1003,E09-1090,0,0.0119953,"ories, and this difference is statistically significant (χ2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T } given a sequence of observed output x = x{t=1...T } and weight vector θ = θ{k=1...K} is given as follows: T X X 1 θk fk (yt−1 , yt , x, t)) P (y|x) = exp( Z t=1 k where Z is the partition function. The feature functions fk (yt−1 , yt , x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position of the sentence in the sequence. Since our feature functions do not depend o"
D14-1085,A00-2024,0,0.0271803,"ke this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure 2, the phrase of food-borne illness can be added to the previous output sentence, despite originating in a source text sentence that is quite different overall. Elsner and"
D14-1085,J05-3002,0,0.251702,"ake the union or intersection of the information present therein. In this paper, we move further along this path in the following ways. First, we present sentence enhancement as a novel technique which extends sentence fusion by combining the subtrees of many sentences into the output sentence, rather than just a few. Doing so allows relevant information from sentences that are not similar to the original input sentences to be added during fusion. As Introduction Sentence fusion is the technique of merging several input sentences into one output sentence while retaining the important content (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013). For example, the input sentences in Figure 1 may be fused into one output sentence. As a text-to-text generation technique, sentence fusion is attractive because it provides an avenue for moving beyond sentence extraction in automatic summarization, while not requiring deep se775 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775–786, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Source text: This fact has been underscored in the last few months"
D14-1085,P10-1143,0,0.0138204,"signed a simple method to approximate event coreference resolution that does not require event coreference labels. This method is based on the intuition that different mentions of an event should contain many of the same participants. Thus, by measuring the similarity of the arguments and the syntactic contexts between the node in the sentence graph and the candidate edge, we can have a measure of the likelihood that they refer to the same event. We would be interested in integrating existing event coreference resolution systems into this step in the future, such as the unsupervised method of Bejan and Harabagiu (2010). Existing event coreference systems tend to focus on cases with different heads (e.g., X kicked Y, then Y was injured), which could increase the possibilities for sentence enhancement, if the event coreference module is sufficiently accurate. However, since our method currently only merges identical heads, we require a more fine-grained method based on distributional measures of similarity. We measure the similarity of these syntactic contexts by aligning the arguments in the syntactic contexts and computing the similarity of the aligned arguments. These problems can be jointly solved as a ma"
D14-1085,C00-1072,0,0.0201224,"ustering, which requires a measure of similarity between sentences and a stopping criterion. We define the similarity between two sentences to be the standard cosine similarity between the lemmata of the sentences, weighted by IDF and excluding stopwords, and clustering is run until a similarity threshold of 0.5 is reached. Since complete-link clustering prefers small coherent clusters and we select the top-scoring cluster in each document collection, the method is somewhat robust to different choices of the stopping threshold. The clusters are scored according to the signature term method of Lin and Hovy (2000), which assigns an importance score to each term accordSentence graph creation After core sentence identification, the next step is to align the nodes of the dependency trees of the core input sentences in order to create the initial sentence graph. The input to this step is the collapsed dependency tree representations of the core sentences produced by the Stanford parser1 . In this representation, preposition nodes are collapsed into the label of the dependency edge between the functor of the prepositional phrase and the prepositional object. Chains of conjuncts are also split, and each argu"
D14-1085,P13-1121,1,0.932664,"by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure 2, the phrase of food-borne illness can be added to the previous output sentence, despite originating in a source text sentence that is quite different overall. Elsner and Santhanam (2011) proposed a supervised method to fuse disparate sentences, which takes as input a small number of sentences with compatible information that have been manually identified by editors of articles. By contrast, our algorithm is unsupervised, and tackles the problem"
D14-1085,W05-1612,0,0.0772574,"l contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised a"
D14-1085,E06-1038,0,0.104556,"s. Output: The outbreak of food-borne illness led to the recall on Tuesday of lots of hot dogs and meats produced at the Bil Mar Foods plant. using local contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding informatio"
D14-1085,C08-1018,0,0.060764,"ess led to the recall on Tuesday of lots of hot dogs and meats produced at the Bil Mar Foods plant. using local contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occur"
D14-1085,W11-1611,0,0.0233177,"Missing"
D14-1085,W11-1607,0,0.255482,"apata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure"
D14-1085,W12-3018,0,0.016449,"Missing"
D14-1085,D08-1019,0,0.0563558,"n of the information present therein. In this paper, we move further along this path in the following ways. First, we present sentence enhancement as a novel technique which extends sentence fusion by combining the subtrees of many sentences into the output sentence, rather than just a few. Doing so allows relevant information from sentences that are not similar to the original input sentences to be added during fusion. As Introduction Sentence fusion is the technique of merging several input sentences into one output sentence while retaining the important content (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013). For example, the input sentences in Figure 1 may be fused into one output sentence. As a text-to-text generation technique, sentence fusion is attractive because it provides an avenue for moving beyond sentence extraction in automatic summarization, while not requiring deep se775 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775–786, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Source text: This fact has been underscored in the last few months by two unexpected outbreaks"
D14-1085,P06-1055,0,0.0301786,"the sentence graph is expanded, the large increase in the oracle score indicates the potential of sentence enhancement for generating high-quality summary sentences. Results and discussion As shown in Table 1, sentence enhancement with coreference outperforms the sentence fusion algorithm of F&S in terms of the Pyramid BE measure and the baseline expansion algorithm, though only the latter difference is statistically significant (p = 0.0196 ). In terms of the ROUGE word overlap 5 The likelihoods are obtained by the PCFG model of CoreNLP version 1.3.2. We experimented with the Berkeley parser (Petrov et al., 2006) as well, with similar results that favour the sentence enhancement with event coreference method, but because the parser failed to parse a number of cases, we do not report those results here. 6 All statistical significance results in this section are for Wilcoxon signed-rank tests. 7 There is no guarantee that these dependency triples form a tree structure. Hence, this is an upper bound. 781 supply such an analysis and provide evidence that human summary writers actually do incorporate elements external to the source text for a reason, namely, that these elements are more specific to the sem"
D14-1085,C10-1037,0,0.0306549,"e kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studie"
D14-1085,J02-4005,0,0.011992,"ce enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure 2, the phrase of food-borne illness can be added to the previous output sentence, despite originating in a source text sentence that is quite different overall. Elsner and Santhanam (2011) proposed a"
D14-1085,N07-1023,0,0.00992592,"utbreak of food-borne illness led to the recall on Tuesday of lots of hot dogs and meats produced at the Bil Mar Foods plant. using local contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences"
D14-1085,I13-1198,0,0.657721,"therein. In this paper, we move further along this path in the following ways. First, we present sentence enhancement as a novel technique which extends sentence fusion by combining the subtrees of many sentences into the output sentence, rather than just a few. Doing so allows relevant information from sentences that are not similar to the original input sentences to be added during fusion. As Introduction Sentence fusion is the technique of merging several input sentences into one output sentence while retaining the important content (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013). For example, the input sentences in Figure 1 may be fused into one output sentence. As a text-to-text generation technique, sentence fusion is attractive because it provides an avenue for moving beyond sentence extraction in automatic summarization, while not requiring deep se775 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775–786, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Source text: This fact has been underscored in the last few months by two unexpected outbreaks of food-borne illness. Outpu"
D14-1085,S12-1034,0,0.050118,"Missing"
D14-1085,D08-1057,0,0.0224337,"into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notio"
D15-1014,D14-1108,0,0.0673941,"Missing"
D15-1014,W04-1013,0,0.0386826,"from also providing a critical evaluation of the linked article. There has in fact been some work along these lines, within the framework of extractive summarization. Lofi and Krestel (2012) describe a system to generate tweets from local government records through keyphrase extraction. Lloret and Palomar (2013) compares various extractive summarization algorithms applied on Twitter data to generate tweets from documents. Lofi and Krestel do not provide a formal evaluation of their model, while Lloret and Palomar compared overlap between system-generated and usergenerated tweets using ROUGE (Lin, 2004). Unfortunately, they also show that there is little correlation between ROUGE scores and the perceived quality of the tweets when rated by human users for indicativeness and interest. More scrutiny is Social media such as Twitter have become an important method of communication, with potential opportunities for NLG to facilitate the generation of social media content. We focus on the generation of indicative tweets that contain a link to an external web page. While it is natural and tempting to view the linked web page as the source text from which the tweet is generated in an extractive summ"
D15-1014,N13-1078,0,0.0262883,"Missing"
D15-1014,W01-0100,0,0.13284,". In another study, Chen et al. (2012) were able to extract sentiment expressions from a corpus of tweets including both formal words and informal slang that bear sentiment. required to determine whether the wholesale adoption of methods and evaluation schemes from extractive summarization is justified. Beyond issues of evaluation measures, it is also unclear whether extraction is the strategy employed by human tweeters. One of the original motivations behind extractive summarization was the observation that human summary writers tended to extract snippets of key phrases from the source text (Mani, 2001). And while it may be true that an automatic tweet generation system need not necessarily follow the same approach to writing as human tweeters, it is still necessary to know what proportion of tweets could be accounted for in an extractive summarization paradigm. With indicative tweets, an additional issue arises in that the genre of the source text is not constrained; for example it may be a news article or an informal blog post. This may be vastly different from the desired formality of tweet itself, and thus, a genre-appropriate extract may not be available. We begin to address the above i"
D15-1014,P13-1121,1,0.846842,"ided by the author, rather than transcripts or slides from the presentation. Conroy et al. (2006) computed an oracle ROUGE score to investigate the same issue of the limits of extraction for news text. Background and Related Work There have been studies on a number of different issues related to Twitter data, including classifying 139 more than half, or around 16,000, contained URLs to an external news article, photo on photo sharing sites, or video. These studies show that extractive summarization algorithms may not generate good quality summaries despite giving high ROUGE evaluation scores. Cheung and Penn (2013) show that for the news genre, extractive summarization systems that are optimized for centrality—that is, getting the core parts of the text into the summary— cannot perform well when compared to model summaries, since the model summaries are abstracted from the document to a large extent. 3 3.1 Politics #apec2014 #G20 #oscarpistorius Events #haiyan #memorialday #ottawashootings International #berlinwall #ebola #erdogan Data Extraction and Preprocessing Using Twitter for Data Extraction As mentioned earlier, there have been numerous studies that used data from the public Twitter feeds. Howeve"
D15-1014,P14-5010,0,0.00542228,"removed those links that contained fewer than a threshold of 150 words. After this preprocessing, the number of useful articles was reduced from 6003 to 3066. There were some further tweet-article pairs where the text of the tweets was identical, these were removed by further preprocessing and the number of unique tweet-article pairs came down to 2471. The final version of the data consists of tweets along with other information about the tweet, such as links to articles, hashtags, time of publication, etc. We also retain the linked article text and preprocessed it using the CoreNLP toolkit (Manning et al., 2014). This includes the URL itself and the text extracted from the article, as well as some extracted information such as sentence boundaries, POS tags for tokens, parse trees and dependency trees. These annotations are used later during our Extracting Data Data was extracted from Twitter using the Twitter REST API using 51 search terms, or hashtags. These hashtags were chosen from a range of topics including pop culture, international summit meetings discussing political issues, lawsuits and trials, social issues and health care issues. All these hashtags were trending (being tweeted about at a h"
D15-1014,P06-2020,0,0.0954045,"Missing"
D15-1014,N13-1039,0,0.0621965,"Missing"
D15-1014,N15-1112,0,0.118434,"Missing"
D15-1014,C14-1083,0,0.203687,"nsitive to stylistic factors as well as the underlying intent of the tweet. 2 Other studies using Twitter data include O’Connor et al. (2010), who use topic summarization for a given search for better browsing. Chakrabarti and Punera (2011) generate an event summary by learning about the event using a Hidden Markov Model over the tweets describing it. Wang et al. (2014) generate a coherent event summary by treating summarization as an optimization problem for topic cohesion. Inouye and Kalita (2011) compare multiple summarization techniques to generate a summary of multipost blogs on Twitter. Wei and Gao (2014) use tweets to help in generating better summaries of news articles. As described in Section 1, we analyze tweet generation using measures inspired by extractive summarization evaluation. There has been one study comparing different text summarization techniques for tweet generation by Lloret and Palomar (2013). Summarization systems were used to generate sentences lesser than 140 characters in length by summarizing documents, which could then be taken to be tweets. The systemgenerated tweets were evaluated using ROUGE measures (Lin, 2004). The ROUGE-1, ROUGE-2 and ROUGE-L measures were used,"
D16-1179,W06-0901,0,0.0569665,"Missing"
D16-1179,P10-1143,0,0.0728286,"Missing"
D16-1179,P92-1002,0,0.6987,"Missing"
D16-1179,J97-4002,0,0.0770244,"ch as informal dialogue where VPE occurs more frequently (Nielsen, 2005). Most current event extraction systems ignore VPE and derive some structured semantic representation by reading information from a shallow dependency parse of a sentence. Such an approach would not only miss many valid links between an elided verb and its arguments, it could also produce nonsensical extractions if applied directly on an auxiliary trigger. In the example above, a naive approach might produce an unhelpful semantic triple such as (Dodge, agent, do). There have been several previous empirical studies of VPE (Hardt, 1997; Nielsen, 2005; Bos and Spenader, 2011; Bos, 2012; Liu et al., 2016). Many previous approaches were restricted to solving specific subclasses of VPE (e.g., VPE triggered by do (Bos, 2012)), or have relied on simple heuristics for some or all of the steps in VPE resolution, such as by picking the most recent previous clause as the antecedent. In this paper, we develop a VPE resolution pipeline which encompasses a broad class of VPEs (Figure 1), decomposed into the following two steps. In the VPE detection step, the goal is to determine whether or not a word triggers VPE. The second step, antec"
D16-1179,D12-1045,0,0.0641218,"Missing"
D16-1179,liu-etal-2014-supervised,0,0.0380319,"Missing"
D16-1179,W16-0705,0,0.251558,"en, 2005). Most current event extraction systems ignore VPE and derive some structured semantic representation by reading information from a shallow dependency parse of a sentence. Such an approach would not only miss many valid links between an elided verb and its arguments, it could also produce nonsensical extractions if applied directly on an auxiliary trigger. In the example above, a naive approach might produce an unhelpful semantic triple such as (Dodge, agent, do). There have been several previous empirical studies of VPE (Hardt, 1997; Nielsen, 2005; Bos and Spenader, 2011; Bos, 2012; Liu et al., 2016). Many previous approaches were restricted to solving specific subclasses of VPE (e.g., VPE triggered by do (Bos, 2012)), or have relied on simple heuristics for some or all of the steps in VPE resolution, such as by picking the most recent previous clause as the antecedent. In this paper, we develop a VPE resolution pipeline which encompasses a broad class of VPEs (Figure 1), decomposed into the following two steps. In the VPE detection step, the goal is to determine whether or not a word triggers VPE. The second step, antecedent identification, requires selecting the clause containing the ve"
D16-1179,P14-5010,0,0.0053977,"rigger for VPE. In our experiments, we used a logistic regression classifier. TOTAL 554 Table 1: Auxiliary categories for VPE and their frequencies in all 25 sections of the WSJ. 4.1 Feature Extraction We created three different sets of features related to the auxiliary and its surrounding context. of the dataset that they examined. 3 VPE Detection Approach and Data We divide the problem into two separate tasks: VPE detection (Section 4), and antecedent identification (Section 5). Our experiments use the entire dataset presented in (Bos and Spenader, 2011). For preprocessing, we used CoreNLP (Manning et al., 2014) to automatically parse the raw text of WSJ for feature extraction. We also ran experiments using goldstandard parses; however, we did not find significant differences in our results2 . Thus, we only report results on automatically generated parses. We divide auxiliaries into the six different categories shown in Table 1, which will be relevant for our feature extraction and model training process, as we will describe. This division is motivated by the fact that different auxiliaries exhibit different behaviours (Bos and Spenader, 2011). The results we present on the different auxiliary catego"
D16-1179,P05-1012,0,0.121477,"Missing"
D16-1179,P07-1031,0,0.0420226,"extraction and model training process, as we will describe. This division is motivated by the fact that different auxiliaries exhibit different behaviours (Bos and Spenader, 2011). The results we present on the different auxiliary categories (see Tables 2 and 4) are obtained from training a single classifier over the entire dataset and then testing on auxiliaries from each category, with the ALL result being the accuracy obtained over all of the test data. 2 An anonymous reviewer recommended that further experiments could be performed by using the more informative NPs created with NML nodes (Vadas and Curran, 2007) on the goldstandard parsed WSJ. 3 For example, “John will go to the store and Mary will do the same/likewise/the opposite”. Do X anaphora and modals are not technically auxiliary verbs, as noted by the annotators of our dataset (Bos and Spenader, 2011), but for the purposes of this study we generalize them all as auxiliaries while simultaneously dividing them into their correct lexical categories. 1736 Auxiliary. Auxiliary features describe the characteristics of the specific auxiliary, including the following:  word identity of the auxiliary  lemma of the auxiliary  auxiliary type (as sho"
D16-1179,D07-1080,0,0.029874,"equals that of the i-1th token before the trigger (for i ∈ {1, 2, 3}, where i = 1 considers the trigger itself). 5.3 Training Algorithm - MIRA Since many potential antecedents share relatively similar characteristics, and since we have many features and few examples, we use the Margin-InfusedRelaxed-Algorithm (MIRA) in order to identify the most likely potential antecedent. MIRA maximizes the margin between the best candidate and the rest of the potential antecedents according to a loss function. It has been used for tasks with similar characteristics, such as statistical machine translation (Watanabe et al., 2007). The training algorithm begins with a random initialization of the weight vector w. The training set contains triggers, each trigger’s candidate antecedents, and their gold standard antecedents; it is reshuffled after each training epoch. We find the K highest-scoring potential antecedents, a1 , . . . , ak , according to the current weight value. A learning rate parameter determines how much we retain the new weight update with respect to the previous weight vector values. MIRA defines the update step of the standard online training algorithm: it seeks to learn a weight vector that, when mult"
D16-1179,W09-1401,0,\N,Missing
D17-1086,Q16-1002,0,0.0434743,".bengio,ryan.lowe}@mail.mcgill.ca {jcheung,dprecup}@cs.mcgill.ca School of Computer Science McGill University Abstract of scripts (Schank and Abelson, 1977). Despite the importance of world knowledge, previous data sets and tasks for reading comprehension have targeted other aspects of the reading comprehension problem, at times explicitly attempting to factor out its influence. In the Daily Mail/CNN dataset (Hermann et al., 2015), named entities such Clarkson and Top Gear are replaced by anonymized entity tokens like ent212. The Children’s Book Test focuses on the role of context and memory (Hill et al., 2016a), and the fictional genre makes it difficult to connect the entities in the stories to real-world knowledge about those entities. As a result, language models have proved to be a highly competitive solution to these tasks. Chen et al. (2016) showed that their attentionbased LSTM model achieves state-of-the-art results on the Daily Mail/CNN data set. In fact, their analysis shows that more than half of the questions can be answered by exact word matching and sentence-level paraphrase detection, and that many of the remaining errors are difficult to solve exactly because the entity anonymizati"
D17-1086,P82-1020,0,0.755588,"Missing"
D17-1086,P16-2019,1,0.851173,"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 825–834 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 1995) and Freebase (Bollacker et al., 2008) consist of entity-relation triples of the form (head, relation, tail). In entity prediction, either the head or tail entity is removed, and the model has to predict the missing entity. Recent efforts have integrated different sources of knowledge, for example combining distributional and relational semantics for building word embeddings (Fried and Duh, 2015; Long et al., 2016). While this task requires understanding and predicting associations between entities, it does not require contextual reasoning with text passages, which is crucial in rare entity prediction. Context [...] , who lived from 1757 to 1827, was admired by a small group of intellectuals and artists in his day, but never gained general recognition as either a poet or painter. [...] Candidate Entities Peter Ackroyd: Peter Ackroyd is an English biographer, novelist and critic with a particular interest in the history and culture of London. [...] William Blake: William Blake was an English poet, painte"
D17-1086,W15-4640,1,0.84064,"ng to that end. Finally, at each time step i, the temporal network outputs an embedding ri (C1 , ..., Cn ) ≡ ri . We use this temporal embedding to predict the probability of the context-entity pair with a P (e = e˜|Ci , Le ) = σ((hei )T W de + b) where σ is the sigmoid function, W and b are model parameters. The cross term (hei )T W de is a dot product between hei and de that weighs the dimensions differently based on the learned parameters W . Similar prediction methods have been used successfully for question answering (Bordes et al., 2014; Yu et al., 2014) and dialogue response retrieval (Lowe et al., 2015). We also experimented with only feeding hei to the predictor, without the cross term, and found this slows down training the lexical encoder. While hei is a function of de , using de in the cross term (hei )T W de provides a much shorter gradient path from the loss to the lexical encoder through de , thus allowing both modules to learn at the same pace. 2 We mix the word / vector notation here since each word w is replaced by its corresponding word embedding vector. 829 P (e = e˜|C1,...,i , Le ) = σ((hei )T W de + rTi V + b) ri ... ¯ i−2 h ¯ i−1 h ¯i = h 1 |E| P e0 ∈E 0 hei hei Ci ... de0 ..."
D17-1086,D14-1162,0,0.0847431,"vectors and the context vector. The entity with the highest cosine similarity score is chosen as the prediction. Figure 4: Accuracies of C ONT E NC, D OUB E NC, and H IER E NC on test set, for different frequency ranges; n is entity frequency in the entire corpus. H IER E NC. Models with the best performance on validation set are saved and used to test on test set. AVG E MB + COS This baseline computes the context embedding by taking the average of some pre-trained word embeddings. The entities’ embeddings are computed in the same way. In our experiments, we choose to use the published GloVe (Pennington et al., 2014) pre-trained word embeddings. Same as above, the prediction is made by considering the cosine similarity between the context embedding and the entity embeddings. 5.3 5.4 Results Empirical results are shown in Table 3. We test our proposed model architectures (detailed in Section 4), along with baselines described in Section 5.2. It is clear from Table 3 that models that only use contextual knowledge give relatively poor performance compared to the ones that utilize lexical resources. The large discrepancy between the context encoder and the double encoder shows that lexical resources play a cr"
D17-1086,P16-1223,0,0.0278373,"comprehension have targeted other aspects of the reading comprehension problem, at times explicitly attempting to factor out its influence. In the Daily Mail/CNN dataset (Hermann et al., 2015), named entities such Clarkson and Top Gear are replaced by anonymized entity tokens like ent212. The Children’s Book Test focuses on the role of context and memory (Hill et al., 2016a), and the fictional genre makes it difficult to connect the entities in the stories to real-world knowledge about those entities. As a result, language models have proved to be a highly competitive solution to these tasks. Chen et al. (2016) showed that their attentionbased LSTM model achieves state-of-the-art results on the Daily Mail/CNN data set. In fact, their analysis shows that more than half of the questions can be answered by exact word matching and sentence-level paraphrase detection, and that many of the remaining errors are difficult to solve exactly because the entity anonymization procedure removes necessary world knowledge. In this paper, we propose a novel task called rare entity prediction, which places the use of external knowledge at its core, with the following key features. First, our task is similar in flavou"
D17-1086,D16-1264,0,0.115657,"Missing"
D17-1086,D11-1141,0,0.0970459,"Missing"
D17-1086,N04-1002,0,0.153871,"Missing"
D18-1094,E17-2068,0,0.145257,"Missing"
D18-1094,D16-1076,0,0.0318967,"Missing"
D18-1094,E17-1104,0,0.0394784,"ass label of the next level, conditioned on a dynamic document representation obtained based on a variant of an attention mechanism (Bahdanau et al., 2015). The contribution of our paper is as follows: A large number of documents are being generated all over the world everyday, and as a result automatic text classification has become an essential tool for searching, retrieving, and managing the text (Allahyari et al., 2017). There has been an increasing trend in developing data-driven neural text classifiers (Collobert et al., 2011; Lai et al., 2015; Zhang et al., 2015; Yogatama et al., 2017; Conneau et al., 2017), due to their ability to handle large-scale corpora and their robustness in automatic feature extraction. However, text classification has become increasingly challenging as the number of categories grows with continually expanding corpus. To alleviate this problem, one form of the external knowledge – class taxonomy – has been introduced to aid the classification in a hierarchical fashion (Koller and Sahami, 1997). In general, hierarchical classifiers can be categorized into two broad approaches: local (top-down and bottom-up) and global (or big-bang) (Silla and Freitas, 2011). The 1. We pro"
D18-1094,N16-1062,0,0.027767,"models are trained without the hierarchical taxonomy of classes and therefore only have results on the leaf-node classification. sult on each level, such as l1 , l2 and l3 . This indicates the per-level classification performance when we provide the true parent class to the classifier while predicting the next class. However, this is not desirable as during inference we should not have access to the correct parent class. Hence we also present the Overall score in Table 2, where the classifier uses its own prediction as the parent class. LSTM with max/mean pooling (Collobert and Weston, 2008b; Lee and Dernoncourt, 2016) and the Structured Self-attentive classifier (Lin et al., 2017) are used for the comparison. We noticed that using the default hyperparameters of the Structured Self-attentive classifier with high attention hops (m &gt;= 8) performed poorly compared to use just one attention hop (m = 1). Therefore, we reported the results of using one attention hop (m = 1) as our baselines for fair comparison. We also compare our classifier to the state-of-theart hierarchical classifier HDLTex (Kowsari et al., 2017). 5 Results Our model is significantly better than the existing state-of-the-art hierarchical base"
D18-1094,P18-1031,0,0.0309364,"oaches is to create a document representation from either the last hidden state of the RNN or via some pooling operations on all hidden states. Furthermore, the attention mechanism (Bahdanau et al., 2015; Sutskever et al., 2014) has been adapted for these CNN/RNN structures for text classification (Lin et al., 2017), providing high interpretability and allowing us to inspect which parts of the text are discriminative for a particular sample. In addition, external knowledge has been examined as a way to boost the performance of text classifiers (Collobert and Weston, 2008a; Ngiam et al., 2011; Howard and Ruder, 2018). One form of external knowledge is built on top of the hierarchical relations of the classes (Koller and Sahami, 1997), where a class taxonomy is used to improve the performance of the end-level classification1 . Most of the hierarchical classifiers2 perform classification by navigating through the hierarchy in top-down approaches (Liu et al., 2001; Quinn and Laier, 2006; Vens et al., 2008), where a local classifier is constructed at each parent node. The state-ofthe-art hierarchical classifier HDLTex is proposed by Kowsari et al. (2017). It combines deep neural networks in the top-down fashi"
D18-1094,N16-1174,0,0.0605563,"n tokens D = (w1 , w2 , ..., wn ) and its category labels of m levels C = (c1 , . . . , cm ), ck ∈ {cl1k , . . . , clskk } where lk indicates the k-th level of the class taxonomy and sk represents the number of classes in level k 3 . A bidirectional LSTM is 1 Classifiers that do not take into account the hierarchy and are only concerned with predicting the leaf nodes are termed flat classifiers in this work. 2 We use the term “hierarchical classifiers” to refer the models that follow the external taxonomy of class labels, which is substantially different from hierarchical attention networks (Yang et al., 2016). In Yang et al. (2016), hierarchical attention networks refer to the hierarchical nature of their attention mechanism; the model attends to the sentences first and then attends to the words. 3 We suppose wi and ci are word embeddings and class embedding respectively. 818 first used to extract features of the document: −−−→ → − −−−−→ ht = LST M (wt , ht−1 ), (1) ←−−− ← − ←−−−− ht = LST M (wt , ht+1 ). Level 1 Categories Level 2 Categories Level 3 Categories Number of documents Mean document length The encoder’s hidden states H = (h1 , . . . , hn ) are → − ← − constructed by the concatenation o"
D18-1220,S12-1063,0,0.0746137,"Missing"
D18-1220,P12-1041,0,0.0452813,"Missing"
D18-1220,J90-1003,0,0.0956742,"assess common-sense reasoning. These include Pronoun Disambiguation Problems (more generalized, Winograd-like passages without the 2 This is why we do not evaluate our method directly on the expanded corpus. twist of a special word or twin) (Morgenstern et al., 2016), the Narrative cloze task (Taylor, 1953), or its more difficult counterpart, the NarrativeQA Reading Comprehension Challenge (Koˇcisk`y et al., 2017). The COPA task was proposed by Roemmele et al. (2011), who also measured the performance of several systems. The most successful used Pointwise Mutual Information (PMI) statistics (Church and Hanks, 1990) between words in the premise and each alternative obtained from a large text corpus (as an implicit way to estimate causal association). More recent work showed that applying the same PMI-based technique on a corpus of stories yields better results (Gordon et al., 2011). The current state-of-the-art approaches leverage co-occurrence statistics extracted using causal cues (Luo et al., 2016; Sasaki et al., 2017). Extended Work: Previously, Emami et al. (2018) proposed a similar knowledge hunting framework to tackle the Winograd Schema Challenge. This work modifies and extends their approach. Ou"
D18-1220,J89-3002,0,0.553297,"Missing"
D18-1220,N18-4004,1,0.770728,"(2011), who also measured the performance of several systems. The most successful used Pointwise Mutual Information (PMI) statistics (Church and Hanks, 1990) between words in the premise and each alternative obtained from a large text corpus (as an implicit way to estimate causal association). More recent work showed that applying the same PMI-based technique on a corpus of stories yields better results (Gordon et al., 2011). The current state-of-the-art approaches leverage co-occurrence statistics extracted using causal cues (Luo et al., 2016; Sasaki et al., 2017). Extended Work: Previously, Emami et al. (2018) proposed a similar knowledge hunting framework to tackle the Winograd Schema Challenge. This work modifies and extends their approach. Our modifications include a queryfiltering step and various other tweaks that improve results by 0.05 F1 for our best model. In addition, we added further experiments and an ablation study that explores the performance of different model components. Finally, we adapted our method to a new dataset, COPA, on which we achieve respectable results. Accordingly, we change the general takeaway of the previous work from a method with strong performance on a single dat"
D18-1220,Y14-1042,0,0.104558,"tudents were bullying the younger ones, so we rescued them. (Answer: the younger ones) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like golfers. (Answer: shepherds) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like dogs. (Answer: sheep) Table 1: Examples of Winograd instances. for manually selected subsets that demand a specific type of reasoning (Sharma et al., 2015; Liu et al., 2016). Others have developed systems for relaxed common sense datasets with looser constraints (Rahman and Ng, 2012; Peng et al., 2015; Kruengkrai et al., 2014). In parallel, more general work on common sense reasoning aims to develop a repository of common knowledge using semi-automatic methods (e.g., Cyc (Lenat, 1995) and ConceptNet (Liu and Singh, 2004)). However, such knowledge bases are necessarily incomplete. In this work, we propose a general method to resolve common sense problems like WSC and COPA. Contrary to previous work, we aim to solve all problem instances rather than a restricted subset. Our method is based on on-the-fly knowledge hunting and operates in four stages. First, it parses an input problem into a representation schema. Next"
D18-1220,P14-5010,0,0.011738,"d and filtered. Finally, the snippets are resolved to their respective antecedents and the results are mapped to a best guess for the original instance’s resolution. We detail these stages below, grounding our description in Winograd instances. 1951 3.1 Semantic Representation Schema The first step is to perform a partial parse of each instance into a shallow semantic representation; that is, a general skeleton of each of the important semantic components in the order that they appear. This is performed using rules related to the syntactic parse of the sentence determined by Stanford CoreNLP (Manning et al., 2014). In general, Winograd instances can be separated into a context clause, which introduces the two competing antecedents, and a query clause, which contains the target pronoun to be resolved. We use the following notation to define the components in our representation schema: E1 , E2 the candidate antecedents P redC the context predicate + discourse connective P the target pronoun P redQ the query predicate E1 and E2 are noun phrases in the sentence. In the WSC, these two are specified without ambiguity. P redC is the context predicate composed of the verb phrase that relates both antecedents t"
D18-1220,W13-3517,0,0.0575485,"Missing"
D18-1220,N15-1082,0,0.214866,"udents) The older students were bullying the younger ones, so we rescued them. (Answer: the younger ones) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like golfers. (Answer: shepherds) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like dogs. (Answer: sheep) Table 1: Examples of Winograd instances. for manually selected subsets that demand a specific type of reasoning (Sharma et al., 2015; Liu et al., 2016). Others have developed systems for relaxed common sense datasets with looser constraints (Rahman and Ng, 2012; Peng et al., 2015; Kruengkrai et al., 2014). In parallel, more general work on common sense reasoning aims to develop a repository of common knowledge using semi-automatic methods (e.g., Cyc (Lenat, 1995) and ConceptNet (Liu and Singh, 2004)). However, such knowledge bases are necessarily incomplete. In this work, we propose a general method to resolve common sense problems like WSC and COPA. Contrary to previous work, we aim to solve all problem instances rather than a restricted subset. Our method is based on on-the-fly knowledge hunting and operates in four stages. First, it parses an input problem into a r"
D18-1220,D10-1048,0,0.0573222,"We run the above four processes on all snippets retrieved for the input Winograd instance. The sum of strengths for the evidence-agents is finally compared to that of the evidence-patients to make the resolution decision. 4 Experiments and Results We tested several versions of our framework on the original 273 Winograd sentences (135 pairs and one triple). These vary in the method of query generation: automatic vs. automatic with synonyms vs. manual. We compared these systems with previous work on the basis of Precision (P), Recall (R), and F1. We used Stanford CoreNLP’s coreference resolver (Raghunathan et al., 2010) during query generation to identify the predicates from the syntactic parse, as well as during antecedent selection to retrieve the coreference chain of a candi1954 date evidence sentence. Python’s Selenium package was used for web-scraping and Bing-USA and Google (top two pages per result) were the search engines. The search results comprise a list of document snippets that contain the queries (for example, “yelled at” and “upset”). We extract the sentence/s within each snippet that contain the query terms, with the added restriction that the terms should be within 70 characters of each othe"
D18-1220,D12-1071,0,0.477667,"(Answer: the older students) The older students were bullying the younger ones, so we rescued them. (Answer: the younger ones) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like golfers. (Answer: shepherds) Sam tried to paint a picture of shepherds with sheep, but they ended up looking more like dogs. (Answer: sheep) Table 1: Examples of Winograd instances. for manually selected subsets that demand a specific type of reasoning (Sharma et al., 2015; Liu et al., 2016). Others have developed systems for relaxed common sense datasets with looser constraints (Rahman and Ng, 2012; Peng et al., 2015; Kruengkrai et al., 2014). In parallel, more general work on common sense reasoning aims to develop a repository of common knowledge using semi-automatic methods (e.g., Cyc (Lenat, 1995) and ConceptNet (Liu and Singh, 2004)). However, such knowledge bases are necessarily incomplete. In this work, we propose a general method to resolve common sense problems like WSC and COPA. Contrary to previous work, we aim to solve all problem instances rather than a restricted subset. Our method is based on on-the-fly knowledge hunting and operates in four stages. First, it parses an inp"
D18-1220,D12-1113,0,0.0666866,"Missing"
D18-1409,P15-2136,0,0.239975,"ate concise summaries with paraphrasing. This work is primarily concerned with extractive ∗ Equal contribution. Jackie C.K. Cheung Mila/McGill University jcheung @cs.mcgill.ca summarization. Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences. Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017). Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences. Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapati et al., 2017). However, this approach has two well-recogniz"
D18-1409,W14-1504,0,0.0479482,"Missing"
D18-1409,P14-1062,0,0.0302712,"rs stems in part from better handling of summary-worthy sentences that come near the end of the document (see Section 7). 2 Related Work Extractive summarization has been widely studied in the past. Recently, neural network-based methods have been gaining popularity over classical methods (Luhn, 1958; Gong and Liu, 2001; Conroy and O’leary, 2001; Mihalcea and Tarau, 2004; Wong et al., 2008), as they have demonstrated stronger performance on large corpora. Central to the neural network-based models is the encoderdecoder structure. These models typically use either a convolution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al., 3740 2014) to represent words at the input level. These vectors are then fed into a decoder network to generate the output summary. The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the"
D18-1409,D14-1181,0,0.00627274,"r handling of summary-worthy sentences that come near the end of the document (see Section 7). 2 Related Work Extractive summarization has been widely studied in the past. Recently, neural network-based methods have been gaining popularity over classical methods (Luhn, 1958; Gong and Liu, 2001; Conroy and O’leary, 2001; Mihalcea and Tarau, 2004; Wong et al., 2008), as they have demonstrated stronger performance on large corpora. Central to the neural network-based models is the encoderdecoder structure. These models typically use either a convolution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al., 3740 2014) to represent words at the input level. These vectors are then fed into a decoder network to generate the output summary. The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the TD(λ) algor"
D18-1409,W04-1013,0,0.416602,"ent parameters θ: igreedy = arg max pθ (i|d) (7) i and calculate the baseline for the current update as r = R(igreedy , a). This baseline has the intuitively satisfying property of only increasing the probability of a sampled label sequence when the summary it induces is better than what would be obtained by greedy decoding. 3.4 Reward Function A final consideration is a concrete choice for the reward function R(i, a). Throughout this work we use: 1 R(i, a) = (ROUGE-1f (i, a) + 3 ROUGE-2f (i, a) + ROUGE-Lf (i, a)). (8) The above reward function optimizes the average of all the ROUGE variants (Lin, 2004) while balancing precision and recall. 4 Model In this section, we discuss the concrete instantiations of the neural network πθ that we use in our experiments. We break πθ up into two components: a document encoder fθ1 , which outputs a sequence of sentence feature vectors (h1 , . . . , hNd ) and a decoder gθ2 which yields sentence affinities: h1 , . . . , hNd = fθ1 (d) πθ (d) = gθ2 (h1 , . . . , hNd ) (9) (10) Encoder. Features for each sentence in isolation are first obtained by applying a word-level Bidirectional Recurrent Neural Network (BiRNN) to the embeddings for the words in the senten"
D18-1409,P18-1063,0,0.0955057,", who proposed to use the TD(λ) algorithm to learn a value function for sentence selection. Rioux et al. (2014) improved this framework by replacing the learning agent with another TD(λ) algorithm. However, the performance of their methods was limited by the use of shallow function approximators, which required performing a fresh round of reinforcement learning for every new document to be summarized. The more recent work of Paulus et al. (2018) and Wu and Hu (2018) use reinforcement learning in a sequential labeling setting to train abstractive and extractive summarizers, respectively, while Chen and Bansal (2018) combines both approaches, applying abstractive summarization to a set of sentences extracted by a pointer network (Vinyals et al., 2015) trained via REINFORCE. However, pre-training with a maximum likelihood objective is required in all of these models. The two works most similar to ours are Yao et al. (2018) and Narayan et al. (2018). Yao et al. (2018) recently proposed an extractive summarization approach based on deep Q learning, a type of reinforcement learning. However, their approach is extremely computationally intensive (a minimum of 10 days before convergence), and was unable to achi"
D18-1409,W04-3252,0,0.831352,"cting and copying text snippets from the document, while abstractive methods aim to generate concise summaries with paraphrasing. This work is primarily concerned with extractive ∗ Equal contribution. Jackie C.K. Cheung Mila/McGill University jcheung @cs.mcgill.ca summarization. Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences. Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017). Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences. Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and La"
D18-1409,P16-1046,0,0.603258,"Tarau, 2004; Wong et al., 2008; K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017). Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences. Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapati et al., 2017). However, this approach has two well-recognized disadvantages. First, it suffers from exposure bias, a form of mismatch between training and testing data distributions which can hurt performance (Ranzato et al., 2015; Bahdanau et al., 2017; Paulus et al., 2018). Second, extractive labels must be generated by a heuristic, as summarization datasets do not generally include ground-truth extractive labels; the ultimate performance of models trained on such labels is thus fundamentally limited by the quality of the heuristic. An alternative to maximum likelihood training 3"
D18-1409,K16-1028,0,0.143425,"Missing"
D18-1409,N18-1158,0,0.322713,"ently, neural network-based methods have been gaining popularity over classical methods (Luhn, 1958; Gong and Liu, 2001; Conroy and O’leary, 2001; Mihalcea and Tarau, 2004; Wong et al., 2008), as they have demonstrated stronger performance on large corpora. Central to the neural network-based models is the encoderdecoder structure. These models typically use either a convolution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al., 3740 2014) to represent words at the input level. These vectors are then fed into a decoder network to generate the output summary. The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the TD(λ) algorithm to learn a value function for sentence selection. Rioux et al. (2014) improved this framework by replacing the learning agent with another TD(λ) algorithm. However, the performance"
D18-1409,D14-1162,0,0.0810841,"ongest common subsequence overlapping with the reference summaries. 5.3 Baselines We compare BANDIT S UM with other extractive methods including: the Lead-3 model, SummaRuNNer (Nallapati et al., 2017), Refresh 1 https://pypi.python.org/pypi/pyrouge/ 0.1.3 2 We use the modified version based on https:// github.com/pltrdy/rouge 3743 Model (Narayan et al., 2018), RNES (Wu and Hu, 2018), DQN (Yao et al., 2018), and NN-SE (Cheng and Lapata, 2016). The Lead-3 model simply produces the leading three sentences of the document as the summary. 5.4 Model Settings We use 100-dimensional Glove embeddings (Pennington et al., 2014) as our embedding initialization. We do not limit the sentence length, nor the maximum number of sentences per document. We use one-layer BiLSTM for word-level RNN, and two-layers BiLSTM for sentence-level RNN. The hidden state dimension is 200 for each direction on all LSTMs. For the decoder, we use a feedforward network with one hidden layer of dimension 100. During training, we use Adam (Kingma and Ba, 2015) as the optimizer with the learning rate of 5e−5 , beta parameters (0, 0.999), and a weight decay of 1e−6 , to maximize the objective function defined in equation (1). We employ gradient"
D18-1409,C08-1124,0,0.348783,"ppets from the document, while abstractive methods aim to generate concise summaries with paraphrasing. This work is primarily concerned with extractive ∗ Equal contribution. Jackie C.K. Cheung Mila/McGill University jcheung @cs.mcgill.ca summarization. Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences. Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017). Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences. Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapa"
D18-1409,K17-1045,0,0.0516673,"ies with paraphrasing. This work is primarily concerned with extractive ∗ Equal contribution. Jackie C.K. Cheung Mila/McGill University jcheung @cs.mcgill.ca summarization. Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences. Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017). Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences. Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapati et al., 2017). However, this approach has two well-recognized disadvantages. First,"
D18-1409,D14-1075,0,0.0745718,"Missing"
D18-1409,D12-1024,0,0.0365743,"volution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al., 3740 2014) to represent words at the input level. These vectors are then fed into a decoder network to generate the output summary. The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the TD(λ) algorithm to learn a value function for sentence selection. Rioux et al. (2014) improved this framework by replacing the learning agent with another TD(λ) algorithm. However, the performance of their methods was limited by the use of shallow function approximators, which required performing a fresh round of reinforcement learning for every new document to be summarized. The more recent work of Paulus et al. (2018) and Wu and Hu (2018) use reinforcement learning in a sequential labeling setting to train abstractive and extractive summarizers, respectively, while"
D18-1409,P17-1099,0,0.615454,"Experiments In this section, we discuss the setup of our experiments. We first discuss the corpora that we used and our evaluation methodology. We then discuss the baseline methods against which we compared, and conclude with a detailed overview of the settings of the model parameters. 5.1 Corpora Three datasets are used for our experiments: the CNN, the Daily Mail, and combined CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016). We use the standard split of Hermann et al. (2015) for training, validating, and testing and the same setting without anonymization on the three corpus as See et al. (2017). The Daily Mail corpus has 196,557 training documents, 12,147 validation documents and 10,397 test documents; while the CNN corpus has 90,266/1,220/1,093 documents, respectively. 5.2 Evaluation The models are evaluated based on ROUGE (Lin, 2004). We obtain our ROUGE scores using the standard pyrouge package1 for the test set evaluation and a faster python implementation of the ROUGE metric2 for training and evaluating on the validation set. We report the F1 scores of ROUGE1, ROUGE-2, and ROUGE-L, which compute the uniform, bigram, and longest common subsequence overlapping with the reference"
D19-1312,P16-1054,0,0.215713,"erring expression; that is, what type of reference should be used (e.g., a proper name). The second is to determine the content of the referring expression (e.g., Ernest Hemingway). Many computational approaches, both rulebased and machine-learning-based, have been proposed for REG. Rule-based models use predefined heuristics and algorithms to determine referential form (Reiter and Dale, 2000; Henschel et al., 2000; Callaway and Lester, 2002). Machine learning-based approaches require training on samples to predict referring expressions (Nenkova and McKeown, 2003; Greenbacker and McCoy, 2009; Ferreira et al., 2016, 2017), often as a classification task. The common limitation of previous REG systems is that they are incapable of generating referring expressions for new, unseen entities. Previous REG setups have tended to focus on form selection rather than content generation, or else they provide a static list of attributes or realization options for models to select from (Belz et al., 2010; Gatt and Belz, 2008). The recent NeuralREG model generates referring expressions in an integrated, endto-end setup, but it requires seeing instances of the entity being referred to in the training set (Ferreira et a"
D19-1312,E17-1062,0,0.12831,"k and Goodman (2012) proposed a rational speaker-listener model 1 https://github.com/mcao610/ProfileREG based on the assumptions that speakers attempt to be informative and that listeners use Bayesian inference to recover the speakers’ intended referents. Orita et al. (2015) extended the previous model by introducing the referent’s discourse salience and a cost term. Ferreira et al. (2016) proposed a naive Bayes model to predict the probability of a particular referential form given a set of feature values, including syntactic position, referential status and recency of references. Similarly, Ferreira et al. (2017) used a naive Bayes model based on syntactic position and referential status to predict the form of a proper name reference. In recent years, deep neural networks have achieved great success in a variety of NLP applications (e.g., machine translation (Bahdanau et al., 2014) and automatic summarization (Rush et al., 2015; See et al., 2017)). To the best of our knowledge, there have only been two models that use deep neural networks for REG. The first is by Ferreira et al. (2016), who use recurrent neural networks (RNNs) to predict the form of referring expressions. They use RNNs to encode a seq"
D19-1312,P18-1182,0,0.55922,"Missing"
D19-1312,P17-1017,0,0.299601,"g Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics should be handled by an in-domain NLG system. We make the minimal assumption that the profile contains just a few sentences about the entity, so that these profiles can easily be written by nonexperts or be automatically extracted. Our model, P ROFILE REG, uses a learned switch variable to decide whether to generate a token from a fixed vocabulary, generate a pronoun, or use information from the profile in order to generate appropriate referring expressions in context. We evaluate our model on the WebNLG corpus (Gardent et al., 2017a). Experimental results show that our model is capable of handling unseen entities that prior work simply cannot handle in our new evaluation setups, while also outperforming them in the original setting.1 Our contributions are as follows. First, we address an important limitation in prior REG studies by creating new test setups that evaluate neural REG models specifically on entities that are not seen in the training set. Second, we propose a new REG model, P ROFILE REG, which outperforms existing REG models in all tested settings according to automatic and human evaluation measures. 2 Relat"
D19-1312,W17-3518,0,0.193591,"g Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics should be handled by an in-domain NLG system. We make the minimal assumption that the profile contains just a few sentences about the entity, so that these profiles can easily be written by nonexperts or be automatically extracted. Our model, P ROFILE REG, uses a learned switch variable to decide whether to generate a token from a fixed vocabulary, generate a pronoun, or use information from the profile in order to generate appropriate referring expressions in context. We evaluate our model on the WebNLG corpus (Gardent et al., 2017a). Experimental results show that our model is capable of handling unseen entities that prior work simply cannot handle in our new evaluation setups, while also outperforming them in the original setting.1 Our contributions are as follows. First, we address an important limitation in prior REG studies by creating new test setups that evaluate neural REG models specifically on entities that are not seen in the training set. Second, we propose a new REG model, P ROFILE REG, which outperforms existing REG models in all tested settings according to automatic and human evaluation measures. 2 Relat"
D19-1312,W08-1108,0,0.021926,"et al., 2000; Callaway and Lester, 2002). Machine learning-based approaches require training on samples to predict referring expressions (Nenkova and McKeown, 2003; Greenbacker and McCoy, 2009; Ferreira et al., 2016, 2017), often as a classification task. The common limitation of previous REG systems is that they are incapable of generating referring expressions for new, unseen entities. Previous REG setups have tended to focus on form selection rather than content generation, or else they provide a static list of attributes or realization options for models to select from (Belz et al., 2010; Gatt and Belz, 2008). The recent NeuralREG model generates referring expressions in an integrated, endto-end setup, but it requires seeing instances of the entity being referred to in the training set (Ferreira et al., 2018). In this work, we address this problem by proposing new REG task setups which test for REG systems’ ability to handle new entities at test time. We also develop a REG system which can handle entities not seen during training using external knowledge about them, generated using extracts of their Wikipedia page. From a practical perspective, it is reasonable to assume that such an entity profil"
D19-1312,P15-1158,0,0.0519523,"Missing"
D19-1312,D14-1162,0,0.0820094,"l and F1-score. The name and pronoun accuracy are computed by comparing names and pronouns in the test set with the generated referring expressions. Pronoun classification evaluation takes all generated referring expressions as two forms: pronoun or non-pronoun and do not consider the actual content. 5.3 Experiment Setting For all experiments, our model has 100dimensional hidden states for the encoder and decoder LSTMs and 50 dimensional for the character LSTM. The word and character embeddings are both set to 100-dimensional. We initialize the word embeddings using pre-trained GloVe vectors (Pennington et al., 2014) and all character embeddings are randomly initialized. The model is trained for up to 35 epochs, or until the performance on the development set does not improve for 5 epochs. We set the learning rate to 0.0037 and the batch size to 64. We apply gradient clipping with a maximum gradient norm of 5. We adopt dropout training with a dropout rate of 0.5. At test time, all referring expressions are generated using greedy decoding. These settings were selected by tuning on the development set. 6 6.1 Results Automatic Evaluation As shown in Table 2 P ROFILE REG outperforms the three baselines in all"
D19-1312,D15-1044,0,0.0371474,"erent’s discourse salience and a cost term. Ferreira et al. (2016) proposed a naive Bayes model to predict the probability of a particular referential form given a set of feature values, including syntactic position, referential status and recency of references. Similarly, Ferreira et al. (2017) used a naive Bayes model based on syntactic position and referential status to predict the form of a proper name reference. In recent years, deep neural networks have achieved great success in a variety of NLP applications (e.g., machine translation (Bahdanau et al., 2014) and automatic summarization (Rush et al., 2015; See et al., 2017)). To the best of our knowledge, there have only been two models that use deep neural networks for REG. The first is by Ferreira et al. (2016), who use recurrent neural networks (RNNs) to predict the form of referring expressions. They use RNNs to encode a sequence of discourse features and apply a softmax layer to generate referential form distribution. The other is the NeuralREG model proposed by Ferreira et al. (2018), an end-to-end system that predicts both the form and content of the referring expression. NeuralREG has an encoder-decoder structure, using LSTM units to e"
D19-1312,P17-1099,0,0.149176,"alience and a cost term. Ferreira et al. (2016) proposed a naive Bayes model to predict the probability of a particular referential form given a set of feature values, including syntactic position, referential status and recency of references. Similarly, Ferreira et al. (2017) used a naive Bayes model based on syntactic position and referential status to predict the form of a proper name reference. In recent years, deep neural networks have achieved great success in a variety of NLP applications (e.g., machine translation (Bahdanau et al., 2014) and automatic summarization (Rush et al., 2015; See et al., 2017)). To the best of our knowledge, there have only been two models that use deep neural networks for REG. The first is by Ferreira et al. (2016), who use recurrent neural networks (RNNs) to predict the form of referring expressions. They use RNNs to encode a sequence of discourse features and apply a softmax layer to generate referential form distribution. The other is the NeuralREG model proposed by Ferreira et al. (2018), an end-to-end system that predicts both the form and content of the referring expression. NeuralREG has an encoder-decoder structure, using LSTM units to encode contextual in"
D19-1312,C00-1045,0,0.909176,"copying a word from the profile. We evaluate our model on three different splits of the WebNLG dataset, and show that it outperforms competitive baselines in all settings according to automatic and human evaluations. 1 Introduction Entities can be expressed by various types of linguistic expressions, including by their names (e.g., Barrack Obama), a definite description (e.g., the former president of the United States), a pronoun (e.g., he, him, his), or a demonstrative (e.g., that person). Many factors play a role in determining what type of expression is appropriate in a particular context (Henschel et al., 2000), including information status, familiarity with the entity, and referential clarity. In this study, we aim to design a model that can generate appropriate referring expressions of entities in an extended passage. Such a system is useful in a variety of natural language generation settings, from dialogue systems to automatic summarization applications (Reiter and Dale, 2000; Krahmer and Van Deemter, 2012). Referring expression generation (REG) can be broken into two steps. The first is to decide the form of referring expression; that is, what type of reference should be used (e.g., a proper na"
D19-1312,J12-1006,0,0.189843,"Missing"
D19-1312,D16-1128,0,0.0740765,"Missing"
D19-1312,N03-2024,0,0.383763,"n into two steps. The first is to decide the form of referring expression; that is, what type of reference should be used (e.g., a proper name). The second is to determine the content of the referring expression (e.g., Ernest Hemingway). Many computational approaches, both rulebased and machine-learning-based, have been proposed for REG. Rule-based models use predefined heuristics and algorithms to determine referential form (Reiter and Dale, 2000; Henschel et al., 2000; Callaway and Lester, 2002). Machine learning-based approaches require training on samples to predict referring expressions (Nenkova and McKeown, 2003; Greenbacker and McCoy, 2009; Ferreira et al., 2016, 2017), often as a classification task. The common limitation of previous REG systems is that they are incapable of generating referring expressions for new, unseen entities. Previous REG setups have tended to focus on form selection rather than content generation, or else they provide a static list of attributes or realization options for models to select from (Belz et al., 2010; Gatt and Belz, 2008). The recent NeuralREG model generates referring expressions in an integrated, endto-end setup, but it requires seeing instances of the entity"
D19-1335,D18-1220,1,0.674323,"ze stylistic artifacts, adversarial filtering, is based on fooling a discriminator that classifies successors as human- or LM-generated. Nevertheless, upon inspecting the data, we found that LM-generated successors still contain repeated tokens and other signatures. A model that exploits these patterns could perform well without using 3383 any common sense. An example regularity found in the WSC is that instances are often composed of two clauses connected by a causal discourse connective, like because (as in (1)). This allows for simplifying assumptions (Liu et al., 2016) or schematizations (Emami et al., 2018). The issue with exploiting these structural regularities is that systems become brittle to perturbations that would not affect the judgment of a human. Limited Size. Comprising only 273 test instances, the main drawback of the Winograd Schema Challenge is its limited size and the absence of training and validation sets for hyperparameter tuning. As a result, achieving above random accuracy on the WSC does not necessarily correspond to capturing common sense; it could be the result of a lucky draw.1 Associativity. The WSC task definition specifies that instances should not be resolvable via st"
D19-1335,P18-2103,0,0.0195292,"ove chance level can be achieved by this deficient model. 2 Related Work Our work presents new findings that reinforce realizations made in the community concerning the validity of a variety of different CSR tasks, most of which are in Natural Language Inference (NLI); some of these include that state-of-the-art models often do very well while being either agnostic to the premises in the task instance (which should be crucial for resolution) or by using linguistic cues that have little or nothing to do with worldknowledge or common-sense reasoning (Gururangan et al., 2018). In similar spirit, Glockner et al. (2018) create an NLI test set specifically to show the deficiencies of state-of-the-art models in inferences that require lexical and world knowledge. Alternatively, validity checks through manual investigation as in (Kalouli et al., 2017) have revealed another NLI corpus to be vulnerable to errors and model exploitation. To the best of our knowledge, our work is the first analysis performed on two very popular CSR tasks, the WSC and SWAG, that have recently garnered considerable attention in the community and on which we are beginning to see models perform relatively well (Trinh and Le, 2018; Zelle"
D19-1335,N18-2017,0,0.037753,"most (but not all) of the performance gain above chance level can be achieved by this deficient model. 2 Related Work Our work presents new findings that reinforce realizations made in the community concerning the validity of a variety of different CSR tasks, most of which are in Natural Language Inference (NLI); some of these include that state-of-the-art models often do very well while being either agnostic to the premises in the task instance (which should be crucial for resolution) or by using linguistic cues that have little or nothing to do with worldknowledge or common-sense reasoning (Gururangan et al., 2018). In similar spirit, Glockner et al. (2018) create an NLI test set specifically to show the deficiencies of state-of-the-art models in inferences that require lexical and world knowledge. Alternatively, validity checks through manual investigation as in (Kalouli et al., 2017) have revealed another NLI corpus to be vulnerable to errors and model exploitation. To the best of our knowledge, our work is the first analysis performed on two very popular CSR tasks, the WSC and SWAG, that have recently garnered considerable attention in the community and on which we are beginning to see models perform"
D19-1335,W17-6915,0,0.0524678,"Missing"
D19-1335,D12-1071,0,0.489907,"nly 273 test instances, the main drawback of the Winograd Schema Challenge is its limited size and the absence of training and validation sets for hyperparameter tuning. As a result, achieving above random accuracy on the WSC does not necessarily correspond to capturing common sense; it could be the result of a lucky draw.1 Associativity. The WSC task definition specifies that instances should not be resolvable via statistics that associate a candidate antecedent to other components of the sentence (Levesque et al., 2011). For example, in “The lions ate the zebras because they are predators” (Rahman and Ng, 2012), the pronoun they can be resolved to lions on the basis of a much stronger association of lions with predators than of zebras with predators. We will call this (flawed) type of instance associative (or non-Google-proof in (Levesque et al., 2011)). Although the WSC should contain no associative sentences, there was no rigorous enforcement of this constraint. We therefore sought to quantify the associative proportion. We only consider sentences to be associative if there is a clear argument for one antecedent being statistically preferred. Table 1 outlines some examples and gives the associativ"
D19-1335,P18-1043,0,0.0873306,"Missing"
D19-1335,D19-1454,0,0.0280361,"Missing"
D19-1335,W18-5446,0,0.0918376,"Missing"
D19-1335,D18-1009,0,0.170743,"opular binary-choice pronoun co-reference problem called the Winograd Schema Challenge (WSC) (Levesque et al., 2011), designed to directly test a machine’s grasp of common sense. What makes sentences like (1) especially challenging for machine learning approaches is that they are formulated such that simple word co-occurrence statistics cannot resolve them at a rate above chance (i.e., the delivery truck is unlikely to co-occur with going so fast much more frequently than the school bus does in large text corpora). In the same vein, a recently proposed common-sense inference task called SWAG (Zellers et al., 2018) further challenges co-occurrence-based approaches. SWAG’s problem instances comprise a partial description, along with four candidate succeeding sentences designed to be distributionally similar. Among these, one successor is the most plausible. An example SWAG instance follows. (2) Someone is lifting the pinata. The pinata a) drops from the swings. b) bounces bigger than a third. c) slumps across his shoulder back. d) falls on the ground. Recently, a number of systems have attained new state-of-the-art results on WSC and SWAG by querying a language model trained on a very large corpus (Trinh"
D19-1620,D12-1091,0,0.0165501,"highest average of ROUGE-1, -2 and -L scores against the abstractive gold standard. 7 Results and Discussion Table 3 reports the F1 scores for ROUGE-1,-2 and -L (Lin, 2004). We use the pyrouge3 wrapper library to evaluate the final models, while training with a faster Python-only implementation4 . We test for significance between the baseline models and our proposed techniques using the bootstrap method. This method was first recommended for testing significance in ROUGE scores by Lin (2004), and has subsequently been advocated as an appropriate measure in works such as Dror et al. (2018) and Berg-Kirkpatrick et al. (2012). 2 We are unable to evaluate this model on the lead overlap measure due to lack of access to the model outputs. 3 www.github.com/bheinzerling/pyrouge 4 www.github.com/Diego999/py-rouge Figure 1: Training curves for BanditSum based models. Average ROUGE is the average of ROUGE-1, -2 and -L F1. The simple entropy regularizer has a small but not significant improvement, and pretraining has a similar improvement only for RNES. But the auxiliary ROUGE loss significantly (p < 0.001) improves over BanditSum, obtaining an extra 0.15 ROUGE points on average. The last column reports the percentage of s"
D19-1620,P15-2136,0,0.0374767,"Missing"
D19-1620,P16-1046,0,0.149251,"Missing"
D19-1620,D18-1409,1,0.897238,"Missing"
D19-1620,P18-1128,0,0.0141444,"rce sentences with the highest average of ROUGE-1, -2 and -L scores against the abstractive gold standard. 7 Results and Discussion Table 3 reports the F1 scores for ROUGE-1,-2 and -L (Lin, 2004). We use the pyrouge3 wrapper library to evaluate the final models, while training with a faster Python-only implementation4 . We test for significance between the baseline models and our proposed techniques using the bootstrap method. This method was first recommended for testing significance in ROUGE scores by Lin (2004), and has subsequently been advocated as an appropriate measure in works such as Dror et al. (2018) and Berg-Kirkpatrick et al. (2012). 2 We are unable to evaluate this model on the lead overlap measure due to lack of access to the model outputs. 3 www.github.com/bheinzerling/pyrouge 4 www.github.com/Diego999/py-rouge Figure 1: Training curves for BanditSum based models. Average ROUGE is the average of ROUGE-1, -2 and -L F1. The simple entropy regularizer has a small but not significant improvement, and pretraining has a similar improvement only for RNES. But the auxiliary ROUGE loss significantly (p < 0.001) improves over BanditSum, obtaining an extra 0.15 ROUGE points on average. The last"
D19-1620,E14-1075,0,0.0976512,"Missing"
D19-1620,P14-1062,0,0.0272008,"Missing"
D19-1620,D18-1208,0,0.0708357,"31.11 30.09 21.65 31.59 32.15 25.76 28.71 30.09 24.91 29.00 28.63 5.00 4.72 0.42 4.72 4.93 4.98 Table 2: BanditSum’s performance—calculated as the average between ROUGE-1,-2, and -L F1—on the validation set of the CNN/Daily Mail corpus. The sentence position information is perturbed at different levels, as explained in Section 4. worse when tested on a mismatched data perturbation. Even when the distortion is at a single lead position in insert-lead and insert-lead3, the performance on the original data is significantly lower than when trained without the distortion. These results corroborate Kedzie et al. (2018)’s findings for RL-based systems. Interestingly, the random model has the best mean performance and the lowest variation indicating that completely removing the position bias may allow a model to focus on learning robust sentence semantics. 5 Learning to Counter Position Bias We present two methods which encourage models to locate key phrases at diverse parts of the article. 5.1 Multi-Stage Training This technique is inspired by the robust results from the random model in section 4. We implement a multi-stage training method for both BanditSum and RNES where in the first few epochs, we train o"
D19-1620,D14-1181,0,0.00776977,"Missing"
D19-1620,Q18-1017,0,0.0548552,"Missing"
D19-1620,W04-1013,0,0.0309211,"eplacing LKL with the negated entropy of PM in eq. 2. This loss penalizes low entropy, helping the model explore, but it is ‘undirected’ compared to our proposed method. We present the results of Lead-3 baseline (first 3 sentences), and two other competitive models—Refresh2 (Narayan et al., 2018a) and NeuSum (Zhou et al., 2018). Lastly, we include results from an oracle summarizer, computed as the triplet of source sentences with the highest average of ROUGE-1, -2 and -L scores against the abstractive gold standard. 7 Results and Discussion Table 3 reports the F1 scores for ROUGE-1,-2 and -L (Lin, 2004). We use the pyrouge3 wrapper library to evaluate the final models, while training with a faster Python-only implementation4 . We test for significance between the baseline models and our proposed techniques using the bootstrap method. This method was first recommended for testing significance in ROUGE scores by Lin (2004), and has subsequently been advocated as an appropriate measure in works such as Dror et al. (2018) and Berg-Kirkpatrick et al. (2012). 2 We are unable to evaluate this model on the lead overlap measure due to lack of access to the model outputs. 3 www.github.com/bheinzerling"
D19-1620,K16-1028,0,0.0809297,"Missing"
D19-1620,D18-1206,0,0.170281,"d, the pretraining approach produces mixed results. We also confirm that when summary-worthy sentences appear late, there is a large performance discrepancy between the oracle summary and state-of-the-art summarizers, indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle. 2 Related Work Modern summarization methods for news are typically based on neural network-based sequenceto-sequence learning (Kalchbrenner et al., 2014; Kim, 2014; Chung et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a; Zhou et al., 2018). In MLE-based training, extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically-generated groundtruth binary labels (Nallapati et al., 2017). Many MLE-based models do not perform as well as their reinforcement learning-based (RL) competitors that directly optimize ROUGE (Paulus et al., 2018; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018). As RL-based models represent the state of the art for extractive summarization, we analyze them in this paper. The closest work to ours is a recent study by Kedzie et al. (2018) whi"
D19-1620,N18-1158,0,0.129587,"d, the pretraining approach produces mixed results. We also confirm that when summary-worthy sentences appear late, there is a large performance discrepancy between the oracle summary and state-of-the-art summarizers, indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle. 2 Related Work Modern summarization methods for news are typically based on neural network-based sequenceto-sequence learning (Kalchbrenner et al., 2014; Kim, 2014; Chung et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a; Zhou et al., 2018). In MLE-based training, extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically-generated groundtruth binary labels (Nallapati et al., 2017). Many MLE-based models do not perform as well as their reinforcement learning-based (RL) competitors that directly optimize ROUGE (Paulus et al., 2018; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018). As RL-based models represent the state of the art for extractive summarization, we analyze them in this paper. The closest work to ours is a recent study by Kedzie et al. (2018) whi"
D19-1620,P18-1061,0,0.105675,"oach produces mixed results. We also confirm that when summary-worthy sentences appear late, there is a large performance discrepancy between the oracle summary and state-of-the-art summarizers, indicating that learning to balance lead bias with other features of news text is a noteworthy issue to tackle. 2 Related Work Modern summarization methods for news are typically based on neural network-based sequenceto-sequence learning (Kalchbrenner et al., 2014; Kim, 2014; Chung et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a; Zhou et al., 2018). In MLE-based training, extractive summarizers are trained with gradient ascent to maximize the likelihood of heuristically-generated groundtruth binary labels (Nallapati et al., 2017). Many MLE-based models do not perform as well as their reinforcement learning-based (RL) competitors that directly optimize ROUGE (Paulus et al., 2018; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018). As RL-based models represent the state of the art for extractive summarization, we analyze them in this paper. The closest work to ours is a recent study by Kedzie et al. (2018) which showed that MLEbas"
D19-6015,P01-1046,0,0.277401,"Missing"
D19-6015,E99-1005,0,0.247003,"mbiguation task, where attested predicate-argument tuples must be disambiguated from pseudo-negative random tuples, or evaluated on their correlation with human plausibility judgments. Selectional preference is one factor in plausibility and thus the two should correlate. Distributional Models Motivated by the distributional hypothesis that words in similar contexts have similar meanings (Harris, 1954), distributional methods learn the representation of a word based on the distribution of its context. The occurrence counts of bigrams in a corpus are correlated with human plausibility ratings (Lapata et al., 1999, 2001), so one might expect that with a large enough corpus, a distributional model would learn to distinguish plausible but atypical events from implausible ones. As ´ S´eaghdha (2010) has shown a counterexample, O that the subject-verb bigram carrot-laugh occurs 855 times in a web corpus, while manservantlaugh occurs zero.1 Not everything that is physically plausible occurs, and not everything that occurs is attested due to reporting bias2 (Gordon and Van Durme, 2013); therefore, modeling semantic plausibility requires systematic inference beyond a distributional cue. We focus on the masked"
D19-6015,D14-1004,0,0.162999,"Missing"
D19-6015,N15-1098,0,0.157792,"Missing"
D19-6015,N19-1423,0,0.0779023,"Missing"
D19-6015,D17-1019,0,0.0210352,"el is a common event, and one would expect the subject-verb-object (s-v-o) triple person-ride-camel to be attested in a large corpus. In contrast, gorilla-ride-camel is uncommon, likely unattested, and yet still semantically plausible. Modeling semantic plausibility then requires distinguishing these plausible events from the semantically nonsensical, e.g. lake-ridecamel. Semantic plausibility is a necessary part of many natural language understanding (NLU) tasks including narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), paragraph reconstruction (Li and Jurafsky, 2017), and hard coreference resolution (Peng Still, the generalization ability of supervised models is limited by the coverage of the training set. We therefore present the more difficult problem of learning physical plausibility directly from text. We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on Wang et al. (2018)’s physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010),"
D19-6015,J10-4007,0,0.149239,"Missing"
D19-6015,W17-2810,0,0.0264033,"November 3, 2019. 2019 Association for Computational Linguistics 2 Related Work marks (Rajpurkar et al., 2018; Wang et al., 2019), including tasks that require explicit commonsense reasoning such as the Winograd Schema Challenge (Sakaguchi et al., 2019). Wang et al. (2018) present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset. This conclusion aligns with other work showing that GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017). More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019). Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003), or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019). Complementary to our work are methods of extracting physical features from a text corpus (Wang et al., 2017; Forbes and Choi, 2017; Bagherinezhad et al., 2016). 2.1 2.2 Selectional Preference C"
D19-6015,P17-1025,0,0.0452068,"3) embeddings do not encode some salient features of objects (Li and Gauthier, 2017). More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019). Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003), or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019). Complementary to our work are methods of extracting physical features from a text corpus (Wang et al., 2017; Forbes and Choi, 2017; Bagherinezhad et al., 2016). 2.1 2.2 Selectional Preference Closely related to semantic plausibility is selectional preference (Resnik, 1996) which concerns the semantic preference of a predicate for its arguments. Here, preference refers to the typicality of arguments: while it is plausible that a gorilla rides a camel, it is not preferred. Current approaches to selectional preference are distributional (Erk et al., 2010; Van de Cruys, 2014) and have shown limited performance in capturing semantic plausibility (Wang et al., 2018). ´ S´eaghdha and Korhonen (2012) have invesO tigated combinin"
D19-6015,S18-2023,0,0.068008,"Missing"
D19-6015,P17-2003,0,0.0141026,"imate across all 20 runs of cross-validation. Of these, we present the event for which BERT was most confident. We note that due to the limited vocabulary size of the dataset, the training set always covers the test set vocabulary when performing 10-fold cross validation. That is to say that every word in the test set has been seen in a different triple in the training set. For example, every verb occurs within 20 triples; therefore, on average a verb in the test set has been seen 18 times in the training set. Supervised performance is dependent on the coverage of the training set vocabulary (Moosavi and Strube, 2017), and it is intractable to have 18 plausibility labels for all verbs across English. Furthermore, supervised models are susceptible to annotation artifacts (Gururangan et al., 2018; Poliak et al., 2018) and do not necessarily even learn 6 Learning from Text Conclusion We show that large, pretrained language models are effective at modeling semantic plausibility in the supervised setting. Supervised models are limited by the coverage of the training set, however; thus, we reframe modeling semantic plausibility as a self-supervised task and present a baseline based on a novel application of BERT"
D19-6015,K18-2016,0,0.0216944,"[CLS] token. We use BERT-large and finetune the entire model in training.4 and perform cross validation on the 3,062 labeled triples (Wang et al., 2018). 3.2 Learning from Text We also present the problem of learning to model physical plausibility directly from text. In this new setting, a model is trained on events extracted from a large corpus and evaluated on a physical plausibility task. Therefore, only the test set covers both typical and atypical plausibility. We create two training sets based on separate corpora: first, we parse English Wikipedia using the StanfordNLP neural pipeline (Qi et al., 2018) and extract attested s-v-o triples. Wikipedia has led to relatively good results for selectional preference (Zhang et al., 2019), and in total we extract 6 million unique triples with a cumulative 10 million occurrences. Second, we use the NELL (Carlson et al., 2010) dataset of 604 million s-v-o triples extracted from the dependency parsed ClueWeb09 dataset. For NELL, we filter out triples with nonalphabetic characters or less than 5 occurrences, resulting in a total 2.5 million unique triples with a cumulative 112 million occurrences. For evaluation, we split Wang et al. (2018)’s 3,062 tripl"
D19-6015,N16-1098,0,0.0240678,"Wang et al., 2018). Introduction A person riding a camel is a common event, and one would expect the subject-verb-object (s-v-o) triple person-ride-camel to be attested in a large corpus. In contrast, gorilla-ride-camel is uncommon, likely unattested, and yet still semantically plausible. Modeling semantic plausibility then requires distinguishing these plausible events from the semantically nonsensical, e.g. lake-ridecamel. Semantic plausibility is a necessary part of many natural language understanding (NLU) tasks including narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), paragraph reconstruction (Li and Jurafsky, 2017), and hard coreference resolution (Peng Still, the generalization ability of supervised models is limited by the coverage of the training set. We therefore present the more difficult problem of learning physical plausibility directly from text. We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on Wang et al. (2018)’s physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as"
D19-6015,P18-2124,0,0.0825472,"Missing"
D19-6015,P19-1459,0,0.0611059,"Missing"
D19-6015,P10-1045,0,0.232412,"Missing"
D19-6015,S12-1025,0,0.52394,"Missing"
D19-6015,P15-1092,0,0.0519651,"Missing"
D19-6015,N15-1082,0,0.0315843,"Missing"
D19-6015,D14-1162,0,0.0851032,"eedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 123–129 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics 2 Related Work marks (Rajpurkar et al., 2018; Wang et al., 2019), including tasks that require explicit commonsense reasoning such as the Winograd Schema Challenge (Sakaguchi et al., 2019). Wang et al. (2018) present the semantic plausibility dataset that we use for evaluation in this work, and they show that distributional methods fail on this dataset. This conclusion aligns with other work showing that GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017). More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019). Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003), or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019). Complementary to our work are methods of extracting physical features from"
D19-6015,N18-2049,0,0.748608,"age understanding (NLU) tasks including narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), paragraph reconstruction (Li and Jurafsky, 2017), and hard coreference resolution (Peng Still, the generalization ability of supervised models is limited by the coverage of the training set. We therefore present the more difficult problem of learning physical plausibility directly from text. We create a training set by parsing and extracting attested s-v-o triples from English Wikipedia, and we provide a baseline for training on this dataset and evaluating on Wang et al. (2018)’s physical plausibility task. We also experiment training on a large set of s-v-o triples extracted from the web as part of the NELL project (Carlson et al., 2010), and find that Wikipedia triples result in better performance. 123 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 123–129 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics 2 Related Work marks (Rajpurkar et al., 2018; Wang et al., 2019), including tasks that require explicit commonsense reasoning such as the Winograd Schema Challenge (Sakaguchi et a"
D19-6015,I17-1021,0,0.0207465,"Mikolov et al., 2013) embeddings do not encode some salient features of objects (Li and Gauthier, 2017). More recent work has similarly concluded that large pretrained language models only learn attested physical knowledge (Forbes et al., 2019). Other datasets which include plausibility ratings are smaller in size and missing atypical but plausible events (Keller and Lapata, 2003), or concern the more complicated problem of multi-event inference in natural language (Zhang et al., 2017; Sap et al., 2019). Complementary to our work are methods of extracting physical features from a text corpus (Wang et al., 2017; Forbes and Choi, 2017; Bagherinezhad et al., 2016). 2.1 2.2 Selectional Preference Closely related to semantic plausibility is selectional preference (Resnik, 1996) which concerns the semantic preference of a predicate for its arguments. Here, preference refers to the typicality of arguments: while it is plausible that a gorilla rides a camel, it is not preferred. Current approaches to selectional preference are distributional (Erk et al., 2010; Van de Cruys, 2014) and have shown limited performance in capturing semantic plausibility (Wang et al., 2018). ´ S´eaghdha and Korhonen (2012) have"
D19-6015,P19-1071,0,0.158081,"triples (Wang et al., 2018). 3.2 Learning from Text We also present the problem of learning to model physical plausibility directly from text. In this new setting, a model is trained on events extracted from a large corpus and evaluated on a physical plausibility task. Therefore, only the test set covers both typical and atypical plausibility. We create two training sets based on separate corpora: first, we parse English Wikipedia using the StanfordNLP neural pipeline (Qi et al., 2018) and extract attested s-v-o triples. Wikipedia has led to relatively good results for selectional preference (Zhang et al., 2019), and in total we extract 6 million unique triples with a cumulative 10 million occurrences. Second, we use the NELL (Carlson et al., 2010) dataset of 604 million s-v-o triples extracted from the dependency parsed ClueWeb09 dataset. For NELL, we filter out triples with nonalphabetic characters or less than 5 occurrences, resulting in a total 2.5 million unique triples with a cumulative 112 million occurrences. For evaluation, we split Wang et al. (2018)’s 3,062 triples into equal sized validation and test sets. Each set thus consists of 1,531 triples. 4 4.1 BERT Supervised. We do no hyperparam"
D19-6015,J03-3005,0,\N,Missing
D19-6015,R11-1055,0,\N,Missing
D19-6015,Q17-1027,0,\N,Missing
D19-6015,K16-1002,0,\N,Missing
E12-1005,D10-1115,0,0.03558,"approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for di"
E12-1005,P07-1073,0,0.022045,"we trained the syntactic models over the AFP subset of Gigaword (~338M words). We also trained the other two models on just the AFP portion for comparison. Note that the AFP portion of Gigaword is three times larger than the BNC corpus (~100M words), on which several previous syntactic models were trained. Because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements. 4.3 Task 1 We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). Controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference. The dataset is separated into subsets depending on the target binary relation (Company X acquires Company Y or Person X was born in Place Y) and the entity pair (e.g., Yahoo and Inktomi) (Table 2). The dataset was constructed semia"
E12-1005,D10-1113,0,0.191836,"to two degrees. For example, the vector for catch might contain a dimension labelled (OBJ,OBJ-1,throw), which indicates the strength of connection between the two verbs through all of the cooccurring direct objects which they share. Unlike E&P, TFP’s model encodes the selectional preferences in a single vector using frequency counts. We extend the model to the sentence level with component-wise addition and multiplication, and word vectors are contextualized by the dependency neighbours. We use a frequency threshold of 10 and a pmi threshold of 2 to prune infrequent word and dependencies. D&L Dinu and Lapata (2010) (D&L) assume a global set of latent senses for all words, and models each word as a mixture over these latent senses. The vector for a word ti in the context of a word cj is modelled by v(ti , cj ) = P (z1 |ti , cj ), ...P (zK |ti , cj ) (19) where z1...K are the latent senses. By making independence assumptions and decomposing probabilities, training becomes a matter of estimating the probability distributions P (zk |ti ) and P (cj |zk ) from data. While Dinu and Lapata (2010) describe two methods to do so, based on non-negative matrix factorization and latent Dirichlet allocation, the perfo"
E12-1005,I05-5002,0,0.0224152,"terested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness. Dolan and Brockett (2005) define paraphrase detection as identifying sentences that are in a bidirectional entailment relation. While such sentences do support exactly the same inferences, we are also interested in the inferences that can be made from similar sentences that are not paraphrases according to this strict definition — a situation that is more often encountered in end applications. Thus, we adopt a less restricted notion of paraphrasis. 3 An Evaluation Framework We now describe a simple, general framework for evaluating semantic models. Our framework consists of the following components: a semantic model t"
E12-1005,D08-1094,0,0.278587,"Missing"
E12-1005,P10-2017,0,0.043097,"Missing"
E12-1005,D11-1129,0,0.0733609,"t word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phras"
E12-1005,W10-2805,0,0.0217593,"various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional mo"
E12-1005,P08-1028,0,0.774921,"and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference. 1 Introduction A number of unsupervised semantic models (Mitchell and Lapata, 2008, for example) have recently been proposed which are inspired at least in part by the distributional hypothesis (Harris, 1954)—that a word’s meaning can be characterized by the contexts in which it appears. Such models represent word meaning as one or more high-dimensional vectors which capture the lexical and syntactic contexts of the word’s occurrences in a training corpus. Much of the recent work in this area has, following Mitchell and Lapata (2008), focused on the notion of compositionality as the litmus test of a truly semantic model. Compositionality is a natural way to construct repres"
E12-1005,D09-1045,0,0.0214476,"l and the results. 4.1 Distributional Semantic Models We tested four recent distributional models and a lemma overlap baseline, which we now describe. We extended several of the models to compositionally construct phrase representations using component-wise vector addition and multiplication, as we note below. Since the focus of this paper is on evaluation methods for such models, we did not experiment with other compositionality 37 operators. We do note, however, that componentwise operators have been popular in recent literature, and have been applied across unrestricted syntactic contexts (Mitchell and Lapata, 2009), so there is value in evaluating the performance of these operators in itself. The models were trained on the Gigaword corpus (2nd ed., ~2.3B words). All models use cosine similarity to measure the similarity between representations, except for the baseline model. Lemma Overlap This baseline simply represents a sentence as the counts of each lemma present in the sentence after removing stop words. Let a sentence x consist of lemma-tokens m1 , . . . , m|x |. The similarity between two sentences is then defined as M(x, x′ ) = #In(x, x′ ) + #In(x′ , x) (15) #In(x, x′ ) = |x| X 1x′ (mi ) (16) i=1"
E12-1005,D09-1001,0,0.476793,"nonical form. The traditional justification for canonical forms is that they allow easy access to a knowledge base to retrieve some desired information, which amounts to a form of inference. Our work can be seen as an extension of this notion to distributional semantic models with a more general notion of representational similarity and inference. There are many regular alternations that semantics models have tried to account for such as passive or dative alternations. There are also many lexical paraphrases which can take drastically different syntactic forms. Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho. Utah is next to Idaho. In distributional semantics, the original sentence similarity test proposed by Kintsch (2001) served as the inspiration for the evaluation performed by Mitchell and Lapata (2008) and most later work in the area. Intransitive verbs are given 34 in the context of their syntactic subject, and candidate synonyms are ranked for their appropriateness. This method targets the fact that a synonym is appropriate for only some of the verb’s se"
E12-1005,N10-1013,0,0.085278,"e representation of a in context, a′ , is given by a′ = va ⊙ Rb (r −1 ) X Rb (r) = f (c, r, b) · vc , (17) (18) c:f (c,r,b)>θ where Rb (r) is the vector describing the selectional preference of word b in relation r, f (c, r, b) is the frequency of this dependency triple, θ is a frequency threshold to weed out uncommon dependency triples (10 in our experiments), and ⊙ is a vector combination operator, here componentwise multiplication. We extend the model to compute sentence representations from the contextualized word vectors using component-wise addition and multiplication. TFP Thater et al. (2010)’s model is also sensitive to selectional preferences, but to two degrees. For example, the vector for catch might contain a dimension labelled (OBJ,OBJ-1,throw), which indicates the strength of connection between the two verbs through all of the cooccurring direct objects which they share. Unlike E&P, TFP’s model encodes the selectional preferences in a single vector using frequency counts. We extend the model to the sentence level with component-wise addition and multiplication, and word vectors are contextualized by the dependency neighbours. We use a frequency threshold of 10 and a pmi thr"
E12-1005,C02-1151,0,0.0344252,"on of Gigaword is three times larger than the BNC corpus (~100M words), on which several previous syntactic models were trained. Because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements. 4.3 Task 1 We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). Controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference. The dataset is separated into subsets depending on the target binary relation (Company X acquires Company Y or Person X was born in Place Y) and the entity pair (e.g., Yahoo and Inktomi) (Table 2). The dataset was constructed semiautomatically using a Google search for the two entities in order with up to seven content words in between. Then, the extracted sentences were hand-labelled with whether the"
E12-1005,P10-1093,0,0.0236113,"ll (2011) uses potential paraphrases directly as dimensions in his expectation vectors. Unfortunately, this approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase"
E12-1005,P10-1097,0,0.0483242,"Missing"
E12-1005,W11-0130,0,0.0693543,"systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness. Dolan and Brockett (2005) define paraphrase detection as identifying sentences that are in a bidirectional entailment relation. While such sentences do support exactly the same inferences, we are also interested in the inferences that can be made from similar sentences that are not paraphrases according to this strict d"
E12-1005,W11-0131,0,0.0135379,"is dataset mostly for parameter tuning. Another is the lexical paraphrase task of McCarthy and Navigli (2009), in which words are given in the context of the surrounding sentence, and the task is to rank a given list of proposed substitutions for that word. The list of substitutions as well as the correct rankings are elicited from annotators. This task was originally conceived as an applied evaluation of WSD systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it"
E12-1005,D11-1016,0,0.0205655,"ity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phrase- and sentence-level representations, an"
E12-1071,W10-3110,0,0.0255613,"Missing"
E12-1071,P10-2046,0,0.660242,"ational methods for detecting DEOs from a corpus. They proposed two unsupervised algorithms which rely on the correlation between DEOs and negative polarity items (NPIs), which by the definition of Ladusaw (1980) must appear in the context of DEOs. An example of an NPI is yet, as in the sentence This project is not complete yet. The first baseline method proposed by DLD09 simply calculates a ratio of the relative frequencies of a word in NPI contexts versus in a general corpus, and the second is a distillation method which appears to refine the baseline ratios using a task-specific heuristic. Danescu-Niculescu-Mizil and Lee (2010) (henceforth DL10) extend this approach to Romanian, where a comprehensive list of NPIs is not available, by proposing a bootstrapping approach to co-learn DEOs and NPIs. DLD09 are to be commended for having identified a crucial component of inference that nevertheless lends itself to a classification-based ap696 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 696–705, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics proach, as we will show. However, as noted by DL10, the performance of the"
E12-1071,N09-1016,0,0.447578,"ailing, allowing reasoning from a set of events to a superset of events as seen in (1). In the scope of a downward-entailing operator (DEO), however, this entailment relation is reversed, such as in the scope of the classical DEO not (2). There are also operators which are neither upward- nor downward entailing, such as the expression exactly three (3). (1) She sang in French. ⇒ She sang. (upward-entailing) (2) She did not sing in French. ⇐ She did not sing. (downward-entailing) (3) Exactly three students sang. 6⇔ Exactly three students sang in French. (neither upward- nor downward-entailing) Danescu-Niculescu-Mizil et al. (2009) (henceforth DLD09) proposed the first computational methods for detecting DEOs from a corpus. They proposed two unsupervised algorithms which rely on the correlation between DEOs and negative polarity items (NPIs), which by the definition of Ladusaw (1980) must appear in the context of DEOs. An example of an NPI is yet, as in the sentence This project is not complete yet. The first baseline method proposed by DLD09 simply calculates a ratio of the relative frequencies of a word in NPI contexts versus in a general corpus, and the second is a distillation method which appears to refine the base"
E12-1071,H92-1045,0,0.141661,"be insufficient from a linguistic perspective, it is nevertheless a useful starting point for computational methods for detecting NPIs and DEOs, and has inspired successful techniques to detect DEOs, like the work by DLD09, DL10, and also this work. In addition to this hypothesis, we further assume that there should only be one plausible DEO candidate per NPI context. While there are counterexamples, this assumption is in practice very robust, and is a useful constraint for our learning algorithm. An analogy can be drawn to the one sense per discourse assumption in word sense disambiguation (Gale et al., 1992). The related—and as we will argue, more difficult—problem of detecting NPIs has also been studied, and in fact predates the work on DEO detection. Hoeksema (1997) performed the first corpus-based study of NPIs, predominantly for Dutch, and there has also been work on detecting NPIs in German which assumes linguistic knowledge of licensing contexts for NPIs (Lichte and Soehn, 2007). Richter et al. (2010) make this assumption as well as use syntactic structure to extract NPIs that are multi-word expressions. Parse information is an especially important consideration in freer-word-order language"
E12-1071,C08-1066,0,0.0317936,"a long-standing challenge in NLP, and there has been considerable debate both on what constitutes inference and what techniques should be used to support inference. One task involving inference that has recently received much attention is that of recognizing textual entailment (RTE), in which the goal is to determine whether a hypothesis sentence can be entailed from a piece of source text (Bentivogli et al., 2010, for example). An important consideration in RTE is whether a sentence or context produces an entailment relation for events that are a superset or subset of the original sentence (MacCartney and Manning, 2008). By default, contexts are upward-entailing, allowing reasoning from a set of events to a superset of events as seen in (1). In the scope of a downward-entailing operator (DEO), however, this entailment relation is reversed, such as in the scope of the classical DEO not (2). There are also operators which are neither upward- nor downward entailing, such as the expression exactly three (3). (1) She sang in French. ⇒ She sang. (upward-entailing) (2) She did not sing in French. ⇐ She did not sing. (downward-entailing) (3) Exactly three students sang. 6⇔ Exactly three students sang in French. (nei"
L18-1537,W08-2222,0,0.0705178,"Missing"
L18-1537,P88-1004,0,0.764623,"Missing"
L18-1537,D11-1142,0,0.0506342,"d as taking arguments). But these resources do not focus on relational nouns in particular. NomBank, by its extension to NomLex (Macleod et al., 1998), includes a short list of 331 relational nouns. By focusing specifically on relational nouns, 1 http://cgi.cs.mcgill.ca/˜enewel3/publications/relationalnouns-lrec-2018 2 https://github.com/enewe101/relational-nouns-lrec-2018 3405 we provide a greater than fourfold increase in the number of labelled relational nouns (1,446). Within the context of Open Information Extraction, earlier work such as ReVerb focused on extracting relations from verbs (Fader et al., 2011). Mausam et al. (2012) examined the role of nouns and adjectives as bearers of predicates as well, showing that doing so increases coverage. Yahya et al. (2014) developed the R E N OUN system, which focuses on extracting information about rarer attributes expressed using nouns. The work most related to ours is the R EL N OUN Open IE system, most recently augmented by Pal and Mausam (2016). This work extracts relations expressed using nouns, including relational nouns, using a combination of deterministic patterns and lexical resources, yielding 209 correct extractions from 2,000 newswire sente"
L18-1537,P10-1160,0,0.0837073,"Missing"
L18-1537,P14-5010,0,0.0089808,"Missing"
L18-1537,D12-1048,0,0.0391966,"). But these resources do not focus on relational nouns in particular. NomBank, by its extension to NomLex (Macleod et al., 1998), includes a short list of 331 relational nouns. By focusing specifically on relational nouns, 1 http://cgi.cs.mcgill.ca/˜enewel3/publications/relationalnouns-lrec-2018 2 https://github.com/enewe101/relational-nouns-lrec-2018 3405 we provide a greater than fourfold increase in the number of labelled relational nouns (1,446). Within the context of Open Information Extraction, earlier work such as ReVerb focused on extracting relations from verbs (Fader et al., 2011). Mausam et al. (2012) examined the role of nouns and adjectives as bearers of predicates as well, showing that doing so increases coverage. Yahya et al. (2014) developed the R E N OUN system, which focuses on extracting information about rarer attributes expressed using nouns. The work most related to ours is the R EL N OUN Open IE system, most recently augmented by Pal and Mausam (2016). This work extracts relations expressed using nouns, including relational nouns, using a combination of deterministic patterns and lexical resources, yielding 209 correct extractions from 2,000 newswire sentences. Our current work"
L18-1537,W04-2705,0,0.104964,"Missing"
L18-1537,C08-1084,0,0.0652638,"Missing"
L18-1537,W16-1307,0,0.0207351,"de a greater than fourfold increase in the number of labelled relational nouns (1,446). Within the context of Open Information Extraction, earlier work such as ReVerb focused on extracting relations from verbs (Fader et al., 2011). Mausam et al. (2012) examined the role of nouns and adjectives as bearers of predicates as well, showing that doing so increases coverage. Yahya et al. (2014) developed the R E N OUN system, which focuses on extracting information about rarer attributes expressed using nouns. The work most related to ours is the R EL N OUN Open IE system, most recently augmented by Pal and Mausam (2016). This work extracts relations expressed using nouns, including relational nouns, using a combination of deterministic patterns and lexical resources, yielding 209 correct extractions from 2,000 newswire sentences. Our current work complements automated extraction systems like R E N OUN and R EL N OUN by assembling a lexicon of relational nouns. Rather than only relying on automatically extracted patterns, we use manual annotation and bootstrapping to create a high-quality lexicon. This lexicon of relational nouns can be used as a semantic resource in relation extraction and other NLP tasks. 3"
L18-1537,D14-1038,0,0.0180302,"a short list of 331 relational nouns. By focusing specifically on relational nouns, 1 http://cgi.cs.mcgill.ca/˜enewel3/publications/relationalnouns-lrec-2018 2 https://github.com/enewe101/relational-nouns-lrec-2018 3405 we provide a greater than fourfold increase in the number of labelled relational nouns (1,446). Within the context of Open Information Extraction, earlier work such as ReVerb focused on extracting relations from verbs (Fader et al., 2011). Mausam et al. (2012) examined the role of nouns and adjectives as bearers of predicates as well, showing that doing so increases coverage. Yahya et al. (2014) developed the R E N OUN system, which focuses on extracting information about rarer attributes expressed using nouns. The work most related to ours is the R EL N OUN Open IE system, most recently augmented by Pal and Mausam (2016). This work extracts relations expressed using nouns, including relational nouns, using a combination of deterministic patterns and lexical resources, yielding 209 correct extractions from 2,000 newswire sentences. Our current work complements automated extraction systems like R E N OUN and R EL N OUN by assembling a lexicon of relational nouns. Rather than only rely"
N13-1104,P98-1013,0,0.192085,"vated features. Likewise, P RO F INDER can easily be used as a semi-supervised system if some slot designations and labeled examples are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), schemata (Rumelhart, 1975), and scripts (Schank and Abelson, 1977). In the linguistics and computational linguistics communities, frame semantics (Fillmore, 1982) uses frames as the central representation of word meaning, culminating in the development of FrameNet (Baker et al., 1998), which contains over 1000 manually annotated frames. A similarly rich lexical resource is the MindNet project (Richardson et al., 1998). Our notion of frame is related to these representations, but there are also subtle differences. For example, Minsky’s frame emphasizes inheritance, which we do not model in this paper1 . As in semantic role labeling, FrameNet focuses on semantic roles and does not model event or frame transitions, so the scope of its frames is often no more than an event in our model. Perhaps the most similar to our frame is Roger Schank’s scripts, which capture prototypical"
N13-1104,P08-1004,0,0.00410704,"reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from we"
N13-1104,N04-1015,0,0.060758,"2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009, inter alia). There are, however, two main differences. First, P RO F INDER contains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky"
N13-1104,N04-1038,0,0.0180699,"m PE−HEAD (ei |Ei ). 4. For each event argument: (a) Generate the slot Si,j from PSLOT (S|E, A, B). (b) Generate the dependency/caseframe emission depi,j ∼ PA−DEP (dep|S) and the lemma of the head word of the event argument ai,j ∼ PA−HEAD (a|S). 3.4 Figure 1: Graphical representation of our model. Hyperparameters, the stickiness factor, and the frame and event initial and transition distributions are not shown for clarity. the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004). 3.3 Full generative story To summarize, the distributions that are learned by our model are the default distributions PBKG (B), PF−INIT (F ), PE−INIT (E); the transition distributions PF−TRAN (Fi+1 |Fi ), PE−TRAN (Ei+1 |Ei ); and the emission distributions PSLOT (S|E, A, B), PE−HEAD (e|E, B), PA−HEAD (a|S), PA−DEP (dep|S). We used additive smoothing with uniform Dirichlet priors for all the multinomials. The overall generative story of our model is as follows: 1. Draw a Bernoulli distribution for PBKG (B) 2. Draw the frame, event, and slot distributions 3. Draw an event head emission distrib"
N13-1104,P08-1090,0,0.702432,"ences. First, P RO F INDER contains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky (2011), which culminated from a series of work for identifying correlated events and arguments in narratives (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). By adopting a probabilistic approach, P RO F INDER has a sound theoretical underpinning, and is easy to modify or extend. For example, in Section 3, we show how P RO F INDER can easily be augmented with additional linguistically-motivated features. Likewise, P RO F INDER can easily be used as a semi-supervised system if some slot designations and labeled examples are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), s"
N13-1104,P09-1068,0,0.579316,"ntains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky (2011), which culminated from a series of work for identifying correlated events and arguments in narratives (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). By adopting a probabilistic approach, P RO F INDER has a sound theoretical underpinning, and is easy to modify or extend. For example, in Section 3, we show how P RO F INDER can easily be augmented with additional linguistically-motivated features. Likewise, P RO F INDER can easily be used as a semi-supervised system if some slot designations and labeled examples are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), schemata (Rumelhart, 1975), and"
N13-1104,P11-1098,0,0.719278,"r of domains and a few slots within a domain. Furthermore, additional manual effort is needed after the frames are defined in order to extract frame components from text (e.g., in annotating examples and designing features to train a supervised learning model). This paradigm makes generalizing across tasks difficult, and might suffer from annotator bias. Recently, there has been increasing interest in au837 Proceedings of NAACL-HLT 2013, pages 837–846, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics tomatically inducing frames from text. A notable example is Chambers and Jurafsky (2011), which first clusters related verbs to form frames, and then clusters the verbs’ syntactic arguments to identify slots. While Chambers and Jurafsky (2011) represents a major step forward in frame induction, it is also limited in several aspects. The clustering used ad hoc steps and customized similarity metrics, as well as an additional retrieval step from a large external text corpus for slot generation. This makes it hard to replicate their approach or adapt it to new domains. Lacking a coherent model, it is also difficult to incorporate additional linguistic insights and prior knowledge. I"
N13-1104,P06-1039,0,0.00911159,"Missing"
N13-1104,P06-2027,0,0.523601,"res two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More rele"
N13-1104,N09-1041,1,0.409716,"and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009, inter alia). There are, however, two main differences. First, P RO F INDER contains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky (2011), which culminated from a series of work for identifyi"
N13-1104,E12-1029,0,0.00536895,"ly reducing engineering effort and requiring no external data. 2 Related Work In information extraction and other semantic processing tasks, the dominant paradigm requires two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While"
N13-1104,P11-1112,0,0.0123109,"ir syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009, inter alia). There are, h"
N13-1104,N09-1069,0,0.00744217,"dmits efficient inference by dynamic programming. In particular, after collapsing the latent assignment of frame, event, and background into a single hidden variable for each clause, the expectation and most probable assignment can be computed using standard forward-backward and Viterbi algorithms on fixed tree structures. Parameter learning can be done using EM by alternating the computation of expected counts and the maximization of multinomial parameters. In particular, P RO F INDER uses incremental EM, which has been shown to have better and faster convergence properties than standard EM (Liang and Klein, 2009). Determining the optimal number of events and slots is challenging. One solution is to adopt a nonparametric Bayesian method by incorporating a hierarchical prior over the parameters (e.g., a Dirichlet process). However, this approach can impose unrealistic restrictions on the model choice and result in intractability which requires sampling or approximate inference to overcome. Additionally, EM learning can suffer from local optima due to its nonconvex learning objective, especially when dealing with a large number hidden states without a good initialization. To address these issues, we adop"
N13-1104,N04-1019,0,0.0702176,"ation from source text. Essentially, we adapted the TAC summarization annotation to create gold-standard slots, and used them to evaluate entity extraction as in MUC-4. Dataset We used the TAC 2010 guidedsummarization dataset in our experiments (Owczarzak and Dang, 2010). This data set consists of text from five domains (termed categories in TAC), each with a template defined by TAC organizers. In total, there are 46 document clusters (termed topics in TAC), each of which contains 20 documents and has eight human-written summaries. Each summary was manually segmented using the Pyramid method (Nenkova and Passonneau, 2004) and each segment was annotated with a slot (termed aspect in TAC) from the corresponding template. Figure 3 shows an example and the full set of templates is available at http://www. nist.gov/tac/2010/Summarization/ Guided-Summ.2010.guidelines.html. In (a) (b) (c) Accidents and Natural Disasters: WHAT: what happened WHEN: date, time, other temporal markers WHERE: physical location WHY: reasons for accident/disaster WHO AFFECTED: casualties... DAMAGES: ... caused by the disaster COUNTERMEASURES: rescue efforts... (W HEN During the night of July 17,) (W HAT a 23-foot &lt;W HAT tsunami) hit the nor"
N13-1104,D07-1075,0,0.0976629,"art results while significantly reducing engineering effort and requiring no external data. 2 Related Work In information extraction and other semantic processing tasks, the dominant paradigm requires two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational"
N13-1104,P06-1055,0,0.0117236,"INDucER), the first probabilistic approach to frame induction. P RO F INDER defines a joint distribution over the words in a document and their frame assignments by modeling frame and event transitions, correlations among events and slots, and their surface realizations. Given a set of documents, P RO F INDER outputs a set of induced frames with learned parameters, as well as the most probable frame assignments that can be used for event and entity extraction. The numbers of events and slots are dynamically determined by a novel application of the split-merge approach from syntactic parsing (Petrov et al., 2006). In end-to-end evaluations from text to entity extraction using standard MUC and TAC datasets, P RO F INDER achieved state-of-the-art results while significantly reducing engineering effort and requiring no external data. 2 Related Work In information extraction and other semantic processing tasks, the dominant paradigm requires two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of wor"
N13-1104,D09-1001,1,0.0633533,"whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzi"
N13-1104,P98-2180,1,0.108186,"s are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), schemata (Rumelhart, 1975), and scripts (Schank and Abelson, 1977). In the linguistics and computational linguistics communities, frame semantics (Fillmore, 1982) uses frames as the central representation of word meaning, culminating in the development of FrameNet (Baker et al., 1998), which contains over 1000 manually annotated frames. A similarly rich lexical resource is the MindNet project (Richardson et al., 1998). Our notion of frame is related to these representations, but there are also subtle differences. For example, Minsky’s frame emphasizes inheritance, which we do not model in this paper1 . As in semantic role labeling, FrameNet focuses on semantic roles and does not model event or frame transitions, so the scope of its frames is often no more than an event in our model. Perhaps the most similar to our frame is Roger Schank’s scripts, which capture prototypical events and participants in a scenario such as restaurant dining. In their approach, however, scripts are manually defined, making it ha"
N13-1104,N06-1039,0,0.061787,"ruct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsuperv"
N13-1104,H01-1054,0,0.0271149,"e Slot: Victim P ERSON /O RG Words: people, priest, leader, member, judge Caseframes: kill&gt;dobj, murder&gt;dobj, release&gt;dobj, report&gt;dobj, kidnap&gt;dobj Figure 2: A partial frame learned by P RO F INDER from the MUC-4 data set, with the most probable emissions for each event and slot. Labels are assigned by the authors for readability. ate P RO F INDER’s capabilities in generalizing to a greater variety of text, we designed and conducted a novel evaluation based on the TAC guidedsummarization dataset. This evaluation was inspired by the connection between summarization and information extraction (White et al., 2001), and reflects a conceptualization of summarization as inducing and extracting structured information from source text. Essentially, we adapted the TAC summarization annotation to create gold-standard slots, and used them to evaluate entity extraction as in MUC-4. Dataset We used the TAC 2010 guidedsummarization dataset in our experiments (Owczarzak and Dang, 2010). This data set consists of text from five domains (termed categories in TAC), each with a template defined by TAC organizers. In total, there are 46 document clusters (termed topics in TAC), each of which contains 20 documents and h"
N13-1104,C98-2175,1,\N,Missing
N13-1104,C98-1013,0,\N,Missing
N18-4004,N15-1082,0,0.123212,"Missing"
N18-4004,D10-1048,0,0.0989026,"on is the fraction of queries (AGQ), automatically generated queries correctly answered instances among answered inwith synonyms (AGQS), and manually generated stances, recall is the fraction of correctly answered queries (MGQ), and compares these to the sysinstances among all instances, and tems of Sharma et al. (2015) (S2015) and Liu et al. (2016b) (L2016). The system developed by Liu F 1 = 2 ∗ P ∗ R/(P + R). et al. (2016b) uses elements extracted manually from the problem instances, so is most closely We used Stanford CoreNLP’s coreference recomparable to our MGQ method. Our best ausolver (Raghunathan et al., 2010) during query tomated framework, AGQS, outperforms S2015 generation to identify the predicates from the synby 0.16 F1, achieving much higher recall (0.39 vs tactic parse, as well as during antecedent selection • The weight couldn’t be lifted by me, because I was so weak. Here, because of the passive voice, E20 plays the agent role, while syntactically being the object. Using rule (2), the sentence is correctly reversed to evidence-agent. 29 # Correct P R F1 AGQ 73 0.53 0.27 0.36 AGQS 106 0.56 0.39 0.46 S2015 49 0.92 0.18 0.30 Systems with manual information: L2016 43 0.61 0.15 0.25 MGQ 118 0.6"
N18-4004,D12-1071,0,0.11168,"Missing"
N18-4004,N04-1038,0,0.0607407,"the original Winograd sentence itself. The task then is to construct the two query sets, C and Q, whose elements are possible entries for T ermC and T ermQ , respectively. We achieve this by identifying the root verbs along with any modifying adjective in the context and query clauses, using Stanford CoreNLP’s dependency parse of the sentence. We then add the root verbs and adjectives into the sets C and Q along with their broader verb phrases (again identified directly using the dependency tree). These extracted queries serve as event information that will be used in the subsequent modules. Bean and Riloff (2004) also learn extraction patterns to support coreference, but unlike our method, their method relies on a static domain and constructs an explicit probabilistic model of the narrative chains learned. Extracting Knowledge from Search Results E10 P red0C E20 E10 P red0C E20 E10 P red0C E10 P red0C + + + + E30 P red0Q P red0Q E30 E30 P red0Q P red0Q E30 We call these evidence sentences. They exhibit a structure similar to the corresponding Winograd question, but with different entities and event order. In particular, P red0C and P red0Q (resulting from the queries T ermC and T ermQ , resp.) should"
N18-4004,Y14-1042,0,0.31247,"Missing"
N19-1396,W08-1106,1,0.715561,"systems fail to understand the source text, in a majority of the cases. 1 Table 1: Examples of generated contrastive summaries. Bold indicate switched words . Introduction Open-domain abstractive summarization is a longstanding goal of the field of automatic summarization. Compared to extractive techniques, abstraction offers the potential to generate much more useful summaries by simplifying and rephrasing the source text (Knight and Marcu, 2002), and furthermore by aggregating information and performing operations which are not possible with extractive techniques (Genest and Lapalme, 2012; Carenini and Cheung, 2008). Recently, a number of abstractive summarization systems based on neural sequence-tosequence architectures have been proposed (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Chen and Bansal, 2018). These systems learn a compressed representation of the source text using an encoder, then generate the output summary using a conditional decoder. Such neural abstractive systems have achieved very good ROUGE scores on different datasets. Our interest in this paper is to investigate how these abstractive systems achieve such results, and whether they represent pro"
N19-1396,P18-1063,0,0.11868,"the field of automatic summarization. Compared to extractive techniques, abstraction offers the potential to generate much more useful summaries by simplifying and rephrasing the source text (Knight and Marcu, 2002), and furthermore by aggregating information and performing operations which are not possible with extractive techniques (Genest and Lapalme, 2012; Carenini and Cheung, 2008). Recently, a number of abstractive summarization systems based on neural sequence-tosequence architectures have been proposed (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Chen and Bansal, 2018). These systems learn a compressed representation of the source text using an encoder, then generate the output summary using a conditional decoder. Such neural abstractive systems have achieved very good ROUGE scores on different datasets. Our interest in this paper is to investigate how these abstractive systems achieve such results, and whether they represent progress towards language understanding and generation. ROUGE arguably provides a limited view of the performance of such systems, as they only relate the system summary to a fixed number of gold-standard summaries. We propose a novel"
N19-1396,W09-0613,0,0.0544136,"yntactic category, as shown in Table 2, using the dependency parse of the texts (Manning et al., 2014). We switch words either within a gold summary, or from the source text and use a single rule at a time for generating a contrastive summary. For example, if the POS tag NNS is matched between a word ‘sides’ in the source text and the word ‘combatants’ in the gold summary, then the Noun rule would apply, and the words would be switched to obtain the contrastive summary. Further, for the Noun and Verb categories, the switched words may not match in number or verb conjugation. We use SimpleNLG (Gatt and Reiter, 2009) to convert the word to the appropriate inflectional form of the destination’s POS tag. 3950 Further restrictions. We place a number of restrictions on the words switched, to ensure that the generated summary is contrastive compared to the gold summary. We do not allow switching of the same words. We also do not allow words to be switched if they are separated by any of the following: ‘or’, ‘and’ or ‘,’, as these are likely to be commutative operators. Furthermore, we only allow switching of words from the source text if the context of the words to be switched sufficiently differ from each oth"
N19-1396,P14-5010,0,0.00252643,"gold summary. There are many types of possible perturbations, but we focus on two strategies: 1) switching words within a gold summary, and 2) replacing a word in gold summary by a word from the source text. We chose these types of perturbations as they are likely to result in “difficult” contrastive summaries that contain words which are likely to appear in a reasonable summary of the source text, but which are nevertheless incorrect. In order to select the words to be swapped, we apply four rules, separated by syntactic category, as shown in Table 2, using the dependency parse of the texts (Manning et al., 2014). We switch words either within a gold summary, or from the source text and use a single rule at a time for generating a contrastive summary. For example, if the POS tag NNS is matched between a word ‘sides’ in the source text and the word ‘combatants’ in the gold summary, then the Noun rule would apply, and the words would be switched to obtain the contrastive summary. Further, for the Noun and Verb categories, the switched words may not match in number or verb conjugation. We use SimpleNLG (Gatt and Reiter, 2009) to convert the word to the appropriate inflectional form of the destination’s P"
N19-1396,K16-1028,0,0.162006,"Open-domain abstractive summarization is a longstanding goal of the field of automatic summarization. Compared to extractive techniques, abstraction offers the potential to generate much more useful summaries by simplifying and rephrasing the source text (Knight and Marcu, 2002), and furthermore by aggregating information and performing operations which are not possible with extractive techniques (Genest and Lapalme, 2012; Carenini and Cheung, 2008). Recently, a number of abstractive summarization systems based on neural sequence-tosequence architectures have been proposed (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Chen and Bansal, 2018). These systems learn a compressed representation of the source text using an encoder, then generate the output summary using a conditional decoder. Such neural abstractive systems have achieved very good ROUGE scores on different datasets. Our interest in this paper is to investigate how these abstractive systems achieve such results, and whether they represent progress towards language understanding and generation. ROUGE arguably provides a limited view of the performance of such systems, as they only relate the system summary to"
N19-1396,W12-3018,0,0.0194874,"Missing"
N19-1396,W17-4504,0,0.0200084,"has focused on optimizing ROUGE, whether implicitly by maximum likelihood training or explicitly by reinforcement learning. While this could certainly capture aspects of the content selection problem, we believe that the focus should now shift towards semantic correctness and readability. Cao et al. (2018) took a step in this direction through their fact-aware neural abstractive summarization system. They use fact descriptions of the source as additional features for the summarizer, and showed improved faithfulness according to human judgments. Multi-task learning is another approach used by Pasunuru et al. (2017) to reduce semantic errors in the generated summaries. They jointly learn summarization and entailment generation tasks, using different encoders but a shared decoder. A number of automatic evaluation metrics have shown high correlation with human judges (Liu and Liu, 2008; Graham, 2015), but these results are either restricted to extractive systems or were performed with respect to human-generated summaries. Correlation values are significantly reduced when performed on abstractive summarization systems and datasets (Toutanova et al., 2016). 3 Generating Contrastive Summaries In this section,"
N19-1396,D15-1044,0,0.22533,"rds . Introduction Open-domain abstractive summarization is a longstanding goal of the field of automatic summarization. Compared to extractive techniques, abstraction offers the potential to generate much more useful summaries by simplifying and rephrasing the source text (Knight and Marcu, 2002), and furthermore by aggregating information and performing operations which are not possible with extractive techniques (Genest and Lapalme, 2012; Carenini and Cheung, 2008). Recently, a number of abstractive summarization systems based on neural sequence-tosequence architectures have been proposed (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Chen and Bansal, 2018). These systems learn a compressed representation of the source text using an encoder, then generate the output summary using a conditional decoder. Such neural abstractive systems have achieved very good ROUGE scores on different datasets. Our interest in this paper is to investigate how these abstractive systems achieve such results, and whether they represent progress towards language understanding and generation. ROUGE arguably provides a limited view of the performance of such systems, as they only rela"
N19-1396,P17-1099,0,0.215301,"summarization is a longstanding goal of the field of automatic summarization. Compared to extractive techniques, abstraction offers the potential to generate much more useful summaries by simplifying and rephrasing the source text (Knight and Marcu, 2002), and furthermore by aggregating information and performing operations which are not possible with extractive techniques (Genest and Lapalme, 2012; Carenini and Cheung, 2008). Recently, a number of abstractive summarization systems based on neural sequence-tosequence architectures have been proposed (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Chen and Bansal, 2018). These systems learn a compressed representation of the source text using an encoder, then generate the output summary using a conditional decoder. Such neural abstractive systems have achieved very good ROUGE scores on different datasets. Our interest in this paper is to investigate how these abstractive systems achieve such results, and whether they represent progress towards language understanding and generation. ROUGE arguably provides a limited view of the performance of such systems, as they only relate the system summary to a fixed number of"
N19-1396,D16-1033,0,0.0188698,"ents. Multi-task learning is another approach used by Pasunuru et al. (2017) to reduce semantic errors in the generated summaries. They jointly learn summarization and entailment generation tasks, using different encoders but a shared decoder. A number of automatic evaluation metrics have shown high correlation with human judges (Liu and Liu, 2008; Graham, 2015), but these results are either restricted to extractive systems or were performed with respect to human-generated summaries. Correlation values are significantly reduced when performed on abstractive summarization systems and datasets (Toutanova et al., 2016). 3 Generating Contrastive Summaries In this section, we describe our method for evaluating summarization systems based on whether they can separate human-written gold summaries from automatically generated contrastive summaries. We define a contrastive summary to be similar to a gold summary, except it contains a perturbation. The perturbation results in either a semantic discrepancy, where facts in source and summary do not corroborate, or a readability issue, where issues with grammar or fluency renders 2 https://github.com/krtin/ ContrastiveSummaries Preposition Verb Adjective Switching Cr"
P09-1008,W08-1005,0,0.0511541,"Missing"
P09-1008,C02-1093,0,0.27108,"ementizer with the field label C in VL sentences. 66 Figure 1: “I could never have done that just for aesthetic reasons.” Sample T¨uBa-D/Z tree, with topological field annotations and edge labels. Topological field layer in bold. periments)3 taken from the German newspaper die tageszeitung. The corpus consists of four levels of annotation: clausal, topological, phrasal (other than clausal), and lexical. We define the task of topological field parsing to be recovering the first two levels of annotation, following Ule (2003). We also tested the parser on a version of the NEGRA corpus derived by Becker and Frank (2002), in which syntax trees have been made projective and topological fields have been automatically added through a series of linguistically informed tree modifications. All internal phrasal structure nodes have also been removed. The corpus consists of 20596 sentences, which we split into subsets of the same size as described by Becker and Frank (2002)4 . The set of topological fields in this corpus differs slightly from the one used in T¨uBa-D/Z, making no distinction between clause types, nor consistently marking field or clause conjunctions. Because of the automatic annotation of topological"
P09-1008,P06-1055,0,0.219796,"lexicalized model. The best Freer-word-order languages such as German exhibit linguistic phenomena that present unique challenges to traditional CFG parsing. Such phenomena produce discontinuous constituents, which are not naturally modelled by projective phrase structure trees. In this paper, we examine topological field parsing, a shallow form of parsing which identifies the major sections of a sentence in relation to the clausal main verb and the subordinating heads. We report the results of topological field parsing of German using the unlexicalized, latent variable-based Berkeley parser (Petrov et al., 2006) Without any language- or model-dependent adaptation, we achieve state-of-the-art results on the T¨uBa-D/Z corpus, and a modified NEGRA corpus that has been automatically annotated with topological fields (Becker and Frank, 2002). We also perform a qualitative error analysis of the parser output, and discuss strategies to further improve the parsing results. 1 Introduction Freer-word-order languages such as German exhibit linguistic phenomena that present unique challenges to traditional CFG parsing. Topic focus ordering and word order constraints that are sensitive to phenomena other than gra"
P09-1008,rohrer-forst-2006-improving,0,0.0501374,"Missing"
P09-1008,P03-1013,0,0.0537322,"Missing"
P09-1008,P03-1014,0,0.0629192,"Missing"
P09-1008,telljohann-etal-2004-tuba,0,0.290979,"Missing"
P09-1008,P06-1064,0,0.0561583,"Missing"
P09-1008,W02-2032,0,0.451703,"Missing"
P09-1008,W06-1614,0,0.0415057,"Missing"
P09-1008,A00-1033,0,0.289515,"Missing"
P10-1020,J08-1001,0,0.197753,"ammatical roles in a sentence ordering experiment, and in fact outperforms simple word-order information as well. We further show that these differences are particularly large when manual syntactic and grammatical role anOne goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly. 1 Introduction One type of coherence modelling that has captured recent research interest is local coherence modelling, which measures the coherence of a document by examining the sim"
P10-1020,N04-1015,0,0.355995,"an text, and show that the added coherence component improves performance slightly, though not statistically significantly. 1 Introduction One type of coherence modelling that has captured recent research interest is local coherence modelling, which measures the coherence of a document by examining the similarity between neighbouring text spans. The entity-based approach, in particular, considers the occurrences of noun phrase entities in a document (Barzilay and Lapata, 2008). Local coherence modelling has been shown to be useful for tasks like natural language generation and summarization, (Barzilay and Lee, 2004) and genre classification (Barzilay and Lapata, 2008). Previous work on English, a language with relatively fixed word order, has identified factors that contribute to local coherence, such as the grammatical roles associated with the entities. There is 186 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 186–195, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics S NF S VF LK MF LK MF VC Millionen von Mark verschwendet der Senat jeden Monat, weil er sparen will. “The senate wastes millions of marks each month, becau"
P10-1020,D07-1009,0,0.398654,"Missing"
P10-1020,P09-1008,1,0.835676,"Missing"
P10-1020,J99-3001,0,0.0528305,"and Gerald Penn Department of Computer Science University of Toronto Toronto, ON, M5S 3G4, Canada {jcheung,gpenn}@cs.toronto.edu Abstract good reason to believe that the importance of these factors vary across languages. For instance, freerword-order languages exhibit word order patterns which are dependent on discourse factors relating to information structure, in addition to the grammatical roles of nominal arguments of the main verb. We thus expect word order information to be particularly important in these languages in discourse analysis, which includes coherence modelling. For example, Strube and Hahn (1999) introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities. They apply their model to pronominal anaphora resolution, identifying potential antecedents of subsequent anaphora by considering syntactic and word order information, classifying constituents by their familiarity to the reader. They find that their approach correctly resolves more pronominal anaphora than a grammatical role-based approach which ignores word order, and the difference between the two approaches is larger in German corpora than"
P10-1020,W07-2321,0,0.235112,"rbs, and prepositional phrases are found here, unless they have been fronted and put in the VF, or are prosodically heavy and postposed to the NF field. The NF (Nachfeld or “post-field”) contains prosodically heavy elements such as postposed prepositional phrases or relative clauses, and occasionally postposed noun phrases. notations are not available. We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting. We add contextual features using topological field transitions to the model of Filippova and Strube (2007b) and achieve a slight improvement over their model in a constituent ordering task, though not statistically significantly. We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling. 2 Background and Related Work 2.1 German Topological Field Parsing Topological fields are sequences of one or more contiguous phrases found in an enclosing syntactic region, which is the clause in the case of the German topological field model (H¨ohle, 1983). These fields may have constraints on the number of words or phrases they contain, and do not necessarily"
P10-1020,telljohann-etal-2004-tuba,0,0.0211737,"ontrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. Similar models of local coherence include various Centering Theory accounts of local coherence ((Kibble and Power, 2004; Poesio et al., 2004) inter alia). The model of Elsner and Charniak (2007) uses syntactic cues to model the discoursenewness of noun phrases. There are also more global content models of topic shifts between sentences like Barzilay and Lee (2004). 3 Sentence Ordering Experiments 3.1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). The task is a binary classification task to identify the original version of a document from another version which contains the sentences in a randomly permuted order, which is taken to be incoherent. We solve this problem in a supervised machine learning setting, where the input is the feature vector representations of the two versions of the document, and the output is"
P10-1020,P07-1041,0,0.339918,"rbs, and prepositional phrases are found here, unless they have been fronted and put in the VF, or are prosodically heavy and postposed to the NF field. The NF (Nachfeld or “post-field”) contains prosodically heavy elements such as postposed prepositional phrases or relative clauses, and occasionally postposed noun phrases. notations are not available. We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting. We add contextual features using topological field transitions to the model of Filippova and Strube (2007b) and achieve a slight improvement over their model in a constituent ordering task, though not statistically significantly. We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling. 2 Background and Related Work 2.1 German Topological Field Parsing Topological fields are sequences of one or more contiguous phrases found in an enclosing syntactic region, which is the clause in the case of the German topological field model (H¨ohle, 1983). These fields may have constraints on the number of words or phrases they contain, and do not necessarily"
P10-1020,P08-4003,0,0.0219725,"corpora by examining how highly the original ordering found in the corpus is ranked compared to other possible orderings of propositions. A metric performs well if it ranks the original ordering better than the alternative orderings. In our next experiment, we incorporate local co1 Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We are not aware of similarly well-tested, publicly available coreference resolution systems that handle all types of anaphora for German. We considered adapting the BART coreference resolution toolkit (Versley et al., 2008) to German, but a number of language-dependent decisions regarding preprocessing, feature engineering, and the learning paradigm would need to be made in order to achieve reasonable performance comparable to state-of-the-art English coreference resolution systems. 192 • the semantic class of the constituent (person, temporal, location, etc.) The biographee, in particular, is marked by its own semantic class. herence information into the system of Filippova and Strube (2007b). We embed entity topological field transitions into their probabilistic model, and show that the added coherence compone"
P10-1020,J09-1003,0,0.0826604,"iment, with topological field-based models outperforming grammatical role and clausal order models. 4 Local Coherence for Natural Language Generation One of the motivations of the entity grid-based model is to improve surface realization decisions in NLG systems. A typical experimental design would pass the contents of the test section of a corpus as input to the NLG system with the ordering information stripped away. The task is then to regenerate the ordering of the information found in the original corpus. Various coherence models have been tested in corpus-based NLG settings. For example, Karamanis et al. (2009) compare several versions of Centering Theory-based metrics of coherence on corpora by examining how highly the original ordering found in the corpus is ranked compared to other possible orderings of propositions. A metric performs well if it ranks the original ordering better than the alternative orderings. In our next experiment, we incorporate local co1 Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We are not aware of similarly well-tested, publicly available coreference resolution systems that handle all types of anaphora f"
P10-1020,J04-4001,0,0.0290645,"work, however, was to adapt the model for use in a low-resource situation when perfect coreference information is not available. This is particularly useful in natural language understanding tasks. They employ a semantic clustering model to relate entities. In contrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. Similar models of local coherence include various Centering Theory accounts of local coherence ((Kibble and Power, 2004; Poesio et al., 2004) inter alia). The model of Elsner and Charniak (2007) uses syntactic cues to model the discoursenewness of noun phrases. There are also more global content models of topic shifts between sentences like Barzilay and Lee (2004). 3 Sentence Ordering Experiments 3.1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). The task is a binary classification task to identify the original version of a document from another version which contains the"
P10-1020,P03-1069,0,0.32964,"Missing"
P10-1020,J06-4002,0,0.135944,"Missing"
P10-1020,P02-1014,0,0.0602176,"to regenerate the ordering of the information found in the original corpus. Various coherence models have been tested in corpus-based NLG settings. For example, Karamanis et al. (2009) compare several versions of Centering Theory-based metrics of coherence on corpora by examining how highly the original ordering found in the corpus is ranked compared to other possible orderings of propositions. A metric performs well if it ranks the original ordering better than the alternative orderings. In our next experiment, we incorporate local co1 Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We are not aware of similarly well-tested, publicly available coreference resolution systems that handle all types of anaphora for German. We considered adapting the BART coreference resolution toolkit (Versley et al., 2008) to German, but a number of language-dependent decisions regarding preprocessing, feature engineering, and the learning paradigm would need to be made in order to achieve reasonable performance comparable to state-of-the-art English coreference resolution systems. 192 • the semantic class of the constituent (person, temporal, location, et"
P10-1020,N07-1051,0,0.0473005,"Missing"
P10-1020,J04-3003,0,0.0408395,"dapt the model for use in a low-resource situation when perfect coreference information is not available. This is particularly useful in natural language understanding tasks. They employ a semantic clustering model to relate entities. In contrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. Similar models of local coherence include various Centering Theory accounts of local coherence ((Kibble and Power, 2004; Poesio et al., 2004) inter alia). The model of Elsner and Charniak (2007) uses syntactic cues to model the discoursenewness of noun phrases. There are also more global content models of topic shifts between sentences like Barzilay and Lee (2004). 3 Sentence Ordering Experiments 3.1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). The task is a binary classification task to identify the original version of a document from another version which contains the sentences in a random"
P10-1020,C08-1098,0,0.0591713,"Missing"
P13-1039,P10-1084,0,0.0143859,"lized Distributional Semantic Vectors Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu Jackie Chi Kit Cheung University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 jcheung@cs.toronto.edu Abstract domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the C RIMINAL I NVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include V ICTIM, S USPECT, AUTHORITIES, P LEA, etc. One problem faced by th"
P13-1039,P11-1098,0,0.12574,"are assumed to represent a 392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose P RO F INDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas P RO F INDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with P RO F INDER in Section 5."
P13-1039,E12-1005,1,0.848272,"s form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializi"
P13-1039,N13-1104,1,0.877836,"enn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu Jackie Chi Kit Cheung University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 jcheung@cs.toronto.edu Abstract domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the C RIMINAL I NVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include V ICTIM, S USPECT, AUTHORITIES, P LEA, etc. One problem faced by this line of work is that, by thei"
P13-1039,N04-1015,0,0.049083,"that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor perRelated Work Probabilistic content models were proposed by Ba"
P13-1039,D12-1091,0,0.0142845,"sterisks (*) indicate that the model is statistically significantly different from P RO F INDER in terms of F1 at p < 0.05. Results We compared D S H MM to two baselines. Our first baseline is P RO F INDER, a stateof-the-art template inducer which Cheung et al. (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011). Our second baseline is our D S H MM model, without the semantic vector component, (HMM w/o semantics). To calculate statistical significance, we use the paired bootstrap method, which can accommodate complex evaluation metrics like F1 (Berg-Kirkpatrick et al., 2012). Table 2 shows that performance of the models. Overall, P RO F INDER significantly outperforms the HMM baseline, but not any of the D S H MM models by F1. D S H MM with contextualized semantic vectors achieves the highest F1s, and are significantly better than P RO F INDER. All of the differences in precision and recall between P RO F INDER and the other models are significant. The baseline HMM model has highly imbalanced precision and recall. We think this is because the model is unable to successfully produce coherent clusters, so the best-case mapping procedure during evaluation picked lar"
P13-1039,de-marneffe-etal-2006-generating,0,0.0151889,"Missing"
P13-1039,D10-1113,0,0.0417348,"broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast,"
P13-1039,N07-1055,0,0.0285409,"nd automatic summarization. Generative probabilistic models have been one popular approach to content modelling. An important advantage of this approach is that the structure of the model can be adapted to fit the assumptions about the structure of the domain and the nature of the end task. As this field has progressed, the formal structures that are assumed to represent a 392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose P RO F INDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model a"
P13-1039,P08-1028,0,0.719464,"inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas P RO F INDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with P RO F INDER in Section 5. tional representations can be modified depending on the specific context in which the word appears (Mitchell and Lapata, 2008, for example). Contextualization has been found to improve performance in tasks like lexical substitution and word sense disambiguation (Thater et al., 2011). In this paper, we propose to inject contextualized distributional semantic vectors into generative probabilistic models, in order to combine their complementary strengths for domain modelling. There are a number of potential advantages that distributional semantic models offer. First, they provide domain-general representations of word meaning that cannot be reliably estimated from the small target-domain corpora on which probabilistic"
P13-1039,D08-1094,0,0.12969,"Missing"
P13-1039,W12-3018,0,0.0423839,"Missing"
P13-1039,P06-2027,0,0.0334713,"formal structures that are assumed to represent a 392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose P RO F INDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas P RO F INDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare w"
P13-1039,W03-1203,0,0.0428824,"d´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor perRelated Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006;"
P13-1039,I11-1127,0,0.0768033,"Missing"
P13-1039,D11-1129,0,0.0444886,"Missing"
P13-1039,N03-1033,0,0.0123923,"dard Inside-Outside and tree-Viterbi algorithms, except that the tree structure is fixed, so there is no need to sum over all possible subtrees. Model parameters are learned by the ExpectationMaximization (EM) algorithm. We tune the hyperparameters (NE , NS , δ, β, k) and the number of EM iterations by two-fold cross-validation1 . 3.3 Experiments We then trained D S H MM and conducted our evaluations on the TAC 2010 guided summarization data set (Owczarzak and Dang, 2010). Lemmatization and extraction of event heads and arguments are done by preprocessing with the Stanford CoreNLP tool suite (Toutanova et al., 2003; de Marneffe et al., 2006). This data set contains 46 topic clusters of 20 articles each, grouped into five topic categories or domains. For example, one topic cluster in the ATTACK category is about the Columbine Massacre. Each topic cluster contains eight human-written “model” summaries (“model” here meaning a gold standard). Half of the articles and model summaries in a topic cluster are used in the guided summarization task, and the rest are used in the update summarization task. Summary and Generative Process In summary, the following steps are applied to train a D S H MM: 1. Train a dis"
P13-1039,N09-1041,0,0.21247,"Domain Modelling With Contextualized Distributional Semantic Vectors Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu Jackie Chi Kit Cheung University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 jcheung@cs.toronto.edu Abstract domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the C RIMINAL I NVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include V ICTIM, S USPECT, AUTHORITIES,"
P13-1039,D12-1022,0,0.0280973,")||Q s=1 To produce a summary, sentences from the source text are greedily added such that KLScore is minimized at each step, until the desired summary length is reached, discarding sentences with fewer than five words. 6.2 A KL-based Criterion Supervised Learning The above unsupervised method results in summaries that closely mirror the source text in terms of the event and slot distributions, but this ignores the fact that not all such topics should be included in a summary. It also ignores genrespecific, stylistic considerations about characteristics of good summary sentences. For example, Woodsend and Lapata (2012) find several factors that indicate sentences should not be included in an extractive summary, such as the presence of personal pronouns. Thus, we implemented a second method, in which we modify the KL criterion above by estimating Pˆ E and Pˆ S from other model summaries that are drawn from the same domain (i.e. topic category), except for those summaries that are written for the specific topic cluster to be used for evaluation. There are four main component distributions from our model that should be considered during extraction: (1) the distribution of events, (2) the distribution of slots,"
P13-1039,W04-1013,0,0.00565936,"Missing"
P13-1039,D12-1106,0,\N,Missing
P13-1121,J05-3002,0,0.396312,"re term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less examined issue is that of aggregation and information synthesis. A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user. For example, a series of related events can be aggregated and expressed as a trend. The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference. Instead, summarization systems need to make more use of domain knowledge. We provid"
P13-1121,N04-1038,0,0.0136924,"bj) (kill, dobj) (die, nsubj) (rise, dobj) (drop, prep to) Sim. 0.82 0.80 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet"
P13-1121,P11-1098,0,0.0234744,"ep to) Sim. 0.82 0.80 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet E VENT synset), or nominal or adjectival predicate. Then,"
P13-1121,hovy-etal-2006-automated,0,0.0697793,"Missing"
P13-1121,P06-2020,0,0.0242331,"Missing"
P13-1121,C00-1072,0,0.377568,"hould contain the parts of the source text that are most similar or representative of the source text. This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of Carbonell and Goldstein (1998), which defines the summarization objective to be a linear combination of a centrality term and a non-redundancy term. Since MMR, much progress has been made on more sophisticated methods of measuring centrality and integrating it with non-redundancy (See Nenkova and McKeown (2011) for a recent survey). For example, term weighting methods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less e"
P13-1121,W03-0510,0,0.0803654,"Missing"
P13-1121,de-marneffe-etal-2006-generating,0,0.016178,"Missing"
P13-1121,W04-1013,0,0.0479388,"Missing"
P13-1121,D09-1032,0,0.0252176,"Missing"
P13-1121,N03-2024,0,0.034325,"ppropriate for our analysis. Word overlap can occur due to shared proper nouns or entity mentions. Good summaries should certainly contain the salient entities in the source text, but when assessing the effect of the domain, different domain instances (i.e., different document clusters in the same domain) would be expected to contain different salient entities. Also, the realization of entities as noun phrases depends strongly on context, which would confound our analysis if we do not also correctly resolve coreference, a difficult problem in its own right. We leave such issues to other work (Nenkova and McKeown, 2003, e.g.). Domains would rather be expected to share slots (a.k.a. aspects), which require a more semantic level of analysis that can account for the various ways in which a particular slot can be expressed. Another consideration is that the structures to be analyzed should be extracted automatically. Based on these criteria, we selected caseframes to be the appropriate unit of analysis. A caseframe is a shallow approximation of the semantic role structure of a proposition-bearing unit like a verb, and are derived from the dependency parse of a sentence1 . 1 Note that caseframes are distinct fro"
P13-1121,N04-1019,0,0.290124,"Missing"
P13-1121,J98-3005,0,0.175416,"Missing"
P13-1121,C10-2122,0,0.0234837,"Missing"
P13-1121,H01-1054,0,0.0173384,"Missing"
P13-1121,A00-2024,0,\N,Missing
P13-1121,J02-4005,0,\N,Missing
P13-1121,W01-0100,0,\N,Missing
P16-2019,D14-1162,0,0.0950273,"ed by minimizing the hinge loss over the training set: X X L= [γ+d(h+l, t)−d(h0 +l, t0 )]+ (h,l,t)∈S (h0 ,l,t0 )∈S 0 4 4.1 where γ is the hinge loss margin and [x]+ represents the positive portion of x. There is an additional constraint that the L2 -norm of entity embeddings (but not relation embeddings) must be 1, which prevents the training process to trivially minimize L by artificially increasing the norms of entity embeddings. 3.2 experimentally to reduce overfitting. We obtain these word vectors using distributed representations computed using word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014). Approximating compositionality by averaging vector representations is simple, yet has some theoretical justification (Tian et al., 2015) and can work well in practice (Wieting et al., 2015). Additional decisions need to be made concerning which parts of the entity description to include. In particular, if an entity description or word definition is longer than several sentences, using the entire description could cause a ‘dilution’ of the desired embedding, as not all sentences will be equally pertinent. We solve this by only considering the first sentence of any entity description, which is"
P16-2019,P15-1173,0,0.0557657,"Missing"
P16-2019,R09-1073,0,0.0237133,". Our method provides a better initialization for the TransE model, not just for the entities that do not appear in the data, but in fact for all entities. This is demonstrated by achieving stateof-the-art mean rank on an entity ranking task on two very different data sets: WordNet synsets with lexical semantic relations (Miller, 1995), and Freebase named entities with general semantic relations (Bollacker et al., 2008). 2 Table 2: Sample entity descriptions from WordNet and Freebase. As Freebase descriptions are lengthy paragraphs, only the first sentence is shown. such as synonym expansion (Sinha and Mihalcea, 2009), relation extraction (Kambhatla, 2004), and calculating the semantic distance between concepts (Mohammad, 2008; Marton et al., 2009). We aim to combine lexical resources and other semantic knowledge, but we do so in the context of neural network-based word embeddings, rather than in specific lexical semantic tasks. Bordes et al. (2011) propose the Structured Embeddings (SE) model, which embeds entities into vectors and relations into matrices. The relation connection between two entities is modeled by the projection of their embeddings into a different vector space. Rothe and Sch¨utze (2015)"
P16-2019,D14-1110,0,0.0786316,"Missing"
P16-2019,N15-1184,0,0.0229715,"e and Sch¨utze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes. Perhaps most related to our work are previous relational models that initialize their embeddings via distributional semantics calculated from a larger corpus. Socher et al. (2013) propose the Neural Tensor Network (NTN), and Yang et al. (2015) the Bilinear model using this technique. Other approaches modify the objective function or change the structure of the model in order to integrate distributional and relational information (Xu et al., 2014; Fried and Duh, 2015; Toutanova and Chen, 2015). Faruqui et al. (2015) retrofit word vectors after they are trained according to distributional criteria. We propose a method that does not necessitate post-processing of the embeddings, and can be applied orthogonally to the previously mentioned improvements. Related Work Dictionary definitions were the core component of early methods in word sense disambiguation (WSD), such as the Lesk algorithm (1986). Chen et al. (2014) build on the use of synset glosses for WSD by leveraging lexical resources. Our work goes further to tie these glosses together with relational semantics, a connection that has not been drawn in"
P16-2019,W15-4007,0,0.0182722,"ifferent vector space. Rothe and Sch¨utze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes. Perhaps most related to our work are previous relational models that initialize their embeddings via distributional semantics calculated from a larger corpus. Socher et al. (2013) propose the Neural Tensor Network (NTN), and Yang et al. (2015) the Bilinear model using this technique. Other approaches modify the objective function or change the structure of the model in order to integrate distributional and relational information (Xu et al., 2014; Fried and Duh, 2015; Toutanova and Chen, 2015). Faruqui et al. (2015) retrofit word vectors after they are trained according to distributional criteria. We propose a method that does not necessitate post-processing of the embeddings, and can be applied orthogonally to the previously mentioned improvements. Related Work Dictionary definitions were the core component of early methods in word sense disambiguation (WSD), such as the Lesk algorithm (1986). Chen et al. (2014) build on the use of synset glosses for WSD by leveraging lexical resources. Our work goes further to tie these glosses together with relational semantics, a connection tha"
P16-2019,P15-1067,0,0.037376,"using the original and most common metrics for relational models: i) the mean of the predicted ranks, and ii) hits@10, which represents the percentage of correct entities found in the top 10 list; however, other metrics are possible, such as mean reciprocal rank (MRR). We evaluate in both the filtered setting, where other correct responses are removed from the lists ranked by the model, and the raw setting, where no changes are made. We compare against the TransE model with random initialization, and the SE model (Bordes et al., 2011). We also compare against the state-ofthe-art TransD model (Ji et al., 2015). This model uses two vectors to represent each entity and relation; one to represent the meaning of the entity, and one to construct a mapping matrix dynamically. This allows for the representation of more diverse entities. 4.2 For Freebase, our models slightly outperform the TransE model with random initialization, with p-values of 0.173 and 0.410 for initialization with descriptions (including stopwords) using GloVe and word2vec, respectively. We also see improvements over the case of direct initialization with word2vec. Further, we set a new state-of-the-art for mean rank on the raw data,"
P16-2019,P13-2087,0,0.0445188,"Missing"
P16-2019,D09-1081,0,0.0196149,"for all entities. This is demonstrated by achieving stateof-the-art mean rank on an entity ranking task on two very different data sets: WordNet synsets with lexical semantic relations (Miller, 1995), and Freebase named entities with general semantic relations (Bollacker et al., 2008). 2 Table 2: Sample entity descriptions from WordNet and Freebase. As Freebase descriptions are lengthy paragraphs, only the first sentence is shown. such as synonym expansion (Sinha and Mihalcea, 2009), relation extraction (Kambhatla, 2004), and calculating the semantic distance between concepts (Mohammad, 2008; Marton et al., 2009). We aim to combine lexical resources and other semantic knowledge, but we do so in the context of neural network-based word embeddings, rather than in specific lexical semantic tasks. Bordes et al. (2011) propose the Structured Embeddings (SE) model, which embeds entities into vectors and relations into matrices. The relation connection between two entities is modeled by the projection of their embeddings into a different vector space. Rothe and Sch¨utze (2015) use Wordnet as a lexical resource to learn embeddings for synsets and lexemes. Perhaps most related to our work are previous relation"
P16-2019,N13-1090,0,0.0298424,"remains large. This is accomplished by minimizing the hinge loss over the training set: X X L= [γ+d(h+l, t)−d(h0 +l, t0 )]+ (h,l,t)∈S (h0 ,l,t0 )∈S 0 4 4.1 where γ is the hinge loss margin and [x]+ represents the positive portion of x. There is an additional constraint that the L2 -norm of entity embeddings (but not relation embeddings) must be 1, which prevents the training process to trivially minimize L by artificially increasing the norms of entity embeddings. 3.2 experimentally to reduce overfitting. We obtain these word vectors using distributed representations computed using word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014). Approximating compositionality by averaging vector representations is simple, yet has some theoretical justification (Tian et al., 2015) and can work well in practice (Wieting et al., 2015). Additional decisions need to be made concerning which parts of the entity description to include. In particular, if an entity description or word definition is longer than several sentences, using the entire description could cause a ‘dilution’ of the desired embedding, as not all sentences will be equally pertinent. We solve this by only considering the first senten"
P18-1256,P17-1055,0,0.0299149,"ghted-pooling neural network architecture (WP). The tokenized input is embedded with pretrained word embeddings and possibly concatenated with one-hot encoded POS tags. The input is then encoded with a bi-directional LSTM, followed by our attention mechanism. The computed attention scores are then used as weights to average the encoded states, in turn connected to a fully connected layer to predict presupposition triggering. pair-wise matching matrix: M = H >H (3) where M is a square matrix ∈ RT ×T . To get a single attention weight per time step, we adopt the attention-over-attention method (Cui et al., 2017). With matrix M , we first compute row-wise attention score Mijr over M : Mijr exp(eij ) = PT t=1 exp(eit ) (4) where eij = Mij . M r can be interpreted as a word-level attention distribution over all other words. Since we would like a single weight per word, we need an additional step to aggregate these attention scores. Instead of simply averaging the scores, we follow (Cui et al., 2017)’s approach which learns the aggregation by an additional attention mechanism. We compute columnwise softmax Mijc over M : exp(eij ) Mijc = PT t=1 exp(etj ) (5) The columns of M r are then averaged, forming v"
P18-1256,P82-1020,0,0.848334,"Missing"
P18-1256,C16-1247,1,0.691629,"of attention-based deep learning models for detecting adverbial triggers. Attention is a promising approach to this task because it allows a model to weigh information from multiple points in the previous context and infer long-range dependencies in the data (Bahdanau et al., 2015). For example, the model could learn to detect multiple instances involving John and restaurants, which would be a good indication that again is appropriate in that context. Also, an attention-based RNN has achieved success in predicting article definiteness, which involves another class of presupposition triggers (Kabbara et al., 2016). As another contribution, we introduce a new weighted pooling attention mechanism designed for predicting adverbial presupposition triggers. Our attention mechanism allows for a weighted averaging of our RNN hidden states where the weights are informed by the inputs, as opposed to a simple unweighted averaging. Our model uses a form of self-attention (Paulus et al., 2018; Vaswani et al., 2017), where the input sequence acts as both the attention mechanism’s query and key/value. Unlike other attention models, instead of simply averaging the scores to be weighted, our approach aggregates (learn"
P18-1256,D14-1181,0,0.00400831,"LSTM hidden states, across all time steps. This is followed by a fully connected layer and softmax function for the binary classification. Our WP model uses the same bidirectional LSTM as this baseline LSTM, and has the same number of parameters, allowing for a fair comparison of the two models. Such a standard LSTM model represents a state-of-the-art language model, as it outperforms more recent models on language modeling tasks when the number of model parameters is controlled for (Melis et al., 2017). For the last model, we use a slight variant of the CNN sentence classification model of (Kim, 2014) based on the Britz tensorflow implementation2 . 5.2 Hyperparameters & Additional Features After tuning, we found the following hyperparameters to work best: 64 units in fully connected layers and 40 units for POS embeddings. We used dropout with probability 0.5 and mini-batch size of 64. For all models, we initialize word embeddings with word2vec (Mikolov et al., 2013) pretrained embeddings of size 300. Unknown words are randomly initialized to the same size as the word2vec embeddings. In early tests on the development datasets, we found that our neural networks would consistently perform bet"
P18-1256,W00-0603,0,0.125854,"twork to predict noun phrase definiteness in English. Their work demonstrates the ability of these attention-based models to pick up on contextual cues for pragmatic reasoning. 2748 Many different classes of construction can trigger presupposition in an utterance, this includes but is not limited to stressed constituents, factive verbs, and implicative verbs (Zare et al., 2012). In this work, we focus on the class of adverbial presupposition triggers. Our task setup resembles the Cloze test used in psychology (Taylor, 1953; E. B. Coleman, 1968; Earl F. Rankin, 1969) and machine comprehension (Riloff and Thelen, 2000), which tests text comprehension via a fill-in-the-blanks task. We similarly pre-process our samples such that they are roughly the same length, and have equal numbers of negative samples as positive ones. However, we avoid replacing the deleted words with a blank, so that our model has no clue regarding the exact position of the possibly missing trigger. Another related work on the Children’s Book Test (Hill et al., 2015) notes that memories that encode sub-sentential chunks (windows) of informative text seem to be most useful to neural networks when interpreting and modelling language. Their"
P18-1256,P02-1017,0,0.137,"ve binary classification tasks. This was not feasible for PTB because of its small size. Finally, because of the commonalities between the adverbs in presupposing similar events, we create a dataset that unifies all instances of the five adverbs found in the Gigaword corpus, with a label “1” indicating the presence of any of these adverbs. 3.2 Data extraction process We define a sample in our dataset as a 3-tuple, consisting of a label (representing the target adverb, or ‘none’ for a negative sample), a list of tokens we extract (before/after the adverb), and a list of corresponding POS tags (Klein and Manning, 2002). In each sample, we also add a special token “@@@@” right before the head word and the corresponding POS tag of the head word, both in positive and negative cases. We add such special tokens to identify the candidate context in the passage to the model. Figure 1 shows a single positive sample in our dataset. We first extract positive contexts that contain a triggering adverb, then extract negative contexts 2749 that do not, controlling for a number of potential confounds. Our positive data consist of cases where the target adverb triggers presupposition by modifying a certain head word which,"
P18-1256,P14-5010,0,0.00511139,"Missing"
P18-1256,J93-2004,0,0.0652305,"le, in language generation applications such as summarization and dialogue systems, adding presuppositional triggers in contextually appropriate loca1 Presupposition of existence are triggered by possessive constructions, names or definite noun phrases. 2747 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2747–2755 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tions can improve the readability and coherence of the generated output. We create two datasets based on the Penn Treebank corpus (Marcus et al., 1993) and the English Gigaword corpus (Graff et al., 2007), extracting contexts that include presupposition triggers as well as other similar contexts that do not, in order to form a binary classification task. In creating our datasets, we consider a set of five target adverbs: too, again, also, still, and yet. We focus on these adverbs in our investigation because these triggers are well known in the existing linguistic literature and commonly triggering presuppositions. We control for a number of potential confounding factors, such as class balance, and the syntactic governor of the triggering ad"
P19-1067,P18-1152,0,0.0232516,"transfer to unseen categories of discourse on Wikipedia articles. 1 Introduction Coherence is a discourse property that is concerned with the logical and semantic organization of a passage, such that the overall meaning of the passage is expressed fluidly and clearly. It is an important quality measure for text generated by humans or machines, and modelling coherence can benefit many applications, including summarization, question answering (Verberne et al., 2007), essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) and text generation (Park and Kim, 2015; Kiddon et al., 2016; Holtzman et al., 2018). The ability to generalize to new domains of text is desirable for NLP models in general. Besides the practical reason of avoiding costly retraining ∗ Work done while the author was an intern at Borealis AI. 678 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 678–687 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics that there are n! possible sentence orderings for a passage with n sentences, thus the sampled negative instances can only cover a tiny proportion of this space, limiting the performance of su"
P19-1067,D16-1032,0,0.0146629,"allenging settings of transfer to unseen categories of discourse on Wikipedia articles. 1 Introduction Coherence is a discourse property that is concerned with the logical and semantic organization of a passage, such that the overall meaning of the passage is expressed fluidly and clearly. It is an important quality measure for text generated by humans or machines, and modelling coherence can benefit many applications, including summarization, question answering (Verberne et al., 2007), essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) and text generation (Park and Kim, 2015; Kiddon et al., 2016; Holtzman et al., 2018). The ability to generalize to new domains of text is desirable for NLP models in general. Besides the practical reason of avoiding costly retraining ∗ Work done while the author was an intern at Borealis AI. 678 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 678–687 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics that there are n! possible sentence orderings for a passage with n sentences, thus the sampled negative instances can only cover a tiny proportion of this space, limiti"
P19-1067,J08-1001,0,0.491209,"lize well for cross-domain coherence scoring, with a novel local discriminative neural model. 2. We propose a set of cross-domain coherence datasets with increasingly difficult evaluation protocols. 3. Our new method outperforms previous methods by a significant margin on both the previous closed domain WSJ dataset as well as on all open-domain ones, setting the new stateof-the-art for coherence modelling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li"
P19-1067,N10-1099,0,0.20968,"Missing"
P19-1067,D17-1070,0,0.125838,"effectively covered by a sampling procedure. In practice, we sample a new set of negatives each time we see a document, hence after many epochs, we can effectively cover the space for even very long documents. Section 5.7 discusses further details on sampling. 4.1 Figure 1: Generic architecture for our proposed model. 4.2 Pre-trained Generative Model as the Sentence Encoder Our model can work with any pre-trained sentence encoder, ranging from the most simplistic average GloVe (Pennington et al., 2014) embeddings to more sophisticated supervised or unsupervised pre-trained sentence encoders (Conneau et al., 2017). As mentioned in the introduction, since generative models can often be turned into sentence encoder, generative coherence model can be leveraged by our model to benefit from the advantages of both generative and discriminative training, similar to (Kiros et al., 2015; Peters et al., 2018). After initialization, we freeze the generative model parameters to avoid overfitting. In Section 5, we will experimentally show that while we do benefit from strong pre-trained encoders, the fact that our local discriminative model improves over previous methods is independent of the choice of sentence enc"
P19-1067,D14-1218,0,0.512794,"08) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach to coherence. First, it assumes that conditional log likelihood is a good proxy for coherence. Second, it assumes that training can well capture the long-range dependencies implied by the generative model. Conditional log likelihood essential"
P19-1067,P11-2022,0,0.348976,"setting the new stateof-the-art for coherence modelling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum"
P19-1067,D17-1019,0,0.313058,"tly ordered list of sentences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative. Discriminative models depend on contrastive learning; they use automatic corruption methods to generate incoherent passages of text, then learn to distinguish coherent passages from incoherent ones. By contrast, generative approaches aim at maximising the likelihood of the training text, which is assumed to be coherent, without seeing incoherent text or explicitly incorporating coherence into the optimization objective. It has been argued that"
P19-1067,N07-1055,0,0.676238,"e semantic relationships between sentences, rather than simply overfit to the structural cues of a specific domain. The standard task used to test a coherence model in NLP is sentence ordering, for example, to distinguish between a coherently ordered list of sentences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative. Discriminative models depend on contrastive learning; they use automatic corruption methods to generate incoherent passages of text, then learn to distinguish coherent passages from incoherent ones. By contr"
P19-1067,D12-1106,0,0.162649,"Ve, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach to coherence. First, it assumes that conditional log likelihood is a good proxy for coheren"
P19-1067,E12-1032,0,0.0246755,"-art for coherence modelling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach t"
P19-1067,J95-2003,0,0.927768,"realisai.com 2 McGill University {joey.bose,jcheung}@cs.mcgill.ca 3 Canada CIFAR Chair, Mila Abstract on every new domain, for coherence modelling, we would also like our model to make decisions based on the semantic relationships between sentences, rather than simply overfit to the structural cues of a specific domain. The standard task used to test a coherence model in NLP is sentence ordering, for example, to distinguish between a coherently ordered list of sentences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative."
P19-1067,P18-1052,0,0.153447,"a coherent sentence does not need to have high conditional loglikelihood, as log likelihood can also be influenced by other factors such as fluency, grammaticality, sentence length, and the frequency of words in a sentence. Second, capturing long-range dependencies in neural sequence models is still an active area of research with many challenges (Trinh et al., 2018), hence there is no guarantee that maximum likelihood learning can faithfully capture the inductive bias behind the first assumption. grid model with convolutional neural network that operates over the entity grid representation. Mohiuddin et al. (2018) extended this model for written asynchronous conversations. Both methods rely on hand-crafted features derived from NLP preprocessing tools to enhance the original entity grid representation. We take a different approach to feature engineering in our work, focusing on the effect of supervised or unsupervised pre-training. Li and Jurafsky (2017) was the first work to use generative models to model coherence and proposed to evaluate the performance of coherence models in an open-domain setting. Most recently, Logeswaran et al. (2018) used an RNN based encoder-decoder architecture to model the c"
P19-1067,P13-1010,0,0.125197,"elling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach to coherence. First, it assumes"
P19-1067,P17-1121,0,0.716253,"tences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative. Discriminative models depend on contrastive learning; they use automatic corruption methods to generate incoherent passages of text, then learn to distinguish coherent passages from incoherent ones. By contrast, generative approaches aim at maximising the likelihood of the training text, which is assumed to be coherent, without seeing incoherent text or explicitly incorporating coherence into the optimization objective. It has been argued that neural discriminative"
P19-1067,D14-1162,0,0.0865179,"er of negatives provides a rich enough learning signal, while at the same time, is not too prohibitively large to be effectively covered by a sampling procedure. In practice, we sample a new set of negatives each time we see a document, hence after many epochs, we can effectively cover the space for even very long documents. Section 5.7 discusses further details on sampling. 4.1 Figure 1: Generic architecture for our proposed model. 4.2 Pre-trained Generative Model as the Sentence Encoder Our model can work with any pre-trained sentence encoder, ranging from the most simplistic average GloVe (Pennington et al., 2014) embeddings to more sophisticated supervised or unsupervised pre-trained sentence encoders (Conneau et al., 2017). As mentioned in the introduction, since generative models can often be turned into sentence encoder, generative coherence model can be leveraged by our model to benefit from the advantages of both generative and discriminative training, similar to (Kiros et al., 2015; Peters et al., 2018). After initialization, we freeze the generative model parameters to avoid overfitting. In Section 5, we will experimentally show that while we do benefit from strong pre-trained encoders, the fac"
P19-1067,N18-1202,0,0.0109268,"ture for our proposed model. 4.2 Pre-trained Generative Model as the Sentence Encoder Our model can work with any pre-trained sentence encoder, ranging from the most simplistic average GloVe (Pennington et al., 2014) embeddings to more sophisticated supervised or unsupervised pre-trained sentence encoders (Conneau et al., 2017). As mentioned in the introduction, since generative models can often be turned into sentence encoder, generative coherence model can be leveraged by our model to benefit from the advantages of both generative and discriminative training, similar to (Kiros et al., 2015; Peters et al., 2018). After initialization, we freeze the generative model parameters to avoid overfitting. In Section 5, we will experimentally show that while we do benefit from strong pre-trained encoders, the fact that our local discriminative model improves over previous methods is independent of the choice of sentence encoder. Model Architecture The specific neural architecture that we use for fθ is illustrated in Figure 1. We assume the use of some pre-trained sentence encoder, which is discussed in the next section. Given an input sentence pair, the sentence encoder maps the sentences to real-valued vecto"
P19-1331,I17-1030,0,0.401005,"pata (2017) propose to use reinforcement learning methods on RNNs to optimize a specific-designed reward based on simplicity, fluency and relevancy; Vu et al. (2018) incorporate memory-augmented neural networks for sentence simplification; Zhao et al. (2018) integrate the transformer architecture and PPDB rules to guide the simplification learning; Sulem et al. (2018b) combine neural MT models with sentence splitting modules for sentence simplification. Edit-based Sentence Simplification The only previous work on sentence simplification by explicitly predicting simplification operations is by Alva-Manchego et al. (2017). Alva-Manchego et al. (2017) use MASSAlign (Paetzold et al., 2017) to obtain ‘silver’ labels for simplification edits and employ a BiLSTM to sequentially predict three of their silver labels—KEEP, REPLACE and DELETE. Essentially, their labelling model is a non-autoregressive classifier with three classes, where a downstream module (Paetzold and Specia, 2017) is required for applying the REPLACE operation and providing the replacement word. We instead propose an end-toend neural programmer-interpreter model for sentence simplification, which does not rely on external simplification rules nor a"
P19-1331,W17-4772,0,0.0999675,"Missing"
P19-1331,E99-1042,0,0.473631,"(without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences1 . 1 Introduction Sentence simplification aims to reduce the reading complexity of a sentence while preserving its meaning. Simplification systems can benefit populations with limited literacy skills (Watanabe et al., 2009), such as children, second language speakers and individuals with language impairments including dyslexia (Rello et al., 2013), aphasia (Carroll et al., 1999) and autism (Evans et al., 2014). Inspired by the success of machine translation, many text simplification (TS) systems treat sentence simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 20"
P19-1331,W14-1215,0,0.0890537,"rge margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences1 . 1 Introduction Sentence simplification aims to reduce the reading complexity of a sentence while preserving its meaning. Simplification systems can benefit populations with limited literacy skills (Watanabe et al., 2009), such as children, second language speakers and individuals with language impairments including dyslexia (Rello et al., 2013), aphasia (Carroll et al., 1999) and autism (Evans et al., 2014). Inspired by the success of machine translation, many text simplification (TS) systems treat sentence simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily"
P19-1331,P16-1154,0,0.06857,"perations explicitly. An overview of our model is shown in Figure 1. 3.1 EditNTS Model EditNTS frames the simplification process as executing a sequence of edit operations on complex tokens monotonically. We define the edit operations as {ADD(W), KEEP, DELETE, STOP}. Similar to the sequence-to-sequence learning models, we assume a fixed-sized vocabulary of V words that can be added. Therefore, the number of prediction candidates of the programmer is V + 3 after including KEEP, DELETE, and STOP. To solve the out-of-vocabulary (OOV) problem, conventional Seq2Seq models utilize a copy mechanism (Gu et al., 2016) that selects a word from source (complex) sentence directly with a trainable pointer. In contrast, EditNTS has the ability to copy OOV words into the simplified sentences by directly learning to predict KEEP on them in complex sentences. We argue that our method has advantage over a copy mechanism in two ways: 1) our method does not need extra parameters for copying; 2) a copy mechanism may lead to the model copying blindly rather than performing simplifications. We detail other constraints on the edit opera3395 Figure 1: Our model contains two parts: the programmer and the interpreter. At ti"
P19-1331,P13-1151,0,0.061958,"nces. 4 4.1 Experiments Dataset Three benchmark text simplification datasets are used in our experiments. WikiSmall contains automatically aligned complex-simple sentence pairs from standard to simple English Wikipedia (Zhu et al., 2010). We use the standard splits of 88,837/205/100 provided by Zhang and Lapata (2017) as train/dev/test sets. WikiLarge (Zhang and Lapata, 2017) is the largest TS corpus with 296,402/2000/359 complex-simple sentence pairs for training/validating/testing, constructed by merging previously created simplification corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). In addition to the automatically aligned references, Xu et al. (2016) created eight more human-written simplified references for each complex sentence in the development/test set of WikiLarge. The third dataset is Newsela (Xu et al., 2015), which consists of 1130 news articles. Each article is rewritten by professional editors four times for children at different grade levels (0-4 from complex to simple). We use the standard splits provided by Zhang and Lapata (2017), which contains 94,208/1129/1076 sentence pairs for train/dev/test. Table 4 provides other statistics on these three benchmark"
P19-1331,P17-1015,0,0.0377083,"toend neural programmer-interpreter model for sentence simplification, which does not rely on external simplification rules nor alignment tools2 . 2 Our model can be combined with these external knowledge base and alignment tools for further performance improvements. Neural Programmer-Interpreter Models The neural programmer-interpreter (NPI) was first proposed by Reed and de Freitas (2016) as a machine learning model that learns to execute programs given their execution traces. Their experiments demonstrate success for 21 tasks including performing addition and bubble sort. It was adopted by Ling et al. (2017) to solve algebraic word problems and by B´erard et al. (2017); Vu and Haffari (2018) to perform automatic post-editing on machine translation outputs. We instead design our NPI model to take monolingual complex input sentences and learn to perform simplification operations on them. 3 Model Conventional sequence-to-sequence learning models map a sequence x = x1 , . . . , x|x |to another one y = y1 , . . . , y|y |, where elements of x and y are drawn from a vocabulary of size V , by modeling the conditional distribution P (yt |y1:t−1 , x) directly. Our proposed model, EditNTS, tackles sentence"
P19-1331,P14-1041,0,0.719266,"(Carroll et al., 1999) and autism (Evans et al., 2014). Inspired by the success of machine translation, many text simplification (TS) systems treat sentence simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily integrate human-curated features into the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimple sentences implicitly. These operations are relatively infrequent, as a large part of the original complex sentence usually remains unchanged in the simplification process (Zhang et al., 2017). This leads to MT-based"
P19-1331,P17-2014,0,0.362824,"simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily integrate human-curated features into the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimple sentences implicitly. These operations are relatively infrequent, as a large part of the original complex sentence usually remains unchanged in the simplification process (Zhang et al., 2017). This leads to MT-based models that often produce outputs that are identical to the inputs (Zhao et al., 2018), which is also confirmed in our experiments. We instead propose"
P19-1331,I17-3001,0,0.0269729,"imize a specific-designed reward based on simplicity, fluency and relevancy; Vu et al. (2018) incorporate memory-augmented neural networks for sentence simplification; Zhao et al. (2018) integrate the transformer architecture and PPDB rules to guide the simplification learning; Sulem et al. (2018b) combine neural MT models with sentence splitting modules for sentence simplification. Edit-based Sentence Simplification The only previous work on sentence simplification by explicitly predicting simplification operations is by Alva-Manchego et al. (2017). Alva-Manchego et al. (2017) use MASSAlign (Paetzold et al., 2017) to obtain ‘silver’ labels for simplification edits and employ a BiLSTM to sequentially predict three of their silver labels—KEEP, REPLACE and DELETE. Essentially, their labelling model is a non-autoregressive classifier with three classes, where a downstream module (Paetzold and Specia, 2017) is required for applying the REPLACE operation and providing the replacement word. We instead propose an end-toend neural programmer-interpreter model for sentence simplification, which does not rely on external simplification rules nor alignment tools2 . 2 Our model can be combined with these external k"
P19-1331,E17-2006,0,0.0226049,". (2018b) combine neural MT models with sentence splitting modules for sentence simplification. Edit-based Sentence Simplification The only previous work on sentence simplification by explicitly predicting simplification operations is by Alva-Manchego et al. (2017). Alva-Manchego et al. (2017) use MASSAlign (Paetzold et al., 2017) to obtain ‘silver’ labels for simplification edits and employ a BiLSTM to sequentially predict three of their silver labels—KEEP, REPLACE and DELETE. Essentially, their labelling model is a non-autoregressive classifier with three classes, where a downstream module (Paetzold and Specia, 2017) is required for applying the REPLACE operation and providing the replacement word. We instead propose an end-toend neural programmer-interpreter model for sentence simplification, which does not rely on external simplification rules nor alignment tools2 . 2 Our model can be combined with these external knowledge base and alignment tools for further performance improvements. Neural Programmer-Interpreter Models The neural programmer-interpreter (NPI) was first proposed by Reed and de Freitas (2016) as a machine learning model that learns to execute programs given their execution traces. Their"
P19-1331,D14-1162,0,0.0823765,"aluations5 of our system outputs compared to the best MT-based systems, external knowledge-based systems, and Seq-Label by three human judges6 with a five-point Likert scale. The volunteers are asked to rate simplifications on three dimensions: 1) fluency (is the output grammatical?), 2) adequacy (how much meaning from the original sentence is preserved?), and 3) simplicity (is the output simper than the original sentence?). 4.4 Training Details We used the same hyperparameters across the three datasets. We initialized the word and edit operation embeddings with 100-dimensional GloVe vectors (Pennington et al., 2014) and the part-ofspeech tag 7 embeddings with 30 dimensions. The number of hidden units was set to 200 for the encoder, the edit LSTM, and the LSTM interpreter. During training, we regularized the encoder with a dropout rate of 0.3 (Srivastava et al., 2014). For optimization, we used Adam (Kingma and Ba, 2014) with a learning rate 0.001 and weight decay of 10−6 . The gradient was clipped to 1 (Pascanu et al., 2013). We used a vocabulary size of 30K and the remaining words were replaced with UNK. In our main experiment, we used the inverse 5 The outputs of PBMT-R, Hybrid, SBMT-SARI and DRESS are"
P19-1331,Q16-1029,0,0.614922,"autism (Evans et al., 2014). Inspired by the success of machine translation, many text simplification (TS) systems treat sentence simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily integrate human-curated features into the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimple sentences implicitly. These operations are relatively infrequent, as a large part of the original complex sentence usually remains unchanged in the simplification process (Zhang et al., 2017). This leads to MT-based models that often"
P19-1331,D17-1062,0,0.646186,"stems can benefit populations with limited literacy skills (Watanabe et al., 2009), such as children, second language speakers and individuals with language impairments including dyslexia (Rello et al., 2013), aphasia (Carroll et al., 1999) and autism (Evans et al., 2014). Inspired by the success of machine translation, many text simplification (TS) systems treat sentence simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily integrate human-curated features into the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimp"
P19-1331,D18-1081,0,0.17708,"Missing"
P19-1331,P18-1016,0,0.0641878,"ernal knowledge base to encourage simplification. On the other side, many NMT-based models have also been proposed for sentence simplification: Nisioi et al. (2017) employ vanilla recurrent neural networks (RNNs) on text simplification; Zhang and Lapata (2017) propose to use reinforcement learning methods on RNNs to optimize a specific-designed reward based on simplicity, fluency and relevancy; Vu et al. (2018) incorporate memory-augmented neural networks for sentence simplification; Zhao et al. (2018) integrate the transformer architecture and PPDB rules to guide the simplification learning; Sulem et al. (2018b) combine neural MT models with sentence splitting modules for sentence simplification. Edit-based Sentence Simplification The only previous work on sentence simplification by explicitly predicting simplification operations is by Alva-Manchego et al. (2017). Alva-Manchego et al. (2017) use MASSAlign (Paetzold et al., 2017) to obtain ‘silver’ labels for simplification edits and employ a BiLSTM to sequentially predict three of their silver labels—KEEP, REPLACE and DELETE. Essentially, their labelling model is a non-autoregressive classifier with three classes, where a downstream module (Paetzol"
P19-1331,D18-1355,0,0.723048,"the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimple sentences implicitly. These operations are relatively infrequent, as a large part of the original complex sentence usually remains unchanged in the simplification process (Zhang et al., 2017). This leads to MT-based models that often produce outputs that are identical to the inputs (Zhao et al., 2018), which is also confirmed in our experiments. We instead propose a novel end-to-end Neural Programmer-Interpreter (Reed and de Freitas, 2016) that learns to explicitly generate edit operations in a sequential fashion, resembling the way that a human editor might perform simplifications on sentences. Our proposed framework consists of a programmer and an interpreter that operate alternately at each time step: the programmer predicts a simplifying edit operation (program) such as ADD, DELETE, or KEEP; the interpreter executes the edit operation while maintaining a context and an edit pointer to"
P19-1331,D18-1341,0,0.0358488,"not rely on external simplification rules nor alignment tools2 . 2 Our model can be combined with these external knowledge base and alignment tools for further performance improvements. Neural Programmer-Interpreter Models The neural programmer-interpreter (NPI) was first proposed by Reed and de Freitas (2016) as a machine learning model that learns to execute programs given their execution traces. Their experiments demonstrate success for 21 tasks including performing addition and bubble sort. It was adopted by Ling et al. (2017) to solve algebraic word problems and by B´erard et al. (2017); Vu and Haffari (2018) to perform automatic post-editing on machine translation outputs. We instead design our NPI model to take monolingual complex input sentences and learn to perform simplification operations on them. 3 Model Conventional sequence-to-sequence learning models map a sequence x = x1 , . . . , x|x |to another one y = y1 , . . . , y|y |, where elements of x and y are drawn from a vocabulary of size V , by modeling the conditional distribution P (yt |y1:t−1 , x) directly. Our proposed model, EditNTS, tackles sentence simplification in a different paradigm by learning the simplification operations expl"
P19-1331,C10-1152,0,0.859559,"dyslexia (Rello et al., 2013), aphasia (Carroll et al., 1999) and autism (Evans et al., 2014). Inspired by the success of machine translation, many text simplification (TS) systems treat sentence simplification as a monolingual translation task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily integrate human-curated features into the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimple sentences implicitly. These operations are relatively infrequent, as a large part of the original complex sentence usually remains unchanged in the simplification proce"
P19-1331,N18-2013,0,0.552089,"task, in which complex-simple sentence pairs 1 Link to our code and data can be found here https: //github.com/yuedongP/EditNTS. are presented to the models as source-target pairs (Zhang and Lapata, 2017). Two major machine translation (MT) approaches are adapted into TS systems, each with its advantages: statistical machine translation (SMT)-based models (Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) can easily integrate human-curated features into the model, while neural machine translation (NMT)-based models (Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018) can operate in an end-to-end fashion by extracting features automatically. Nevertheless, MTbased models must learn the simplifying operations that are embedded in the parallel complexsimple sentences implicitly. These operations are relatively infrequent, as a large part of the original complex sentence usually remains unchanged in the simplification process (Zhang et al., 2017). This leads to MT-based models that often produce outputs that are identical to the inputs (Zhao et al., 2018), which is also confirmed in our experiments. We instead propose a novel end-to-end Neural Programmer-Inter"
P19-1331,D11-1038,0,0.418618,"; 2) we design an NPI-based model that simulates the editing process by a programmer and an interpreter, which outperforms the state-of-the-art neural MT-based TS models by large margins in terms of SARI and is judged by humans as simpler and overall better. 2 Related Work MT-based Sentence Simplification SMT-based models and NMT-based models have been the main approaches for sentence simplification. They rely on learning simplification rewrites implic3394 itly from complex-simple sentence pairs. For SMT-based models, Zhu et al. (2010) adopt a tree-based SMT model for sentence simplification; Woodsend and Lapata (2011) propose a quasi-synchronous grammar and use integer linear programming to score the simplification rules; Wubben et al. (2012) employ a phrase-based MT model to obtain candidates and re-rank them based on the dissimilarity to the complex sentence; Narayan and Gardent (2014) develop a hybrid model that performs sentence splitting and deletion first and then re-rank the outputs similar to Wubben et al. (2012); Xu et al. (2016) propose SBMT-SARI, a syntax-based machine translation framework that uses an external knowledge base to encourage simplification. On the other side, many NMT-based models"
P19-1331,P12-1107,0,0.305907,"Missing"
P19-1331,Q15-1021,0,0.12371,"We use the standard splits of 88,837/205/100 provided by Zhang and Lapata (2017) as train/dev/test sets. WikiLarge (Zhang and Lapata, 2017) is the largest TS corpus with 296,402/2000/359 complex-simple sentence pairs for training/validating/testing, constructed by merging previously created simplification corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). In addition to the automatically aligned references, Xu et al. (2016) created eight more human-written simplified references for each complex sentence in the development/test set of WikiLarge. The third dataset is Newsela (Xu et al., 2015), which consists of 1130 news articles. Each article is rewritten by professional editors four times for children at different grade levels (0-4 from complex to simple). We use the standard splits provided by Zhang and Lapata (2017), which contains 94,208/1129/1076 sentence pairs for train/dev/test. Table 4 provides other statistics on these three benchmark training 3397 sets. Vocabulary size comp simp WikiLarge WikiSmall Newsela 201,841 113,368 41,066 168,962 93,835 30,193 Sentence length comp simp 25.17 24.26 25.94 18.51 20.33 15.89 Table 4: Statistics on the vocabulary sizes and the average"
P19-1386,N04-1038,0,0.103578,"capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task. 1 Introduction Coreference resolution is one of the best known tasks in Natural Language Processing (NLP). Despite a large body of work in the area over the last few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) ha"
P19-1386,P06-1005,0,0.123761,"Missing"
P19-1386,P15-1136,0,0.0684901,"s a cluster with the pronoun (No Decision), or creating a cluster that contains the pronoun with the wrong candidate (Incorrect Decision). To obtain a score specific to our task, we compute a Task-Specific Accuracy which discards all of the cases in which the model makes no decision relevant to the target pronoun or chooses both entities as co-referring to the target pronoun. 4 Experiments and Results In this section, we compare the performance of five representative coreference systems on our task: Stanford’s rule-based system (Raghunathan et al., 2010) (Rule), Stanford’s statistical system (Clark and Manning, 2015) (Stat), Clark and Manning (2016)’s deep reinforcement learning system (Deep-RL), Martschat and Strube (2015)’s latent tree model (Latent), and Lee et al. (2018)’s end-to-end neural system (E2E). We also report the accuracy of the state-of-the-art model, E2E, after retraining on K NOW R EF and on K NOW R EF+CoNLL. Additionally, we develop a task-specific model for K NOW R EF: a discriminatively trained finetuned instance of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018). We train our task-specific BERT according to recent work on language models (LMs) for"
P19-1386,D16-1245,0,0.108755,"facilitate performance comparisons. The quality of these tasks led to their widespread use and the emergence of many resolution systems, ranging from hand-engineered methods to deeplearning approaches. The multi-pass sieve system of Raghunathan et al. (2010) is fully deterministic and makes use of mention attributes like gender and number; it maintained the best results on the CoNLL 2011 task for a number of years (Lee et al., 2011). Later, lexical learning approaches emerged as the new state of the art (Durrett and Klein, 2013), followed more recently by neural models (Wiseman et al., 2016; Clark and Manning, 2016). The current state-of-the-art result on the CoNLL 2012 task is by an end-to-end neural model from Lee et al. (2018) that does not rely on a syntactic parser or a hand-engineered mention detector. 2.2 Gender bias in general coreference resolution Zhao et al. (2018) observed that state-of-the-art 3953 methods for coreference resolution become genderbiased, exploiting various stereotypes that leak from society into data. They devise a dataset of 3,160 manually written sentences called WinoBias that serves both as a gender-bias test for coreference resolution models and as a training set to count"
P19-1386,doddington-etal-2004-automatic,0,0.106883,"eterring models from exploiting surface cues, as well as in transferring to models trained on other co-reference tasks like GAP, leading to state-of-the-art results. 2 2.1 Related Work General coreference resolution Automated techniques for standard coreference resolution — that is, the task of correctly partitioning the entities and events that occur in a document into resolution classes — date back to decision trees and hand-written rules (Hobbs, 1977; McCarthy, 1995). The earliest evaluation corpora were the Message Understanding Conferences (MUC) (Grishman and Sundheim, 1996) and the ACE (Doddington et al., 2004). These focused on noun phrases tagged with coreference information, but were limited in either size or annotation coverage. The datasets of Pradhan et al. (2011, 2012) from the CoNLL-2011 and CoNLL-2012 Shared Tasks were proposed as large-scale corpora with high inter-annotator agreement. They were constructed by restricting the data to coreference phenomena with highly consistent annotations, and were packaged with a standard evaluation framework to facilitate performance comparisons. The quality of these tasks led to their widespread use and the emergence of many resolution systems, ranging"
P19-1386,P13-1012,0,0.0209092,"few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, including the CoNLL 2011 and 2012 shared coreference resolution tasks (Pradhan et al., 2011, 2012). These datasets were proposed as the first comprehensively tagged and large-scale corpora for coreference resolution, to spur progress in state-of-theart techniques. According to Durrett and Klein (2013), this progress would contribute in the “uphill battle” of modelling not just syntax and discourse, but also semantic compatibility based on world knowledge and context. Despite improvements in benchmark dataset performance,"
P19-1386,D13-1203,0,0.265562,"s * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, including the CoNLL 2011 and 2012 shared coreference resolution tasks (Pradhan et al., 2011, 2012). These datasets were proposed as the first comprehensively tagged and large-scale corpora for coreference resolution, to spur progress in state-of-theart techniques. According to Durrett and Klein (2013), this progress would contribute in the “uphill battle” of modelling not just syntax and discourse, but also semantic compatibility based on world knowledge and context. Despite improvements in benchmark dataset performance, the question of what exactly current systems learn or exploit remains open, particularly with recent neural coreference resolution models. Lee et al. (2017) note that their model does “little in the uphill battle of making coreference decisions that require world knowledge,” and highlight a few examples in the CoNLL 2012 task that rely on more complex understanding or infe"
P19-1386,C96-1079,0,0.0501675,"itching in expanding our corpus, further deterring models from exploiting surface cues, as well as in transferring to models trained on other co-reference tasks like GAP, leading to state-of-the-art results. 2 2.1 Related Work General coreference resolution Automated techniques for standard coreference resolution — that is, the task of correctly partitioning the entities and events that occur in a document into resolution classes — date back to decision trees and hand-written rules (Hobbs, 1977; McCarthy, 1995). The earliest evaluation corpora were the Message Understanding Conferences (MUC) (Grishman and Sundheim, 1996) and the ACE (Doddington et al., 2004). These focused on noun phrases tagged with coreference information, but were limited in either size or annotation coverage. The datasets of Pradhan et al. (2011, 2012) from the CoNLL-2011 and CoNLL-2012 Shared Tasks were proposed as large-scale corpora with high inter-annotator agreement. They were constructed by restricting the data to coreference phenomena with highly consistent annotations, and were packaged with a standard evaluation framework to facilitate performance comparisons. The quality of these tasks led to their widespread use and the emergen"
P19-1386,W11-1902,0,0.251188,"rea over the last few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, including the CoNLL 2011 and 2012 shared coreference resolution tasks (Pradhan et al., 2011, 2012). These datasets were proposed as the first comprehensively tagged and large-scale corpora for coreference resolution, to spur progress in state-of-theart techniques. According to Durrett and Klein (2013), this progress would contribute in the “uphill battle” of modelling not just syntax and discourse, but also semantic compatibility based on world knowledge and context. Despite improvements in benchmark"
P19-1386,D17-1018,0,0.335122,"000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, including the CoNLL 2011 and 2012 shared coreference resolution tasks (Pradhan et al., 2011, 2012). These datasets were proposed as the first comprehensively tagged and large-scale corpora for coreference resolution, to spur progress in state-of-theart techniques. According to Durrett and Klein (2013), this progress would contribute in the “uphill battle” of modelling not just syntax and discourse, but also semantic compatibility based on world knowledge and context. Despite improvements in benchmark dataset performance, the question of wh"
P19-1386,N18-2108,0,0.0677725,"Missing"
P19-1386,Q15-1029,0,0.0141336,"candidate (Incorrect Decision). To obtain a score specific to our task, we compute a Task-Specific Accuracy which discards all of the cases in which the model makes no decision relevant to the target pronoun or chooses both entities as co-referring to the target pronoun. 4 Experiments and Results In this section, we compare the performance of five representative coreference systems on our task: Stanford’s rule-based system (Raghunathan et al., 2010) (Rule), Stanford’s statistical system (Clark and Manning, 2015) (Stat), Clark and Manning (2016)’s deep reinforcement learning system (Deep-RL), Martschat and Strube (2015)’s latent tree model (Latent), and Lee et al. (2018)’s end-to-end neural system (E2E). We also report the accuracy of the state-of-the-art model, E2E, after retraining on K NOW R EF and on K NOW R EF+CoNLL. Additionally, we develop a task-specific model for K NOW R EF: a discriminatively trained finetuned instance of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018). We train our task-specific BERT according to recent work on language models (LMs) for the WSC (Trinh and Le, 2018). We first construct a modified version of the data wherein we duplicate each sen"
P19-1386,D10-1048,0,0.596042,"arge body of work in the area over the last few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, including the CoNLL 2011 and 2012 shared coreference resolution tasks (Pradhan et al., 2011, 2012). These datasets were proposed as the first comprehensively tagged and large-scale corpora for coreference resolution, to spur progress in state-of-theart techniques. According to Durrett and Klein (2013), this progress would contribute in the “uphill battle” of modelling not just syntax and discourse, but also semantic compatibility based on world knowledge and context. Despite improve"
P19-1386,D09-1101,0,0.0555566,"number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task. 1 Introduction Coreference resolution is one of the best known tasks in Natural Language Processing (NLP). Despite a large body of work in the area over the last few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, inclu"
P19-1386,D12-1071,0,0.46258,"Missing"
P19-1386,N18-2002,0,0.12251,"Missing"
P19-1386,N03-1033,0,0.027675,"d. (K = Alex) Tom arrives to where Vanessa was tied, but she has come free of her lead. K NOW R EF Example 1: Table 1: Examples of K NOW R EF instances. tains connectives.3 We use a regular expression to ensure that there is only one connective cluster (e.g. “, and though”), and that there are at least two non-stopwords before this connective and a pronoun after it. As a final check, we ensure that no pronoun occurs before the connective, which tends to remove sentences which are not self-contained. 3.1.3 Antecedent Filtering On the remaining set of sentences, we use Stanford’s Maxent tagger (Toutanova et al., 2003) to infer a flat part-of-speech (POS) labelling. Using the inferred POS tags, we ensure that there are exactly two noun phrases (NPs) before the connective that do not re-occur after it (a re-occurrence after the connective means that the pronoun likely refers to the non-repeated noun phrase). The mentioned checks resulted in roughly 100,000 sentences across all three corpora. At least some of these remaining sentences have similar properties to Winograd schema sentences; that is, the two noun phrases (NPs) and the pronoun share the same type. From here, we keep only sentences where the type i"
P19-1386,P00-1023,0,0.257917,"often fail to capture context, instead relying on the gender or number of candidate antecedents to make a decision. We then use problem-specific insights to propose a data-augmentation trick called antecedent switching to alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task. 1 Introduction Coreference resolution is one of the best known tasks in Natural Language Processing (NLP). Despite a large body of work in the area over the last few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee"
P19-1386,Q18-1042,0,0.267329,"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3952–3961 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics obvious syntactic/semantic cues are ineffective. Previous approaches to common sense reasoning, based on logical formalisms (Bailey et al., 2015) or deep neural models (Liu et al., 2016), have solved only restricted subsets of the WSC with high precision. These shortcomings can in part be attributed to the limited size of the corpus (273 instances), which is a side effect of its hand-crafted nature. Webster et al. (2018) recently presented a corpus called GAP that consists of about 4,000 unique binary coreference instances from English Wikipedia. This corpus is intended to address gender bias and the mentioned size limitations of the WSC. We believe that gender bias in coreference resolution is part and parcel of a more general problem: current models are unable to abstract away from the entities in the sentence to take advantage of the wider context to make a coreference decision. To tackle this issue, we present a coreference resolution corpus called K NOW R EF that specifically targets the ability of syste"
P19-1386,N15-1082,0,0.425054,"Missing"
P19-1386,W12-4501,0,0.27771,"Missing"
P19-1386,W11-1901,0,0.355084,"alleviate this tendency in models. Finally, we show that antecedent switching yields promising results on other tasks as well: we use it to achieve state-of-the-art results on the GAP coreference task. 1 Introduction Coreference resolution is one of the best known tasks in Natural Language Processing (NLP). Despite a large body of work in the area over the last few decades (Morton, 2000; Bean and Riloff, 2004; McCallum and Wellner, 2005; Rahman and Ng, 2009), the task remains challenging. Many resolution decisions require extensive world knowledge and understanding common points of reference (Pradhan et al., 2011). In the case of pronominal anaphora resolution, these forms of “common sense” become much more important when cues * equal contribution like gender and number do not by themselves indicate the correct resolution (Trichelair et al., 2018). To date, most existing methods for coreference resolution (Raghunathan et al., 2010; Lee et al., 2011; Durrett et al., 2013; Lee et al., 2017, 2018) have been evaluated on a few popular datasets, including the CoNLL 2011 and 2012 shared coreference resolution tasks (Pradhan et al., 2011, 2012). These datasets were proposed as the first comprehensively tagged"
P19-1386,N16-1114,0,0.056358,"valuation framework to facilitate performance comparisons. The quality of these tasks led to their widespread use and the emergence of many resolution systems, ranging from hand-engineered methods to deeplearning approaches. The multi-pass sieve system of Raghunathan et al. (2010) is fully deterministic and makes use of mention attributes like gender and number; it maintained the best results on the CoNLL 2011 task for a number of years (Lee et al., 2011). Later, lexical learning approaches emerged as the new state of the art (Durrett and Klein, 2013), followed more recently by neural models (Wiseman et al., 2016; Clark and Manning, 2016). The current state-of-the-art result on the CoNLL 2012 task is by an end-to-end neural model from Lee et al. (2018) that does not rely on a syntactic parser or a hand-engineered mention detector. 2.2 Gender bias in general coreference resolution Zhao et al. (2018) observed that state-of-the-art 3953 methods for coreference resolution become genderbiased, exploiting various stereotypes that leak from society into data. They devise a dataset of 3,160 manually written sentences called WinoBias that serves both as a gender-bias test for coreference resolution models and"
P19-1386,N18-2003,0,0.248215,"neural coreference resolution models. Lee et al. (2017) note that their model does “little in the uphill battle of making coreference decisions that require world knowledge,” and highlight a few examples in the CoNLL 2012 task that rely on more complex understanding or inference. Because these cases are infrequent in the data, systems can perform very well on the CoNLL tasks according to standard metrics by exploiting surface cues. Highperforming models have also been observed to rely on social stereotypes present in the data, which could unfairly impact their decisions for some demographics (Zhao et al., 2018). There is a recent trend, therefore, to develop more challenging and diverse coreference tasks. Perhaps the most popular of these is the Winograd Schema Challenge (WSC), which has emerged as an alternative to the Turing test (Levesque et al., 2011). The WSC task is carefully controlled such that heuristics involving syntactic salience, the number and gender of the antecedents, or other 3952 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3952–3961 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics obvious"
S18-2001,cybulska-vossen-2014-using,0,0.730338,"; Yang et al., 2015), as well as other probabilistic models (Lu and Ng, 2017). Some recent work uses intuitions combining representation learning with clustering, but does not augment the loss function for the purpose of building clusterable representations (Krause et al., 2016; Choubey and Huang, 2017). 2 We formulate the task of event coreference resolution as creating clusters of event mentions which refer to the same event. For the purposes of this work, we define an event mention to be a set of tokens that correspond to the action of some event. Consider the sentence below (borrowed from Cybulska and Vossen (2014)): 3 Related Work The recent work on event coreference can be categorized according to the assumed level of event representation. In the predicate-argument alignment paradigm (Roth and Frank, 2012; Wolfe et al., 2013), links are simply drawn between predicates in different documents. This work only considers cross-document event coreference (Wolfe et al., 2013, 2015), and no within-document coreference. At the other extreme, the ACE and ERE datasets annotate rich internal event structure, with specific taxonomies that describe the annotated events and their types (Linguistic Data Consortium, 2"
S18-2001,D17-1226,0,0.650355,"Missing"
S18-2001,P16-1061,0,0.0336491,"d like to compare with: the first contains all mentions in the same document as the current mention em, and the second contains all mentions in the data we are asked to cluster. For each of these sets, we compute: the average word overlap and average lemma overlap (measured by harmonic similarity) between em and each of the other mentions in the set. We thus add two feature vector entries for each of the sets: the average word overlap between em and the other mentions in the set, and the average lemma overlap between em and the other mentions in the set. Contextual Inspired by the approach of Clark and Manning (2016) in the entity coreference task, we extract, for the token sets below, (i) the token’s word2vec word embedding (Mikolov et al., 2013) (or average if there are multiple); and, (ii) the one-hot count vector of the token’s lemma5 (or sum if there are multiple), for each event mention, em: • the first token of em; • the last token of em; • all tokens in the em; • each of the two tokens preceding em; • each of the two tokens following em; • all of the five tokens preceding em; • all of the five tokens following em; • all of the tokens in em’s sentence. 4.2 Comparative Document It is necessary to in"
S18-2001,W15-0801,0,0.24688,"rence on Lexical and Computational Semantics (*SEM), pages 1–10 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics using an hourglass-shaped neural network. We propose a mechanism to modulate this training by introducing Clustering-Oriented Regularization (CORE) terms into the objective function of the learner; these terms impel the model to produce similar embeddings for coreferential event mentions, and dissimilar embeddings otherwise. Our model obtains strong results on withinand cross-document event coreference resolution, matching or outperforming the system of Cybulska and Vossen (2015) on the ECB+ corpus on all six evaluation measures. We achieve these gains despite the fact that our model requires significantly less pre-annotated or pre-detected information in terms of the internal event structure. Our model’s improvements upon the baselines show that our supervised representation learning framework creates new embeddings that capture the abstract distributional relations between samples and their clusters, suggesting that our framework can be generalized to other clustering tasks1 . Previous work on model design for event coreference has focused on clustering over a lingu"
S18-2001,W16-6112,0,0.0549872,"nt mentions into a space that is tuned specifically for clustering. The representation learner is trained to predict which event cluster the event mention belongs to, Introduction Event coreference resolution is the task of determining which event mentions expressed in language refer to the same real-world event instances. The ability to resolve event coreference has improved the quality of downstream tasks such as automatic text summarization (Vanderwende et al., 2004), questioning-answering (Berant et al., 2014), headline generation (Sun et al., 2015), and text-mining in the medical domain (Ferracane et al., 2016). Event mentions are comprised of an action component (or, head) and surrounding arguments. Consider the following passages, drawn from two different documents; the heads of the event mentions are in boldface and the subscripts indicate mention IDs: (1) The president’s speechm1 shockedm2 the audience. He announcedm3 several new controversial policies. 1 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 1–10 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics using an hourglass-shaped neural network. We propose a mechanism to"
S18-2001,W99-0201,0,0.0669189,"n terms of the internal event structure. Our model’s improvements upon the baselines show that our supervised representation learning framework creates new embeddings that capture the abstract distributional relations between samples and their clusters, suggesting that our framework can be generalized to other clustering tasks1 . Previous work on model design for event coreference has focused on clustering over a linguistically rich set of features. Most models require a pairwise-prediction based supervised learning step which predicts whether or not a pair of event mentions is coreferential (Bagga and Baldwin, 1999; Chen et al., 2009; Cybulska and Vossen, 2015). Other work focuses on the clustering step itself, aggregating local pairwise decisions into clusters, for example by graph partitioning (Chen and Ji, 2009). There has also been work using nonparametric Bayesian clustering techniques (Bejan and Harabagiu, 2014; Yang et al., 2015), as well as other probabilistic models (Lu and Ng, 2017). Some recent work uses intuitions combining representation learning with clustering, but does not augment the loss function for the purpose of building clusterable representations (Krause et al., 2016; Choubey and"
S18-2001,J14-2004,0,0.336387,"other clustering tasks1 . Previous work on model design for event coreference has focused on clustering over a linguistically rich set of features. Most models require a pairwise-prediction based supervised learning step which predicts whether or not a pair of event mentions is coreferential (Bagga and Baldwin, 1999; Chen et al., 2009; Cybulska and Vossen, 2015). Other work focuses on the clustering step itself, aggregating local pairwise decisions into clusters, for example by graph partitioning (Chen and Ji, 2009). There has also been work using nonparametric Bayesian clustering techniques (Bejan and Harabagiu, 2014; Yang et al., 2015), as well as other probabilistic models (Lu and Ng, 2017). Some recent work uses intuitions combining representation learning with clustering, but does not augment the loss function for the purpose of building clusterable representations (Krause et al., 2016; Choubey and Huang, 2017). 2 We formulate the task of event coreference resolution as creating clusters of event mentions which refer to the same event. For the purposes of this work, we define an event mention to be a set of tokens that correspond to the action of some event. Consider the sentence below (borrowed from"
S18-2001,bejan-harabagiu-2008-linguistic,0,0.0165897,"el of event representation. In the predicate-argument alignment paradigm (Roth and Frank, 2012; Wolfe et al., 2013), links are simply drawn between predicates in different documents. This work only considers cross-document event coreference (Wolfe et al., 2013, 2015), and no within-document coreference. At the other extreme, the ACE and ERE datasets annotate rich internal event structure, with specific taxonomies that describe the annotated events and their types (Linguistic Data Consortium, 2005, 2016). In these datasets, only withindocument coreference is annotated. The creators of the ECB (Bejan and Harabagiu, 2008) and ECB+ (Cybulska and Vossen, 2014), annotate events according to a level of abstraction between that of the predicate-argument approach and the ACE approach, being most similar to the TimeML paradigm (Pustejovsky et al., 2003). In these datasets, both within-document and crossdocument coreference relations are annotated. We use the ECB+ corpus in our experiments because it solves the lack of lexical diversity found within the ECB by adding 502 new annotated documents, providing a total of 982 documents. Event Coreference Resolution Model (3) On Monday Lindsay Lohan checked into rehab in Mal"
S18-2001,K16-1024,0,0.0165347,"erential (Bagga and Baldwin, 1999; Chen et al., 2009; Cybulska and Vossen, 2015). Other work focuses on the clustering step itself, aggregating local pairwise decisions into clusters, for example by graph partitioning (Chen and Ji, 2009). There has also been work using nonparametric Bayesian clustering techniques (Bejan and Harabagiu, 2014; Yang et al., 2015), as well as other probabilistic models (Lu and Ng, 2017). Some recent work uses intuitions combining representation learning with clustering, but does not augment the loss function for the purpose of building clusterable representations (Krause et al., 2016; Choubey and Huang, 2017). 2 We formulate the task of event coreference resolution as creating clusters of event mentions which refer to the same event. For the purposes of this work, we define an event mention to be a set of tokens that correspond to the action of some event. Consider the sentence below (borrowed from Cybulska and Vossen (2014)): 3 Related Work The recent work on event coreference can be categorized according to the assumed level of event representation. In the predicate-argument alignment paradigm (Roth and Frank, 2012; Wolfe et al., 2013), links are simply drawn between pr"
S18-2001,D14-1159,0,0.0309912,"into clusters (Section 2). We demonstrate these points by proposing a method that learns to embed event mentions into a space that is tuned specifically for clustering. The representation learner is trained to predict which event cluster the event mention belongs to, Introduction Event coreference resolution is the task of determining which event mentions expressed in language refer to the same real-world event instances. The ability to resolve event coreference has improved the quality of downstream tasks such as automatic text summarization (Vanderwende et al., 2004), questioning-answering (Berant et al., 2014), headline generation (Sun et al., 2015), and text-mining in the medical domain (Ferracane et al., 2016). Event mentions are comprised of an action component (or, head) and surrounding arguments. Consider the following passages, drawn from two different documents; the heads of the event mentions are in boldface and the subscripts indicate mention IDs: (1) The president’s speechm1 shockedm2 the audience. He announcedm3 several new controversial policies. 1 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 1–10 c New Orleans, June 5-6, 2018. 2018 Associ"
S18-2001,H05-1004,0,0.455956,"ained by agglomerative clustering over the original unweighted features. Again, we optimize the τ similarity threshold over the validation set. MUC (Vilain et al., 1995). Link-level measure which counts the minimum number of link changes required to obtain the correct clustering from the predictions; it does not account for correctly predicted singletons. 5.2.2 (Bagga and Baldwin, 1998). Mention-level measure which computes precision and recall for each individual mention, overcoming the singleton problem of MUC, but can problematically count the same coreference chain multiple times. CEAF-M (Luo, 2005). Mention-level measure which reflects the percentage of mentions that are in the correct coreference chains. Note that precision and recall are the same in this measure since we use pre-annotated mentions. 5.2.3 Representation Learning. We test four different model variants: • CCE: uses only categorical-cross-entropy in the loss function (Equation 1); CEAF-E (Luo, 2005). Entity-level measure computed by aligning predicted with the gold chains, not allowing one chain to have more than one alignment, overcoming the problem of B3 . • CORE: uses only clustering-oriented regularization; i.e., the"
S18-2001,P13-2012,0,0.0473812,"Missing"
S18-2001,P14-2005,0,0.014734,"of mentions that are in the correct coreference chains. Note that precision and recall are the same in this measure since we use pre-annotated mentions. 5.2.3 Representation Learning. We test four different model variants: • CCE: uses only categorical-cross-entropy in the loss function (Equation 1); CEAF-E (Luo, 2005). Entity-level measure computed by aligning predicted with the gold chains, not allowing one chain to have more than one alignment, overcoming the problem of B3 . • CORE: uses only clustering-oriented regularization; i.e., the attract and repulse terms (Equations 3 and 4); BLANC (Luo et al., 2014). Computes two Fscores in terms of the pairwise quality of coreference decisions and non-coreference decisions, and averages these scores together for the final results. • CORE+CCE: includes categorical-crossentropy and the attract and repulse terms (Equation 5); CoNLL. The mean of MUC, B3 , and CEAF-E. • CORE+CCE+L EMMA: initializes the agglomerative clustering with clusters computed by lemma-δ (with a differently tuned value of δ than the baseline) and continues the clustering process using the similarities between the embeddings created by CORE+CCE. Models We compare our representation-lear"
S18-2001,Q15-1037,0,0.447171,"revious work on model design for event coreference has focused on clustering over a linguistically rich set of features. Most models require a pairwise-prediction based supervised learning step which predicts whether or not a pair of event mentions is coreferential (Bagga and Baldwin, 1999; Chen et al., 2009; Cybulska and Vossen, 2015). Other work focuses on the clustering step itself, aggregating local pairwise decisions into clusters, for example by graph partitioning (Chen and Ji, 2009). There has also been work using nonparametric Bayesian clustering techniques (Bejan and Harabagiu, 2014; Yang et al., 2015), as well as other probabilistic models (Lu and Ng, 2017). Some recent work uses intuitions combining representation learning with clustering, but does not augment the loss function for the purpose of building clusterable representations (Krause et al., 2016; Choubey and Huang, 2017). 2 We formulate the task of event coreference resolution as creating clusters of event mentions which refer to the same event. For the purposes of this work, we define an event mention to be a set of tokens that correspond to the action of some event. Consider the sentence below (borrowed from Cybulska and Vossen"
S18-2001,P14-2006,0,0.0926642,"es a difficult baseline to beat. A δ-similarity threshold is introduced, and we merge two mentions with the same head-lemma if and only if the cosine-similarity between the TFIDF vectors of their corresponding documents is greater than δ. This δ parameter is tuned to maximize B3 performance on the validation set, which we found occurs when δ = 0.67. Evaluation Measures Since there is no consensus in the coreference resolution literature on the best evaluation measure, we present results obtained according to six different measures, as is common in previous work. We use the scorer presented by Pradhan et al. (2014). In this task, the term “coreference chain” is synonymous with “cluster”. U NSUPERVISED. This is the result obtained by agglomerative clustering over the original unweighted features. Again, we optimize the τ similarity threshold over the validation set. MUC (Vilain et al., 1995). Link-level measure which counts the minimum number of link changes required to obtain the correct clustering from the predictions; it does not account for correctly predicted singletons. 5.2.2 (Bagga and Baldwin, 1998). Mention-level measure which computes precision and recall for each individual mention, overcoming"
S18-2001,S12-1030,0,0.0126003,"for the purpose of building clusterable representations (Krause et al., 2016; Choubey and Huang, 2017). 2 We formulate the task of event coreference resolution as creating clusters of event mentions which refer to the same event. For the purposes of this work, we define an event mention to be a set of tokens that correspond to the action of some event. Consider the sentence below (borrowed from Cybulska and Vossen (2014)): 3 Related Work The recent work on event coreference can be categorized according to the assumed level of event representation. In the predicate-argument alignment paradigm (Roth and Frank, 2012; Wolfe et al., 2013), links are simply drawn between predicates in different documents. This work only considers cross-document event coreference (Wolfe et al., 2013, 2015), and no within-document coreference. At the other extreme, the ACE and ERE datasets annotate rich internal event structure, with specific taxonomies that describe the annotated events and their types (Linguistic Data Consortium, 2005, 2016). In these datasets, only withindocument coreference is annotated. The creators of the ECB (Bejan and Harabagiu, 2008) and ECB+ (Cybulska and Vossen, 2014), annotate events according to"
S18-2001,P15-1045,0,0.0311205,"hese points by proposing a method that learns to embed event mentions into a space that is tuned specifically for clustering. The representation learner is trained to predict which event cluster the event mention belongs to, Introduction Event coreference resolution is the task of determining which event mentions expressed in language refer to the same real-world event instances. The ability to resolve event coreference has improved the quality of downstream tasks such as automatic text summarization (Vanderwende et al., 2004), questioning-answering (Berant et al., 2014), headline generation (Sun et al., 2015), and text-mining in the medical domain (Ferracane et al., 2016). Event mentions are comprised of an action component (or, head) and surrounding arguments. Consider the following passages, drawn from two different documents; the heads of the event mentions are in boldface and the subscripts indicate mention IDs: (1) The president’s speechm1 shockedm2 the audience. He announcedm3 several new controversial policies. 1 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 1–10 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics usin"
S18-2001,C16-1183,0,0.810091,"models, clustering proceeds until a pre-determined similarity threshold, τ , is reached. We tuned τ on the validation set, doing grid search for τ ∈ [0, 1] to maximize B3 accuracy4 . Preliminary experimentation led us to use cosine-similarity (see cosine distance in Equation 2) to measure vector similarity, and single-linkage for clustering decisions. We experimented with two initialization schemes for agglomerative clustering. In the first scheme, each event mention is initialized as its own cluster, as is standard. In the second, we initialized clusters using the lemma-δ baseline defined by Upadhyay et al. (2016). This baseline merges all event mentions with the same head lemma that are in documents with document-level similarity that is higher than a threshold δ. Upadhyay et al. showed that it is a strong indicator of event coreference, so we experimented with initializing our clustering algorithm in this way. We call this model variant CORE+CCE+L EMMA, and describe the parameter tuning procedures in more detail in Section 5. 3.4.1 Attractive Regularization The first desirable property for the embeddings is that mentions that belong to the same cluster should have low cosine distance between each oth"
S18-2001,M95-1005,0,0.391721,"rformance on the validation set, which we found occurs when δ = 0.67. Evaluation Measures Since there is no consensus in the coreference resolution literature on the best evaluation measure, we present results obtained according to six different measures, as is common in previous work. We use the scorer presented by Pradhan et al. (2014). In this task, the term “coreference chain” is synonymous with “cluster”. U NSUPERVISED. This is the result obtained by agglomerative clustering over the original unweighted features. Again, we optimize the τ similarity threshold over the validation set. MUC (Vilain et al., 1995). Link-level measure which counts the minimum number of link changes required to obtain the correct clustering from the predictions; it does not account for correctly predicted singletons. 5.2.2 (Bagga and Baldwin, 1998). Mention-level measure which computes precision and recall for each individual mention, overcoming the singleton problem of MUC, but can problematically count the same coreference chain multiple times. CEAF-M (Luo, 2005). Mention-level measure which reflects the percentage of mentions that are in the correct coreference chains. Note that precision and recall are the same in th"
S18-2001,N15-1002,0,0.0435293,"Missing"
S18-2001,W09-3208,0,\N,Missing
S18-2001,W09-4303,0,\N,Missing
S18-2016,P98-1012,0,0.0186712,"iments and Results Set all dev FT 27 15 lemma while in the remaining instances the frame is evoked by different lemmas, and d) last but not least, the frame types themselves have long-tailed distribution. Table 3 shows examples of frames and verb lemmas that lexicalize them; in the table, the most frequent lemma for each frame type is italicized. 4.2 Evaluation Measures We evaluate our method’s performance on a) clustering input strings to frame types, and b) clustering syntactic arguments to semantic role types. To this end, we report the harmonic mean of BCubed precision and recall (B C F) (Bagga and Baldwin, 1998), and purity (P U), inverse purity (I P U) and their harmonic mean (F P U) (Steinbach et al., 2000) as figures of merit. These measures reflect a notion of similarity between the distribution of instances in the obtained clusters and the gold/evaluation data based on certain criteria and alone may lack sufficient information for a fair understanding of the system’s performance. While P U and I P U are easy to interpret (by establishing an analogy between them and precision and recall in classification tasks), they may be deceiving under certain conditions (as explained by Amig´o et al., 2009,"
S18-2016,P98-1013,0,0.806852,"requency of the usages of lexical items in our experiments in the corpora used to train embeddings (i.e., an English web corpus (Sch¨afer, 2015) and PTB’s WSJ). We derive our data for evaluation from the PTB’s WSJ sections parsed (using Schuster and Manning, 2016) to the enhanced UD format. We augment these sentences with semantic role annotations obtained from Prague Semantic Dependencies (PSD) (Cinkova et al., 2012) from the SDP resource (Oepen et al., 2016). Using EngVallex (Cinkov´a et al., 2014) and SemLink (Bonial et al., 2013), we semi-automatically annotate verbs with FrameNet frames (Baker et al., 1998). We choose 1k random sentences and manually verify the semi-automatic mappings to eventually build our evaluation dataset of approximately 5k instances (all). From this data, we use a random subset of 200 instances (dev) during the development and for parameter tuning (see Table 1 for detailed statistics). FI 5,324 200 V 169 35 AT 13 7 RF 0.94 0.95 V 167 34 GR 56 24 AI 10,893 450 AIG 7,305 277 PA 0.67 0.62 RA 0.76 0.76 Input strings extracted from the UD parses: GR, AIG, RF , RA , and PA denote, respectively, the number of distinct grammatical relations, syntactic arguments that are a semanti"
S18-2016,D13-1178,0,0.0233078,"tic role induction (Carreras and Marquez, 2005; Lang and Lapata, 2010, 2011; Titov and Klementiev, 2012; Swier and Stevenson, 2004), our method differs from them in that we attempt to include frame head grouping information for inducing roles associated to them. In other words, these methods leave out the problem of sense/frame grouping in their models. Our work differs in objective from methods for unsupervised template induction in information extraction (IE) (e.g., MUC-style frames in Chambers and Jurafsky (2009, 2011) and its later refinements such as (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013), and in a broader sense attempts towards ontology learning and population from text (Cimiano et al., 2005)). Our focus is on lexicalized elementary syntactic structures, identifying lexical semantic relationships, and thereby finding salient patterns in syntax–semantic interface. However, in IE BCF 55.43 0.36 76.71 7.79 11.71 74.65 ±1.38 Table 5: Results on clustering of syntactic arguments to semantic roles. many incomplete yet homogeneous clusters (as we discuss below). With respect to roles, however, the method’s performance and its output remains very similar to the syntactic baseline (B"
S18-2016,P09-1068,0,0.0457929,"input). Despite similarities between our method and those proposed previously to address unsupervised semantic role induction (Carreras and Marquez, 2005; Lang and Lapata, 2010, 2011; Titov and Klementiev, 2012; Swier and Stevenson, 2004), our method differs from them in that we attempt to include frame head grouping information for inducing roles associated to them. In other words, these methods leave out the problem of sense/frame grouping in their models. Our work differs in objective from methods for unsupervised template induction in information extraction (IE) (e.g., MUC-style frames in Chambers and Jurafsky (2009, 2011) and its later refinements such as (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013), and in a broader sense attempts towards ontology learning and population from text (Cimiano et al., 2005)). Our focus is on lexicalized elementary syntactic structures, identifying lexical semantic relationships, and thereby finding salient patterns in syntax–semantic interface. However, in IE BCF 55.43 0.36 76.71 7.79 11.71 74.65 ±1.38 Table 5: Results on clustering of syntactic arguments to semantic roles. many incomplete yet homogeneous clusters (as we discuss below). With respect"
S18-2016,P11-1098,0,0.0551436,"Missing"
S18-2016,W16-5901,0,0.0258635,"ally, given finite sets of frames F and of semantic roles R, our underlying CFG G = hN, T, P, Si is as follows: • T = Tv ∪Tn ∪D∪{root, EOS}, where Tv is the set of possible verbal heads, Tn is the set of possible lexicalizations (fillers) for arguments, and D is a finite set of dependency relations; root and EOS are special symbols. f • N = {S} ∪ {Fhf |f ∈ F} ∪ {Frem |f ∈ F} ∪ f r {Fg |f ∈ F, g ∈ D}∪{R |r ∈ R}∪{V f |f ∈ F} ∪ {Dg |g ∈ D}. • P contains the following rules: 3.1.1 As a solution to sparsity of observations, we modify the IO algorithm slightly. We adapt the procedures described in (Eisner, 2016) with the exception that for computing inside and outside probabilities, instead of mapping terminals to nonterminals using an exact matching of the right-handsides of the rules (and respectively their assigned parameters), we use embedding-based similarities. I.e., for computing inside probabilities, given symbol a as input, instead of considering A →θ as rewrite rules and updating the parse chart only by asserting θ in it, we also consider B →θ bs in which instead of θ we assert α × θs in the IO table, where α is the r2 coefficient correlations of embeddings for a and bs. During the outside"
S18-2016,P13-1121,1,0.796956,"he verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that evoke the same frame (e.g., to group instances of pack that evoke the frame P LACING with instances of verbs load, pile, place, and so on when used to evoke the same P LACING frame). The motivation for this work is twofold. On the one hand, the frame induction techniques we propose can be useful in the context of applications such as text summarization (Cheung and Penn, 2013), question answering (Frank et al., 2007; Shen and Lapata, 2007), and so on, for languages where we lack a frame-annotated resource for supervised frame induction, or to expand the coverage of already existing resources. On the other hand, we are interested in theoretical linguistic insights into frame structure. In this sense, our We present a method for unsupervised lexical frame acquisition at the syntax–semantics interface. Given a set of input strings derived from dependency parses, our method generates a set of clusters that resemble lexical frame structures. Our work is motivated not on"
S18-2016,C14-1123,0,0.0202861,"le 4: Results for head groupings: # C denotes the number of induced clusters by each method/baseline; the last two rows reports the average and the standard deviation for the obtained results using the L-PCFG model.The remaining abbreviations are introduced in § 4.2 and 4.3. Method A LL I N 1 1CP ER I 1CP ER G R R24 TK -URL L-PCFG (Avg.) L-PCFG (Std. Dev.) #C 1 7257 32 24 333 24 ±5.29 PU 47.73 100 92.89 47.73 85.7 90.36 ±0.33 IPU 100 0.18 79.83 5.36 15.01 79.25 ±1.31 FPU 64.62 0.36 85.86 9.64 25.54 84.44 ±0.61 5 Related Work Our work differs from most work on word sense induction (WSI), e.g. (Goyal and Hovy, 2014; Lau et al., 2012; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011), in that not only do we discern different senses of a lexical item but also we group the induced senses into more general meaning categories (i.e., FrameNet’s grouping). Hence, our model must be able to capture lexical relationships other than polysemy, e.g., synonymy, antonymy (opposite verbs), troponymy, etc.. However, our method can be adapted to WSI, too. Firstly, we can assume that word senses are ‘incompatible’ and thus they necessarily evoke different frames; subsequently, the induced frame clusters can be se"
S18-2016,N13-1104,1,0.953969,"dobj and iobj and they precede prepositional and complement dependents (i.e., nmod:* and *comp).1 Consider (1) as an example; the corresponding string is the yield of the tree in Fig. 1. 2 From a Latent Model to L-PCFG (1) We assume that frames and semantic roles are the latent variables of a probabilistic model. Given the probability mass function pmf(F 1 , . . . , F n , R1 . . . Rk , D1 , . . . , Dm ; C, θ) as our model, we denote latent frames F i , 1 ≤ i ≤ n, and roles Ri , 1 ≤ i ≤ k for observations that are annotated syntactically using Di , 1 ≤ i ≤ m in the input corpus C. Inspired by Cheung et al. (2013), we approximate the probability of a specific frame f with head v, semantic roles r1 . . . rk filled by words w1 . . . wk and corresponding syntactic dependencies d1 . . . dk (under Johnsubj offerroot flowersdobj to Marynmod:to Given the fixed structure of input strings, we design a CFG that rewrites them to our expected hierarchical frame structure consisting of elements F , R, D while capturing the conditional probabilities from Eq. (1). The tree assigning a frame F of type x with semantic roles of type a, b, c to (1) is for 1 Phrasal arguments are reduced to their syntactic head given by t"
S18-2016,P04-1048,0,0.0791473,"¨at D¨usseldorf McGill University kallmeyer@hhu.de zadeh@phil.hhu.de jcheung@cs.mcgill.ca Abstract types in FrameNet. This allows us to generalize across frames concerning semantic roles. As part of this, we study possible ways to automatically generate more abstract lexical-semantic representations from lexicalized dependency structures. In our task, grouping verb tokens into frames requires not only distinguishing between different senses of verbs, but also identifying a range of lexical relationships (e.g., synonymy, opposite verbs, troponymy, etc.) among them. Hence (as Modi et al., 2012; Green et al., 2004), our problem definition differs from most work on unsupervised fine-grained frame induction using verb sense disambiguation (e.g., Kawahara et al., 2014; Peng et al., 2017). Similarly, forming role clusters yields generalization from several alternate linkings between semantic roles and their syntactic realization. Given, for instance, an occurrence of the verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that"
S18-2016,J98-4004,0,0.114592,"een verb types and frame types. But similar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is"
S18-2016,E14-1007,0,0.042034,"Missing"
S18-2016,N03-1016,0,0.066084,"ice of the frame f and the semantic roles r of the k fillers (i.e., x, a, b, and c in Fig. 1). The probabilities of the rules correspond to the conditional probabilities in Eq. (1). The probabilf ity of S → Fhf Frem gives p(F = f ), the probf ability of V → v gives p(V = v|F = f ), and so on. During the subsequent inside-outside (IO) split-and-merge training procedure, the inventory of frames and roles and the probabilities corresponding to our rules are estimated so that the overall likelihood of observations is maximized. 3 The IO Algorithm 3.1.2 Split We alter the splitting procedure from (Klein and Manning, 2003a; Petrov et al., 2006) for our application. In (Klein and Manning, 2003b; Petrov et al., 2006), during split, a non-terminal symbol (which represents a random variable in the underlying probabilistic model) is split and its related production rules are duplicated independently of its parent, or sibling nodes. We can apply such a context-independent split only to the Rr nonf s must split dependently w.r.t. terminals but the F... their sibling nodes that define the frame structure. Therefore, to split frame x to two frames y and z, x , Fx → we replace the entire set {S → Fhx Frem h x x x , V x"
S18-2016,W17-3405,0,0.0171902,"aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is that we can adapt and reuse statistical inference techniqu"
S18-2016,P03-1054,0,0.0238273,"ice of the frame f and the semantic roles r of the k fillers (i.e., x, a, b, and c in Fig. 1). The probabilities of the rules correspond to the conditional probabilities in Eq. (1). The probabilf ity of S → Fhf Frem gives p(F = f ), the probf ability of V → v gives p(V = v|F = f ), and so on. During the subsequent inside-outside (IO) split-and-merge training procedure, the inventory of frames and roles and the probabilities corresponding to our rules are estimated so that the overall likelihood of observations is maximized. 3 The IO Algorithm 3.1.2 Split We alter the splitting procedure from (Klein and Manning, 2003a; Petrov et al., 2006) for our application. In (Klein and Manning, 2003b; Petrov et al., 2006), during split, a non-terminal symbol (which represents a random variable in the underlying probabilistic model) is split and its related production rules are duplicated independently of its parent, or sibling nodes. We can apply such a context-independent split only to the Rr nonf s must split dependently w.r.t. terminals but the F... their sibling nodes that define the frame structure. Therefore, to split frame x to two frames y and z, x , Fx → we replace the entire set {S → Fhx Frem h x x x , V x"
S18-2016,N10-1137,0,0.152583,"te verbs), troponymy, etc.. However, our method can be adapted to WSI, too. Firstly, we can assume that word senses are ‘incompatible’ and thus they necessarily evoke different frames; subsequently, the induced frame clusters can be seen directly as clusters of word senses. Otherwise, the proposed method can be adapted for WSI by altering its initialization, e.g., by building one-model-at-a-time for each word form (i.e., simply altering the input). Despite similarities between our method and those proposed previously to address unsupervised semantic role induction (Carreras and Marquez, 2005; Lang and Lapata, 2010, 2011; Titov and Klementiev, 2012; Swier and Stevenson, 2004), our method differs from them in that we attempt to include frame head grouping information for inducing roles associated to them. In other words, these methods leave out the problem of sense/frame grouping in their models. Our work differs in objective from methods for unsupervised template induction in information extraction (IE) (e.g., MUC-style frames in Chambers and Jurafsky (2009, 2011) and its later refinements such as (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013), and in a broader sense attempts toward"
S18-2016,P11-1112,0,0.0620286,"Missing"
S18-2016,J05-1004,0,0.243132,"rforms several baselines on a portion of the Wall Street Journal sentences that we have newly annotated for evaluation purposes. 1 Introduction We propose a method for building coarse lexical frames automatically from dependency parsed sentences; i.e., without using any explicit semantic information as training data. The task involves grouping verbs that evoke the same frame (i.e., are considered to be the head of this frame) and further clustering their syntactic arguments into latent semantic roles. Hence, our target structures stand between FrameNet (Ruppenhofer et al., 2016) and PropBank (Palmer et al., 2005) frames. Similar to FrameNet and in contrast to PropBank, we assume a many-to-many relationship between verb types and frame types. But similar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations be"
S18-2016,E12-1060,0,0.0314706,"Missing"
S18-2016,K17-1019,0,0.0196706,"mantic roles. As part of this, we study possible ways to automatically generate more abstract lexical-semantic representations from lexicalized dependency structures. In our task, grouping verb tokens into frames requires not only distinguishing between different senses of verbs, but also identifying a range of lexical relationships (e.g., synonymy, opposite verbs, troponymy, etc.) among them. Hence (as Modi et al., 2012; Green et al., 2004), our problem definition differs from most work on unsupervised fine-grained frame induction using verb sense disambiguation (e.g., Kawahara et al., 2014; Peng et al., 2017). Similarly, forming role clusters yields generalization from several alternate linkings between semantic roles and their syntactic realization. Given, for instance, an occurrence of the verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that evoke the same frame (e.g., to group instances of pack that evoke the frame P LACING with instances of verbs load, pile, place, and so on when used to evoke the same P LACI"
S18-2016,N09-1069,0,0.0239812,"large web corpora. Each frame instance is then represented using a (m + 1, n)-tensor, in which m is the total number of argument types/clusters given by our model at its current stage and n is the dimensionality of the embeddings that represent words that fill these arguments. To this, we add the embedding for the verb that lexicalizes the head of the frame, which gives us the final (m + 1, n)-tensor. For two frame-instances represented by tensors a and b, the similarity for their arguments is Parameters for this new merged G are updated through a few IO iterations (in an incremental fashion (Liang and Klein, 2009)), and finally G is used to obtain a new clustering. The process is repeated for this newly obtained clustering until all the resulting cluster-wise sc similarities are less than a threshold β. sim-arg(a, b) = m 1 X 2 a ~i b ~i r ( v , v ), k i=1 in which v~i s are embeddings for the ith argument P filler ( nj=1 v~i j 6= 0), r2 is the coefficient of deP 2 a ~i b ~i termination, and k = [r ( v , v ) 6= 0]. If an i Computing all derivations for each input string is time consuming and makes the merge process computationally expensive, particularly in the first few iterations. We resolve this issu"
S18-2016,D08-1048,0,0.0776969,"Missing"
S18-2016,P06-1055,0,0.330051,"ilar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is that we can adapt and reuse statistical infe"
S18-2016,P05-1010,0,0.0857042,"and frame types. But similar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is that we can adapt and r"
S18-2016,S17-2039,1,0.882872,"Missing"
S18-2016,P08-1028,0,0.070302,"and arg mini,j sim(ai , bj ) (sim is given by Eq. 2 below) and calculate their harmonic mean as the similarity sc between cx and cy . Cluster pairs are sorted in a descending order by sc . Given a threshold δ, for all sc (cx , cy ) &gt; δ, their corresponding production rules (i.e., the similar set of rules mentioned in the split procedure) are merged and their parameters are updated to the arithmetic mean of their origin rules. Similarity Between Frame Instances When necessary (such as during merge), we compute embedding-based similarities between frame instances similar to methods proposed in (Mitchell and Lapata, 2008; Clark, 2013). We build a ndimensional embedding for each word appearing in our input strings from large web corpora. Each frame instance is then represented using a (m + 1, n)-tensor, in which m is the total number of argument types/clusters given by our model at its current stage and n is the dimensionality of the embeddings that represent words that fill these arguments. To this, we add the embedding for the verb that lexicalizes the head of the frame, which gives us the final (m + 1, n)-tensor. For two frame-instances represented by tensors a and b, the similarity for their arguments is P"
S18-2016,W12-1901,0,0.706951,"ich-Heine-Universit¨at D¨usseldorf McGill University kallmeyer@hhu.de zadeh@phil.hhu.de jcheung@cs.mcgill.ca Abstract types in FrameNet. This allows us to generalize across frames concerning semantic roles. As part of this, we study possible ways to automatically generate more abstract lexical-semantic representations from lexicalized dependency structures. In our task, grouping verb tokens into frames requires not only distinguishing between different senses of verbs, but also identifying a range of lexical relationships (e.g., synonymy, opposite verbs, troponymy, etc.) among them. Hence (as Modi et al., 2012; Green et al., 2004), our problem definition differs from most work on unsupervised fine-grained frame induction using verb sense disambiguation (e.g., Kawahara et al., 2014; Peng et al., 2017). Similarly, forming role clusters yields generalization from several alternate linkings between semantic roles and their syntactic realization. Given, for instance, an occurrence of the verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’"
S18-2016,L16-1376,0,0.0432275,"Missing"
S18-2016,E12-1003,0,0.563617,"Missing"
S18-2016,P11-1148,0,0.0462252,"Missing"
S18-2016,D07-1002,0,0.353749,"distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that evoke the same frame (e.g., to group instances of pack that evoke the frame P LACING with instances of verbs load, pile, place, and so on when used to evoke the same P LACING frame). The motivation for this work is twofold. On the one hand, the frame induction techniques we propose can be useful in the context of applications such as text summarization (Cheung and Penn, 2013), question answering (Frank et al., 2007; Shen and Lapata, 2007), and so on, for languages where we lack a frame-annotated resource for supervised frame induction, or to expand the coverage of already existing resources. On the other hand, we are interested in theoretical linguistic insights into frame structure. In this sense, our We present a method for unsupervised lexical frame acquisition at the syntax–semantics interface. Given a set of input strings derived from dependency parses, our method generates a set of clusters that resemble lexical frame structures. Our work is motivated not only by its practical applications (e.g., to build, or expand the"
W08-1106,P99-1071,0,0.179453,"iding a context in which the need for generationbased methods is especially great. 1 Introduction There are two main approaches to the task of summarization—extraction and abstraction (Hahn and Mani, 2000). Extraction involves concatenating extracts taken from the corpus into a summary, whereas abstraction involves generating novel sentences from information extracted from the corpus. It has been observed that in the context of multidocument summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al., 1999). However, there has been little work identifying specific factors which might affect the performance of each strategy in summarizing evaluative documents 2 Related Work There has been little work comparing extractive and abstractive multi-document summarization. A previous study on summarizing evaluative text (Carenini et. al, 2006) showed that extraction and abstraction performed about equally well, though for different reasons. The study, however, did not 1 Authors are listed in alphabetical order. 33 look at the effect of the controversiality of the corpus on the relative performance of th"
W08-1106,E06-1039,1,0.901475,"(Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the stren"
W08-1106,W00-1407,1,0.765382,"e user. This mapping provides a better conceptual organization of the CFs 34 by grouping together semantically similar CFs, such as jpeg picture and jpeg slide show under the UDF JPEG. For the purposes of our study, feature extraction, polarity/strength identification and the mapping from CFs to UDFs are not done automatically as in (Hu and Liu, 2004) and (Carenini et al, 2005). Instead, “gold standard” annotations by humans are used in order to focus on the effect of the summarization strategy. 3.1 SEA summaries, which is adapted from GEA and is based on guidelines from argumentation theory (Carenini and Moore, 2000), sometimes sounded unnatural. We found that controversially rated UDF features (roughly balanced positive and negative evaluations) were treated as contrasts to those which were uncontroversially rated (either mostly positive, or mostly negative evaluations). In SEA, contrast relations between features are realized by cue phrases signalling contrast such as “however” and “although”. These cue phrases appear to signal a contrast that is too strong for the relation between controversial and uncontroversial features. An example of a SEA summary suffering from this problem can be found in Figure"
W08-1106,W02-2116,0,0.0558891,"Missing"
W08-1106,W04-1013,0,0.0527875,"For example, summaries which misrepresented the polarity of the evaluations for a certain feature were not penalized, and human summaries sometimes produced contradictory statements about the distribution of the opinions. In one case, one model summary claimed that a feature is positively rated, while another claimed the opposite, whereas the machine summary indicated that this feature drew mixed reviews. Clearly, only one of these positions should be regarded as correct. Further work is needed to resolve these problems. There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et"
W08-1106,N04-1019,0,0.0120131,"a summarizer for its intended purpose. (e.g. (McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in evaluating specific properties of the summary such as the grammaticality and the content, which may be difficult to evaluate with an overall task-based approach. Furthermore, the design of the task may intrinsically favour abstractive or extractive summarization. As an extreme example, asking for a list of specific comments from users would clearly favour extractive summarization. Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative. Multiple human summaries are taken to be models, and chunks of meaning known as Summary Content Units (SCU) are manually identified. Peer summaries are evaluated based on how many SCUs they share with the model summaries, and the number of model summaries in which these SCUs are found. Although this method has been tested in DUC 2006 and DUC 2005 (Passonneau et al., 2006), (Passonneau et al., 2005) in the domain of news articles, it has not been tested for evaluative text. A pilot study"
W08-1106,W00-0403,0,0.0706408,"rs to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the strength, and polarity (positive or negative) of the evaluation. For instance, in a corpus of customer reviews, the sentence “Excellent picture quality - on par with my Pioneer, Panasonic, and JVC players.” contains an opinion on the feature picture quality of a DVD player, and is a very positive evaluation (+3 on a scale from -3 to +3). We rely on methods from previous work for these tasks (Hu and Liu, 2004). Onc"
W08-1106,N03-1020,0,\N,Missing
W09-2804,W04-1013,0,0.00250549,"s in a UDF hierarchy, and sampled from these distributions to generate new UDF hierarchies and evaluations. In total, we generated 36 sets of data, which covered a realistic set of possible scenarios in term of feature hierarchy structures as well as in term of distribution of evaluations for each feature. Table 2 presents some statistics on the generated data sets. 4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans, which is the view taken by existing summarization evaluation schemes such as ROUGE (Lin, 2004) and the Pyramid method (Nenkova et al., 2007). For evaluating our content selection strategy, we conducted a user study asking human participants to perform a selection task to create “gold standard” selections. Participants viewed and selected UDF features using a Treemap information visualization. See Figure 2 for an example. We recruited 25 university students or graduates, who were each presented with 19 to 20 of the cases we generated as described above. Each case represented a different hypothetical product, which was represented by a UDF hierarchy, as well as P/S evaluations from -3 to"
W09-2804,P05-1018,0,0.0328204,"any meaningful conclusion from the dataset.2 Thus, we stochastically θ(u) = imp pos(u)/(imp pos(u)+imp neg(u)) The distribution-based multiplier E(u, v) is the Jensen-Shannon divergence from Ber(θ(u)) to Ber(θ(v)), plus one for multiplicative identity when the divergence is zero. E(u, v) = JS(θ(u), θ(v)) + 1 The final formula for the information coverage cost is thus 2 Using a constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exists has been done in other NLP tasks such as topic boundary detection (Reynar, 1994) and local coherence modelling (Barzilay and Lapata, 2005). We are encouraged, however, that subsequent to our experiment, more resources for opinion anald(u, v) = dir moi(v) × T (u, v) × E(u, v) Consider the following example consisting of four-node UDF tree and importance scores. 11 # Features # Evaluated Features # Children (depth 0) # Children (depth 1 fertile) mean 55.3889 21.6667 11.3056 5.5495 std. 8.5547 5.9722 0.7753 1.7724 visualizing data in the customer review domain, even for novice users (Carenini et al., 2006). In a Treemap, the feature hierarchy is represented by nested rectangles, with parent features being larger rectangles, and chi"
W09-2804,P04-1035,0,0.0062224,"lements, and returning the items with the highest scores. In GEA (Generator of Evaluative Arguments), evaluative arguments are generated to describe an entity as positive or negative (Carenini and Moore, 2006). An entity is decomposed into a hierarchy of features, and a relevance score is independently calculated for each feature, based on the preferences of the user and the value of that feature for the product. Content selection involves selecting the most relevant features for the current user. There is also work in sentiment analysis relying on optimization or clustering-based approaches. Pang and Lee (2004) frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences. They produce compressed versions of movie reviews using just the subjective sentences, which retain the polarity information of the review. Gamon et al. (2005) use a heuristic approach to cluster sentences drawn from car reviews, grouping sentences that share common terms, especially those salient in the domain such as ‘drive’ or ‘handling’. The resulting clusters are displayed by a Treemap visualization. Our work is most similar to the content selection method of the mul"
W09-2804,W08-1106,1,0.839395,"d in the summarization process. For example, in multi-modal summarization, complex information can be more effectively conveyed by combining graphics and text (Tufte et al., 1998). While graphics can present large amounts of data compactly and support the discovery of trends and relationships, text is much more effective at explaining key points about the data. In another case specific to opinion summarization, the controversiality of the opinions in a corpus was found to correlate with the type of text summary, with abstractive summarization being preferred when the controversiality is high (Carenini and Cheung, 2008). We first test whether our optimization-based approach can achieve reasonable performance on content selection alone. As a contribution of this paper, we compare our optimization-based approach to a previously proposed heuristic method. Because our approach replaces a set of myopic decisions with an extensively studied procedure (the p-median problem) that is able to find a global solution, we hypothesized our approach would produce better selections. The results of our study indicate that our optimization-based content selection strategy performs about as well as the heuristic method. These"
W09-2804,P94-1050,0,0.110698,"of products, which limits our ability to draw any meaningful conclusion from the dataset.2 Thus, we stochastically θ(u) = imp pos(u)/(imp pos(u)+imp neg(u)) The distribution-based multiplier E(u, v) is the Jensen-Shannon divergence from Ber(θ(u)) to Ber(θ(v)), plus one for multiplicative identity when the divergence is zero. E(u, v) = JS(θ(u), θ(v)) + 1 The final formula for the information coverage cost is thus 2 Using a constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exists has been done in other NLP tasks such as topic boundary detection (Reynar, 1994) and local coherence modelling (Barzilay and Lapata, 2005). We are encouraged, however, that subsequent to our experiment, more resources for opinion anald(u, v) = dir moi(v) × T (u, v) × E(u, v) Consider the following example consisting of four-node UDF tree and importance scores. 11 # Features # Evaluated Features # Children (depth 0) # Children (depth 1 fertile) mean 55.3889 21.6667 11.3056 5.5495 std. 8.5547 5.9722 0.7753 1.7724 visualizing data in the customer review domain, even for novice users (Carenini et al., 2006). In a Treemap, the feature hierarchy is represented by nested rectang"
W09-2804,J08-1001,0,\N,Missing
W15-4003,S12-1051,0,0.0114011,"o optimize distributional criteria based on similarity correlations or predicting a word in context. However, it is not enough to rely solely on these criteria. Similarity only supports relative reasoning about relations between concepts, and it is difficult to adapt such measures to make absolute inferences about the truth value of a proposition. The applications of distributional semantics (DS) to date have reflected this bias. The most common approach to evaluate DS models has been to correlate predicted similarity judgments against judgments gathered from humans (Finkelstein et al., 2002; Agirre et al., 2012). More recent applications in paraphrase detection (Socher et al., 2011), textual entailment (Beltagy et al., 2013) and analogical reasoning (Mikolov et al., 2013) are also primarily concerned with the relationships between phrases. A more serious issue is that distributional semantics alone seems to be insufficient for handling rarely occurring events and entities, if we treat them as just another target phrase in the corpus. Consider the following passage: We propose to base the development of vector-space models of semantics on concept extensions, which defines concepts to be sets of entiti"
W15-4003,N07-4013,0,0.0341594,"Missing"
W15-4003,P13-2078,0,0.0303936,"ral models have recently been proposed which combine distributional with ontological information (Fried and Duh, 2014; Yang et al., 2014). The goal of these papers is to encode the ontological relationships as some kind of regularity in the learned vector space, usually as a linear transformation; e.g., that objective encourages there to be a consistent vector addition operation that represents the part-of relationship between two concepts. By contrast, our work argues for an entirely different kind of objective function for a vector-space model motivated by classical compositional semantics. Herbelot and Ganesalingam (2013) investigate KL-divergence of a semantic vector as a simple information-theoretic measure to determine hypernym/hyponym relations, but found that this was outperformed by a word frequency baseline. Other work employs distributional similarity to learn to cluster concepts into a hierarchy (Yamada et al., 2009, for example). There have also been supervised methods for hypernymy detection (Roller et al., 2014, for example). Typically, this is done for upward-entailing concept-to-concept reasoning, for example between word pairs (e.g., van entails car) as in the BLESS data set (Baroni and Lenci, 2"
W15-4003,W11-2501,0,0.0234707,"Ganesalingam (2013) investigate KL-divergence of a semantic vector as a simple information-theoretic measure to determine hypernym/hyponym relations, but found that this was outperformed by a word frequency baseline. Other work employs distributional similarity to learn to cluster concepts into a hierarchy (Yamada et al., 2009, for example). There have also been supervised methods for hypernymy detection (Roller et al., 2014, for example). Typically, this is done for upward-entailing concept-to-concept reasoning, for example between word pairs (e.g., van entails car) as in the BLESS data set (Baroni and Lenci, 2011). Another thread of related work is in relation extraction (Banko et al., 2007; Bunescu and Mooney, 2007; Riedel et al., 2013, for example), and knowledge base population, such as the TAC shared task (McNamee and Dang, 2009). This This passage is an edited version of his Wikipedia article. 23 3.1 work is concerned with extracting the relationships between entities, in order to improve the quality of a database. Our work can be seen as a way of integrating distributional semantics into large-scale reasoning about entities. Most recently, Gupta et al. (2015) investigate a similar problem, using"
W15-4003,P14-1023,0,0.0386411,"ffect of each of these components, we also train baseline versions of the model that omit one or the other feature set. Thus, we compare the following three sets of features: DS We derive a distributional vector of features from word2vec, a popular recent approach to distributional semantics (Mikolov et al., 2013). We use the 300-dimensional pre-trained vectors available on their website, which include both singleword and multi-word entities. We chose word2vec as it is a popular recent model of distributional semantics which has been shown to work well on a variety of existing semantic tasks (Baroni et al., 2014). We leave the comparison of this model to other recent distributional semantic models (Pennington et al., 2014, for example) to future work. Framework and Model Our model is designed to learn entity representations that are useful for predicting concept extensions, which are sets of entities in the domain. Let C = {c1 , c2 , ...} be the set of concepts of interest, and E = {e1 , e2 , ...} be the set of entities. Since we are interested in extensional meaning, each concept c is defined by its extension, exten(c), a set of elements drawn from E. Rather than explicitly enumerating these sets, we"
W15-4003,N13-1090,0,0.218678,"imilarity only supports relative reasoning about relations between concepts, and it is difficult to adapt such measures to make absolute inferences about the truth value of a proposition. The applications of distributional semantics (DS) to date have reflected this bias. The most common approach to evaluate DS models has been to correlate predicted similarity judgments against judgments gathered from humans (Finkelstein et al., 2002; Agirre et al., 2012). More recent applications in paraphrase detection (Socher et al., 2011), textual entailment (Beltagy et al., 2013) and analogical reasoning (Mikolov et al., 2013) are also primarily concerned with the relationships between phrases. A more serious issue is that distributional semantics alone seems to be insufficient for handling rarely occurring events and entities, if we treat them as just another target phrase in the corpus. Consider the following passage: We propose to base the development of vector-space models of semantics on concept extensions, which defines concepts to be sets of entities. We investigate two sources of knowledge about entities that could be relevant: distributional information provided by word or phrase embeddings, and ontologica"
W15-4003,S13-1002,0,0.0434693,"Missing"
W15-4003,D14-1162,0,0.0748651,"eature set. Thus, we compare the following three sets of features: DS We derive a distributional vector of features from word2vec, a popular recent approach to distributional semantics (Mikolov et al., 2013). We use the 300-dimensional pre-trained vectors available on their website, which include both singleword and multi-word entities. We chose word2vec as it is a popular recent model of distributional semantics which has been shown to work well on a variety of existing semantic tasks (Baroni et al., 2014). We leave the comparison of this model to other recent distributional semantic models (Pennington et al., 2014, for example) to future work. Framework and Model Our model is designed to learn entity representations that are useful for predicting concept extensions, which are sets of entities in the domain. Let C = {c1 , c2 , ...} be the set of concepts of interest, and E = {e1 , e2 , ...} be the set of entities. Since we are interested in extensional meaning, each concept c is defined by its extension, exten(c), a set of elements drawn from E. Rather than explicitly enumerating these sets, we instead aim to learn a function f : E → P(C) that maps an input entity to the concepts of which it is an eleme"
W15-4003,N13-1008,0,0.0181451,"m/hyponym relations, but found that this was outperformed by a word frequency baseline. Other work employs distributional similarity to learn to cluster concepts into a hierarchy (Yamada et al., 2009, for example). There have also been supervised methods for hypernymy detection (Roller et al., 2014, for example). Typically, this is done for upward-entailing concept-to-concept reasoning, for example between word pairs (e.g., van entails car) as in the BLESS data set (Baroni and Lenci, 2011). Another thread of related work is in relation extraction (Banko et al., 2007; Bunescu and Mooney, 2007; Riedel et al., 2013, for example), and knowledge base population, such as the TAC shared task (McNamee and Dang, 2009). This This passage is an edited version of his Wikipedia article. 23 3.1 work is concerned with extracting the relationships between entities, in order to improve the quality of a database. Our work can be seen as a way of integrating distributional semantics into large-scale reasoning about entities. Most recently, Gupta et al. (2015) investigate a similar problem, using a logistic regression model to map features derived from distributional methods to referential properties of countries that a"
W15-4003,P07-1073,0,0.0282718,"asure to determine hypernym/hyponym relations, but found that this was outperformed by a word frequency baseline. Other work employs distributional similarity to learn to cluster concepts into a hierarchy (Yamada et al., 2009, for example). There have also been supervised methods for hypernymy detection (Roller et al., 2014, for example). Typically, this is done for upward-entailing concept-to-concept reasoning, for example between word pairs (e.g., van entails car) as in the BLESS data set (Baroni and Lenci, 2011). Another thread of related work is in relation extraction (Banko et al., 2007; Bunescu and Mooney, 2007; Riedel et al., 2013, for example), and knowledge base population, such as the TAC shared task (McNamee and Dang, 2009). This This passage is an edited version of his Wikipedia article. 23 3.1 work is concerned with extracting the relationships between entities, in order to improve the quality of a database. Our work can be seen as a way of integrating distributional semantics into large-scale reasoning about entities. Most recently, Gupta et al. (2015) investigate a similar problem, using a logistic regression model to map features derived from distributional methods to referential propertie"
W15-4003,C14-1097,0,0.0333322,"Missing"
W15-4003,P05-1045,0,0.0220323,"and parameter tuning on a development set prevents the model from overfitting to the training data by simply mirroring the appropriate values of the input vector. 4 Experiments Our experiments were conducted on the collaborative knowledge base, Freebase. We extracted three classes of entities from the June 9, 2014 dump of Freebase by taking instances of top-level concepts (i.e., Freebase types) corresponding to People, Organizations, and Locations, as shown in Table 1. We chose these classes because they are the entity classes most often modelled by other work in NLP, such as by NER taggers (Finkel et al., 2005). These classes also tend to be a part of many different scenarios, thus there should be rich ontological 4.1 Method We applied the models described above to predict the concept memberships of entities in the fifty most common concepts of each entity class. We focused on the most common concepts, because they are likely to be the important high-level divisions in the entity class, and are also more likely 25 Entity category People Organizations Locations Freebase ID /people/person /organization/organization /location/location N entities (train + dev + test) 23053 + 7684 + 7685 4771 + 1591 + 15"
W15-4003,D09-1097,0,0.0423499,"Missing"
W15-4003,Q14-1006,0,0.0287966,"grate distributional semantics with formal, compositional semantics. For example, semantic relations between concepts could be detected based on their formal, set-theoretic definitions, as shown in Section 5. The framework and model presented in this paper suggest a natural way to predict these and other semantic relations without the need for another classification step. It would also be interesting to see whether the ontological information/concept extensions, which in this work was supplied by a knowledge base, could be derived or augmented through other means, such as by using image data (Young et al., 2014). ing definitions of intersection and union between fuzzy sets A and B: µA∩B = min(µA , µB ) (4) µA∪B = max(µA , µB ). (5) A concept c is a hyponymy of another concept if exten(c) ⊆ exten(c0 ). We determine the subset relation in fuzzy logic reducing it to fuzzy set intersection and set equality, and we determine fuzzy set equality by using a generalized version of Jaccard similarity using L1-norms: c0 A⊆B ↔A∩B =A kµA∩B k1 fuzzyJ(µA , µB ) = . kµA∪B k1 (6) (7) The degree of hyponymy of ci to cj , hypo(ci , cj ), is then simply hypo(ci , cj ) = fuzzyJ(µi∩j , µi ). We present several subset rela"
W16-5809,W14-3917,0,0.0321731,"Missing"
W16-5809,P00-1031,0,0.174148,"ation in Codeswitched Data (Chittaranjan et al., 2014; King et al., 2014). The most common technique was to employ supervised machine learning algorithms (e.g., extended Markov Models and Conditional Random Field) to train a classifier. Codeswitched language identification has been previously studied with other language pairs, (Carter et al., 2013; Nguyen and Dogruoz, 2014; Vyas et al., 2014; Das and Gamb¨ack, 2013; Voss et al., 2014). However, very few articles discuss codeswitched Pinyin-English input specifically. There has been research on improving the error tolerance of Pinyinbased IME. Chen and Lee (2000) propose a statistical segmentation and a trigram-based language model to convert Pinyin sequences into Chinese character sequences in a manner that is robust to single-character Pinyin misspellings. They also propose a paradigm called modeless Pinyin that tries to eliminate the necessity of toggling on and off the Pinyin input method. While their modeless Pinyin works on Pinyin generating a single Chinese character or a single English word, our experiments in this paper attempt to generate an entire sequence of Chinese characters and English words. Research in improving the codeswitched text"
W16-5809,W14-3908,0,0.27887,"that word-level models produce better conversion performance. Our automatic conversion method achieves the same level of performance as an oracle system with perfect knowledge of token-level language identity. This result demonstrates that Pinyin identification is not the bottleneck towards developing a Chinese-English codeswitched IME, and that future work should focus on the Pinyin-to-Chinese character conversion step. 2 Related Work Several models for MAN-EN codeswitched language identification were developed as part of the First Shared Task on Language Identification in Codeswitched Data (Chittaranjan et al., 2014; King et al., 2014). The most common technique was to employ supervised machine learning algorithms (e.g., extended Markov Models and Conditional Random Field) to train a classifier. Codeswitched language identification has been previously studied with other language pairs, (Carter et al., 2013; Nguyen and Dogruoz, 2014; Vyas et al., 2014; Das and Gamb¨ack, 2013; Voss et al., 2014). However, very few articles discuss codeswitched Pinyin-English input specifically. There has been research on improving the error tolerance of Pinyinbased IME. Chen and Lee (2000) propose a statistical segmentatio"
W16-5809,W14-5316,0,0.0639672,"Missing"
W16-5809,N13-1131,0,0.0689139,"Missing"
W16-5809,W14-3909,0,0.021528,"input is segmented into one of (1) a Pinyin sequence representing Chinese words7 , (2) an English word, or (3) other (space and punctuation). Each chunk is labeled as one of pinyin, non-pinyin or other. The Pinyin sequences representing Chinese words are indirectly segmented according to the word segmentation of 7 Note that a Chinese word can be either a single Chinese character or a concatenation of multiple Chinese characters. 74 4.1 Feature Extraction We selected features to pass into our models by drawing upon recent work in codeswitched language identification (Chittaranjan et al., 2014; Lin et al., 2014; Nguyen and Dogruoz, 2014). We explored a number of different options for features, and the final set was chosen based on performance on a heldout development set, as follows. Word-Based Models (WBM) The following features were chosen for each segment s: • • • • • • • Identity of s, converted to lower case Whether s is a legal sequence of Pinyin Whether s is upper-cased Whether s is capitalized Whether s is a number The token occurring prior to s in the sequence The token occurring after to s in the sequence Letter-Based Models (LBM) The following features were chosen for each segment t: • •"
W16-5809,P13-1018,0,0.0304885,"ers to simulate codeswitched text input. Our method achieves the same level of performance as an oracle system that has perfect knowledge of token-level language identity. This result demonstrates that Pinyin identification is not the bottleneck towards developing a ChineseEnglish codeswitched Input Method Editor, and future work should focus on the Pinyinto-Chinese character conversion step. 1 Introduction As more people are connected to the Internet around the world, an increasing number of multilingual texts can be found, especially in informal, online platforms such as Twitter and Weibo1 (Ling et al., 2013). In this paper, we focus on short Mandarin1 Weibo is a micro-blogging service similar to Twitter that is widely used in China. (1) CS: 这个thermal exchanger的thermal conductivity太低. MAN: 这个换热器的热传导系数太低. EN: The thermal conductivity of this thermal exchanger is too low. A natural first step in processing codeswitched text is to identify which parts of the text are expressed in which language, as having an accurate codeswitched language identification system seems to be a crucial building block for further processing such as POS tagging. Recently, Solorio et al. (2014) organized the first shared ta"
W16-5809,voss-etal-2014-finding,0,0.028525,"inese character conversion step. 2 Related Work Several models for MAN-EN codeswitched language identification were developed as part of the First Shared Task on Language Identification in Codeswitched Data (Chittaranjan et al., 2014; King et al., 2014). The most common technique was to employ supervised machine learning algorithms (e.g., extended Markov Models and Conditional Random Field) to train a classifier. Codeswitched language identification has been previously studied with other language pairs, (Carter et al., 2013; Nguyen and Dogruoz, 2014; Vyas et al., 2014; Das and Gamb¨ack, 2013; Voss et al., 2014). However, very few articles discuss codeswitched Pinyin-English input specifically. There has been research on improving the error tolerance of Pinyinbased IME. Chen and Lee (2000) propose a statistical segmentation and a trigram-based language model to convert Pinyin sequences into Chinese character sequences in a manner that is robust to single-character Pinyin misspellings. They also propose a paradigm called modeless Pinyin that tries to eliminate the necessity of toggling on and off the Pinyin input method. While their modeless Pinyin works on Pinyin generating a single Chinese character"
W16-5809,D14-1105,0,0.0252686,"uture work should focus on the Pinyin-to-Chinese character conversion step. 2 Related Work Several models for MAN-EN codeswitched language identification were developed as part of the First Shared Task on Language Identification in Codeswitched Data (Chittaranjan et al., 2014; King et al., 2014). The most common technique was to employ supervised machine learning algorithms (e.g., extended Markov Models and Conditional Random Field) to train a classifier. Codeswitched language identification has been previously studied with other language pairs, (Carter et al., 2013; Nguyen and Dogruoz, 2014; Vyas et al., 2014; Das and Gamb¨ack, 2013; Voss et al., 2014). However, very few articles discuss codeswitched Pinyin-English input specifically. There has been research on improving the error tolerance of Pinyinbased IME. Chen and Lee (2000) propose a statistical segmentation and a trigram-based language model to convert Pinyin sequences into Chinese character sequences in a manner that is robust to single-character Pinyin misspellings. They also propose a paradigm called modeless Pinyin that tries to eliminate the necessity of toggling on and off the Pinyin input method. While their modeless Pinyin works on"
W16-6010,W14-4012,0,0.0049453,"Missing"
W16-6010,D14-1179,0,0.00580028,"Missing"
W16-6010,W01-0521,0,0.0328525,"ns of style and semantic content in order to generate output text in the desired style. In the rest of the paper, we discuss our plans to investigate stylistic transfer with neural networks in more detail. We will also propose several evaluation criteria for stylistic transfer and discuss evaluation methodologies using human user studies. 2 Related Work Capturing stylistic variation is a long-standing problem in NLP. Sekine (1997) and Ratnaparkhi (1999) consider the different categories in the Brown corpus to be domains. These include general fiction, romance and love story, press: reportage. Gildea (2001), on the other hand, refers to these categories as genres. Different NLP sub-communities use the terms domain, style and genre to denote slightly different concepts (Lee, 2001). From a linguistic point of view, domains could be thought of as broad subject fields, while genre can be seen as a category assigned on the basis of external criteria such as intended audience, purpose, and activity type. Style conveys the social context in which communication occurs and define particular ways of using language to engage with the audiences to which the text is accessible. Some linguists would argue tha"
W16-6010,P06-2058,0,0.121331,"s to which the text is accessible. Some linguists would argue that style and domain are two attributes characterizing genre (e.g., (Lee, 2001)) while others view genre and domain as aspects representing style (e.g., (Moessner, 2001)). The notion of genre has been the focus of related NLP tasks. In genre classification (Petrenz and Webber, 2011; Sharoff et al., 2010; Feldman et al., 2009), the task is to categorize the text into one of several genres. In author identification (Houvardas and Stamatatos, 2006; Chaski, 2001), the goal is to identify the author of a text, while author obfuscation (Kacmarcik and Gamon, 2006; Juola and Vescovi, 2011) consists in modifying aspects of the texts so that forensic analysis fails to reveal the identity of the author. 44 In (Pavlick and Tetreault, 2016), an analysis of formality in online written communication is presented. A set of linguistic features is proposed based on a study of human perceptions of formality across multiple genres. Those features are fed to a statistical model that classifies texts as having a formal or informal style. At the lexical level, Brooke et al. (2010) focused on constructing lexicons of formality that can be used in tasks such as genre c"
W16-6010,Q16-1005,0,0.00778424,"domain as aspects representing style (e.g., (Moessner, 2001)). The notion of genre has been the focus of related NLP tasks. In genre classification (Petrenz and Webber, 2011; Sharoff et al., 2010; Feldman et al., 2009), the task is to categorize the text into one of several genres. In author identification (Houvardas and Stamatatos, 2006; Chaski, 2001), the goal is to identify the author of a text, while author obfuscation (Kacmarcik and Gamon, 2006; Juola and Vescovi, 2011) consists in modifying aspects of the texts so that forensic analysis fails to reveal the identity of the author. 44 In (Pavlick and Tetreault, 2016), an analysis of formality in online written communication is presented. A set of linguistic features is proposed based on a study of human perceptions of formality across multiple genres. Those features are fed to a statistical model that classifies texts as having a formal or informal style. At the lexical level, Brooke et al. (2010) focused on constructing lexicons of formality that can be used in tasks such as genre classification or sentiment analysis. In (Inkpen and Hirst, 2004), a set list of near-synonyms is given for a target word, and one synonym is selected based on several types of"
W16-6010,J11-2004,0,0.0144471,"fields, while genre can be seen as a category assigned on the basis of external criteria such as intended audience, purpose, and activity type. Style conveys the social context in which communication occurs and define particular ways of using language to engage with the audiences to which the text is accessible. Some linguists would argue that style and domain are two attributes characterizing genre (e.g., (Lee, 2001)) while others view genre and domain as aspects representing style (e.g., (Moessner, 2001)). The notion of genre has been the focus of related NLP tasks. In genre classification (Petrenz and Webber, 2011; Sharoff et al., 2010; Feldman et al., 2009), the task is to categorize the text into one of several genres. In author identification (Houvardas and Stamatatos, 2006; Chaski, 2001), the goal is to identify the author of a text, while author obfuscation (Kacmarcik and Gamon, 2006; Juola and Vescovi, 2011) consists in modifying aspects of the texts so that forensic analysis fails to reveal the identity of the author. 44 In (Pavlick and Tetreault, 2016), an analysis of formality in online written communication is presented. A set of linguistic features is proposed based on a study of human perce"
W16-6010,A97-1015,0,0.0622762,"s, but also allows us to frame stylistic transfer as a weakly supervised problem without parallel data, in which the model learns to disentangle and recombine latent representations of style and semantic content in order to generate output text in the desired style. In the rest of the paper, we discuss our plans to investigate stylistic transfer with neural networks in more detail. We will also propose several evaluation criteria for stylistic transfer and discuss evaluation methodologies using human user studies. 2 Related Work Capturing stylistic variation is a long-standing problem in NLP. Sekine (1997) and Ratnaparkhi (1999) consider the different categories in the Brown corpus to be domains. These include general fiction, romance and love story, press: reportage. Gildea (2001), on the other hand, refers to these categories as genres. Different NLP sub-communities use the terms domain, style and genre to denote slightly different concepts (Lee, 2001). From a linguistic point of view, domains could be thought of as broad subject fields, while genre can be seen as a category assigned on the basis of external criteria such as intended audience, purpose, and activity type. Style conveys the soc"
W16-6010,N16-1005,0,0.0141252,"r writing style. Since the problem is framed as a machine translation problem, it relies on parallel data where the source “language” is the original text to be paraphrased–in that case, Shakespeare texts–and the “translation” is the equivalent modern English version of those Shakespeare texts. Accordingly, for each source sentence, there exists a parallel sentence having the target style. They also present some baselines which do not make use of parallel sentences and instead rely on manually compiled dictionaries of expressions commonly found in Shakespearean English. In a more recent work, Sennrich et al. (2016) carry out translation from English to German while controlling the degree of politeness. This is done in the context of neural machine translation by adding side constraints. Specifically, they mark up the source language of the training data (in this case, English) with a feature that encodes the use of honorifics seen in the target language (in this case, German). This allows them to control the honorifics that are produced at test time. 3 Proposed Approach Recently, RNN-based models have been successfully used in machine translation (Cho et al., 2014b; Cho et al., 2014a; Sutskever et al.,"
W16-6010,sharoff-etal-2010-web,0,0.0142269,"seen as a category assigned on the basis of external criteria such as intended audience, purpose, and activity type. Style conveys the social context in which communication occurs and define particular ways of using language to engage with the audiences to which the text is accessible. Some linguists would argue that style and domain are two attributes characterizing genre (e.g., (Lee, 2001)) while others view genre and domain as aspects representing style (e.g., (Moessner, 2001)). The notion of genre has been the focus of related NLP tasks. In genre classification (Petrenz and Webber, 2011; Sharoff et al., 2010; Feldman et al., 2009), the task is to categorize the text into one of several genres. In author identification (Houvardas and Stamatatos, 2006; Chaski, 2001), the goal is to identify the author of a text, while author obfuscation (Kacmarcik and Gamon, 2006; Juola and Vescovi, 2011) consists in modifying aspects of the texts so that forensic analysis fails to reveal the identity of the author. 44 In (Pavlick and Tetreault, 2016), an analysis of formality in online written communication is presented. A set of linguistic features is proposed based on a study of human perceptions of formality ac"
W16-6010,D15-1199,0,0.0295424,"Missing"
W16-6010,C12-1177,0,0.035547,"man perceptions of formality across multiple genres. Those features are fed to a statistical model that classifies texts as having a formal or informal style. At the lexical level, Brooke et al. (2010) focused on constructing lexicons of formality that can be used in tasks such as genre classification or sentiment analysis. In (Inkpen and Hirst, 2004), a set list of near-synonyms is given for a target word, and one synonym is selected based on several types of preferences, e.g., stylistic (degree of formality). We aim to generalize this work beyond the lexical level. A similar work is that of Xu et al. (2012) which propose using phrase-based machine translation systems to carry out paraphrasing while targeting a particular writing style. Since the problem is framed as a machine translation problem, it relies on parallel data where the source “language” is the original text to be paraphrased–in that case, Shakespeare texts–and the “translation” is the equivalent modern English version of those Shakespeare texts. Accordingly, for each source sentence, there exists a parallel sentence having the target style. They also present some baselines which do not make use of parallel sentences and instead rel"
W16-6010,C10-2011,0,\N,Missing
W17-5531,W10-2923,0,0.319996,"specific to the user who initiates the dialogue. Even in constrained domains it has been observed that a user’s perception of success is more indicative of user satisfaction than an objective measure (Walker et al., 2000; Williams and Young, 2004). Thus, we aim to let the original participant be the judge of success and build models that can predict success based on information communicated rather than enforcing a rigorous definition in our models. An impediment in building models that predict success (or interactive agents) in these domains is the lack of success labels in current datasets (Kim et al., 2010; Lowe et al., 2015). These labels can be difficult to collect as forums often do not proIn goal-driven dialogue systems, success is often defined based on a structured definition of the goal. This requires that the dialogue system be constrained to handle a specific class of goals and that there be a mechanism to measure success with respect to that goal. However, in many human-human dialogues the diversity of goals makes it infeasible to define success in such a way. To address this scenario, we consider the task of automatically predicting success in goal-driven human-human dialogues using"
W17-5531,P15-1107,0,0.016207,"mple behind using RNNs is their ability to predict complex non-linear discourse features. For example, consider the following comments: Rejected: Thanks for the advice. I tried this change, but I am still encountering the same error. 4.2 Turn-Based Hierarchical Recurrent Neural Network In this model, we extend the Flat RNN to model the natural hierarchy that occurs in dialogues. This allows our model to separate the flow of content throughout the dialogue from the natural language realization of each turn. Our model is similar to the encoder models used in previous work (Sordoni et al., 2015; Li et al., 2015). Similar to the Flat RNN, each word is projected into its vectorized word embedding. Then for each turn ti , we feed its word embeddings through the same RNN (the turn-level RNN) which outputs an encoded version of that turn, ten,i . We feed all these encoded turn vectors into a higher-level RNN (the dialogue-level RNN) which takes into account the context of the dialogue. We also use LSTM units (Hochreiter and Schmidhuber, 1997) with GloVe embeddings (Pennington et al., 2014). Refer to Figure 3 (b) for the model architecture. 257 Figure 3: Flat (a) and Turn-Based Hierarchical (b) RNN models"
W17-5531,W15-4640,1,0.938842,"ser who initiates the dialogue. Even in constrained domains it has been observed that a user’s perception of success is more indicative of user satisfaction than an objective measure (Walker et al., 2000; Williams and Young, 2004). Thus, we aim to let the original participant be the judge of success and build models that can predict success based on information communicated rather than enforcing a rigorous definition in our models. An impediment in building models that predict success (or interactive agents) in these domains is the lack of success labels in current datasets (Kim et al., 2010; Lowe et al., 2015). These labels can be difficult to collect as forums often do not proIn goal-driven dialogue systems, success is often defined based on a structured definition of the goal. This requires that the dialogue system be constrained to handle a specific class of goals and that there be a mechanism to measure success with respect to that goal. However, in many human-human dialogues the diversity of goals makes it infeasible to define success in such a way. To address this scenario, we consider the task of automatically predicting success in goal-driven human-human dialogues using only the information"
W17-5531,W16-3634,1,0.889547,"Missing"
W17-5531,P98-2219,0,0.152125,"k has largely focused on small domains where one can manually define every task the system or participants can perform and what it means to complete the task. Success, as defined by task completion, is easier to evaluate in traditional dialogue systems which have been highly scripted. These systems are designed for restricted domains in which the relevant ontology and language generation prompts or templates have been specified. Such systems include the Let’s Go Pittsburgh Bus System (Raux et al., 2003), the Cambridge Restaurant System (Thomson and Young, 2010), and the ELVIS email assistant (Walker et al., 1998). Scaling these systems to larger domains, such as those found in online forums, is difficult because expanding their ontologies becomes infeasible. The PARADISE framework (Walker et al., 1997) was proposed to automatically evaluate dialogues where the quality of a dialogue can be seen to consist of task success and costs such as the dialogue length. Here, task success has a rigid definition, where for each dialogue system using this framework, the designer must specify what attributes need to be communicated by the system to achieve a goal. This definition makes it clear what success looks li"
W17-5531,D14-1162,0,0.0837651,"ate. We insert a special token, hti (with its own embedding), to denote the separation between turns of the dialogue. At the end of the dialogue, the RNN makes a prediction using a logistic regression unit on the final hidden state of the network. This allows us to learn discourse features beyond bag-of-words (BOW) as we maintain word ordering. See Figure 3(a) for the architecture. In our implementation, we use Long Short Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) to account for long-term dependencies. We use Theano (Bastien et al., 2012) and pretrained GloVe word embeddings (Pennington et al., 2014). Optimization is done using the ADAM optimizer (Kingma and Ba, 2014) to minimize cross-entropy between the model predictions, success, and the actual success labels, y. Preprocessing Before continuing with any experiments, we preprocess each question, answer, and comment by removing HTML tags, and replacing numbers and code with generic tags. 4 Flat Recurrent Neural Network Recurrent Neural Network Models for Dialogue Success Prediction Presented with a dialogue that consists of a series of turns (question, answer, and comments), we would like to classify whether that dialogue was successful"
W17-5531,P97-1035,0,0.624821,"Missing"
W17-5531,N16-1015,0,0.0331839,"Missing"
W17-5531,C98-2214,0,\N,Missing
W18-1002,speer-havasi-2012-representing,0,0.251538,"e closest triple found in the training set: “same relation and minor rewording” (1), “difComparison of KBC and Wikipedia evaluations First, we directly test if the performance of a model on the KBC task is predictive of its performance on the mining task. We follow the mining evaluation protocol from (Li et al., 2016): we 2 We note that random test set consists of worse quality triples than confidence-based split. However, the latter leads to a serious bias in evaluation. We leave addressing this tradeoff for future work. 3 The threshold is selected based on a separate development set, as in (Speer and Havasi, 2012). 10 XXX X Novelty 1 Dataset XXXX ferent relation and minor rewording” (2), “same relation and related word” (3), “different relation and related word” (4), “no directly related triple” (5). We ignore a small percentage of triples that are not describing commonsense knowledge, as well as false triples (some in the random subset, and a large percentage in the Wikipedia dataset). To give a better intuition, we provide example triples for the confidence-based split of Li et al. (2016). In Category 1 (defined as “same relation and minor rewording”), we find (“egg”, “IsA”, “food”), which has a clos"
W18-1002,W13-3515,0,\N,Missing
W18-1002,P15-1014,0,\N,Missing
W18-1002,N15-1098,0,\N,Missing
W18-1002,P16-1137,0,\N,Missing
W18-1002,P17-1025,0,\N,Missing
