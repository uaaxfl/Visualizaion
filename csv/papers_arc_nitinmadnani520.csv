2020.nlposs-1.20,User-centered {\\&} Robust {NLP} {OSS}: Lessons Learned from Developing {\\&} Maintaining {RSMT}ool,2020,-1,-1,1,1,16057,nitin madnani,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),0,"For the last 5 years, we have developed and maintained RSMTool {--} an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its cross-disciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper."
2020.bea-1.2,Using {PRMSE} to evaluate automated scoring systems in the presence of label noise,2020,-1,-1,2,1,16058,anastassia loukina,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"The effect of noisy labels on the performance of NLP systems has been studied extensively for system training. In this paper, we focus on the effect that noisy labels have on system evaluation. Using automated scoring as an example, we demonstrate that the quality of human ratings used for system evaluation have a substantial impact on traditional performance metrics, making it impossible to compare system evaluations on labels with different quality. We propose that a new metric, PRMSE, developed within the educational measurement community, can help address this issue, and provide practical guidelines on using PRMSE."
2020.acl-main.697,Automated Evaluation of Writing {--} 50 Years and Counting,2020,-1,-1,2,0,19993,beata klebanov,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page{'}s seminal 1966 paper to frame the presentation. We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology."
W19-4401,The many dimensions of algorithmic fairness in educational applications,2019,0,0,2,1,16058,anastassia loukina,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people{'}s lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers{'} native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.
W19-4432,Toward Automated Content Feedback Generation for Non-native Spontaneous Speech,2019,0,0,6,0.470817,24182,suyoun yoon,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"In this study, we developed an automated algorithm to provide feedback about the specific content of non-native English speakers{'} spoken responses. The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and/or read passages and integrated the core content in their spoken responses. Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models: (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features. Both models achieved a substantially improved performance over the majority baseline, and the combination of the two models achieved a significant further improvement. In particular, the models were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions. The accuracy and F-score of the best model for the questions included in the train set were 0.80 and 0.68, respectively. Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner{'}s response, based on automatically detected missing key points."
P19-3024,My Turn To Read: An Interleaved {E}-book Reading Tool for Developing and Struggling Readers,2019,0,1,1,1,16057,nitin madnani,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Literacy is crucial for functioning in modern society. It underpins everything from educational attainment and employment opportunities to health outcomes. We describe My Turn To Read, an app that uses interleaved reading to help developing and struggling readers improve reading skills while reading for meaning and pleasure. We hypothesize that the longer-term impact of the app will be to help users become better, more confident readers with an increased stamina for extended reading. We describe the technology and present preliminary evidence in support of this hypothesis."
W18-2504,The {ACL} {A}nthology: Current State and Future Directions,2018,-1,-1,3,0,3945,daniel gildea,Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS}),0,"The Association of Computational Linguistic{'}s Anthology is the open source archive, and the main source for computational linguistics and natural language processing{'}s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology{'}s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards."
W18-0506,Second Language Acquisition Modeling,2018,0,7,5,0,8763,burr settles,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present the task of \textit{second language acquisition (SLA) modeling}. Given a history of errors made by learners of a second language, the task is to predict errors that they are likely to make at arbitrary points in the future. We describe a large corpus of more than 7M words produced by more than 6k learners of English, Spanish, and French using Duolingo, a popular online language-learning app. Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus, which attracted 15 teams and synthesized work from various fields including cognitive science, linguistics, and machine learning."
N18-3008,Atypical Inputs in Educational Applications,2018,0,0,6,0.470817,24182,suyoun yoon,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",0,"In large-scale educational assessments, the use of automated scoring has recently become quite common. While the majority of student responses can be processed and scored without difficulty, there are a small number of responses that have atypical characteristics that make it difficult for an automated scoring system to assign a correct score. We describe a pipeline that detects and processes these kinds of responses at run-time. We present the most frequent kinds of what are called non-scorable responses along with effective filtering models based on various NLP and speech processing technologies. We give an overview of two operational automated scoring systems {---}one for essay scoring and one for speech scoring{---} and describe the filtering models they use. Finally, we present an evaluation and analysis of filtering models used for spoken responses in an assessment of language proficiency."
C18-2025,Writing Mentor: Self-Regulated Writing Feedback for Struggling Writers,2018,0,1,1,1,16057,nitin madnani,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"Writing Mentor is a free Google Docs add-on designed to provide feedback to struggling writers and help them improve their writing in a self-paced and self-regulated fashion. Writing Mentor uses natural language processing (NLP) methods and resources to generate feedback in terms of features that research into post-secondary struggling writers has classified as developmental (Burstein et al., 2016b). These features span many writing sub-constructs (use of sources, claims, and evidence; topic development; coherence; and knowledge of English conventions). Prelimi- nary analysis indicates that users have a largely positive impression of Writing Mentor in terms of usability and potential impact on their writing."
C18-1094,Automated Scoring: Beyond Natural Language Processing,2018,0,3,1,1,16057,nitin madnani,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful."
W17-5052,A Large Scale Quantitative Exploration of Modeling Strategies for Content Scoring,2017,0,4,1,1,16057,nitin madnani,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We explore various supervised learning strategies for automated scoring of content knowledge for a large corpus of 130 different content-based questions spanning four subject areas (Science, Math, English Language Arts, and Social Studies) and containing over 230,000 responses scored by human raters. Based on our analyses, we provide specific recommendations for content scoring. These are based on patterns observed across multiple questions and assessments and are, therefore, likely to generalize to other scenarios and prove useful to the community as automated content scoring becomes more popular in schools and classrooms."
W17-4609,Speech- and Text-driven Features for Automated Scoring of {E}nglish Speaking Tasks,2017,0,0,2,0.63817,16058,anastassia loukina,Proceedings of the Workshop on Speech-Centric Natural Language Processing,0,"We consider the automatic scoring of a task for which both the content of the response as well its spoken fluency are important. We combine features from a text-only content scoring system originally designed for written responses with several categories of acoustic features. Although adding any single category of acoustic features to the text-only system on its own does not significantly improve performance, adding all acoustic features together does yield a small but significant improvement. These results are consistent for responses to open-ended questions and to questions focused on some given source material."
W17-1605,Building Better Open-Source Tools to Support Fairness in Automated Scoring,2017,27,5,1,1,16057,nitin madnani,Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing,0,"Automated scoring of written and spoken responses is an NLP application that can significantly impact lives especially when deployed as part of high-stakes tests such as the GREÂ® and the TOEFLÂ®. Ethical considerations require that automated scoring algorithms treat all test-takers fairly. The educational measurement community has done significant research on fairness in assessments and automated scoring systems must incorporate their recommendations. The best way to do that is by making available automated, non-proprietary tools to NLP researchers that directly incorporate these recommendations and generate the analyses needed to help identify and resolve biases in their scoring systems. In this paper, we attempt to provide such a solution."
W16-0501,The Effect of Multiple Grammatical Errors on Processing Non-Native Writing,2016,23,4,3,0,21492,courtney napoles,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this work, we estimate the deterioration of NLP processing given an estimate of the amount and nature of grammatical errors in a text. From a corpus of essays written by English-language learners, we extract ungrammatical sentences, controlling the number and types of errors in each sentence. We focus on six categories of errors that are commonly made by English-language learners, and consider sentences containing one or more of these errors. To evaluate the effect of grammatical errors, we measure the deterioration of ungrammatical dependency parses using the labeled F-score, an adaptation of the labeled attachment score. We find notable differences between the influence of individual error types on the dependency parse, as well as interactions between multiple errors."
W16-0515,Model Combination for Correcting Preposition Selection Errors,2016,18,0,1,1,16057,nitin madnani,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Many grammatical error correction approaches use classifiers with specially-engineered features to predict corrections. A simpler alternative is to use n-gram language model scores. Rozovskaya and Roth (2011) reported that classifiers outperformed a language modeling approach. Here, we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach."
W16-0524,Automatically Scoring Tests of Proficiency in Music Instruction,2016,14,0,1,1,16057,nitin madnani,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,We present preliminary work on automatically scoring constructed responses elicited as part of a certification test designed to measure the effectiveness of the test-taker as a K-12 music teacher. This content scoring differs from most previous work in that the responses are relatively long and are written by an adult population of generally proficient English writers. We obtain reasonably good scoring performance for all the test questions using simple features. We carry out some initial error analysis and show that there is still room for improvement.
P16-4014,Language Muse: Automated Linguistic Activity Generation for {E}nglish Language Learners,2016,12,3,1,1,16057,nitin madnani,Proceedings of {ACL}-2016 System Demonstrations,0,"Current education standards in the U.S. require school students to read and understand complex texts from different subject areas (e.g., social studies). However, such texts usually contain figurative language, complex phrases and sentences, as well as unfamiliar discourse relations. This may present an obstacle to students whose native language is not English xe2x80x94 a growing sub-population in the US. 1 One way to help such students is to create classroom activities centered around linguistic elements found in subject area texts (DelliCarpini, 2008). We present a web-based tool that uses NLP algorithms to automatically generate customizable linguistic activities that are grounded in language learning research."
W15-0610,The Impact of Training Data on Automated Short Answer Scoring Performance,2015,9,8,2,1,34075,michael heilman,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Automatic evaluation of written responses to content-focused assessment items (automated short answer scoring) is a challenging educational application of natural language processing. It is often addressed using supervised machine learning by estimating models to predict human scores from detailed linguistic features such as wordn-grams. However, training data (i.e., human-scored responses) can be difficult to acquire. In this paper, we conduct experiments using scored responses to 44 prompts from 5 diverse datasets in order to better understand how training set size and other factors relate to system performance. We believe this will help future researchers and practitioners working on short answer scoring to answer practically important questions such as, xe2x80x9cHow much training data do I need?xe2x80x9d"
W15-0619,Preliminary Experiments on Crowdsourced Evaluation of Feedback Granularity,2015,18,1,1,1,16057,nitin madnani,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Providing writing feedback to English language learners (ELLs) helps them learn to write better, but it is not clear what type or how much information should be provided. There have been few experiments directly comparing the effects of different types of automatically generated feedback on ELL writing. Such studies are difficult to conduct because they require participation and commitment from actual students and their teachers, over extended periods of time, and in real classroom settings. In order to avoid such difficulties, we instead conduct a crowdsourced study on Amazon Mechanical Turk to answer questions concerning the effects of type and amount of writing feedback. We find that our experiment has several serious limitations but still yields some interesting results."
N15-1111,Effective Feature Integration for Automated Short Answer Scoring,2015,14,16,3,0,6885,keisuke sakaguchi,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"A major opportunity for NLP to have a realworld impact is in helping educators score student writing, particularly content-based writing (i.e., the task of automated short answer scoring). A major challenge in this enterprise is that scored responses to a particular question (i.e., labeled data) are valuable for modeling but limited in quantity. Additional information from the scoring guidelines for humans, such as exemplars for each score level and descriptions of key concepts, can also be used. Here, we explore methods for integrating scoring guidelines and labeled responses, and we find that stacked generalization (Wolpert, 1992) improves performance, especially for small training sets."
W14-1810,An Explicit Feedback System for Preposition Errors based on {W}ikipedia Revisions,2014,28,5,1,1,16057,nitin madnani,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper presents a proof-of-concept tool for providing automated explicit feedback to language learners based on data mined from Wikipedia revisions. The tool takes a sentence with a grammatical error as input and displays a ranked list of corrections for that error along with evidence to support each correction choice. We use lexical and part-of-speech contexts, as well as query expansion with a thesaurus to automatically match the error with evidence from the Wikipedia revisions. We demonstrate that the tool works well for the task of preposition selection errors, evaluating against a publicly available corpus."
P14-2029,Predicting Grammaticality on an Ordinal Scale,2014,20,27,3,1,34075,michael heilman,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and find that our approach yields state-of-the-art performance."
P14-2041,Content Importance Models for Scoring Writing From Sources,2014,14,12,2,0.25874,19993,beata klebanov,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,Selection of information from external sources is an important skill assessed in educational measurement. We address an integrative summarization task used in an assessment of English proficiency for nonnative speakers applying to higher education institutions in the USA. We evaluate a variety of content importance models that help predict which parts of the source material should be selected by the test-taker in order to succeed on this task.
W13-1722,Automated Scoring of a Summary-Writing Task Designed to Measure Reading Comprehension,2013,17,16,1,1,16057,nitin madnani,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We introduce a cognitive framework for measuring reading comprehension that includes the use of novel summary writing tasks. We derive NLP features from the holistic rubric used to score the summaries written by students for such tasks and use them to design a preliminary, automated scoring system. Our results show that the automated approach performs well on summaries written by students for two different passages."
W13-1739,Detecting Missing Hyphens in Learner Text,2013,6,3,4,0,4873,aoife cahill,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present a method for automatically detecting missing hyphens in English text. Our method goes beyond a purely dictionary-based approach and also takes context into account. We evaluate our model on artificially generated data as well as naturally occurring learner text. Our best-performing model achieves high precision and reasonable recall, making it suitable for inclusion in a system that gives feedback to language learners."
S13-2046,{ETS}: Domain Adaptation and Stacking for Short Answer Scoring,2013,9,27,2,1,34075,michael heilman,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data (i.e., human-scored responses) may or may not be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge."
S13-1013,{HENRY}-{CORE}: Domain Adaptation and Stacking for Text Similarity,2013,17,5,2,1,34075,michael heilman,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"This paper describes a system for automatically measuring the semantic similarity between two texts, which was the aim of the 2013 Semantic Textual Similarity (STS) task (Agirre et al., 2013). For the 2012 STS task, Heilman and Madnani (2012) submitted the PERP system, which performed competitively in relation to other submissions. However, approaches including word and n-gram features also performed well (Bxc2xa8 ar et al., 2012;"
Q13-1009,Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data,2013,39,6,2,0.25874,19993,beata klebanov,Transactions of the Association for Computational Linguistics,0,We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15{\%} improvement in the accuracy of the seed lexicon on 3-way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7{\%} improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews).
P13-4025,{P}ara{Q}uery: Making Sense of Paraphrase Collections,2013,8,0,2,0,24131,lili kotlerman,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections. We present ParaQuery, a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection, analyze its utility for a particular domain, and compare it to other popular lexical similarity resources xe2x80x90 all within a single interface."
N13-1055,Robust Systems for Preposition Error Correction Using {W}ikipedia Revisions,2013,26,26,2,0,4873,aoife cahill,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. We present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections."
W12-2005,Exploring Grammatical Error Correction with Not-So-Crummy Machine Translation,2012,14,15,1,1,16057,nitin madnani,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"To date, most work in grammatical error correction has focused on targeting specific error types. We present a probe study into whether we can use round-trip translations obtained from Google Translate via 8 different pivot languages for whole-sentence grammatical error correction. We develop a novel alignment algorithm for combining multiple round-trip translations into a lattice using the TERp machine translation metric. We further implement six different methods for extracting whole-sentence corrections from the lattice. Our preliminary experiments yield fairly satisfactory results but leave significant room for improvement. Most importantly, though, they make it clear the methods we propose have strong potential and require further study."
S12-1076,{ETS}: Discriminative Edit Models for Paraphrase Scoring,2012,17,14,2,1,34075,michael heilman,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts. However, many systems that address these tasks focus on a single task and may or may not generalize well. In this work, we extend an existing machine translation metric, TERp (Snover et al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline."
N12-1003,Identifying High-Level Organizational Elements in Argumentative Discourse,2012,10,34,1,1,16057,nitin madnani,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Argumentative discourse contains not only language expressing claims and evidence, but also language used to organize these claims and pieces of evidence. Differentiating between the two may be useful for many applications, such as those that focus on the content (e.g., relation extraction) of arguments and those that focus on the structure of arguments (e.g., automated essay scoring). We propose an automated approach to detecting high-level organizational elements in argumentative discourse that combines a rule-based system and a probabilistic sequence model in a principled manner. We present quantitative results on a dataset of human-annotated persuasive essays, and qualitative analyses of performance on essays and on political debates."
N12-1019,Re-examining Machine Translation Metrics for Paraphrase Identification,2012,22,111,1,1,16057,nitin madnani,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years. We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community."
W11-2111,{E}-rating Machine Translation,2011,26,17,3,0,43862,kristen parton,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We describe our submissions to the WMT11 shared MT evaluation task: MTeRater and MTeRater-Plus. Both are machine-learned metrics that use features from e-raterxc2xae, an automated essay scoring engine designed to assess writing proficiency. Despite using only features from e-rater and without comparing to translations, MTeRater achieves a sentence-level correlation with human rankings equivalent to BLEU. Since MTeRater only assesses fluency, we build a meta-metric, MTeRater-Plus, that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics. This meta-metric has a higher correlation with human rankings than either MTeRater or individual MT metrics alone. However, we also find that e-rater features may not have significant impact on correlation in every case."
W11-0810,"The Web is not a {PERSON}, Berners-Lee is not an {ORGANIZATION}, and {A}frican-{A}mericans are not {LOCATIONS}: An Analysis of the Performance of Named-Entity Recognition",2011,12,6,3,0,44375,robert krovetz,Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,0,"Most work on evaluation of named-entity recognition has been done in the context of competitions, as a part of Information Extraction. There has been little work on any form of extrinsic evaluation, and how one tagger compares with another on the major classes: PERSON, ORGANIZATION, and LOCATION. We report on a comparison of three state-of-the-art named entity taggers: Stanford, LBJ, and IdentiFinder. The taggers were compared with respect to: 1) Agreement rate on the classification of entities by class, and 2) Percentage of ambiguous entities (belonging to more than one class) co-occurring in a document. We found that the agreement between the taggers ranged from 34% to 58%, depending on the class and that more than 40% of the globally ambiguous entities co-occur within the same document. We also propose a unit test based on the problems we encountered."
P11-2089,They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems,2011,24,24,1,1,16057,nitin madnani,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing."
W10-0730,Measuring Transitivity Using Untrained Annotators,2010,7,10,1,1,16057,nitin madnani,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,Hopper and Thompson (1980) defined a multi-axis theory of transitivity that goes beyond simple syntactic transitivity and captures how much action takes place in a sentence. Detecting these features requires a deep understanding of lexical semantics and real-world pragmatics. We propose two general approaches for creating a corpus of sentences labeled with respect to the Hopper-Thompson transitivity schema using Amazon Mechanical Turk. Both approaches assume no existing resources and incorporate all necessary annotation into a single system; this is done to allow for future generalization to other languages. The first task attempts to use language-neutral videos to elicit human-composed sentences with specified transitivity attributes. The second task uses an iterative process to first label the actors and objects in sentences and then annotate the sentences' transitivity. We examine the success of these techniques and perform a preliminary classification of the transitivity of held-out data.
N10-1041,Putting the User in the Loop: Interactive Maximal Marginal Relevance for Query-Focused Summarization,2010,6,16,2,0,888,jimmy lin,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,This work represents an initial attempt to move beyond single-shot summarization to interactive summarization. We present an extension to the classic Maximal Marginal Relevance (MMR) algorithm that places a user in the loop to assist in candidate selection. Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 show that interactively-constructed responses are significantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactive summarization.
J10-3003,Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods,2010,122,168,1,1,16057,nitin madnani,Computational Linguistics,0,"The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language-words, phrases, and sentences-is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation."
W09-0441,"Fluency, Adequacy, or {HTER}? {E}xploring Different Human Judgments with a Tunable {MT} Metric",2009,15,158,2,0,44205,matthew snover,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments."
W08-0209,Combining Open-Source with Research to Re-engineer a Hands-on Introductory {NLP} Course,2008,18,3,1,1,16057,nitin madnani,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"We describe our first attempts to re-engineer the curriculum of our introductory NLP course by using two important building blocks: (1) Access to an easy-to-learn programming language and framework to build hands-on programming assignments with real-world data and corpora and, (2) Incorporation of interesting ideas from recent NLP research publications into assignment and examination problems. We believe that these are extremely important components of a curriculum aimed at a diverse audience consisting primarily of first-year graduate students from both linguistics and computer science. Based on overwhelmingly positive student feedback, we find that our attempts were hugely successful."
2008.amta-papers.13,Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization,2008,-1,-1,1,1,16057,nitin madnani,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases."
W07-2312,Measuring Variability in Sentence Ordering for News Summarization,2007,17,23,1,1,16057,nitin madnani,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment."
W07-0716,Using Paraphrases for Parameter Tuning in Statistical Machine Translation,2007,25,70,1,1,16057,nitin madnani,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality."
H05-1098,"The {H}iero Machine Translation System: Extensions, Evaluation, and Analysis",2005,27,54,3,0,3180,david chiang,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems."
