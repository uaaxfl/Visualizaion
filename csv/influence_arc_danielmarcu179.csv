2003.mtsummit-systems.2,W02-1018,1,0.87931,"Missing"
2003.mtsummit-systems.2,P01-1050,1,\N,Missing
2004.iwslt-evaluation.9,N03-1017,1,0.0272244,"Missing"
2004.iwslt-evaluation.9,J04-4002,1,\N,Missing
A00-2002,J93-1004,0,0.0190444,"Missing"
A00-2002,P97-1062,0,0.027455,"elations, etc. Our goal is to learn these systematic discourse mapping rules and exploit them in a machine translation context. 3 Towards machine If for any tuple (Ts, Tt, C> such a sequence of actions can be derived, it is then possible to use a corpus of (Ts, Tt, C) tuples in order to automatically learn to derive from an unseen tree Ts,, which has the same structural properties as the trees Ts, a tree Ttj, which has structural properties similar to those of the trees Tt. In order to solve the problem in definition 3.1, we extend the shift-reduce parsing paradigm applied by Magerman (1995), Hermjakob and Mooney (1997), and MarcH (1999). In this extended paradigm, the transfer process starts with an e m p t y Stack and an Input List that contains a sequence of elementary discourse trees edts, one edt for each edu in the tree Ts given as input. The status and rhetorical relation associated with each edt is undefined. At each step, the transfer module applies an operation that is aimed at building from the units in T, the discourse tree Tt. In the context of our discourse-transfer module, we need 7 types of operations: • SHIFT operations transfer the first edt from the input list into the stack; a discourse-b"
A00-2002,C94-2183,0,0.0229105,"Missing"
A00-2002,P95-1037,0,0.00978947,"ions, 5 as LIST relations, etc. Our goal is to learn these systematic discourse mapping rules and exploit them in a machine translation context. 3 Towards machine If for any tuple (Ts, Tt, C> such a sequence of actions can be derived, it is then possible to use a corpus of (Ts, Tt, C) tuples in order to automatically learn to derive from an unseen tree Ts,, which has the same structural properties as the trees Ts, a tree Ttj, which has structural properties similar to those of the trees Tt. In order to solve the problem in definition 3.1, we extend the shift-reduce parsing paradigm applied by Magerman (1995), Hermjakob and Mooney (1997), and MarcH (1999). In this extended paradigm, the transfer process starts with an e m p t y Stack and an Input List that contains a sequence of elementary discourse trees edts, one edt for each edu in the tree Ts given as input. The status and rhetorical relation associated with each edt is undefined. At each step, the transfer module applies an operation that is aimed at building from the units in T, the discourse tree Tt. In the context of our discourse-transfer module, we need 7 types of operations: • SHIFT operations transfer the first edt from the input list"
A00-2002,P99-1047,1,0.736642,"Missing"
A00-2002,W99-0307,1,0.74555,"NOBJECT-ATTRIBUTE-E is 11o longer m a d e explicit in the English text. Some of the differences between the two discourse trees in Figure 1 have been traditionally addressed 10 Corpus k~ (#) k, (#) k,~ (#) k~ (#) Japanese English 0.856 (80) 0.925 (60) 0.785 (3377) 0.866 (1826) 0.724 (3377) 0.839 (1826) 0.650 (3377) 0.748 (1826) Table 1: Tagging reliability The tool and the annotation protocol are available at http://www.isi.edu/~marcu/software/. The annotation procedure yielded over the entire corpus 2641 Japanese edus and 2363 English edus. We computed the reliability of the annotation using Marcu et al. (1999)'s method for computing kappa statistics (Siegel and Castellan, 1988) over hierarchical structures. Table 1 displays average kappa statistics that reflect the reliability of the annotation of elementary discourse units, k~,, hierarchical discourse spans, ks, hierarchical nuclearity assignments, k,~, and hierarchical rhetorical relation assignments, k~. Kappa figures higher than 0.8 correspond to good agreement; kappa figures higher than 0.6 correspond to acceptable agreement. All kappa statistics were statistically significant at levels higher than a = 0.01. In addition to the kappa statistics"
A00-2002,H94-1024,0,0.106176,"Missing"
benjamin-etal-2002-translation,W99-0906,1,\N,Missing
benjamin-etal-2002-translation,J99-4005,1,\N,Missing
benjamin-etal-2002-translation,W98-1005,1,\N,Missing
benjamin-etal-2002-translation,W01-1409,0,\N,Missing
benjamin-etal-2002-translation,P02-1039,1,\N,Missing
benjamin-etal-2002-translation,P01-1067,1,\N,Missing
benjamin-etal-2002-translation,P01-1050,1,\N,Missing
benjamin-etal-2002-translation,knight-al-onaizan-1998-translation,1,\N,Missing
benjamin-etal-2002-translation,P97-1017,1,\N,Missing
benjamin-etal-2002-translation,W01-0504,1,\N,Missing
benjamin-etal-2002-translation,P01-1030,1,\N,Missing
C00-1031,P98-1011,0,0.122549,"Missing"
C00-1031,P98-1044,1,0.949232,"oach that assumes that all anaphors can be resolved intra-unit; Linear-1 models an approach that corresponds roughly to centering (Grosz et al., 1995). Linear-k is consistent with the assumptions that underlie most current anaphora resolution systems, which look back k units in order to resolve an anaphor. Discourse-VT-k models. In this class of models, LPAs include all the referential expressions found in the discourse unit under scrutiny and the k discourse units that hierarchically precede it. The units that hierarchically precede a given unit are determined according to Veins Theory (VT) (Cristea et al., 1998), which is described briefly below. 2.2 Veins Theory VT extends and formalizes the relation between discourse structure and reference proposed by Fox (1987). It identifies ”veins”, i.e., chains of elementary discourse units, over discourse structure trees that are built according to the requirements put forth in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). One of the conjectures of VT is that the vein expression of an elementary discourse unit provides a coherent ”abstract” of the discourse fragment that contains that unit. As an internally coherent discourse fragment, most of"
C00-1031,W98-1119,0,0.0489352,"Missing"
C00-1031,J86-3001,0,0.152384,"Missing"
C00-1031,J95-2003,0,0.699716,"tly (Hobbs, 1978; Lappin and Leass, 1994; Mitkov,  On leave from the Faculty of Computer Science, University “Al. I. Cuza” of Iasi. 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981; Grosz and Sidner, 1986; Grosz et al., 1995; Fox, 1987; Vonk et al., 1992; Azzam et al., 1998; Hitzeman and Poesio, 1998). These studies claim, on the one hand, that the use of referents in naturally occurring texts imposes constraints on the interpretation of discourse; and, on the other, that the structure of discourse constrains the LPAs to which anaphors can be resolved. The oddness of the situation can be explained by the fact that both groups seem prima facie to be right. Empirical experiments studies that employ linear techniques for determining the LPAs of anaphors report recall and precision anaphora resolution results in the"
C00-1031,P98-1090,0,0.0384449,"Missing"
C00-1031,W97-1307,0,0.0321218,"ce University of Southern California Los Angeles, CA, USA marcu@isi.edu Valentin Tablan Department of Computer Science University of Sheffield United Kingdom v.tablan@sheffield.ac.uk Abstract We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 1 Introduction Most current anaphora resolution systems implement a pipeline architecture with three modules (Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). 1. A C OLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it. 2. A F ILTER module eliminates referees incompatible with the anaphor from the LPA. 3. A P REFERENCE module determines the most likely antecedent on the basis of an ordering policy. In most cases, the C OLLECT module determines an LPA by enumerating all antecedents in a window of text that precedes the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge et al., 1998). This window can"
C00-1031,J94-4002,0,0.0316058,"itute and Department of Computer Science University of Southern California Los Angeles, CA, USA marcu@isi.edu Valentin Tablan Department of Computer Science University of Sheffield United Kingdom v.tablan@sheffield.ac.uk Abstract We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 1 Introduction Most current anaphora resolution systems implement a pipeline architecture with three modules (Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). 1. A C OLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it. 2. A F ILTER module eliminates referees incompatible with the anaphor from the LPA. 3. A P REFERENCE module determines the most likely antecedent on the basis of an ordering policy. In most cases, the C OLLECT module determines an LPA by enumerating all antecedents in a window of text that precedes the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge"
C00-1031,W99-0307,1,0.81955,"inks of the REs in the other units, the full equivalence class can be determined. This is consistent with the distinction between ”direct” and ”indirect” references discussed by Cristea, et al.(1998). 3 The Experiment 3.1 Materials We used thirty newspaper texts whose lengths varied widely; the mean  is 408 words and the standard deviation  is 376. The texts were annotated manually for co-reference relations of identity (Hirschman and Chinchor, 1997). The co-reference relations define equivalence classes on the set of all marked referents in a text. The texts were also manually annotated by Marcu et al. (1999) with discourse structures built in the style of Mann and Thompson (1988). Each discourse analysis yielded an average of 52 elementary discourse units. See (Hirschman and Chinchor, 1997) and (Marcu et al., 1999) for details of the annotation processes. Figure 2: The RST analysis of the text in figure 1. The tree is represented using the conventions proposed by Mann and Thompson (1988). 3.2 Comparing potential to establish co-referential links 3.2.1 Method The annotations for co-reference relations and rhetorical structure trees for the thirty texts were fused, yielding representations that ref"
C00-1031,W97-1303,0,0.242773,"Computer Science University of Southern California Los Angeles, CA, USA marcu@isi.edu Valentin Tablan Department of Computer Science University of Sheffield United Kingdom v.tablan@sheffield.ac.uk Abstract We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure. 1 Introduction Most current anaphora resolution systems implement a pipeline architecture with three modules (Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). 1. A C OLLECT module determines a list of potential antecedents (LPA) for each anaphor (pronoun, definite noun, proper name, etc.) that have the potential to resolve it. 2. A F ILTER module eliminates referees incompatible with the anaphor from the LPA. 3. A P REFERENCE module determines the most likely antecedent on the basis of an ordering policy. In most cases, the C OLLECT module determines an LPA by enumerating all antecedents in a window of text that precedes the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge et al., 1998)"
C00-1031,J81-4001,0,0.520573,"Missing"
C00-1031,C98-1044,1,\N,Missing
C00-1031,C98-1087,0,\N,Missing
C00-1031,C98-1011,0,\N,Missing
C00-1076,J98-4001,0,0.0253546,"ntions that one can associate to the whole text. One intention is that discussed above, which is associated with analysis 1.a. Another intention depends on unit B1 and the JUSTIFICATION relation that holds between units A1 and B1 ; this intention is associated with the analyses shown in figure 1.c and 1.e. And another intention depends on unit B1 and the JUSTIFICATION relation that holds between units D1 and B1 ; this intention is associated with the analyses shown in figure 1.b and 1.d. Reasoning from text structures to intentions can be also beneficial in a context such as that described by Lochbaum (1998) because the rhetorical constraints can help prune the space of shared plans that would characterize an intentional interpretation of a discourse. Using intentions for managing rhetorical ambiguities. Assume now that besides providing judgments concerning the rhetorical relations that hold between various units, an analyst (or a program) provides judgments of intentions as well. If, for example, besides the relations given in (2) a program determines that the DSP of span [ A 1 ; D 1 ] dominates the DSP of unit D 1 , the theory that corresponds to these judgments and the axioms given in section"
C00-1076,J92-4007,0,0.171388,"-D1 A1-C1 c) D1 JUSTIFICATION A1 B1-C1 B1-C1 A1 EVIDENCE EVIDENCE B1 B1 C1 d) D1 C1 e) Figure 1: The set of all RS-trees that can be built for text (1). the situation presented in C1 will increase the reader’s negative regard for the situation presented in C1 ; and the situation presented in D1 is a restatement of the situation presented in A1 . Marcu (1996) has shown that on the basis of only the rhetorical judgments in (2) and without considering intentions, there are five valid RS-trees that one can build for text (1) (see figure 1). What happens though when we consider intentions as well? Moore and Pollack (1992) have already shown that different high-level intentions yield different RS-trees. But how do we formalize the relationship between intentions and rhetorical structures? For example, how can we use the discourse trees in figure 1 in order to determine the primary intention associated with each analysis? And how can we determine what would be the corresponding dominance relations in a GST account of the same text? Consider also a slightly different problem: assume that besides rhetorical judgments, such as those shown in (2), one can also make intentional judgments. For example, assume that one"
C00-1076,J96-3006,0,0.300786,"in (2), one can also make intentional judgments. For example, assume that one is interested in an interpretation in which one knows that the DSP of segment [A1 ; D1 ], which contains all units from A1 to D1, dominates the DSP of segment [C1 ; D1 ]. Then what is the primary intention of the text in that case? And how many discourse trees are both valid and consistent with that intentional judgment? Neither RST nor GST can answer these questions on their own. However, a unified theory can. In this paper, we provide such a theory. 2 The limits of Moser and Moore’s approach In a recent proposal, Moser and Moore (1996) argued that the primary intentions in a GST representation can be derived from the nuclei of the corresponding RST representation. Although their proposal is consistent with the cases in which each textual span is characterized by an explicit nucleus that encodes the primary intention of that span (as in the case of text (1)), it seems that an adequate account of the correspondence between GST and RST is somewhat more complicated. For example, in the case of text (3) below, whose RST analysis is shown in figure 2, we cannot apply Moser and Moore’s approach because we can associate the primary"
C00-1076,J86-3001,0,\N,Missing
D07-1006,J00-1004,0,0.0127509,"ative model. (Zens et al., 2004) introduced a model featuring a symmetrized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004). In contrast with their approaches, we have a very flat, one-level notion of dependency, which is bilingually motivated and learned automatically from the parallel corpus. This idea of dependency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature functions. These feat"
D07-1006,N06-1013,0,0.0262688,"involving the combination of heuristically derived feature functions. These feature functions generally include the prediction of some type of generative model, such as the HMM model or Model 4. A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005). (Lacoste-Julien et al., 2006) created a discriminative model able to model 1-to-1, 1-to2 and 2-to-1 alignments for which the best results were obtained using features based on symmetric HMMs trained to agree, (Liang et al., 2006), and 58 intersected Model 4. (Ayan and Dorr, 2006) defined a discriminative model which learns how to combine the predictions of several alignment algorithms. The experiments performed included Model 4 and the HMM extensions of (Lopez and Resnik, 2005). (Moore et al., 2006) introduced a discriminative model of 1-to-N and M-to-1 alignments, and similarly to (Lacoste-Julien et al., 2006) the best results were obtained using HMMs trained to agree and intersected Model 4. LEAF is not bound by the structural restrictions present either directly in these models, or in the features derived from the generative models used. We also iterate the generat"
D07-1006,J93-2003,0,0.0754534,"−1 and deterministically set ψi = χi . We can also specialize our generative story to the consecutive word M-to-N alignments used in “phrase-based” models, though in this case the conditioning of the generation decisions would be quite different. This involves adding checks on source and target connection geometry to the generative story which, if violated, would return “failure”; naturally this is at the cost of additional deficiency. 2.2 Unsupervised Parameter Estimation We can perform maximum likelihood estimation of the parameters of this model in a similar fashion 54 to that of Model 4 (Brown et al., 1993), described thoroughly in (Och and Ney, 2003). We use Viterbi training (Brown et al., 1993) but neighborhood estimation (Al-Onaizan et al., 1999; Och and Ney, 2003) or “pegging” (Brown et al., 1993) could also be used. To initialize the parameters of the generative model for the first iteration, we use bootstrapping from a 1-to-N and a M-to-1 alignment. We use the intersection of the 1-to-N and M-to-1 alignments to establish the head word relationship, the 1-to-N alignment to delineate the target word cepts, and the M-to-1 alignment to delineate the source word cepts. In bootstrapping, a probl"
D07-1006,P03-1012,0,0.0160584,"odel featuring a symmetrized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004). In contrast with their approaches, we have a very flat, one-level notion of dependency, which is bilingually motivated and learned automatically from the parallel corpus. This idea of dependency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature functions. These feature functions generally include the prediction"
D07-1006,P05-1033,0,0.0534291,"IZA++ Model 4. We believe that the features in LEAF are too high dimensional to use for the Arabic/English task without the backoffs available in the semi-supervised models. The baseline semi-supervised system (line 3) was run for three iterations and the resulting alignments were combined with the “union” heuristic. We ran the LEAF semi-supervised system for two iterations. The best result is the LEAF semi-supervised system (line 4), with a gain of 5.4 F-Measure over the baseline semi-supervised system. For Arabic/English translation we train a state of the art hierarchical model similar to (Chiang, 2005) using our Viterbi alignments. The translation test data used is described in Table 2. We use two trigram language models, one built using the English portion of the training data and the other built using additional English news data. The test set is from the NIST 2005 translation task. LEAF had the best performance scoring 1.43 BLEU better than the baseline semi-supervised system, which is statistically significant. 57 5 Previous Work The LEAF model is inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4 (Brown et al., 1993). Much of th"
D07-1006,H05-1022,0,0.291779,"5 translation task. LEAF had the best performance scoring 1.43 BLEU better than the baseline semi-supervised system, which is statistically significant. 57 5 Previous Work The LEAF model is inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4 (Brown et al., 1993). Much of the additional work on generative modeling of 1to-N word alignments is based on the HMM model (Vogel et al., 1996). (Toutanova et al., 2002) and (Lopez and Resnik, 2005) presented a variety of refinements of the HMM model particularly effective for low data conditions. (Deng and Byrne, 2005) described work on extending the HMM model using a bigram formulation to generate 1-to-N alignment structure. The common thread connecting these works is their reliance on the 1-to-N approximation, while we have defined a generative model which does not require use of this approximation, at the cost of having to rely on local search. There has also been work on generative models for other alignment structures. (Wang and Waibel, 1998) introduced a generative story based on extension of the generative story of Model 4. The alignment structure modeled was “consecutive M to non-consecutive N”. (Ma"
D07-1006,P06-1097,1,0.924217,"gmax a X λm hm (f, a, e) (7) m We decompose the new generative model presented in Section 2 in both translation directions to provide the initial feature functions for our loglinear model, features 1 to 10 and 16 to 25 in Table 1. We use backoffs for the translation decisions (features 11 and 26 and the HMM translation tables which are features 12 and 27) and the target cept size distributions (features 13, 14, 28 and 29 in Table 1), as well as heuristics which directly control the number of unaligned words we generate (features 15 and 30 in Table 1). We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. The initial M-step bootstraps parameters as described in Section 2.2 from a M-to-1 and a 1-to-N alignment. We then perform the D-step following (Fraser and 55 A@ @ D B nC nn~n~~ @@ n n @ nn ~ n@n@n ~~~ nnn E A@ @ D B nC nn~n~~ @@ n n @ nn ~ n@n@n ~~~ nnn E Figure 2: Two alignments with the same translational correspondence Marcu, 2006b). Given the feature function parameters estimated in the M-step and the feature function weights λ determined in the D-step, the E-step searches for the Viterbi alignment for the full training corpus. We use 1 − F-Measure as our error crit"
D07-1006,H05-1012,0,0.245971,"arallel corpus. This idea of dependency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature functions. These feature functions generally include the prediction of some type of generative model, such as the HMM model or Model 4. A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005). (Lacoste-Julien et al., 2006) created a discriminative model able to model 1-to-1, 1-to2 and 2-to-1 alignments for which the best results were obtained using features based on symmetric HMMs trained to agree, (Liang et al., 2006), and 58 intersected Model 4. (Ayan and Dorr, 2006) defined a discriminative model which learns how to combine the predictions of several alignment algorithms. The experiments performed included Model 4 and the HMM extensions of (Lopez and Resnik, 2005). (Moore et al., 2006) introduced a discriminative model of 1-to-N and M-to-1 alignments, and similarly to (Lacoste-"
D07-1006,J99-4005,0,0.0357066,"lignment structure where, for instance, a source word generates target words but no link between any of the target words and the source word appears in the intersection, so it is not clear which target word is the target head word. To address this, we consider each of the N generated target words as the target head word in turn and assign this configuration 1/N of the counts. For each iteration of training we search for the Viterbi solution for millions of sentences. Evidence that inference over the space of all possible alignments is intractable has been presented, for a similar problem, in (Knight, 1999). Unlike phrasebased SMT, left-to-right hypothesis extension using a beam decoder is unlikely to be effective because in word alignment reordering is not limited to a small local window and so the necessary beam would be very large. We are not aware of admissible or inadmissible search heuristics which have been shown to be effective when used in conjunction with a search algorithm similar to A* search for a model predicting over a structure like ours. Therefore we use a simple local search algorithm which operates on complete hypotheses. (Brown et al., 1993) defined two local search operation"
D07-1006,N03-1017,1,0.191664,"covered a methodological problem with our baseline systems, which is that two alignments which have the same translational correspondence can have different F-Measures. An example is shown in Figure 2. To overcome this problem we fully interlinked the transitive closure of the undirected bigraph formed by each alignment hypothesized by our baseline alignment systems1 . This operation maps the alignment shown to the left in Figure 2 to the alignment shown to the right. This operation does not change the collection of phrases or rules extracted from a hypothesized alignment, see, for instance, (Koehn et al., 2003). Working with this fully interlinked representation we found that the best settings of α were α = 0.1 for the Arabic/English task and α = 0.4 for the French/English task. 4 Experiments 4.1 Data Sets We perform experiments on two large alignments tasks, for Arabic/English and French/English data sets. Statistics for these sets are shown in Table 2. All of the data used is available from the Linguistic Data Consortium except for the French/English 1 All of the gold standard alignments were fully interlinked as distributed. We did not modify the gold standard alignments. 1 chi(χi |ei ) source wo"
D07-1006,N06-1015,0,0.221337,"endency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature functions. These feature functions generally include the prediction of some type of generative model, such as the HMM model or Model 4. A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005). (Lacoste-Julien et al., 2006) created a discriminative model able to model 1-to-1, 1-to2 and 2-to-1 alignments for which the best results were obtained using features based on symmetric HMMs trained to agree, (Liang et al., 2006), and 58 intersected Model 4. (Ayan and Dorr, 2006) defined a discriminative model which learns how to combine the predictions of several alignment algorithms. The experiments performed included Model 4 and the HMM extensions of (Lopez and Resnik, 2005). (Moore et al., 2006) introduced a discriminative model of 1-to-N and M-to-1 alignments, and similarly to (Lacoste-Julien et al., 2006) the best r"
D07-1006,N06-1014,0,0.37423,"uristic symmetrizaS YSTEM GIZA++ (F RASER AND M ARCU , 2006 B ) LEAF UNSUPERVISED LEAF SEMI - SUPERVISED F RENCH /E NGLISH F-M EASURE (α = 0.4) BLEU 73.5 30.63 74.1 31.40 74.5 76.3 31.86 A RABIC /E NGLISH F- MEASURE (α = 0.1) BLEU 75.8 51.55 79.1 52.89 72.3 84.5 54.34 Table 3: Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment, this was extended in (Koehn et al., 2003). We have used insights from these works to help determine the structure of our generative model. (Zens et al., 2004) introduced a model featuring a symmetrized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004). In contrast"
D07-1006,P05-1057,0,0.0961153,"al., 2006) the best results were obtained using HMMs trained to agree and intersected Model 4. LEAF is not bound by the structural restrictions present either directly in these models, or in the features derived from the generative models used. We also iterate the generative/discriminative process, which allows the discriminative predictions to influence the generative model. Our work is most similar to work using discriminative log-linear models for alignment, which is similar to discriminative log-linear models used for the SMT decoding (translation) problem (Och and Ney, 2002; Och, 2003). (Liu et al., 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. (Fraser and Marcu, 2006b) described symmetrized training of a 1-toN log-linear model and a M-to-1 log-linear model. These models took advantage of features derived from both training directions, similar to the symmetrized lexicons of (Zens et al., 2004), including features derived from the HMM model and Model 4. However, despite the symmetric lexicons, these models were only able to optimize the performance of the 1-to-N model and the M-to-1 model separat"
D07-1006,W05-0812,0,0.110907,"nglish portion of the training data and the other built using additional English news data. The test set is from the NIST 2005 translation task. LEAF had the best performance scoring 1.43 BLEU better than the baseline semi-supervised system, which is statistically significant. 57 5 Previous Work The LEAF model is inspired by the literature on generative modeling for statistical word alignment and particularly by Model 4 (Brown et al., 1993). Much of the additional work on generative modeling of 1to-N word alignments is based on the HMM model (Vogel et al., 1996). (Toutanova et al., 2002) and (Lopez and Resnik, 2005) presented a variety of refinements of the HMM model particularly effective for low data conditions. (Deng and Byrne, 2005) described work on extending the HMM model using a bigram formulation to generate 1-to-N alignment structure. The common thread connecting these works is their reliance on the 1-to-N approximation, while we have defined a generative model which does not require use of this approximation, at the cost of having to rely on local search. There has also been work on generative models for other alignment structures. (Wang and Waibel, 1998) introduced a generative story based on"
D07-1006,W02-1018,1,0.955473,"is terminated when no operation results in an improvement. (Och and Ney, 2003) discussed efficient implementation. In our model, because the alignment structure is richer, we define the following operations: move French non-head word to new head, move English non-head word to new head, swap heads of two French non-head words, swap heads of two English non-head words, swap English head word links of two French head words, link English word to French word making new head words, unlink English and French head words. We use multiple restarts to try to reduce search errors. (Germann et al., 2004; Marcu and Wong, 2002) have some similar operations without the head word distinction. 3 Semi-supervised parameter estimation Equation 6 defines a log-linear model. Each feature function hm has an associated weight λm . Given a vector of these weights λ, the alignment search problem, i.e. the search to return the best alignment a ˆ of the sentences e and f according to the model, is specified by Equation 7. P exp( m λm hm (a, e, f )) P pλ (f, a|e) = P 0 0 a0 ,f 0 exp( m λm hm (a , e, f )) (6) a ˆ = argmax a X λm hm (f, a, e) (7) m We decompose the new generative model presented in Section 2 in both translation dire"
D07-1006,C04-1032,0,0.0448064,"nt structure. The common thread connecting these works is their reliance on the 1-to-N approximation, while we have defined a generative model which does not require use of this approximation, at the cost of having to rely on local search. There has also been work on generative models for other alignment structures. (Wang and Waibel, 1998) introduced a generative story based on extension of the generative story of Model 4. The alignment structure modeled was “consecutive M to non-consecutive N”. (Marcu and Wong, 2002) defined the Joint model, which modeled consecutive word M-to-N alignments. (Matusov et al., 2004) presented a model capable of modeling 1-toN and M-to-1 alignments (but not arbitrary M-toN alignments) which was bootstrapped from Model 4. LEAF directly models non-consecutive M-to-N alignments. One important aspect of LEAF is its symmetry. (Och and Ney, 2003) invented heuristic symmetrizaS YSTEM GIZA++ (F RASER AND M ARCU , 2006 B ) LEAF UNSUPERVISED LEAF SEMI - SUPERVISED F RENCH /E NGLISH F-M EASURE (α = 0.4) BLEU 73.5 30.63 74.1 31.40 74.5 76.3 31.86 A RABIC /E NGLISH F- MEASURE (α = 0.1) BLEU 75.8 51.55 79.1 52.89 72.3 84.5 54.34 Table 3: Experimental Results tion of the output of a 1-t"
D07-1006,P06-1065,0,0.223701,"o-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005). (Lacoste-Julien et al., 2006) created a discriminative model able to model 1-to-1, 1-to2 and 2-to-1 alignments for which the best results were obtained using features based on symmetric HMMs trained to agree, (Liang et al., 2006), and 58 intersected Model 4. (Ayan and Dorr, 2006) defined a discriminative model which learns how to combine the predictions of several alignment algorithms. The experiments performed included Model 4 and the HMM extensions of (Lopez and Resnik, 2005). (Moore et al., 2006) introduced a discriminative model of 1-to-N and M-to-1 alignments, and similarly to (Lacoste-Julien et al., 2006) the best results were obtained using HMMs trained to agree and intersected Model 4. LEAF is not bound by the structural restrictions present either directly in these models, or in the features derived from the generative models used. We also iterate the generative/discriminative process, which allows the discriminative predictions to influence the generative model. Our work is most similar to work using discriminative log-linear models for alignment, which is similar to discrimina"
D07-1006,P02-1038,0,0.0451781,"similarly to (Lacoste-Julien et al., 2006) the best results were obtained using HMMs trained to agree and intersected Model 4. LEAF is not bound by the structural restrictions present either directly in these models, or in the features derived from the generative models used. We also iterate the generative/discriminative process, which allows the discriminative predictions to influence the generative model. Our work is most similar to work using discriminative log-linear models for alignment, which is similar to discriminative log-linear models used for the SMT decoding (translation) problem (Och and Ney, 2002; Och, 2003). (Liu et al., 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. (Fraser and Marcu, 2006b) described symmetrized training of a 1-toN log-linear model and a M-to-1 log-linear model. These models took advantage of features derived from both training directions, similar to the symmetrized lexicons of (Zens et al., 2004), including features derived from the HMM model and Model 4. However, despite the symmetric lexicons, these models were only able to optimize the performance of the 1-to-N mo"
D07-1006,J03-1002,0,0.41614,"tion l Y p(f, a|e) =[ g(χi |ei )] i=1 l Y [ i=1 l Y [ δ(χi , −1)w−1 (µi − i|classe (ei ))] δ(χi , 1)t1 (τi1 |ei )] i=1 l Y [ δ(χi , 1)s(ψi |ei , γi )] let f be the string f πik = τik i=1 We note that the steps which return “failure” are required because the model is deficient. Deficiency means that a portion of the probability mass in the model is allocated towards generative stories which would result in infeasible alignment structures. Our model has deficiency in the non-spurious target word placement, just as Model 4 does. It has additional deficiency in the source word linking decisions. (Och and Ney, 2003) presented results suggesting that the additional parameters required to ensure that a model is not deficient result in inferior performance, but we plan to study whether this is the case for our generative model in future work. Given e, f and a candidate alignment a, which represents both the links between source and target head-words and the head-word connections of the non-head words, we would like to calculate p(f, a|e). The formula for this is: 53 [s0 (ψ0 | l X ψi )] i=1 [ ψ0 Y t0 (τ0k )] k=1 ψi l Y Y t&gt;1 (τik |ei , classh (τi1 ))] [ i=1 k=2 ψi l Y Y [ Dik (πik )] i=1 k=1 where: δ(i, i0 )"
D07-1006,J04-4002,0,0.109529,"est symmetrization heuristic for this system was “union”, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. We observe that LEAF unsupervised (line 3) is competitive with GIZA++ (line 1), and is in fact competitive with the baseline semi-supervised result (line 2). We ran the LEAF semi-supervised system for two iterations (line 4). The best result is the LEAF semi-supervised system, with a gain of 1.8 F-Measure over the LEAF unsupervised system. For French/English translation we use a state of the art phrase-based MT system similar to (Och and Ney, 2004; Koehn et al., 2003). The translation test data is described in Table 2. We use two trigram language models, one built using the English portion of the training data and the other built using additional English news data. The BLEU scores reported in this work are calculated using lowercased and tokenized data. For semi-supervised LEAF the gain of 0.46 BLEU over the semi-supervised baseline is not statistically significant (a gain of 0.78 BLEU would be required), but LEAF semi-supervised compared with GIZA++ is significant, with a gain of 1.23 BLEU. We note that this shows a large gain in tran"
D07-1006,P03-1021,0,0.0606469,"te-Julien et al., 2006) the best results were obtained using HMMs trained to agree and intersected Model 4. LEAF is not bound by the structural restrictions present either directly in these models, or in the features derived from the generative models used. We also iterate the generative/discriminative process, which allows the discriminative predictions to influence the generative model. Our work is most similar to work using discriminative log-linear models for alignment, which is similar to discriminative log-linear models used for the SMT decoding (translation) problem (Och and Ney, 2002; Och, 2003). (Liu et al., 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. (Fraser and Marcu, 2006b) described symmetrized training of a 1-toN log-linear model and a M-to-1 log-linear model. These models took advantage of features derived from both training directions, similar to the symmetrized lexicons of (Zens et al., 2004), including features derived from the HMM model and Model 4. However, despite the symmetric lexicons, these models were only able to optimize the performance of the 1-to-N model and the"
D07-1006,W02-1012,0,0.0197476,"Missing"
D07-1006,C96-2141,0,0.986805,"eft-most target non-head word d&gt;2 (4j|classf (fj )) movement for subsequent target non-head words t(fj |ei ) translation without dependency on word-type t(fj |ei ) translation table from final HMM iteration s(ψi |γi ) target cept size without dependency on source head word e s(ψi |ei ) target cept size without dependency on γi target spurious word penalty (same features, other direction) Table 1: Feature functions gold standard alignments which are available from the authors. 4.2 Experiments To build all alignment systems, we start with 5 iterations of Model 1 followed by 4 iterations of HMM (Vogel et al., 1996), as implemented in GIZA++ (Och and Ney, 2003). For all non-LEAF systems, we take the best performing of the “union”, “refined” and “intersection” symmetrization heuristics (Och and Ney, 2003) to combine the 1-to-N and M-to-1 directions resulting in a M-to-N alignment. Because these systems do not output fully linked alignments, we fully link the resulting alignments as described at the end of Section 3. The reader should recall that this does not change the set of rules or phrases that can be extracted using the alignment. We perform one main comparison, which is of semi-supervised systems, w"
D07-1006,P98-2221,0,0.138288,", 1996). (Toutanova et al., 2002) and (Lopez and Resnik, 2005) presented a variety of refinements of the HMM model particularly effective for low data conditions. (Deng and Byrne, 2005) described work on extending the HMM model using a bigram formulation to generate 1-to-N alignment structure. The common thread connecting these works is their reliance on the 1-to-N approximation, while we have defined a generative model which does not require use of this approximation, at the cost of having to rely on local search. There has also been work on generative models for other alignment structures. (Wang and Waibel, 1998) introduced a generative story based on extension of the generative story of Model 4. The alignment structure modeled was “consecutive M to non-consecutive N”. (Marcu and Wong, 2002) defined the Joint model, which modeled consecutive word M-to-N alignments. (Matusov et al., 2004) presented a model capable of modeling 1-toN and M-to-1 alignments (but not arbitrary M-toN alignments) which was bootstrapped from Model 4. LEAF directly models non-consecutive M-to-N alignments. One important aspect of LEAF is its symmetry. (Och and Ney, 2003) invented heuristic symmetrizaS YSTEM GIZA++ (F RASER AND"
D07-1006,J97-3002,0,0.399103,"our generative model. (Zens et al., 2004) introduced a model featuring a symmetrized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004). In contrast with their approaches, we have a very flat, one-level notion of dependency, which is bilingually motivated and learned automatically from the parallel corpus. This idea of dependency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature"
D07-1006,P01-1067,0,0.0485432,"al., 2004) introduced a model featuring a symmetrized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004). In contrast with their approaches, we have a very flat, one-level notion of dependency, which is bilingually motivated and learned automatically from the parallel corpus. This idea of dependency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature functions. These feature functions generally i"
D07-1006,C04-1006,0,0.152164,"mportant aspect of LEAF is its symmetry. (Och and Ney, 2003) invented heuristic symmetrizaS YSTEM GIZA++ (F RASER AND M ARCU , 2006 B ) LEAF UNSUPERVISED LEAF SEMI - SUPERVISED F RENCH /E NGLISH F-M EASURE (α = 0.4) BLEU 73.5 30.63 74.1 31.40 74.5 76.3 31.86 A RABIC /E NGLISH F- MEASURE (α = 0.1) BLEU 75.8 51.55 79.1 52.89 72.3 84.5 54.34 Table 3: Experimental Results tion of the output of a 1-to-N model and a M-to-1 model resulting in a M-to-N alignment, this was extended in (Koehn et al., 2003). We have used insights from these works to help determine the structure of our generative model. (Zens et al., 2004) introduced a model featuring a symmetrized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and"
D07-1006,C04-1060,0,0.0184119,"trized lexicon. (Liang et al., 2006) showed how to train two HMM models, a 1-to-N model and a M-to-1 model, to agree in predicting all of the links generated, resulting in a 1-to-1 alignment with occasional rare 1to-N or M-to-1 links. We improve on these works by choosing a new structure for our generative model, the head word link structure, which is both symmetric and a robust structure for modeling of nonconsecutive M-to-N alignments. In designing LEAF, we were also inspired by dependency-based alignment models (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Cherry and Lin, 2003; Zhang and Gildea, 2004). In contrast with their approaches, we have a very flat, one-level notion of dependency, which is bilingually motivated and learned automatically from the parallel corpus. This idea of dependency has some similarity with hierarchical SMT models such as (Chiang, 2005). The discriminative component of our work is based on a plethora of recent literature. This literature generally views the discriminative modeling problem as a supervised problem involving the combination of heuristically derived feature functions. These feature functions generally include the prediction of some type of generativ"
D07-1006,J07-3002,1,\N,Missing
D07-1006,C98-2216,0,\N,Missing
D07-1006,P01-1030,1,\N,Missing
D07-1078,A00-2018,0,0.0483426,"t collapses the two NNP’s, so as to generalize this rule, getting rule R 5 and rule R6 . We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R 6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 746–754, Pra"
D07-1078,J07-2003,0,0.115537,"ization methods in terms of BLEU on Chinese-to-English translation tasks. 7.1 Experimental setup Our bitext consists of 16M words, all in the mainland-news domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes 752 Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We"
D07-1078,P97-1003,0,0.0992221,"w tree node that collapses the two NNP’s, so as to generalize this rule, getting rule R 5 and rule R6 . We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R 6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning,"
D07-1078,P03-2041,0,0.134446,"CA, 90292 {wwang,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translate"
D07-1078,N04-1035,1,0.126799,"mployed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree π that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. (1) unbinarized tree NPB NNP1 NNP2 NNP3 NNP4* viktor chernomyrdin VIKTOR−CHERNOMYRDIN (2) left-binarization (3) right-/hea"
D07-1078,P06-1121,1,0.147896,"ng,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase"
D07-1078,J99-4004,0,0.10699,"g all the way to the left, for example, from tree (1) to tree (2) and to tree (4) in Figure 2, does not enable us to acquire a substructure that yields NNP3 NNP4 and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 4.2 Parallel binarization Simple binarizations transform a parse tree into another single parse tree. Parallel binarization will transform a parse tree into a binarization forest, desirably packed to enable dynamic programming when extracting translation rules from it. Borrowing terms from parsing semirings (Goodman, 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗node corresponds to a tree node in the unbinarized tree; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node in the unbinarized tree and it contains one or more ⊗nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing. Figure 3 shows a packed forest obtained by packing trees (4) an"
D07-1078,N04-1014,1,0.65413,"1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phrase. Also suppose that we want to translate a Chinese phrase VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE into English. What we desire is that if we have another rule R2 as shown in Figure 1, we could somehow compose it with R1 to obtain the desirable translation. We unfortunately cannot do this because R1 and R2 are not further decomposable and their substructures cannot be re-used. The requirement that all translation rules have exactly one root node"
D07-1078,W06-1606,1,0.615337,"nguageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTO"
D07-1078,J93-2004,0,0.0333178,"style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phras"
D07-1078,P04-1084,0,0.0211306,"the binarization bias for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same lan747 guage and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, sy"
D07-1078,W05-0908,0,0.0127138,"nous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes 752 Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We see that all the binarization methods improve the baseline system that does not apply any binarization algorithm. The EM-binarization performs the best among all the restructuring methods, leading to 1.0 BLEU point improvement. We also computed the bootstrap p-values (Riezler and Maxwell, 2005) for the pairwise BLEU comparison between the baseline system and any of the system trained from binarized trees. The significance test shows that the EM binarization result is statistically significant better than the baseline system (p &gt; 0.005), even though the baseline is already quite strong. To our best knowledge, 37.94 is the highest BLEU score on this test set to date. Also as shown in Table 1, the grammars trained from the binarized training trees are almost two times of the grammar size with no binarization. The extra rules are substructures factored out by these binarization methods."
D07-1078,N06-1033,1,0.539983,"for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same lan747 guage and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars bu"
D07-1078,J02-1005,0,\N,Missing
D07-1078,J08-3004,1,\N,Missing
D07-1079,J93-2003,0,0.0123288,"ased machine translation model on several levels. We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and K"
D07-1079,P05-1033,0,0.253777,"on of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great nu"
D07-1079,J03-4003,0,0.00486268,"t linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables — there are no source words in them. Because of this, they potentially apply to any sentence. Lexical rules (their counterpart) far outnumb"
D07-1079,W06-1628,0,0.194461,"Missing"
D07-1079,P03-2041,0,0.113635,"accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a sci"
D07-1079,P06-1097,1,0.521845,"s |fwords ), and p(fwords |ewords ). 4 Differences in Phrasal Coverage Both the ATS model and the GHKM model extract linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables — there are no source words in th"
D07-1079,N04-1035,1,0.716228,"titution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phrase-based and syntax-based extraction methods and phrase pair coverage. We also add"
D07-1079,P06-1121,1,0.651314,"s/mt/ mt06eval official results.html 756 Phrase pairs are extracted over the entire training corpus. Due to differing alignments, some phrase pairs that cannot be learned from one example may be learned from another. These pairs are then counted, once for each time they are seen in a training example, and these counts are used as the basis for maximum likelihood probability features, such as p(f |e) and p(e|f ). 3 Syntax-based Extraction The GHKM syntax-based extraction method for learning statistical syntax-based translation rules, presented first in (Galley et al., 2004) and expanded on in (Galley et al., 2006), is similar to phrase-based extraction in that it extracts rules consistent with given word alignments. A primary difference is the use of syntax trees on the target side, rather than sequences of words. The basic unit of translation is the translation rule, consisting of a sequence of words and variables in the source language, a syntax tree in the target language having words or variables at the leaves, and again a vector of feature values which describe this pair’s likelihood. Translation rules can: • look like phrase pairs with syntax decoration: NPB(NNP(prime) NNP(minister) ↔ NNP(keizo)"
D07-1079,P03-1011,0,0.0154978,"he translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is impor"
D07-1079,2006.amta-papers.8,1,0.137531,"on. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phr"
D07-1079,N03-1017,1,0.0988731,"We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear"
D07-1079,W06-1606,1,0.648295,"them. Since this is a clear deficiency, we will focus on analyzing these phrase pairs (which we call ATS-useful) and the reasons they were not learned. Table 4 shows a breakdown, categorizing each of these missing ATS-useful phrase pairs and the reasons they were not able to be learned. The most common reason is straightforward: by extracting only the minimally-sized rules, GHKM is unable to learn many larger phrases that ATS learns. If GHKM can make a word-level analysis, it will do that, at the expense of a phrase-level analysis. Galley et al. (2006) propose one solution to this problem and Marcu et al. (2006) propose another, both of which we explore in Sections 5.1 and 5.2. context is too constrained. For example, ATS can easily learn the phrase ® ↔ prime minister and is then free to use it in many contexts. But GHKM learns 45 different rules, each that translate this phrase pair in a unique context. Figure 6 shows a sampling. Notice that though many variations are present, the decoder is unable to use any of these rules to produce certain noun phrases, such as “current Japanese Prime Minister Shinzo Abe”, because no rule has the proper number of English modifiers. NPB(NNP(prime) NNP(minister) x"
D07-1079,P04-1083,0,0.041989,"all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific underst"
D07-1079,J03-1002,0,0.00571114,"e , fwords |ehead ), p(ewords |fwords ), and p(fwords |ewords ). 4 Differences in Phrasal Coverage Both the ATS model and the GHKM model extract linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables"
D07-1079,J04-4002,0,0.085683,"each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these impor"
D07-1079,P03-1021,0,0.0125312,"rees, so that wide constituents are broken down into multiple levels of tree structure. The approach we take here is head-out binarization (Wang et al., 2007), where any constituent with more than two children is split into partial constituents. The children to the left of the head word Category of ATS-useful phrase pairs Too large Extra target words in GHKM rules Extra source words in GHKM rules Other (e.g. parse failures) Total missing useful phrase pairs Chinese 12 218 424 9 663 Arabic 9 27 792 7 835 with four references for measuring BLEU. Tuning was done using Maximum BLEU hill-climbing (Och, 2003). Features used for the ATS system were the standard set. For the syntax-based translation system, we used a similar set of features. Table 8: reasons that ATS-useful phrase pairs are still not extracted as phrasal rules, with composed and SPMT model 1 rules in place Development set Test set are binarized one direction, while the children to the right are binarized the other direction. The top node retains its original label (e.g. NPB), while the new partial constituents are labeled with a bar (e.g. NPB). Figure 7 shows an example. Figure 7: head-out binarization in the target language: S, NPB"
D07-1079,P05-1034,0,0.0730264,"al machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution i"
D07-1079,D07-1078,1,0.313261,"ot the smallest rule that can explain the phrase pair, but it is still valuable for its syntactic context. 5.3 Restructuring Trees Table 8 updates the causes of missing ATS-useful phrase pairs. Most are now caused by syntactic constraints, thus we need to address these in some way. GHKM translation rules are affected by large, flat constituents in syntax trees, as in the prime minister example earlier. One way to soften this constraint is to binarize the trees, so that wide constituents are broken down into multiple levels of tree structure. The approach we take here is head-out binarization (Wang et al., 2007), where any constituent with more than two children is split into partial constituents. The children to the left of the head word Category of ATS-useful phrase pairs Too large Extra target words in GHKM rules Extra source words in GHKM rules Other (e.g. parse failures) Total missing useful phrase pairs Chinese 12 218 424 9 663 Arabic 9 27 792 7 835 with four references for measuring BLEU. Tuning was done using Maximum BLEU hill-climbing (Och, 2003). Features used for the ATS system were the standard set. For the syntax-based translation system, we used a similar set of features. Table 8: reaso"
D07-1079,P98-2230,0,0.0514584,"antitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each h"
D07-1079,P01-1067,1,0.221445,"t al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phrase-based and syntax-based extraction methods and phrase pair"
D07-1079,N06-1033,1,0.126112,"-based extraction in that it extracts rules consistent with given word alignments. A primary difference is the use of syntax trees on the target side, rather than sequences of words. The basic unit of translation is the translation rule, consisting of a sequence of words and variables in the source language, a syntax tree in the target language having words or variables at the leaves, and again a vector of feature values which describe this pair’s likelihood. Translation rules can: • look like phrase pairs with syntax decoration: NPB(NNP(prime) NNP(minister) ↔ NNP(keizo) BÁÈ® D# NNP(obuchi)) (Zhang et al., 2006). During decoding, features from each translation rule are combined with a language model using a log-linear model to compute the score of the entire translation. The GHKM extractor learns translation rules from an aligned parallel corpus where the target side has been parsed. This corpus is conceptually a list of tuples of <source sentence, target tree, bi-directional word alignments&gt; which serve as training examples, one of which is shown in Figure 3. • carry extra contextual constraints: VP(VBD(said) x0 :SBAR-C) ↔ x 0  can translate to (according to this rule, said only if some Chinese se"
D07-1079,C98-2225,0,\N,Missing
D11-1046,N07-1051,0,\N,Missing
D11-1046,N04-1035,1,\N,Missing
D11-1046,N10-1015,0,\N,Missing
D11-1046,D10-1027,0,\N,Missing
D11-1046,J93-2003,0,\N,Missing
D11-1046,W02-1001,0,\N,Missing
D11-1046,W09-0424,0,\N,Missing
D11-1046,P10-1146,0,\N,Missing
D11-1046,P10-1017,1,\N,Missing
D11-1046,D07-1006,1,\N,Missing
D11-1046,P08-1108,0,\N,Missing
D11-1046,D08-1024,0,\N,Missing
D11-1046,P08-1064,0,\N,Missing
D11-1046,N10-1014,0,\N,Missing
D11-1046,P06-1065,0,\N,Missing
D11-1046,H05-1011,0,\N,Missing
D11-1046,D09-1106,0,\N,Missing
D11-1046,H05-1012,0,\N,Missing
D11-1046,P07-1003,0,\N,Missing
D11-1046,P09-1063,0,\N,Missing
D11-1046,P06-2014,0,\N,Missing
D11-1046,P10-1147,0,\N,Missing
D11-1046,N10-1069,0,\N,Missing
D11-1046,D09-1024,0,\N,Missing
D11-1046,W09-2303,0,\N,Missing
D11-1046,P05-1057,0,\N,Missing
D11-1046,N03-1017,1,\N,Missing
D11-1046,J03-1002,0,\N,Missing
D11-1046,J97-3002,0,\N,Missing
D11-1046,P09-1104,0,\N,Missing
D11-1046,W08-0306,0,\N,Missing
D11-1046,P06-1121,1,\N,Missing
D11-1046,W06-3119,0,\N,Missing
D11-1046,J07-2003,0,\N,Missing
D11-1046,D07-1080,0,\N,Missing
D11-1046,H05-1010,0,\N,Missing
D11-1046,N06-1015,0,\N,Missing
D11-1046,P06-1009,0,\N,Missing
D11-1046,2008.amta-papers.18,0,\N,Missing
D15-1136,P13-2009,0,0.017486,"ta that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in gene"
D15-1136,W13-2322,1,0.831018,"ng Representation Using Syntax-Based Machine Translation Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May Information Sciences Institute Computer Science Department University of Southern California {pust, ulf, knight, marcu, jonmay}@isi.edu Abstract polarit soldier 0 ARG die-01 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing ne"
D15-1136,J93-2003,0,0.0479423,"training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation in results. 3 https://github.com/jflanigan/jamr LDC2013E117, a pre-released ve"
D15-1136,P13-2131,1,0.81492,"tion that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section 5). 4. Integrating several semantic knowledge sources into the task (Section 6). 5. Developing tuning method"
D15-1136,A00-2018,0,0.379684,"Missing"
D15-1136,P96-1041,0,0.0575384,"e SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use Engli"
D15-1136,N09-1025,1,0.453031,"Missing"
D15-1136,P97-1003,0,0.0336152,"acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5 In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful label to characterize an instance and its roles. We initially choose the concept label, resul"
D15-1136,P05-1045,0,0.00345268,"semantic parsing, we have not yet discussed the use of any semantic resources. In this section we rectify that omission. 6.1 Rules from Numerical Quantities and Named Entities While the majority of string-to-tree rules in SBMT systems are extracted from aligned parallel data, it is common practice to dynamically generate additional rules to handle the translation of dates and numerical quantities, as these follow common patterns and are easily detected at decode-time. We follow this practice here, and additionally detect person names at decode-time using the Stanford Named Entity Recognizer (Finkel et al., 2005). We use cased, tokenized source data to build the decode-time rules. We add indicator features to these rules so that our tuning methods can decide how favorable the resources are. We leave as future work the incorporation of named-entity rules for other classes, since most available namedentity recognition beyond person names is at a granularity level that is incompatible with AMR (e.g. we can recognize ‘Location’ but not distinguish between ‘City’ and ‘Country’). 6.2 Hierarchical Semantic Categories fear-01 die-01 ARG1 * Figure 5: Final modification of the AMR data; semantically clustered p"
D15-1136,P14-1134,0,0.298361,"(AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1 ARG1 ins t t ins - 1 fear-01 y G 1 st in AR We present a parser for Abstract Meaning Representation (AMR). We treat E"
D15-1136,N04-1035,1,0.67464,"constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Sentences 10,313 1,368 1,371 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al.,"
D15-1136,P06-1121,1,0.857425,"ht changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. 2 Sentences 10,313 1,368 1,371 Syntax-Based Machine Translation Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the"
D15-1136,J02-3001,0,0.0353382,"sting machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR parsing, as fortunately, machine translation techn"
D15-1136,D10-1063,0,0.0114874,"nd a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic"
D15-1136,W14-3362,0,0.0118261,"re widely available, anyone wishing to generate AMR from text need only follow our recipe and retrain an existing framework with relevant data to quickly obtain state-of-the-art results. Since SBMT and AMR parsing are, in fact, distinct tasks, as outlined in Figure 2, to adapt the SBMT parsing framework to AMR parsing, we develop novel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization"
D15-1136,U11-1005,0,0.0154014,"KM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending that algorithm to handle word-to-node alignment. Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b). Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and relation finding (Bach and Badaskar, 2007). 10 Conclusion By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline semantic parser with little additional effort. By further restructuring our data to appropriately model the behavior we want to capture, we are able to rapi"
D15-1136,C12-1083,1,0.768754,"Missing"
D15-1136,H90-1020,0,0.139969,"s the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsup"
D15-1136,P12-1051,0,0.0106212,"parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM’s word-toterminal alignment requirement by extending tha"
D15-1136,N09-2036,1,0.774328,"ation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-totree rules is induced from training tuples using the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). 2. Local feature calculation: Statistical and indicator features, as described by Chiang et al. (2009), are calculated over the rule grammar. 3. Language model calculation: A KneserNey-interpolated 5-gram language model (Chen and Goodman, 1996) is learned from the yield of the target training trees. 4. Decoding: A beamed bottom-up chart decoder (Pust and Knight, 2009; Hopkins and Langmead, 2010) calculates the optimal derivations given a source string and feature parameter set. 5. Tuning: Feature parameters are optimized using the MIRA learning approach (Chiang et al., 2009) to maximize the objective, typically B LEU (Papineni et al., 2002), associated with a tuning corpus. We initially use this system with no modifications and pretend that English–AMR is a language pair indistinct from any other. 3 Data and Comparisons We use English–AMR data from the AMR 1.0 corpus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often use"
D15-1136,kingsbury-palmer-2002-treebank,0,0.217756,"ht, Daniel Marcu, Jonathan May Information Sciences Institute Computer Science Department University of Southern California {pust, ulf, knight, marcu, jonmay}@isi.edu Abstract polarit soldier 0 ARG die-01 The soldier was not afraid of dying. The soldier was not afraid to die. The soldier did not fear death. Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithm"
D15-1136,D07-1038,1,0.835975,"Missing"
D15-1136,P02-1040,0,0.0972587,"vel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representation that is suitable for string-to-tree SBMT rule extraction and decoding (Section 4.1). 2 LDC Catalog number 2014T12 See e.g. the related work section of Huck et al. (2014). 1143 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143–1154, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Target Nodes Edges Alignments SBMT tree labeled unlabeled words to leaves Children Accuracy Metric ordered B LEU (Papineni et al., 2002) AMR parsing graph unlabeled labeled words to leaves + words to edges unordered Smatch (Cai and Knight, 2013) Corpus Training Development Test Tokens 218,021 29,484 30,263 Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization. Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique that leverages the fact that child nodes in AMR are unordered (Section 4.4). 3. Introducing a hierarchical AMR-specific language model to ensure generation of likely parent-child relationships (Section"
D15-1136,D14-1048,1,0.817579,", 2004), the AMR corpus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English–AMR alignments by using the unsupervised alignment approach of Pourdamghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint. All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work’s JAMR software according to the provided instructions.3 We also compare our results to published scores in the recent work of Wang et al. (2015). Their work uses slightly different data than that used here 4 but in practice we have not seen significant variation i"
D15-1136,J10-2004,1,0.846218,"m this SBMT-compliant rewrite. 4.2 Tree Restructuring While the transformation in Figure 3c is acceptable to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good performance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules produced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity. We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more 5 In Figure 3 the negative polarity marker ‘-’ is a string. Disconnected referents labeled ‘*’ are treated as AMR instances with no roles. than three children (i.e. instances with more than one role), to allow generalization of flat structures. However, our trees are unlike syntactic constituent trees in that they do not have labeled nonterminal nodes, so we have no natural choice of an intermediate (“bar”) label. We must choose a meaningful labe"
D15-1136,N15-1040,0,0.404884,"lish renderings. Introduction Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic annotation (Banarescu et al., 2013). It includes entity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual entities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in numerous ways, e.g., by assigning the same conceptual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example. AMR parsing is a new research problem, with only a few papers published to date (Flanigan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 English/AMR pairs.1 New research problems can be tackled either by developing new algorithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algorithms/techniques to the problem at hand. In this paper, we investigate the second approach. The AMR parsing problem bears a strong formal resemblance to syntax-based machine translation (SBMT) of the string-to-tree variety, in that 1 ARG1 ins t t ins - 1 fear-01 y G 1 st in AR We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR convers"
D15-1136,N10-1000,0,0.0447018,"Missing"
D15-1136,P15-1095,0,0.503983,"Missing"
D15-1136,N06-1056,0,0.0542131,"labels and restructures to resolve discrepancies between dependency standards and AMR’s specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that"
D15-1136,P07-1121,0,0.024563,"ese works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modifying training data and adding lightweight AMRspecific features. Several other recent works have used a machine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery corpus (Zelle and Mooney, 1996). The WASP system of Wong and Mooney (2006) uses hierarchical SMT techniques and does not apply semanticspecific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reordering component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they perform a transformation of the input semantic representation so that it is amenable to use in an existing machine translation system. However, they are unable to reach the state of the art"
D15-1136,N03-1031,0,\N,Missing
H05-1013,N04-1001,0,\N,Missing
H05-1013,P05-1077,0,\N,Missing
H05-1013,P02-1014,0,\N,Missing
H05-1013,J01-4004,0,\N,Missing
H05-1013,P03-1001,0,\N,Missing
H05-2009,W04-1708,0,0.0607236,"Missing"
J00-3005,P94-1006,0,0.015365,"c pressure that caused the liquid water to evaporate, one needs to understand that without the information presented in the satellite, the reader may not know the particular CAUSE of the situation presented in the nucleus. In spite of the large number of discourse-related theories that have been proposed so far, there have emerged no algorithms capable of deriving the discourse structure of free, unrestricted texts. On one hand, the theories developed in the traditional, truth-based semantic perspective (Kamp 1981; Lascarides and Asher 1993; Asher 1993; Hobbs et al. 1993; Kamp and Reyle 1993; Asher and Lascarides 1994; Kameyama 396 Marcu Rhetorical Parsing of Unrestricted Texts 1994; Polanyi and van den Berg 1996; van den Berg 1996; Gardent 1997; Schilder 1997; Cristea and Webber 1997; Webber et al. 1999) take the position that discourse structures can be built only in conjunction with fully specified clause and sentence syntactic structures. These theories have a grammar as their backbone and rely on sophisticated logics of belief and default logics in order to intertwine and characterize the sentence- and discourse-based linguistic phenomena. Despite their formal elegance, implementations of these theori"
J00-3005,P99-1045,0,0.0183222,"ting. It is likely, for instance, that one m a y w a n t to systematically exclude from an abstract information that is s u b s u m e d b y the satellite of an EXAMPLE relation; to do so, it is necessary to identify correctly the relation. The rhetorical s u m m a r i z e r is a niche application that shows h o w an understanding of the hierarchical organization of text can make solving difficult natural language problems easier. Recent research has s h o w n that b y exploiting the structure of discourse, one can decrease storage space in information retrieval applications (CorstonOliver and Dolan 1999) and address discourse-specific problems in machine translation (Marcu, Carlson, and Watanabe, 2000). It is possible that discourse structures of the kinds derived b y this parser can have a positive impact on other problems as well. For example, Cristea et al. (1999) have s h o w n that a hierarchical m o d e l of discourse has a higher potential for i m p r o v i n g the performance of a coreference resolution system than a linear m o d e l of discourse. A n d Hirschman et al. (1999) have suggested that certain types of questions can be better answered if one has access to rhetorical structu"
J00-3005,W99-0106,1,0.720732,"m m a r i z e r is a niche application that shows h o w an understanding of the hierarchical organization of text can make solving difficult natural language problems easier. Recent research has s h o w n that b y exploiting the structure of discourse, one can decrease storage space in information retrieval applications (CorstonOliver and Dolan 1999) and address discourse-specific problems in machine translation (Marcu, Carlson, and Watanabe, 2000). It is possible that discourse structures of the kinds derived b y this parser can have a positive impact on other problems as well. For example, Cristea et al. (1999) have s h o w n that a hierarchical m o d e l of discourse has a higher potential for i m p r o v i n g the performance of a coreference resolution system than a linear m o d e l of discourse. A n d Hirschman et al. (1999) have suggested that certain types of questions can be better answered if one has access to rhetorical structure representations of the texts that contain the answers to the questions. H o w m u c h of an impact the rhetorical parser presented here can have on solving these problems, of course, remains an empirical question. Acknowledgments I am grateful to Graeme Hirst for t"
J00-3005,P97-1012,0,0.217582,"r CAUSE of the situation presented in the nucleus. In spite of the large number of discourse-related theories that have been proposed so far, there have emerged no algorithms capable of deriving the discourse structure of free, unrestricted texts. On one hand, the theories developed in the traditional, truth-based semantic perspective (Kamp 1981; Lascarides and Asher 1993; Asher 1993; Hobbs et al. 1993; Kamp and Reyle 1993; Asher and Lascarides 1994; Kameyama 396 Marcu Rhetorical Parsing of Unrestricted Texts 1994; Polanyi and van den Berg 1996; van den Berg 1996; Gardent 1997; Schilder 1997; Cristea and Webber 1997; Webber et al. 1999) take the position that discourse structures can be built only in conjunction with fully specified clause and sentence syntactic structures. These theories have a grammar as their backbone and rely on sophisticated logics of belief and default logics in order to intertwine and characterize the sentence- and discourse-based linguistic phenomena. Despite their formal elegance, implementations of these theories cannot yet handle naturally occurring texts, such as that shown in (1). On the other hand, the theories aimed at characterizing the constraints that pertain to the str"
J00-3005,P97-1011,0,0.0354618,"Missing"
J00-3005,J95-2003,0,0.0363837,"Missing"
J00-3005,J86-3001,0,0.608161,"ntactic structures. These theories have a grammar as their backbone and rely on sophisticated logics of belief and default logics in order to intertwine and characterize the sentence- and discourse-based linguistic phenomena. Despite their formal elegance, implementations of these theories cannot yet handle naturally occurring texts, such as that shown in (1). On the other hand, the theories aimed at characterizing the constraints that pertain to the structure of unrestricted texts and the computational mechanisms that would enable the derivation of these structures (van Dijk 1972; Zock 1985; Grosz and Sidner 1986; Mann and Thompson 1988; Polanyi 1988, 1996; Hobbs 1990) are either too informal or incompletely specified to support a fully automatic approach to discourse analysis. In this paper, I explore the ground found at the intersection of these two lines of research. More precisely, I explore the extent to which rhetorical structures of the kind shown in Figure 1 can be built automatically by relying only on cohesion and connectives, i.e., phrases such as for example, and, although, and however that are used &quot;to link linguistic units at any level&quot; (Crystal 1991, 74).1 The results show that although"
J00-3005,J97-1003,0,0.855283,"a discourse m a r k e r in unit i that a rhetorical relation R holds either b e t w e e n units i - 2 and i - 1 or b e t w e e n units i and i + 1, for example, such a hypothesis will be ill-formed. In the discourse analyses I have carried out so far, I have never come across an example that w o u l d require one to deal with exclusively disjunctive hypotheses different from that s h o w n graphically in Figure 2. 2.3 Determining Rhetorical Relations Using Cohesion 2.3.1 Pro Arguments. Youmans (1991), H o e y (1991), Morris and Hirst (1991), Salton et al. (1995), Salton and Allan (1995), and Hearst (1997) have s h o w n that w o r d cooccurrences and more sophisticated forms of lexical cohesion can be used to determine segments of topical and thematic continuity. A n d Morris and Hirst (1991) have also s h o w n that there is a correlation b e t w e e n cohesion-defined textual segments and hierarchical, intentionally defined segments (Grosz and Sidner 1986). For example, if the first three paragraphs of a text talk about the m o o n and the subsequent two paragraphs talk about the Earth, it is possible that the rhetorical structure of the text is characterized b y two spans that subsume these"
J00-3005,J93-3003,0,0.166195,"otation schema that I used in the study. In Section 4, I explain how the annotated data was used to derive algorithms that identify connective occurrences (Section 4.2), determine elementary units of discourse and determine which connectives have a discourse function (Section 4.3), and hypothesize rhetorical relations that hold between elementary units and spans of texts (Section 4.4). 405 Computational Linguistics Volume 26, Number 3 3.1 Materials Many researchers have published lists of potential discourse markers and cue phrases (Halliday and Hasan 1976; Grosz and Sidner 1986; Martin 1992; Hirschberg and Litman 1993; Knott 1995; Fraser 1996). I took the union of their lists and created an initial set of more than 450 potential discourse markers. For each potential discourse marker, I then used an automatic procedure that extracted from the Brown corpus a set of text fragments. Each text fragment contained a &quot;window&quot; of approximately 300 words and an emphasized occurrence of a cue phrase. My initial goal was to select for each cue phrase 10 texts in which the phrase was used at the beginning of a sentence and 20 texts in which the phrase was used in the middle of a sentence. (In a prestudy, I had noticed"
J00-3005,P99-1042,0,0.00679341,"iting the structure of discourse, one can decrease storage space in information retrieval applications (CorstonOliver and Dolan 1999) and address discourse-specific problems in machine translation (Marcu, Carlson, and Watanabe, 2000). It is possible that discourse structures of the kinds derived b y this parser can have a positive impact on other problems as well. For example, Cristea et al. (1999) have s h o w n that a hierarchical m o d e l of discourse has a higher potential for i m p r o v i n g the performance of a coreference resolution system than a linear m o d e l of discourse. A n d Hirschman et al. (1999) have suggested that certain types of questions can be better answered if one has access to rhetorical structure representations of the texts that contain the answers to the questions. H o w m u c h of an impact the rhetorical parser presented here can have on solving these problems, of course, remains an empirical question. Acknowledgments I am grateful to Graeme Hirst for the help and advice he gave me during every stage of this work; and to Marilyn Walker for suggestions that led to significant improvements, especially in the Evaluation section of the paper. I am also grateful to four anony"
J00-3005,C94-2183,0,0.0313,"its of texts. 6. R e l a t e d W o r k When this research was carried out, there was no rhetorical parser for English. However, very recently, Corston-Oliver (1998) has explored a different facet of the work described here and investigated the possibility of using syntactic information to hy443 Computational Linguistics Volume 26, Number 3 pothesize relations. His system uses 13 rhetorical relations and builds discourse trees for articles in Microsoft&apos;s Encarta 96 Encyclopedia. I believe that the research that comes closest to that described in this chapter is that of Sumita et al. (1992) and Kurohashi and Nagao (1994). Sumita et al. (1992) report on a discourse analyzer for Japanese, which differs from mine in a number of ways. Particularly important is the fact that the theoretical foundations of Sumita et al.&apos;s analyzer do not seem to be able to accommodate the ambiguity of discourse markers; in their system, discourse markers are considered unambiguous with respect to the relations that they signal. In contrast, my rhetorical parser uses a mathematical model in which this ambiguity is acknowledged and appropriately treated. Furthermore, the discourse trees that the rhetorical parser builds are more cons"
J00-3005,P97-1013,1,0.55944,"a sentence/paragraph, it not only signaled a CONCESSION relation between two clauses, but its use also correlated with an ELABORATION relation that held between two sentences or paragraphs. Overall, I have manually analyzed 2,100 of the text fragments in the corpus. I annotated only 2,100 fragments because the task was too time-consuming to complete. Of the 2,100 instances of cue phrases that I considered, 1,197 had a discourse function, 773 were sentential, and 244 were pragmatic. 4 The taxonomy of relations that I used to label the 1,197 discourse uses in the corpus contained 54 relations. Marcu (1997b) lists their names and the number of instances in which each rhetorical relation was used. The number of relations is much larger than 24, which is the size of the taxonomy proposed initially by Mann and Thompson (1988), because during the corpus analysis, it often happened that the relations proposed by Mann and Thompson seemed inadequate to capture the semantics of the relationship between the units under consideration. Because the study described here was exploratory, I considered it appropriate to introduce relations that would better capture the meaning of these relationships. The rheto"
J00-3005,P99-1047,1,0.701864,"Missing"
J00-3005,A00-2002,1,0.703814,"xclude from an abstract information that is s u b s u m e d b y the satellite of an EXAMPLE relation; to do so, it is necessary to identify correctly the relation. The rhetorical s u m m a r i z e r is a niche application that shows h o w an understanding of the hierarchical organization of text can make solving difficult natural language problems easier. Recent research has s h o w n that b y exploiting the structure of discourse, one can decrease storage space in information retrieval applications (CorstonOliver and Dolan 1999) and address discourse-specific problems in machine translation (Marcu, Carlson, and Watanabe, 2000). It is possible that discourse structures of the kinds derived b y this parser can have a positive impact on other problems as well. For example, Cristea et al. (1999) have s h o w n that a hierarchical m o d e l of discourse has a higher potential for i m p r o v i n g the performance of a coreference resolution system than a linear m o d e l of discourse. A n d Hirschman et al. (1999) have suggested that certain types of questions can be better answered if one has access to rhetorical structure representations of the texts that contain the answers to the questions. H o w m u c h of an impa"
J00-3005,J91-1002,0,0.172078,"a k e sense. Although one can hypothesize on the basis of the occurrence of a discourse m a r k e r in unit i that a rhetorical relation R holds either b e t w e e n units i - 2 and i - 1 or b e t w e e n units i and i + 1, for example, such a hypothesis will be ill-formed. In the discourse analyses I have carried out so far, I have never come across an example that w o u l d require one to deal with exclusively disjunctive hypotheses different from that s h o w n graphically in Figure 2. 2.3 Determining Rhetorical Relations Using Cohesion 2.3.1 Pro Arguments. Youmans (1991), H o e y (1991), Morris and Hirst (1991), Salton et al. (1995), Salton and Allan (1995), and Hearst (1997) have s h o w n that w o r d cooccurrences and more sophisticated forms of lexical cohesion can be used to determine segments of topical and thematic continuity. A n d Morris and Hirst (1991) have also s h o w n that there is a correlation b e t w e e n cohesion-defined textual segments and hierarchical, intentionally defined segments (Grosz and Sidner 1986). For example, if the first three paragraphs of a text talk about the m o o n and the subsequent two paragraphs talk about the Earth, it is possible that the rhetorical struc"
J00-3005,J97-2002,0,0.00593415,"ATCH-PAREN E n E NOTHING MATCH-DASH NOTHING B NOTHING sentence and p a r a g r a p h b o u n d a r i e s at places that are inappropriate. This simple algorithm correctly located all of the p a r a g r a p h b o u n d a r i e s and all b u t one of the sentence boundaries f o u n d in the texts that I used to evaluate the clause-like unit and discourse marker identification algorithm that I will present in Section 4.3.3. Other texts and semistructured H T M L / S G M L d o c u m e n t s m a y n e e d more sophisticated algorithms to solve this segmentation problem, such as those described b y Palmer and Hearst (1997). 4.3.3 The Clause-Like Unit and Discourse Marker Identification Algorithm. On the basis of the information derived from the corpus, I have designed an algorithm that identifies elementary textual unit boundaries in sentences and cue phrases that have a discourse function. Figure 6 shows only its skeleton and focuses on the variables and steps that are used to determine the elementary units. The steps that assert the discourse function of a m a r k e r are not shown; however, these steps are m e n t i o n e d in the discussion of the algorithm given below. Marcu (1997b) provides a full descrip"
J00-3005,J99-3001,0,0.0137182,"ities in the syntactic structure of sentences. As in the case of Sumita&apos;s system, Kurohashi and Nagao&apos;s system takes as input a sequence of parse trees; hence, in order to work, it must be preceded by a full syntactic analysis of the text. The elementary units of the discourse trees built by Kurohashi and Nagao are sentences. Since the systems developed by Corston-Oliver (1998), Sumita et al. (1992), and Kurohashi and Nagao (1994) were not evaluated intrinsically, it is difficult to compare the performance of their systems to ours. A parallel line of research has been investigated recently by Strube and Hahn (1999). They have extended the centering model proposed by Grosz, Joshi, and Weinstein (1995) by devising algorithms that build hierarchies of referential discourse segments. These hierarchies induce a discourse structure on text, which constrains the reachability of potential anaphoric antecedents. The referential segments are constructed through an incremental process that compares the centers of each sentence with those of the structure that has been built up to that point. The referential structures that are built by Hahn and Strube exploit a language facet different from that exploited by the r"
J00-3005,P99-1006,0,0.0130591,"presented in the nucleus. In spite of the large number of discourse-related theories that have been proposed so far, there have emerged no algorithms capable of deriving the discourse structure of free, unrestricted texts. On one hand, the theories developed in the traditional, truth-based semantic perspective (Kamp 1981; Lascarides and Asher 1993; Asher 1993; Hobbs et al. 1993; Kamp and Reyle 1993; Asher and Lascarides 1994; Kameyama 396 Marcu Rhetorical Parsing of Unrestricted Texts 1994; Polanyi and van den Berg 1996; van den Berg 1996; Gardent 1997; Schilder 1997; Cristea and Webber 1997; Webber et al. 1999) take the position that discourse structures can be built only in conjunction with fully specified clause and sentence syntactic structures. These theories have a grammar as their backbone and rely on sophisticated logics of belief and default logics in order to intertwine and characterize the sentence- and discourse-based linguistic phenomena. Despite their formal elegance, implementations of these theories cannot yet handle naturally occurring texts, such as that shown in (1). On the other hand, the theories aimed at characterizing the constraints that pertain to the structure of unrestricte"
J00-3005,J94-2004,0,0.0379624,"ut of that text. That is, it assumes that sentences, paragraphs, and sections correspond to hierarchical spans in the rhetorical representation of the text that they subsume. Obviously, this assumption is controversial because there is no clear-cut evidence that the rhetorical structure of a text correlates with its paragraph structure, for example. In fact, some psycholinguistic and empirical research of Heurley (1997) and Hearst (1997) indicates that paragraph breaks do not always occur at the same locations as the thematic boundaries. In contrast, experiments of Bruder and Wiebe (1990) and Wiebe (1994) show that paragraph breaks help readers to interpret private-state sentences in narratives, i.e., sentences about psychological states such as wanting and perceptual states such as seeing. Hence, paragraph breaks play an important role in story comprehension. In my own experiments (see Section 5), I observed that, in nine out of ten cases, human judges manually built rhetorical structures that correlated with the underlying paragraph boundaries. The main reason for assuming that the orthographic layout of text correlates with its rhetorical structure is primarily one of efficiency. In the sam"
J00-3005,W99-0104,0,\N,Missing
J05-4003,W03-1004,0,0.0478967,"Missing"
J05-4003,J90-2002,0,0.524324,"Missing"
J05-4003,J93-2003,0,0.0540102,"Missing"
J05-4003,P02-1033,0,0.180882,"ting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. 1. Introduction Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a part"
J05-4003,P03-1003,1,0.293929,"sh between parallel and non-parallel sentence pairs. For this purpose, we compute and exploit word-level alignments between the sentences in each pair. A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other. Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990). Since then, they were found useful in many other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question answering [Echihabi and Marcu 2003]). Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus. Each figure contains two alignments. The one on the left is a correct alignment, produced by a human, while the one on the right 482 Munteanu and Marcu Exploiting Non-Parallel Corpora Figure 3 Alignments between two parallel sentences. was computed automatically. As can be seen from the gloss next to the Arabic words, the sentences in Figure 3 are parallel while the sentences in Figure 4 are not. In a correct alignment between two non-parallel sentences, most words would h"
J05-4003,W04-3208,0,0.538141,"Missing"
J05-4003,C94-2178,0,0.00897613,"However, identifying good translations in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004"
J05-4003,P98-1069,0,0.541533,"xico” can be translated as any of the country names listed in the example. These examples also show that the problem of finding only true translation pairs is hard. Two sentences may share many content words and yet express different meanings (see Figure 14, example 1). However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance. 8. Related Work While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004). We are aware of only three previous efforts aimed at discovering parallel sentences. Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus. Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English"
J05-4003,P91-1023,0,0.061533,"lso show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. 1. Introduction Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This"
J05-4003,P04-1067,0,0.0204281,"These examples also show that the problem of finding only true translation pairs is hard. Two sentences may share many content words and yet express different meanings (see Figure 14, example 1). However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance. 8. Related Work While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004). We are aware of only three previous efforts aimed at discovering parallel sentences. Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus. Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stories from different sources and time periods. The first two systems e"
J05-4003,W04-3250,0,0.15316,"Missing"
J05-4003,W02-2018,0,0.0047693,"the model of our data, taking into account the constraints imposed by the feature functions, is a log linear combination of these functions. Thus, for our classification problem, we have: P(ci |sp) = 1 Z(sp) k  f (c,sp) λjij j=1 where ci is the class (c0 =”parallel”, c1 =”not parallel”), Z(sp) is a normalization factor, and fij are the feature functions (indexed both by class and by feature). The resulting model has free parameters λj , the feature weights. The parameter values that maximize the likelihood of a given training corpus can be computed using various optimization algorithms (see [Malouf 2002] for a comparison of such algorithms). 3.1 Features for Parallel Sentence Identification For our particular classification problem, we need to find feature functions that distinguish between parallel and non-parallel sentence pairs. For this purpose, we compute and exploit word-level alignments between the sentences in each pair. A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other. Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of"
J05-4003,P97-1039,0,0.0260992,"uality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. 1. Introduction Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This is especially p"
J05-4003,J99-1003,0,0.00949887,"ood translations in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004). We describe"
J05-4003,moore-2002-fast,0,0.401362,"ns in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004). We describe how to build"
J05-4003,P04-1066,0,0.0854263,"Missing"
J05-4003,N04-1034,1,0.423165,"Missing"
J05-4003,P03-1021,0,0.0172429,"Missing"
J05-4003,P02-1038,0,0.0603802,"improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. 1. Introduction Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political d"
J05-4003,J03-1002,0,0.00842875,"once, we choose which instance to link with so that we minimize the number of crossings with already existing links. The second heuristic attempts to improve the choice of the most likely English translation of a foreign word. Our translation probabilities are automatically learned from parallel data, and we learn values for both t( fj |ei ) and t(ei |fj ). We can therefore decide that the most likely English translation of fj is argmaxei {t( fj |ei ), t(ei |fj )}. Using both sets of probabilities is likely to help us make a better-informed decision. Using this alignment strategy, we follow (Och and Ney 2003) and compute one alignment for each translation direction ( f → e and e → f ), and then combine them. Och and Ney present three combination methods: intersection, union, and refined (a form of intersection expanded with certain additional neighboring links). Thus, for each sentence pair, we compute five alignments (two modified-IBMModel-1 plus three combinations) and then extract one set of general features and five sets of alignment features (as described in the previous section). 3.3 Training and Testing We create training instances for our classifier from a small parallel corpus. The simple"
J05-4003,P02-1040,0,0.092068,"Missing"
J05-4003,P99-1067,0,0.833474,"ated as any of the country names listed in the example. These examples also show that the problem of finding only true translation pairs is hard. Two sentences may share many content words and yet express different meanings (see Figure 14, example 1). However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance. 8. Related Work While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004). We are aware of only three previous efforts aimed at discovering parallel sentences. Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus. Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stori"
J05-4003,J03-3002,0,0.777168,"s cannot be judged in isolation, and context becomes an important factor. Barzilay and Elhadad (2003) make use of contextual information by detecting the topical structure of the articles in the two corpora and aligning them at paragraph level based on the topic assigned to each paragraph. Afterwards, they proceed and align sentences within paragraph pairs using dynamic programming. Their results show that both the induced topical structure and the paragraph alignment improve the precision of their extraction method. A line of research that is both complementary and related to ours is that of Resnik and Smith (2003). Their STRAND Web-mining system has a purpose that is similar to ours: to identify translational pairs. However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences. Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs. Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method. 9. Discussion The most important feature of our parallel sentence selection approach is its robustness. Comparable corpora are inherently noisy enviro"
J05-4003,P03-1010,0,0.504463,"Missing"
J05-4003,E03-1050,0,0.052266,"Missing"
J05-4003,P94-1012,0,0.0151496,"corpora. However, identifying good translations in comparable corpora is hard. Even texts that convey the same information will exhibit great differences at the sentence level. Consider the two newspaper articles in Figure 1. They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs that are approximate translations of each other. Discovering these links automatically is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document. Thus, they perform poorly on comparable, non-parallel texts. What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context. This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu,"
J05-4003,N01-1026,0,0.0520375,"lel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. 1. Introduction Parallel texts—texts that are translations of each other—are an important resource in many NLP applications. They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001). Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register. There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations). This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., par"
J05-4003,H01-1035,0,0.0267863,"Missing"
J05-4003,J93-1004,0,\N,Missing
J05-4003,C98-1066,0,\N,Missing
J05-4004,P00-1041,0,0.056655,"Missing"
J05-4004,W03-1004,0,0.0611087,"Missing"
J05-4004,J93-2003,0,0.0107334,"Missing"
J05-4004,C96-2183,0,0.107973,"Missing"
J05-4004,P02-1057,1,0.91229,"Missing"
J05-4004,P03-1011,0,0.015612,"tates identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely na¨ıve in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is just a collection of nearb"
J05-4004,A00-1043,0,0.0762817,"Missing"
J05-4004,J02-4006,0,0.345357,"rtz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These models treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. Such mappings can be considered word-to-word alignments, but as our results show (see Section 5), these models are too weak for capturing the sophisticated operations that are employed by humans in summarizing texts. To date, there has been very little work on the word alignment task in the context of summarization. The most relevant work is that of Jing (2002), in which a hidden Markov alignment model is applied to the task of identifying word and phrase-level correspondences between documents and abstracts. Unfortunately, this model is only able to align words that are identical up to their stems, and thus suffers from a problem of recall. This also makes it ill-suited to the task of learning how to perform abstraction, in which one would desire to know how words get changed. For example, Jing’s model cannot identify any of the following alignments from Figure 1: (Connecting Point ↔ Connecting Point Systems), (Mac ↔ Macintosh), (retailer ↔ seller)"
J05-4004,W01-0100,0,0.210724,"Missing"
J05-4004,P00-1056,0,0.0810572,". By doing so, one could recreate the extracts at each iteration using the previous iteration’s parameters to make better and shorter extracts. Similarly, one might only allow summary words to align to words found in their corresponding extract sentences, which would serve to significantly speed up training and, combined with the parameterized extracts, might not hurt performance. A final option, but one that we do not advocate, would be to give up on phrases and train the model in a word-to-word fashion. This could be coupled with heuristic phrasal creation as is done in machine translation (Och and Ney 2000), but by doing this, one completely loses the probabilistic interpretation that makes this model so pleasing. Aside from computational considerations, the most obvious future effort along the lines of this model is to incorporate it into a full document summarization system. Since this can be done in many ways, including training extraction systems, compression systems, headline generation systems, and even extraction systems, we left this to future work so that we could focus specifically on the alignment task in this article. Nevertheless, the true usefulness of this model will be borne out"
J05-4004,J03-1002,0,0.0395148,"which one would desire to know how words get changed. For example, Jing’s model cannot identify any of the following alignments from Figure 1: (Connecting Point ↔ Connecting Point Systems), (Mac ↔ Macintosh), (retailer ↔ seller), (Macintosh ↔ Apple Macintosh systems) and (January 1989 ↔ last January). Word alignment (and, to a lesser degree, phrase alignment) has been an active topic of research in the machine translation community. Based on these efforts, one might be initially tempted to use readily available alignment models developed in the context of machine translation, such as GIZA++ (Och and Ney 2003), to obtain wordlevel alignments in document, abstract corpora. However, as we will show (Section 5), the alignments produced by such a system are inadequate for the document, abstract alignment task. 1.4 Article Structure In this article, we describe a novel, general model for automatically inducing wordand phrase-level alignments between documents and their human-written abstracts. Beginning in Section 2, we will describe the results of human annotation of such alignments. Based on this annotation, we will investigate the empirical linguistic properties of such alignments, including lexi"
J05-4004,W04-3219,0,0.117887,"Missing"
J05-4004,W97-0710,0,0.116112,"Missing"
J05-4004,C96-2141,0,0.09177,"Missing"
J05-4004,P01-1067,0,0.0259234,"s empirical variance. We model null states identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely na¨ıve in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is"
J05-4004,P00-1038,0,\N,Missing
J05-4004,P03-2041,0,\N,Missing
J05-4004,J96-2004,0,\N,Missing
J07-3002,H05-1009,0,0.123424,"Missing"
J07-3002,J93-2003,0,0.0861831,"requently been decoupled from the translation task and assumptions have been made about measuring alignment quality for machine translation which, it turns out, are not justified. In particular, none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation performance. This paper explains this state of affairs and presents steps towards measuring alignment quality in a way which is predictive of statistical machine translation performance. 1. Introduction Automatic word alignment (Brown et al. 1993) is a vital component of all statistical machine translation (SMT) approaches. There were a number of research papers presented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so forth, outlining techniques for attempting to increase word alignment quality. Despite this high level of interest, none of these techniques has been shown to result in a large gain in translation performance as measured by BLEU (Papineni et al. 2001) or any other metric. We find this lack of correlation between previous word alignment quality metrics and BLEU counterintuitive, because we and other res"
J07-3002,P03-1012,0,0.0574918,"Missing"
J07-3002,W05-0814,1,0.75988,"ific task, such as improving phrasal SMT, and calculate an appropriate α to be used. Individual researchers working on the same phrasal SMT tasks as those reported here (or on very similar tasks) could use the values of α we calculated. Our work invalidates some of the conclusions of recent alignment work which presented only evaluations based on metrics like AER or balanced F-Measure, and explains the lack of correlation in the few works which presented both such a metric 301 Computational Linguistics Volume 33, Number 3 and final MT results. A good example of the former are our own results (Fraser and Marcu 2005). The work presented there had the highest balanced F-Measure scores for the Romanian/English WPT05 shared task, but based on the findings here it is possible that a different algorithm tuned for the correct criterion would have had better MT performance. Other work includes many papers working on alignment models where words are allowed to participate in a maximum of one link. These models generally have higher precision and lower recall than IBM Model 4 symmetrized using the “Refined” or “Union” heuristics. Recall that in Section 3.1 we showed that AER is broken in a way that favors precisio"
J07-3002,P06-1097,1,0.766274,"Missing"
J07-3002,P04-1064,0,0.259175,"Missing"
J07-3002,H05-1012,0,0.380852,"Missing"
J07-3002,N03-1017,1,0.0224132,"Missing"
J07-3002,N06-1015,0,0.392134,"Missing"
J07-3002,P05-1057,0,0.334734,"Missing"
J07-3002,W05-0809,0,0.018286,"Missing"
J07-3002,W03-0301,0,0.0191539,"aluation.3 The English side of the bitext is 99.3 million words. The translation development set is the “NIST 2002 Dry Run,” and the test set is the “NIST 2003 evaluation set.” We have annotated gold standard alignments for 100 parallel sentences using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8 of the data (12.4 million English words) and a large task using all of the data. The Romanian/English training data was used for the tasks on Romanian/English alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea, and Pedersen 2005). We carefully removed two sections of news bitext to use as the translation development and test sets. The English side of the training corpus is 964,000 words. The alignment set is the first 148 annotated sentences used for the 2003 task (there were 3,181 Sure links). 2 Sure links are by definition also Possible. 3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm. 294 Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation 2.2 Measuring Translation Performance Changes Caused By Alignment In phrased-based SM"
J07-3002,P03-1021,0,0.0758597,"d alignment are called Sure links. Some of the alignment sets also have links which are not Sure links but are Possible links (Och and Ney 2003). Possible links which are not Sure2 may be present but need not be present. We evaluate the translation performance of SMT systems by translating a heldout translation test set and measuring the BLEU score of our hypothesized translations against one or more reference translations. We also have an additional held-out translation set, the development set, which is employed by the MT system to train the weights of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three different language pairs, examining French to English, Arabic to English, and Romanian to English translation tasks. The training data for the French/English data set is taken from the LDC Canadian Hansard data set, from which the word aligned data (presented in Och and Ney 2003) was also taken. The English side of the bitext is 67.4 million words. We used a separate Canadian Hansard data set (released by ISI) as the source of the translation test set and development set. We evaluate two different tasks using this data, a medium task where 1/8 of the data (8."
J07-3002,J03-1002,0,0.35012,"2.1 Data To build an SMT system we require a bitext and a word alignment of that bitext, as well as language models built from target language data. In all of our experiments, we will hold the bitext and target language resources constant, and only vary how we construct the word alignment. The gold standard word alignment sets we use have been manually annotated using links between words showing translational correspondence. Links which must be present in a hypothesized alignment are called Sure links. Some of the alignment sets also have links which are not Sure links but are Possible links (Och and Ney 2003). Possible links which are not Sure2 may be present but need not be present. We evaluate the translation performance of SMT systems by translating a heldout translation test set and measuring the BLEU score of our hypothesized translations against one or more reference translations. We also have an additional held-out translation set, the development set, which is employed by the MT system to train the weights of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three different language pairs, examining French to English, Arabic to English, and Romanian to English tr"
J07-3002,2001.mtsummit-papers.68,0,0.0149733,"s towards measuring alignment quality in a way which is predictive of statistical machine translation performance. 1. Introduction Automatic word alignment (Brown et al. 1993) is a vital component of all statistical machine translation (SMT) approaches. There were a number of research papers presented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so forth, outlining techniques for attempting to increase word alignment quality. Despite this high level of interest, none of these techniques has been shown to result in a large gain in translation performance as measured by BLEU (Papineni et al. 2001) or any other metric. We find this lack of correlation between previous word alignment quality metrics and BLEU counterintuitive, because we and other researchers have measured this correlation in the context of building SMT systems that have benefited from using the BLEU metric in improving performance in open evaluations such as the NIST evaluations.1 We confirm experimentally that previous metrics do not predict BLEU well and develop a methodology for measuring alignment quality that is predictive of BLEU. We ∗ USC/ISI - Natural Language Group, 4676 Admiralty Way, Suite 1001, Marina del Rey"
J07-3002,C96-2141,0,0.961969,"Missing"
J07-3002,P02-1040,0,\N,Missing
J07-3002,P06-1065,0,\N,Missing
J07-3002,H05-1011,0,\N,Missing
J07-3002,J00-2004,0,\N,Missing
J07-3002,J08-4005,0,\N,Missing
J07-3002,H05-1010,0,\N,Missing
J10-2004,P98-1006,0,0.0838333,"Missing"
J10-2004,J93-2003,0,0.0129373,"Missing"
J10-2004,A00-2018,0,0.126804,"yle trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment tr"
J10-2004,J07-2003,0,0.240062,"putational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efﬁcient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efﬁciency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smal"
J10-2004,D09-1037,0,0.100934,"Missing"
J10-2004,P97-1003,0,0.0914444,"ng, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difﬁcult challenge, whereas the source sentence is ﬁxed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training"
J10-2004,D07-1079,1,0.88608,"traction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning"
J10-2004,P06-1121,1,0.821544,"and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up computing the joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enu"
J10-2004,N04-1035,1,0.876041,"re 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities"
J10-2004,J99-4004,0,0.0216439,"ubstructure that yields NNP2 , NNP3 , and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 254 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 7 Packed forest obtained by packing trees (3) and (6) in Figure 6. 3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed to enable dynamic programming when we extract translation rules from it. Borrowing terms from parsing semirings (Goodman 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗-node corresponds to a tree node in the unbinarized tree or a new tree node introduced during tree binarization; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree or in one of its binarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node and it contains one or more ⊗-nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing"
J10-2004,N06-1031,1,0.897685,"a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conc"
J10-2004,J98-4004,0,0.110894,"use problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase .W #L Z Æ{ 3ä VIKTOR CHERNOMYRDIN AND HIS COLL"
J10-2004,P03-1054,0,0.00631755,"in Figure 11(b) yields ungrammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by reﬁning/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) reﬁnes a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of subst"
J10-2004,J08-3004,1,0.883026,"Missing"
J10-2004,W04-3250,0,0.0206993,"ecause the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the"
J10-2004,W02-1018,1,0.803481,"Missing"
J10-2004,J93-2004,0,0.0464715,"Missing"
J10-2004,D07-1038,1,0.870717,"Missing"
J10-2004,P04-1084,0,0.0245554,"Missing"
J10-2004,D08-1022,0,0.0550391,"t rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. r r Condition 1: If we reach an additive e-forest node, for each of its children, which"
J10-2004,J04-4002,0,0.187296,"Missing"
J10-2004,P03-1021,0,0.0115692,"idates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this art"
J10-2004,P06-1055,0,0.704636,"e ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough"
J10-2004,P07-1065,0,0.0152868,"is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that th"
J10-2004,C96-2141,0,0.0804986,"Missing"
J10-2004,D07-1078,1,0.89473,"Missing"
J10-2004,J97-3002,0,0.829465,"we notice that re-structuring tends to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029"
J10-2004,P01-1067,1,0.646868,"uracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029 35.33 35.85 — 0.050 Baseline2 (EM re-structuring but no re-labeli"
J10-2004,P02-1039,1,0.437148,"ts and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transduce"
J10-2004,J02-1005,0,\N,Missing
J10-2004,D08-1033,0,\N,Missing
J10-2004,J12-2006,0,\N,Missing
J10-2004,C98-1006,0,\N,Missing
J10-2004,N06-1033,1,\N,Missing
L16-1067,P13-2131,1,0.824032,"9 68.7 23.4 R∗ 100.0 100.0 67.1 41.8 92.2 92.4 46.7 Figure 3: System name annotated as ‘PBMT base PRO’. Table 2: Performance of atom detector in terms of Precision, Recall, and F1 score, and reconstruction from survey response (R∗ ). 5. Baseline Evaluation Data From the collected data, five papers are used for development, and 62 are used for evaluation. Evaluation Metrics We evaluate system performance of atom detection with precision and recall. We approach the evaluation of the linked structured representation by transforming it into a directed acyclic graph and computing the Smatch score (Cai and Knight, 2013), previously used to evaluate the similarity between Abstract Meaning Representation (AMR) structures. 5.1. Atom Detection Evaluation Table 2 shows the performance of atom detection. As annotators do not tell us where the information is located, we match the annotated atoms to every substring in the structured text and present annotation recall from text as R∗ in Table 2. This presents a soft ceiling for our baseline approach. Finding annotated dataset name, size, and system name atoms was challenging due to abbreviations, PDF-to-text conversion errors, lexical diversity and name expansion, as"
L16-1067,councill-etal-2008-parscit,0,0.0238814,"3 4. St. Dev. 1.38 1.78 89.65 9.85 4.14 1.52 5.01 0.34 We present a pipelined pattern-based system that extracts individual atoms from a plain text logical representation of a machine translation paper and selects and links them into a structured representation. 4.1. Table 1: Structured text and survey response mean and standard deviation. Figure 2: Survey form to collect annotations. commercial TET system.2 As tables are often used to report experimental results, we pay special attention to their extraction. We extract tabular information using TableSeer3 (Liu et al., 2007). We use ParsCit4 (Councill et al., 2008) to derive the hierarchical structure of sections and subsections. We produce the final representation of papers with a system that combines the inputs of all three components. The process produces structured text, split into sections and subsections with parsed tables accompanied by captions, but does not include figures. 3.2. Structured Representation Annotation Annotators are presented with the papers in PDF format. Ideally, annotators would highlight relevant information in the text and link it to the structured representation. However, such linking is very time consuming. As an alternativ"
L16-1067,I11-1001,0,0.0369291,"Missing"
L16-1067,W06-1606,1,0.770132,"Missing"
L16-1067,W09-3607,0,0.0130031,"a structured representation of the experimental data (bottom). Two result values are associated with the PBMT system and two with the SPMT-Comb system. The RESULT values are connected to DATASETS via the test or train relation, and to an EXPERIMENT TYPE. We refer to the individual pieces of information that comprise the structured representation as atoms. In the example, there are 15 atoms, including Chinese-English, BLEU, 31.46 and 2002 NIST. 3. Data and Annotation In order to construct the dataset, we started by selecting papers related to Machine Translation from the ACL Anthology corpus (Radev et al., 2009) using keyword search and targeting of MT related workshops. For a random sample of 67 papers, we asked the annotators to provide a structured representation of experimental results as defined in the previous section. In order to aid structured information extraction, we automatically produced a structured text representation of each paper. The representation consists of plain text split into sections and subsections, as well as parsed tables. The annotated dataset is released alongside the paper1 to promote future research. In total, 1063 atoms were annotated. Additional dataset statistics ar"
L16-1067,P11-4002,0,0.394826,"Missing"
N03-1017,P97-1003,0,0.13436,"slation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997]. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typi"
N03-1017,P01-1030,1,0.141267,"Missing"
N03-1017,2002.tmi-papers.9,0,0.138734,"ad sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from rece"
N03-1017,W02-1018,1,0.154807,"Missing"
N03-1017,P00-1056,1,0.180643,"ed phrases could filter out such non-intuitive pairs. We carried out experiments to compare the performance of three different methods to build phrase translation probability tables. We also investigate a number of variations. We report most experimental results on a GermanEnglish translation task, since we had sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde,"
N03-1017,W99-0604,1,0.228551,"f phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction. 3.1 Phrases from Word-Based Alignments The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described in more detail in Section 4.5. We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside [Och et al., 1999]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency:    count           count     No smoothing is performed. 3.2 Syntactic Phrases If we collect all phrase pairs that are consistent with word alignments, this includes many non-intuitive phrases. For instance, translations for phrases such as “house the” may be learned. Intuitively we would be inclined to believe that such phrases do not help: Restricting possible Consistent with Imamura [2002], we define a syntactic phrase as a word sequence that is covere"
N03-1017,W01-1408,1,0.139726,"Missing"
N03-1017,2001.mtsummit-papers.68,0,0.154084,"14k 373k BLEU  .27    .26  Table 1: Size of the phrase translation table in terms of distinct phrase pairs (maximum phrase length 4)   .25    .24    4 Experiments .23 We used the freely available Europarl corpus 2 to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing. In all experiments in Section 4.1-4.6 we translate from German to English. We measure performance using the BLEU score [Papineni et al., 2001], which estimates the accuracy of translation output with respect to a reference translation. .22 4.1 Comparison of Core Methods     .21   .20  .19 AP Joint M4 Syn  .18 10k 20k 40k 80k 160k 320k Training Corpus Size   Figure 1: Comparison of the core methods: all phrase pairs consistent with a word alignment (AP), phrase pairs from the joint model (Joint), IBM Model 4 (M4), and only syntactic phrases (Syn) First, we compared the performance of the three methods for phrase extraction head-on, using the same decoder (Section 2) and the same trigram language model. Figure 1 displays th"
N03-1017,C00-2105,0,0.142196,"Missing"
N03-1017,J97-3002,0,0.294318,"i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997]. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typically only translation of phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction. 3.1 Phrases from Word-Based Alignments The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described"
N03-1017,P01-1067,0,0.277456,"r to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table. Our experiments show that high levels of performance can be achieved with fairly simple means. In fact, for most of the steps necessary to build a phrase-based system, tools and resources are freely available for researchers in the field. More sophisticated approaches that make use of syntax do not lead to better performance. In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models [Yamada and Knight, 2001], proves to be harmful. Our experiments also show, that small phrases of up to three words are sufficient for obtaining high levels of accuracy. Performance differs widely depending on the methods used to build the phrase translation table. We found extraction heuristics based on word alignments to be better than a more principled phrase-based alignment method. However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus. 1 Introduction 2 Evaluation Framework Various researchers have improved the quality of statistica"
N03-1017,J93-2003,0,\N,Missing
N03-1017,P02-1040,0,\N,Missing
N03-1024,W02-1022,0,0.0515429,"ions in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious prob1. 3. 5. 7. 9. 11. At least 12 people were killed in the battle last week. Last week’s fight took at least 12 lives. The battle of last week killed at least 12 persons. At least 12 died in the battle last week. During last week’s fighting, at least 12 people died. Last week’s fighting took the"
N03-1024,N03-1003,0,0.711823,"f all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic. For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many “bad” paths such as the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting (See Section 4.2.2 for a more quantitative analysis.). It’s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). But we chose to approach this problem from another direction. As a result, we propose a new syntax-based algorithm to produce FSAs. In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2). We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3). An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4). Since our representations encode thousands and sometimes millions of equivalent ve"
N03-1024,P01-1008,0,0.902573,"1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective. Assume, for example, that we know that text pairs (stock market rose, stock Kevin Knight and Daniel Marcu Information Sciences Institute University of Southern California Marina Del Rey, CA 90292 USA {knight,marcu}@isi.edu market gained) and (stock market rose, stock prices rose) have the same meaning. If we memorized only these tw"
N03-1024,P99-1071,0,0.0539953,"native semantic renderings, which may be used to evaluate the quality of translations. 1 Introduction In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns. Although this is adequate in the context of some applications, it is clearly too"
N03-1024,A00-2018,0,0.0441775,"Missing"
N03-1024,P98-1116,1,0.637441,"): multiple English translations of many foreign language texts. For instance, when given as input the 11 semantically equivalent English translations in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning. Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}. The contexts in which these are correct paraphrases are also conveniently captured in the representation. In previous work, Langkilde and Knight (1998) used word lattices for language generation, but their method involved hand-crafted rules. Bangalore et al. (2001) and Barzilay and Lee (2002) both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs. For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious prob1. 3. 5. 7. 9. 11. At least 12 people were killed in the battle last week. Last week’s fight took at least 12 lives. The battle of last week killed at l"
N03-1024,C94-1051,0,0.0496961,"e sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. 1 Introduction In the past, paraphrases have come under the scrutiny of many research communities. Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines (Sparck Jones and Tait, 1984). Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999). And researchers in multi-document text summarization (Barzilay et al., 1999), information extraction (Shinyama et al., 2002), and question answering (Lin and Pantel, 2001; Hermjakob et al., 2002) have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems. In previous work (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Shinyama et al., 2002), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, an"
N03-1024,C98-1112,1,\N,Missing
N03-1030,kingsbury-palmer-2002-treebank,0,\N,Missing
N03-1030,J97-2002,0,\N,Missing
N03-1030,A00-2018,0,\N,Missing
N03-1030,J93-2004,0,\N,Missing
N03-1030,W01-1605,1,\N,Missing
N03-1030,J02-3001,0,\N,Missing
N03-1030,P95-1037,0,\N,Missing
N03-1030,fillmore-etal-2002-framenet,0,\N,Missing
N03-2016,P01-1030,1,0.457519,"s split into words, and all possible word pairings are stored in a file. Numbers and punctuation are not considered, since we feel that they warrant a more specific approach. After sorting and removing duplicates, the file represents all possible one-to-one word alignments of the bitext. Also removed are the pairs that include English 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set au"
N03-2016,N01-1020,0,0.323315,"uage to another (e.g. English sprint and Japanese supurinto). In a broad sense, cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word alignments of test sentences. In this paper, we investigate"
N03-2016,P00-1056,0,0.17161,"Missing"
N03-2016,P02-1040,0,0.0821725,"ation are not considered, since we feel that they warrant a more specific approach. After sorting and removing duplicates, the file represents all possible one-to-one word alignments of the bitext. Also removed are the pairs that include English 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set augmented with a set of cognates. The set consisted of two copies of a list produced by applyi"
N03-2016,1992.tmi-1.7,0,0.578863,"sh night and German nacht) or borrowing from one language to another (e.g. English sprint and Japanese supurinto). In a broad sense, cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word ali"
N03-2016,J93-2003,0,\N,Missing
N04-1024,J97-1003,0,0.0403929,"ect, but only loosely related to the essay topic. For this student, we would also want the system to provide appropriate feedback to, so that the student could revise the thesis statement text appropriately. In earlier work, Foltz, Kintsch & Landauer (1998), and Wiemer-Hastings & Graesser (2000) have developed systems that also examine coherence in student writing. Their systems measure lexical relatedness between text segments by using vector-based similarity between adjacent sentences. This linear approach to similarity scoring is in line with the TextTiling scheme (Hearst and Plaunt, 1993; Hearst, 1997), which may be used to identify the subtopic structure of a text. Miltsakaki and Kukich (2000) have also addressed the issue of establishing the coherence of student essays, using the Rough Shift element of Centering Theory. Again, this previous work looks at the relatedness of adjacent text segments, and does not explore global aspects of text coherence. Hierarchical models of discourse have been applied to the question of coherence (Mann and Thompson, 1986), but so far these have been more useful in language generation than in determining how coherent a given text is, or in identifying the s"
N04-1024,A00-2019,0,0.0862846,"Missing"
N04-1035,2003.mtsummit-papers.6,1,0.303093,"tistical MT. We take this approach in our paper. Of course, the broad statistical MT program is aimed at a wider goal than the conventional rule-based program – it seeks to understand and explain human translation data, and automatically learn from it. For this reason, we think it is important to learn from the model/data explainability studies of Fox (2002) and to extend her results. In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003). Our basic idea is to create transformation rules that condition on larger fragments of tree structure. It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples. But our main interest is in collecting a large set of such rules automatically through corpus analysis. The search for these rules is driven exactly by the problems raised by Fox (2002) – cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better"
N04-1035,P03-2041,0,0.926149,"road statistical MT program is aimed at a wider goal than the conventional rule-based program – it seeks to understand and explain human translation data, and automatically learn from it. For this reason, we think it is important to learn from the model/data explainability studies of Fox (2002) and to extend her results. In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003). Our basic idea is to create transformation rules that condition on larger fragments of tree structure. It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples. But our main interest is in collecting a large set of such rules automatically through corpus analysis. The search for these rules is driven exactly by the problems raised by Fox (2002) – cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better rules. Section 2 of this paper describes algorithms for"
N04-1035,W02-1039,0,0.537809,"Dept. of Computer Science Information Sciences Institute University of California University of Southern California Los Angeles, CA 90024 Marina Del Rey, CA 90292 mhopkins@cs.ucla.edu Abstract We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. 1 Introduction In a very interesting study of syntax in statistical machine translation, Fox (2002) looks at how well proposed translation models fit actual translation data. One such model embodies a restricted, linguistically-motivated notion of word re-ordering. Given an English parse tree, children at any node may be reordered prior to translation. Nodes are processed independently. Previous to Fox (2002), it had been observed that this model would prohibit certain re-orderings in certain language pairs (such as subjectVP(verb-object) into verb-subject-object), but Fox carried out the first careful empirical study, showing that many other common translation patterns fall outside the sco"
N04-1035,P03-1011,0,0.0428361,"e focused on providing a well-founded mathematical theory and efficient, linear algorithms for learning syntactically motivated transformation rules from parallel corpora. One can easily imagine a range of techniques for defining probability distributions over the rules that we learn. We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems. 5 Conclusion The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora. Our empirical results suggest that this may be too strong of an assumption. To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments. The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects. 1. Our rules provide a good, realistic indicator of the complexities inherent in translatio"
N04-1035,N03-1017,1,0.077612,"parsing errors also cause trouble, as a normally well-behaved re-ordering environment can be disrupted by wrong phrase attachment. For other language pairs, the divergence is expected to be greater. In the face of these problems, we may choose among several alternatives. The first is to abandon syntax in statistical machine translation, on the grounds that syntactic models are a poor fit for the data. On this view, adding syntax yields no improvement over robust phrasesubstitution models, and the only question is how much {knight,marcu}@isi.edu does syntax hurt performance. Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive. Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus. One approach here is that of Wu (1997), in which word-movement is modeled by rotations at unlabeled, binary-branching nodes. At each sentence pair, the parse adapts to explain the translatio"
N04-1035,P00-1056,0,0.251997,"to explain the data well. 3.2 Data We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999). For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words). Cases other than one-to-one sentence mappings were eliminated. For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000). The corpus contains two kinds of alignments: S (sure) for unambiguous cases and P (possible) for unclear cases, e.g. idiomatic expressions and missing function words (S ⊆ P ). In order to be able to make legitimate comparisons between the two language pairs, we also used GIZA++ to obtain machine-generated word alignments for Hansard: we trained it with the 500 sentences and additional data representing 13.7 million English words (taken from the Hansard and European parliament corpora). 3.3 Results From a theoretical point of view, we have shown that our model can fully explain the transforma"
N04-1035,J03-1002,0,0.0261537,"study with that of Fox (2002). The additional language pair provides a good means of evaluating how our transformation rule extraction method scales to more problematic language pairs for which child-reordering models are shown not to explain the data well. 3.2 Data We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999). For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words). Cases other than one-to-one sentence mappings were eliminated. For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000). The corpus contains two kinds of alignments: S (sure) for unambiguous cases and P (possible) for unclear cases, e.g. idiomatic expressions and missing function words (S ⊆ P ). In order to be able to make legitimate comparisons between the two language pairs, we also used GIZA++ to obtain machine-generated word alignments for Hansard: we trained it with the 500 sen"
N04-1035,J97-3002,0,0.857464,"er robust phrasesubstitution models, and the only question is how much {knight,marcu}@isi.edu does syntax hurt performance. Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive. Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus. One approach here is that of Wu (1997), in which word-movement is modeled by rotations at unlabeled, binary-branching nodes. At each sentence pair, the parse adapts to explain the translation pattern. If the same unambiguous English sentence were to appear twice in the corpus, with different Chinese translations, then it could have different learned parses. A third direction is to maintain English syntax and investigate alternate transformation models. After all, many conventional translation systems are indeed based on syntactic transformations far more expressive than what has been proposed in syntax-based statistical MT. We tak"
N04-1035,P01-1067,1,0.719731,"ossing due to a modal. In this paper, we focused on providing a well-founded mathematical theory and efficient, linear algorithms for learning syntactically motivated transformation rules from parallel corpora. One can easily imagine a range of techniques for defining probability distributions over the rules that we learn. We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems. 5 Conclusion The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora. Our empirical results suggest that this may be too strong of an assumption. To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments. The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects. 1. Our rules provide a good, realistic indicator of the comple"
N04-1035,J03-4003,0,\N,Missing
N06-1001,W04-3237,0,0.0273301,"capitalization tag sequence. Associating a tag in the output with the corresponding A capitalizer is a tagger that recovers the capitalization tag for each input lowercased word, outputting a well-capitalized sentence. Since each lowercased word can have more than one tag, and associating a tag with a lowercased word can result in more than one surface form (e.g., /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solve the capitalization ambiguities. For example, Lita et al. (2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entropy Markov model (MEMM) combining features involving words and their cases. Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts. However, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough. For example, if the sentence “click ok to save your changes to /home/doc .” in the above example is the translation of the French sentence “CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFICATIONS DANS /HOME/DOC .”, the correct capitalization result should pr"
N06-1001,N04-1035,1,0.501754,"wo words in E represents the dependency between them captured by monolingual n-gram language models. We also assume that both E and F have phrase boundaries available (denoted by the square brackets), and that A is the phrase alignment. ˜i is the i-th In Figure 3, F˜j is the j-th phrase of F , E phrase of E, and they align to each other. We do not require a word alignment; instead we find it reason˜i can be aligned to any able to think that a word in E adapted to syntax-based machine translation, too. To this end, the translational correspondence is described within a translation rule, i.e., (Galley et al., 2004) (or a synchronous production), rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus. 2 The capitalization model p(E|F, A) itself does not require the existence of e. This means that in principle this model can also be viewed as a capitalized translation model that performs translation and capitalization in an integrated step. In our paper, however, we consider the case where the machine translation output e is given, which is reflected by the the fact that GEN(e) takes e as input in Formula 1. word in F˜j . A"
N06-1001,P03-1020,0,0.768398,"getting the surface form “Click OK to save your changes to /home/DOC .”. We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs. 1 Introduction Capitalization is the process of recovering case information for texts in lowercase. It is also called truecasing (Lita et al., 2003). Usually, capitalization itself tries to improve the legibility of texts. It, however, can affect the word choice or order when interacting with other models. In natural language processing, a good capitalization model has been shown useful for tasks like name entity recognition, automatic content extraction, speech recognition, modern word processors, and machine translation (MT). Capitalization can be viewed as a sequence labeling process. The input to this process is a sentence in lowercase. For each lowercased word in the input sentence, we have several available capitalization tags: init"
N06-1001,W02-1018,1,0.822044,"d bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set. We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser-Ney smoothing instead of a mixture. A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext. The capitalizer was incorporated into the MT system as a postprocessing module — it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process. 6 Experiments 6.2 6.1 Settings We conducted capitalization experiments on three language pairs: English-to-French (E→F) with a bilingual corpus from the Information Technology (IT) domain; French-to-English (F→E) with a bilingual corpus from the general news domain; and Chinese-to-English (C→E) with a bilingual corpus from t"
N06-1001,P99-1021,0,0.0214936,"for each low2 Finput Lower Case {F } Lower Case {f } Train Translation Model Lower Case {e} Train Language Model f Translation Model MT Decoder {E} Train Monolingual Capitalization Model Languagel Model Monolingual Cap Model e Capitalization Eoutput Figure 1: The monolingual capitalization scheme employed by most statistical MT systems. ercased sentence e, they find the label sequence T that maximizes p(T |e). They use a maximum entropy Markov model (MEMM) to combine features of words, cases and context (i.e., tag transitions). Gale et al. (1994) report good results on capitalizing 100 words. Mikheev (1999) performs capitalization using simple positional heuristics. 3 Monolingual Capitalization Scheme Translation and capitalization are usually performed in two successive steps because removing case information from the training of translation models substantially reduces both the source and target vocabulary sizes. Smaller vocabularies lead to a smaller translation model with fewer parameters to learn. For example, if we do not remove the case information, we will have to deal with at least nine probabilities for the English-French word pair (click, cliquez). This is because either “click” or “c"
N06-1001,J04-4002,0,0.0154102,"alignment can be quite computationally expensive as it requires to translate the entire training corpus; also a phrase aligner is not always available. We therefore generate the training data using a na¨ıve phrase aligner (NPA) instead of resorting to a real one. The input to the NPA is a word-aligned bilingual corpus. The NPA stochastically chooses for each sentence pair one segmentation and phrase alignment that is consistent with the word alignment. An aligned phrase pair is consistent with the word alignment if neither phrase contains any word aligning to a word outside the other phrase (Och and Ney, 2004). The NPA chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths. This distribution can be obtained from the trace output of a phrase-based MT Languages E→F (IT) F→E (news) C→E (news) Entire Corpus (#W) Training Dev Test-Prec. 62M 13K 15K 144M 11K 22K 50M 8K 17K Test-BLEU (#sents) 763 241 919 Table 2: Corpora used in experiments. decoder on a small development set. The NPA has to retry if the current source phrase cannot find any consistent target phrase. Unaligned target words are attached to the left phrase. Heuristics are employed to pre"
N06-1001,2001.mtsummit-papers.68,0,0.0158198,"assess the impact of our capitalizer on end-to-end translation performance; in this case, the capitalizer may operate on ungrammatical sentences. We chose to work with these three language pairs because we wanted to test our capitalization model on both English and French target MT systems and in cases where the source language has no case information (such as in Chinese). We estimated the feature functions, such as the log probabilities in the language model, from the 6 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al., 2001). In this context, the capitalizer operates on potentially ill-formed, MTproduced outputs. To this end, we first integrated our bilingual capitalizer into the phrase-based SMT system as a postprocessing module. The decoder of the MT system was modified to provide the capitalizer with the case-preserved source sentence, the lowercased translation, and the phrase boundaries and their alignments. Based on this information, our bilingual capitalizer recovers the case information of the lowercased translation, outputting a capitalized target sentence. The case-restored machine translations were eva"
N06-1001,P04-1007,0,0.356563,"in Formula 1. word in F˜j . A probabilistic model defined on this graph is a Conditional Random Field. Therefore, it is natural to formulate the bilingual capitalization model using CRFs:3 ! (2) ! (3) I X 1 pλ (E|F, A) = exp λi fi (E, F, A) Z(F, A, λ) i=1 where Z(F, A, λ) = X E∈GEN(e) exp I X λi fi (E, F, A) i=1 fi (E, F, A), i = 1...I are the I features, and λ = (λ1 , ..., λI ) is the feature weight vector. Based on this capitalization model, the decoder in the capitalizer looks for the best E ∗ such that ∗ E = arg maxE∈GEN(e,F ) I X λi fi (E, F, A) (4) i=1 4.2 Parameter Estimation Following Roark et al. (2004), Lafferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights λ maximizing the regularized log-likelihood LLR (λ) of the training data {E (n) , F (n) , A(n) , n = 1, ..., N }. LLR (λ) = N X “ ” ||λ||2 log p E (n) |F (n) , A(n) − 2σ 2 n=1 (5) 4.3 Feature Functions We define features based on the alignment graph in Figure 3. Each feature function is defined on a word. Monolingual language model feature. The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei : fLM (Ei , F, A) = log p(Ei |Ei−1 , ..., Ei−n+1 ) (6)"
N06-1001,P02-1040,0,\N,Missing
N12-1017,W11-2101,0,0.0765686,"Missing"
N12-1017,W11-2103,0,0.0472853,"Missing"
N12-1017,W11-2107,0,0.0116398,"translations divided by the mean score for the human translations: the higher this number, the better a metric separates machine from human produced outputs. Under HyTER, m/h is about 1.9, which shows that the HyTER scores for machine translations are, on average, almost twice as high as for human translations. Under Likert – a score assigned by human annotators who compare pairs of sentences at a time–, the quotient is higher, suggesting that human raters make stronger distinctions between human and machine translations. The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), B LEU and TERp (Snover et al., 2009). These results show that HyTER separates machine from human translations better than alternative metrics. 4.2 Ranking MT systems by quality We rank the five machine translation systems according to several widely used metrics (see Figure 3). Our results show that B LEU, Meteor and TERp do not rank the systems in the same way as HTER and humans do, while the HyTER metric yields the correct ranking. Also, separation between the quality of the five systems is higher under HyTER, HTER, and Likert than under alternative metrics. 4.3 Correlations with HTER We"
N12-1017,W05-0831,0,0.0202412,"permutations of x with their associated distance costs, and LS is the one-state Levenshtein transducer whose output weight for a string pair (x,y) is the Levenshtein distance between x, and y, and the symbol ◦ denotes composition. The model is depicted in Figure 2. def H(x, Y) = Πx ◦ LS ◦ Y (2) 166 Permutations. We define an FSA Πx that allows permutations according to certain constraints. Allowing all permutations of the hypothesis x would increase the search space to factorial size and make inference NP-complete (Cormode and Muthukrishnan, 2007). We use local-window constraints (see, e.g., Kanthak et al. (2005)), where words may move within a fixed window of size k; these constraints are of size O(n) with a constant factor k, where n is the length of the translation hypothesis x. Lazy Evaluation. For efficiency, we use lazy evaluation when defining the search space H(x, Y). This means we never explicitly compose Πx , LS, and Y. Parts of the composition that our inference algorithm does not explore are not constructed, saving computation time and memory. Permutation paths in Πx are constructed on demand. Similarly, the reference set Y is expanded on demand, and large parts may remain unexpanded.1 Exa"
N12-1017,P08-2021,1,0.78711,"ace H(x, Y). This means we never explicitly compose Πx , LS, and Y. Parts of the composition that our inference algorithm does not explore are not constructed, saving computation time and memory. Permutation paths in Πx are constructed on demand. Similarly, the reference set Y is expanded on demand, and large parts may remain unexpanded.1 Exact Inference. To compute uhyter(x, Y), we define the composition H(x, Y) and can apply any shortest-path search algorithm (Mohri, 2002). We found that using the A* algorithm (Hart et al., 1972) was the most efficient; we devised an A* heuristic similar to Karakos et al. (2008). Runtime. Computing the HyTER score takes 30 ms per sentence on networks by single annotators (combined all-annotator networks: 285 ms) if no 1 These on-demand operations are supported by the OpenFst library (Allauzen et al., 2007); specifically, to expand the RTNs into FSAs we use the Replace operation. Metric [100-0]-B LEU, 1 ref [100-0]-B LEU, 3 refs [100-0]-Meteor, 1 ref [100-0]-Meteor, 3 refs [100-0]-TERp, 1 ref [100-0]-TERp, 3 refs HyTER U HyTER SPU [100-0]-Likert Arabic-English Human mean Machine mean 59.90 69.14 41.49 57.44 60.13 65.70 55.98 62.91 35.87 46.48 27.08 39.52 18.42 34.94 1"
N12-1017,C04-1072,0,0.0626158,"ults show that for large documents, all metrics correlate well with HTER. However, as the sizes of the documents decrease, and especially at the sentence level, HyTER provides especially high correlation with HTER as compared to the other metrics. As a side note, we can see that using reordering when computing the HyTER score does not give consistently better results – see the (r5) numbers, which searched over hypothesis permutations within a local window of size 5; this shows that most reorderings are already captured in the networks. In all experiments, we use B LEU with plus-one smoothing (Lin and Och, 2004). 5 Using meaning-equivalent networks for human translation evaluation In this section, we present a use case for the HyTER metric outside of machine translation. 5.1 Setup and problem Language Testing units assess the translation proficiency of thousands of applicants interested in performing language translation work for the US Government. Job candidates typically take a written test in which they are asked to translate four passages (i.e., paragraphs) of increasing difficulty into English. The passages are at difficulty levels 2, 2+, 3, and 4 on the Interagency Language Roundable (ILR) scal"
N12-1017,P02-1040,0,0.100882,"es us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics. 1 In contrast, human-informed metrics have other deficiencies: they have large variance across human judges (Bojar et al., 2011) and produce unstable results from one evaluation to another (Przybocki et al., 2011). Because evaluation scores are not computed automatically, systems developers cannot automatically tune to human-based metrics. Motivation During the last decade, automatic evaluation metrics (Papineni et al., 2002; Snover et al., 2006; Lavie and Denkowski, 2009) have helped researchers accelerate the pace at which they improve machine translation (MT) systems. And human-assisted metrics (Snover et al., 2006) have enabled and supported large-scale U.S. government sponsored programs, such as DARPA GALE (Olive et al., 2011). However, these metrics have started to show signs of wear and tear. Automatic metrics are often criticized for providing non-intuitive scores – few researchers can explain to casual users what a B LEU score of 27.9 means. And researchers have grown increasingly concerned that automati"
N12-1017,2006.amta-papers.25,0,0.801851,"metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics. 1 In contrast, human-informed metrics have other deficiencies: they have large variance across human judges (Bojar et al., 2011) and produce unstable results from one evaluation to another (Przybocki et al., 2011). Because evaluation scores are not computed automatically, systems developers cannot automatically tune to human-based metrics. Motivation During the last decade, automatic evaluation metrics (Papineni et al., 2002; Snover et al., 2006; Lavie and Denkowski, 2009) have helped researchers accelerate the pace at which they improve machine translation (MT) systems. And human-assisted metrics (Snover et al., 2006) have enabled and supported large-scale U.S. government sponsored programs, such as DARPA GALE (Olive et al., 2011). However, these metrics have started to show signs of wear and tear. Automatic metrics are often criticized for providing non-intuitive scores – few researchers can explain to casual users what a B LEU score of 27.9 means. And researchers have grown increasingly concerned that automatic metrics have a stro"
N12-1017,W09-0441,0,0.0545897,"e human translations: the higher this number, the better a metric separates machine from human produced outputs. Under HyTER, m/h is about 1.9, which shows that the HyTER scores for machine translations are, on average, almost twice as high as for human translations. Under Likert – a score assigned by human annotators who compare pairs of sentences at a time–, the quotient is higher, suggesting that human raters make stronger distinctions between human and machine translations. The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), B LEU and TERp (Snover et al., 2009). These results show that HyTER separates machine from human translations better than alternative metrics. 4.2 Ranking MT systems by quality We rank the five machine translation systems according to several widely used metrics (see Figure 3). Our results show that B LEU, Meteor and TERp do not rank the systems in the same way as HTER and humans do, while the HyTER metric yields the correct ranking. Also, separation between the quality of the five systems is higher under HyTER, HTER, and Likert than under alternative metrics. 4.3 Correlations with HTER We know that current metrics (e.g., B LEU,"
N12-1061,P01-1005,0,0.019455,"ery, making use of an existing word alignment model for this task. We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline. Figure 1: Example of a word alignment resulting from noisy parallel data. The structure of the resulting alignment makes it difficult to find and extract parallel fragments via the standard heuristics or simply by inspection. How can we discover automatically those parallel fragments hidden within such data? 1 Introduction A decade ago, Banko and Brill (2001) showed that scaling to very large corpora is game-changing for a variety of tasks. Methods that work well in a smalldata setting often lose their luster when moving to large data. Conversely, other methods that seem to perform poorly in that same small-data setting, may perform markedly differently when trained on large data. Perhaps most importantly, Banko and Brill showed that there was no significant variation in performance among a variety of methods trained atscale with large training data. The takeaway? If you desire to scale to large datasets, use a simple solution for your task, and t"
N12-1061,D08-1024,0,0.0377505,"Missing"
N12-1061,W04-3208,0,0.0405894,"ire large parallel corpora to train state-of-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems."
N12-1061,N04-1035,1,0.807662,"Missing"
N12-1061,P06-1011,1,0.838591,"f-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. We also depart from previous work in that we on"
N12-1061,P06-1055,0,0.0759963,"Missing"
N12-1061,2007.mtsummit-papers.50,0,0.316354,"dels. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. We also depart from previous work in that we only consider parallel corpora th"
N12-1061,D11-1046,1,0.912677,"As an illustrative example, consider the following sentence pair in our training corpus taken from LDC2005T10. This is the sentence pair shown in Figure 1: fate brought us together on that wonderful summer day 2 Detecting Noisy Data In order to extract previously unextractable good parallel data, we must first detect the bad data. In doing so, we will make use of existing machinery in a novel way. We directly use the alignment model to detect weak or undesirable data for translation. 2.1 Alignment Model as Noisy Data Detector The alignment model we use in our experiments is that described in (Riesa et al., 2011), modified to output full derivation trees and model scores along with alignments. Our reasons for using this particular alignment method are twofold: it provides a natural way to hierarchically partition subsentential segments, and is also empirically quite accurate in modeling word alignments, in general. This latter quality is important, not solely for downstream translation quality, but also for the basis of our claims with respect to detecting noisy or unsuitable data: The alignment model we employ is discriminatively trained to know what good alignments between parallel data look like. W"
N12-1061,C10-1124,0,0.0352748,"Missing"
N12-1061,I05-1023,0,0.0285264,"ora to train state-of-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. We also depart from"
N16-1029,N03-1002,0,0.00885673,"2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech"
N16-1029,W13-2322,1,0.0318382,", and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotations, to automatically map 9, 514 entity types in DBPedia to three main entity types of interest: Person (PER), Location (LOC) and Organization (ORG). Then we adopt a language-independent crosslingual entity linking system (Wang et al., 2015) 4 5 to link each IL name mention to English DBPedia. This linker is based on an unsupervised quantified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowledge networks from KB. Each IL name is matched with"
N16-1029,P00-1011,0,0.050254,"ng (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in"
N16-1029,E03-1038,0,0.0920325,"et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which"
N16-1029,N13-1006,0,0.0156684,"We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Cat"
N16-1029,C02-1025,0,0.0358446,"typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such"
N16-1029,D10-1098,0,0.00551045,"s “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 199"
N16-1029,W99-0613,0,0.0761662,"ey that aims to acquire a wide-range of IL-specific knowledge from native speakers in an efficient way. The survey categorizes questions and organizes them into a tree structure, so that the order of questions is chosen based on the answers of previous questions. The survey answers are then automatically translated into rules, patterns or gazetteers in the tagger. Some example questions are shown in Table 2. 3.3 Mono-lingual Expectation Mining We use a bootstrapping method to acquire IL patterns from unlabeled mono-lingual IL documents. Following the same idea in (Agichtein and Gravano, 2000; Collins and Singer, 1999), we first use names identified by high-confident rules as seeds, and generalize patterns from the contexts of these seeds. Then we evaluate the patterns and apply high-quality ones to find more names as new seeds. This process is repeated iteratively 2 . We define a pattern as a triple ⟨lef t, name, right⟩, where name is a name, left and right3 are context vectors with weighted terms (the weight is computed based on each token’s tf-idf score). For example, from a Hausa sentence “gwamnatin kasar Sin ta samar wa kasashen yammacin Afirka ... (the Government of China has given ... products to the"
N16-1029,P98-1045,0,0.125064,"er et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to process a surprise I"
N16-1029,demiros-etal-2000-named,0,0.0375117,"stic phenomena. Typing Accuracy* Overall F-score 84.1 93.6 86.2 92.8 72.0 69.1 82.3 40.7 51.6 33.8 65.1 35.0 43.6 47.1 * typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2"
N16-1029,P05-1045,0,0.0179112,"preposition suffixes as many as you can (e.g. “’da” in “Ankara’da yaşıyorum (I live in Ankara)” is a preposition suffix which means “in”). Translation 1. Please translate the following English words and phrases: - organization suffix: agency, group, council, party, school, hospital, company, office, ... - time expression: January, ..., December; Monday, ..., Sunday; ... Table 2: Survey Question Examples Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Be"
N16-1029,W06-2005,0,0.0333424,"9; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to process a surprise IL within very short time using very few resources. The TIDES 2003 Surprise Language Hindi Named En"
N16-1029,P05-1051,1,0.726173,"ncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as lan"
N16-1029,Y09-1024,1,0.804456,"pervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish"
N16-1029,P12-1073,0,0.0266001,"for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney a"
N16-1029,li-etal-2014-comparison,1,0.597073,"Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rul"
N16-1029,W98-1002,0,0.0349112,"al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (Hana et al., 2006), Serbo-croatian (Nenadić and Spasić, 2000), Swedish (Dalianis and Åström, 2001) and Turkish (Tür et al., 2003). However, most of previous work relied on substantial amount of resources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of labeled data, or a huge"
N16-1029,W03-0430,0,0.0347683,"ntified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al."
N16-1029,E99-1001,0,0.0394154,"Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Arévalo et al., 2002), Portuguese (H"
N16-1029,N03-2025,0,0.0601139,"usa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Ita"
N16-1029,J03-1002,0,0.00415466,"uary, ..., December; Monday, ..., Sunday; ... Table 2: Survey Question Examples Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company,"
N16-1029,W04-2905,0,0.0351028,"uire expected names from topically-related English documents for a given IL document. We apply the Stanford name tagger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation table from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most similar pronunciation as each English name. For example, the Bengali name “টিন ে য়ার” and “Tony Blair” have the same Soundex code “T500 B460”. 3.5 4 Supervised Active Learning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotation"
N16-1029,E12-2015,0,0.00508774,"omputed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Gr"
N16-1029,D08-1112,0,0.0103785,"abeled pool ϕ(·) ← query strategy, B ← query batch size M ← maximum number of tokens while Length(L)&lt; M do θ = train(L); for b ∈ {1, 2, ..., B} do x∗b = arg maxx∈U ϕ(x) L = L ∪ {x∗b , label(x∗b )} U = U − x∗b end for end while T ∑ M ∑ Name 4,713 1,619 6,119 4120 4,954 2,694 3,745 Unique Name 2,820 950 3,375 2,871 3,314 1,323 2,337 IL Dev. Docs 12,495 13,652 1,616 4,597 10,000 10,000 427 IL-English Docs 169 645 145 166 191 484 252 Table 3: Data Statistics Jing et al. (2004) proposed an entropy measure for active learning for image retrieval task. We compared it with other measures proposed by (Settles and Craven, 2008) and found that sequence entropy (SE) is most effective for our name tagging task. We use ϕSE to represent how informative a sentence is: ϕSE (x) = − Language IL Test Docs Bengali 100 Hausa 100 Tagalog 100 Tamil 100 Thai 100 Turkish 100 Yoruba 100 Pθ (yt = m)logPθ (yt = m) each expectation-driven rule based on its precision score on a small development set of ten documents. Then we apply these rules in the priority order of their confidence values. When the results of two taggers are conflicting on either mention boundary or type, if the applied rule has high confidence we will trust its outpu"
N16-1029,P13-1106,0,0.256647,"ame identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al"
N16-1029,D15-1081,1,0.782214,"earning Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia5 . Each entry is associated with a set of types such as Company, Actor and Agent. We utilize the Abstract Meaning Representation corpus (Banarescu et al., 2013) which contains both entity type and linked KB title annotations, to automatically map 9, 514 entity types in DBPedia to three main entity types of interest: Person (PER), Location (LOC) and Organization (ORG). Then we adopt a language-independent crosslingual entity linking system (Wang et al., 2015) 4 5 to link each IL name mention to English DBPedia. This linker is based on an unsupervised quantified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowledge networks from KB. Each IL name is matched with candidate entities in English KB using name translation pairs derived from inter-lingual KB links in Wikipedia and DBPedia. We also apply the wordfor-word translation tables constructed from parallel data as described in Section 3.4 to translate some uncommon names. Then it performs semantic compariso"
N16-1029,D09-1158,0,0.00820629,"low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEnglish languages such as in Chinese (Ji and Grishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Maloney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (Béchet et al., 2000), German (Thielen, 1995), Italian (Cucchiarelli et al., 1998), Gree"
N16-1029,P02-1060,0,0.0248791,"1 35.0 43.6 47.1 * typing accuracy is computed on correctly identified names Table 5: Breakdown Scores Table 5 presents the detailed break-down scores for all languages. We can see that name identification, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as “makaran256 Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrapping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009). Name tagging has been explored for many nonEng"
N16-1029,C98-1045,0,\N,Missing
N16-1089,D15-1138,0,0.281597,"vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natural Language Communication Research Problem Definition Pr"
N16-1089,N16-1181,0,0.0468114,"ation Semantics 3 Encoder Figure 2: Our models all follow the above architecture. 1-Hot word vectors (orange) are fed as input to a Feed-Forward or Recurrent Neural Network for encoding. A semantic representation is extracted (green), which in conjunction with knowledge of the world (blue) is grounded to predict an action. can be trained independently (Sections 5.2 and 5.3) or jointly as a single End-to-End model (Section 5.4). This division of labor also allows for differing amounts of human intervention both during training and in the interpretation of actions and bears some resemblance to (Andreas et al., 2016). Specifically, we will first present results where the model predicts a fixed semantic interpretation of actions which are easily human interpretable (Encoder + Representation). In this setting, the experimenter/human then must convert the semantics to actions in the world. Second, we remove the human interpreter and train a model for Grounding and Predicting from our semantic representation. Finally, we maintain our architecture but remove the human entirely, forcing the model to both converge to and interpret its own internal semantic representation. The model architecture, regardless of ho"
N16-1089,P09-1010,0,0.0488923,"ahon et al., 2006; Yu and Siskind, 2013) but the data, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more"
N16-1089,P11-1028,0,0.022072,"ata, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natura"
N16-1089,S14-2006,0,0.0155527,"umes the ability to plan and execute individual and complex actions: slide the adidas block 2 blocks straight up . then slide it 6 block spaces to the left . The models we introduce in this paper have difficulty dealing with these kind of commands. 5 Models In order to correctly interpret commands in context, we need models that ground entities and understand spatial relations, shapes, and the compositionality of language. This is a large and fertile space for exploration. In contrast with previous work which attempts to produce deep semantic interpretations of commands (Kim and Mooney, 2012; Dukes, 2014), in this paper we explore the degree to which we can solve our communication problem using semantic free models. We are quick to note though that our framework can also assess the performance of semantic-heavy approaches. We outline here three basic neural models that provide a set of reasonable baselines for other researchers interested in solving this problem. Each approach assumes less knowledge injection than the previous. As discussed in Section 3.1, in all three models, the eventual output is a tuple specifying where to find the block to move and where to move it: (x, y, z)S and (x, y,"
N16-1089,D12-1040,0,0.012365,"d in a manner that assumes the ability to plan and execute individual and complex actions: slide the adidas block 2 blocks straight up . then slide it 6 block spaces to the left . The models we introduce in this paper have difficulty dealing with these kind of commands. 5 Models In order to correctly interpret commands in context, we need models that ground entities and understand spatial relations, shapes, and the compositionality of language. This is a large and fertile space for exploration. In contrast with previous work which attempts to produce deep semantic interpretations of commands (Kim and Mooney, 2012; Dukes, 2014), in this paper we explore the degree to which we can solve our communication problem using semantic free models. We are quick to note though that our framework can also assess the performance of semantic-heavy approaches. We outline here three basic neural models that provide a set of reasonable baselines for other researchers interested in solving this problem. Each approach assumes less knowledge injection than the previous. As discussed in Section 3.1, in all three models, the eventual output is a tuple specifying where to find the block to move and where to move it: (x, y, z"
N16-1089,N15-1015,0,0.0193808,"ar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natural Language Communication Research Problem Definition Problem-Solution Sequences. In orde"
N16-1089,P14-5010,0,0.00373747,"of each block in Imagei (a discrete representation of the world corresponding to the image), and a natural language command that a robot needs to understand in order to operate on the world in Imagei . The output consists of an Imagei+1 (what the robot should build) and the (x, y, z) coordinates of each block in Imagei+1 . The training/development/test sections of the data contain ∼177K/31K/48K tokens for the decorated blocks. The overall lexical type and token counts for our data are presented in Table 2. To compute statistics all text was lower-cased and tokenized using Stanford’s CoreNLP (Manning et al., 2014). For the MNIST configurations, digits 0-9 are present in the test data as drawn with both logos and numbered blocks. In contrast, only half the digits appear with MNIST Types Tokens Train Dev Test Total 1,506 583 645 177K 31K 48K 1,359 / 257K Blank Types Tokens 961 444 575 Command Length (l) 58K 8K 17K 1≤l≤5 5 < l ≤ 10 10 < l ≤ 20 20 < l ≤ 40 40 < l ≤ 80 1,172 / 84K Table 2: Type and token counts for the Logo and Number decorated block data sets (left) and the Blank blocks (right). # of Commands MNIST Random 81 3,817 5,752 2,028 192 0 61 995 1,329 107 Table 3: A breakdown of the number of com"
N16-1089,H93-1005,0,0.14877,"e progress in Natural Language Processing can be attributed to defining problems of broad interest (e.g. parsing and machine translation); collecting or creating publicly available corpora that encode meaningful hinput, outputi samples (e.g. Penn TreeBank and LDC Parallel Corpora); and devising simple, objective and computable evaluation metrics to automatically assess the performance of algorithms designed to solve the problems of interest, independent of the approach or technology used (e.g. ParseEval and Bleu). Previous work Most research on Human-Robot Interaction (Klingspor et al., 1997; Thompson et al., 1993; Mavridis, 2015) bridges the gap between natural language commands and the physical world via a set of pre-defined templates characterized by a small vocabulary and grammar. Progress on language in this area has largely focused on grounding visual attributes (Kollar et al., 2013; Matuszek et al., 2014) and on learning spatial relations and actions for small vocabularies with hard-coded abstract concepts (Steels and Vogt, 1997; Roy, 2002; 751 Proceedings of NAACL-HLT 2016, pages 751–761, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Guadarrama et al."
N16-1089,N15-1173,0,0.0156622,"hough robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Roth, 2011; Branavan et al., 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015) and videos (Malmaud et al., 2015; Venugopalan et al., 2015). However, to our knowledge, there is no body of work that focuses on understanding the relation between natural language and complex actions and goals or on explaining flexibly the actions taken by a robot in natural language utterances. As observed by Klingspor (1997), there is a big gap between the formulaic interactions that are typical of state-of-the-art human-robot communications and human-human interactions, which are more abstract. 3 3.1 A Framework for Human-Robot Natural Language Communication Research Problem Definition Problem-Solution Sequences. In order to build models that unde"
N16-1089,H89-1033,0,0.82265,"dyuret@ku.edu.tr Abstract As robots become increasingly ubiquituous, we need to learn to interact with them intelligently, in the same manner we interact with members of our own species. To make rapid progress in this area, we propose to use an intellectual framework that has the same ingredients that have transformed our field: appealing science problem definitions; publicly available datasets; and easily computable, objective evaluation metrics. In this paper, we study the problem of HumanRobot Natural Language Communication in a setting inspired by a traditional AI problem – blocks world (Winograd, 1972). After reviewing previous work (Section 2), we propose a novel Human-Robot Communication Problem that is testable empirically (Section 3.1) and we describe the publicly available datasets (Section 3.2) and evaluation metric that we devised to support our research (Section 6). We then introduce a set of algorithms for solving our problem and we evaluate their performance both objectively and subjectively (Sections 4–8). We propose a framework for devising empirically testable algorithms for bridging the communication gap between humans and robots. We instantiate our framework in the context of"
N16-1089,P13-1006,0,0.00952796,"world via a set of pre-defined templates characterized by a small vocabulary and grammar. Progress on language in this area has largely focused on grounding visual attributes (Kollar et al., 2013; Matuszek et al., 2014) and on learning spatial relations and actions for small vocabularies with hard-coded abstract concepts (Steels and Vogt, 1997; Roy, 2002; 751 Proceedings of NAACL-HLT 2016, pages 751–761, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Guadarrama et al., 2013). Language is sometimes grounded into simple actions (MacMahon et al., 2006; Yu and Siskind, 2013) but the data, while multimodal, is relatively formulaic, the vocabularies are small, and the grammar is constrained. Although robots have significantly increased their autonomy and ability to plan, that has not resulted, to date, in more flexible human-robot communication protocols that would enable robots to understand free-er form language and/or acquire simple and complex concepts via human-robot interactions. Recently the connection between less formulaic language and simple actions has been explored successfully in the context of simulated worlds (Branavan et al., 2009; Goldwasser and Ro"
N16-1089,Q13-1005,0,\N,Missing
P01-1014,P98-1032,1,0.889356,"Missing"
P01-1014,E99-1015,0,\N,Missing
P01-1014,C98-1032,1,\N,Missing
P01-1014,J86-3001,0,\N,Missing
P01-1030,J99-4005,1,\N,Missing
P01-1030,J93-2003,0,\N,Missing
P01-1030,P97-1047,0,\N,Missing
P01-1030,P96-1021,0,\N,Missing
P01-1030,P97-1037,0,\N,Missing
P01-1050,J93-2003,0,0.056222,"Missing"
P01-1050,1999.tmi-1.3,0,0.044439,"(1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus i"
P01-1050,P01-1030,1,0.377653,"nch word    , where B ranges from 1 to the number of words  (fertility of e ) into which e is translated. For example, the English word a b c c X ^  Z WT S _ _ S  Q S W_ S W S ` _ [`  V ] “no” in Figure 2 is a word of fertility 2 that is translated into “aucun” and “ne”. P Q R Q S A The rest of the factors denote distorsion probabilities (d), which capture the probability that words change their position when translated from one language into another; the probability of some French words being generated from an invisible English NULL element (p  ), etc. See (Brown et al., 1993) or (Germann et al., 2001) for a detailed discussion of this translation model and a description of its parameters. 3 Building a statistical translation memory Companies that specialize in producing highquality human translations of documentation and news rely often on translation memory tools to increase their productivity (Sprung, 2000). Building high-quality TMEM is an expensive process that requires many person-years of work. Since we are not in the fortunate position of having access to an existing TMEM, we decided to build one automatically. We trained IBM translation model 4 on 500,000 English-French sentence pa"
P01-1050,J99-4005,0,0.0333409,"However, if the two phrases are stored in the TMEM, producing such a translation becomes feasible. If optimal decoding algorithms capable of searching exhaustively the space of all possible translations existed, using TMEMs in the style presented in this paper would never improve the performance of a system. Our approach works because it biases the decoder to search in subspaces that are likely to yield translations of high probability, subspaces which otherwise may not be explored. The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999). It is clear that one of the main strengths of the TMEM is its ability to encode contextual, longdistance dependencies that are incongruous with the parameters learned by current context poor, reductionist channel models. Unfortunately, the criterion used by the decoder in order to choose between a translation produced starting from a gloss and one produced starting from a TMEM is biased in favor of the gloss-based translation. It is possible for the decoder to produce a perfect translation using phrases from the TMEM, and yet, to discard the perfect translation in favor of an incorrect trans"
P01-1050,1992.tmi-1.15,0,0.0434074,"lation examples that are relevant for translating unseen sentences, and modifying and integrating translation fragments to produce correct outputs. Sato (1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phra"
P01-1050,J99-1003,0,0.0165182,"Missing"
P01-1050,W99-0604,0,0.0406784,"ll partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not. A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model 1 For the rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this framework, the source"
P01-1050,C00-2172,0,0.138981,"d algorithms. els. And it can be applied in conjunction with existing translation memories. To do this, one would simply have to train the statistical model on the translation memory provided as input, determine the Viterbi alignments, and enhance the existing translation memory with word-level alignments as produced by the statistical translation model. We suspect that using manually produced TMEMs can only increase the performance as such TMEMs undergo periodic checks for quality assurance. The work that comes closest to using a statistical TMEM similar to the one we propose here is that of Vogel and Ney (2000), who automatically derive from a parallel corpus a hierarchical TMEM. The hierarchical TMEM consists of a set of transducers that encode a simple grammar. The transducers are automatically constructed: they reflect common patterns of usage at levels of abstractions that are higher than the words. Vogel and Ney (2000) do not evaluate their TMEM-based system, so it is difficult to empirically compare their approach with ours. From a theoretical perspective, it appears though that the two approaches are complementary: Vogel and Ney (2000) identify abstract patterns of usage and then use them dur"
P01-1050,P98-2230,0,0.0127621,"parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees. Veale and Way (1997) store complete sentences; new translations are generated by modifying the TMEM translation that is most similar to the input sentence. Others store phrases; new translations are produced by optimally partitioning the input into phrases that match examples from the TMEM (Maruyana and Watanabe, 1992), or by finding all partial matches and then choosing the best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language mod"
P01-1050,P01-1067,0,0.0992252,"e best possible translation using a multi-engine translation system (Brown, 1999). With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1). In this framework, the source language, let’s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993). (Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001).) In the noisy-channel framework, a monolingual corpus is used to derive a statistical language model that assigns a probability to a sequence of words or phrases, thus enabling one to distinguish between sequences of words that are grammatically correct and sequences that are not. A sentence-aligned parallel corpus is then used in order to build a probabilistic translation model 1 For the rest of this paper, we use the terms source and target languages according to the jargon specific to the noisy-channel framework. In this framework, the source language is the language into which the machin"
P01-1050,C92-4203,0,\N,Missing
P01-1050,C98-2225,0,\N,Missing
P02-1047,A00-2018,0,\N,Missing
P02-1047,W01-1605,1,\N,Missing
P02-1047,P01-1005,0,\N,Missing
P02-1057,A00-2023,0,\N,Missing
P02-1057,A00-1043,0,\N,Missing
P02-1057,P97-1003,0,\N,Missing
P02-1057,P00-1038,0,\N,Missing
P02-1057,P00-1041,0,\N,Missing
P02-1057,C96-2183,0,\N,Missing
P02-1057,W01-1605,1,\N,Missing
P02-1057,P99-1042,0,\N,Missing
P03-1003,J93-2003,0,0.00657739,"Missing"
P03-1003,A88-1019,0,0.0274773,"em using maximum entropy models that are trained on question-answer corpora. From this perspective then, the fundamental problem of question answering is that of finding spaces where the distance between questions and sentences that contain correct answers is small and where the distance between questions and sentences that contain incorrect answers is large. In this paper, we propose a new space and a new metric for computing this distance. Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997), part of speech tagging (Church, 1988), machine translation (Brown et al., 1993), information retrieval (Berger and Lafferty, 1999), and text summarization (Knight and Marcu, 2002), we develop a noisy channel model for QA. This model explains how a given sentence SA that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of questionanswer pairs (Q, SA), we can train a probabilistic model for estimating the conditional probability P(Q |SA). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR en"
P03-1003,lin-2002-web,0,0.0125711,"mely complex. They contain tens of modules that do everything from information retrieval, sentence parsing (Ittycheriah and Roukos, 2002; Hovy et al., 2001; Moldovan et al, 2002), question-type pinpointing (Ittycheriah and Roukos, 2002; Hovy et al., 2001; Moldovan et al, 2002), semantic analysis (Xu et al., Hovy et al., 2001; Moldovan et al, 2002), and reasoning (Moldovan et al, 2002). They access external resources such as the WordNet (Hovy et al., 2001, Pasca and Harabagiu, 2001, Prager et al., 2001), the web (Brill et al., 2001), structured, and semistructured databases (Katz et al., 2001; Lin, 2002; Clarke, 2001). They contain feedback loops, ranking, and re-ranking modules. Given their complexity, it is often difficult (and sometimes impossible) to understand what contributes to the performance of a system and what doesn’t. In this paper, we propose a new approach to QA in which the contribution of various resources and components can be easily assessed. The fundamental insight of our approach, which departs significantly from the current architectures, is that, at its core, a QA system is a pipeline of only two modules: • An IR engine that retrieves a set of M documents/N sentences th"
P05-1009,J99-4005,0,\N,Missing
P05-1009,J93-2003,0,\N,Missing
P05-1009,P02-1040,0,\N,Missing
P05-1009,P95-1034,0,\N,Missing
P05-1009,J98-4003,0,\N,Missing
P05-1009,W00-2004,0,\N,Missing
P05-1009,W02-2105,0,\N,Missing
P05-3023,melvin-etal-2004-creation,1,0.752599,"ive Poster and Demonstration Sessions, c pages 89–92, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ASR English Prompts or TTS English GUI: prompts, confirmations, ASR switch Prompts or TTS Farsi Dialog Manager ASR Farsi MT SMT English to Farsi Farsi to English English to Farsi Farsi to English Figure 1: Architecture of the Transonics system. The Dialogue Manager acts as the hub through which the individual components interact. our own large-scale simulated doctor-patient dialogue corpus based on recordings of medical students examining standardized patients (details in Belvin et al. 2004).1 The Farsi acoustic models r equired an eclectic approach due to the lack of existing labeled speech corpora. The approach included borrowing acoustic data from English by means of developing a sub-phonetic mapping between the two languages, as detailed in (Srinivasamurthy & Narayanan 2003), as well as use of a small existing Farsi speech corpus (FARSDAT), and our own team-internally generated acoustic data. Language modeling data was also obtained from multiple sources. The Defense Language Institute translated approximately 600,000 words of English medical dialogue data (including our stan"
P06-1011,W04-3250,0,0.0571054,"Missing"
P06-1011,J00-2004,0,0.0274164,"agments is a difficult task. It requires the ability to recognize translational equivalence in very noisy environments, namely sentence pairs that express different (although overlapping) content. However, a good solution to this problem would have a strong impact on parallel data acquisition efforts. Enabling the exploitation of corpora that do not share parallel sentences would greatly increase the amount of comparable data that can be used for SMT. 1 82 http://www-2.cs.cmu.edu/$sim$lemur Likelihood-Ratio (LLR) statistic (Dunning, 1993), which has also been used by Moore (2004a; 2004b) and Melamed (2000) as a measure of word association. Generally speaking, this statistic gives a measure of the likelihood that two samples are not independent (i.e. generated by the same probability distribution). We use it to estimate the independence of pairs of words which cooccur in our parallel corpus. If source word and target word  are independent (i.e. they are not translations of each other),      , we would expect that  i.e. the distribution of  given that is present is the same as the distribution of  when is not present. The LLR statistic gives a measure of the likelihood o"
P06-1011,W04-3243,0,0.112812,"parallel subsentential fragments is a difficult task. It requires the ability to recognize translational equivalence in very noisy environments, namely sentence pairs that express different (although overlapping) content. However, a good solution to this problem would have a strong impact on parallel data acquisition efforts. Enabling the exploitation of corpora that do not share parallel sentences would greatly increase the amount of comparable data that can be used for SMT. 1 82 http://www-2.cs.cmu.edu/$sim$lemur Likelihood-Ratio (LLR) statistic (Dunning, 1993), which has also been used by Moore (2004a; 2004b) and Melamed (2000) as a measure of word association. Generally speaking, this statistic gives a measure of the likelihood that two samples are not independent (i.e. generated by the same probability distribution). We use it to estimate the independence of pairs of words which cooccur in our parallel corpus. If source word and target word  are independent (i.e. they are not translations of each other),      , we would expect that  i.e. the distribution of  given that is present is the same as the distribution of  when is not present. The LLR statistic gives a"
P06-1011,J05-4003,1,0.815495,"slated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. 1 Introduction Recently, there has been a surge of interest in the automatic creation of parallel corpora. Several researchers (Zhao and Vogel, 2002; Vogel, 2003; Resnik and Smith, 2003; Fung and Cheung, 2004a; Wu and Fung, 2005; Munteanu and Marcu, 2005) have shown how fairly good-quality parallel sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most language pairs. Since comparable corpora exist in large quantities and for many languages – tens of thousands of words of news describing the same events are produced daily – the ability to exploit them for parallel data acquisition is highly beneficial for the S"
P06-1011,J03-1002,0,0.00756262,"gnment by greedily linking each English word with its best translation candidate from the Romanian sentence. For each of the linked target words, the corresponding signal value is the probability of the link (there can be at most one link for each target word). Thus, if target word  is linked to source word , the signal value cor   responding to  is  (the distribution described in Section 2.2), i.e. the probability that  is the translation of . For the remaining target words, the signal value should reflect the probability that they are not Word-align the parallel corpus. Following Och and Ney (2003), we run GIZA++ in both directions, and then symmetrize the alignments using the refined heuristic.  Compute all LLR scores. There will be an LLR score for each pair of words which are linked at least once in the word-aligned corpus     Classify all  as either        (positive association) if    ,  or   (negative association) otherwise.  For   each    , compute the normalizing factors   and    .      Divide all   terms by the corresponding normalizing factors to obtain     .  Divide all"
P06-1011,J04-4002,0,0.0958659,"Missing"
P06-1011,P02-1040,0,0.0886944,"Missing"
P06-1011,P99-1067,0,0.832372,"e; since most of the parallel data in this corpus exists at sentence level, the extracted fragments cannot bring much additional knowledge. The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score. This indicates that LLR-Lex is a higher-quality lexicon than GIZALex, and an important component of our method. 4 Previous Work Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004). Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web. Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document. Thus, the previous research most relevant to this paper is that aimed at mining comparable corpora for parallel sentences. The earliest efforts in this direction are those of"
P06-1011,J03-3002,0,0.795251,"approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. 1 Introduction Recently, there has been a surge of interest in the automatic creation of parallel corpora. Several researchers (Zhao and Vogel, 2002; Vogel, 2003; Resnik and Smith, 2003; Fung and Cheung, 2004a; Wu and Fung, 2005; Munteanu and Marcu, 2005) have shown how fairly good-quality parallel sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most language pairs. Since comparable corpora exist in large quantities and for many languages – tens of thousands of words of news describing the same events are produced daily – the ability to exp"
P06-1011,H01-1033,0,0.0162295,"two association types. The first type of association will provide us with our (cleaner) lexicon, while the second will allow us to estimate probabilities of words not being translations of each other. Before describing our new method more formally, we address the notion of word cooccurrence. In the work of Moore (2004a) and Melamed (2000), two words cooccur if they are present in a pair of aligned sentences in the parallel training corpus. However, most of the words from aligned sentences are actually unrelated; therefore, this is a rather weak notion of cooccurrence. We follow Resnik et. al (2001) and adopt a stronger definition, based not on sentence alignment but on word alignment: two words cooccur if they are linked together in the word-aligned parallel training corpus. We thus make use of the significant amount of knowledge brought in by the word alignment procedure.  We compute  , the LLR score for words  and , using the formula presented by Moore (2004b), which we do not repeat here due to lack of space. We then use these values to compute two conditional probability distributions:     , the probability that source word transUsing Log-Likelihood-Ratios to Es"
P06-1011,C04-1089,0,0.0121572,"xtracted fragments cannot bring much additional knowledge. The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score. This indicates that LLR-Lex is a higher-quality lexicon than GIZALex, and an important component of our method. 4 Previous Work Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004). Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web. Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document. Thus, the previous research most relevant to this paper is that aimed at mining comparable corpora for parallel sentences. The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003). Both methods extend algorithms des"
P06-1011,C04-1122,0,0.0156002,"cannot bring much additional knowledge. The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score. This indicates that LLR-Lex is a higher-quality lexicon than GIZALex, and an important component of our method. 4 Previous Work Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004). Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web. Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document. Thus, the previous research most relevant to this paper is that aimed at mining comparable corpora for parallel sentences. The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003). Both methods extend algorithms designed to perform sentence al"
P06-1011,P03-1010,0,0.048655,"ehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004). Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web. Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document. Thus, the previous research most relevant to this paper is that aimed at mining comparable corpora for parallel sentences. The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003). Both methods extend algorithms designed to perform sentence alignment of parallel texts: they use dynamic programming to do sentence alignment of documents hypothesized to be similar. These approaches are only applicable to corpora which are at most “noisy-parallel”, i.e. 5 Conclusion We have presented a simple and effective method for extracting sub-sentential fragments from comparable corpora. We also presented a method for computing a probabilistic lexicon based on the LLR statistic, which produces a higher quality lexicon. We showed that using this lexicon helps improve the precision of"
P06-1011,I05-1023,0,0.28395,"e sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. 1 Introduction Recently, there has been a surge of interest in the automatic creation of parallel corpora. Several researchers (Zhao and Vogel, 2002; Vogel, 2003; Resnik and Smith, 2003; Fung and Cheung, 2004a; Wu and Fung, 2005; Munteanu and Marcu, 2005) have shown how fairly good-quality parallel sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most language pairs. Since comparable corpora exist in large quantities and for many languages – tens of thousands of words of news describing the same events are produced daily – the ability to exploit them for parallel data acquisition is"
P06-1011,J93-2003,0,0.0098686,"hypothesis. The LLR score of a word pair is low when these two distributions are very similar (i.e. the words are independent), and high otherwise (i.e. the words are strongly associated). However, high LLR scores can indicate ei    ) ther a positive association (i.e.  or a negative one; and we can distinguish between    them by checking whether    . Figure 3: A Parallel Fragment Extraction System tomatically from the same initial parallel corpus. The first one, GIZA-Lex, is obtained by running the GIZA++2 implementation of the IBM word alignment models (Brown et al., 1993) on the initial parallel corpus. One of the characteristics of this lexicon is that each source word is associated with many possible translations. Although most of its high-probability entries are good translations, there are a lot of entries (of non-negligible probability) where the two words are at most related. As an example, in our GIZA-Lex lexicon, each source word has an average of 12 possible translations. This characteristic is useful for the first two stages of the extraction pipeline, which are not intended to be very precise. Their purpose is to accept most of the existing parallel"
P06-1011,J93-1003,0,0.181221,"Missing"
P06-1011,W04-3208,0,0.338957,"ch segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. 1 Introduction Recently, there has been a surge of interest in the automatic creation of parallel corpora. Several researchers (Zhao and Vogel, 2002; Vogel, 2003; Resnik and Smith, 2003; Fung and Cheung, 2004a; Wu and Fung, 2005; Munteanu and Marcu, 2005) have shown how fairly good-quality parallel sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most language pairs. Since comparable corpora exist in large quantities and for many languages – tens of thousands of words of news describing the same events are produced daily – the ability to exploit them for parallel"
P06-1011,C04-1151,0,0.669999,"ch segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. 1 Introduction Recently, there has been a surge of interest in the automatic creation of parallel corpora. Several researchers (Zhao and Vogel, 2002; Vogel, 2003; Resnik and Smith, 2003; Fung and Cheung, 2004a; Wu and Fung, 2005; Munteanu and Marcu, 2005) have shown how fairly good-quality parallel sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most language pairs. Since comparable corpora exist in large quantities and for many languages – tens of thousands of words of news describing the same events are produced daily – the ability to exploit them for parallel"
P06-1011,P98-1069,0,0.75986,"t improve performance; since most of the parallel data in this corpus exists at sentence level, the extracted fragments cannot bring much additional knowledge. The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score. This indicates that LLR-Lex is a higher-quality lexicon than GIZALex, and an important component of our method. 4 Previous Work Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004). Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web. Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document. Thus, the previous research most relevant to this paper is that aimed at mining comparable corpora for parallel sentences. The earliest efforts in this direction a"
P06-1011,P04-1067,0,0.117412,"t sentence level, the extracted fragments cannot bring much additional knowledge. The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score. This indicates that LLR-Lex is a higher-quality lexicon than GIZALex, and an important component of our method. 4 Previous Work Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004). Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web. Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document. Thus, the previous research most relevant to this paper is that aimed at mining comparable corpora for parallel sentences. The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003). Both methods ex"
P06-1011,E03-1050,0,\N,Missing
P06-1011,C98-1066,0,\N,Missing
P06-1011,P04-1066,0,\N,Missing
P06-1039,H05-1086,0,0.0118823,"CA 90292 me@hal3.name,marcu@isi.edu Abstract relevant documents for a given query. For both of these tasks, BAYE S UM performs well, even when the underlying retrieval model is noisy. The idea of leveraging known relevant documents is known as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks. Viewed from the perspective of IR, our work can be interpreted in two ways. First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005). Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner. That is, BAYE S UM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998). We present BAYE S UM (for “Bayesian summarization”), a model for sentence extraction in query-focused summarization. BAYE S UM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYE S UM is not afflicted by the paucity of information in short queries. We show that ap"
P06-1039,H05-1115,0,0.04623,"Missing"
P06-1097,P05-1057,0,0.214078,"other built using additional English news data. We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system. Table 8 shows our results. We report BLEU (Papineni et al., 2001) multiplied by 100. We also show the F-measure after heuristic symmetrization of the alignment test sets. The table shows that F (A, S, α) = 1 α Precision(A,S) (1−α) + Recall (A,S) (3) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 4 We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. 774 S YSTEM A/E U NSUP. M ODEL 4 UNION A/E EMD 3 UNION F/E U NSUP. M ODEL 4 REFINED F/E EMD 2 REFINED BLEU 49.16 50.84 30.63 31.56 F- MEASURE 64.6 69.4 76.4 81.2 Table 8: Evaluation of Translation Quality tions with a relatively small number of parameters. An initial model is es"
P06-1097,H05-1011,0,0.612532,"We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system. Table 8 shows our results. We report BLEU (Papineni et al., 2001) multiplied by 100. We also show the F-measure after heuristic symmetrization of the alignment test sets. The table shows that F (A, S, α) = 1 α Precision(A,S) (1−α) + Recall (A,S) (3) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 4 We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. 774 S YSTEM A/E U NSUP. M ODEL 4 UNION A/E EMD 3 UNION F/E U NSUP. M ODEL 4 REFINED F/E EMD 2 REFINED BLEU 49.16 50.84 30.63 31.56 F- MEASURE 64.6 69.4 76.4 81.2 Table 8: Evaluation of Translation Quality tions with a relatively small number of parameters. An initial model is estimated in a supervised fashion using the l"
P06-1097,J03-1002,0,0.113385,"4 F- MEASURE F TO E 65.6 / 60.5 73.8 / 75.1 F- MEASURE E TO F 53.6 / 50.2 74.2 / 73.5 F- MEASURE B EST S YMM . 69.1 / 64.6 ( UNION ) 76.5 / 76.4 ( REFINED ) Table 2: Baseline Results. F-measures are presented on both the alignment discriminative training set and the alignment test set sub-corpora, separated by /. consisting of links from one English word to zero or more Foreign words. It is standard practice to improve the final alignments by combining the “F to E” and “E to F” directions using symmetrization heuristics. We use the “union”, “refined” and “intersection” heuristics defined in (Och and Ney, 2003) which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment. In Table 2, we report the best symmetrized results. The low F-measure scores of the baselines motivate our work. discriminative training set and alignment test set. Translation quality is evaluated by translating a held-out translation test set. An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/Engl"
P06-1097,J93-2003,0,0.0595758,"Missing"
P06-1097,P03-1021,0,0.726197,"nion”, “refined” and “intersection” heuristics defined in (Och and Ney, 2003) which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment. In Table 2, we report the best symmetrized results. The low F-measure scores of the baselines motivate our work. discriminative training set and alignment test set. Translation quality is evaluated by translating a held-out translation test set. An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC. We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998). To train our baseline systems we follow a standard procedure. The models were trained two times, first using French or Arabic as the source language and then using English as the source language. For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Mod"
P06-1097,P04-1023,0,0.624189,"57.0 ( UNION ) 72.0 / 66.4 ( UNION ) 74.1 / 68.1 ( UNION ) 74.7 / 69.4 ( UNION ) 76.4 / 77.3 ( REFINED ) 79.6 / 80.4 ( REFINED ) 79.9 / 81.2 ( REFINED ) Table 7: Semi-Supervised Training Task F-measure the new algorithm4 . Like Ittycheriah and Roukos (2005), we converted the alignment discriminative training corpus links into a special corpus consisting of parallel sentences where each sentence consists only of a single word involved in the link. We found that the information in the links was “washed out” by the rest of the data and resulted in no change in the alignment test set’s F-Measure. Callison-Burch et al. (2004) showed in their work on combining alignments of lower and higher quality that the alignments of higher quality should be given a much higher weight than the lower quality alignments. Using this insight, we found that adding 10,000 copies of the special corpus to our training data resulted in the highest alignment test set gain, which was a small gain of 0.6 F-Measure. This result suggests that while the link information is useful for improving FMeasure, our improved methods for training are producing much larger improvements. 5 our algorithm produces heuristically symmetrized final alignments"
P06-1097,2001.mtsummit-papers.68,0,0.0129094,"tion of EMD were used to build phrasal SMT systems, as were the symmetrized Model 4 alignments (the baseline). Aside from the final alignment, all other resources were held constant between the baseline and contrastive SMT systems, including those based on lower level alignments models such as IBM Model 1. For all of our experiments, we use two language models, one built using the English portion of the training data and the other built using additional English news data. We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system. Table 8 shows our results. We report BLEU (Papineni et al., 2001) multiplied by 100. We also show the F-measure after heuristic symmetrization of the alignment test sets. The table shows that F (A, S, α) = 1 α Precision(A,S) (1−α) + Recall (A,S) (3) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 4 We would"
P06-1097,P03-1012,0,0.0569136,"ng constraints on clusters. For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters. with the baseline (with the exception of (Moore, 2005)). We interleave discriminative training with EM and are therefore performing semi-supervised training. We show that semi-supervised training leads to better word alignments than running unsupervised training followed by discriminative training. Another important difference with previous work is that we are concerned with generating many-to-many word alignments. Cherry and Lin (2003) and Taskar et al. (2005) compared their results with Model 4 using “intersection” by looking at AER (with the “Sure” versus “Possible” link distinction), and restricted themselves to considering 1-to-1 alignments. However, “union” and “refined” alignments, which are many-to-many, are what are used to build competitive phrasal SMT systems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003). (Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically,"
P06-1097,H05-1010,0,0.502317,"m BLEU (Och, 2003) for 25 iterations individually for each system. Table 8 shows our results. We report BLEU (Papineni et al., 2001) multiplied by 100. We also show the F-measure after heuristic symmetrization of the alignment test sets. The table shows that F (A, S, α) = 1 α Precision(A,S) (1−α) + Recall (A,S) (3) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 4 We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. 774 S YSTEM A/E U NSUP. M ODEL 4 UNION A/E EMD 3 UNION F/E U NSUP. M ODEL 4 REFINED F/E EMD 2 REFINED BLEU 49.16 50.84 30.63 31.56 F- MEASURE 64.6 69.4 76.4 81.2 Table 8: Evaluation of Translation Quality tions with a relatively small number of parameters. An initial model is estimated in a supervised fashion using the labeled data, and this"
P06-1097,C96-2141,0,0.855901,"mponents of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC. We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998). To train our baseline systems we follow a standard procedure. The models were trained two times, first using French or Arabic as the source language and then using English as the source language. For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4. We quantify the quality of the resulting hypothesized alignments with F-measure using the manually aligned sets. We present the results for three different conditions in Table 2. For the “F to E” direction the models assign non-zero probability to alignments consisting of links from one Foreign word to zero or more English words, while for “E to F” the models assign non-zero probability to alignments 3 Improving Word Alignments 3.1 Discriminative Reranking of the IBM Models We reinterpret the five groups of parameters of Model 4 listed in the first five lines of"
P06-1097,H05-1012,0,0.639419,"TARTING P OINT A/E EMD: I TERATION 1 A/E EMD: I TERATION 2 A/E EMD: I TERATION 3 F/E S TARTING P OINT F/E EMD: I TERATION 1 F/E EMD: I TERATION 2 F- MEASURE F TO E 58.6 / 54.4 68.4 / 62.2 69.8 / 63.1 70.6 / 65.4 72.4 / 73.9 78.7 / 80.2 79.4 / 80.5 F- MEASURE E TO F 47.7 / 39.9 61.6 / 57.7 64.1 / 59.5 64.3 / 59.2 71.5 / 71.8 79.3 / 79.6 79.8 / 80.5 B EST S YMM . 62.1 / 57.0 ( UNION ) 72.0 / 66.4 ( UNION ) 74.1 / 68.1 ( UNION ) 74.7 / 69.4 ( UNION ) 76.4 / 77.3 ( REFINED ) 79.6 / 80.4 ( REFINED ) 79.9 / 81.2 ( REFINED ) Table 7: Semi-Supervised Training Task F-measure the new algorithm4 . Like Ittycheriah and Roukos (2005), we converted the alignment discriminative training corpus links into a special corpus consisting of parallel sentences where each sentence consists only of a single word involved in the link. We found that the information in the links was “washed out” by the rest of the data and resulted in no change in the alignment test set’s F-Measure. Callison-Burch et al. (2004) showed in their work on combining alignments of lower and higher quality that the alignments of higher quality should be given a much higher weight than the lower quality alignments. Using this insight, we found that adding 10,0"
P06-1097,W98-1426,0,\N,Missing
P06-1097,2003.mtsummit-papers.53,0,\N,Missing
P06-1097,W04-3208,0,\N,Missing
P06-1097,soricut-etal-2002-using,1,\N,Missing
P06-1097,N04-1014,0,\N,Missing
P06-1097,J93-1004,0,\N,Missing
P06-1097,W02-2103,0,\N,Missing
P06-1097,J91-1004,0,\N,Missing
P06-1097,W95-0115,0,\N,Missing
P06-1097,W99-0604,0,\N,Missing
P06-1097,W99-0906,0,\N,Missing
P06-1097,J97-2001,0,\N,Missing
P06-1097,N04-1035,1,\N,Missing
P06-1097,J99-4005,0,\N,Missing
P06-1097,1994.amta-1.18,0,\N,Missing
P06-1097,1999.tc-1.8,0,\N,Missing
P06-1097,W99-0626,0,\N,Missing
P06-1097,W96-0501,0,\N,Missing
P06-1097,W95-0114,0,\N,Missing
P06-1097,A00-2023,0,\N,Missing
P06-1097,J98-1001,0,\N,Missing
P06-1097,J97-4005,0,\N,Missing
P06-1097,W96-0208,0,\N,Missing
P06-1097,C00-2172,0,\N,Missing
P06-1097,C00-2135,0,\N,Missing
P06-1097,C00-2105,0,\N,Missing
P06-1097,C94-1007,0,\N,Missing
P06-1097,W00-0801,0,\N,Missing
P06-1097,W06-1626,0,\N,Missing
P06-1097,C94-2183,0,\N,Missing
P06-1097,C00-2089,0,\N,Missing
P06-1097,W98-1230,0,\N,Missing
P06-1097,2002.tmi-papers.20,0,\N,Missing
P06-1097,W02-1021,0,\N,Missing
P06-1097,N03-2036,0,\N,Missing
P06-1097,W01-1407,0,\N,Missing
P06-1097,P98-1069,0,\N,Missing
P06-1097,C98-1066,0,\N,Missing
P06-1097,J03-3002,0,\N,Missing
P06-1097,P00-1059,0,\N,Missing
P06-1097,W98-1005,0,\N,Missing
P06-1097,C00-1007,0,\N,Missing
P06-1097,C02-1012,0,\N,Missing
P06-1097,C96-2098,0,\N,Missing
P06-1097,J07-3002,1,\N,Missing
P06-1097,W01-1412,0,\N,Missing
P06-1097,N01-1020,0,\N,Missing
P06-1097,J90-2002,0,\N,Missing
P06-1097,P03-1054,0,\N,Missing
P06-1097,P02-1040,0,\N,Missing
P06-1097,P95-1026,0,\N,Missing
P06-1097,P95-1034,0,\N,Missing
P06-1097,W01-1409,0,\N,Missing
P06-1097,P99-1067,0,\N,Missing
P06-1097,J94-4003,0,\N,Missing
P06-1097,N06-1031,0,\N,Missing
P06-1097,P98-1116,0,\N,Missing
P06-1097,C98-1112,0,\N,Missing
P06-1097,P02-1039,0,\N,Missing
P06-1097,1995.tmi-1.21,0,\N,Missing
P06-1097,P01-1067,0,\N,Missing
P06-1097,J95-4004,0,\N,Missing
P06-1097,P03-1011,0,\N,Missing
P06-1097,P01-1050,1,\N,Missing
P06-1097,P93-1003,0,\N,Missing
P06-1097,P91-1034,0,\N,Missing
P06-1097,J98-1004,0,\N,Missing
P06-1097,knight-al-onaizan-1998-translation,0,\N,Missing
P06-1097,N06-1001,1,\N,Missing
P06-1097,J96-4002,0,\N,Missing
P06-1097,J04-2003,0,\N,Missing
P06-1097,P04-1067,0,\N,Missing
P06-1097,P03-1057,0,\N,Missing
P06-1097,A00-1031,0,\N,Missing
P06-1097,N04-1022,0,\N,Missing
P06-1097,P06-1055,0,\N,Missing
P06-1097,W03-0430,0,\N,Missing
P06-1097,P03-2041,0,\N,Missing
P06-1097,P05-1074,0,\N,Missing
P06-1097,P99-1068,0,\N,Missing
P06-1097,P96-1021,0,\N,Missing
P06-1097,P02-1051,0,\N,Missing
P06-1097,N03-1017,1,\N,Missing
P06-1097,P02-1038,0,\N,Missing
P06-1097,P97-1037,0,\N,Missing
P06-1097,J97-3002,0,\N,Missing
P06-1097,W02-1039,0,\N,Missing
P06-1097,P04-1068,0,\N,Missing
P06-1097,J03-1005,0,\N,Missing
P06-1097,P06-1121,1,\N,Missing
P06-1097,J08-3004,0,\N,Missing
P06-1097,N04-1021,0,\N,Missing
P06-1097,P97-1017,0,\N,Missing
P06-1097,2006.iwslt-evaluation.12,0,\N,Missing
P06-1097,P03-1020,0,\N,Missing
P06-1097,W01-0504,0,\N,Missing
P06-1097,W00-2004,0,\N,Missing
P06-1097,fleming-cohen-2000-mixed,0,\N,Missing
P06-1097,N06-1033,0,\N,Missing
P06-1097,P00-1056,0,\N,Missing
P06-1097,P97-1047,0,\N,Missing
P06-1097,W00-1401,0,\N,Missing
P06-1097,P96-1029,0,\N,Missing
P06-1097,P01-1030,1,\N,Missing
P06-1121,N04-1014,1,\N,Missing
P06-1121,N04-1035,1,\N,Missing
P06-1121,C00-2092,0,\N,Missing
P06-1121,P01-1067,1,\N,Missing
P06-1121,J04-4002,0,\N,Missing
P06-1121,P05-1033,0,\N,Missing
P06-1121,J97-3002,0,\N,Missing
P06-1121,W02-1039,0,\N,Missing
P06-1121,J08-3004,1,\N,Missing
P06-1121,N06-1033,1,\N,Missing
P06-1139,W00-2004,0,0.02382,"resent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep,"
P06-1139,W02-2105,0,0.0549781,"e realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGEN"
P06-1139,W03-0501,0,0.155099,"the WIDL-expressions with -gram language model distributions. The output presented in Figure 4 is the most likely headline realization produced by our system. &gt; &gt;  4 Headline Generation using WIDL-expressions We employ the WIDL formalism (Section 2) and the WIDL-NGLM-A V algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-expressions for Headline Generation. We generate WIDLexpressions starting from an input document. First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003). This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document. We associate probability distributions with these phrases using their frequency (we assume QÃ ê  Ã &  M? 3 · Q/ Headline Generation Evaluation. To evaluate the ac"
P06-1139,P95-1034,0,0.0402743,"rocessing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and sema"
P06-1139,N03-1017,1,0.0296461,"Missing"
P06-1139,koen-2004-pharaoh,0,0.0190861,"= corresponding to a Chinese span = , we use a probabilistic beam Z and a histogram beam : to beam out low probability translation alternatives. At this point, each = span is “tiled” with likely transla= tions = taken from the translation table. Tiles that are adjacent are joined together in  by ] &gt; a a larger &gt;[Rtile operator, where  . That is, reordering of opthe component tiles are permitted by the erators (assigned non-zero probability), but the longer the movement from the original order of the tiles, the lower the probability. (This distortion model is similar with the one used in (Koehn, 2004).) When multiple tiles are available for the , they are joined by a operasame span tor, where is specified by the probability distributions specified in the translation table. Usually, statistical phrase-based translation tables specify not only one, but multiple distributions that account for context preferences. In our experiments, we consider four probability distributions: &apos; (&apos; (&apos;Ya &apos;ca ^`_ B B_ ^ .b ^`_ B , and . b B3_ ^ , where ^ and B are Chinese-English phrase translations as they appear in the translation table. In Figure 6, we show an example of WIDL-expression created by this algo"
P06-1139,W04-1013,0,0.00676458,"2.9 SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Abstractive Keywords Webcl WIDL-A  Figure 5: Headlines generated automatically using a WIDL-based sentence realization system. Table 1: Headline generation evaluation. We compare extractive algorithms against abstractive algorithms, including our WIDL-based algorithm. and the other half is used as test set (273 documents). We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGE (Lin, 2004). For each input document, we train two language models, using the SRI Language Model Toolkit (with modified Kneser-Ney smoothing). A general trigram language model, trained on 170M English words from the Wall Street Journal, is used to model fluency. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to p"
P06-1139,P03-1021,0,0.00448332,"on 170M English words from the Wall Street Journal, is used to model fluency. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to produce headlines that have restrictions in the number of words allowed (10, in our case). The interpolation weights A (Equation 2) are trained using discriminative training (Och, 2003) using ROUGE as the objective function, on the development set. The results are presented in Table 1. We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch). For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words.  HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al., 2003), and Topiary  is our implementation of the Topiary system (Zajic e"
P06-1139,P02-1040,0,0.0713665,"Missing"
P06-1139,P05-1009,1,0.934841,"n be written under a log-linear future (admissible) scores for the bIceGflgkh states. model as in Equation 1: The algorithm PUSH es each state from the current bdc5eGfNgih into a priority queue m , which sorts EGFIH J? A J &gt; J DB C DB (1) the states according to their total score (current n ?J G E I F H A J &gt; J DB .LK admissible). In the next iteration, XZY5[^]o_la is a singleton set containing the state POP ed out from the We can formulate the search problem of finding top of m . The admissible heuristic function we use the most probable realization B under this model is the one defined in (Soricut and Marcu, 2005), as shown in Equation 2, and therefore we do not using Equation 1 (unnormalized) for computing need to be concerned about computing expensive the event costs. Given the existence of the adnormalization factors. missible heuristic and the monotonicity property ? J NMPORQ . F C DB NMPOSQ . F EGFIH A J &gt; J DB (2) of the unfolding provided by the priority queue m , the proof for A V optimality (Russell and Norvig, = For a given WIDL-expression over , the set 1995) guarantees that WIDL-NGLM-A V finds a is defined by  , and feature function path in 9qp that provides an optimal solution.  C  ¶"
P06-1139,2003.mtsummit-papers.20,0,0.0236122,"onsiderable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc., for Penman and FUF), syntactic relations (such as subject, object, premod, etc., for HALogen), or lexical dependencies (Fergus, Amalgam). Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications. Second, most of the recent"
P06-1139,P01-1030,1,\N,Missing
P06-2103,W02-2111,0,0.0771219,"Missing"
P06-2103,J04-4001,0,0.0189338,"anner that takes advantage of their individual strengths. An integral part of this framework are algorithms for searching and training these stochastic coherence models. We evaluate the performance of our models and algorithms and show empirically that utilitytrained log-linear coherence models outperform each of the individual coherence models considered. 1 Introduction Various theories of discourse coherence (Mann and Thompson, 1988; Grosz et al., 1995) have been applied successfully in discourse analysis (Marcu, 2000; Forbes et al., 2001) and discourse generation (Scott and de Souza, 1990; Kibble and Power, 2004). Most of these efforts, however, have limited applicability. Those that use manually written rules model only the most visible discourse constraints (e.g., the discourse connective “although” marks a CONCESSION relation), while being oblivious to fine-grained lexical indicators. And the methods that utilize manually annotated corpora (Carlson et al., 2003; Karamanis et al., 2004) and supervised learning algorithms have high costs associated with the annotation procedure, and cannot be easily adapted to different domains and genres. In contrast, more recent research has focused on stochastic a"
P06-2103,P03-1069,0,0.429431,"ntion, using large collections of existing human-authored documents. These models are attractive due to their increased scalability and portability. As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005)? Because the problem is NP-complete (Althaus et al., 2005), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful. In this paper, we propose an A search algorithm for the discourse ordering problem that comes with strong theoretical guarantees. For a wide range of practical problems (discourse order"
P06-2103,W98-1411,0,0.131518,"Missing"
P06-2103,P05-1018,0,0.268516,"ting human-authored documents. These models are attractive due to their increased scalability and portability. As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005)? Because the problem is NP-complete (Althaus et al., 2005), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful. In this paper, we propose an A search algorithm for the discourse ordering problem that comes with strong theoretical guarantees. For a wide range of practical problems (discourse ordering of up to 15 units), the algorithm finds an optim"
P06-2103,J91-1002,0,0.0331286,"rties, similar to those stipulated by Centering Theory (Grosz et al., 1995). Their model learns distribution patterns for transitions between discourse entities that are abstracted into their syntactic roles – subject (S), object (O), other (X), missing (-). The feature values are computed using an entity-grid representation for the discourse that records the syntactic role of each entity as it appears in each sentence. Also, salient entities are differentiated from casually occurring entities, based on the widely used assumption that occurrence frequency correlates with discourse prominence (Morris and Hirst, 1991; Grosz et al., 1995). We exclude the coreference information from this model, as the discourse ordering problem cannot accommodate current coreference solutions, which assume a pre-specified order (Ng, 2005). In the jargon of (Barzilay and Lapata, 2005), the model we implemented is called Syntax+Salience. The probability assigned to a text AAB""   . by this Entity-Based model (henceforth called EB) can be locally computed (i.e., at sentence transition level) using C feature functions, as follows: 0 ED    "" = #$  37F $ HG 7 I 7  #;   #  0  Here, I 7  #;   #  are"
P06-2103,N04-1015,0,0.364034,"arge collections of existing human-authored documents. These models are attractive due to their increased scalability and portability. As each of these stochastic models captures different aspects of coherence, an important question is whether we can combine them in a model capable of exploiting all coherence indicators. A frequently used testbed for coherence models is the discourse ordering problem, which occurs often in text generation, complex question answering, and multi-document summarization: given discourse units, what is the most coherent ordering of them (Marcu, 1996; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005)? Because the problem is NP-complete (Althaus et al., 2005), it is critical how coherence model evaluation is intertwined with search: if the search for the best ordering is greedy and has many errors, one is not able to properly evaluate whether a model is better than another. If the search is exhaustive, the ordering procedure may take too long to be useful. In this paper, we propose an A search algorithm for the discourse ordering problem that comes with strong theoretical guarantees. For a wide range of practical problems (discourse ordering of up to 15 units),"
P06-2103,P05-1020,0,0.0314204,", object (O), other (X), missing (-). The feature values are computed using an entity-grid representation for the discourse that records the syntactic role of each entity as it appears in each sentence. Also, salient entities are differentiated from casually occurring entities, based on the widely used assumption that occurrence frequency correlates with discourse prominence (Morris and Hirst, 1991; Grosz et al., 1995). We exclude the coreference information from this model, as the discourse ordering problem cannot accommodate current coreference solutions, which assume a pre-specified order (Ng, 2005). In the jargon of (Barzilay and Lapata, 2005), the model we implemented is called Syntax+Salience. The probability assigned to a text AAB""   . by this Entity-Based model (henceforth called EB) can be locally computed (i.e., at sentence transition level) using C feature functions, as follows: 0 ED    "" = #$  37F $ HG 7 I 7  #;   #  0  Here, I 7  #;   #  are feature values, and G 7 are Local Models of Discourse Coherence Stochastic local models of coherence work under the assumption that well-formed discourse can be characterized in terms of specific distribution"
P06-2103,P03-1021,0,0.00442779,"to the reference order). The range of BLEU scores is between 0 (the worst) and 1 (the best). We run different discriminative training sessions using TAU and BLEU, and train two different sets of the + parameters for Equation 1. The log-linear models thus obtained are called Loglinear   and Log-linear    , respectively. Utility-based Training In addition to the modeling problem, we must also address the training problem, which amounts to finding appropriate values for the   parameters from Equation 1. The solution we employ here is the discriminative training procedure of Och (2003). This procedure learns an optimal setting of the A parameters using as optimality criterion the utility of the proposed solution. There are two necessary ingredients to implement Och’s (2003) training procedure. First, it needs a search algorithm that is able to produce ranked  -best lists of the most promising candidates in a reasonably fast manner (Huang  -best and Chiang, 2005). We accommodate  8H8 algorithm, computation within the IDL-CH-HB which decodes bag-of-units IDL-expressions at an average speed of 75.4 sec./exp. on a 3.0 GHz CPU Linux machine, for an average input of 11.5 uni"
P06-2103,J93-2003,0,0.00424429,"cal models of coherence work under the assumption that well-formed discourse can be characterized in terms of specific distributions of local recurring patterns. These distributions can be defined at the lexical level or entity-based levels. Word-Coocurrence Coherence Models. We propose a new coherence model, inspired by (Knight, 2003), that models the intuition that the usage of certain words in a discourse unit (sentence) tends to trigger the usage of other words in subsequent discourse units. (A similar intuition holds for the Machine Translation models generically known as the IBM models (Brown et al., 1993), which assume that certain words in a source language sentence tend to trigger the usage of certain words in a target language translation of that sentence.) We train models able to recognize local recurring patterns of word usage across sentences in an unsupervised manner, by running an ExpectationMaximization (EM) procedure over pairs of consecutive sentences extracted from a large collection of training documents1 . We expect EM to detect and assign higher probabilities to recurring word patterns compared to casually occurring word patterns. A local coherence model based on IBM Model 1 ass"
P06-2103,P02-1040,0,0.0887367,"of our algorithms and models in a well-controlled setting. As described in Section 3, our framework can be used in applications such as multi-document summarization. In fact, Barzilay et al. (2002) formulate the multi-document summarization problem as an information ordering problem, and show that naive ordering algorithms such as majority ordering (select most frequent orders across input documents) and chronological ordering (order facts according to publication date) do not always yield coherent summaries. BLEU. One of the most successful metrics for judging machine-generated text is BLEU (Papineni et al., 2002). It counts the number of unigram, bigram, trigram, and four-gram matches between hypothesis and reference, and combines them using geometric mean. For the discourse ordering problem, we represent hypotheses and references by index sequences (e.g., “4 2 3 1” is a hypothesis order over four discourse units, in which the first and last units have been swapped with reData. For training and testing, we use documents from two different genres: newspaper articles and accident reports written by government officials (Barzilay and Lapata, 2005). The first collection (henceforth called EARTHQUAKES) con"
P06-2103,J95-2003,0,0.968336,"benefit from their complementary strengths. The model comWe describe a generic framework for integrating various stochastic models of discourse coherence in a manner that takes advantage of their individual strengths. An integral part of this framework are algorithms for searching and training these stochastic coherence models. We evaluate the performance of our models and algorithms and show empirically that utilitytrained log-linear coherence models outperform each of the individual coherence models considered. 1 Introduction Various theories of discourse coherence (Mann and Thompson, 1988; Grosz et al., 1995) have been applied successfully in discourse analysis (Marcu, 2000; Forbes et al., 2001) and discourse generation (Scott and de Souza, 1990; Kibble and Power, 2004). Most of these efforts, however, have limited applicability. Those that use manually written rules model only the most visible discourse constraints (e.g., the discourse connective “although” marks a CONCESSION relation), while being oblivious to fine-grained lexical indicators. And the methods that utilize manually annotated corpora (Carlson et al., 2003; Karamanis et al., 2004) and supervised learning algorithms have high costs a"
P06-2103,P05-1009,1,0.821264,"computed 1 as current admissible) to control the unfolding of an input IDL-graph, by processing, at each unfolding step, the most inexpensive state (extracted from the top of G ). The admissibility of the future costs and the monotonicity property enforced by the priority queue guarantees that IDL-CH-A finds an optimal solution to Equation 1 (Russell and Norvig, 1995). The IDL-CH-HB  algorithm uses a histogram beam  to control the unfolding of an input IDLgraph, by processing, at each unfolding step, the Search Algorithms Algorithms that operate on IDL-graphs have been recently proposed by Soricut and Marcu (2005). We extend these algorithms to take as input IDLgraphs over non-atomic symbols (such that the coherence models can operate inside terms like C E D , and F from Figure 1), and also to work under models with hidden variables such as CM (Section 2.2). These algorithm, called IDL-CH-A (A search for IDL-expressions under Coherence models) and IDL-CH-HB  (Histogram-Based beam search for IDL-expressions under Coherence models, with histogram beam  ), assume an alphabet 3 of nonatomic (visible) variables (over which the input IDL-expressions are defined), and an alphabet  of hidden variables. The"
P06-2103,W05-1506,0,\N,Missing
P06-2103,W01-1605,1,\N,Missing
P06-2103,P04-1050,0,\N,Missing
P06-2103,P04-1051,0,\N,Missing
P10-1017,P06-1009,0,0.0134938,"s and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for"
P10-1017,J93-2003,0,0.0417809,"-*!&.( /0213 4( 5!67! * ,8.( 9:; <)+,=.( 1 A8BC( ! &gt;?@ DEFG* ) #G(1 ?H() * Figure 1: Model-4 alignment vs. a gold standard. Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. Bold gray boxes show links gained after fully connecting the alignment. Introduction Automatic word alignment is generally accepted as a first step in training any statistical machine translation system. It is a vital prerequisite for generating translation tables, phrase tables, or syntactic transformation rules. Generative alignment models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system. Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficu"
P10-1017,N04-1035,1,0.814872,"a corpus of 50 million words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f -string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006). We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data. Acknowledgements We would like to thank our colleagues in the Natural Language Group at ISI for many meaningful discussions and the anonymous reviewers f"
P10-1017,P06-2014,0,0.023207,"model are currently: K., Ë, Ë@, ËAK., Others either we did not experiment ù , Õº, AÒº, Ñê, AÒê. with, or seemed to provide no significant benefit, and are not included. 163 0.775 ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog. 0.765 Related Work Training F−measure 5 0.77 Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments. Very recent work in word alignment has also started to report downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HM"
P10-1017,P06-1121,1,0.846282,"on words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f -string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006). We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data. Acknowledgements We would like to thank our colleagues in the Natural Language Group at ISI for many meaningful discussions and the anonymous reviewers for their thoughtful su"
P10-1017,J07-2003,0,0.956609,"ntire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 1 d ea br e th e an at th e m ﺍﻛﻞ ﺍﻟﺮﺟﻞ ﺍﳋﺒﺰ S th e m an VP NP NP ﺍﻛﻞ ﺍﻟﺮﺍﺟﻞ ﺍﳋﺒﺰ ate man the bread br e th"
P10-1017,D08-1024,0,0.0326722,"n o tions of the top n = max |2f |, 10 scoring singlelink alignments. We limit the number of total partial alignments αv kept at each node to k. If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst off the heap and replace it with our new partial alignment if its score is better than the current worst. 3 Discriminative training We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al. (2008). We define: Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order. At each nonterminal node v we wish to combine the partial alignments of its children u1 , . . . , uc . We use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to select the k-best combinations of the partial alignments of u1 , . . . , uc (Line 19). Note 2 We find empirically that using binarized trees reduces search errors in cube pruning. 160 . alignment structure into a single high-dimensional feature vector. Our hierarchical search framework allows us to compute these features when"
P10-1017,H05-1012,0,0.00895971,"-art translation system. Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on th"
P10-1017,J03-4003,0,0.0151198,"t can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. Word Alignment as a Hypergraph Algorithm input The input to our alignment algorithm is a sentence-pair (en1 , f1m ) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = en1 , let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. Overview We present a brief overview here and delve deeper in Section 2.1. Word alignments are built bottom-up on the parse tree. Each node v in the tree holds partial alignments sorted by score. 158 uu11u1111 uu12 u1212 uu13 u1313 uu11u1111 uu12 u1212 uu13 u1313 uu11u1111 uu12 u1212 uu13 u1313 uu21 u2121 2.2 2.2 2.2 4.1 4.1 4.1 5.5 5.5 5.5 uu21 u2121 2.2 2.2 2.2 4.1 4.1 4.1 5.5 5.5 5.5 uu21 u2121 2.2 2.2 2.2 4.1 4.1 4.1 5.5 5.5 5.5 uu22 u2222 2.4 2.4 2.4 3.5 3.5 3.5 7.2 7.2 7.2 uu22 u2222 2.4 2.4 2.4 3.5 3.5 3.5 7.2 7.2 7.2 uu22 u2222 2.4 2.4 2.4 3.5 3.5 3.5 7"
P10-1017,N06-1015,0,0.0214506,"Missing"
P10-1017,W02-1001,0,0.0598473,"figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NP node is constructed by composing the best hypothesis at the terminal node labeled “the” and the 2ndbest hypothesis at the terminal node labeled “man”. (We ignore terminal nodes in this toy example.) Hypotheses at the root node imply full alignment structures. 2 word alignments, from which we can efficiently extract the k-best. We handle an arbitrary number of features, compute them efficiently, and score alignments using a linear model. We train the parameters of the model using averaged perceptron (Collins, 2002) modified for structured outputs, but can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. Word Alignment as a Hypergraph Algorithm input The input to our alignment algorithm is a sentence-pair (en1 , f1m ) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = en1 , let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al.,"
P10-1017,P05-1057,0,0.0130449,", there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with"
P10-1017,H05-1011,0,0.0260694,"t models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system. Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Mannin"
P10-1017,P07-1003,0,0.269444,"llowing the foreign preposition. Figure 7 illustrates this pattern. • We observe the Arabic prefix ð, transliterated w- and generally meaning and, to prepend to most any word in the lexicon, so we define features p¬w (e |f ) and p¬w ( f |e). If f begins with w-, we strip off the prefix and return the values of p(e |f ) and p( f |e). Otherwise, these features return 0. • Finally, we have a tree-distance feature to avoid making too many many-to-one (from many English words to a single foreign word) links. This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). For any pair of links (ei , f ) and (e j , f ) in which the e words differ but the f word is the same token in each, return the tree height of first common ancestor of ei and e j . • We also include analogous feature functions for several functional and pronominal prefixes and suffixes.4 4.2 Nonlocal features These features comprise the combination cost component of a partial alignment score and may fire when concatenating two partial alignments to create a larger span. Because these features can look into any two arbitrary subtrees, they are considered nonlocal features as defined by Huang"
P10-1017,P06-1065,0,0.0368222,"guage pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our"
P10-1017,D07-1006,1,0.476955,"ystem on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. 6 0.76 0.755 0.75 0.745 0.74 0.735 0.73 0 5 10 15 20 25 Training epoch 30 35 40 F-measure Figure 8: Learning curves for 10 random restarts over time for parallel averaged perceptron training. These plots show the current F-measure on the training set as time passes. Perceptron training here is quite stable, converging to the same general neighborhood each t"
P10-1017,J03-1002,0,0.0205008,"on training. These plots show the current F-measure on the training set as time passes. Perceptron training here is quite stable, converging to the same general neighborhood each time. 0.76 0.75 0.74 0.73 0.72 0.71 0.70 0.69 0.68 0.67 Model 1 HMM Model 4 Initial alignments Figure 9: Model robustness to the initial alignments from which the p(e |f ) and p( f |e) features are derived. The dotted line indicates the baseline accuracy of GIZA++ Model 4 alone. Experiments 6.1 We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ney, 2003) with both the union and grow-diagfinal heuristics. We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training. We also align the test data using GIZA++5 along with 50 million words of English. Alignment Quality We empirically choose our beam size k from the results of a series of experiments, setting k=1, 2, 4, 8, 16, 32, and 64. We find setting k = 16 to yield the highest accuracy on our held-out test data. Using wider beams resu"
P10-1017,W08-0306,0,0.0410511,"criminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus. 6 0.76 0.755 0.75 0.745 0.74 0.735 0.73 0 5 10 15 20 25 Training epoch 30 35 40 F-measure Figure 8: Learning curves for 10 random restarts over time"
P10-1017,P06-1055,0,0.0562457,"Collins, 2002) modified for structured outputs, but can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments. Word Alignment as a Hypergraph Algorithm input The input to our alignment algorithm is a sentence-pair (en1 , f1m ) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = en1 , let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. Overview We present a brief overview here and delve deeper in Section 2.1. Word alignments are built bottom-up on the parse tree. Each node v in the tree holds partial alignments sorted by score. 158 uu11u1111 uu12 u1212 uu13 u1313 uu11u1111 uu12 u1212 uu13 u1313 uu11u1111 uu12 u1212 uu13 u1313 uu21 u2121 2.2 2.2 2.2 4.1 4.1 4.1 5.5 5.5 5.5 uu21 u2121 2.2 2.2 2.2 4.1 4.1 4.1 5.5 5.5 5.5 uu21 u2121 2.2 2.2 2.2 4.1 4.1 4.1 5.5 5.5 5.5 uu22 u2222 2.4 2.4 2.4 3.5 3.5 3.5 7.2 7.2 7.2 uu22 u2222 2.4 2.4 2.4 3.5 3.5 3.5 7.2"
P10-1017,W01-1812,0,0.0111853,"as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistic"
P10-1017,P02-1040,0,0.101409,"Missing"
P10-1017,P09-1104,0,0.191532,"Missing"
P10-1017,H05-1010,0,0.0181451,"n-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the"
P10-1017,W05-1506,0,0.0326704,"ut, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 1 d ea br e th e an at"
P10-1017,P08-1058,0,0.0128254,"n word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f -string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006). We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data. Acknowledgements We would like to thank our colleagues in the Natural Language Group at ISI for many meaningful discussions and the anonymous reviewers for their thoughtful suggestions. This research was supported by DARPA contract HR0011-06-C-0022 under s"
P10-1017,P07-1019,0,0.192759,"ging the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 1 d ea br e th e an at th e m ﺍﻛﻞ ﺍﻟﺮﺟﻞ ﺍﳋﺒﺰ S th e m an VP NP NP ﺍﻛﻞ ﺍﻟﺮﺍﺟﻞ ﺍﳋﺒﺰ ate man the bread br e th"
P10-1017,J97-3002,0,0.357429,"her we did not experiment ù , Õº, AÒº, Ñê, AÒê. with, or seemed to provide no significant benefit, and are not included. 163 0.775 ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog. 0.765 Related Work Training F−measure 5 0.77 Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments. Very recent work in word alignment has also started to report downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments. DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance inste"
P10-1017,P08-1067,0,0.258739,"ult to extend a given generative model with feature functions without changing the entire generative story. This difficulty has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; LacosteJulien et al., 2006; Moore et al., 2006). We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment. Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of 157 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 1 d ea br e th e an at th e m ﺍﻛﻞ"
P10-1017,P08-1000,0,\N,Missing
P95-1020,P94-1009,0,0.020705,"erances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of 144 the phenomenon"
P95-1020,P90-1012,0,0.031289,"clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other"
P95-1020,P92-1009,0,0.280275,"uences of utterances. 1 Pragmatics and Defeasibility It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990). To understand conversationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994). Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first obse"
P95-1020,W91-0201,0,0.0165244,"them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules. Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of 144 the phenomenon preferred to keep defeasibility outside the mathematical world. For Frege (1892), Russell (1905), and Quine (1949) &quot;everything exists&quot;; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994). We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts). 1. Most linguistic approaches account for the defeasibility of pragmatic inferences by analyzing them in a context that consists of all or some of the previous utterances, including the current one. Context (Karttunen, 1974; Kay, 1992), procedural rules (Gazdar, 1979; Karttunen and Peters, 1979), lexical and syntactic structure (Weischedel, 1979), intentions (Hir"
P95-1020,E93-1033,1,0.753967,"A &quot;-,married&quot;(x)) (VUtx)(-4bachelorU(=) --~ marriedi(x)) (vUtx )(-~bachelor&quot;( x ) --~ adulta( x )) (vu'x)(--,bachelorU(x) .-, maled(=)) Chris is not a bachelor presupposes that Chris is a male adult; Chris regrets that Mary came to the party presupposes that Mary came to the party. There is no contradiction between these two presuppositions, 148 3.3 P r a g m a t i c i n f e r e n c e s in s e q u e n c e s o f utterances We have already mentioned that speech repairs constitute a good benchmark for studying the generation and cancellation of pragmatic inferences along sequences of utterances (McRoy and Hirst, 1993). Suppose, for example, that Jane has two friends - John Smith and John Pevler - - and that her roommate Mary has met only John Smith, a married fellow. Assume now that Jane has a conversation with Mary in which Jane mentions only the name John because she is not aware that Mary does not know about the other John, who is a five-year-old boy. In this context, it is natural for Mary to become confused and to come to wrong conclusions. For example, Mary may reply that John is not a bachelor. Although this is true for both Johns, it is more appropriate for the married fellow than for the five-year"
P97-1013,J87-1002,0,0.0197435,"Missing"
P97-1013,C92-1045,0,0.0276602,"Missing"
P97-1013,J95-2003,0,0.03983,"Missing"
P97-1013,J86-3001,0,0.106071,"Oberlander (1992), Kamp and Reyle (1993), Grover et al. (1994), and Pr0st, Scha, and van den Berg (1994) take the position that discourse structures can be built only in conjunction with fully specified clause and sentence structures. And Hobbs&apos;s theory (1990) assumes that sophisticated knowledge bases and inference mechanisms are needed for determining the relations between discourse units. Despite the formal elegance of these approaches, they are very domain dependent and, therefore, unable to handle more than a few restricted exampies. On the other hand, although the theories described by Grosz and Sidner (1986), Polanyi (1988), and Mann and Thompson (1988) are successfully applied manually, they ,are too informal to support an automatic approach to discourse analysis. In contrast with this previous work, the rhetorical parser that we present builds discourse trees for unrestricted texts. We first discuss the key concepts on which our approach relies (section 2) and the corpus analysis (section 3) that provides the empirical data for our rhetorical parsing algorithm. We discuss then an algorithm that recognizes discourse usages of cue phrases and that determines clause boundaries within sentences. La"
P97-1013,P94-1003,0,0.0242303,"apitalizing on the occurrences of certain lexicogrammatical constructs. Such constructs can include tense Introduction Researchers of natural language have repeatedly acknowledged that texts are not just a sequence of words nor even a sequence of clauses and sentences. However, despite the impressive number of discourse-related theories that have been proposed so far, there have emerged no algorithms capable of deriving the discourse structure of an unrestricted text. On one hand, efforts such as those described by Asher (1993), Lascarides, Asher, and Oberlander (1992), Kamp and Reyle (1993), Grover et al. (1994), and Pr0st, Scha, and van den Berg (1994) take the position that discourse structures can be built only in conjunction with fully specified clause and sentence structures. And Hobbs&apos;s theory (1990) assumes that sophisticated knowledge bases and inference mechanisms are needed for determining the relations between discourse units. Despite the formal elegance of these approaches, they are very domain dependent and, therefore, unable to handle more than a few restricted exampies. On the other hand, although the theories described by Grosz and Sidner (1986), Polanyi (1988), and Mann and Thompson"
P97-1013,J93-3003,0,0.0995132,"and the sizes of the spans that a given cue marks. However, given that the structure that we are trying to build is highly constrained, such a prediction proved to be unnecessary: the overall constraints on the structure of discourse that we enumerated in the beginning of this section cancel out most of the configurations of elementary constraints that do not yield correct discourse trees. Consider, for example, the following text: • Psycholinguistic and other empirical research (Kintsch, 1977; Schiffrin, 1987; Segal, Duchan, and Scott, 1991; Cahn, 1992; Sanders, Spooren, and Noordman, 1992; Hirschberg and Litman, 1993; Knott, 1995; Costermans and Fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as &quot;macroconnectors&quot; between larger textual units. Therefore, we can use them as rhetorical indicators at any of the following levels: clause, sentence, paragraph, and text. (1) [Although discourse markers are ambiguous, l] [one can use them to build discourse trees for unrestricted texts: 2] [this will lead to many new applications in natural language processing)] • The number of discourse markers in a typical text - - approxima"
P97-1013,P92-1001,0,0.0464864,"Missing"
P97-1013,J88-2003,0,0.0611747,"Missing"
P97-1013,J81-4001,0,0.0595133,"Missing"
P97-1013,J88-2006,0,0.0394473,"Missing"
P99-1047,J95-4004,0,0.0700905,"Missing"
P99-1047,P97-1012,0,0.0861608,"Missing"
P99-1047,J97-1003,0,0.0971227,"eatures that denote whether the first and last units of the trees in focus contain potential discourse markers and the position of these markers in the corresponding textual units (beginning, middle, or end). Ace S 60.00 58.013 56.00 54.0~ 52.0G Operational features. ~0.0~ • Features that specify what the last five parsing operations performed by the parser were. 3 46.00 Semantic-similarity-based features. t, / ,,1c,~es x l03 0.5tl • Features that denote the semantic similarity between the textual segments subsumed by the trees in focus. This similarity is computed by applying in the style of Hearst (1997) a cosine-based metric on the morphed segments. • Features that denote Wordnet-based measures of similarity between the bags of words in the promotion sets of the trees in focus. We use 14 Wordnetbased measures of similarity, one for each Wordnet relation (Fellbaum, 1998). Each of these similarities is computed using a metric similar to the cosine-based metric. Wordnet-based similarities reflect the degree of synonymy, antonymy, meronymy, hyponymy, etc. between the textual segments subsumed by the trees in focus. We also use 14 x 13/2 relative Wordnet-based measures of similarity, one for each"
P99-1047,P97-1062,0,0.0374559,"ifornia 4 6 7 6 Admiralty Way, Suite 1001 Marina del Rey, C A 90292-6601 marcu @ isi. edu Abstract We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources. I Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997). In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts. Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees. 2 The Corpus We used a corpus of 90 rhetori"
P99-1047,C94-2183,0,0.0482988,"res that we use are sufficient for determining the hierarchical structure of texts and the nuclearity statuses of discourse segments. However, they are insufficient for determining correctly the elementary units of discourse and the rhetorical relations that hold between discourse segments. 28.7 45.3 62.8 53.9 34.3 36.0 57.9 63.2 35.3 25.7 45.5 44.7 7 Related w o r k The rhetorical parser presented here is the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et al., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998)employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carded out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that learns"
P99-1047,P95-1037,0,0.0677918,"of Southern California 4 6 7 6 Admiralty Way, Suite 1001 Marina del Rey, C A 90292-6601 marcu @ isi. edu Abstract We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources. I Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997). In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts. Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees. 2 The Corpus W"
P99-1047,P97-1013,1,0.801271,"7 clusters of rhetorical similarity. Since there are 6 types of reduce operations and since each discourse tree in our study uses relation 368 cal unit boundaries, we relied on features that model both the local and global contexts. The local context consists of a window of size 5 that enumerates the Part-Of-Speech (POS) tags of the lexeme under scrutiny and the two lexemes found immediately before and after it. The POS tags are determined automatically, using the Brill tagger (1995). Since discourse markers, such as because and and, have been shown to play a major role in rhetorical parsing (Marcu, 1997), we also consider a list of features that specify whether a lexeme found within the local contextual window is a potential discourse marker. The local context also contains features that estimate whether the lexemes within the window are potential abbreviations. The global context reflects features that pertain to the boundary identification process. These features specify whether a discourse marker that introduces expectations (Cristea and Webber, 1997) (such as although) was used in the sentence under consideration, whether there are any commas or dashes before the estimated end of the sent"
P99-1047,W99-0307,1,0.676159,"consisted in assigning edu and parenthetical unit boundaries, in assembling edus and spans into discourse trees, and in labeling the relations between edus and spans with rhetorical relation names from a taxonomy of 71 relations. No explicit distinction was made between intentional, informational, and textual relations. In addition, we also marked two constituency relations that were ubiquitous in our corpora and that often subsumed complex rhetorical constituents. These relations were ATTRIBUTION, which was used to label the relation between a reporting and a reported clause, and APPOSITION. Marcu et al. (1999) discuss in detail the annotation tool and protocol and assess the inter-judge agreement and the reliability of the annotation. 3 The parsing model We model the discourse parsing process as a sequence of shift-reduce operations. As front-end, the parser uses a discourse segmenter, i.e., an algorithm that partitions the input text into edus. The discourse segmenter, which is also decision-based, is presented and evaluated in section 4. The input to the parser is an empty stack and an input list that contains a sequence of elementary discourse trees, edts, one edt for each edu produced by the di"
P99-1047,J97-2002,0,0.0160399,"Missing"
P99-1047,J92-4001,0,\N,Missing
soricut-etal-2002-using,1999.tc-1.8,0,\N,Missing
soricut-etal-2002-using,J93-2003,0,\N,Missing
soricut-etal-2002-using,knight-al-onaizan-1998-translation,1,\N,Missing
soricut-etal-2002-using,W98-0301,1,\N,Missing
soricut-etal-2002-using,P01-1030,1,\N,Missing
W00-1403,J96-2004,0,0.00617404,"manually construct the discourse structure of all to Japanese span [1,1]. The Position-Independent Japanese and English texts it, the corpus. 10~. of figures offer a more optimistic metric for comparing the Japanese and English texts were rhetorically discourse trees. They span a wider range of values labeled by two of us. The agreement was stathan the Position-Dependent figures, which enables tistically significant (Kappa = 0.65.0 &gt; 0.01 for a finer grained comparison, which in turn enables Japanese and Kappa = 0.748,0 &gt; 0.01 for Ena better characterization of the differ.ences between glish (Carletta, 1996; Siegel-and Castellan, 1988)). Japanese and English discourse structures. The tool and the annotation protocol are available at. http://www, isi.edt,/~r, zarcu/softwa,-e/. For each In order to provide a better estimate of how close pair of Japanese-English discourse, structures, we t w o discourse trees were, we computed Positionalso built, manually an alignment file, which specified Dependent and -Independent recall and precision figthe correspondence between the edus of the Japanese ures for the sentential level (where units are given and English texts. by edus and spans are given by sets o"
W00-1403,W94-0308,0,0.404258,"viable option. .Many of the earl3 implementations of MGEN systems have adopted the perspective that text planners can be implemented as language-independent modules (lordanskaja el, ;11., 1992: Goldberg et el., 1994), possibly followed by a hm:aricatwn stage, in which discourse l.rees are re-written to refleet~ language-specific constraints (R6sner and Stede. 1992; St,ede, 1999). Although such an approach may 17 be adequate for highly restricted text genres, such as weather forecasts, it usually poses problems for less restricted genres. Studies of instruction manuals (RTsner and Stede, 1992; Delin et al., 1994: Delin et al., 1996) suggest that there are variations with respect to the way high-level communicative g o a l s are realized across languages. For example, Delin et al. (1994) noticed that sentences (1), (2), and (3), which were taken from a trilingual instruction manual for a step-aerobics machine, yield nonisomorphic Rhetorical Structure (Mann and T h o m p son, 1988) analyses in English, French, and German respectively (see Figure 1). English: [The stepping load can be altered I ] [by loosening the locking lever2] [and changing the position of the cylinder foota]. (1) French: [Pour modif"
W00-1403,C96-1050,0,0.0303652,"of the earl3 implementations of MGEN systems have adopted the perspective that text planners can be implemented as language-independent modules (lordanskaja el, ;11., 1992: Goldberg et el., 1994), possibly followed by a hm:aricatwn stage, in which discourse l.rees are re-written to refleet~ language-specific constraints (R6sner and Stede. 1992; St,ede, 1999). Although such an approach may 17 be adequate for highly restricted text genres, such as weather forecasts, it usually poses problems for less restricted genres. Studies of instruction manuals (RTsner and Stede, 1992; Delin et al., 1994: Delin et al., 1996) suggest that there are variations with respect to the way high-level communicative g o a l s are realized across languages. For example, Delin et al. (1994) noticed that sentences (1), (2), and (3), which were taken from a trilingual instruction manual for a step-aerobics machine, yield nonisomorphic Rhetorical Structure (Mann and T h o m p son, 1988) analyses in English, French, and German respectively (see Figure 1). English: [The stepping load can be altered I ] [by loosening the locking lever2] [and changing the position of the cylinder foota]. (1) French: [Pour modifier la charge d'appui"
W00-1403,C92-3158,0,0.613814,"0755 Los Angeles, CA 90089 lmcarls@afterlife, ncsc. rail m watanab@usc, edu marcu@isi, edu Abstract We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations. We discuss implications of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems. 1 Introduction The natural language generation community has emphasized for a number of years the strengths of multilingual generation (MGEN) systems (Iordanskaja et al., 1992; RTsner and Stede, 1992; Reiter and Mellish, 1993; Goldberg et al., 1994; Paris et al., 1995; Power and Scott, 1998). These strengths concern the reuse of knowledge, the support for early drafts in several languages, the support for maintaining consistency when making changes, the support for producing alternative formulations, and the potential for producing higher quality outputs than machine translation. (The weaknesses concern the high-cost of building large, language-independent knowledge bases, and the dilficulty of producing high-quality. broad-coverage generation algorithms.) From an"
W00-1403,W99-0307,1,0.864913,"rse trees for 40 Japanese texts and both have a subtree t, tl and 12 are more similar their corresponding translations. The texts, sethan if they were if they didn't share ally subtree. lected randomly from the ARPA corpus (White For instance, for the spans at the sub-sentential level and O'Connell, 1994), contained on average about in the trees in Figure 2 the position-independent 460 words. We developed a discourse annotarecall is 6/10 and the position-independent precition protocol for ,Japanese and English along the sion is 6/11 because in addition to spans [1,2], [4,4], lines followed by Marcu et al. (1999). We used [5,5], and [1,5], one can also match Japanese spat, Marcu's discourse annotation tool (1999) in order [1,1] to English spa,, [2,2] and Japanese spa,, [2,2] to manually construct the discourse structure of all to Japanese span [1,1]. The Position-Independent Japanese and English texts it, the corpus. 10~. of figures offer a more optimistic metric for comparing the Japanese and English texts were rhetorically discourse trees. They span a wider range of values labeled by two of us. The agreement was stathan the Position-Dependent figures, which enables tistically significant (Kappa = 0."
W00-1403,A00-2002,1,0.838052,"generating texts in lanlations, at different levels of granularity. They proguage P, the MGEN system works as a monolinpose that discourse phenomena should be accounted gum generator. When generating texts in language for at a more abstract level than RST relations O, the MGEN system generates a text plan in lanand they present a classification system in terms of ""stratification"", ..'!me£afunction?., ,,and .::p~radig,: ........guage.-~, xnapsitdr~to.=taaag,uageO.,~ anti,then ~proceeds further with the sentence planning and realization matic/syntagmatic axiality"" that enables one to repstages. Marcu et al. (2000) present and evaluate a resent discourse structures at multiple levels of abdiscourse-tree rewriting algorithm that exploits mastraction. chine learning methods in order to map Japanese Adopting such an approach could be an extremely discourse trees into discourse trees that resemble rewarding enterprise. Unfortunately, the research English-specific renderings. of Delin et al. (1994) and Bateman and Rondhuis (1997) cannot be applied yet to unrestricted doThe advantage of such an approach is that the tree-rewriting modules can be also used in the conmains. Generation and Enablement are only two"
W00-1403,W98-1411,0,0.058192,"Missing"
W00-1403,P98-2173,0,0.0154971,"e annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations. We discuss implications of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems. 1 Introduction The natural language generation community has emphasized for a number of years the strengths of multilingual generation (MGEN) systems (Iordanskaja et al., 1992; RTsner and Stede, 1992; Reiter and Mellish, 1993; Goldberg et al., 1994; Paris et al., 1995; Power and Scott, 1998). These strengths concern the reuse of knowledge, the support for early drafts in several languages, the support for maintaining consistency when making changes, the support for producing alternative formulations, and the potential for producing higher quality outputs than machine translation. (The weaknesses concern the high-cost of building large, language-independent knowledge bases, and the dilficulty of producing high-quality. broad-coverage generation algorithms.) From an economic perspective, the more a system can rely on language independent modules for the purpose of multilingual gene"
W00-1403,H94-1024,0,0.0187322,"Missing"
W00-1403,J93-4004,0,\N,Missing
W00-1403,C98-2168,0,\N,Missing
W00-1507,P98-1032,1,0.926631,"g quality. This paper provides two examples of the flexibility of e-rater’s modular architecture for continued application development toward these goals. Specifically, we discuss a) how additional features from rhetorical parse trees were integrated into e-rater, and b) how the salience of automatically generated discourse-based essay summaries was evaluated for use as instructional feedback through the re-use of erater’s topical analysis module. 1 Introduction E-rater is an operational automated essay scoring system that was designed to score essays based on holistic scoring guide criteria (Burstein, et al 1998), specifically for the Graduate Management Admissions Test (GMAT). Holistic scoring guides instruct the human reader to assign an essay score based on the quality of writing characteristics in an essay. For instance, the reader is to assess the overall quality of the writer’s use of syntactic variety, the organization of ideas, and appropriate vocabulary use. E-rater combines several NLP tools to identify syntactic, discourse, and vocabulary-based features. In developing this automated essay scoring application, we have two primary goals. We are continually experimenting with e-rater to enrich"
W00-1507,C94-1042,0,0.0415332,"Missing"
W00-1507,P97-1013,1,0.836556,"ting defined in the scoring guide criteria. Currently, e-rater features represent these scoring guide criteria: syntactic variety, organization of ideas, and vocabulary usage. E-rater discourse features capture the criterion, organization of ideas, at a high level. However, the existing discourse features are linear, and do not express relationships across a text. Hierarchical discourse relations can be expressed with rhetorical structure theory (RST) features (Mann and Thompson, 1989). In an experiment, we evaluated the potential use of RST features in e-rater. An existing rhetorical parser (Marcu, 1997) was used to generate parse trees for essay samples from 20 test questions to the GMAT. A program was written to identify the RST features in essays, compute counts of tokens, types and ratios of the features, and to store the three categories of feature counts in vectors for each essay. For the RST vector files, separate files were output for each type of feature count (tokens, types, and ratios). The model building program was modified to introduce the new RST variables. In this way, the RST feature variables could be evaluated either individually or in combination during model building -- a"
W00-1507,C98-1032,1,\N,Missing
W01-1605,P01-1014,1,0.170014,"al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Hovy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units • The internal nodes of the tree correspond to contiguous text spans • Each node is characterized by its nuclearity – a nucleus indicates a more essential unit of information, while a satellite indicates a supporting or information. • bac"
W01-1605,J97-1002,0,0.00597207,"Missing"
W01-1605,J86-3001,0,0.217721,"s. 1 Introduction The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mil corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal"
W01-1605,J97-1003,0,0.11527,", have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mil corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it w"
W01-1605,J93-3003,0,0.0145982,"we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed. However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy. It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises. Reseachers in the field have proposed a number of competing hypotheses about what constitutes an elementary discourse unit. While some take the elementary units to be clauses (Grimes, 1975; Givon, 1983; Longacre, 1983), others take them to be prosodic units (Hirschberg and Litman, 1993), turns of talk (Sacks, 1974), sentences (Polanyi, 1988), intentionally defined discourse segments (Grosz and Sidner, 1986), or the “contextually indexed representation of information conveyed by a semiotic gesture, asserting a single state of affairs or partial state of affairs in a discourse world,” (Polanyi, 1996, p.5). Regardless of their theoretical stance, all agree that the elementary discourse units are non-overlapping spans of text. Our goal was to find a balance between granularity of tagging and ability to identify units consistently on a large scale. In the end, we chose the clause"
W01-1605,W99-0307,1,0.622854,"nge of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mil corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selection of theoretical appr"
W01-1605,A00-2002,1,0.323266,"s that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Hovy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse, called elementary discourse units • The internal nodes of the tree correspond to contiguous text spans • Each node is characterized by its nuclearity"
W01-1605,J93-2004,0,0.056556,"fort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selection of theoretical approach, annotation methodology, training, and quality assurance. The resulting corpus contains 385 documents of American English selected from the Penn Treebank (Marcus et al., 1993), annotated in the framework of Rhetorical Structure Theory. We believe this resource holds great promise as a rich new source of textlevel information to support multiple lines of research for language understanding applications. 2 Framework Two principle goals underpin the creation of this discourse-tagged corpus: 1) The corpus should be grounded in a particular theoretical approach, and 2) it should be sufficiently large enough to offer potential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications."
W01-1605,J95-3007,0,0.390802,"of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mil corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selectio"
W01-1605,J93-4004,0,0.0214101,"e decided to use RST for three reasons: • It is a framework that yields rich annotations that uniformly capture intentional, semantic, and textual features that are specific to a given text. • Previous research on annotating texts with rhetorical structure trees (Marcu et al., 1999) has shown that texts can be annotated by multiple judges at relatively high levels of agreement. We aimed to produce annotation protocols that would yield even higher agreement figures. • Previous research has shown that RST trees can play a crucial role in building natural language generation systems (Hovy, 1993; Moore and Paris, 1993; Moore, 1995) and text summarization systems (Marcu, 2000); can be used to increase the naturalness of machine translation outputs (Marcu et al. 2000); and can be used to build essayscoring systems that provide students with discourse-based feedback (Burstein et al., 2001). We suspect that RST trees can be exploited successfully in the context of other applications as well. In the RST framework, the discourse structure of a text can be represented as a tree defined in terms of four aspects: • The leaves of the tree correspond to text fragments that represent the minimal units of the discourse"
W01-1605,P95-1018,0,0.147764,"e analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mil corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs. The paper describes the challenges we faced in building a corpus of this level of complexity and scope – including selectio"
W01-1605,W99-0620,0,0.0565458,"energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mil corpora (Moser and Moore, 1995; Marcu et al., 1999). In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research. Our main goal in undertaking this effort was to create a reference corpus for community-wide use. Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic D"
W01-1605,M95-1002,0,0.00490963,"cu/discourse. 6 Discussion A growing number of groups have developed or are developing discourse-annotated corpora for text. These can be characterized both in terms of the kinds of features annotated as well as by the scope of the annotation. Features may include specific discourse cues or markers, coreference links, identification of rhetorical relations, etc. The scope of the annotation refers to the levels of analysis within the document, and can be characterized as follows: • sentential: annotation of features at the intra-sentential or inter-sentential level, at a single level of depth (Sundheim, 1995; Tsou et al., 2000; Nomoto and Matsumoto, 1999; Rebeyrolle, 2000). • hierarchical: annotation of features at multiple levels, building upon lower levels of analysis at the clause or sentence level (Moser and Moore, 1995; Marcu, et al. 1999) • document-level: broad characterization of document structure such as identification of topical segments (Hearst, 1997), linking of large text segments via specific relations (Ferrari, 1998; Rebeyrolle, 2000), or defining text objects with a text architecture (Pery-Woodley and Rebeyrolle, 1998). Developing corpora with these kinds of rich annotation is a"
W01-1605,W00-1206,0,0.0264521,"Missing"
W01-1605,wayne-2000-multilingual,0,0.00909433,"y and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications. 1 Introduction The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999; Ts’ou et al., 2000), and hierarchical analyses of small Mary Ellen Okurowski Department of Defense Ft. George G. Meade MD 20755 meokuro@romulus.ncsc.mi"
W01-1605,P99-1032,0,0.390238,"Missing"
W01-1605,J97-1005,0,\N,Missing
W01-1605,P97-1011,0,\N,Missing
W01-1605,P87-1023,0,\N,Missing
W02-1018,J93-2003,0,0.157456,"Missing"
W02-1018,P01-1030,1,0.908874,"Missing"
W02-1018,P01-1050,1,0.856064,"Missing"
W02-1018,W99-0604,0,0.541958,"Missing"
W02-1018,P01-1067,0,0.559305,"Missing"
W02-1037,W99-0905,0,\N,Missing
W02-1037,J93-2003,0,\N,Missing
W02-1037,P98-1069,0,\N,Missing
W02-1037,C98-1066,0,\N,Missing
W02-1037,P99-1067,0,\N,Missing
W02-2102,W02-2103,1,\N,Missing
W02-2102,A00-2018,0,\N,Missing
W02-2102,A00-2023,0,\N,Missing
W02-2102,C00-1007,0,\N,Missing
W02-2102,P98-1116,1,\N,Missing
W02-2102,C98-1112,1,\N,Missing
W02-2102,P98-1035,0,\N,Missing
W02-2102,C98-1035,0,\N,Missing
W02-2102,P02-1039,1,\N,Missing
W02-2102,P01-1067,1,\N,Missing
W02-2102,P99-1042,0,\N,Missing
W02-2102,J01-2004,0,\N,Missing
W02-2102,P02-1057,1,\N,Missing
W02-2102,P01-1017,0,\N,Missing
W04-1016,P99-1071,0,0.142688,"f we show many humans the same two sentences, they will produce similar summaries. Of course we do not penalize one human for using different words than another. The sentence fusion task is interesting after performing sentence extraction, the extracted sentences often contain superfluous information. It has been further observed that simply compressing sentences individually and concatenating the results leads to suboptimal summaries (Daum´e III and Marcu, 2002). The use of sentence fusion in multidocument summarization has been extensively explored by Barzilay in her thesis (Barzilay, 2003; Barzilay et al., 1999), though in the multi-document setting, one has redundancy to fall back on. Additionally, the sentence fusion task is sufficiently constrained that it makes possible more complex and linguistically motivated manipulations than are reasonable for full document or multi-document summaries (and for which simple extraction techniques are unlikely to suffice). 3 Data Collection Our data comes from a collection of computer product reviews from the Ziff-Davis corporation. This corpus consists of roughly seven thousand documents paired with human written abstracts. The average document was 1080 words"
W04-1016,P00-1038,0,0.0268218,"urbing lack of agreement. 1 Introduction and Motivation The practices of automatic summarization vary widely across many dimensions, including source length, summary length, style, source, topic, language, and structure. Most typical are summaries of a single news document down to a headline or short summary, or of a collection of news documents down to a headline or short summary (Hahn and Harman, 2002). A few researchers have focused on other aspects of summarization, including single sentence (Knight and Marcu, 2002), paragraph or short document (Daum´e III and Marcu, 2002), query-focused (Berger and Mittal, 2000), or speech (Hori et al., 2003). The techniques relevant to, and the challenges faced in each of these tasks can be quite different. Nevertheless, they all rely on one critical assumption: there exists a notion of (relative) importance between pieces of information in a document (or utterance), regardless of whether we can detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive"
W04-1016,P02-1057,1,0.90435,"Missing"
W04-1016,W04-3216,1,0.838325,"Missing"
W04-1016,W03-0508,0,0.279232,"evant to, and the challenges faced in each of these tasks can be quite different. Nevertheless, they all rely on one critical assumption: there exists a notion of (relative) importance between pieces of information in a document (or utterance), regardless of whether we can detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly o"
W04-1016,J02-4006,0,0.0715519,"Missing"
W04-1016,N03-1020,0,0.0605769,"n detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend to show little (statistically significant) difference from one another. Moreover, a baseline system that simply takes the first sentences of a document performs just as well or better than int"
W04-1016,W03-1101,0,0.0363138,"Missing"
W04-1016,N04-1019,0,0.398312,"different. Nevertheless, they all rely on one critical assumption: there exists a notion of (relative) importance between pieces of information in a document (or utterance), regardless of whether we can detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend"
W04-1016,P02-1040,0,0.0727142,"cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend to show little (statistically significant) difference from one another. Moreover, a baseline system that simply takes the first sentences of a document performs just as well or better than intelligently crafted systems when summarizing news stories. Additionally, studies of vast numbers of summar"
W04-1016,W03-2805,0,0.0152985,"deed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend to show little (statistically significant) difference from one another. Moreover, a baseline system that simply takes the first sentences of a document performs just as well or better than intelligently crafted systems whe"
W04-1016,2003.mtsummit-papers.9,0,\N,Missing
W04-3216,P00-1041,0,0.0589207,"Missing"
W04-3216,J93-2003,0,0.0154546,"Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {hdaume,marcu}@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, researchers in text summarization have employed one of several techniques. Some research"
W04-3216,P02-1057,1,0.909237,"Missing"
W04-3216,J02-4006,0,0.308806,"Document/Abstract Alignment Hal Daum´e III and Daniel Marcu Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {hdaume,marcu}@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, research"
W04-3216,J00-2011,0,0.0432923,"the alphabet K. πj is the probability of beginning in state j. The transition probability ai,j is the probability of transitioning from state i to state j. bi,j,k¯ is the probability of emitting (the non-empty) observation sequence k¯ while transitioning from state i to state j. Finally, x t denotes the state after emitting t symbols. The full derivation of the model is too lengthy to include; the interested reader is directed to (Daum´e III and Marcu, 2002b) for the derivations and proofs of the formulae. To assist the reader in understanding the mathematics, we follow the same notation as (Manning and Schutze, 2000). The formulae for the calculations are summarized in Table 2. 2.2.1 Forward algorithm The forward algorithm calculates the probability of an observation sequence. We define α j (t) as the probability of being in state j after emitting the first t − 1 symbols (in whatever grouping we want). 2.2.2 Backward algorithm Just as we can compute the probability of an observation sequence by moving forward, so can we calculate it by going backward. We define β i (t) as the probability of emitting the sequence o Tt given that we are starting out in state i. 2.2.3 Best path We define a path as a sequence"
W04-3216,J03-1002,0,0.0640924,"In Figure 1, we observe several phenomena: • Alignments can occur at the granularity of words and at the granularity of phrases. • The ordering of phrases in an abstract can be different from the ordering in the document. • Some abstract words do not have direct correspondents in the document, and some document words are never used. It is thus desirable to be able to automatically construct alignments between documents and their abstracts, so that the correspondences between the pairs are obvious. One might be initially tempted to use readily-available machine translation systems like GIZA++ (Och and Ney, 2003) to perform such Connecting Point has become the single largest Mac retailer after tripling it ’s Macintosh sales since January 1989 . Connecting Point Systems tripled it ’s sales of Apple Macintosh systems since last January . It is now the single largest seller of Macintosh . Figure 1: Example abstract/text alignment. alignments. However, as we will show, the alignments produced by such a system are inadequate for this task. The solution that we propose to this problem is an alignment model based on a novel mathematical structure we call the Phrase-Based HMM. 2 Designing a Model As observed"
W04-3216,C96-2141,0,0.0664892,"ak for Possible alignments (considering only the 40 independently annotated pairs). When considering only Sure alignments, the kappa statistic (over 7.2 million items, 2 annotators and 2 categories) for agreement was 0.63. When words from the ignore-list were thrown out, this rose to 0.68. Carletta (1995) suggests that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement. 3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds to multiple document sentences. In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human. Each pair w"
W04-3216,J96-2004,0,\N,Missing
W04-3233,J99-2004,0,0.0237495,"erforms comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient. Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is employed, which leads to globally poorer parses. In contrast, th"
W04-3233,J96-1002,0,0.0103478,"(CRFs). We adopt a slight variant of the MEMM framework. 2.1 Maximum Entropy Tagging Model In the formulation of the maximum entropy tagging model, we assume that the probability distribution of tags takes the form of an exponential distribution, parameterized by a sequence of feature weights, λm 1 , where there are m-many features. Thus, we where Zti−1 ,w¯ is a normalizing factor. Like other maximum entropy approaches, this distribution is unimodal and optimal values for the λs can be found through various algorithms; we use GIS. A good introduction to maximum entropy models can be found in (Berger et al., 1996). In our approach, we use a tag set of exactly five tags: {open, close, in, out, sing}. An open tag is assigned to all words that open a bracketing (regardless of the number of brackets opened) and do not also close a bracketing. A close tag is assigned to all words that close a bracketing and do not also open one. An in tag is assigned to all words enclosed in an NP, but which neither open nor close one. An out tag is assigned to all words which are not enclosed in an NP. A sing(leton) tag is assigned to all words that both open and close a bracketing (regardless of whether they open or close"
W04-3233,J95-4004,0,0.0482029,"Missing"
W04-3233,A00-2018,0,0.0919932,"ecoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient. Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is employed, which lead"
W04-3233,J03-4003,0,0.277131,"mined only at decoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient. Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is emp"
W04-3233,P00-1007,0,0.0178366,"tition, NP Bracketing systems were trained on sections 15-18 of the Wall Street Journal corpus, while section 20 was used for testing. The bracketing information was extracted directly from the Penn Treebank, essentially disregarding all non-NP brackets. An example bracketed sentence is in Figure 1. There have been several successful approaches reported in the literature to solve this task. Tjong Kim Sang (1999) first used repeated chunking to attain an f-score of 82.98 during the CoNLL competition and subsequently (Sang, 2002) an f-score of 83.79 using a combination of two different systems. Krymolowski and Dagan (2000) have obtained similar results using more training data and lexicalization. Brandts (1999) has used cascaded HMMs to solve the NP Bracketing problem; however, he evaluated his system only on German NPs, so his results cannot be directly compared. Obviously, the difficulty that arises in NP Bracketing that differentiates it from NP Chunking is the issue of embedded NPs, thus requiring output in the 30 obtain a distribution for P rλm (ti ti−1 , w) ¯ of the 1 form:   m X 1 (1) λj fj (ti , ti−1 , w) ¯  exp  Zti−1 ,w¯ Charniak Collins Bracketer+SVM Bracketer Seconds to Parse (normalized) 25 20"
W04-3233,N01-1025,0,0.0610191,"Missing"
W04-3233,P02-1038,0,0.0920675,"Missing"
W04-3233,W95-0107,0,0.0539695,"amework based on support vector machines. We solve the problem of hierarchical structure in our tagging model by modeling underspecified tags, which are fully determined only at decoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), t"
W04-3233,E99-1016,0,\N,Missing
W04-3233,N03-1028,0,\N,Missing
W05-0814,J93-2003,0,0.0703643,"292-6601 marcu@isi.edu Abstract We discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate. 1 Introduction ISI participated in the WPT05 Romanian-English word alignment task. The system used for baseline experiments is two runs of IBM Model 4 (Brown et al., 1993) in the GIZA++ (Och and Ney, 2003) implementation, which includes smoothing extensions to Model 4. For symmetrization, we found that Och and Ney’s “refined” technique described in (Och and Ney, 2003) produced the best AER for this data set under all experimental conditions. We experimented with a statistical model for inducing a stemmer cross-lingually, but found that the best performance was obtained by simply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating t"
W05-0814,2004.iwslt-papers.2,0,0.0116132,"FROM E NGLISH WORD TTABLE ESTIMATED FROM INTERSECTION OF TWO STARTING ALIGNMENTS FOR THIS ITERATION TRANSLATION TABLE FROM E NGLISH TO ROMANIAN M ODEL 1 I TERATION 5 TRANSLATION TABLE FROM ROMANIAN TO E NGLISH M ODEL 1 I TERATION 5 BACKOFF FERTILITY ( FERTILITY ESTIMATED OVER ALL E NGLISH WORDS ) ZERO FERTILITY E NGLISH WORD PENALTY NON - ZERO FERTILITY E NGLISH WORD PENALTY consider English to be the source language and Romanian the target language. The log-linear alignment model is specified by equation 3. The model assigns non-zero probabilities only to 1-to-many alignments, like Model 4. (Cettolo and Federico, 2004) used a log-linear model trained using error minimization for the translation task, 3 of the submodels were taken from Model 4 in a similar way to our first 5 submodels. P exp( m λm hm (f, a, e)) P pλ (a, f |e) = P (3) f,e,a exp( m λm hm (f, a, e)) Given λ, the alignment search problem is to find the alignment a of highest probability according to equation 3. We solve this using the local search defined in (Brown et al., 1993). We set λ as follows. Given a sequence A of alignments we can calculate an error function, E(A). For these experiments average sentence AER was used. We wish to minimize"
W05-0814,W03-0305,0,0.037944,"ind a setting for λ that reduces AER on discriminative training set (new D-step) We use the first 148 sentences of the 2003 test set for the discriminative training set. 10 settings for λ are found, the hypothesis list is augmented using the results of 10 searches using these settings, and then another 10 settings for λ are found. We then select the best λ. The discriminative training regimen is otherwise similar to (Och, 2003). 5 Experiments Table 2 provides a comparison of our baseline systems using the “refined” symmetrization metric with the best limited resources track system from WPT03 (Dejean et al., 2003) on the 2003 test set. The best results are obtained by stemming both English and Romanian words to the first four letters, as described in section 2. Table 3 provides details on our shared task submission. RUN1 is the word-based baseline system. RUN2 is the stem-based baseline system. RUN4 uses only the first 6 submodels, while RUN5 uses all 11 submodels. RUN3 had errors in processing, so we omit it. Results: • Our new 1-to-many alignment model and training method are successful, producing decreases of 0.03 AER when the source is Romanian, and 0.01 AER when the source is English. Table 2: Sum"
W05-0814,J03-1002,0,0.0414906,"discuss results on the shared task of Romanian-English word alignment. The baseline technique is that of symmetrizing two word alignments automatically generated using IBM Model 4. A simple vocabulary reduction technique results in an improvement in performance. We also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate. 1 Introduction ISI participated in the WPT05 Romanian-English word alignment task. The system used for baseline experiments is two runs of IBM Model 4 (Brown et al., 1993) in the GIZA++ (Och and Ney, 2003) implementation, which includes smoothing extensions to Model 4. For symmetrization, we found that Och and Ney’s “refined” technique described in (Och and Ney, 2003) produced the best AER for this data set under all experimental conditions. We experimented with a statistical model for inducing a stemmer cross-lingually, but found that the best performance was obtained by simply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating the maximization of likelihood and"
W05-0814,P03-1021,0,0.00824257,"(a, f |e) = P (3) f,e,a exp( m λm hm (f, a, e)) Given λ, the alignment search problem is to find the alignment a of highest probability according to equation 3. We solve this using the local search defined in (Brown et al., 1993). We set λ as follows. Given a sequence A of alignments we can calculate an error function, E(A). For these experiments average sentence AER was used. We wish to minimize this error function, so we select λ accordingly: argmin λ X a ˜ E(˜ a)δ(˜ a, (argmax pλ (a, f |e))) (4) a Maximizing performance for all of the weights at once is not computationally tractable, but (Och, 2003) has described an efficient one-dimensional search for a similar problem. We search over each λm (holding the others constant) using this technique to find the best λm to update and the best value to update it to. We repeat the process until no further gain can be found. Our new training method is: REPEAT • Start with submodels and lambda from previous iteration 93 • Find Viterbi alignments on entire training corpus using new model (similar to E-step of Model 4 training) • Reestimate submodel parameters from Viterbi alignments (similar to M-step of Model 4 Viterbi training) • Find a setting fo"
W05-0814,P03-1050,0,0.0143884,"r than using likelihood training. Turning off the extensions to GIZA++ and training p0 as in (Brown et al., 1993) produces a substantial increase in AER. 3 Vocabulary Size Reduction Romanian is a Romance language which has a system of suffixes for inflection which is richer than English. Given the small amount of training data, we decided that vocabulary size reduction was desirable. As a baseline for vocabulary reduction, we tried reducing words to prefixes of varying sizes for both English and Romanian after lowercasing the corpora. We also tried Porter stemming (Porter, 1997) for English. (Rogati et al., 2003) extended Model 1 with an additional hidden variable to represent the split points in Arabic between the prefix, the stem and the suffix to generate a stemming for use in Cross-Lingual Information Retrieval. As in (Rogati et al., 2003), we can find the most probable stemming given the model, apply this stemming, and retrain our word alignment system. However, we can also use the modified model directly to find the best word alignment without converting the text to its stemmed form. We introduce a variable rj for the Romanian stem and a variable sj for the Romanian suffix (which when concatenat"
W05-0814,C96-2141,0,0.157821,"y simply lowercasing both the English and Romanian text and removing all but the first four characters of each word. We also tried a new model and a new training criterion based on alternating the maximization of likelihood and minimization of the alignment error rate. For these experiments, we have implemented an alignment package for IBM Model 4 using a hillclimbing search and Viterbi training as described in (Brown et al., 1993), and extended this to use new submodels. The starting point is the final alignment generated using GIZA++’s implementation of IBM Model 1 and the Aachen HMM model (Vogel et al., 1996). Paper organization: Section 2 is on the baseline, Section 3 discusses vocabulary reduction, Section 4 introduces our new model and training method, Section 5 describes experiments, Section 6 concludes. We use the following notation: e refers to an English sentence composed of English words labeled ei . f refers to a Romanian sentence composed of Romanian words labeled fj . a is an alignment of e to f . We use the term “Viterbi alignment” to denote the most probable alignment we can find, rather than the true Viterbi alignment. 2 Baseline To train our systems, Model 4 was trained two times, f"
W06-1606,J93-2003,0,0.0178886,"Missing"
W06-1606,P05-1033,0,0.882263,"(Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less data, have shown promising results when compared on the same test sets with mature phrase-based systems. To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task. In this paper we present such a system. Our translation models rely upon and naturally exploit submodels (feature functions) that have 2.1 An intuitive introduction to SPMT After being exposed to 100M+ words o"
W06-1606,J04-4002,0,0.831104,"de with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less"
W06-1606,J03-4003,0,0.00529773,"by the following derivation: r4 (r9 (r7 ), r3 (r6 (r12 (r8 )))). Figure 2: English parse tree derivation of the Chinese string COMINGFROM FRANCE AND RUSSIA pDE ASTRO- -NAUTS. ple, c(Θ) = (π, F, A). The probability of each derivation θi is given by the product of the probabilities of all the rules p(rj ) in the derivation (see equation 4). P r(π, F, A) = X Y p(rj ) (4) θi ∈Θ,c(Θ)=(π,F,A) rj ∈θi In order to acquire the rules specific to our model and to induce their probabilities, we parse the English side of our corpus with an in-house implementation (Soricut, 2005) of Collins parsing models (Collins, 2003) and we word-align the parallel corpus with the Giza++2 implementation of the IBM models (Brown et al., 1993). We use the automatically derived hEnglish-parse-tree, English-sentence, Foreign-sentence, Word-levelalignmenti tuples in order to induce xRS rules for several models. 2.2.2 SPMT Model 1 In our simplest model, we assume that each tuple (π, F, A) in our automatically annotated corpus could be produced by applying a combination of minimally syntactified, lexicalized, phrase-based compatible xRS rules, and minimal/necessary, non-lexicalized xRS rules. We call a rule non-lexicalized whenev"
W06-1606,N04-1035,1,0.399768,"minimal rules (lexicalized and non-lexicalized) by applying the algorithm proposed by Galley et al. (2006) and then remove the lexicalized rules. We remove the Galley et al.’s lexicalized rules because they are either already accounted for by the minimally syntactified, lexicalized, phrasebased-compatible xRS rules or they subsume noncontinuous source-target phrase pairs. It is worth mentioning that, in our framework, a rule is defined to be “minimal” with respect to a foreign/source language phrase, i.e., it is the minimal xRS rule that yields that source phrase. In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (π, F, A) tuple. Under SPMT model 1, the tree in Figure 2 can be produced, for example, by the following derivation: r4 (r9 (r7 ), r3 (r6 (r12 (r8 )))). Figure 2: English parse tree derivation of the Chinese string COMINGFROM FRANCE AND RUSSIA pDE ASTRO- -NAUTS. ple, c(Θ) = (π, F, A). The probability of each derivation θi is given by the product of the probabilities of all the rules p(rj ) in the derivation (see equation 4). P r(π, F, A) = X Y p(rj ) (4) θi ∈Θ,c(Θ)=(π,F,A) rj ∈θi In order to acquire the rules s"
W06-1606,P03-1021,0,0.664719,"iments described in this paper, we use the following submodels (feature functions): Syntax-based-like submodels: • m1inv(ri ) is the IBM model 1 inverse probability computed over the bags of words that occur on the source and target sides of a rule. • lm(e) is the language model probability of the target translation under an ngram language model. • wp(e) is a word penalty model designed to favor longer translations. All these models are combined log-linearly during decoding. The weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by Och (2003). The phrase-based-like submodels have been proved useful in phrase-based approaches to SMT (Och and Ney, 2004). The first two syntaxbased submodels implement a “fused” translation and lexical grounded distortion model (p root ) and a syntax-based distortion model (p cfg ). The indicator submodels are used to determine the extent to which our system prefers lexicalized vs. nonlexicalized rules; simple vs. composed rules; and high vs. low count rules. • proot (ri ) is the root normalized conditional probability of all the rules in a model. • pcfg (ri ) is the CFG-like probability of the non-lex"
W06-1606,P05-1034,0,0.755662,"Missing"
W06-1606,N04-4026,0,0.108335,"Missing"
W06-1606,N03-1017,1,0.0780121,"Section 5, we conclude with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained o"
W06-1606,N06-1033,1,0.131843,"by definition pcfg = 1. 48 3 Decoding 4 Experiments 4.1 3.1 Decoding with one SPMT model We evaluate our models on a Chinese to English machine translation task. We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences. The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006). The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3. • PBMT, a strong state of the art phrase-based system that implements the alignment template model (Och and Ney, 2004); this is the system ISI has used in the 2004 and 2005 NIST evaluations. • four SPMT systems (M1, M1C, M2, M2C) that implement each of the models discussed in this paper; The decoder is capable of producing nbest derivations and nbest lists (Knight and Graehl, 2005), which are used for Maximum"
W06-1606,W02-1018,1,0.779424,"odels empirically. In Section 5, we conclude with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spit"
W06-1606,E06-1005,0,0.0241966,"Missing"
W06-1606,P06-1121,1,\N,Missing
W16-5907,P10-1131,0,0.0467125,"Missing"
W16-5907,N10-1083,0,0.157395,"ampling methods like MCMC has enabled the development of unsupervised systems for tag and grammar induction, alignment, topic models and more. These latent variable models discover hidden structure in text which aligns to known linguistic phenomena and whose clusters are easily identifiable. Recently, much of supervised NLP has found great success by augmenting or replacing context, features, and word representations with embeddings derived from Deep Neural Networks. These models allow for learning highly expressive non-convex functions by simply backpropagating prediction errors. Inspired by Berg-Kirkpatrick et al. (2010), who bridged the gap between supervised and unsupervised training with features, we bring neural networks to unsupervised learning by providing evidence that even in ∗ This research was carried out while all authors were at the Information Sciences Institute. unsupervised settings, simple neural network models trained to maximize the marginal likelihood can outperform more complicated models that use expensive inference. In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete da"
W16-5907,P15-2143,1,0.836803,"damental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun. Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings. z1 zt 1 zt zt+1 zT x1 xt 1 xt xt+1 xT Figure 1: Pictorial representation of a Hidden Markov Model. Latent variable (zt ) transitions depend on the previous value (zt−1 ), and emit an observed word (xt ) at each time step. 3.1 The Hidden Markov Model A common model for this task, and our primary workhorse, is the Hidden Markov Model trained with the unsupervised message passing algorithm, BaumWelch (Welch, 2003). Model HMMs model a sentence by assuming that (a) every word token is generated by a latent clas"
W16-5907,P11-1087,0,0.098067,"multiplying transitions and emissions across all time steps (Eq. 6). Finding the optimal sequence of latent classes corresponds to computing an argmax over the values of z. p(x, z) = n+1 Y t=1 p(zt |zt−1 ) n Y p(xt |zt ) (6) t=1 Because our task is unsupervised we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm’s outline is provided in Algorithm 1. Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p(zt = i, zt+1 = j |x, θ) ∝ αi (t)p(zt+1 = j|zt = i) ×βj (t + 1)p(xt+1 |zt+1 = j) Update θ until Converged inclu"
W16-5907,J92-4003,0,0.464732,"the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers. 3.2 Additional Comparisons While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities. Brown clusters do not account for a word’s membership in multiple syntactic classes, but are a very strong baseline for tag induction. It is possible our approach could be improved by augmenting our objective function to include mutual in"
W16-5907,D10-1056,0,0.224537,"nce is extracted for every sentence and evaluated on three metrics. Many-to-One (M-1) Many-to-one computes the most common true part-of-speech tag for each cluster. It then computes tagging accuracy as if the cluster were replaced with that tag. This metric is easily gamed by introducing a large number of clusters. One-to-One (1-1) One-to-One performs the same computation as Many-to-One but only one cluster is allowed to be assigned to a given tag. This prevents the gaming of M-1. V-Measure (VM) V-Measure is an F-measure which trades off conditional entropy between the clusters and gold tags. Christodoulopoulos et al. (2010) found VM is to be the most informative and consistent metric, in part because it is agnostic to the number of induced tags. 8 Data and Parameters To evaluate our approaches, we follow the existing literature and train and test on the full WSJ corpus. 1 This interpretation does not complicate the computation of forward-backward messages when running Baum-Welch, though it does, by design, break Markovian assumption about knowledge of the past. Initialization In addition to architectural choices we have to initialize all of our parameters. Word embeddings (and character embeddings in the CNN) ar"
W16-5907,D11-1059,0,0.0188252,"will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers. 3.2 Additional Comparisons While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities. Brown clusters do not account for a word’s membership in multiple syntactic classes, but are a very strong baseline for tag induction. It is possible our approach could be improved by augmenting our objective function to include mutual information computations or a bias towards a harder clustering. 4 Neural HMM The aforementioned training of an"
W16-5907,D11-1005,0,0.0365435,"elihood can outperform more complicated models that use expensive inference. In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data. Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014). Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily applied to other latent variable models where inference i"
W16-5907,P11-1061,0,0.0262916,"ral network models trained to maximize the marginal likelihood can outperform more complicated models that use expensive inference. In this work, we show how a single latent variable sequence model, Hidden Markov Models (HMMs), can be implemented with neural networks by simply optimizing the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data. Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014). Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily app"
W16-5907,D07-1031,0,0.0531705,"s are latent. The probability of a given sequence of observations x and latent variables z is given by multiplying transitions and emissions across all time steps (Eq. 6). Finding the optimal sequence of latent classes corresponds to computing an argmax over the values of z. p(x, z) = n+1 Y t=1 p(zt |zt−1 ) n Y p(xt |zt ) (6) t=1 Because our task is unsupervised we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm’s outline is provided in Algorithm 1. Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p"
W16-5907,N15-1144,0,0.249275,"ed we do not have a priori access to these distributions, but they can be estimated via Baum-Welch. The algorithm’s outline is provided in Algorithm 1. Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007). Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p(zt = i, zt+1 = j |x, θ) ∝ αi (t)p(zt+1 = j|zt = i) ×βj (t + 1)p(xt+1 |zt+1 = j) Update θ until Converged including the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016)"
W16-5907,D15-1166,0,0.0607821,"of simply maximizing performance on tag induction, a more subtle, but powerful contribution of this work may be its demonstration of the easy and effective nature of using neural networks with Bayesian models traditionally trained by EM. We hope this approach scales well to many other domains and tasks. Weight Initialization If we run our best model (NHMM+Conv+LSTM) with all the weights initialized from a uniform distribution U(−10−4 , 10−4 )3 we find a dramatic drop in V-Measure performance (61.7 vs 71.7 in Table 3). This is consistent with the common wisdom that unlike supervised learning (Luong et al., 2015), weight initialization is important to achieve good performance on unsupervised tasks. It is possible that performance could be further enhance via the popular technique of ensembling, would would allow for combining models which converged to different local optima. This work was supported by Contracts W911NF-151-0543 and HR0011-15-C-0115 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). Additional thanks to Christos Christodoulopoulos. LSTM Layers And Dropout We find that dropout is important in training an unsupervised NHMM. References 3 We ch"
W16-5907,H94-1020,0,0.462941,"θ ln p(x, z |θ) is easy to evaluate, we can perform direct marginal likelihood optimization (Salakhutdinov et al., 2003). We do not address here the question of semi-supervised training, but believe the framework we present lends itself naturally to the incorporation of constraints or labeled data. Next, we demonstrate the application of this framework to HMMs in the service of part-of-speech tag induction. 3 Part-of-Speech Induction Part-of-speech tags encode morphosyntactic information about a language and are a fundamental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun. Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word em"
W16-5907,N16-1076,0,0.0212823,"ly, Lin et al. (2015) extended several models 65 Algorithm 1 Baum-Welch Algorithm Randomly Initialize distributions (θ) repeat Compute forward messages: ∀i,t αi (t) Compute backward messages: ∀i,t βi (t) Compute posteriors: p(zt = i |x, θ) ∝ αi (t)βi (t) p(zt = i, zt+1 = j |x, θ) ∝ αi (t)p(zt+1 = j|zt = i) ×βj (t + 1)p(xt+1 |zt+1 = j) Update θ until Converged including the HMM to include pre-trained word embeddings learned by different skip-gram models. Our work will fully neuralize the HMM and learn embeddings during the training of our generative model. There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers. 3.2 Additional Comparisons While the main focus of our paper is the seamless extension of an unsupervised generative latent variable model with neural networks, for completeness we will also include comparisons to other techniques which do not adhere to the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word type"
W16-5907,D14-1197,0,0.021002,"the incomplete data likelihood. The key insight is to perform standard forward-backward inference to compute posteriors of latent variables and then backpropagate the posteriors through the networks to maximize the likelihood of the data. Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al., 1991). Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014). Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016). Common to this work and ours is the use of neural networks to produce potentials. The approach presented here is easily applied to other latent variable models where inference is tractable and are typically trained with EM. We believe there are three important strengths: 1. Using a neural network to produce model probabilities allows for seamless integration of additional context not easily represented by conditi"
W16-5907,D11-1118,0,0.0732365,"a language and are a fundamental tool in downstream NLP applications. In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of taking raw text and both discovering these latent clusters and assigning them to words in situ. Classes can be very specific (e.g. six types of verbs in English) to their syntactic role. Example tags are shown in Table 1. In this example, board is labeled as a singular noun while Pierre Vinken is a singular proper noun. Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings. z1 zt 1 zt zt+1 zT x1 xt 1 xt xt+1 xT Figure 1: Pictorial representation of a Hidden Markov Model. Latent variable (zt ) transitions depend on the previous value (zt−1 ), and emit an observed word (xt ) at each time step. 3.1 The Hidden Markov Model A common model for this task, and our primary workhorse, is the Hidden Markov Model trained with the unsupervised message passing algorithm, BaumWelch (Welch, 2003). Model HMMs model a sentence by assuming that (a) every word token is genera"
W16-5907,N16-1036,1,0.81703,"ut also reduces the computational cost of comput2 This is the default parameter initialization in Torch. 68 M-1 1-1 VM Base HMM Brown 62.5 41.4 53.3 68.2 49.9 63.0 SOTA Architecture The first parameter is the number of hidden units. We chose 512 because it was the largest power of two we could fit in memory. When we extended our model to include the convolutional emission network, we only used 128 units, due to the intensive computation of Char-CNN over the whole vocabulary per minibatch. The second design choice was the number of LSTM layers. We used a three layer LSTM as it worked well for (Tran et al., 2016), and we applied dropout (Srivastava et al., 2014) over the vertical connections of the LSTMs (Pham et al., 2014) with a rate of 0.5. Finally, the maximum number of inner loop updates applied per batch is set to six. We train all the models for five epochs and perform gradient clipping whenever the gradient norm is greater than five. To determine when to stop applying the gradient during training we simply check when the log probability < 10−4 ) or if the maximum has converged ( new−old old number of inner loops has been reached. All optimization was done using Adam (Kingma and Ba, 2015) with"
W16-5907,D13-1140,1,0.635974,"ore data, would yield even greater gains. 11 Future Work In addition to parameter tuning and multilingual evaluation, the biggest open questions for our approach are the effects of additional data and augmenting the loss function. Neural networks are notoriously data hungry, indicating that while we achieve competitive results, it is possible our model will scale well when run with large corpora. This would likely require the use of techniques like NCE (Gutmann and Hyvärinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013). Secondly, despite focusing on ways to augment an HMM, Brown clustering and systems inspired by it perform very well. They aim to maximize mutual information rather than likelihood. It is possible that augmenting or constraining our loss will yield additional performance gains. Outside of simply maximizing performance on tag induction, a more subtle, but powerful contribution of this work may be its demonstration of the easy and effective nature of using neural networks with Bayesian models traditionally trained by EM. We hope this approach scales well to many other domains and tasks. Weight"
W16-5907,D12-1086,0,0.0771546,"Missing"
W17-2315,W13-2004,0,0.0600022,"Missing"
W17-2315,W09-1404,0,0.0199083,"rly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs"
W17-2315,W13-2003,0,0.0848906,"Missing"
W17-2315,W11-1827,0,0.0615738,"Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs in the AMR graph. AMR has bee"
W17-2315,W13-2014,0,0.103813,"Missing"
W17-2315,W09-1401,0,0.669066,"ntifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community has been working towards the goal of creating a curated knowledge base of biomolecule entity interactions. The scientific literature in the biomedical domain runs to millions of articles and is an excellent source of such information. However, automatically extracting information from text is a challenge because natural language allows us to express the same information in several different ways. The series of Genia Event Extraction shared tasks (Kim et al., 2009, 2011, 2013, 2016) has resulted in various significant approaches to biomolecule event extraction spanning methods that use learnt patterns from annotated text (Bui et al., 2013) to machine learning methods (Bj¨orne and Salakoski, 2013) that use syntactic parses as features. In this work, we find that a semantic analysis of text that relies on Abstract Meaning Representations (Banarescu et al., 2013) is highly useful because it normalizes many lexical and syntactic variations in text. 1 E1 = phosphorylation of radixin; E2 = LPA induces E1. We hypothesize that an event structure is a subThis d"
W17-2315,W16-3003,0,0.0251471,"Missing"
W17-2315,W09-1407,0,0.0421696,"relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point an"
W17-2315,W11-1802,0,0.209454,"fying the relation between them. For e.g. in figure 1 the path {‘induce-01’, ‘arg0’, ‘LPA’} suggests that LPA is the cause of induce. We encode this path using word embeddings pre-trained on millions of biomedical text and develop two pipelined neural network models: (a) to identify the theme of an interaction; and (b) to identify the cause of the interaction, if there exists one. 2 2.1 AMR based event extraction model Task description The biomedical event extraction task in this work is adopted from the Genia Event Extraction subtask of the well-known BioNLP shared task ((Kim et al., 2009), (Kim et al., 2011), (Kim et al., 2013)). Table 2 shows a sample event annotation for the sentence in Figure 1. The protein annotations T1- T4 are given as starting points. The task is to identify the events E1-E4 with their interaction type and arguments. Table 1 describes the various event types and the arguments they accept. The first four event types require only unary theme argument. The binding event can take a variable number of theme arguments. The last four events take a theme argument and, when expressed, also a cause argument. Their theme or cause may in turn be another event, creating a nested event"
W17-2315,P14-1134,0,0.022913,"1 Figure 1: AMR with sample event annotations for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several"
W17-2315,W13-2010,0,0.0148371,"column corresponds to the results of EVEX (Hakala et al., 2013) model on the 2013 test set. Certain notable numbers are emphasized and discussed under results 5.4. limited annotated data. Distant supervision techniques have been successfully used before for relation extraction (Mintz et al., 2009) in general domain. Recent work by (Liu et al., 2014) uses minimal supervision strategy for extracting relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (B"
W17-2315,W13-2005,0,0.0151187,"to the results of EVEX (Hakala et al., 2013) model on the 2013 test set. Certain notable numbers are emphasized and discussed under results 5.4. limited annotated data. Distant supervision techniques have been successfully used before for relation extraction (Mintz et al., 2009) in general domain. Recent work by (Liu et al., 2014) uses minimal supervision strategy for extracting relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 201"
W17-2315,S16-1166,0,0.0218552,"This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community has been worki"
W17-2315,P12-1076,0,0.0200641,"e may in turn be another event, creating a nested event (For e.g. event E2 in Table 2). Experimental results show that our model, although achieves a reasonable precision, suffers from low recall. Our third contribution is a distant supervision (Mintz et al., 2009) based approach to collect additional annotated training data. Distant supervision works on the assumption that given a known relation between two entities, a sentence containing the two entities is likely to express this relation and hence can serve as training data for that relation. Data gathered using such a method can be noisy (Takamatsu et al., 2012). Roth et al. (2013) have discussed several prior work that address this issue. In our work, we introduce a method based on AMR path heuristic 2.2 Model description We cast this event extraction problem as a subgraph identification problem. Given a sentence we 127 Event Type Gene expression Transcription Localization Protein catabolism ==[SVT-TOTAL]== Binding Phosphorylation Regulation Positive regulation Negative regulation ==[REG-TOTAL]== ==[ALL-TOTAL]== first obtain its AMR graph automatically using an AMR parser (Pust et al., 2015). Next, we identify protein nodes and interaction nodes in"
W17-2315,N15-1040,0,0.0124831,"ample event annotations for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biom"
W17-2315,P09-1113,0,0.228838,"1-E4 with their interaction type and arguments. Table 1 describes the various event types and the arguments they accept. The first four event types require only unary theme argument. The binding event can take a variable number of theme arguments. The last four events take a theme argument and, when expressed, also a cause argument. Their theme or cause may in turn be another event, creating a nested event (For e.g. event E2 in Table 2). Experimental results show that our model, although achieves a reasonable precision, suffers from low recall. Our third contribution is a distant supervision (Mintz et al., 2009) based approach to collect additional annotated training data. Distant supervision works on the assumption that given a known relation between two entities, a sentence containing the two entities is likely to express this relation and hence can serve as training data for that relation. Data gathered using such a method can be noisy (Takamatsu et al., 2012). Roth et al. (2013) have discussed several prior work that address this issue. In our work, we introduce a method based on AMR path heuristic 2.2 Model description We cast this event extraction problem as a subgraph identification problem. G"
W17-2315,N15-1119,1,0.71907,"our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs in the AMR graph. AMR has been successfully used for deeper semantic tasks like entity linking (Pan et al., 2015) and abstractive summarization (Mihalcea et al., 2015). Work by Garg et al. (2015) is the first one to make use of AMR representation for extracting interactions from biomedical text. They use graph kernel methods to answer the binary question of whether a given AMR subgraph expresses an interaction or not. Our work departs from theirs in that they concentrate only on binary interactions whereas we use AMR to identify complex nested events. Also, our approach additionally makes use of distant supervision to cope with the problem of 7 Conclusion In this work, we show the effectiveness of using"
W17-2315,D15-1136,1,0.798412,"ions for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community ha"
W17-2315,W11-1807,0,0.0121183,"th between pk and pi via tj ; and the label is 1 if pk is the cause of the event em , 0 otherwise. 5.3 Results and Discussion 6 Related work The biomedical event extraction task described in this work was first introduced in the BioNLP Shared Task in 2009 (Kim et al., 2009). This task helped shift the focus of relation extraction efforts from identifying simple binary interactions to identifying complex nested events that better represent the biological interactions stated frequently in text. Existing approaches to this task include SVM (Bj¨orne and Salakoski, 2013) other ML based approaches (Riedel and McCallum, 2011; Miwa et al., 2010, 2012). Methods like LSTM model setup We implement our LSTM model using the lasagne library. For the first LSTM model, we use softmax as our non-linear function and optimize the cat132 Event Type Gene expression Transcription Localization Protein catabolism ==[SVT-TOTAL]== Binding Phosphorylation Regulation Positive regulation Negative regulation ==[REG-TOTAL]== ==[ALL-TOTAL]== Recall 66.33 55.10 36.55 73.33 57.82 27.61 49.21 16.30 25.98 23.17 21.81 44.42 LSTM Precision 66.55 28.57 63.72 84.62 60.86 25.94 53.75 29.18 35.16 30.50 31.61 51.01 F1 66.44 37.63 46.45 78.57 57.27"
W17-2315,W13-2002,0,\N,Missing
W17-2315,W13-2322,1,\N,Missing
W98-0301,P97-1012,0,0.0136251,"cord, for each elementary unit, the set of parenthetical units that belong to it. • Actions SET_AND (SET_OR) instructs the analyzer to store the information that the input stream contains the lexeme and (or). • Action DUAL instructs the analyzer to insert a textual boundary immediately before the cue phrase under consideration if there is no other cue phrase that immediately precedes it. If there exists such a cue phrase, the analyzer will behave as in the case of the action COMMA. The action DUAL is usually associated with cue phrases that can introduce some expectations about the discourse (Cristea and Webber, 1997). For example, the cue phrase although in text (5) signals a rhetorical relation of CONCESSION between the clause to which it belongs and the previous clause. However, in text (6), where although is preceded by an and, it signals a rhetorical relation of CONCESSION between the clause to which it belongs and the next clause in the text. (5) [I went to the theater] [although I had a terrible headache.] (6) [The trip was fun,] [and although we were badly bitten by blackflies,] [I do not regret it.] or a discourse marker whose associated action is COMMA_PAREN has been identified, the algorithm ign"
W98-0301,J86-3001,0,0.083759,"Missing"
W98-0301,J97-1003,0,0.0503209,"and discourse markers in unrestricted natural language texts. The knowledge was derived from a comprehensive corpus analysis. 1 Motivation The automatic identification of discourse segments and discourse markers in unrestricted texts is crucial for solving many outstanding problems in natural language processing, which range from syntactic and semantic analysis, to anaphora resolution and text summarization. Most of the algorithmic research in discourse segmentation focused on segments of coarse granularity (Grosz and Hirschberg, 1992; Hirschberg and Litman, 1993; Passonneau and Litman, 1997; Hearst, 1997; Yaari, 1997). These segments were defined intentionally in terms of Grosz and Sidner&apos;s theory (1986) or in terms of an intuitive notion of ""topic"". However, in case of applications such as anaphora resolution, discourse parsing, and text summarization, even sentences might prove to be too large discourse segments. For example, if we are to defive the discourse structure of texts using an RSTlike representation (Mann and Thompson, 1988), we will need to determine the elementary textual units that contribute rhetorically to the understanding of those texts; usually, these units are clause-like"
W98-0301,J93-3003,0,0.0649021,"ges in order to determine automatically clause boundaries and discourse markers in unrestricted natural language texts. The knowledge was derived from a comprehensive corpus analysis. 1 Motivation The automatic identification of discourse segments and discourse markers in unrestricted texts is crucial for solving many outstanding problems in natural language processing, which range from syntactic and semantic analysis, to anaphora resolution and text summarization. Most of the algorithmic research in discourse segmentation focused on segments of coarse granularity (Grosz and Hirschberg, 1992; Hirschberg and Litman, 1993; Passonneau and Litman, 1997; Hearst, 1997; Yaari, 1997). These segments were defined intentionally in terms of Grosz and Sidner&apos;s theory (1986) or in terms of an intuitive notion of ""topic"". However, in case of applications such as anaphora resolution, discourse parsing, and text summarization, even sentences might prove to be too large discourse segments. For example, if we are to defive the discourse structure of texts using an RSTlike representation (Mann and Thompson, 1988), we will need to determine the elementary textual units that contribute rhetorically to the understanding of those"
W98-0301,W97-0713,1,0.927692,"tive notion of ""topic"". However, in case of applications such as anaphora resolution, discourse parsing, and text summarization, even sentences might prove to be too large discourse segments. For example, if we are to defive the discourse structure of texts using an RSTlike representation (Mann and Thompson, 1988), we will need to determine the elementary textual units that contribute rhetorically to the understanding of those texts; usually, these units are clause-like units. Also, if we want to select the most important parts o f a text, sentences might prove again to be too large segments (Marcu, 1997a; Teufel and Moens, 1998): in some cases, only one of the clauses that make up a sentence should be selected for summarization. In this paper, I present a surface-based algorithm that uses cue phrases (connectives) in order to deterrnine not only the elementary textual units of text but also the phrases that have a discourse function. The algorithm is empirically grounded in an extensive corpus analysis of cue phrases and is consistent with the psycholinguistic position advocated by Caron (1997, p. 70). Caron argues that ""&apos;rather than conveying information about states of things, connectives"
W98-0301,J97-1005,0,0.0348442,"omatically clause boundaries and discourse markers in unrestricted natural language texts. The knowledge was derived from a comprehensive corpus analysis. 1 Motivation The automatic identification of discourse segments and discourse markers in unrestricted texts is crucial for solving many outstanding problems in natural language processing, which range from syntactic and semantic analysis, to anaphora resolution and text summarization. Most of the algorithmic research in discourse segmentation focused on segments of coarse granularity (Grosz and Hirschberg, 1992; Hirschberg and Litman, 1993; Passonneau and Litman, 1997; Hearst, 1997; Yaari, 1997). These segments were defined intentionally in terms of Grosz and Sidner&apos;s theory (1986) or in terms of an intuitive notion of ""topic"". However, in case of applications such as anaphora resolution, discourse parsing, and text summarization, even sentences might prove to be too large discourse segments. For example, if we are to defive the discourse structure of texts using an RSTlike representation (Mann and Thompson, 1988), we will need to determine the elementary textual units that contribute rhetorically to the understanding of those texts; usually, these units a"
W98-1124,J97-1003,0,0.00736203,"heories is that good texts exhibit a well-defined topical structure. In our approach, we assume that a discourse tree is ""better"" if it exhibits a high-level structure that matches as much as possible the topical boundaries of the text for which that structure is built. In order to capture this intuition, when we build discourse trees, we associate with each node of a tree a clustering score. For the leaves, this score is 0; for the internal nodes, the score is given by the similarity between the immediate children. The similarity is computed using a traditional cosine metric, in the style of Hearst (1997). We consider that a discourse tree ,4 is ""better"" than another discourse tree t3 if the sum of the clustering scores associated with the nodes of A is higher than the sum of the clustering scores associated with the nodes of/3. The m a r k e r - b a s e d metric. Naturally occurring texts use a wide range of discourse markers, which signal coherence relations between textual spans of various sizes. We assume that a discourse structure should reflect explicitly as many of the discourse relations that are signaled by discourse markers. In other words, we assume that a discourse structure .4 is"
W98-1124,A97-1042,0,0.288282,"the structure of discourse. 1 Motivation Current approaches to automatic summarization employ techniques that assume that textual salience correlates with a wide range of linguistic phenomena. Some of these approaches assume that important textual units contain words that are used frequently (Luhn, 1958; Edmundson, 1968) or words that are used in the title and section headings (Edmundson, 1968). Some of them assume that important sentences are located at the beginning or end of paragraphs (Baxendale, 1958) or at positions that can be determined through training for each particular text genre (Lin and Hovy, 1997). Other systems assume that important sentences in texts contain ""bonus"" words and phrases, such as significant, important, in conclusion and In this paper we show, while unimportant sentences contain ""stigma"" words such as hardly and impossible (Edmundson, 1968; Kupiec et al., 1995; Teufel and Moens, 1997). Other systems assume that important sentences and concepts are the highest connected entities in more or less elaborate semantic structures (Skorochodko, 1971; Hoey, 1991; Salt0n and Allan, 1995; Mani and Bloedorn, 1998; Barzilay and Elhadad, 199_7). And, yet, others assume that important"
W98-1124,W97-0713,1,0.850014,"as significant, important, in conclusion and In this paper we show, while unimportant sentences contain ""stigma"" words such as hardly and impossible (Edmundson, 1968; Kupiec et al., 1995; Teufel and Moens, 1997). Other systems assume that important sentences and concepts are the highest connected entities in more or less elaborate semantic structures (Skorochodko, 1971; Hoey, 1991; Salt0n and Allan, 1995; Mani and Bloedorn, 1998; Barzilay and Elhadad, 199_7). And, yet, others assume that important sentences and clauses are derivable from a discourse representation of texts (Ono et al., 1994; Marcu, 1997a; Marcu, 1997c). A variety of systems (Edmundson, 1968; Kupiec et al., 1995; Teufel and Moens, 1997; Lin. 1998; Mani and Bloedorn, 1998) were designed to integrate subsets of the heuristics mentioned above. In these approaches, 206 More precisely, we study the relationship between the structure of discourse and a set of summarization heuristics that are employed by current systems. A tight coupling of the two, which is achieved by applying a simple learning mechanism, gives us two advantages over previous methods. First, two corpora of manually built summaries enable us to learn genre-specifi"
W98-1124,P97-1013,1,0.542861,"as significant, important, in conclusion and In this paper we show, while unimportant sentences contain ""stigma"" words such as hardly and impossible (Edmundson, 1968; Kupiec et al., 1995; Teufel and Moens, 1997). Other systems assume that important sentences and concepts are the highest connected entities in more or less elaborate semantic structures (Skorochodko, 1971; Hoey, 1991; Salt0n and Allan, 1995; Mani and Bloedorn, 1998; Barzilay and Elhadad, 199_7). And, yet, others assume that important sentences and clauses are derivable from a discourse representation of texts (Ono et al., 1994; Marcu, 1997a; Marcu, 1997c). A variety of systems (Edmundson, 1968; Kupiec et al., 1995; Teufel and Moens, 1997; Lin. 1998; Mani and Bloedorn, 1998) were designed to integrate subsets of the heuristics mentioned above. In these approaches, 206 More precisely, we study the relationship between the structure of discourse and a set of summarization heuristics that are employed by current systems. A tight coupling of the two, which is achieved by applying a simple learning mechanism, gives us two advantages over previous methods. First, two corpora of manually built summaries enable us to learn genre-specifi"
W98-1124,W97-0710,0,0.0188644,"Missing"
W98-1124,W97-0703,0,\N,Missing
W98-1124,C94-1056,0,\N,Missing
W99-0106,P98-1011,0,0.0305787,"Missing"
W99-0106,P98-1044,1,0.870719,"Missing"
W99-0106,J95-2003,0,0.183646,"Missing"
W99-0106,P98-1090,0,0.0343354,"Missing"
W99-0106,J94-4002,0,0.270617,"Computer Science Vassar College Poughkeepsie, NY, USA dcristea @ info iasi. r@ ideOcs.vassar.edu Daniel Marcu Information Sciences Institute and Department of Computer Science University of Southern California Los Angeles, CA, USA Valentin Tablan Department of Computer Science University &quot;A.I. Cuza&apos;&quot; la~i, Rom~Lnia valyt@ infoiasi.ro marcu @ isi. edu Abstract PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, frequency, proximity, etc. In some cases, anaphora resolution systems implement these modules explicitly (I-Iobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in Order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981; Gmsz and Sidner, 1986; Grosz et aL, 1995; Fox, 1987; Vonk et al., 1992; Azzam et al., 1998; Hitzeman and P.oesio, 1998). These studies claim, on th"
W99-0106,W99-0307,1,0.785428,"998). 3 The Experiment 3.1 MaterhJs We used thirty newspaper texts whose lengths varied widely; the mean o is 408 words and the standard deviation/~ is 376. The texts were annotated manually for co-reference relations of identity (ITh&apos;schman and Chinchor, 1997). The coreference relations define equivalence classes on the set of all marked referents in a text. The texts were also manually annotated with discourse structures built in the style of Mann and Thompson (1988). Each analysis yielded an average of 52 elementary discourse units. Details of the discourse annotation process are given in (Marcu et al., 1999). 3-~ Comparing potential to establish co-referential links 3~,.1 Method The annotations for co-reference relations and rhetorical structure trees for the thirty texts were fused, yielding representations that ~flect not only the discourse structure, but also the c~reference 49 equivalence classes specific to each text. Based on this information, we evaluated the potential of each of the two classes of models discussed in section 2 (Linear-k and Discourse-VT-k) to correctly estab• lish co-referential links as follows: For each model, each k, and each marked referential expression a, we determi"
W99-0106,W97-1303,0,0.0336328,"College Poughkeepsie, NY, USA dcristea @ info iasi. r@ ideOcs.vassar.edu Daniel Marcu Information Sciences Institute and Department of Computer Science University of Southern California Los Angeles, CA, USA Valentin Tablan Department of Computer Science University &quot;A.I. Cuza&apos;&quot; la~i, Rom~Lnia valyt@ infoiasi.ro marcu @ isi. edu Abstract PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, frequency, proximity, etc. In some cases, anaphora resolution systems implement these modules explicitly (I-Iobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997). The fact that current anaphora resolution systems rely exclusively on the linear nature of texts in Order to determine the LPA of an anaphor seems odd, given that several studies have claimed that there is a strong relation between discourse structure and reference (Sidner, 1981; Gmsz and Sidner, 1986; Grosz et aL, 1995; Fox, 1987; Vonk et al., 1992; Azzam et al., 1998; Hitzeman and P.oesio, 1998). These studies claim, on the one hand, th"
W99-0106,J81-4001,0,0.05935,"Missing"
W99-0106,W98-1427,0,\N,Missing
W99-0106,W98-1119,0,\N,Missing
W99-0106,P89-1031,0,\N,Missing
W99-0106,P99-1079,0,\N,Missing
W99-0106,C98-1044,1,\N,Missing
W99-0106,J94-2006,0,\N,Missing
W99-0106,P87-1022,0,\N,Missing
W99-0106,J96-2004,0,\N,Missing
W99-0106,W99-0309,0,\N,Missing
W99-0106,J86-3001,0,\N,Missing
W99-0106,C98-1011,0,\N,Missing
W99-0106,W97-1307,0,\N,Missing
W99-0307,J96-2004,0,0.0226131,"d the kappa coefficient k (Siegel and Castellan, 1988), a statistic used extensively in previous empirical studies of discourse. The kappa coefficient measures palrwise agreement among a set of coders who make category judgements, correcting for chance expected agreement (see equation (3) helow, where P(A) is the proportion of times a set of coders agree and P ( E ) is the proportion of times a set of coders are expected to agree by chance). Problems with the method. It has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders (Carletta, 1996). Although we agree with this, we believe that more experiments of the kind reported here will have to be carried out before we can produce a tagging manual that is usable by naive coders. In our experiment, it is not clear how k -~ P(A) - P ( E ) (3) 1- P(E) Carletta (1996) suggests that the units over which the kappa statistic is computed affects the outcome. To account for this, we computed the kappa statistics in two ways: 51 1. The first statistic, kw, reflects inter-annotator agreement under the assumption that edu and parenthetical unit boundaries can be inserted after any word in a tex"
W99-0307,J97-1002,0,0.340853,"have primarily focused on identifying discourse segment boundaries and their linguistic correlates. Very little attention has been paid so far to the highlevel, rhetorical relations that hold between discourse segments. In some cases, the role of these relations was considered to fall outside the scope of a study (Flammia and Zue, 1995); in other cases, judgements were made with respect to a taxonomy of very few intention-based relations (usually dominance and satisfaction-precedence) (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Litman, 1987; Passonneau and Litman, 1997; Carletta et al., 1997). And in the only case in which a rich taxonomy of 29 relations was used (Moser and Moore, 1997), the corpus was small and specific to a very restricted genre: written interactions between a student and tutor on the subject of fault location and repair in electronic circuitry. In spite of many influential proposals in the linguistic of discourse structures and relations (Ballard et al., 1971; Grimes, 1975; Halliday and Hasan, 1976; Martin, 1992; Mann and Thompson, 1988; Sanders et al., 1992; Sanders et al., 1993; Asher, In this paper, we describe an experiment designed to answer these question"
W99-0307,P87-1023,0,0.0918328,"? 1 Introduction Empirical studies of discourse structure have primarily focused on identifying discourse segment boundaries and their linguistic correlates. Very little attention has been paid so far to the highlevel, rhetorical relations that hold between discourse segments. In some cases, the role of these relations was considered to fall outside the scope of a study (Flammia and Zue, 1995); in other cases, judgements were made with respect to a taxonomy of very few intention-based relations (usually dominance and satisfaction-precedence) (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Litman, 1987; Passonneau and Litman, 1997; Carletta et al., 1997). And in the only case in which a rich taxonomy of 29 relations was used (Moser and Moore, 1997), the corpus was small and specific to a very restricted genre: written interactions between a student and tutor on the subject of fault location and repair in electronic circuitry. In spite of many influential proposals in the linguistic of discourse structures and relations (Ballard et al., 1971; Grimes, 1975; Halliday and Hasan, 1976; Martin, 1992; Mann and Thompson, 1988; Sanders et al., 1992; Sanders et al., 1993; Asher, In this paper, we des"
W99-0307,J97-1005,0,0.0384107,"udies of discourse structure have primarily focused on identifying discourse segment boundaries and their linguistic correlates. Very little attention has been paid so far to the highlevel, rhetorical relations that hold between discourse segments. In some cases, the role of these relations was considered to fall outside the scope of a study (Flammia and Zue, 1995); in other cases, judgements were made with respect to a taxonomy of very few intention-based relations (usually dominance and satisfaction-precedence) (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Litman, 1987; Passonneau and Litman, 1997; Carletta et al., 1997). And in the only case in which a rich taxonomy of 29 relations was used (Moser and Moore, 1997), the corpus was small and specific to a very restricted genre: written interactions between a student and tutor on the subject of fault location and repair in electronic circuitry. In spite of many influential proposals in the linguistic of discourse structures and relations (Ballard et al., 1971; Grimes, 1975; Halliday and Hasan, 1976; Martin, 1992; Mann and Thompson, 1988; Sanders et al., 1992; Sanders et al., 1993; Asher, In this paper, we describe an experiment designed"
W99-0307,P97-1012,0,\N,Missing
