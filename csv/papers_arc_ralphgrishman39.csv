2021.eacl-main.172,Learning Relatedness between Types with Prototypes for Relation Extraction,2021,-1,-1,2,1,10780,lisheng fu,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Relation schemas are often pre-defined for each relation dataset. Relation types can be related from different datasets and have overlapping semantics. We hypothesize we can combine these datasets according to the semantic relatedness between the relation types to overcome the problem of lack of training data. It is often easy to discover the connection between relation types based on relation names or annotation guides, but hard to measure the exact similarity and take advantage of the connection between the relation types from different datasets. We propose to use prototypical examples to represent each relation type and use these examples to augment related types from a different dataset. We obtain further improvement (ACE05) with this type augmentation over a strong baseline which uses multi-task learning between datasets to obtain better feature representation for relations. We make our implementation publicly available: https://github.com/fufrank5/relatedness"
W18-6126,A Case Study on Learning a Unified Encoder of Relations,2018,0,1,4,1,10780,lisheng fu,Proceedings of the 2018 {EMNLP} Workshop W-{NUT}: The 4th Workshop on Noisy User-generated Text,0,"Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. An individual corpus is often small, and the models may often be biased or overfitted to the corpus. We hypothesize that we can learn a better representation by combining multiple relation datasets. We attempt to use a shared encoder to learn the unified feature representation and to augment it with regularization by adversarial training. The additional corpora feeding the encoder can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both datasets."
I17-2072,Domain Adaptation for Relation Extraction with Domain Adversarial Neural Network,2017,0,13,4,1,10780,lisheng fu,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Relations are expressed in many domains such as newswire, weblogs and phone conversations. Trained on a source domain, a relation extractor{'}s performance degrades when applied to target domains other than the source. A common yet labor-intensive method for domain adaptation is to construct a target-domain-specific labeled dataset for adapting the extractor. In response, we present an unsupervised domain adaptation method which only requires labels from the source domain. Our method is a joint model consisting of a CNN-based relation classifier and a domain-adversarial classifier. The two components are optimized jointly to learn a domain-independent representation for prediction on the target domain. Our model outperforms the state-of-the-art on all three test domains of ACE 2005."
W16-1618,A Two-stage Approach for Extending Event Detection to New Types via Neural Networks,2016,28,13,4,1,118,thien nguyen,Proceedings of the 1st Workshop on Representation Learning for {NLP},0,None
N16-1034,Joint Event Extraction via Recurrent Neural Networks,2016,29,86,3,1,118,thien nguyen,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1088,Entity Linking with a Paraphrase Flavor,2016,17,0,3,1,34827,maria pershina,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The task of Named Entity Linking is to link entity mentions in the document to their correct entries in a knowledge base and to cluster NIL mentions. Ambiguous, misspelled, and incomplete entity mention names are the main challenges in the linking process. We propose a novel approach that combines two state-of-the-art models â for entity disambiguation and for paraphrase detection â to overcome these challenges. We consider name variations as paraphrases of the same entity mention and adopt a paraphrase model for this task. Our approach utilizes a graph-based disambiguation model based on Personalized Page Rank, and then refines and clusters its output using the paraphrase similarity between entity mention strings. It achieves a competitive performance of 80.5{\%} in B3+F clustering score on diagnostic TAC EDL 2014 data."
D16-1085,Modeling Skip-Grams for Event Detection with Convolutional Neural Networks,2016,25,30,2,1,118,thien nguyen,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-4502,Improving Event Detection with {A}bstract {M}eaning {R}epresentation,2015,25,16,4,1,1813,xiang li,Proceedings of the First Workshop on Computing News Storylines,0,"Event Detection (ED) aims to identify instances of specified types of events in text, which is a crucial component in the overall task of event extraction. The commonly used features consist of lexical, syntactic, and entity information, but the knowledge encoded in the Abstract Meaning Representation (AMR) has not been utilized in this task. AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. In this paper, we demonstrate the effectiveness of AMR to capture and represent the deeper semantic contexts of the trigger words in this task. Experimental results further show that adding AMR features on top of the traditional features can achieve 67.8% (with 2.1% absolute improvement) F-measure (F1), which is comparable to the state-of-the-art approaches."
W15-2709,Idiom Paraphrases: Seventh Heaven vs Cloud Nine,2015,39,5,3,1,34827,maria pershina,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"The goal of paraphrase identification is to decide whether two given text fragments have the same meaning. Of particular interest in this area is the identification of paraphrases among short texts, such as SMS and Twitter. In this paper, we present idiomatic expressions as a new domain for short-text paraphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community."
W15-1506,Relation Extraction: Perspective from Convolutional Neural Networks,2015,47,167,2,1,118,thien nguyen,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"Up to now, relation extraction systems have made extensive use of features generated by linguistic analysis modules. Errors in these features lead to errors of relation detection and classification. In this work, we depart from these traditional approaches with complicated feature engineering by introducing a convolutional neural network for relation extraction that automatically learns features from sentences and minimizes the dependence on external toolkits and resources. Our model takes advantages of multiple window sizes for filters and pre-trained word embeddings as an initializer on a non-static architecture to improve the performance. We emphasize the relation extraction problem with an unbalanced corpus. The experimental results show that our system significantly outperforms not only the best baseline systems for relation extraction but also the state-of-the-art systems for relation classification."
R15-1010,Improving Event Detection with Active Learning,2015,15,4,4,0,30065,kai cao,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Event Detection (ED), one aspect of Information Extraction, involves identifying instances of specified types of events in text. Much of the research on ED has been based on the specifications of the 2005 ACE [Automatic Content Extraction] event task 1 , and the associated annotated corpus. However, as the event instances in the ACE corpus are not evenly distributed, some frequent expressions involving ACE events do not appear in the training data, adversely affecting performance. In this paper, we demonstrate the effectiveness of a Pattern Expansion technique to import frequent patterns extracted from external corpora to boost ED performance. The experimental results show that our pattern-based system with the expanded patterns can achieve 70.4% (with 1.6% absolute improvement) F-measure over the baseline, an advance over current state-of-the-art systems."
R15-1011,Improving Event Detection with Dependency Regularization,2015,9,2,3,0,30065,kai cao,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Event Detection (ED) is an Information Extraction task which involves identifying instances of specified types of events in text. Most recent research on Event Detection relies on pattern-based or featurebased approaches, trained on annotated corpora, to recognize combinations of event triggers, arguments, and other contextual information. These combinations may each appear in a variety of linguistic forms. Not all of these event expressions will have appeared in the training data, thus adversely affecting ED performance. In this paper, we demonstrate the effectiveness of Dependency Regularization techniques to generalize the patterns extracted from the training data to boost ED performance. The experimental results on the ACE 2005 corpus show that our pattern-based system with the expanded patterns can achieve 70.49% (with 2.57% absolute improvement) F-measure over the baseline, which advances the state-of-the-art for such systems."
R15-1026,Jointly Embedding Relations and Mentions for Knowledge Population,2015,16,4,4,0,26849,miao fan,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"This paper contributes a joint embedding model for predicting relations between a pair of entities in the scenario of relation inference. It differs from most stand-alone approaches which separately operate on either knowledge bases or free texts. The proposed model simultaneously learns low-dimensional vector representations for both triplets in knowledge repositories and the mentions of relations in free texts, so that we can leverage the evidence both resources to make more accurate predictions. We use NELL to evaluate the performance of our approach, compared with cutting-edge methods. Results of extensive experiments show that our model achieves significant improvement on relation extraction."
P15-2060,Event Detection and Domain Adaptation with Convolutional Neural Networks,2015,32,99,2,1,118,thien nguyen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources.
P15-1062,Semantic Representations for Domain Adaptation: A Case Study on the Tree Kernel-based Method for Relation Extraction,2015,43,19,3,1,118,thien nguyen,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We study the application of word embeddings to generate semantic representations for the domain adaptation problem of relation extraction (RE) in the tree kernelbased method. We systematically evaluate various techniques to generate the semantic representations and demonstrate that they are effective to improve the generalization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement). In addition, we compare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach."
N15-3007,{ICE}: Rapid Information Extraction Customization for {NLP} Novices,2015,8,8,2,0.834067,12154,yifan he,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We showcase ICE, an Integrated Customization Environment for Information Extraction. ICE is an easy tool for non-NLP experts to rapidly build customized IE systems for a new domain."
N15-1026,Personalized Page Rank for Named Entity Disambiguation,2015,12,41,3,1,34827,maria pershina,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in microand 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions.
W14-6002,Jargon-Term Extraction by Chunking,2014,20,4,6,0,3082,adam meyers,Proceedings of the {COLING} Workshop on Synchronic and Diachronic Approaches to Analyzing Technical Language,0,"NLP definitions of Terminology are usually application-dependent. IR terms are noun sequences that characterize topics. Terms can also be arguments for relations like abbreviation, definition or IS-A. In contrast, this paper explores techniques for extracting terms fitting a broader definition: noun sequences specific to topics and not well-known to naive adults. We describe a chunkingbased approach, an evaluation, and applications to non-topic-specific relation extraction."
P14-2012,Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction,2014,32,51,2,1,118,thien nguyen,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Relation extraction suffers from a performance loss when a model is applied to out-of-domain data. This has fostered the development of domain adaptation techniques for relation extraction. This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors."
P14-2119,Infusion of Labeled Data into Distant Supervision for Relation Extraction,2014,29,34,4,1,34827,maria pershina,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models. However, in some cases a small amount of human labeled data is available. In this paper, we demonstrate how a state-of-theart multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve."
W13-2515,Gathering and Generating Paraphrases from {T}witter with Application to Normalization,2013,25,30,3,1,4068,wei xu,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a comparable corpus of temporally and topically related messages on Twitter which often express semantically identical information through distinct surface forms. We demonstrate the utility of this new resource on the task of paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art paraphrase and normalization systems 1 ."
W13-1103,A Preliminary Study of Tweet Summarization using Information Extraction,2013,24,21,2,1,4068,wei xu,Proceedings of the Workshop on Language Analysis in Social Media,0,"Although the ideal length of summaries differs greatly from topic to topic on Twitter, previous work has only generated summaries of a pre-fixed length. In this paper, we propose an event-graph based method using information extraction techniques that is able to create summaries of variable length for different topics. In particular, we extend the Pageranklike ranking algorithm from previous work to partition event graphs and thereby detect finegrained aspects of the event to be summarized. Our preliminary results show that summaries created by our method are more concise and news-worthy than SumBasic according to human judges. We also provide a brief survey of datasets and evaluation design used in previous work to highlight the need of developing a standard evaluation for automatic tweet summarization task."
R13-1051,Confidence Estimation for Knowledge Base Population,2013,15,4,2,1,1813,xiang li,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of83.5%, Pearson coefficient of54.2%, and2.3%absolute improvement in F-measure score through a weighted voting strategy."
R13-1052,Towards Fine-grained Citation Function Classification,2013,15,16,4,1,1813,xiang li,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We look into the problem of recognizing citation functions in scientific literature, trying to reveal authorsxe2x80x99 rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%."
P13-2117,Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction,2013,23,71,4,1,4068,wei xu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. We adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments."
N13-1095,Distant Supervision for Relation Extraction with an Incomplete Knowledge Base,2013,17,159,2,1,1264,bonan min,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of xe2x80x9cnegativexe2x80x9c examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a seriousflaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms."
I13-1081,An Efficient Active Learning Framework for New Relation Types,2013,13,7,2,1,10780,lisheng fu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Relation extraction is a fundamental task in information extraction. Different methods have been studied for building a relation extraction system. Supervised training of models for this task has yielded good performance, but at substantial cost for the annotation of large training corpora (About 40K same-sentence entity pairs). Semi-supervised methods can only require a seed set, but the performance is very limited when the seed set is very small, which is not very satisfactory for real relation extraction applications. The trade-off of annotation and performance is also hard to decide in practice. Active learning strategies allow users to gradually improve the model and to achieve comparable performance to supervised methods with limited annotation. Recent study shows active learning on this task needs much fewer labels for each type to build a useful relation extraction application. We feel active learning is a good direction to do relation extraction and presents a more efficient active learning framework. This framework starts from a better balance between positive and negative samples, and boosts by interleaving self-training and co-testing. We also studied the reduction of annotation cost by enforcing argument type constraints. Experiments show a substantial speed-up by comparison to previous state-of-the-art pure co-testing active learning framework. We obtain reasonable performance with only a hundred labels for individual ACE 2004 relation types. We also developed a GUI tool for real human-in-the-loop active learning trials. The goal of building relation extraction systems in a very short time seems to be promising."
W12-3011,Structural Linguistics and Unsupervised Information Extraction,2012,23,4,1,1,10781,ralph grishman,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction ({AKBC}-{WEKEX}),0,A precondition for extracting information from large text corpora is discovering the information structures underlying the text. Progress in this direction is being made in the form of unsupervised information extraction (IE). We describe recent work in unsupervised relation extraction and compare its goals to those of grammar discovery for science sublanguages. We consider what this work on grammar discovery suggests for future directions in unsupervised IE.
min-grishman-2012-challenges,Challenges in the Knowledge Base Population Slot Filling Task,2012,12,6,2,1,1264,bonan min,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The Knowledge Based Population (KBP) evaluation track of the Text Analysis Conferences (TAC) has been held for the past 3 years. One of the two tasks of KBP is slot filling: finding within a large corpus the values of a set of attributes of given people and organizations. This task has proven very challenging, with top systems rarely exceeding 30{\%} F-measure. In this paper, we present an error analysis and classification for those answers which could be found by a manual corpus search but were not found by any of the systems participating in the 2010 evaluation. The most common sources of failure were limitations on inference, errors in coreference (particularly with nominal anaphors), and errors in named entity recognition. We relate the types of errors to the characteristics of the task and show the wide diversity of problems that must be addressed to improve overall performance."
E12-1020,Compensating for Annotation Errors in Training a Relation Extractor,2012,23,2,2,1,1264,bonan min,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The well-studied supervised Relation Extraction algorithms require training data that is accurate and has good coverage. To obtain such a gold standard, the common practice is to do independent double annotation followed by adjudication. This takes significantly more human effort than annotation done by a single annotator. We do a detailed analysis on a snapshot of the ACE 2005 annotation files to understand the differences between single-pass annotation and the more expensive nearly three-pass process, and then propose an algorithm that learns from the much cheaper single-pass annotation and achieves a performance on a par with the extractor trained on multi-pass annotated data. Furthermore, we show that given the same amount of human labor, the better way to do relation annotation is not to annotate with high-cost quality assurance, but to annotate more."
D12-1094,Ensemble Semantics for Large-scale Unsupervised Relation Extraction,2012,44,40,3,1,1264,bonan min,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (synonymous) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a real-world dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web."
C12-1177,Paraphrasing for Style,2012,29,48,4,1,4068,wei xu,Proceedings of {COLING} 2012,0,"We present initial investigation into the task of paraphrasing language while targeting a particular writing style. The plays of William Shakespeare and their modern translations are used as a testbed for evaluating paraphrase systems targeting a specific style of writing. We show that even with a relatively small amount of parallel training data, it is possible to learn paraphrase models which capture stylistic phenomena, and these models outperform baselines based on dictionaries and out-of-domain parallel text. In addition we present an initial investigation into automatic evaluation metrics for paraphrasing writing style. To the best of our knowledge this is the first work to investigate the task of paraphrasing text with the goal of targeting a specific style of writing."
W11-4001,INVITED TALK 1: The Knowledge Base Population Task: Challenges for Information Extraction,2011,0,0,1,1,10781,ralph grishman,Proceedings of the {RANLP} 2011 Workshop on Information Extraction and Knowledge Acquisition,0,None
W11-4002,Fine-grained Entity Set Refinement with User Feedback,2011,19,2,2,1,1264,bonan min,Proceedings of the {RANLP} 2011 Workshop on Information Extraction and Knowledge Acquisition,0,"State of the art semi-supervised entity set expansion algorithms produce noisy results, which need to be refined manually. Sets expanded for intended fine-grained concepts are especially noisy because these concepts are not well represented by the limited number of seeds. Such sets are usually incorrectly expanded to contain elements of a more general concept. We show that fine-grained control is necessary for refining such sets and propose an algorithm which uses both positive and negative user feedback for iterative refinement. Experimental results show that it improves the quality of fine-grained sets significantly."
R11-1002,Acquiring Topic Features to improve Event Extraction: in Pre-selected and Balanced Collections,2011,13,16,2,1,38198,shasha liao,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Event extraction is a particularly challenging type of information extraction (IE) that may require inferences from the whole article. However, most current event extraction systems rely on local information at the phrase or sentence level, and do not consider the article as a whole, thus limiting extraction performance. Moreover, most annotated corpora are artificially enriched to include enough positive samples of the events of interest; event identification on a more balanced collection, such as unfiltered newswire, may perform much worse. In this paper, we investigate the use of unsupervised topic models to extract topic features to improve event extraction both on test data similar to training data, and on more balanced collections. We compare this unsupervised approach to a supervised multi-label text classifier, and show that unsupervised topic modeling can get better results for both collections, and especially for a more balanced collection. We show that the unsupervised topic model can improve trigger, argument and role labeling by 3.5%, 6.9% and 6% respectively on a pre-selected corpus, and by 16.8%, 12.5% and 12.7% on a balanced corpus."
P11-2045,Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction,2011,17,7,2,1,38198,shasha liao,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference."
P11-1053,Semi-supervised Relation Extraction with Large-scale Word Clustering,2011,29,80,2,1,40578,ang sun,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system."
P11-1115,Knowledge Base Population: Successful Approaches and Challenges,2011,36,220,2,0.688976,716,heng ji,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking -- linking names in context to entities in the KB -- and Slot Filling -- adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (slots) derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges."
I11-1080,Using Prediction from Sentential Scope to Build a Pseudo Co-Testing Learner for Event Extraction,2011,22,6,2,1,38198,shasha liao,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Event extraction involves the identification of instances of a type of event, along with their attributes and participants. Developing a training corpus by annotating events in text is very labor intensive, and so selecting informative instances to annotate can save a great deal of manual work. We present an active learning (AL) strategy, pseudo co-testing, based on one view from a classifier aiming to solve the original problem of event extraction, and another view from a classifier aiming to solve a coarser granularity task. As the second classifier can provide more graded matching from a wider scope, we can build a set of pseudocontention-points which are very informative, and can speed up the AL process. Moreover, we incorporate multiple selection criteria into the pseudo cotesting, seeking training examples that are informative, representative, and varied. Experiments show that pseudo co-testing can reduce annotation labor by 81%; incorporating multiple selection criteria reduces the labor by a further 7%."
I11-1117,Passage Retrieval for Information Extraction using Distant Supervision,2011,25,8,2,1,4068,wei xu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In this paper, we propose a keyword-based passage retrieval algorithm for information extraction, trained by distant supervision. Our goal is to be able to extract attributes of people and organizations more quickly and accurately by first ranking all the potentially relevant passages according to their likelihood of containing the answer and then performing a traditional deeper, slower analysis of individual passages. Using Freebase as our source of known relation instances and Wikipedia as our text source, we collected a weighted set of"
D11-1119,Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models,2011,32,12,4,1,4068,wei xu,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale n-gram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly.
W10-3909,Large Corpus-based Semantic Feature Extraction for Pronoun Coreference,2010,11,1,2,1,38198,shasha liao,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"Semantic information is a very important factor in c o e e r e n r e s o u i o n The combination of large corpora and xe2x80x98deepxe2x80x99 analysis procedures has made it possible to acquire a range of semantic information and apply it to this task. In this paper, we generate two statistically-based semantic features from a large corpus and measure their influence on p r o n u n coreference. One is contextual compatibility, which decides if the antcedent can be used in the anaphorxe2x80x99s context; the other is role pair, which decides if the actions asserted of the antecedent and the anaphor are likely to apply to the same entity. We apply a semantic labeling system and a baseline coreference system to a large corpus to generate semantic patterns and convert them into features in a MaxEnt model. These features produce an absolute gain of 1.5% to 1.7% in resolution accuracy (a 6% reductin in e rrors). To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails. We also compare the performance of patterns extracted from semantic role labeling and syntax."
P10-1081,Using Document Level Cross-Event Inference to Improve Event Extraction,2010,9,131,2,1,38198,shasha liao,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0% (absolute) gain in trigger (event) classification, and more than 8% gain for argument (role) classification in ACE event extraction."
N10-1036,Utility Evaluation of Cross-document Information Extraction,2010,7,1,5,0.732759,716,heng ji,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a utility evaluation to determine whether cross-document information extraction (IE) techniques measurably improve user performance in news summary writing. Two groups of subjects were asked to perform the same time-restricted summary writing tasks, reading news under different conditions: with no IE results at all, with traditional single-document IE results, and with cross-document IE results. Our results show that, in comparison to using source documents only, the quality of summary reports assembled using IE results, especially from cross-document IE, was significantly better and user satisfaction was higher. We also compare the impact of different user groups on the results."
grishman-2010-impact,The Impact of Task and Corpus on Event Extraction Systems,2010,8,7,1,1,10781,ralph grishman,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The term Âevent extractionÂ covers a wide range of information extraction tasks, and methods developed and evaluated for one task may prove quite unsuitable for another. Understanding these task differences is essential to making broad progress in event extraction. We look back at the MUC and ACE tasks in terms of one characteristic, the breadth of the scenario â how wide a range of information is subsumed in a single extraction task. We examine how this affects strategies for collecting information and methods for semi-supervised training of new extractors. We also consider the heterogeneity of corpora â how varied the topics of documents in a corpus are. Extraction systems may be intended in principle for general news but are typically evaluated on topic-focused corpora, and this evaluation context may affect system design. As one case study, we examine the task of identifying physical attack events in news corpora, observing the effect on system performance of shifting from an attack-event-rich corpus to a more varied corpus and considering how the impact of this shift may be mitigated."
C10-2137,Semi-supervised Semantic Pattern Discovery with Guidance from Unsupervised Pattern Clusters,2010,33,22,2,1,40578,ang sun,Coling 2010: Posters,0,"We present a simple algorithm for clustering semantic patterns based on distributional similarity and use cluster memberships to guide semi-supervised pattern discovery. We apply this approach to the task of relation extraction. The evaluation results demonstrate that our novel bootstrapping procedure significantly outperforms a standard bootstrapping. Most importantly, our algorithm can effectively prevent semantic drift and provide semi-supervised learning with a natural stopping criterion."
C10-1077,Filtered Ranking for Bootstrapping in Event Extraction,2010,17,23,2,1,38198,shasha liao,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Several researchers have proposed semi-supervised learning methods for adapting event extraction systems to new event types. This paper investigates two kinds of bootstrapping methods used for event extraction: the document-centric and similarity-centric approaches, and proposes a filtered ranking method that combines the advantages of the two. We use a range of extraction tasks to compare the generality of this method to previous work. We analyze the results using two evaluation metrics and observe the effect of different training corpora. Experiments show that our new ranking method not only achieves higher performance on different evaluation metrics, but also is more stable across different bootstrapping corpora."
W09-2809,A Parse-and-Trim Approach with Information Significance for {C}hinese Sentence Compression,2009,13,5,2,1,4068,wei xu,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"In this paper, we propose an event-based approach for Chinese sentence compression without using any training corpus. We enhance the linguistically-motivated heuristics by exploiting event word significance and event information density. This is shown to improve the preservation of important information and the tolerance of POS and parsing errors, which are more common in Chinese than English. The heuristics are only required to determine possibly removable constituents instead of selecting specific constituents for removal, and thus are easier to develop and port to other languages and domains. The experimental results show that around 72% of our automatic compressions are grammatically and semantically correct, preserving around 69% of the most important information on average."
R09-1032,"Cross-document Event Extraction and Tracking: Task, Evaluation, Techniques and Challenges",2009,22,32,2,0.898876,716,heng ji,Proceedings of the International Conference {RANLP}-2009,0,This paper proposes a new task of cross-document event extraction and tracking and its evaluation metrics. We identify important person entities which are frequently involved in events as xe2x80x98centroid entitiesxe2x80x99. Then we link the events involving the same centroid entity along a time line. We also present a system performing this task and our current approaches to address the main research challenges. We demonstrate that global inference from background knowledge and cross-document event aggregation are crucial to enhance the performance. This new task defines several extensions to the traditional single-document Information Extraction paradigm beyond xe2x80x98slot fillingxe2x80x99.
P09-2089,Updating a Name Tagger Using Contemporary Unlabeled Data,2009,13,4,2,1,28222,cristina mota,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"For many NLP tasks, including named entity tagging, semi-supervised learning has been proposed as a reasonable alternative to methods that require annotating large amounts of training data. In this paper, we address the problem of analyzing new data given a semi-supervised NE tagger trained on data from an earlier time period. We will show that updating the unlabeled data is sufficient to maintain quality over time, and outperforms updating the labeled data. Furthermore, we will also show that augmenting the unlabeled data with older data in most cases does not result in better performance than simply using a smaller amount of current unlabeled data."
P09-1048,"Who, What, When, Where, Why? Comparing Multiple Approaches to the Cross-Lingual 5{W} Task",2009,23,20,5,0,43862,kristen parton,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Cross-lingual tasks are especially difficult due to the compounding effect of errors in language processing and errors in machine translation (MT). In this paper, we present an error analysis of a new cross-lingual task: the 5W task, a sentence-level understanding task which seeks to return the English 5W's (Who, What, When, Where and Why) corresponding to a Chinese sentence. We analyze systems that we developed, identifying specific problems in language processing and MT that cause errors. The best cross-lingual 5W system was still 19% worse than the best monolingual 5W system, which shows that MT significantly degrades sentence-level understanding. Neither source-language nor target-language analysis was able to circumvent problems in MT, although each approach had advantages relative to the other. A detailed error analysis across multiple systems suggests directions for future research on the problem."
P08-1030,Refining Event Extraction through Cross-Document Inference,2008,7,156,2,1,716,heng ji,Proceedings of ACL-08: HLT,1,"We apply the hypothesis of xe2x80x9cOne Sense Per Discoursexe2x80x9d (Yarowsky, 1995) to information extraction (IE), and extend the scope of xe2x80x9cdiscoursexe2x80x9d from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event extraction task 1 . Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence."
mota-grishman-2008-ne,Is this {NE} tagger getting old?,2008,15,9,2,1,28222,cristina mota,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper focuses on the influence of changing the text time frame on the performance of a named entity tagger. We followed a twofold approach to investigate this subject: on the one hand, we analyzed a corpus that spans 8 years, and, on the other hand, we assessed the performance of a name tagger trained and tested on that corpus. We created 8 samples from the corpus, each drawn from the articles for a particular year. In terms of corpus analysis, we calculated the corpus similarity and names shared between samples. To see the effect on tagger performance, we implemented a semi-supervised name tagger based on co-training; then, we trained and tested our tagger on those samples. We observed that corpus similarity, names shared between samples, and tagger performance all decay as the time gap between the samples increases. Furthermore, we observed that the corpus similarity and names shared correlate with the tagger F-measure. These results show that named entity recognition systems may become obsolete in a short period of time."
N07-1067,Question Answering Using Integrated Information Retrieval and Information Extraction,2007,11,23,3,0,49339,barry schiffman,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"This paper addresses the task of providing extended responses to questions regarding specialized topics. This task is an amalgam of information retrieval, topical summarization, and Information Extraction (IE). We present an approach which draws on methods from each of these areas, and compare the effectiveness of this approach with a query-focused summarization approach. The two systems are evaluated in the context of the prosecution queries like those in the DARPA GALE distillation evaluation."
W06-3607,Re-Ranking Algorithms for Name Tagging,2006,19,38,3,1,716,heng ji,Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,0,"Integrating information from different stages of an NLP processing pipeline can yield significant error reduction. We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference. We evaluate three state-of-the-art re-ranking algorithms (MaxEnt-Rank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and cross-document inference."
W06-0206,Data Selection in Semi-supervised Learning for Name Tagging,2006,15,36,2,1,716,heng ji,Proceedings of the Workshop on Information Extraction Beyond The Document,0,"We present two semi-supervised learning techniques to improve a state-of-the-art multi-lingual name tagger. For English and Chinese, the overall system obtains 1.7% - 2.1% improvement in F-measure, representing a 13.5% -- 17.4% relative reduction in the spurious, missing, and incorrect tags. We also conclude that simply relying upon large corpora is not in itself sufficient: we must pay attention to unlabeled data selection too. We describe effective measures to automatically select documents and sentences."
P06-2055,Analysis and Repair of Name Tagger Errors,2006,8,18,2,1,716,heng ji,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Name tagging is a critical early stage in many natural language processing pipelines. In this paper we analyze the types of errors produced by a tagger, distinguishing name classification and various types of name identification errors. We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction. We show through examples and performance measurement how different stages can correct different types of errors. The resulting accuracy approaches that of individual human annotators."
P05-1051,Improving Name Tagging by Reference Resolution and Relation Detection,2005,9,30,2,1,716,heng ji,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Information extraction systems incorporate multiple stages of linguistic analysis. Although errors are typically compounded from stage to stage, it is possible to reduce the errors in one stage by harnessing the results of the other stages. We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger. We use an N-best approach to generate multiple hypotheses and have them re-ranked by subsequent stages of processing. We obtained thereby a reduction of 24% in spurious and incorrect name tags, and a reduction of 14% in missed tags."
P05-1052,Extracting Relations with Integrated Information Using Kernel Methods,2005,17,260,2,1,21189,shubin zhao,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels."
H05-1003,Using Semantic Relations to Refine Coreference Decisions,2005,11,33,3,1,716,heng ji,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses. Experiments show that this new framework can improve performance on two quite different languages - English and Chinese.
W04-2705,The {N}om{B}ank Project: An Interim Report,2004,11,245,7,1,3082,adam meyers,Proceedings of the Workshop Frontiers in Corpus Annotation at {HLT}-{NAACL} 2004,0,None
W04-0705,Applying Coreference to Improve Name Recognition,2004,12,11,2,1,716,heng ji,Proceedings of the Conference on Reference Resolution and Its Applications,0,"We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese. We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger. For names with low confidence, we show how these names can be filtered using coreference features to improve accuracy. In addition, we present rules which use coreference information to correct some name tagging errors. Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters. These combined methods yield an absolute improvement of about 3.1% in tagger F score."
P04-1053,Discovering Relations among Named Entities from Large Corpora,2004,6,327,3,0,44785,takaaki hasegawa,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization. Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities. Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations."
meyers-etal-2004-annotating,Annotating Noun Argument Structure for {N}om{B}ank,2004,4,83,7,1,3082,adam meyers,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"When complete, NomBank will provide annotation of noun arguments in Penn Treebank II (PTB). In PropBank, University of Pennsylvania annotators provide similar information for verbs. Given nominalization/verb mappings, the combination of NomBank and PropBank allows for generalization of arguments across parts of speech. This paper describes our annotation task including factors which make assigning role labels to noun arguments a challenging task."
C04-1109,Discriminative Slot Detection Using Kernel Methods,2004,15,8,3,1,21189,shubin zhao,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Most traditional information extraction approaches are generative models that assume events exist in text in certain patterns and these patterns can be regenerated in various ways. These assumptions limited the syntactic clues being considered for finding an event and confined these approaches to a particular syntactic level. This paper presents a discriminative framework based on kernel SVMs that takes into account different levels of syntactic information and automatically identifies the appropriate clues. Kernels are used to represent certain levels of syntactic structure and can be combined in principled ways as input for an SVM. We will show that by combining a low level sequence kernel with a high level kernel on a GLARF dependency graph, the new approach outperformed a good rule-based system on slot filler detection for MUC-6."
C04-1127,Cross-lingual Information Extraction System Evaluation,2004,8,20,3,1,51460,kiyoshi sudo,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we discuss the performance of cross-lingual information extraction systems employing an automatic pattern acquisition module. This module, which creates extraction patterns starting from a user's narrative task description, allows rapid customization to new extraction tasks. We compare two approaches: (1) acquiring patterns in the source language, performing source language extraction, and then translating the resulting templates to the target language, and (2) translating the texts and performing pattern discovery and extraction in the target language. We demonstrate an average of 8--10% more recall using the first approach. We discuss some of the problems with machine translation and their effect on pattern discovery which lead to this difference in performance."
P03-1029,An Improved Extraction Pattern Representation Model for Automatic {IE} Pattern Acquisition,2003,8,106,3,1,51460,kiyoshi sudo,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns."
N03-4013,pre-{CODIE}{--}Crosslingual On-Demand Information Extraction,2003,1,2,3,1,51460,kiyoshi sudo,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"Our research addresses two central issues of information extraction --- portability and multilinguality. We are creating information extraction (IE) systems that take foreign-language input and generate English tables of extracted information, and that can be easily adapted to new extraction tasks. We want to minimize the human intervention required for customization to a new scenario (type of facts or events of interest), and allow the user to interact with the system entirely in English. As a prototype, we have developed the pre-CODIE system, an experimental Crosss-lingual On-Demand Information Extraction system that extracts facts or events of interest from Japanese source text without requiring user knowledge of Japanese."
meyers-etal-2002-formal,Formal Mechanisms for Capturing Regularizations,2002,20,3,2,1,3082,adam meyers,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"While initial treebanks and treebank parsers primarily involved surface analysis, recent work focuses on predicate argument (PA) structure. PA structure provides means to regularize variants (e.g., actives/passives) of sentences so that individual patterns may have better coverage (in MT, QA, IE, etc.), offsetting the sparse data problem. We encode such PA information in the GLARF framework. Our previous work discusses procedures for producing GLARF from treebanks and parsed data. This paper shows that GLARF is particularly well-suited for capturing regularization. We discuss crucial components of GLARF and demonstrate that other frameworks would require equivalent components to adequately express regularization."
nobata-etal-2002-summarization,Summarization System Integrated with Named Entity Tagging and {IE} pattern Discovery,2002,7,31,4,0,51944,chikashi nobata,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We have introduced information extraction technique such as named entity tagging and pattern discovery to a summarization system based on sentence extraction technique, and evaluated the performance in the Document Understanding Conference 2001 (DUC-2001). We participated in the Single Document Summarization task in DUC-2001 and achieved one of the best performance in subjective evaluation of summarization results."
calzolari-etal-2002-standards,Standards {\\&} best practice for multilingual computational lexicons: {ISLE} {MILE} and more{''},2002,1,5,2,0,18003,nicoletta calzolari,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
atkins-etal-2002-resources,From Resources to Applications. Designing the Multilingual {ISLE} Lexical Entry,2002,4,10,7,0,53506,sue atkins,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
calzolari-etal-2002-towards,Towards Best Practice for Multiword Expressions in Computational Lexicons,2002,6,99,3,0,18003,nicoletta calzolari,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The importance a nd role of multi-word expressions (MWE) in the description and p rocessing o f natural l anguage has been long recognized. However, multi-word information has often been relegated to the marginal role of idiosyncratic lexical information. The need for MWE lexicons grows even more acute for multi-lingual applications, for which (sometimes complex) correspondences must be identified, classified, and recorded. Within the XMELLT and ISLE projects we have started to investigate the potential to develop multi-lingual, multi-word expression lexicons incorporating both syntactic and semantic information. We aim at specifying means to acquire and represent multi-word lexical entries for multiple languages, and establishing uniform (or inter-translatable) standards for describing multi-word lexical entries. We explored theoretical approaches used in large lexicon-building projects, in p articular FrameNet and SIMPLE. They constitute interesting frameworks for the explicit syntactic and semantic representation of MWEs, due mainly to their ability to capture semantic multidimensionality, through frame e lements and qualia relations respectively. We a lso developed an abstract data model for lexical information together with a representation in XML for it. Our goal is to define a set of minimal lexicon xe2x80x9cobjectsxe2x80x9d, which can serve not only as a model for MWEs but also for lexical data in general."
huttunen-etal-2002-diversity,Diversity of Scenarios in Information extraction,2002,9,17,3,0,41098,silja huttunen,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
C02-1154,Unsupervised Learning of Generalized Names,2002,14,95,3,1,12064,roman yangarber,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We present an algorithm, NOMEN, for learning generalized names in text. Examples of these are names of diseases and infectious agents, such as bacteria and viruses. These names exhibit certain properties that make their identification more complex than that of regular proper names, NOMEN uses a novel form of bootstrapping to grow sets of textual instances and of their contextual patterns. The algorithm makes use of competing evidence to boost the learning of several categories of names simultaneously. We present results of the algorithm on a large corpus. We also investigate the relative merits of several evaluation strategies."
C02-1165,Complexity of Event Structure in {IE} Scenarios,2002,10,19,3,0,41098,silja huttunen,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper presents new Information Extraction scenarios which are linguistically and structurally more challenging than the traditional MUC scenarios. Traditional views on event structure and template design are not adequate for the more complex scenarios.The focus of this paper is to show the complexity of the scenarios, and propose a way to recover the structure of the event. First we identify two structural factors that contribute to the complexity of scenarios: the scattering of events in text, and inclusion relationships between events. These factors cause difficulty in representing the facts in an unambiguous way. Then we propose a modular, hierarchical representation where the information is split in atomic units represented by templates, and where the inclusion relationships between the units are indicated by links. Lastly, we discuss how we may recover this representation from text, with the help of linguistic cues linking the events."
W01-1511,Covering Treebanks with {GLARF},2001,16,27,2,0,53791,meyers,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"This paper introduces GLARF, a framework for predicate argument structure. We report on converting the Penn Treebank II into GLARF by automatic methods that achieved about 90% precision/recall on test sentences from the Penn Treebank. Plans for a corpus of hand-corrected output, extensions of GLARF to Japanese and applications for MT are also discussed."
H01-1009,Automatic Pattern Acquisition for {J}apanese Information Extraction,2001,4,47,3,1,51460,kiyoshi sudo,Proceedings of the First International Conference on Human Language Technology Research,0,"One of the central issues for information extraction is the cost of customization from one scenario to another. Research on the automated acquisition of patterns is important for portability and scalability. In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence. We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text. The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns."
moreno-etal-2000-treebank,A Treebank of {S}panish and its Application to Parsing,2000,8,22,2,0,28753,antonio moreno,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper presents joint research between a Spanish team and an American one on the development and exploitation of a Spanish treebank. Such treebanks for other languages have proven valuable for the development of high-quality parsers and for a wide variety of language studies. However, when the project started, at the end of 1997, there was no syntactically annotated corpus for Spanish. This paper describes the design of such a treebank and its initial application to parser construction. 1. Constructing a Spanish treebank 1.1. Preliminary considerations This paper presents joint research between a Spanish team and an American one on the development and exploitation of a Spanish treebank. Such treebanks for other languages have proven valuable for the development of high-quality parsers and for a wide variety of language studies. As there was no previous experience in building a syntactically annotated corpus for Spanish, the first effort consisted necessarily in writing a set of annotation guidelines. The starting point was the existing documentation at that time, especially the Penn Treebank project (Marcus, Santorini and Marcinkiewicz, 1993; Bies et al., 1995), the EAGLES preliminary recommendations (EAGLES, 1996), and the Negra corpus (Skut et al., 1997). Our experience in developing Spanish NLP systems told us that a pure phrase structure annotation (typical of the English treebanks) would not be enough for inducing relevant rules for Spanish. At the least, information about agreement and syntactic functions is necessary for Spanish, and we wanted to incorporate that information in our trees in the form of features. The treebank has been created mostly by hand, although some automatic pre-tagging of the data is performed, as described below, to speed treebank creation."
macleod-etal-2000-american,The {A}merican National Corpus: A Standardized Resource for {A}merican {E}nglish,2000,12,15,3,1,51384,catherine macleod,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Linguistic research has become heavily reliant on text corpora over the past ten years. Such resources are becoming increasingly available through efforts such as the Linguistic Data Consortium (LDC) in the US and the European Language Resources Association (ELRA) in Europe. However, in the main the corpora that are gathered and distributed through these and other mechanisms consist of texts which can be easily acquired and are available for re-distribution without undue problems of copyright, etc. This practice has resulted in a vast over-representation among available corpora of certain genres, in particular newspaper samples, which comprise the greatest percentage of texts currently available from, for example, the LDC, and which also dominate the training data available for speech recognition purposes. Other available corpora typically consist of technical reports, transcriptions of parliamentary and other proceedings, short telephone conversations, and the like. The upshot of this is that corpusbased natural language processing has relied heavily on language samples representative of usage in a handful of limited and linguistically specialized domains."
C00-2136,Automatic Acquisition of Domain Knowledge for Information Extraction,2000,10,187,2,1,12064,roman yangarber,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In developing an Information Extraction (IE) system for a new class of events or relations, one of the major tasks is identifying the many ways in which these events or relations may be expressed in text. This has generally involved the manual analysis and, in some cases, the annotation of large quantities of text involving these events. This paper presents an alternative approach, based on an automatic discovery procedure, EXDISCO, which identifies a set of relevant documents and a set of event patterns from un-annotaled text, starting from a small set of seed patterns. We evaluate EXDISCO by comparing the performance of discovered patterns against that of manually constructed systems on actual extraction tasks."
C00-1078,Chart-Based Transfer Rule Application in Machine Translation,2000,11,18,3,1,3082,adam meyers,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Transfer-based Machine Translation systems require a procedure for choosing the set of transfer rules for generating a target language translation from a given source language sentence. In an MT system with many competing transfer rules, choosing the best set of transfer rules for translation may involve the evaluation of an explosive number of competing sets. We propose a solution to this problem based on current best-first chart parsing algorithms."
A00-1039,Unsupervised Discovery of Scenario-Level Patterns for Information Extraction,2000,15,92,2,1,12064,roman yangarber,Sixth Applied Natural Language Processing Conference,0,"Information Extraction (IE) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base---a time-consuming and expensive process. We have implemented a system for finding patterns automatically from un-annotated text. Starting with a small initial set of seed patterns proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall."
X98-1012,Research in Information Extraction: 1996-98,1998,8,1,1,1,10781,ralph grishman,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"Information extraction involves picking out specified types of information from natural language text. Recent Message Understanding Conferences [1,2,3] have developed a spectrum of such tasks, and we have worked on two of them, at opposite ends of the spectrum: the named entity task, which involves identifying and classifying names, and the scenario template task, which involves extracting critical information (participants, location, date, etc.) about specified classes of events."
X98-1016,Transforming Examples into Patterns for Information Extraction,1998,10,6,2,1,12064,roman yangarber,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"Information Extraction (IE) systems today are commonly based on pattern matching. The patterns are regular expressions stored in a customizable knowledge base. Adapting an IE system to a new subject domain entails the construction of a new pattern base --- a time-consuming and expensive task. We describe a strategy for building patterns from examples. To adapt the IE system to a new domain quickly, the user chooses a set of examples in a training text, and for each example gives the logical form entries which the example induces. The system transforms these examples into patterns and then applies meta-rules to generalize these patterns."
W98-1118,Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition,1998,13,196,4,0,42826,andrew borthwick,Sixth Workshop on Very Large Corpora,0,"This paper describes a novel statistical namedentity (i.e. proper name) recognition system built around a maximum entity framework. By working v,ithin the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published. 1 I N T R O D U C T I O N Named entity recognition is one of the simplest of the common message understanding tasks. The objective is to identify and categorize all members of certain categories of proper names from a given corpus. The specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference (MUC-7), in which the task was to identify names falling into one of seven categories: person, organization, location, date, time, percentage, and monetary amount. This paper describes a new system called Maximum Entropy Named Entity or MENE (pronounced meanie). By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision. These knowledge sources include capitalization features, lexical features, and features indicating the current section of text. It makes use of a broad array of dictionaries of useful single or multi-word terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary. Our dictio152 naries required no manual editing and were either downloaded from the web or were simply obvious lists entered by hand. This system, built from off-the-shelf knowledge sources, contained no hand-generated pat terns and achieved a result which is comparable with that of the best statistical systems. Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation. Given appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on upper-case English. We also feel that there are plenty of avenues to explore in enhancing the system's performance on English-language newspaper"
W98-1120,A Decision Tree Method for Finding and Classifying Names in {J}apanese Texts,1998,0,108,2,0.606061,1087,satoshi sekine,Sixth Workshop on Very Large Corpora,0,None
W98-0604,Using {NOMLEX} to Produce Nominalization Patterns for Information Extraction,1998,9,33,4,1,3082,adam meyers,The Computational Treatment of Nominals,0,None
P98-2139,Deriving Transfer Rules from Dominance-Preserving Alignments,1998,9,24,3,1,3082,adam meyers,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,None
M98-1011,{NYU}: Description of the Proteus/{PET} System as Used for {MUC}-7 {ST},1998,0,70,2,1,12064,roman yangarber,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,None
M98-1018,{NYU}: Description of the {MENE} Named Entity System as Used in {MUC}-7,1998,0,203,4,0,42826,andrew borthwick,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,None
C98-2134,Deriving Transfer Rules from Dominance-Preserving Alignments,1998,9,24,3,1,3082,adam meyers,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,None
meyers-etal-1998-multilingual,A multilingual procedure for dictionary-based sentence alignment,1998,25,17,3,1,3082,adam meyers,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper describes a sentence alignment technique based on a machine readable dictionary. Alignment takes place in a single pass through the text, based on the scores of matches between pairs of source and target sentences. Pairings consisting of sets of matches are evaluated using a version of the Gale-Shapely solution to the stable marriage problem. An algorithm is described which can handle N-to-1 (or 1-to-N) matches, for n {\mbox{$\geq$}} 0, i.e., deletions, 1-to-1 (including scrambling), and 1-to-many matches. A simple frequency based method for acquiring supplemental dictionary entries is also discussed. We achieve high quality alignments using available bilingual dictionaries, both for closely related language pairs (Spanish/English) and more distantly related pairs (Japanese/English)."
X96-1029,The Role of Syntax in Information Extraction,1996,2,11,1,1,10781,ralph grishman,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"Our group at New York University has developed a number of information extraction systems over the past decade. In particular, we have been participants in the Message Understanding Conferences (MUCs) since MUC-1. During this time, while experimenting with many aspects of system design, we have retained a basic approach in which information extraction involves a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure [2]. Because we have a good, broad-coverage English grammar and a moderately effective method for recovering from parse failures, this approach held us in fairly good stead."
X96-1039,Building an Architecture: A {CAWG} Saga,1996,-1,-1,1,1,10781,ralph grishman,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,None
X96-1043,{TIPSTER} Text Phase {II} Architecture Design Version 2.1p 19 {J}une 1996,1996,0,18,1,1,10781,ralph grishman,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"The TIPSTER Program aims to push the technology for access to information in large (multi-GB) text collections, in particular for the analysts in Government agencies. Technology is being developed for document detection (information retrieval) and for data extraction from free text."
X96-1047,Design of the {MUC}-6 Evaluation,1996,4,14,1,1,10781,ralph grishman,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"The sixth in a series of Message Understanding Conferences, which are designed to promote and evaluate research in information extraction, was held last fall. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe the development of the message understanding task over the course of the prior MUCs, some of the motivations for the new format, and the steps which led up to the formal evaluation."
C96-1078,Alignment of Shared Forests for Bilingual Corpora,1996,14,56,3,1,3082,adam meyers,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Research in example-based machine translation (EBMT) has been hampered by the lack of efficient tree alignment algorithms for bilingual corpora. This paper describes an alignment algorithm for EBMT whose running time is quadratic in the size of the input parse trees. The algorithm uses dynamic programming to score all possible matching nodes between structure-sharing trees or forests. We describe the algorithm, various optimizations, and our implementation."
C96-1079,{M}essage {U}nderstanding {C}onference- 6: A Brief History,1996,0,909,1,1,10781,ralph grishman,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We have recently completed the sixth in a series of Message Understanding Conferences which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations."
C96-1080,The Influence of Tagging on the Classification of Lexical Complements,1996,1,3,3,1,51384,catherine macleod,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"A large corpus (about 100 MB of text) was selected and examples of 750 frequently occurring verbs were tagged with their complement class as defined by a large computational syntactic dictionary, COMLEX Syntax. This tagging task led to the refinement of already existing classes and to the addition of classes that had previously not been defined. This has resulted in the enrichment and improvement of the original COMLEX Syntax dictionary. Tagging also provides statistical data which will allow users to select more common complements of a particular verb and ignore rare usage. We discuss below some of the problems encountered in tagging and their resolution."
M95-1001,Design of the {MUC}-6 Evaluation,1995,3,85,1,1,10781,ralph grishman,"Sixth Message Understanding Conference ({MUC}-6): Proceedings of a Conference Held in {C}olumbia, {M}aryland, November 6-8, 1995",0,"The sixth in a series of Message Understanding Conferences, which are designed to promote and evaluate research in information extraction, was held last fall. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe the development of the message understanding task over the course of the prior MUCs, some of the motivations for the new format, and the steps which led up to the formal evaluation."
M95-1014,The {NYU} System for {MUC}-6 or Where{'}s the Syntax?,1995,3,106,1,1,10781,ralph grishman,"Sixth Message Understanding Conference ({MUC}-6): Proceedings of a Conference Held in {C}olumbia, {M}aryland, November 6-8, 1995",0,"Over the past five MUCs, New York University has clung faithfully to the idea that information extraction should begin with a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure. Because we have a good, broad-coverage English grammar and a moderately effective method for recovering from parse failures, this approach held us in fairly good stead."
1995.iwpt-1.26,A Corpus-based Probabilistic Grammar with Only Two Non-terminals,1995,-1,-1,2,0.606061,1087,satoshi sekine,Proceedings of the Fourth International Workshop on Parsing Technologies,0,"The availability of large, syntactically-bracketed corpora such as the Penn Tree Bank affords us the opportunity to automatically build or train broad-coverage grammars, and in particular to train probabilistic grammars. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. To make maximal use of context, we have automatically constructed, from the Penn Tree Bank version 2, a grammar in which the symbols S and NP are the only real nonterminals, and the other non-terminals or grammatical nodes are in effect embedded into the right-hand-sides of the S and NP rules. For example, one of the rules extracted from the tree bank would be S -{\textgreater} NP VBX JJ CC VBX NP [1] ( where NP is a non-terminal and the other symbols are terminals {--} part-of-speech tags of the Tree Bank). The most common structure in the Tree Bank associated with this expansion is (S NP (VP (VP VBX (ADJ JJ) CC (VP VBX NP)))) [2]. So if our parser uses rule [1] in parsing a sentence, it will generate structure [2] for the corresponding part of the sentence. Using 94{\%} of the Penn Tree Bank for training, we extracted 32,296 distinct rules ( 23,386 for S, and 8,910 for NP). We also built a smaller version of the grammar based on higher frequency patterns for use as a back-up when the larger grammar is unable to produce a parse due to memory limitation. We applied this parser to 1,989 Wall Street Journal sentences (separate from the training set and with no limit on sentence length). Of the parsed sentences (1,899), the percentage of no-crossing sentences is 33.9{\%}, and Parseval recall and precision are 73.43{\%} and 72 .61{\%}."
H94-1003,The {C}omlex Syntax Project: The First Year,1994,7,13,2,1,51384,catherine macleod,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled."
H94-1021,Whither Written Language Evaluation?,1994,3,15,1,1,10781,ralph grishman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"Common evaluations have grown to be a major component of all the ARPA Human Language Technology programs. In the written language community, the largest evaluation program has been the series of Message Understanding Conferences, which began in 1987 [2,3]. These evaluations have focussed on the task of analyzing text and automatically filling templates describing certain classes of events. These conferences have certainly been a major impetus in the development of systems for performing such information extraction tasks, and thus in demonstrating the potential practical value of some of the written language processing technology."
H94-1109,Research in Natural Language Processing,1994,37,3,1,1,10781,ralph grishman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"Publisher Summary This chapter deals with the topic of natural language processing in the field of artificial intelligence (AI). The subject of natural language processing covers processing issues at all levels of natural language understanding including speech recognition, syntactic and semantic analysis of sentences, reference to the discourse context, conversational inference and implicature, and discourse planning and generation. The chapter also covers the syntactic and semantic processing of sentences to deliver semantic objects suitable for referring, inferring, and other related functions. Even though this chapter is confined mainly to the syntactic and semantic section of the topic, it also demonstrates how it is impossible to entirely separate it from the broader context. All language processors can be viewed as being comprised of three elements. The first is grammar, which defines the legal ways in which constituents may combine both syntactically and semantically to yield other constituents. The second component of a processor is a non-deterministic algorithm that uses the rules of the grammar to deliver such structural descriptions for a given sentence. However, this component does not itself determine what happens when more than one rule can apply in a given state of the processor. This last responsibility devolves to the third component, the oracle, or mechanism for resolving such local processing ambiguities. The oracle decides which action should be taken at points in the analysis where the non-deterministic algorithm allows more than one. This chapter describes these three components in detail and briefly discusses the areas where these components are lacking."
C94-2119,Generalizing Automatically Generated Selectional Patterns,1994,12,81,1,1,10781,ralph grishman,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text from the same domain. This information, however, is necessarily incomplete. We report on measurements of the degree of selectional coverage obtained with different sizes of corpora. We then describe a technique for using the corpus to identify selectionally similar terms, and for using this similarity to broaden the selectional coverage for a fixed corpus size."
C94-1042,Comlex Syntax: Building a Computational Lexicon,1994,8,194,1,1,10781,ralph grishman,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled."
M93-1016,{N}ew {Y}ork {U}niversity Description of the {PROTEUS} System as Used for {MUC}-5,1993,-1,-1,1,1,10781,ralph grishman,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,None
H93-1050,Smoothing of Automatically Generated Selectional Constraints,1993,5,29,1,1,10781,ralph grishman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text from the same domain. Better coverage of the domain can be obtained by appropriate generalization of the specific word patterns which are collected. We report here on an approach to automatically make suitable generalizations: using the co-occurrence data to compute a confusion matrix relating individual words, and then using the confusion matrix to smooth the original frequency data."
H93-1059,Session 10: THE LEXICON,1993,0,0,1,1,10781,ralph grishman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
H93-1060,The {COMLEX} Syntax Project,1993,4,8,1,1,10781,ralph grishman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics. One natural candidate for such a resource is a broad-coverage dictionary, since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word. The Linguistic Data Consortium has begun an effort to create several such lexical resources, under the rubric COMLEX (COMmon LEXicon); one of these projects is the COMLEX Syntax Project."
H93-1101,Research in Natural Language Processing,1993,1,1,1,1,10781,ralph grishman,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"This chapter discusses the commercial significance of natural language processing (NLP). Natural languages are interesting because they constitute the most important medium for human communication. Special knowledge and processing techniques are required to recognize presence or absence of the concept amendment of authorization of securities in the concerned documents. NLP offers capabilities to accept such queries, and retrieve the correct set of relevant documents. Certain industry trends are aggravating the need for documentation. The complexity of products is on the rise, life cycles of electronic products are decreasing, and an increasing number of functions inside products are being realized with software. NLP is a subfield of artificial intelligence (AI) that deals with the processing of natural language. The emphasis in NLP has been on devising procedures that are computationally effective, that is, ultimately result in the techniques being encoded in a suitable computer program. This emphasis distinguishes NLP from the study of natural languages within other disciplines such as linguistics and cognitive psychology."
M92-1015,{N}ew {Y}ork {U}niversity {PROTEUS} System: {MUC}-4 Test Results and Analysis,1992,-1,-1,1,1,10781,ralph grishman,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,None
M92-1032,{N}ew {Y}ork {U}niversity Description of the {PROTEUS} System as Used for {MUC}-4,1992,-1,-1,1,1,10781,ralph grishman,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,None
H92-1115,Research in Natural Language Processing,1992,0,0,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"Our central research focus is on the automatic acquisition of knowledge about language (both syntactic and semantic) from corpora. We wish to understand how the knowledge so acquired can enhance natural language applications, including document retrieval, information extraction, and machine translation. In addition to experimenting with acquisition procedures, we are continuing to develop the infrastructure needed for these applications (grammars and dictionaries, parsers, evaluation procedures, etc.)."
C92-2099,Acquisition of Selectional Patterns,1992,16,63,1,1,10781,ralph grishman,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
A92-1022,Evaluating Parsing Strategies Using Standardized Parse Files,1992,9,38,1,1,10781,ralph grishman,Third Conference on Applied Natural Language Processing,0,"The availability of large files of manually-reviewed parse trees from the University of Pennsylvania tree bank, along with a program for comparing system-generated parses against these standard parses, provides a new opportunity for evaluating different parsing strategies. We discuss some of the restructuring required to the output of our parser so that it could be meaningfully compared with these standard parses. We then describe several heuristics for improving parsing accuracy and coverage, such as closest attachment of modifiers, statistical grammars, and fitted parses, and present a quantitative evaluation of the improvements obtained with each strategy."
1992.tmi-1.23,Combining rationalist and empiricist approaches to machine translation,1992,11,16,1,1,10781,ralph grishman,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"Better methods are needed for acquiring the knowledge which must go into machine translation systems. The call for papers for this conference contrast two approaches: the rationalist (based on linguistic theory) and the empiricist (based on analysis of large corpora). We suggest in this paper an intermediate approach which draws on the strengths of both. In this approach, parallel corpora in the source and target languages would be analyzed to produce parses and syntactically regularized tree structures. The individual source and target language trees would then be aligned, yielding a set of correspondences between source and target structures involving specific words. Classes of closely related words would be identified from a distributional analysis of the parsed corpora, and these classes would be used in turn to generalize the correspondences. These generalized correspondences would then serve as the transfer rules of a machine translation system."
M91-1013,{N}ew {Y}ork {U}niversity {PROTEUS} System: {MUC}-3 Test Results and Analysis,1991,2,8,1,1,10781,ralph grishman,"{T}hird {M}essage {U}understanding {C}onference ({MUC}-3): Proceedings of a Conference Held in {S}an {D}iego, {C}alifornia, {M}ay 21-23, 1991",0,"Evaluating the degree of improvement over the MUC-3 runs is complicated by the changes between MUC-3 an d MUC-4: there were changes in the template structure, the MURDER templates were eliminated, content mappin g constraints were incorporated into the scoring program, and the rules for manual remapping were much more con strained. We resumed system development specifically for MUC (with regular runs and rescorings) in mid-March , approximately two weeks before the Dry Run was due, and the modifications prior to the Dry Run primaril y reflected the changes needed for the new template structure (no significant changes were made to concepts, ver b models, inference rules, etc .). The changes between our final MUC-3 scores and our Dry Run scores thus roughl y reflect the changes due to the change in the task -for both TST1 and TST2, a loss of about 7 points of recall . During the following 8 weeks, we made a number of system modifications which recovered much of this loss of recal l and substantially improved system precision."
M91-1028,{N}ew {Y}ork {U}niversity: Description of the {PROTEUS} System as Used for {MUC}-3,1991,6,35,1,1,10781,ralph grishman,"{T}hird {M}essage {U}understanding {C}onference ({MUC}-3): Proceedings of a Conference Held in {S}an {D}iego, {C}alifornia, {M}ay 21-23, 1991",0,"The Proteus system which we have used for MUC-5 is largely unchanged from that used for MUC-3 and MUC-4. It has three main components: a syntactic analyzer, a semantic analyzer, and a template generator."
M91-1036,Computational Aspects of Discourse in the Context of {MUC}-3,1991,22,14,6,0,51398,lucja iwanska,"{T}hird {M}essage {U}understanding {C}onference ({MUC}-3): Proceedings of a Conference Held in {S}an {D}iego, {C}alifornia, {M}ay 21-23, 1991",0,"Discourse comprises those phenomena that usually do not arise when processing a single sentence. It appears to be the most difficult and probably the least understood aspect of automated message understanding. Five out of fifteen sites on a MUC-3 survey listed discourse as their main weakness and an area in which to concentrate future research. Virtually all systems presented here take a sentence-by-sentence approach to text understanding. Parsing and domain-dependent interpretation of sentences or sentence fragments (usually the latter) are followed by modules that attempt to connect these interpretations into a coherent whole. This paper gives an overview of the modules that make the transition from the interpretation of sentences to the interpretation of the text that contains these sentences. Systems presented in this paper exhibit various degrees of the following discourse understanding capabilities:xe2x80xa2 identifying portions of text that describe different domain events; this includes the capability of recognizing a single event and the capability of distinguishing multiple events;xe2x80xa2 resolving references:- pronoun references, e.g., finding the referent of It in the sentence It took place this morning,- proper name references, e.g., understanding that Luis Galan may be referred to as Senator Galan;- definite references, e.g., deciding what is the referent for The attack in the sentence The attack look us by surprise.xe2x80xa2 discourse representation : representation at the message level."
H91-1095,Robust and Portable Text Processing,1991,0,0,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,None
H90-1053,Statistical Parsing of Messages,1990,7,47,2,0,57555,mahesh chitrao,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The recent trend in natural language processing research has been to develop systems that deal with text concerning small, well defined domains. One practical application for such systems is to process messages pertaining to some very specific task or activity [5]. The advantage of dealing with such domains is twofold - firstly, due to the narrowness of the domain, it is possible to encode most of the knowledge related to the domain and to make this knowledge accessible to the natural language processing system, which in turn can use this knowledge to disambiguate the meanings of the messages. Secondly, in such a domain, there is not a great diversity of language constructs and therefore it becomes easier to construct a grammar which will capture all the constructs which exist in this sub-language."
H90-1094,Research in Text Processing: Creating Robust and Portable Systems,1990,0,0,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Our goal is to improve the technology for retrieving passages, extracting specific facts, and creating formatted data bases from large text collections. In particular, we are concerned with developing techniques for automatically training language processing systems to the syntax and semantics of particular domains and types of text in order to improve system performance."
C90-3023,Causal and Temporal Text Analysis: The Role of the Domain Model,1990,6,5,1,1,10781,ralph grishman,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"It is generally recognized that interpreting natural language input may require access to detailed knowledge of the domain involved. This is particularly tree for multi-sentence discourse, where we must not only analyze the individual sentences but also establish the connections between them. Simple semantic constraints an object classification hierarchy, a catalog of meaningful semantic relations are not sufficient. However, the appropriate structure for integrating a language analyzer with a complex dynamic (time-dependent) model --one which can scale up beyond 'toy' domains is not yet well understood."
C90-3071,Information Extraction and Semantic Constraints,1990,8,4,1,1,10781,ralph grishman,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"We consider the problem of extracting specified types of information from natural language text. To properly analyze the text, we wish to apply semantic (selectional) constraints whenever possible; however, we cannot expect to have semantic patterns for all the input we may encounter in real texts. We therefore use preference semantics: selecting the analysis which maximizes the number of semantic patterns matched. We describe a specific information extraction task, and report on the benefits of using preference semantics for this task."
H89-2011,Preference Semantics for Message Understanding,1989,6,17,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"The design of effective natural language processing systems requires a combination of the theoretical and the practical. We want to have a theoretically well-founded design so that we can take advantage of gradual improvements in our knowledge of syntax, semantics, discourse structures, and the subject domain. At the same time we need to adopt a practical approach which recognizes the inevitable shortcomings of our knowledge in these areas. We need to create robust systems which are able to deal appropriately with these shortcomings. We are interested in particular in systems for extracting specified information from a text. Such systems are robust if they are able to extract at least partial information despite the presence of ill-formed or unexpected syntactic, semantic, or discourse structures."
H89-2023,Data Collection and Evaluation {II},1989,0,5,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"This session focussed on two inter-related issues: (1) performance assessment for spoken language systems and (2) experience to date in speech corpora collection for these systems. The session included formal presentations from representatives of SRI International, MIT's Laboratory for Computer Science, BBN Systems and Technologies Corporation, and Carnegie Mellon University's School of Computer Science."
H89-2069,Robust Natural Language Analysis,1989,0,0,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"Our basic goal is the development of more robust systems for extracting information from natural language text. A robust system is one which is able to extract at least partial information despite the presence of ill-formed or unexpected syntactic, semantic, or discourse structures. Our approach has two aspects: First, we incorporate a rich set of syntactic, semantic, and discourse constraints, so that one type of constraint can guide us to a correct analysis even if another type is violated. Second, we provide mechanisms for relaxing individual constraints, and for scoring alternative analyses, so that the analysis violating the fewest constraints (and therefore, presumably, the best analysis obtainable for a sentence) will be selected."
H89-1033,Natural Language Understanding,1989,0,0,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"Our task, broadly stated, is the development of systems for the understanding of narrative messages in limited domains. Improving the current state-of-the-art for such systems will require a better understanding of how to capture and utilize domain information, and how to effectively combine the various sources of information (syntactic, semantic, and discourse) to create a robust language analyzer."
H89-1034,Analyzing Telegraphic Messages,1989,7,7,1,1,10781,ralph grishman,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"Most people have little difficulty reading telegraphic-style messages such asSHIPMENT GOLD BULLION ARRIVING STAGECOACH JAN. 7 3 PMeven though lots of material has been omitted which would be required in standard English, such as articles, prepositions, and verbs. Our concern in this paper is how to process such messages by computer. Even though people don't send many telegrams anymore, this problem is still of importance because many military messages are written in this telegraphic style:2 FLARES SIGHTED 230704Z6 SOUTH APPROX 5 MI SPA ESTABLISHED(here 230704Z6 is the time, and SPA is the Submarine Probability Area)."
A88-1009,Responding to Semantically Ill-Formed Input,1988,11,5,1,1,10781,ralph grishman,Second Conference on Applied Natural Language Processing,0,One cause of failure in natural language interfaces is semantic overshoot; this is reflected in input sentences which do not correspond to any semantic pattern in the system. We describe a system which provides helpful feedback in such cases by identifying the semantically closest inputs which the system would be able to understand.
A88-1010,Evaluation of a Parallel Chart Parser,1988,7,11,1,1,10781,ralph grishman,Second Conference on Applied Natural Language Processing,0,"We describe a parallel implementation of a chart parser for a shared-memory multiprocessor. The speed-ups obtained with this parser have been measured for a number of small natural-language grammars. For the largest of these, part of an operational question-answering system, the parser ran 5 to 7 times faster than the serial version."
1988.tmi-1.17,A comparative study of {J}apanese and {E}nglish sublanguage patterns,1988,7,5,3,0,54617,virginia teller,Proceedings of the Second Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"Abstract : As part of a project to develop a Japanese-English machine translation system for technical texts within a limited domain, we conducted a study to investigate the roles that sublanguage techniques and operator-argument grammar would play in the analysis and transfer stages of the system. The data consisted of fifty sentences from the Japanese and English versions of the FOCUS Query Language Primer, which were decomposed into elementary sentence patterns. A total of 187 pattern instances were found for Japanese and 191 for English. When the elements of these elementary sentences were classified and compared with their counterparts in the other language, we identified 43 word classes in Japanese and 43 corresponding English word classes. These word classes formed 32 sublanguage patterns in each language, 29 of which corresponded to patterns in the other language. This paper examines in detail these correspondences as well as the mismatches between sublanguage patterns in Japanese and English. The high level of agreement found between sublanguage categories and patterns in Japanese and English suggests that these categories and patterns can facilitate analysis and transfer. Use of operator-argument grammar, which incorporates operator trees as an intermediate representation, substantially reduces the amount of structural transfer needed in the system."
P86-1001,Tutorial Abstracts,1986,-1,-1,1,1,10781,ralph grishman,24th Annual Meeting of the Association for Computational Linguistics,1,None
J86-3002,Discovery Procedures for Sublanguage Selectional Patterns: Initial Experiments,1986,15,75,1,1,10781,ralph grishman,Computational Linguistics,0,"Selectional constraints specify, for a particular domain, the combinations of semantic classes acceptable in subject-verb-object relationships and other syntactic structures. These constraints are important in blocking incorrect analyses in natural language processing systems. However, these constraints are domain-specific and hence must be developed anew when a system is ported to a new domain. A discovery procedure for selectional constraints is therefore essential in enhancing the portability of such systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from a sample of texts in a domain, and then using these patterns as the basis for selectional constraints in analyzing further texts. We discuss some of the difficulties in automating the collection process, and describe two experiments that measure the completeness of these patterns and their effectiveness compared with manually-prepared patterns. We then describe and evaluate a procedure for selectional constraint relaxation, intended to compensate for gaps in the set of patterns. Finally, we suggest how these procedures could be combined with a system that queries a domain expert, in order to produce a more efficient discovery procedure."
H86-1002,"{PROTEUS} and {PUNDIT}: {RESEARCH} {IN} {TEXT} {UNDERSTANDING} at the {D}epartment of {C}omputer {S}cience, {N}ew {Y}ork {U}niversity and {S}ystem {D}evelopment {C}orporation -- A {B}urroughs Company",1986,0,0,1,1,10781,ralph grishman,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"Abstract : We are engaged in the development systems capable of analyzing short narrative messages dealing with a limited domain and extracting the information contained in the narrative. These systems are initially being applied to messages describing equipment failure. This work is a joint effort of New York University and the System Development Corp. for the DARPA Strategic Computing Program. Our aim is to create a system reliable enough for use in an operational environment. This is a formidable task, both because the texts are unedited (and so contain various errors) and because the complexity of any real domain precludes us from assembling a complete collection of the relationships and domain knowledge relevant to understanding texts in the domain. Our basic approach to increasing reliability will be to bring to bear on the analysis task as many different types of constraints as possible. These include constraints related to syntax, semantics, domain knowledge, and discourse structure. In order to be able to capture the detailed knowledge about the domain that is needed for correct message analyses, we are initially limiting ourselves to messages about one particular piece of equipment (the starting air compressor); if we are successful in this narrow domain, we intend to gradually broaden our system."
H86-1009,Model-based Analysis of Messages about Equipment,1986,14,4,1,1,10781,ralph grishman,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"The aim of PROTEUS - a system for the analysis of short technical texts - is to increase the reliability of the analysis process through the integration of syntactic and semantic constraints, domain knowledge, and knowledge of discourse structure. This system is initially being applied to the analysis of messages describing the failure, diagnosis, and repair of selected pieces of equipment. This has required us to develop a detailed model of the structure and function of the equipment involved. We focus in this paper on the nature of this model and the roles it plays in the syntactic and semantic analysis of the text."
H86-1010,An Equipment Model and Its Role in the Interpretation of Nominal Compounds,1986,-1,-1,2,0,57583,tomasz ksiezyk,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,None
P84-1023,Automated Determination of Sublanguage Syntactic Usage,1984,6,12,1,1,10781,ralph grishman,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"Sublanguages differ from each other, and from the standard language, in their syntactic, semantic, and discourse properties. Understanding these differnces is important if we are to improve our ability to process these sublanguages. We have developed a semiautomatic procedure for identifying sublanguage syntactic usage from a sample of text in the sublanguage. We describe the results of applying this procedure to three text samples: two sets of medical documents and a set of equipment failure messages."
C82-1014,Natural Language Interfaces Using Limited Semantic Information,1982,8,10,1,1,10781,ralph grishman,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In order to analyze their input properly, natural language interfaces require access to domain-specific semantic information. However, design considerations for practical systems -- in particular, the desire to construct interfaces which are readily portable to new domains -- require us to limit and segregate this domain-specific information. We consider here the possibility of limiting ourselves to a characterization of the structure of information in a domain. This structure is captured in a domain information schema , which specifies the semantic classes of the domain, the words and phrases which belong to these classes, and the predicate-argument relationships among members of these classes which are meaningful in the domain. We describe how this schema is used by the various stages of two large natural language processing systems."
P81-1022,Parsing,1981,0,0,1,1,10781,ralph grishman,19th Annual Meeting of the Association for Computational Linguistics,1,None
C80-1075,Conjunctions and Modularity in Language Analysis Procedures,1980,8,3,1,1,10781,ralph grishman,{COLING} 1980 Volume 1: The 8th International Conference on Computational Linguistics,0,"The further enrichment of natural language systems depends in part on finding ways of factoring the effects of various linguistic phenomena, so that these systems can be partitioned into modules of comprehensible size and structure. Coordinate conjunction has a substantial impact on all aspects of syntactic analysis -constituent structure, grammatical constraints, and transformations. If the rules of syntactic analysis were directly expanded to accomodate conjunction, their size would increase severalfold. We describe below the mechanisms we have used to localize the effect of conjunction in our natural language analyzer, so that most of the rules of our grammar need not explicitly take conjunction into account."
P79-1025,Response Generation in Question - Answering Systems,1979,5,15,1,1,10781,ralph grishman,17th Annual Meeting of the Association for Computational Linguistics,1,None
J76-2006,A Survey of Syntactic Analysis Procedures for Natural Language,1976,-1,-1,1,1,10781,ralph grishman,American Journal of Computational Linguistics,0,None
