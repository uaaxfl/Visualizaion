2021.findings-emnlp.242,{``}Be nice to your wife! The restaurants are closed{''}: Can Gender Stereotype Detection Improve Sexism Classification?,2021,-1,-1,3,1,7012,patricia chiril,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"In this paper, we focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. We propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation based on sentence similarity with multilingual external datasets, and (3) a set of deep learning experiments first to detect gender stereotypes and then, to use this auxiliary task for sexism detection. Although the presence of stereotypes does not necessarily entail hateful content, our results show that sexism classification can definitively benefit from gender stereotype detection."
2020.lrec-1.175,An Annotated Corpus for Sexism Detection in {F}rench Tweets,2020,-1,-1,2,1,7012,patricia chiril,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper presents the first French corpus annotated for sexism detection composed of about 12,000 tweets. In a context of offensive content mediation on social media now regulated by European laws, we think that it is important to be able to detect automatically not only sexist content but also to identify if a message with a sexist content is really sexist (i.e. addressed to a woman or describing a woman or women in general) or is a story of sexism experienced by a woman. This point is the novelty of our annotation scheme. We also propose some preliminary results for sexism detection obtained with a deep learning approach. Our experiments show encouraging results."
2020.acl-main.373,He said {``}who{'}s gonna take care of your children when you are at {ACL}?{''}: Reported Sexist Acts are Not Sexist,2020,-1,-1,2,1,7012,patricia chiril,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet{'}s vectorial representations (word embeddings, linguistic features, and various generalization strategies). Our results are encouraging and constitute a first step towards offensive content moderation."
S19-2087,The binary trio at {S}em{E}val-2019 Task 5: Multitarget Hate Speech Detection in Tweets,2019,0,0,3,1,7012,patricia chiril,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"The massive growth of user-generated web content through blogs, online forums and most notably, social media networks, led to a large spreading of hatred or abusive messages which have to be moderated. This paper proposes a supervised approach to hate speech detection towards immigrants and women in English tweets. Several models have been developed ranging from feature-engineering approaches to neural ones."
2019.jeptalnrecital-court.21,Multilingual and Multitarget Hate Speech Detection in Tweets,2019,-1,-1,3,1,7012,patricia chiril,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Social media networks have become a space where users are free to relate their opinions and sentiments which may lead to a large spreading of hatred or abusive messages which have to be moderated. This paper proposes a supervised approach to hate speech detection from a multilingual perspective. We focus in particular on hateful messages towards two different targets (immigrants and women) in English tweets, as well as sexist messages in both English and French. Several models have been developed ranging from feature-engineering approaches to neural ones. Our experiments show very encouraging results on both languages."
E17-1025,Exploring the Impact of Pragmatic Phenomena on Irony Detection in Tweets: A Multilingual Corpus Study,2017,28,16,3,1,18778,jihen karoui,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter{'}s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first time a multi-layered annotation schema for irony and its application to a corpus of French, English and Italian tweets. We detail each layer, explore their interactions, and discuss our results according to a qualitative and quantitative perspective."
S16-1190,{LIMSI} at {S}em{E}val-2016 Task 12: machine-learning and temporal information to identify clinical events and time expressions,2016,9,3,2,0.24151,5675,cyril grouin,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Our experiments rely on a combination of machine-learning (CRF) and rule-based (HeidelTime) systems. First, a CRF system identifies both EVENTS and TIMEX3, along with polarity values for EVENT and types of TIMEX. Second, the HeidelTime tool identifies DOCTIME and TIMEX3 elements, and computes DocTimeRel for each EVENT identified by the CRF. Third, another CRF system computes DocTimeRel for each previously identified EVENT, based on DocTimeRel computed by HeidelTime. In the first submission, all EVENTS and TIMEX3 are identified through one general CRF model while in the second submission, we combined two CRF models (one for both EVENT and TIMEX3, and one only for TIMEX3) and we applied post-processing rules on the outputs."
P15-2106,Towards a Contextual Pragmatic Model to Detect Irony in Tweets,2015,26,26,3,1,18778,jihen karoui,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper proposes an approach to capture the pragmatic context needed to infer irony in tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form Not(P1) is ironic if and only if one can assess the absurdity of P1. Our first results are encouraging and show that deriving a pragmatic contextual model is feasible."
2015.jeptalnrecital-long.3,Identification de facteurs de risque pour des patients diab{\\'e}tiques {\\`a} partir de comptes-rendus cliniques par des approches hybrides,2015,-1,-1,2,0.24151,5675,cyril grouin,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons les m{\'e}thodes que nous avons d{\'e}velopp{\'e}es pour analyser des comptes- rendus hospitaliers r{\'e}dig{\'e}s en anglais. L{'}objectif de cette {\'e}tude consiste {\`a} identifier les facteurs de risque de d{\'e}c{\`e}s pour des patients diab{\'e}tiques et {\`a} positionner les {\'e}v{\'e}nements m{\'e}dicaux d{\'e}crits par rapport {\`a} la date de cr{\'e}ation de chaque document. Notre approche repose sur (i) HeidelTime pour identifier les expressions temporelles, (ii) des CRF compl{\'e}t{\'e}s par des r{\`e}gles de post-traitement pour identifier les traitements, les maladies et facteurs de risque, et (iii) des r{\`e}gles pour positionner temporellement chaque {\'e}v{\'e}nement m{\'e}dical. Sur un corpus de 514 documents, nous obtenons une F-mesure globale de 0,8451. Nous observons que l{'}identification des informations directement mentionn{\'e}es dans les documents se r{\'e}v{\`e}le plus performante que l{'}inf{\'e}rence d{'}informations {\`a} partir de r{\'e}sultats de laboratoire."
2015.jeptalnrecital-court.22,D{\\'e}tection automatique de l{'}ironie dans les tweets en fran{\\c{c}}ais,2015,-1,-1,3,1,18778,jihen karoui,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article pr{\'e}sente une m{\'e}thode par apprentissage supervis{\'e} pour la d{\'e}tection de l{'}ironie dans les tweets en fran{\c{c}}ais. Un classifieur binaire utilise des traits de l{'}{\'e}tat de l{'}art dont les performances sont reconnues, ainsi que de nouveaux traits issus de notre {\'e}tude de corpus. En particulier, nous nous sommes int{\'e}ress{\'e}s {\`a} la n{\'e}gation et aux oppositions explicites/implicites entre des expressions d{'}opinion ayant des polarit{\'e}s diff{\'e}rentes. Les r{\'e}sultats obtenus sont encourageants."
2015.jeptalnrecital-court.40,"M{\\'e}dicaments qui soignent, m{\\'e}dicaments qui rendent malades : {\\'e}tude des relations causales pour identifier les effets secondaires",2015,-1,-1,3,0,35053,franccois morlanehondere,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous nous int{\'e}ressons {\`a} la mani{\`e}re dont sont exprim{\'e}s les liens qui existent entre un traitement m{\'e}dical et un effet secondaire. Parce que les patients se tournent en priorit{\'e} vers internet, nous fondons cette {\'e}tude sur un corpus annot{\'e} de messages issus de forums de sant{\'e} en fran{\c{c}}ais. L{'}objectif de ce travail consiste {\`a} mettre en {\'e}vidence des {\'e}l{\'e}ments linguistiques (connecteurs logiques et expressions temporelles) qui pourraient {\^e}tre utiles pour des syst{\`e}mes automatiques de rep{\'e}rage des effets secondaires. Nous observons que les modalit{\'e}s d{'}{\'e}criture sur les forums ne permettent pas de se fonder sur les expressions temporelles. En revanche, les connecteurs logiques semblent utiles pour identifier les effets secondaires."
W14-6305,Fine-grained semantic categorization of opinion expressions for consensus detection (Cat{\\'e}gorisation s{\\'e}mantique fine des expressions d{'}opinion pour la d{\\'e}tection de consensus) [in {F}rench],2014,0,1,2,0.208485,7013,farah benamara,TALN-RECITAL 2014 Workshop DEFT 2014 : D{\\'E}fi Fouille de Textes (DEFT 2014 Workshop: Text Mining Challenge),0,None
moriceau-tannier-2014-french,{F}rench Resources for Extraction and Normalization of Temporal Expressions with {H}eidel{T}ime,2014,13,7,1,1,7014,veronique moriceau,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe the development of French resources for the extraction and normalization of temporal expressions with HeidelTime, a open-source multilingual, cross-domain temporal tagger. HeidelTime extracts temporal expressions from documents and normalizes them according to the TIMEX3 annotation standard. Several types of temporal expressions are extracted: dates, times, durations and temporal sets. French resources have been evaluated in two different ways: on the French TimeBank corpus, a corpus of newspaper articles in French annotated according to the ISO-TimeML standard, and on a user application for automatic building of event timelines. Results on the French TimeBank are quite satisfaying as they are comparable to those obtained by HeidelTime in English and Spanish on newswire articles. Concerning the user application, we used two temporal taggers for the preprocessing of the corpus in order to compare their performance and results show that the performances of our application on French documents are better with HeidelTime. The French resources and evaluation scripts are publicly available with HeidelTime."
F14-2011,User evaluation of a multiple answer extraction system on the Web ({\\'E}valuation d{'}un syst{\\`e}me d{'}extraction de r{\\'e}ponses multiples sur le Web par comparaison {\\`a} des humains) [in {F}rench],2014,0,0,2,1,40007,mathieuhenri falco,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
C14-1114,Ranking Multidocument Event Descriptions for Building Thematic Timelines,2014,15,5,3,0,6565,kiemhieu nguyen,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper tackles the problem of timeline generation from traditional news sources. Our system builds thematic timelines for a general-domain topic defined by a user query. The system selects and ranks events relevant to the input query. Each event is represented by a one-sentence description in the output timeline. We present an inter-cluster ranking algorithm that takes events from multiple clusters as input and that selects the most salient and relevant events. A cluster, in our work, contains all the events happening in a specific date. Our algorithm utilizes the temporal information derived from a large collection of extensively temporal analyzed texts. Such temporal information is combined with textual contents into an event scoring model in order to rank events based on their salience and query-relevance."
F13-3006,An Interface for Validating and Evaluating Thematic Timelines (Une interface pour la validation et l{'}{\\'e}valuation de chronologies th{\\'e}matiques) [in {F}rench],2013,0,0,2,0.498703,5686,xavier tannier,Proceedings of TALN 2013 (Volume 3: System Demonstrations),0,None
D13-1098,Building Event Threads out of Multiple News Articles,2013,17,3,2,0.498703,5686,xavier tannier,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set 1 ."
P12-1077,Finding Salient Dates for Building Thematic Timelines,2012,27,22,4,0,40008,remy kessler,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related."
bittar-etal-2012-temporal,Temporal Annotation: A Proposal for Guidelines and an Experiment with Inter-annotator Agreement,2012,10,3,3,0,27920,andre bittar,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This article presents work carried out within the framework of the ongoing ANR (French National Research Agency) project Chronolines, which focuses on the temporal processing of large news-wire corpora in English and French. The aim of the project is to create new and innovative interfaces for visualizing textual content according to temporal criteria. Extracting and normalizing the temporal information in texts through linguistic annotation is an essential step towards attaining this objective. With this goal in mind, we developed a set of guidelines for the annotation of temporal and event expressions that is intended to be compatible with the TimeML markup language, while addressing some of its pitfalls. We provide results of an initial application of these guidelines to real news-wire texts in French over several iterations of the annotation process. These results include inter-annotator agreement figures and an error analysis. Our final inter-annotator agreement figures compare favorably with those reported for the TimeBank 1.2 annotation project."
tannier-etal-2012-evolution,Evolution of Event Designation in Media: Preliminary Study,2012,4,2,2,0.498703,5686,xavier tannier,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Within the general purpose of information extraction, detection of event descriptions is often an important clue. An important characteristic of event designation in texts, and especially in media, is that it changes over time. Understanding how these designations evolve is important in information retrieval and information extraction. Our first hypothesis is that, when an event first occurs, media relate it in a very descriptive way (using verbal designations) whereas after some time, they use shorter nominal designations instead. Our second hypothesis is that the number of different nominal designations for an event tends to stabilize itself over time. In this article, we present our methodology concerning the study of the evolution of event designations in French documents from the news agency AFP. For this preliminary study, we focused on 7 topics which have been relatively important in France. Verbal and nominal designations of events have been manually annotated in manually selected topic-related passages. This French corpus contains a total of 2064 annotations. We then provide preliminary interesting statistical results and observations concerning these evolutions."
falco-etal-2012-kitten,{K}itten: a tool for normalizing {HTML} and extracting its textual content,2012,10,4,2,1,40007,mathieuhenri falco,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The web is composed of a gigantic amount of documents that can be very useful for information extraction systems. Most of them are written in HTML and have to be rendered by an HTML engine in order to display the data they contain on a screen. HTML file thus mix both informational and rendering content. Our goal is to design a tool for informational content extraction. A linear extraction with only a basic filtering of rendering content would not be enough as objects such as lists and tables are linearly coded but need to be read in a non-linear way to be well interpreted. Besides these HTML pages are often incorrectly coded from an HTML point of view and use a segmentation of blocks based on blank space that cannot be transposed in a text filewithout confusing syntactic parsers. For this purpose, we propose the Kitten tool that first normalizes HTML file into unicode XHTML file, then extracts the informational content into a text filewith a special processing for sentences, lists and tables."
2011.jeptalnrecital-court.6,G{\\'e}n{\\'e}ration automatique de questions {\\`a} partir de textes en fran{\\c{c}}ais (Automatic generation of questions from texts in {F}rench),2011,-1,-1,3,0,44944,louis viron,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons dans cet article un g{\'e}n{\'e}rateur automatique de questions pour le fran{\c{c}}ais. Le syst{\`e}me de g{\'e}n{\'e}ration proc{\`e}de par transformation de phrases d{\'e}claratives en interrogatives et se base sur une analyse syntaxique pr{\'e}alable de la phrase de base. Nous d{\'e}taillons les diff{\'e}rents types de questions g{\'e}n{\'e}r{\'e}es. Nous pr{\'e}sentons {\'e}galement une {\'e}valuation de l{'}outil, qui d{\'e}montre que 41 {\%} des questions g{\'e}n{\'e}r{\'e}es par le syst{\`e}me sont parfaitement bien form{\'e}es."
tannier-moriceau-2010-fidji,{FIDJI}: Web Question-Answering at Quaero 2009,2010,6,3,2,0.47499,5686,xavier tannier,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents the participation of FIDJI system to the Web Question-Answering evaluation campaign organized by Quaero in 2009. FIDJI is an open-domain question-answering system which combines syntactic information with traditional QA techniques such as named entity recognition and term weighting in order to validate answers through multiple documents. It was originally designed to process ``clean'' document collections. Overall results are significantly lower than in traditional campaigns but results (for French evaluation) are quite good compared to other state-of-the-art systems. They show that a syntax-based strategy, applied on uncleaned Web data, can still obtain good results. Moreover, we obtain much higher scores on ``complex'' questions, i.e. `how' and `why' questions, which are more representative of real user needs. These results show that questioning the Web with advanced linguistic techniques can be done without heavy pre-processing and with results that come near to best systems that use strong resources and large structured indexes."
quintard-etal-2010-question,Question Answering on Web Data: The {QA} Evaluation in Qu{\\ae}ro,2010,10,24,6,0,42953,ludovic quintard,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In the QA and information retrieval domains progress has been assessed via evaluation campaigns(Clef, Ntcir, Equer, Trec).In these evaluations, the systems handle independent questions and should provide one answer to each question, extracted from textual data, for both open domain and restricted domain. Qu{\ae}ro is a program promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Among the many research areas concerned by Qu{\ae}ro. The Quaero project organized a series of evaluations of Question Answering on Web Data systems in 2008 and 2009. For each language, English and French the full corpus has a size of around 20Gb for 2.5M documents. We describe the task and corpora, and especially the methodologies used in 2008 to construct the test of question and a new one in the 2009 campaign. Six types of questions were addressed, factual, Non-factual(How, Why, What), List, Boolean. A description of the participating systems and the obtained results is provided. We show the difficulty for a question-answering system to work with complex data and questions."
grappy-etal-2010-corpus,A Corpus for Studying Full Answer Justification,2010,7,6,5,0,42529,arnaud grappy,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Question answering (QA) systems aim at retrieving precise information from a large collection of documents. To be considered as reliable by users, a QA system must provide elements to evaluate the answer. This notion of answer justification can also be useful when developping a QA system in order to give criteria for selecting correct answers. An answer justification can be found in a sentence, a passage made of several consecutive sentences or several passages of a document or several documents. Thus, we are interesting in pinpointing the set of information that allows to verify the correctness of the answer in a candidate passage and the question elements that are missing in this passage. Moreover, the relevant information is often given in texts in a different form from the question form: anaphora, paraphrases, synonyms. In order to have a better idea of the importance of all the phenomena we underlined, and to provide enough examples at the QA developer's disposal to study them, we decided to build an annotated corpus."
2010.jeptalnrecital-court.6,Une {\\'e}tude des questions {``}complexes{''} en question-r{\\'e}ponse,2010,-1,-1,1,1,7014,veronique moriceau,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La plupart des syst{\`e}mes de question-r{\'e}ponse ont {\'e}t{\'e} con{\c{c}}us pour r{\'e}pondre {\`a} des questions dites {``}factuelles{''} (r{\'e}ponses pr{\'e}cises comme des dates, des lieux), et peu se sont int{\'e}ress{\'e}s au traitement des questions complexes. Cet article pr{\'e}sente une typologie des questions en y incluant les questions complexes, ainsi qu{'}une typologie des formes de r{\'e}ponses attendues pour chaque type de questions. Nous pr{\'e}sentons {\'e}galement des exp{\'e}riences pr{\'e}liminaires utilisant ces typologies pour les questions complexes, avec de bons r{\'e}sultats."
2009.jeptalnrecital-court.6,Apport de la syntaxe dans un syst{\\`e}me de question-r{\\'e}ponse : {\\'e}tude du syst{\\`e}me {FIDJI}.,2009,-1,-1,1,1,7014,veronique moriceau,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article pr{\'e}sente une s{\'e}rie d{'}{\'e}valuations visant {\`a} {\'e}tudier l{'}apport d{'}une analyse syntaxique robuste des questions et des documents dans un syst{\`e}me de questions-r{\'e}ponses. Ces {\'e}valuations ont {\'e}t{\'e} effectu{\'e}es sur le syst{\`e}me FIDJI, qui utilise {\`a} la fois des informations syntaxiques et des techniques plus {``}traditionnelles{''}. La s{\'e}lection des documents, l{'}extraction de la r{\'e}ponse ainsi que le comportement selon les diff{\'e}rents types de questions ont {\'e}t{\'e} {\'e}tudi{\'e}s."
W06-1808,Numerical Data Integration for Cooperative Question-Answering,2006,6,15,1,1,7014,veronique moriceau,Proceedings of the Workshop {KRAQ}{'}06: Knowledge and Reasoning for Language Processing,0,"In this paper, we present an approach which aims at providing numerical answers in a question-answering system. These answers are generated in a cooperative way: they explain the variation of numerical values when several values, apparently incoherent, are extracted from the web as possible answers to a question."
W06-1415,Generating Intelligent Numerical Answers in a Question-Answering System,2006,12,9,1,1,7014,veronique moriceau,Proceedings of the Fourth International Natural Language Generation Conference,0,"In this paper, we present a question-answering system on the Web which aims at generating intelligent answers to numerical questions. These answers are generated in a cooperative way: besides a direct answer, comments are generated to explain to the user the variation of numerical data extracted from the Web. We present the content determination and realisation tasks. We also present some elements of evaluation with respect to end-users."
moriceau-2006-language,Language Challenges for Data Fusion in Question-Answering,2006,6,0,1,1,7014,veronique moriceau,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Search engines on the web and most existing question-answering systems provide the user with a set of hyperlinks and/or web page extracts containing answer(s) to a question. These answers are often incoherent to a certain degree (equivalent, contradictory, etc.). It is then quite difficult for the user to know which answer is the correct one. In this paper, we present an approach which aims at providing synthetic numerical answers in a question-answering system. These answers are generated in natural language and, in a cooperative perspective, the aim is to explain to the user the variation of numerical values when several values, apparently incoherent, are extracted from the web as possible answers to a question. We present in particular how lexical resources are essential to answer extraction from the web, to the characterization of the variation mode associated with the type of information and to answer generation in natural language."
W05-1625,Answer Generation with Temporal Data Integration,2005,4,3,1,1,7014,veronique moriceau,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,None
W04-0202,{COOPML}: Towards Annotating Cooperative Discourse,2004,21,2,2,0,7013,farah benamara,Proceedings of the Workshop on Discourse Annotation,0,"In this paper, we present a preliminary version of COOPML, a language designed for annotating cooperative discourse. We investigate the different linguistic marks that identify and characterize the different forms of cooperativity found in written texts from FAQs, Forums and emails."
