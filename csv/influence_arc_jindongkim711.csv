2020.iwltp-1.10,P14-5010,0,0.0093506,"ily in their conventions for specifying input and output. Table 1 gives an overview of the synchronous protocol APIs for several NLPaaS services, including the CLARIN-D (Hinrichs et al., 2010) and CLARIN-PL (Piasecki, 2014) services from the European CLARIN project; ETRI NLP API Korean NLP5 , developed and maintained by the Electronics and Telecommunications Research Institute (ETRI); Google Natural Language API, a commercial service provided by Google; and PubDictionaries (Kim et al., 2019), a service provided by the Database Center for Life Science (DBCLS). We also include Stanford CoreNLP (Manning et al., 2014), which is one of the most widely used NLP toolsets that is also implemented as a NLPaaS web service. Figure 3: General asynchronous protocol with status checking POST request) and completed by a response from the server to the client. Synchronous protocols block activity on the client as well as the server while the request is being processed; therefore, to avoid resource starvation in unexpected situations (network problems, errors, etc.), the request is typically subjected to a conservative timeout.4 Therefore, when requests are expected to take an extended amount of time, e.g. in order to"
2020.paclic-1.25,W18-5503,0,0.0632011,"Missing"
2020.paclic-1.25,N06-2015,0,0.220931,"Missing"
2020.paclic-1.25,D19-1588,0,0.0458678,"Missing"
2020.paclic-1.25,P16-1101,0,0.110295,"Missing"
2020.paclic-1.25,W18-6013,1,0.812314,"gold set can only be performed on a limited part of the evaluation set sharing the same part of raw corpus to be annotated. notation: morphological analysis, lexical sense analysis, named entity analysis, subject anaphora resolution, co-reference resolution, dependency analysis, and semantic roles analysis. The evaluation sets are also constructed by the same layers. 2 In Korean, the word segment divided by white space is called ”Eojeol”, this is composed of a noun or verb stem combined with a postposition (”Josa”) or ending (”Eomi”) that function as inflectional and derivational particles. (Noh et al., 2018) 3 In this project, we are constructed 7 linguistic layers of corpus annotations as gold set to validate 7 linguistic layers of corpus annotation (evaluation set) constructed by other project groups. The evaluation sets after validation can be downloaded at https://corpus.korean.go.kr/. Figure 1: The flow of the corpus annotation and validation process in this paper. Blue-coloureds indicate our process and result, and green-coloureds indicate evaluatee group’s process and result. That’s why we present an additional method to validate in the range of evaluation set without a gold set. Thus, we"
2020.paclic-1.25,N19-1176,0,0.053472,"Missing"
C00-2165,J95-2004,0,\N,Missing
C08-1079,P06-1005,0,0.0147153,"by 3.64 points. We further conducted a small test by excluding this combination from the netype feature group, but the success rate remained unchanged from the baseline result. This signifies that this combination contributed the most to the above increase. The combination of C netype and P semw features exploits the co-ocurrence of the semantic type of the candidate antecedent and the context word, which appears in some relationship with the pronoun. This combination feature uses the information similar to the semantic compatibility features proposed by Yang (Yang et al., 2005) and Bergsma (Bergsma and Lin, 2006). Depending on the pronoun type, the feature extractor decides which relationship is used. For example, the resolver successfully recognizes the antecedent of the pronoun its in this discourse: “HSF3 is constitutively expressed in the erythroblast cell line HD6 , the lymphoblast cell line MSB , and embryo fibroblasts , and yet its DNA-binding activity is induced only upon exposure of HD6 cells to heat shock ,” because HSF3 was detected as a Protein entity, which has a strong association with the governing head noun activity of the pronoun. Another example is the correct anaphora link between “"
C08-1079,P07-1107,0,0.0125654,"nce annotated corpus for the bio domain, containing 1999 MEDLINE abstracts. While there are quite a few works on this task for the bio-medical domain, for other domains, and especially for the news domain, a myriad of works on pronoun resolution has been carried out by the NLP researchers (Mitkov, 2002). Since Soon (Soon et al., 2001) started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (Haghighi and Klein, 2007). Such methods were claimed to be comparable with traditional methods. However, the problems caused by domain differences, which strongly affect a deep-semantics related task like pronoun resolution, have not yet been studied well enough. In order to recognize the important factors in 625 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 625–632 Manchester, August 2008 building an effective machine learning-based pronoun resolution system, and in particular for the bio-domain, we have built a machine learningbased pronoun resolver and observed t"
C08-1079,W04-0711,0,0.0232289,"Missing"
C08-1079,J01-4004,0,0.432911,"ere small. In the former system, 46 and 54 MEDLINE abstracts were used for the development set, and the test set respectively, and the test set in the latter work contained only sixteen anaphoric pronouns. Contrary to their work, in this work we made use of GENIA, a large co-reference annotated corpus for the bio domain, containing 1999 MEDLINE abstracts. While there are quite a few works on this task for the bio-medical domain, for other domains, and especially for the news domain, a myriad of works on pronoun resolution has been carried out by the NLP researchers (Mitkov, 2002). Since Soon (Soon et al., 2001) started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (Haghighi and Klein, 2007). Such methods were claimed to be comparable with traditional methods. However, the problems caused by domain differences, which strongly affect a deep-semantics related task like pronoun resolution, have not yet been studied well enough. In order to recognize the important factors in 625 Proceedings of the 22"
C08-1079,P05-1021,0,0.0134296,"hich contributed to the increase by 3.64 points. We further conducted a small test by excluding this combination from the netype feature group, but the success rate remained unchanged from the baseline result. This signifies that this combination contributed the most to the above increase. The combination of C netype and P semw features exploits the co-ocurrence of the semantic type of the candidate antecedent and the context word, which appears in some relationship with the pronoun. This combination feature uses the information similar to the semantic compatibility features proposed by Yang (Yang et al., 2005) and Bergsma (Bergsma and Lin, 2006). Depending on the pronoun type, the feature extractor decides which relationship is used. For example, the resolver successfully recognizes the antecedent of the pronoun its in this discourse: “HSF3 is constitutively expressed in the erythroblast cell line HD6 , the lymphoblast cell line MSB , and embryo fibroblasts , and yet its DNA-binding activity is induced only upon exposure of HD6 cells to heat shock ,” because HSF3 was detected as a Protein entity, which has a strong association with the governing head noun activity of the pronoun. Another example is"
C08-1079,J96-1002,0,0.011934,"h a pronomial antecedent, while in the MUC corpus, this kind of link is allowed. In order to achieve the fairest comparative experimental results, we uniformly choose the nearest item in the co-reference chain of a pronoun, and make a gold anaphora link. This policy is best suited for ACE, thanks to the symmetric scheme used. 627 Table 4: Sizes of the data sets (number of anaphoric pronoun) Training set Test set GENIA 1442 357 ACE 2427 633 MUC 371 240 3 Implementation 3.1 Pronoun resolution model We built a machine learning based pronoun resolution engine using a Maximum Entropy ranker model (Berger et al., 1996), similar with Denis and Baldridge’s model (Denis and Baldridge, 2007). For every anaphoric pronoun π, the ranker selects the most likely antecedent candidate α, from a set of k candidate markables. ∑ exp ( ni=1 λi fi (π, αj )) ∑n Pr (αj |π) = ∑ k exp ( i=1 λi fi (π, αk )) Figure 2: Analysis of anaphoric pronoun in different data sets (1) We constructed the training examples in the following way: for each gold anaphora link in the training corpus, we created a positive instance, and negative training instances are created by pairing the pronoun with all of the other markables appearing in a wi"
I13-1157,W09-1401,1,\N,Missing
I13-1157,W03-0804,0,\N,Missing
L18-1327,ide-etal-2014-language,1,0.911249,"h papers for associations and connections (such as between drugs and side effects, or genes and disease pathways) that humans reading each paper individually might not notice. Up to now, the use of NLP technologies has required considerable skill in the field. However, recent development of environments for constructing customizable NLP applications has opened the door for scientists to exploit NLP technologies for discovering and mining information from massive bodies of scientific publications such as those found in PubMed, PLoS, Web of Science, etc. The Language Applications (LAPPS) Grid1 (Ide et al., 2014) provides an infrastructure for rapid development of natural language processing applications (NLP) by providing access to a wide range of tools and making them both syntactically and semantically interoperable. The LAPPS Grid uses the Galaxy platform2 (Giardine et al., 2005), originally developed for use by genomics researchers with little computational expertise, as its workflow engine. The Galaxy interface and the interoperability among tools together provide an intuitive and easy-to-use platform that enables users to experiment with and exploit NLP tools and resources without the need to d"
L18-1327,I08-2122,0,0.537432,"Environment (XSEDE)6 and the associated Jetstream7 cloud environment. These resources allow users to create virtual machines configured as specialized versions of the LAPPS Grid on the remote resource. If necessary, access can be given to specified domain servers holding secure data. 2.1. http://www.lappsgrid.org http://incommon.org 6 https://www.xsede.org 7 https://jetstream-cloud.org 8 http://galaxyproject.org 5 2.2. Comparison to existing platforms for biomedical text analysis The two most well-known platforms that currently support scientific literature mining are the UIMA-based UCompare (Kano et al., 2008) and a more recently developed platform named Argo (Rak et al., 2012). Both of these systems allow the user to assemble modular pipelines and perform evaluations against a gold standard. U-Compare is plagued by instabilities of platform interoperability, permissions, and the like, and typically requires the intercession of a specialized software engineer. Argo attempts to ameliorate some of these problems by providing a webbased interface to the underlying UIMA-based system, but suffers from many of the same problems as U-Compare and is seemingly unsupported at this time. Frameworks that suppo"
nguyen-etal-2008-challenges,H05-1059,1,\N,Missing
nguyen-etal-2008-challenges,P06-1005,0,\N,Missing
nguyen-etal-2008-challenges,P06-1006,0,\N,Missing
nguyen-etal-2008-challenges,P07-1107,0,\N,Missing
nguyen-etal-2008-challenges,J01-4004,0,\N,Missing
nguyen-etal-2008-challenges,P98-2143,0,\N,Missing
nguyen-etal-2008-challenges,C98-2138,0,\N,Missing
ohta-etal-2006-linguistic,I05-2038,1,\N,Missing
ohta-etal-2006-linguistic,W05-1308,0,\N,Missing
ohta-etal-2006-linguistic,W04-1207,0,\N,Missing
ohta-etal-2006-linguistic,W05-0304,0,\N,Missing
ohta-etal-2006-linguistic,W04-3111,0,\N,Missing
ohta-etal-2006-linguistic,tateisi-tsujii-2004-part,1,\N,Missing
ohta-etal-2006-linguistic,W04-1201,0,\N,Missing
P03-1038,W99-0615,1,0.899042,"Missing"
P03-1038,P94-1025,0,0.146263,"Missing"
P03-1038,W96-0213,0,\N,Missing
P03-1038,A00-1031,0,\N,Missing
P06-4005,P05-1011,1,0.777319,"Missing"
P06-4005,I05-1018,1,0.813084,"eather conditions forced them to scrub Monday’s scheduled return.” 3 MEDIE: a search engine for MEDLINE Figure 2 shows the top page of the MEDIE. MEDIE is an intelligent search engine for the accurate retrieval of relational concepts from MEDLINE 2 (Miyao et al., 2006). Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying Enju and a term recognizer. 3.1 Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (Tsuruoka et al., 2005; Hara et al., 2005), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees (Tateisi et al., 2005). The HPSG parser generates parse trees in a stand-off format that can be converted to XML by combining it with the original text. We also annotated technical terms of genes and diseases in our developed corpus. Technical terms are annotated simply by exact matching of dictio2 Functions of MEDIE 4 Info-PubMed: a GUI-based MEDLINE search tool Info-PubMed is a MEDLINE search tool with GUI, helping users to find information about biomedical entities such as genes"
P06-4005,W05-1511,1,0.867094,"Missing"
P06-4005,I05-2038,1,0.900999,"e predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MOD, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. The lexicon of the grammar was extracted from Sections 02-21 of Penn Treebank (39,832 sentences). In the table, ‘HPSG-PTB’ means that the statistical model was trained on Penn Treebank. ‘HPSG-GENIA’ means that the statistical model was trained on both Penn Treebank and GENIA treebank as described in (Hara et al., 2005). The GENIA treebank (Tateisi et al., 2005) consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Figure 1 shows a part of the parse tree and feaRecently, biomedical researchers have been facing the vast repository of research papers, e.g. MEDLINE. These researchers are eager to search biomedical correlations such as protein-protein or gene-disease associations. The use of natural language processing technology is expected to reduce their burden, and various attempts of information extraction using NLP has been being made (Blaschke and Valencia, 2002; Hao et al., 2005; Chun et al., 2006). However, the framework of traditi"
P06-4005,W04-3102,0,0.0153543,"of Informatics, Kogakuin University ¶ Information Technology Center, University of Tokyo † 1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 17 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17–20, c Sydney, July 2006. 2006 Association for Computational Linguistics nary entries and the terms separated by space, tab, period, comma, hat, colon, semi-colon, brackets, square brackets and slash in MEDLINE. The entire dictionary was generated by applying the automatic generation method of name variations (Tsuruoka and Tsujii, 2004) to the GENA dictionary for the gene names (Koike and Takagi, 2004) and the UMLS (Unified Medical Language System) meta-thesaurus for the disease names (Lindberg et al., 1993). It was generated by applying the name-variation generation method, and we obtained 4,467,855 entries of a gene and disease dictionary. 3.2 MEDIE provides three types of search, semantic search, keyword search, GCL search. GCL search provides us the most fundamental and powerful functions in which users can specify the boolean relations, linear order relation and structural relations with variables. Trained users can enjoy all functions in MEDIE by the GCL search, but it is not easy for"
P06-4005,P06-1128,1,\N,Missing
W03-1313,W03-2416,1,0.916478,"emas, XPointer, SAX, etc. The higher level standards, of meta-data (RDF) and ontologies (OWL) have been especially influential in encoding biomedical resources. However, there remains the question how to best encode the structure of the text themselves, how to mark-up added linguistic analyses, and how to implement linkages between the text and and further resources, such as lexica, thesauri and ontologies. As discussed in (Ide and Brew, 2000), in order to qualify as a “good” annotated corpus, its encoding should provide for reusabilty and extensibily. In this paper we build on previous work (Erjavec et al., 2003) and show how to develop a standardised encoding for biomedical corpora. We base our discussion on the case of the GENIA corpus (Ohta et al., 2002), which is originaly encoded in GPML, the GENIA Project Markup Language, an XML DTD. We re-encode the corpus into a standardised annotation scheme, based on the Text Encoding Initiative Guidelines P4 (Sperberg-McQueen and Burnard, 2002), and specify a constructive mapping from the original DTD to the developed encoding via a XSLT transformation. One of the motivations for such an re-encoding is that TEI is well-designed and widely accepted architect"
W03-1313,W02-1706,0,0.0309681,"r direct reference to the token stream of the text, so if this is incorrect, errors will propagate to all other annotations. It is also interesting to note that current annotation practice is more and more leaning toward standoff markup, i.e., annotations that are separated from the primary data (text) and make reference to it only via pointers. However, it is beneficial to have some markup in the primary data to which it is possible to refer, and this markup is, almost exclusivelly, that of tokens; see e.g., (Freese et al., 2003). Version V1.1 of GENIA has been also annotated with LTG tools (Grover et al., 2002). In short, the corpus is tokenised, and then part-of-speech tagged with two taggers, each one using a different tagset, and the nouns and verbs lemmatised. Additionally, the deverbal nominalisations are assigned their verbal stems. The conversion to TEI is also able to handle this additional markup, by using the TEI.analysis module. The word and punctuation tokens are encoded as hwi and hci elements respectively, which are further marked with type and lemma and the locally defined c1, c2 and vstem. An example of such markup is given in Figure 3. Given the high density of technical terms, biom"
W04-1213,W04-1219,0,0.627413,"ied by the eight participating systems; Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs). The most frequently applied In the example, “T- and B-lymphocyte” is annotated as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performa"
W04-1213,W04-1217,0,0.448851,"ls were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performance. One interpretation on this may be the CRF is often regarded as a kind of version-upped model of the MEMM (in the sense that both are conditional, exponential models) and thus is replacing MEMM. 5.2 purpose taggers (Lee et al., 2004). BeseNP tags and deep syntactic features were also exploited by several systems but the effectiveness was not clearly examined. The top-ranked two systems incorporated information from gaz"
W04-1213,W04-1221,0,0.551357,"ed as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performance. One interpretation on this may be the CRF is often regarded as a kind of version-upped model of the MEMM (in the sense that both are conditional, exponential models) and thus is replacing MEMM. 5.2 purpose taggers (Lee et al., 2004). BeseNP tags"
W04-1213,W04-1220,0,0.0495577,"rticipating systems; Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs). The most frequently applied In the example, “T- and B-lymphocyte” is annotated as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at this time also the MEMM system yields quite good performance. One interpreta"
W04-1213,W04-1218,0,0.0450635,"Missing"
W04-1213,W04-1215,0,0.0331929,"s Roughly four types of classification models were applied by the eight participating systems; Support Vector Machines (SVMs), Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs). The most frequently applied In the example, “T- and B-lymphocyte” is annotated as one structure but involves two entity names, “T-lymphocyte” and “B-lymphocyte”, whereas “lymphocytes” is annotated as one and involves as many entity names. 72 models were SVMs with totally five systems adopting SVMs as the classification models either in isolation (Park et al., 2004; Lee et al., 2004) or in combination with other models (Zhou and Su, 2004; Song et al., 2004; R¨ossler, 2004). HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R¨ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most successfully applied model in the CoNLL-2003 Shared Task of Named Entity Recognition, and at th"
W04-1213,W04-1214,0,\N,Missing
W04-1213,W04-1216,0,\N,Missing
W04-1213,W03-0419,0,\N,Missing
W09-1301,doddington-etal-2004-automatic,0,0.0136738,"e.g. database curation efforts, most domain RE efforts target relations involving biologically relevant changes in the involved entities, commonly to the complete exclusion of static relations. However, static relations such as entity membership in a family and one entity being a part of another are not only 1 relevant IE targets in themselves but can also play an important supporting role in IE systems not primarily targeting them. In this paper, we investigate the role of static relations in causal RE and event extraction. Here, we use relation extraction in the MUC and ACE (Sundheim, 1995; Doddington et al., 2004) sense to refer to the task of extracting binary relations, ordered pairs of entities, where both participating entities must be specified and their roles (agent, patient, etc.) are fixed by the relation. By contrast, event extraction is understood to involve events (things that happen) and representations where the number and roles of participants may vary more freely. We refer to relations where one one entity causes another to change as causal relations; typical domain examples are phosphorylation and activation. Static relations, by contrast, hold between two entities without implication o"
W09-1301,S07-1003,0,0.00870172,"Missing"
W09-1301,W09-1401,1,0.666271,"well studied and several biomedProceedings of the Workshop on BioNLP, pages 1–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ical NER systems are available (see e.g. (Wilbur et al., 2007; Leaman and Gonzalez, 2008)), and most domain IE approaches are NE-driven: a typical way to cast the RE task is as deciding for each pair of co-occurring NEs whether a relevant relation is stated for them in context. Like the previous LLL and BioCreative2-PPI relation extraction tasks (N´edellec, 2005; Krallinger et al., 2007), the BioNLP’09 shared task on event extraction (Kim et al., 2009) similarly proceeds from NEs, requiring participants to detect events and determine the roles given NEs play in them. Any domain IE approach targeting nontrivial causal NE relations or events necessarily involves decisions relating to static relations. Consider, for example, the decision whether to extract a relation between NE1 and NE2 in the following cases (affects should here be understood as a placeholder for any relevant statement of causal relation): 1) NE1 affects NE2 gene 2) NE1 affects NE2 promoter 3) NE1 affects NE2 mutant 4) NE1 affects NE2 antibody 5) NE1 affects NE2 activator The"
W09-1301,de-marneffe-etal-2006-generating,0,0.054107,"Missing"
W09-1301,W01-0511,0,0.0903008,"nt relations To avoid unnecessary division of relations that imply in our context similar interpretation and processing, we define a task-specific Variant relation that encompasses a set of possible relation types holding between an NE and its variants along multiple different axes. One significant class of cases annotated as Variant includes expressions such as NE gene and NE protein, under the interpretation that NE refers to the abstract information that is “realized” as either DNA, RNA or protein form, and the entity to one of these realizations (for alternative interpretations, see e.g. (Rosario and Hearst, 2001; Heimonen et al., 2008)). The Variant relation is also used to annotate NEentity relations where the entity expresses a different state of the NE, such as a phosphorylated or mutated state. While each possible post-translational modification, for example, could alternatively be assigned a specific relation type, in the present IE context these would only increase the difficulty of the task without increasing the applicability of the resulting annotation. the corpus contained frequent cases where the stated relationship of the NE to the entity involved different types of relevant relations (e."
W09-1301,M95-1002,0,0.226052,"biologists and e.g. database curation efforts, most domain RE efforts target relations involving biologically relevant changes in the involved entities, commonly to the complete exclusion of static relations. However, static relations such as entity membership in a family and one entity being a part of another are not only 1 relevant IE targets in themselves but can also play an important supporting role in IE systems not primarily targeting them. In this paper, we investigate the role of static relations in causal RE and event extraction. Here, we use relation extraction in the MUC and ACE (Sundheim, 1995; Doddington et al., 2004) sense to refer to the task of extracting binary relations, ordered pairs of entities, where both participating entities must be specified and their roles (agent, patient, etc.) are fixed by the relation. By contrast, event extraction is understood to involve events (things that happen) and representations where the number and roles of participants may vary more freely. We refer to relations where one one entity causes another to change as causal relations; typical domain examples are phosphorylation and activation. Static relations, by contrast, hold between two enti"
W09-1301,I05-2038,1,0.170621,"ation criteria, NE and entity types, granularity of relations, etc.), we find the outcome — which was neither planned for nor forced on the data — a very encouraging sign of the sufficiency of the task setting for this and related domain IE tasks. 3.4 We created the data set by building on the annotation of the GENIA Event corpus (Kim et al., 2008), making use of the rich set of annotations already contained in the corpus: term annotation for NEs and other entities (Ohta et al., 2002), annotation of events between these terms, and treebank structure closely following the Penn Treebank scheme (Tateisi et al., 2005). Other/Out annotation We apply a catch-all category, Other/Out, for annotating candidate (NE, entity) pairs between which there is no relevant static relation. This label is thus applied to a number of quite different cases: causal relations, both implied (e.g. NE receptors, NE response element) and explicitly stated (NE binds the [site]), relations where the entity is considered too far removed from the NE to support reliable inference of a role for the NE in causal relations/events involving the entity (e.g. [antibodies] for NE), and cases where no relation is stated (e.g. NE and other [pro"
W09-1321,P98-1013,0,0.0124696,"on (Linguis4cally‐oriented seman4cs) Class: Mo)on Theme: NF‐kappa B Source: from cytosol Goal: to nucleus Class: Releasing Theme: IL‐6 Agent: PBMC GENIA expression (Biologically‐oriented seman4cs) Class: Localiza)on Theme: NF‐kappa B FromLoc: cytosol ToLoc: nucleus Theme: IL‐6 FromLoc: (inside of) PMBC ToLoc: (outside of) PMBC Figure 1: A comparison of the linguistically-oriented and biologicallyoriented structure of semantics event corpus. Expressions mentioning the four classes were examined and manually classiﬁed into linguistically-oriented frames, represented by those deﬁned in FrameNet (Baker et al., 1998). FN frames associated to a bio-molecular event class constitute a list of possible perspectives in mentioning phenomena of the class. The rest of this paper is structured in the following way: Section 2 reviews the existing work on semantic structures and expression varieties in the bio-medical domain, and provides a comparison to our work. In section 3, we describe the GENIA event corpus, and the FrameNet frames used as linguistically-oriented classes in our investigation. Sections 4 and 5 explain the methods and results of the corpus investigation; in particular the sections investigate how"
W09-1321,S07-1018,0,0.0588238,"Missing"
W09-1321,J02-3001,0,0.00863432,"ioInfer (Pyysalo et al., 2007) and the GENIA event corpus (Kim et al., 2008) provide annotations of such semantic structures on col162 lections of bio-medical articles. Domain-oriented semantic structures are valuable assets because their representation suits information needs in the domain; however, the extraction of such structures is difﬁcult due to the large gap between the text and these structures. On the other hand, the extraction of linguisticallyoriented semantics from text has long been studied in computational linguistics, and has recently been formalized as Semantic Role Labeling (Gildea and Jurafsky, 2002), and semantic structure extraction (Baker et al., 2007)(Surdeanu et al., 2008). Semantic structures in such tasks are exempliﬁed in the middle of ﬁgure 1. The linguistically-oriented semantic structures are easier to extract, although the information is not practical to the domain. We aim at relating linguistically-oriented frames of semantics with domain-oriented classes, thus making a step forward in utilizing the computational linguistic resources for the bio-medical TM. Of all the differences in the two type of semantics, we focused on the fact that the former frames are more sensitive to"
W09-1321,W04-3111,0,0.0815074,"Missing"
W09-1321,W04-2705,0,0.0871914,"and conclusion in section 6 and 7. 2 Related Work Existing work on semantics approached domainoriented semantic structures from linguisticallyoriented semantics. In contrast, our approach uses domain-oriented semantics to ﬁnd the linguistic semantics that represent them. We believe that the two different approaches could complement each other. The PASbio(Wattarujeekrit et al., 2004) proposes Predicate Argument Structures (PASs), a type of linguistically-oriented semantic structures, for domain-speciﬁc lexical items, based on PASs de163 ﬁned in PropBank(Wattarujeekrit et al., 2004) and NomBank(Meyers et al., 2004). The PASs are deﬁned per lexical item, and is therefore distinct from a biologically-oriented representation of events. (Cohen et al., 2008) investigated syntactic alternations of verbs and their nominalized forms which occurred in the PennBioIE corpus(Kulick et al., 2004), whilst keeping PASs of the PASBio in their minds. The BioFrameNet(Dolbey et al., 2006) is an attempt to extend the FrameNet with speciﬁc frames to the bio-medical domain, and to apply the frames to corpus annotation. Our attempts were similar, in that both were: 1) utilizing the FN frames or their extensions to classify me"
W09-1321,W08-2121,0,0.0424515,"Missing"
W09-1321,C98-1013,0,\N,Missing
W09-1401,P05-1022,0,0.128444,"of event extraction, we prepared publicly available BioNLP resources readily available for the shared task. Several fundamental BioNLP tools were provided through U-Compare (Kano et al., 2009)2 , which included tools for tokenization, sentence segmentation, part-of-speech tagging, chunking and syntactic parsing. Participants were also provided with the syntactic analyses created by a selection of parsers. We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser3 , implementing Collins’ parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008)4 . We also applied the GDep5 , native dependency parser trained on the GENIA Treebank 2 http://u-compare.org/ http://www.cis.upenn.edu/∼dbikel/software.html 4 http://www.cs.brown.edu/∼dmcc/biomedical.html 5 http://www.cs.cmu.edu/∼sagae/parser/gdep/ 3 Team UTurku JULIELab Task 1-1-- Org 3C+2BI 1C+2L+2B ConcordU 1-3 3C Word Porter OpenNLP Porter Stanford UT+DBCLS 12- 2C Porter VIBGhent UTokyo 1-3 1-- 2C+1B 3C Porter, GTag UNSW UZurich 1-1-- 1C+1B 3C ASU+HU+BU 123 6C+2BI LingPipe, Morpha Porter Cam UAntwerp 1-12- 3C"
W09-1401,M98-1001,0,0.713763,"sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance cap"
W09-1401,de-marneffe-etal-2006-generating,0,0.604282,"Missing"
W09-1401,W04-1213,1,0.414702,"ted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance capable of supporting practical applications, the recent interest of the bio-TM community is shifting toward IE. Similarly to LLL and BioCreative, the BioNLP’09 Shared Task (the BioNLP task, hereafter) also addresses bio-IE, but takes a definitive step further toward f"
W09-1401,P08-2026,0,0.440867,"ailable for the shared task. Several fundamental BioNLP tools were provided through U-Compare (Kano et al., 2009)2 , which included tools for tokenization, sentence segmentation, part-of-speech tagging, chunking and syntactic parsing. Participants were also provided with the syntactic analyses created by a selection of parsers. We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser3 , implementing Collins’ parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008)4 . We also applied the GDep5 , native dependency parser trained on the GENIA Treebank 2 http://u-compare.org/ http://www.cis.upenn.edu/∼dbikel/software.html 4 http://www.cs.brown.edu/∼dmcc/biomedical.html 5 http://www.cs.cmu.edu/∼sagae/parser/gdep/ 3 Team UTurku JULIELab Task 1-1-- Org 3C+2BI 1C+2L+2B ConcordU 1-3 3C Word Porter OpenNLP Porter Stanford UT+DBCLS 12- 2C Porter VIBGhent UTokyo 1-3 1-- 2C+1B 3C Porter, GTag UNSW UZurich 1-1-- 1C+1B 3C ASU+HU+BU 123 6C+2BI LingPipe, Morpha Porter Cam UAntwerp 1-12- 3C 3C Porter GTag UNIMAN 1-- 4C+2BI Porter GTag SCAI UAveiro USzeged 1-1-1-3 1C 1C+"
W09-1401,W09-1313,1,0.461582,"differences in annotation principles compared to other biomedical NE corpora. For instance, the NE annotation in the widely applied GENETAG corpus (Tanabe et al., 2005) does not differentiate proteins from genes, while GENIA annotation does. Such differences have caused significant inconsistency in methods and resources following different annotation schemes. To remove or reduce the inconsistency, GENETAG-style NE annotation, which we term gene-or-gene-product (GGP) annotation, has been added to the GENIA corpus, with appropriate revision of the original annotation. For details, we refer to (Ohta et al., 2009). The NE annotation used in the BioNLP task data is based on this annotation. 3.2 Argument revision The GENIA event annotation was made based on the GENIA event ontology, which uses a loose typing system for the arguments of each event class. For example, in Figure 2(a), it is expressed that the binding event involves two proteins, TRAF2 and CD40, and that, in the case of CD40, its cytoplasmic domain takes part in the binding. Without constraints on the type of theme arguments, the following two annotations are both legitimate: (Type:Binding, Theme:TRAF2, Theme:CD40) (Type:Binding, Theme:TRAF2"
W09-1401,W09-1301,1,0.380624,"om (a) PMID7541987 (simplified), (b) PMID10224278, (c) PMID10090931, (d) PMID9243743, (e) PMID7635985. (Type:Binding, Theme1:TRAF2, Theme2:CD40, Site2:cytoplasmic domain) Note that the protein, CD40, and its domain, cytoplasmic domain, are associated by argument numbering. To resolve issues related to the mapping between proteins and related entities systematically, we introduced partial static relation annotation for relations such as Part-Whole, drawing in part on similar annotation of the BioInfer corpus (Pyysalo et al., 2007). For details of this part of the revision process, we refer to (Pyysalo et al., 2009). Figure 2 shows some challenging cases. In (b), the site GATA motifs is not identified as an argument of the binding event, because the protein containing it is not stated. In (c), among the two sites (PEBP2 site and promoter) of the gene GM-CSF, only the more specific one, PEBP2, is annotated. The equivalent entity annotation in the revised GENIA corpus covers also cases other than simple apposition, illustrated in Figure 3. A frequent case in biomedical literature involves use of the slash symbol (“/”) to state synonyms. The slash symbol is ambiguous as it is used also to indicate dimerized"
W09-1401,strassel-etal-2008-linguistic,0,0.0454823,"r event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance capable of supporting practical applications, the recent i"
W09-1401,I05-2038,1,0.385704,"bioinformaticians (BI), biologists (B) and liguists (L). This may be attributed in part to the fact that the event extraction task required complex computational modeling. The role of computer scientists may be emphasized in part due to the fact that the task was novel to most participants, requiring particular efforts in framework design and implementation and computational resources. This also suggests there is room for improvement from more input from biologists. In total, 42 teams showed interest in the shared task and registered for participation, and 24 teams sub7.2 Evaluation results (Tateisi et al., 2005), and a version of the C&C CCG deep parser6 adapted to biomedical text (Rimell and Clark, 2008). The text of all documents was segmented and tokenized using the GENIA Sentence Splitter and the GENIA Tagger, provided by U-Compare. The same segmentation was enforced for all parsers, which were run using default settings. Both the native output of each parser and a representation in the popular Stanford Dependency (SD) format (de Marneffe et al., 2006) were provided. The SD representation was created using the Stanford tools7 to convert from the PTB scheme, the custom conversion introduced by (Ri"
W09-1401,J04-4004,0,\N,Missing
W10-1903,W09-1403,0,0.198686,"Missing"
W10-1903,W09-1313,1,0.711856,"applied to statements that involve the occurrence of a change in the state of an entity – even if stated as having occurred in the past, or only hypothetically – but not in cases merely discussing the state or properties of entities, even if these can serve as the basis for inference that a specific change has occurred. We found that many of the spans an2.5 Annotation results The new PTM annotation covers 157 PubMed abstracts. Following the model of the BioNLP shared task, all mentions of specific gene or gene product names in the abstracts were annotated, applying the annotation criteria of (Ohta et al., 2009). This new named entity annotation covers 1031 gene/gene product mentions, thus averaging more than six mentions per annotated abstract. In total, 422 events of which 405 are of the novel PTM 23 Event type Glycosylation Hydroxylation Methylation Acetylation Positive reg. Phosphorylation Protein modification TOTAL Count 122 103 90 90 12 3 2 422 applies a pipeline architecture consisting of three supervised classification-based modules: a trigger detector, an event edge detector, and an event detector. In evaluation on the BioNLP shared task test data, the system extracted phosphorylation events"
W10-1903,W09-1401,1,0.848379,"and the specific modified site are expected to be of more practical interest. However, we note that the greater number of multi-argument events is expected to make the dataset more challenging as an extraction target. 3 Evaluation 3.2 To estimate the capacity of the newly annotated resource to support the extraction of the targeted PTM events and the performance of current event extraction methods at open-domain PTM extraction, we performed a set of experiments using an event extraction method competitive with the state of the art, as established in the BioNLP shared task on event extraction (Kim et al., 2009a; Bj¨orne et al., 2009). 3.1 Data Preparation The corpus data was split into training and test sets on the document level with a sampling strategy that aimed to preserve a roughly 3:1 ratio of occurrences of each event type between training and test data. The test data was held out during system development and parameter selection and only applied in a single final experiment. The event extraction system was trained using the 112 abstracts of the training set, further using 24 of the abstracts Methods 6 We note that in the BioNLP shared task data, all arguments were contained within single se"
W10-1903,P06-4005,1,\N,Missing
W10-1903,P06-1128,1,\N,Missing
W11-0221,W06-1615,0,0.0295068,"difficult for the parser. If two question sentences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the"
W11-0221,W05-1102,0,0.0259069,"d by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction from declarative sentences. A"
W11-0221,W07-2202,1,0.870942,"ower is very high. If NL queries can be automatically translated into SPARQL queries, human users can access their desired knowledge without learning the complex query language of SPARQL. This paper presents our preliminary work for NL query processing, with focus on syntactic parsing. We first build a small treebank of natural language queries, which are from Genomics track (Hersh et al., 2004; Hersh et al., 2005; Hersh et al., 2006; Hersh et al., 2007) topics (Section 2 and 3). The small treebank is then used to test the performance of a state-of-the-art parser, Enju (Ninomiya et al., 2007; Hara et al., 2007) (Section 4). The results show that a parser trained on Wall-StreetJournal (WSJ) articles and Medline abstracts will not work well on query sentences. Next, we experiment an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results enlighten directions for effective 3 http://www.biomoby.org/ http://esw.w3.org/HCLSIG BioRDF Subgroup 4 http://www.w3.org/TR/rdf-sparql-query/ http://www.w3.org/RDF/ 164 Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 1"
W11-0221,P06-1063,0,0.0664676,"Missing"
W11-0221,P03-1054,0,0.00401217,"+ entity type” (45 sentences), “which + entity type” (4 sentences), or “In what + entity type” (1 sentence). In contrast, the GENIA Treebank Corpus (Tateisi et al., 2005)5 is estimated to have no imperative sentences and only seven interrogative sentences (see Section 5.2.2). Thus, the sentence constructions in GTREC04–07 are very different from those in the GENIA treebank. 3 Treebanking GTREC query sentences We built a treebank (with POS) on 196 query sentences following the guidelines of the GENIA Treebank (Tateisi and Tsujii, 2006). The queries were first parsed using the Stanford Parser (Klein and Manning, 2003), and manual correction was made 5 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/ wiki.cgi?page=GENIA+Treebank SBARQ 4 Parsing system and extraction of imperative and question sentences SQ VP WHNP[i168] NP[i169→i168] WDT NNS VBP What toxicities are NP[→i169] VBN [] associated [] PP IN NN with cytarabine Figure 2: The tree structure for an interrogative sentence by the second author. We tried to follow the guideline of the GENIA Treebank as closely as possible, but for the constructions that are rare in GENIA, we used the ATIS corpus in Penn Treebank (Bies et al., 1995), which is also a colle"
W11-0221,H94-1020,0,0.0238544,"ysis of the 2004 track and other known biologist information needs. The derived templates were used as the commands to find articles describing biological interests such as methods or roles of genes. Although the templates were in the form “Find articles describing ...”, actual obtained imperatives begin with “Describe the procedure or method for” (12 sentences), “Provide information about” (36 sentences) or “Provide information on” (12 sentences). GTREC06 consists only of wh-questions where a wh-word constitutes a noun phrase by itself (i.e. its 165 part-of-speech is the WP in Penn Treebank (Marcus et al., 1994) POS tag set) or is an adverb (WRB). In the 2006 track, the templates for the 2005 track were reformulated into the constructions of questions and were then utilized for deriving the questions. For example, the templates to find articles describing the role of a gene involved in a given disease is reformulated into the question “What is the role of gene in disease?” GTREC07 consists only of wh-questions where a wh-word serves as a pre-nominal modifier (WDT). In the 2007 track, unlike in those of last two years, questions were not categorized by the templates, but were based on biologists’ info"
W11-0221,P06-1043,0,0.026801,"tences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction fro"
W11-0221,N10-1004,0,0.02207,"sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction from declarative sentences. Although sentence constru"
W11-0221,W07-2208,1,0.810083,"t, yet the expressive power is very high. If NL queries can be automatically translated into SPARQL queries, human users can access their desired knowledge without learning the complex query language of SPARQL. This paper presents our preliminary work for NL query processing, with focus on syntactic parsing. We first build a small treebank of natural language queries, which are from Genomics track (Hersh et al., 2004; Hersh et al., 2005; Hersh et al., 2006; Hersh et al., 2007) topics (Section 2 and 3). The small treebank is then used to test the performance of a state-of-the-art parser, Enju (Ninomiya et al., 2007; Hara et al., 2007) (Section 4). The results show that a parser trained on Wall-StreetJournal (WSJ) articles and Medline abstracts will not work well on query sentences. Next, we experiment an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results enlighten directions for effective 3 http://www.biomoby.org/ http://esw.w3.org/HCLSIG BioRDF Subgroup 4 http://www.w3.org/TR/rdf-sparql-query/ http://www.w3.org/RDF/ 164 Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, A"
W11-0221,D08-1050,0,0.0222677,"ce are known, the sentence has a very different construction from declarative sentences. Although sentence constructions have gained little attention, a notable exception is (Judge et al., 2006). They pointed out low accuracy of state-ofthe-art parsers on questions, and proposed super171 vised parser adaptation by manually creating a treebank of questions. The question sentences are annotated with phrase structure trees in the PTB scheme, although function tags and empty categories are omitted. An LFG parser trained on the treebank then achieved a significant improvement in parsing accuracy. (Rimell and Clark, 2008) also worked on question parsing. They collected question sentences from TREC 9-12, and annotated the sentences with POSs and CCG (Steedman, 2000) lexical categories. They reported a significant improvement in CCG parsing without phrase structure annotations. On the other hand, (Judge et al., 2006) also implied that just increasing the training data would not be enough. We went further from their work, built a small but complete treebank for NL queries, and explored what really occurred in HPSG parsing. 7 Conclusion In this paper, we explored the problem in parsing queries. We first attempted"
W11-0221,N03-1027,0,0.0338607,"m conjunctions seems to be difficult for the parser. If two question sentences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a se"
W11-0221,E03-1008,0,0.0380273,"er. If two question sentences were concatenated by conjunctions into one sentence, the parser would tend to fail to analyze the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very dif"
W11-0221,I05-2038,1,0.816043,"involved in a given disease is reformulated into the question “What is the role of gene in disease?” GTREC07 consists only of wh-questions where a wh-word serves as a pre-nominal modifier (WDT). In the 2007 track, unlike in those of last two years, questions were not categorized by the templates, but were based on biologists’ information needs where the answers were lists of named entities of a given type. The obtained questions begin with “what + entity type” (45 sentences), “which + entity type” (4 sentences), or “In what + entity type” (1 sentence). In contrast, the GENIA Treebank Corpus (Tateisi et al., 2005)5 is estimated to have no imperative sentences and only seven interrogative sentences (see Section 5.2.2). Thus, the sentence constructions in GTREC04–07 are very different from those in the GENIA treebank. 3 Treebanking GTREC query sentences We built a treebank (with POS) on 196 query sentences following the guidelines of the GENIA Treebank (Tateisi and Tsujii, 2006). The queries were first parsed using the Stanford Parser (Klein and Manning, 2003), and manual correction was made 5 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/ wiki.cgi?page=GENIA+Treebank SBARQ 4 Parsing system and extract"
W11-0221,W06-2902,0,0.0135891,"e the sentence construction for the latter sentence. The remaining errors in Table 6 would imply that we should also re-consider the model designs or the framework itself for the parser in addition to just increasing the training data. 6 Related work Since domain adaptation has been an extensive research area in parsing research (Nivre et al., 2007), a lot of ideas have been proposed, including un/semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). Their main focus was on adapting parsing models trained with a specific genre of text (in most cases PTB-WSJ) to other genres of text, such as biomedical research papers. A major problem tackled in such a task setting is the handling of unknown words and domain-specific ways of expressions. However, as we explored, parsing NL queries involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different construction from declarative sentences. Although sentence constructions have gained little attention, a notable except"
W11-0221,D07-1096,0,\N,Missing
W11-1801,S10-1006,0,0.034309,"Missing"
W11-1801,W11-1802,1,0.673169,"ined bio-IE in these directions is emphasized as the main theme of the second event. This paper summarizes the entire BioNLP-ST 2011, covering the relationships between tasks and similar broad issues. Each task is presented in detail in separate overview papers and extraction systems in papers by participants. 1 Proceedings of BioNLP Shared Task 2011 Workshop, pages 1–6, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics 2 Main tasks BioNLP-ST 2011 includes four main tracks (with five tasks) representing fine-grained bio-IE. 2.1 Genia task (GE) The GE task (Kim et al., 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al., 2008). The data represents a focused domain of molecular biology: transcription factors in human blood cells. The purpose of the GE task is two-fold: to measure the progress of the community since the last event, and to evaluate generalization of the technology to full papers. For the second purpose, the provided data is composed of two collections: the abstract collection, identical to the BioNLP-ST 2009 data, and the new full paper collection. Progress on the task is measured through the unchang"
W11-1801,W10-1905,1,0.365086,"t.Bossy@jouy.inra.fr Ngan Nguyen Jun’ichi Tsujii University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series repres"
W11-1801,W11-1811,1,0.812584,"Missing"
W11-1801,W10-1903,1,0.814538,"Missing"
W11-1801,W11-1803,1,0.724562,"Missing"
W11-1801,N10-1123,0,0.139216,"Ngan Nguyen Jun’ichi Tsujii University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move to"
W11-1801,W09-1301,1,0.390547,"sk (Pyysalo et al., 2011b) involves the recognition of two binary part-of relations between entities: P ROTEIN -C OMPONENT and S UBUNITC OMPLEX. The task is motivated by specific challenges: the identification of the components of proteins in text is relevant e.g. to the recognition of Site arguments (cf. GE, EPI and ID tasks), and relations between proteins and their complexes relevant to any task involving them. REL setup is informed by recent semantic relation tasks (Hendrickx et al., 2010). The task data, consisting of new annotations for GE data, extends a previously introduced resource (Pyysalo et al., 2009; Ohta et al., 2010a). Supporting tasks 3.3 Gene renaming task (REN) BioNLP-ST 2011 includes three supporting tasks designed to assist in primary the extraction tasks. Other supporting resources made available to participants are presented in (Stenetorp et al., 2011). 3.1 Protein coreference task (CO) The CO task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections betwe"
W11-1801,W10-1919,1,0.755144,"diseases from full-text publica2 tions. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to P ROTEIN four new entity types, including C HEMICAL and O RGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (Pyysalo et al., 2010; Ananiadou et al., 2011), including the support of systems such as PATRIC (http://patricbrc.org). 2.4 Bacteria track The bacteria track consists of two tasks, BB and BI. 2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al., 2011) is to extract the habitats of bacteria mentioned in textbooklevel texts written for non-experts. The texts are Web pages about the state of the art knowledge about bacterial species. BB targets general relations, Localization and PartOf , and is challenging in that texts contain more coreferences than usual, habitat references are not necessarily nam"
W11-1801,W11-1804,1,0.768915,"Missing"
W11-1801,W11-1812,1,0.906991,"elevant to task topics, including major protein modification types and their reverse reactions. For capturing the ways in which different entities participate in these events, the task extends the GE argument roles with two new roles specific to the domain, Sidechain and Contextgene. The task design and setup are oriented toward the needs of pathway extraction and curation for domain databases (Wu et al., 2003; Ongenaert et al., 2008) and are informed by previous studies on extraction of the target events (Ohta et al., 2010b; Ohta et al., 2010c). 2.3 Infectious diseases task (ID) The ID task (Pyysalo et al., 2011a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publica2 tions. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to P ROTEIN four new entity types, including C HEMICAL and O RGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domai"
W11-1801,W11-1816,1,0.850837,"Missing"
W11-1801,I05-2038,1,0.684261,"O task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections between event triggers and protein references is a major part of event extraction, it becomes much harder if one is replaced with a coreferencing expression. The CO task seeks to address this problem. The data sets for the task were produced based on MedCO annotation (Su et al., 2008) and other Genia resources (Tateisi et al., 2005; Kim et al., 2008). 3 The REN task (Jourde et al., 2011) objective is to extract renaming pairs of Bacillus subtilis gene/protein names from PubMed abstracts, motivated by discrepancies between nomenclature databases that interfere with search and complicate normalization. REN relations partially overlap several concepts: explicit renaming mentions, synonymy, and renaming deduced from biological proof. While the task is related to synonymy relation extraction (Yu and Agichtein, 2003), it has a novel definition of renaming, one name permanently replacing the other. 4 Schedule Table 2 shows the"
W11-1801,W10-1901,0,0.042682,"University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move toward fine-grain"
W11-1801,W09-1401,1,\N,Missing
W11-1801,W11-1809,1,\N,Missing
W11-1801,W11-1810,1,\N,Missing
W11-1802,W09-1402,0,0.269227,"Missing"
W11-1802,W11-1828,0,0.541098,"Missing"
W11-1802,W11-1820,0,0.231727,"Missing"
W11-1802,P05-1022,0,0.00957377,"particular efforts in framework design and implementation and computational resources. The ’09 column suggests that previous experience in the task may have affected to the performance of the teams, especially in a complex task like the GE task. Table 5 shows the proﬁle of the systems. A notable observation is that four teams developed their systems based on the model of UTurku09 (Bj¨orne et al., 2009) which was the winning sys10 tem of BioNLP-ST 2009. It may show an inﬂuence of the BioNLP-ST series in the task. For syntactic analyses, the prevailing use of Charniak Johnson re-ranking parser (Charniak and Johnson, 2005) using the self-trained biomedical model from McClosky (2008) (McCCJ) which is converted to Stanford Dependency (de Marneffe et al., 2006) is notable, which may also be an inﬂuence from the results of BioNLP-ST 2009. The last two teams, XABioNLP and HCMUS, who did not use syntactic analyses could not get a performance comparable to the others, which may suggest the importance of using syntactic analyses for a complex IE task like GE task. 5 5.1 Results Task 1 Table 6 shows the ﬁnal evaluation results of Task 1. For reference, the reported performance of the two systems, UTurku09 and Miwa10 is"
W11-1802,P08-2026,0,0.0148642,"Missing"
W11-1802,W11-1806,0,0.327907,"Missing"
W11-1802,W11-1822,0,0.147744,"Missing"
W11-1802,W10-1905,0,0.0118461,"h may also be an inﬂuence from the results of BioNLP-ST 2009. The last two teams, XABioNLP and HCMUS, who did not use syntactic analyses could not get a performance comparable to the others, which may suggest the importance of using syntactic analyses for a complex IE task like GE task. 5 5.1 Results Task 1 Table 6 shows the ﬁnal evaluation results of Task 1. For reference, the reported performance of the two systems, UTurku09 and Miwa10 is listed in the top. UTurku09 was the winning system of Task 1 in 2009 (Bj¨orne et al., 2009), and Miwa10 was the best system reported after BioNLP-ST 2009 (Miwa et al., 2010b). Particularly, the latter made Team FAUST UMASS UTurku MSR-NLP ConcordU UWMadison Stanford BMI@ASU CCP-BTMG TM-SCS XABioNLP HCMUS ’09 √ √ √ √ √ √ √ Task 1212123 1-1-3 1-1-121-1-1-1-- People 3C 1C 1BI 4C 2C 2C 3C+1.5L 3C 3BI 1C 4C 6L reference (Riedel et al., 2011) (Riedel and McCallum, 2011) (Bjrne and Salakoski, 2011) (Quirk et al., 2011) (Kilicoglu and Bergler, 2011) (Vlachos and Craven, 2011) (McClosky et al., 2011) (Emadzadeh et al., 2011) (Liu et al., 2011) (Bui and Sloot, 2011) (Casillas et al., 2011) (Minh et al., 2011) Table 4: Team proﬁles: The ’09 column indicates whether at least"
W11-1802,W11-1808,0,0.46262,"Missing"
W11-1802,W11-1805,0,0.244116,"Missing"
W11-1802,W11-1807,0,\N,Missing
W11-1802,W11-1825,0,\N,Missing
W11-1802,de-marneffe-etal-2006-generating,0,\N,Missing
W11-1802,N10-1123,0,\N,Missing
W11-1802,W09-1418,0,\N,Missing
W11-1802,W09-1401,1,\N,Missing
W11-1802,W09-1406,1,\N,Missing
W11-1802,W11-1827,0,\N,Missing
W11-1802,W10-1901,0,\N,Missing
W11-1802,W11-1819,0,\N,Missing
W11-1802,W11-1824,0,\N,Missing
W11-1802,W11-1826,0,\N,Missing
W11-1802,W11-1801,1,\N,Missing
W11-1811,W97-1306,0,0.0342516,"cted to an anaphora through more than one surface link, we call it an indirect protein antecedent, and the antecedents in the middle of the chain intermediate antecedents. The performance evaluated in this mode may be directly connected to the potential performance in main IE tasks: the more the (anaphoric) protein references are found, the more the protein-related events may be found. For this reason, the protein coreference mode is chosen as the primary evaluation mode. Evaluation results for both evaluation modes are given in traditional precision, recall and f-score, which are similar to (Baldwin, 1997). Team UU Member 1 NLP 5.1 UZ CU UT US UC 5 NLP 2 NLP 1 biochemist 2 AI 3 NLP, 1 BioNLP Surface coreference A response expression is matched with a gold expression following partial match criterion. In particular, a response expression is considered correct when it covers the minimal boundary, and is included in the maximal boundary of expression. Maximal boundary is the span of expression annotation, and minimal boundary is the head of expression, as defined in MUC annotation schemes (Chinchor, 1998). A response link is correct when its two argument expressions are correctly matched with thos"
W11-1811,P10-1143,0,0.0685994,"Missing"
W11-1811,M98-1001,0,0.172105,"as standard tasks of information extraction (IE), coreference resolution (Ng, 2010; Bejan and Harabagiu, 2010) is more and more recognized as an important component of IE for a higher performance. Without coreference resolution, the performance of IE is often substantially limited due to an abundance of coreference structures in natural language text, i.e. information pieces written in text with involvement of a coreference structure are hard to be captured (Miwa et al., 2010). There have been several attempts for coreference resolution, particularly for newswire texts (Strassel et al., 2008; Chinchor, 1998). It is also one of the lessons from BioNLP Shared Task (BioNLPST, hereafter) 2009 that coreference structures in biomedical text substantially hinder the progress of fine-grained IE (Kim et al., 2009). To address the problem of coreference resolution in molecular biology literature, the Protein Coreference (COREF) task is arranged in BioNLP-ST 2011 as a supporting task. While the task itself is not an IE task, it is expected to be a useful component in performing the main IE tasks more effectively. To establish a stable evaluation and to observe the effect of the results of the task to the ma"
W11-1811,O04-1011,0,0.276438,"ch more non-anaphoric definite noun phrases than anaphoric ones, which making it difficult to train an effective classier for anaphoricity determination. We have to seek for a better method for solving the DNP links, in order to significantly improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submission (Table 12). Sub type both 1 it 1 such 2 their 1 thes"
W11-1811,P10-1142,0,0.0656707,"Missing"
W11-1811,C08-1079,1,0.852972,"ve to seek for a better method for solving the DNP links, in order to significantly improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submission (Table 12). Sub type both 1 it 1 such 2 their 1 these 2 this 2 whose 1 Type PRON PRON DNP PRON DNP DNP RELAT Count 2 17 2 27 26 20 1 Sub type both 2 its 1 that 1 them 1 they 1 those 1 whose 2 Type PRON PRON RELAT"
W11-1811,D10-1048,0,0.0577386,"his type. A possi80 ble reason of this is because there are much more non-anaphoric definite noun phrases than anaphoric ones, which making it difficult to train an effective classier for anaphoricity determination. We have to seek for a better method for solving the DNP links, in order to significantly improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submis"
W11-1811,P10-2029,0,0.0631713,"improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submission (Table 12). Sub type both 1 it 1 such 2 their 1 these 2 this 2 whose 1 Type PRON PRON DNP PRON DNP DNP RELAT Count 2 17 2 27 26 20 1 Sub type both 2 its 1 that 1 them 1 they 1 those 1 whose 2 Type PRON PRON RELAT PRON PRON PRON RELAT Count 4 61 37 1 5 9 0 Sub type either 1 one 2 the 2 these 1 t"
W11-1811,strassel-etal-2008-linguistic,0,0.0555665,"Missing"
W11-1811,I05-2038,1,0.7862,"considered in the secondary evaluation mode. See section 5 for more detail. 4 Type RELAT PRON Anaphora DNP APPOS N/C Antecedent TOTAL Train 1193 738 296 9 11 2116 4363 Dev 254 149 58 1 1 451 914 Test 349 269 91 3 2 674 1388 Table 1: Statistics of coreference entities in COREF data sets: N/C = not-classified. DNP indicates definite NPs or demonstrative NPs, e.g. NPs that begin with the, this, etc. Data Preparation The data sets for the COREF task are produced based on three resources: MedCO coreference annotation (Su et al., 2008), Genia event annotation (Kim et al., 2008), and Genia Treebank (Tateisi et al., 2005). Although the three have been developed independently from each other, they are annotations made to the same corpus, the Genia corpus (Kim et al., 2008). Since COREF was focused on finding anaphoric references to proteins (or genes), only relevant annotations were extracted from the MedCO corpus though the following process: 1. From MedCo annotation, coreference entities that were pronouns and definite base NPs were extracted, which became candidate anaphoric expressions. The base NPs were determined by consulting Genia Tree Bank. 2. Among the candidate anaphoric expressions, those that could"
W11-1811,W09-1401,1,\N,Missing
W11-1816,I05-2038,1,\N,Missing
W11-1816,W11-1825,0,\N,Missing
W11-1816,de-marneffe-etal-2006-generating,0,\N,Missing
W11-1816,W11-1802,1,\N,Missing
W11-1816,W11-1803,1,\N,Missing
W11-1816,J93-2004,0,\N,Missing
W11-1816,W11-1812,1,\N,Missing
W11-1816,D10-1096,0,\N,Missing
W11-1816,N10-1123,0,\N,Missing
W11-1816,J08-1002,1,\N,Missing
W11-1816,J04-4004,0,\N,Missing
W11-1816,W06-2920,0,\N,Missing
W11-1816,W09-1401,1,\N,Missing
W11-1816,P96-1025,0,\N,Missing
W11-1816,D07-1111,1,\N,Missing
W11-1816,P05-1022,0,\N,Missing
W11-1816,P06-1055,0,\N,Missing
W11-1816,P08-1006,1,\N,Missing
W11-1816,W10-3006,0,\N,Missing
W11-1816,P04-1014,0,\N,Missing
W11-1816,W07-2416,0,\N,Missing
W11-1816,C10-1088,1,\N,Missing
W11-1816,W11-1809,0,\N,Missing
W11-1816,W11-1824,0,\N,Missing
W11-1816,W11-1810,0,\N,Missing
W11-1816,W11-1801,1,\N,Missing
W11-1816,W11-1804,1,\N,Missing
W12-2412,W11-1828,0,0.0929443,"Missing"
W12-2412,W11-1820,0,0.0684248,"Missing"
W12-2412,P11-1098,0,0.0165262,"c span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise fol"
W12-2412,doddington-etal-2004-automatic,0,0.282956,"IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the requirement to match trigger spans 2. only require entity texts, not spans, to match The first alternative criterion has also been previously consider"
W12-2412,W11-1824,0,0.0264993,"Missing"
W12-2412,P10-1160,0,0.0661515,"Missing"
W12-2412,W11-1827,0,0.568696,"Missing"
W12-2412,W11-1801,1,0.941981,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1822,0,0.0679042,"Missing"
W12-2412,W11-1826,0,0.0323279,"Missing"
W12-2412,P11-1163,0,0.0329128,"l only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation and each event extracted by a system is considered in"
W12-2412,P09-1113,0,0.0205423,"tly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and event annotation is associated with a specific span of text. Contrasted to the alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating"
W12-2412,W11-1803,1,0.935394,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,D07-1075,0,0.0343243,"IE systems to identify a specific span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode"
W12-2412,W11-1804,1,0.922475,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1812,1,0.828716,"ferring to the real-world entities in text, the overall task is “text-bound” in the sense of requiring not only the extraction of targeted statements from text, but also the identification of specific regions of text expressing each piece of extracted information. Events can further be marked with modifiers identifying additional features such as being explicitly negated or stated in a speculative context. Figure 1 shows an illustration of event annotations. This BioNLP ST 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the GE (Kim et al., 2011c), ID (Pyysalo et al., 2011a) and EPI (Ohta et al., 2011) tasks. A variant of this representation that omits event triggers was applied in the BioNLP ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3"
W12-2412,W11-1825,0,0.02855,"P ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3.1 Following the BioNLP ST 2011, the MSR-NLP group called for the release of outputs from various participating systems (Quirk et al., 2011) and made analyses of their system available.2 Despite the obvious benefits of the availability of these resources, we are not aware of other groups following this example prior to the time of this publication. To create the combined resource, we approached each group that participated in the three targeted BioNLP ST 2011 main tasks to ask for their support to the creation of a dataset including analyses from their event extraction systems. This suggestion met with the support of all but a few groups that were approached.3 The groups providing analyses from their systems into this merged resou"
W12-2412,D11-1001,0,0.0233845,"alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation"
W12-2412,W11-1808,0,0.31655,"Missing"
W12-2412,W11-1816,1,0.901463,"Missing"
W12-2412,H91-1059,0,0.261268,"on systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the re"
W12-2412,W10-1921,1,0.866603,"Missing"
W12-2412,W11-1821,0,0.0439833,"Missing"
W12-2412,W11-0204,0,0.0629482,"Missing"
W12-2412,W11-1805,0,0.0284448,"Missing"
W12-2412,W11-1802,1,\N,Missing
W12-2412,W11-1809,0,\N,Missing
W12-2412,W11-1811,1,\N,Missing
W12-2412,W11-1810,0,\N,Missing
W12-2425,J08-1002,0,0.0384469,"Missing"
W12-2430,W06-3328,0,0.0795968,"ning samples to bootstrap, and that an improvement can be obtained only with a well chosen text collection. 1 Introduction While machine learning-based approaches are becoming more and more popular for the development of natural language processing (NLP) systems, corpora with annotation are regarded as a critical resource for the training process. Nonetheless, the creation of corpus annotation is an expensive and timeconsuming work (Cohen et al., 2005), and it is often the case that lack of sufcient annotation hinders the development of NLP systems. Bootstrapping method (Becker et al., 2005; Vlachos and Gasperin, 2006) can be considered as a way to automatically inate the amount of corpus annotation to complement the lack of sufcient annotation. In this study, we report the experimental results on the effect of bootstrapping for the training of protein name recognizers, particularly in the situation when we have only a small amount of corpus annotations. In summary, we begin with a small corpus with manual annotation for protein names. A named entity tagger trained on the small corpus is applied to a big collection of text, to obtain more annotation. We hope the newly created annotation to be precise enou"
W12-2430,W05-1306,0,\N,Missing
W12-3622,W10-0735,0,0.024608,"nnotation project, annotators, e.g., domain experts, need to be recruited, trained, then deployed for actual annotation. After the annotation project is over, usually they are dismissed. The same cycle then needs to be repeated for a new annotation project. In this setup, the recruitment and training of annotators actually take non-trivial cost. Recently, crowdsourcing, e.g., Amazon Mechanical Turk (MTurk, hereafter), is gaining a big attention as a source of finding intelligent human labor. For corpus annotation also, the usability of MTurk has been explored (Callison-Burch and Dredze, 2010; Buzek et al., 2010; Little et al., 2009). There are also other efforts to achieve a large-scale annotation based on community-wide efforts (Ide et al., 2010), which shows current trends toward sys153 tematic incorporation of contributions from a community rather than from a small group. In this work, we propose a community-sourcing annotation framework (CSAF, hereafter) which defines the components and protocol of a computer system to enable community-sourcing annotation. It is similar to MTurk to some extent in its concept, but it is more specifically designed for corpus annotation tasks, particularly for thos"
W12-3622,W10-0701,0,0.0150324,"rious corpus annotation. For an annotation project, annotators, e.g., domain experts, need to be recruited, trained, then deployed for actual annotation. After the annotation project is over, usually they are dismissed. The same cycle then needs to be repeated for a new annotation project. In this setup, the recruitment and training of annotators actually take non-trivial cost. Recently, crowdsourcing, e.g., Amazon Mechanical Turk (MTurk, hereafter), is gaining a big attention as a source of finding intelligent human labor. For corpus annotation also, the usability of MTurk has been explored (Callison-Burch and Dredze, 2010; Buzek et al., 2010; Little et al., 2009). There are also other efforts to achieve a large-scale annotation based on community-wide efforts (Ide et al., 2010), which shows current trends toward sys153 tematic incorporation of contributions from a community rather than from a small group. In this work, we propose a community-sourcing annotation framework (CSAF, hereafter) which defines the components and protocol of a computer system to enable community-sourcing annotation. It is similar to MTurk to some extent in its concept, but it is more specifically designed for corpus annotation tasks, p"
W12-3622,P10-2013,0,0.0274826,"n project is over, usually they are dismissed. The same cycle then needs to be repeated for a new annotation project. In this setup, the recruitment and training of annotators actually take non-trivial cost. Recently, crowdsourcing, e.g., Amazon Mechanical Turk (MTurk, hereafter), is gaining a big attention as a source of finding intelligent human labor. For corpus annotation also, the usability of MTurk has been explored (Callison-Burch and Dredze, 2010; Buzek et al., 2010; Little et al., 2009). There are also other efforts to achieve a large-scale annotation based on community-wide efforts (Ide et al., 2010), which shows current trends toward sys153 tematic incorporation of contributions from a community rather than from a small group. In this work, we propose a community-sourcing annotation framework (CSAF, hereafter) which defines the components and protocol of a computer system to enable community-sourcing annotation. It is similar to MTurk to some extent in its concept, but it is more specifically designed for corpus annotation tasks, particularly for those which require special expertise from annotators, e.g., domain knowledge. With “community”, it means a group of people who are regarded as"
W13-2001,W11-1802,1,0.929598,"tand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST’13 web site. 2.1 Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al., 2008) that represents domain knowledge of NFκB proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al., 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al., 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more “real” task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the coreference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the eve"
W13-2001,P05-1022,0,0.0190745,"Date 4 Participation GE 1-2-3 EVEX BioNLP-ST’13 organization BioNLP-ST’13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were TEES-2.1 • • BioSEM • NCBI • DlutNLP • HDS 4NLP • NICTA • USheff • UZH • HCMUS • • • • • CG PC GRO GRN BB 1 - 2-3 • • NaCTeM • • NCBI • RelAgent • UET-NII • ISI • OSEE U. of Ljubljana K.U. Leuven IRISATexMex Boun • • • • • • • • • • • • • • LIPN • LIMSI • • • Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTur"
W13-2001,W11-0214,1,0.829223,"t Error Rate (Makhoul et al., 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures. Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al., 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al., 2011b; Ohta et al., 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities. 2.4 Gene Regulation Network in Bacteria (GRN) 2.6 Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats wi"
W13-2001,W12-4304,1,0.844492,"Missing"
W13-2001,W09-1401,1,0.905104,"Missing"
W13-2001,W11-1801,1,0.712795,"Missing"
W13-2001,J08-1002,0,\N,Missing
W13-2001,W11-0215,1,\N,Missing
W13-2001,W11-1810,1,\N,Missing
W13-2002,W11-1802,1,0.940553,"verview Jin-Dong Kim and Yue Wang and Yamamoto Yasunori Database Center for Life Science (DBCLS) Research Organization of Information and Systems (ROIS) {jdkim|wang|yy}@dbcls.rois.ac.jp Abstract al., 2008b) which represented domain knowledge around NFκB proteins. There were also some efforts to explore the possibility of literature mining for pathway construction (Kim et al., 2008a; Oda et al., 2008). The GE task was designed to make such an effort a community-driven one by sharing available resources, e.g., benchmark data sets, and evaluation tools, with the community. In its second edition (Kim et al., 2011b) organized in BioNLP-ST 2011 (Kim et al., 2011a), the data sets were extended to include full text articles. The data sets consisted of two collections. The abstract collection, that had come from the first edition, was used again to measure the progress of the community between 2009 and 2011 editions, and the full text collection, that was newly created, was used to measure the generalization of the technology to full text papers. In its third edition this year, while succeeding the fundamental characteristics from its previous editions, the GE task tries to evolve with the goal to make it"
W13-2002,P11-1163,0,0.0279571,"fine-grained information written in literature. BioNLP Shared Task (ST) series (Kim et al., 2009; Kim et al., 2011a) is one of the community-wide efforts to address the problem. Since its initial organization in 2009, BioNLP-ST series has published a number of finegrained information extraction (IE) tasks motivated for bioinformatics projects. Having solicited wide participation from the community of natural language processing, machine learning, and bioinformatics, it has contributed to the production of rich resources for fine-grained BioIE, e.g., TEES1 (Bj¨orne and Salakoski, 2011), SBEP2 (McClosky et al., 2011) and EVEX3 (Van Landeghem et al., 2011). The Genia Event Extraction (GE) task is a seminal task of BioNLP-ST. It was first organized as the sole task of the initial 2009 edition of BioNLPST. The task was originally designed and implemented based on the Genia event corpus (Kim et 1 https://github.com/jbjorne/TEES/wiki http://nlp.stanford.edu/software/eventparser.shtml 3 http://www.evexdb.org/ 4 However, if necessary, the online evaluation for the previous editions of GE task may be used, which is available at http://bionlp-st.dbcls.jp/GE/. 2 8 Proceedings of the BioNLP Shared Task 2013 Workshop"
W13-2002,W11-1803,0,0.0689312,"and E/P = Event/Protein). comparable to previous editions. It is the consequence of a design choice of the organizers with the notion that (1) relevant resources are substantially accumulated through last two editions, and that (2) therefore the importance of training data set may be reduced while the importance of development and test data sets needs to be kept. Instead, participants may utilize, for example, the abstract collection of the 2011 edition, of which the annotation was produced by the same annotators with almost same principles. As another example, the data sets of the EPI task (Ohta et al., 2011) also may be utilized for the newly added protein modification events. Table 3 shows the statistics of annotated event types in different sections of the full papers in the data sets. For the analysis, the sections are classified to five groups as follows: Figure 2: Event distribution in different sections to newer ones. Note that among 34 papers, 14 were from the full text collection of 2011 edition data sets, and 20 were newly collected this time. The annotation to the all 34 papers were produced by the same annotators who also produced annotations for the previous editions of GE task. • The"
W13-2002,W11-0204,0,0.190352,"Missing"
W13-2002,W09-1401,1,\N,Missing
W13-2002,W11-1828,0,\N,Missing
W13-2002,W11-1801,1,\N,Missing
W13-5202,D12-1035,0,0.13967,"y et al., 2012) and, most germaine to this work, converting natural language questions into SPARQL queries. Although a body of work on SPARQL query generation from natural language questions has been growing, no consensus has yet developed about how to evaluate such systems. (Abacha and Zweigenbaum, 2012) evaluated their system by manual inspection of the SPARQL queries that Jin-Dong Kim Database Center for Life Science they generated. No gold standard was prepared— the authors examined each query and determined whether or not it accurately represented the original natural language question. (Yahya et al., 2012) used two human judges to manually examine the output of their system at three points— disambiguation, SPARQL query construction, and the answers returned. If the judges disagreed, a third judge examined the output. (McCarthy et al., 2012) does not have a formal evaluation, but rather gives two examples of the output of the SPARQL Assist system. (This is not a system for query generation from natural language questions per se, but rather an application for assisting in query constructions through methods like autocompletion suggestions.) (Unger et al., 2012) is evaluated on the basis of a gold"
W16-3003,W13-2003,0,0.184846,"Missing"
W16-3003,W13-3518,0,0.0232378,"ce, bionlp-st-ge-2016-uniprot, bionlp-st-ge-2016-coref,pmc-enju-pas, • bionlp-st-ge-2016-uniprot: UniProt ID annotation bionlp-spacy-parsed,GO-BP • bionlp-st-ge-2016-coref: coreference annotation 2.3 KB • pmc-enju-pas: deep dependency parsing by Enju (Miyao and Tsujii, 2008) By the KB, we mean a SPARQL endpoint populated with RDF statements which are results of conversion from the GE task results. To achieve the goal of establishing a seamless connection from the IE to KB, an automatic process is designed and implemented into PubAnnotation for: • bionlp-spacy-parsed: dependency parsing spacy (Honnibal et al., 2013) • UBERON-AE: anatomical entities in UBERON (Mungall et al., 2012) • ICD10: disease names as defined in ICD10 • conversion of annotations to RDF statements, and 2 Except for the coreference annotation, which is originally produced manually. • feeding the statements into a SPARQL endpoint. 25 Also, a SPARQL-driven user interface to search the KB is designed and implemented. SPARQL queries to search the KB simple. For example, following query instructs the system to search for spans (?s) that denote an object (?o) which is a uniprot:q04206. PREFIX tao:<http://pubannotation.org/ontology/tao.owl#>"
W16-3003,W12-2425,1,0.852247,"ralized resources for shared task organization with which the cost of organizing shared tasks ∗ Corresponding author, jdkim@dbcls.rois.ac.jp 23 Proceedings of the 4th BioNLP Shared Task Workshop, pages 23–31, c Berlin, Germany, August 13, 2016. 2016 Association for Computational Linguistics 2 Design both projects. With this feature, any corpus with manual annotation can potentially serve as a shared task: any one can attempt to automatically reproduce the manual annotation, and evaluate the accuracy. 2.1 Platform To achieve the first goal of generalizing the shared task system, PubAnnotation (Kim and Wang, 2012) was chosen as the platform. There were several reasons for the choice. Firstly, as a public repository of literature annotation, PubAnnotation provides various ways of submitting and accessing annotation data sets, which are fundamental for shared task organization. Secondly, it features an automatic text alignment function, which provides a reliable solution for aligning annotations collected from different groups. Thirdly, it is a near mature system, which has a growing user base with more than hundred of data sets. While PubAnnotation provides many useful functions, a shared task organizat"
W16-3003,J08-1002,0,0.0355122,"sk. The annotation data sets can be retrieved individually or altogether through the RESTful API. For example, by accessing the following URL, the annotations shown in Figure 9 can be obtained in JSON at once: http://pubannotation.org/docs/ sourcedb/PMC/sourceid/3245220/divs/ 11/spans/4375-4513/annotations.json? projects=bionlp-st-ge-2016-reference, bionlp-st-ge-2016-uniprot, bionlp-st-ge-2016-coref,pmc-enju-pas, • bionlp-st-ge-2016-uniprot: UniProt ID annotation bionlp-spacy-parsed,GO-BP • bionlp-st-ge-2016-coref: coreference annotation 2.3 KB • pmc-enju-pas: deep dependency parsing by Enju (Miyao and Tsujii, 2008) By the KB, we mean a SPARQL endpoint populated with RDF statements which are results of conversion from the GE task results. To achieve the goal of establishing a seamless connection from the IE to KB, an automatic process is designed and implemented into PubAnnotation for: • bionlp-spacy-parsed: dependency parsing spacy (Honnibal et al., 2013) • UBERON-AE: anatomical entities in UBERON (Mungall et al., 2012) • ICD10: disease names as defined in ICD10 • conversion of annotations to RDF statements, and 2 Except for the coreference annotation, which is originally produced manually. • feeding th"
W16-3003,W11-1802,1,0.885516,"tform for shared information extraction (IE) tasks, and for an IEdriven knowledge base (KB) system. On the newly implemented shared task platform, the GE task is run as an experimental task. The task and the platform has been tested by two teams who cooperated with the organizers. The paper presents the new shared task system and discusses on the experimental submissions. 1 Introduction Since its first introduction in 2009 as the task of the first BioNLP Shared Task (BioNLP-ST) organization, the Genia event extraction (GE) task has been one of the most investigated IE tasks (Kim et al., 2009; Kim et al., 2011; Kim et al., 2013). The biggest contribution of BioNLP-ST might be that it introduced fine-grained and highly structured information extraction (IE) tasks to the community of biomedical information extraction (BioIE), when the research in the community was weighted toward extracting binary relations (Krallinger et al., 2007; Lu et al., 2004; Chun et al., 2006). Since then the tasks of BioNLP-ST have motivated and nourished the community to develop a number of biomedical event extraction systems (Bj¨orne and Salakoski, 2013; Miwa et al., 2010), Originally designed as tasks based on intrinsic e"
W16-3003,W09-1401,1,\N,Missing
W16-3003,W13-2002,1,\N,Missing
W16-4712,W16-2925,1,0.542968,"ar words with same meaning like, for example, irregular nouns (e.g., phalanx vs phalanges, or femur vs femora) might be grouped into different clusters. Therefore, this paper presents results of experiments designed to evaluate a dictionary that tries to address the lexical variability of phenotype terms. Extending dictionaries with new terms has improved performance of, for example, gene phenotype recognizers (Funk et al., 2016). To help improve the performance (focusing on recall) of automatic phenotype CR process, we previously generated a dictionary of lexical variants for all HPO tokens (Kocbek and Groza, 2016), and here we present results of using this dictionary to annotate a gold corpus capturing text spans from 228 abstracts. The latter were manually annotated with Human Phenotype Ontology (HPO) concepts and harmonized by three curators (Groza et al., 2015). We expect that adding lexical variants will improve the recall of the annotation process, however, we also try to measure the effect of parameter tuning on the precision of the system. 2 Methods We used the dictionary of lexical variant clusters for all concepts and their synonyms in the HPO. Each HPO term and synonym was then extended with"
W19-4021,L18-1726,0,0.0588349,"Missing"
W19-4021,W12-2425,1,0.81516,"A CAS (Götz and Suhre, 2004) data model. This section briefly introduces the three platforms comprising our ecosystem (Figure 1) . Each represents a particular class of systems: a repository for annotated corpora, an NLP services platform, and an interactive annotation platform. These are introduced as platforms and not as tools as they are designed as open and extensible software systems. All are open source software and users can set up their own installations, e.g. for their own project, lab, or community. Some also run a canonical instance accessible to any registered user. PubAnnotation (Kim and Wang, 2012) takes on the role of the annotation repository in our ecosystem. It links all contributed annotations through references to canonical texts. It also supports annotation development coupled with PubDictionaries, a similarly open repository of dictionaries (term lexicons, etc.) to which users can add by registering their own dictionaries or modifying those already in the repository; as well as TextAE, a browser-based visualizer/editor for text annotation. The service-oriented architecture makes it easy for end-users to customize annotation tools by engaging in the annotation process from start"
W19-4021,C18-2002,1,0.789242,"Missing"
W19-4021,P13-4020,0,0.0296041,"ation to better automate knowledge extraction and identify relevant information in the literature has become an increasingly major activity over the past decade. Numerous platforms and frameworks that support text annotation have been developed, including the General Architecture for Text Engineering (GATE (Cunningham et al., 2013)), CLARIN WebLicht (Hinrichs et al., 2010), the Language Applications (LAPPS) Grid (Ide et al., 2014), OpenMinTeD (Labropoulou et al., 2018), and several systems based on the Unstructured Information Management Architecture (UIMA (Ferrucci et al., 2009)), e.g. ARGO (Rak et al., 2013), Apache cTAKES (Savova et al., 2010), DKPro Core (Eckart de Castilho and Gurevych, 2014). However, due to factors such as the often highly domain-specific vocabularies in specialized areas of science, these frameworks are rarely usable outof-the-box. As a result, scholars interested in mining publications may spend considerable effort to adapt existing annotation tools and resources to their particular domains of research (e.g., tune 189 Proceedings of the 13th Linguistic Annotation Workshop, pages 189–194 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Pla"
W99-0615,A88-1019,0,0.284497,"Missing"
W99-0615,J88-1003,0,0.105902,"Missing"
Y09-2040,W04-1217,0,0.0124046,"Include the example in the section of related guidelines. Create a link to the annotation in the annotation layer from the guideline. Figure 5 shows an example of feature vector generation. When the cursor stops at the word “Noreen” in the text “to comment on Mr. Noreen’s claims,”Pemberton said. ,” the system produce the feature vector from the target word, “Noreen” and its surrounding words. We considered the preceding and following three words. Three features are extracted from each word: word form, word shape, and part-of-speech (POS). These features work well for named entity recognition (Finkel et al., 2004). For word shape, we considered 6 types: aa (all lower-case letters), AA (all capital letters), aA (mixed, begins with a lower-case letter), Aa (mixed, begins with a capical letter), Num (all numerical letters) and Num a (mixed with numerical and alphabet letters). 6 Evaluation The purpose of this work is to provide an integrated framework for corpus annotation and guideline production, and to improve the accessibility between an annotated corpus and guidelines. In this section, we present two evaluations. The first evaluation is for the advantage of the proposed framework in the actual annota"
Y09-2040,W04-3111,0,0.0245017,"ented annotation guidelines are indispensable tools for the proper use of Penn Treebank. The annotaiton policy of the SUSANNE corpus (Sampson, 2002) is published as a part of a book. Despite the importance of annotation guidelines, the process of guideline production has not been studied extensively. Even some of the latest annotation projects relied on traditional ways of communication and documentation for guideline production. For example the Caderige project (Alphonse et al., 2004) used e-mails for communication between annotators, and the archive became database of guidelines. PennBioIE (Kulick et al., 2004) repeatdly updated a web page dedicated to documentation of guidelines. GENIA made use of a Wiki system.2 Although adopting web-based documentation enhanced the guideline production and utilization in sharing and searching, it is difficult to conclude that the guideline production process is well integrated with the annotation process. On the other hand, we produce the guideline using the examples from annotated corpus, and we annotate the corpus using the annotation guideline. The annotator must very often switch between the annotation system and guideline management system during annotation"
Y09-2040,J93-2004,0,0.0315193,"ple, MMAX is designed for multi-level annotation. Knowtator puts its focus on ontology-based annotation. GATE is a language engineering infrastructure. Both WordFreak and XConc Suite focus on flexibility of the format of corpus and annotation. As far as the authors know, however, there is no tool supporting guideline production in an integrated way. 2.2 Annotation guidelines and their production Although only few studies on guideline production exist, researchers have long recognized the importance of documenting the annotation policy. One of the most popular annotated corpora, Penn Treebank (Marcus et al., 1993), is also well known for the comprehensive documentation of its annotation policy. Its well documented annotation guidelines are indispensable tools for the proper use of Penn Treebank. The annotaiton policy of the SUSANNE corpus (Sampson, 2002) is published as a part of a book. Despite the importance of annotation guidelines, the process of guideline production has not been studied extensively. Even some of the latest annotation projects relied on traditional ways of communication and documentation for guideline production. For example the Caderige project (Alphonse et al., 2004) used e-mails"
Y09-2040,N03-4009,0,0.0339052,"ropose a framework in which the association between the guidelines and the corpus is important, and support the accessibility between the guideline and the corpus. In addition, we can systemically integrate the management of the annotation guideline into the annotation process in the proposed framework. We present GuideLink, an implementation of the annotation framework, which is integrated with the existing annotation tool, XConc Suite.1 2 Related works 2.1 Tools for corpus annotation Many software tools have been developed for supporting corpus annotation. Well-known ones include WordFreak (Morton and LaCivita, 2003), MMAX (Mueller and Strube, 2001), Knowtator (Ogren, 2006), GATE (Cunningham et al., 2002) and XConc Suite. While they are all widely used, each has its own strength. For example, MMAX is designed for multi-level annotation. Knowtator puts its focus on ontology-based annotation. GATE is a language engineering infrastructure. Both WordFreak and XConc Suite focus on flexibility of the format of corpus and annotation. As far as the authors know, however, there is no tool supporting guideline production in an integrated way. 2.2 Annotation guidelines and their production Although only few studies"
Y09-2040,W04-1207,0,\N,Missing
