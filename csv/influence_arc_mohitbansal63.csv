2020.acl-main.233,P11-1020,0,0.0143246,"ai et al., 2019). In particular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the gene"
2020.acl-main.233,P19-1285,0,0.11736,"et al. (2019) further augmented the above LSTM caption generator with a set of three discriminators that score generated sentences based on defined metrics, i.e., relevance, linguistic diversity, and inter-sentence coherence. Though different, both these methods use LSTMs as the language decoder. Recently, transformers (Vaswani et al., 2017) have proven to be more effective than RNNs (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014), etc.), demonstrating superior performance in many sequential modeling tasks (Vaswani et al., 2017; Zhou et al., 2018; Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019). Zhou et al. (2018) first introduced the transformer model to the video paragraph captioning task, with a transformer captioning module decoding natural language sentences from encoded video segment representations. This transformer captioning model is essentially the same as the original transformer (Vaswani et al., 2017) for machine translation, except that it takes a video representation rather than a source sentence representation as its encoder input. However, in such design, each video segment caption is decoded individually without knowing the context (i.e., previou"
2020.acl-main.233,W14-3348,0,0.0163482,"r than 100 for video and 20 for text and set the maximum number of video segments to 6 for ActivityNet Captions and 12 for YouCookII. Finally, we build vocabularies based on words that occur at least 5 times for ActivityNet Captions and 3 times for YouCookII. The resulting vocabulary contains 3,544 words for ActivityNet Captions and 992 words for YouCookII. Evaluation Metrics (Automatic and Human) We evaluate the captioning performance at paragraph-level, following (Park et al., 2019; Xiong et al., 2018), reporting numbers on standard metrics, including BLEU@4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), CIDErD (Vedantam et al., 2015). Since these metrics mainly focus on whether the generated paragraph matches the ground-truth paragraph, they fail to evaluate the redundancy of these multi-sentence paragraphs. Thus, we follow previous works (Park et al., 2019; Xiong et al., 2018) to evaluate repetition using R@4. It measures the degree of N-gram (N=4) repetition in the descriptions. Besides the automated metrics, we also conduct human evaluations to provide additional comparisons between the methods. We consider two aspects in human evaluation, relevance (i.e., how related is a generated para"
2020.acl-main.233,N19-1423,0,0.511527,"coding process. Park et al. (2019) further augmented the above LSTM caption generator with a set of three discriminators that score generated sentences based on defined metrics, i.e., relevance, linguistic diversity, and inter-sentence coherence. Though different, both these methods use LSTMs as the language decoder. Recently, transformers (Vaswani et al., 2017) have proven to be more effective than RNNs (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014), etc.), demonstrating superior performance in many sequential modeling tasks (Vaswani et al., 2017; Zhou et al., 2018; Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019). Zhou et al. (2018) first introduced the transformer model to the video paragraph captioning task, with a transformer captioning module decoding natural language sentences from encoded video segment representations. This transformer captioning model is essentially the same as the original transformer (Vaswani et al., 2017) for machine translation, except that it takes a video representation rather than a source sentence representation as its encoder input. However, in such design, each video segment caption is decoded individually without knowing the cont"
2020.acl-main.233,D18-1117,0,0.0616968,"visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descriptions. Though individual sentences may precisely describe the corresponding event segments, when put together the sentences of"
2020.acl-main.233,P02-1040,0,0.107056,"6b). We truncate sequences longer than 100 for video and 20 for text and set the maximum number of video segments to 6 for ActivityNet Captions and 12 for YouCookII. Finally, we build vocabularies based on words that occur at least 5 times for ActivityNet Captions and 3 times for YouCookII. The resulting vocabulary contains 3,544 words for ActivityNet Captions and 992 words for YouCookII. Evaluation Metrics (Automatic and Human) We evaluate the captioning performance at paragraph-level, following (Park et al., 2019; Xiong et al., 2018), reporting numbers on standard metrics, including BLEU@4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), CIDErD (Vedantam et al., 2015). Since these metrics mainly focus on whether the generated paragraph matches the ground-truth paragraph, they fail to evaluate the redundancy of these multi-sentence paragraphs. Thus, we follow previous works (Park et al., 2019; Xiong et al., 2018) to evaluate repetition using R@4. It measures the degree of N-gram (N=4) repetition in the descriptions. Besides the automated metrics, we also conduct human evaluations to provide additional comparisons between the methods. We consider two aspects in human evaluation, relevance (i"
2020.acl-main.233,P17-1117,1,0.916773,"articular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descr"
2020.acl-main.233,D17-1103,1,0.902834,"articular, MART can generate more coherent (e.g., coreference and order), less redundant paragraphs without losing paragraph accuracy (visual relevance). 2 Related Work Video Captioning Recently, video captioning has attracted much attention from both the computer vision and the natural language processing community. Methods for the task share the same intrinsic nature of taking a video as the input and outputting a language description that can best describe the content, though they differ from each other on whether a single sentence (Wang et al., 2019; Xu et al., 2016; Chen and Dolan, 2011; Pasunuru and Bansal, 2017a) or multiple sentences (Rohrbach et al., 2014; Krishna et al., 2017; Xiong et al., 2018; Zhou et al., 2018; Gella et al., 2018; Park et al., 2019) are generated for the given video. In this paper, our goal falls into the category of generating a paragraph (multiple sentences) conditioned on an input video with several pre-defined event segments. One line of work (Zhou et al., 2018, 2019) addresses the video paragraph captioning task by decoding each video event segment separately into a sentence. The final paragraph description is obtained by concatenating the generated single sentence descr"
2020.acl-main.233,D19-1514,1,0.865631,"is used as the basis of our approach. Different from RNNs (e.g., LSTM (Hochreiter and Schmidhuber, 1997), GRU (Chung et al., 2014), etc) that use recurrent structure to model long-term dependencies, transformer relies on self-attention to learn the dependencies between input words. Transformers have proven to be more efficient and powerful than RNNs, with superior performance in many sequential modeling tasks, including machine translation (Vaswani et al., 2017), language modeling/pre-training (Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019) and multi-modal representation learning (Tan and Bansal, 2019; Chen et al., 2019; Sun et al., 2019). Additionally, Zhou et al. (2018) have shown that a transformer model can generate better captions than the LSTM model. However, transformer architectures are still unable to model history information well. This problem is identified in the task of language modeling as context fragmentation (Dai et al., 2019), i.e., each language segment is modeled individually without knowing its surrounding context, leading to inefficient optimization and inferior performance. To resolve this issue, Transformer-XL (Dai et al., 2019) introduces the idea of recurrence to"
2020.acl-main.435,D16-1044,0,0.102072,"Missing"
2020.acl-main.435,D18-1167,1,0.777955,"ble https://github.com/hyounghk/ VideoQADenseCapFrameGate-ACL2020 at: is related to watching and listening to videos that are shared in huge amounts via the internet and new high-speed networks. Videos convey a diverse breadth of rich information, such as dynamic spatiotemporal relationships between people/objects, as well as events. Hence, it has become important to develop automated models that can accurately extract such precise multimodal information from videos (Tapaswi et al., 2016; Maharaj et al., 2017; Kim et al., 2017; Jang et al., 2017; Gao et al., 2017; Anne Hendricks et al., 2017; Lei et al., 2018, 2020). Video question answering is a representative AI task through which we can evaluate such abilities of an AI agent to understand, retrieve, and return desired information from given video clips. In this paper, we propose a model that effectively integrates multimodal information and locates the relevant frames from diverse, complex video clips such as those from the video+dialogue TVQA dataset (Lei et al., 2018), which contains questions that need both the video and the subtitles to answer. When given a video clip and a natural language question based on the video, naturally, the first"
2020.acl-main.435,2020.acl-main.730,1,0.91614,"al., 2017; Melas-Kyriazi et al., 2018) have been introduced to densely and broadly capture the diverse aspects and salient regions of an image. Especially, dense caption describes an image in object level and gives useful salient regional information about objects such as attributes and actions. In this paper, we take advantage of this dense caption’s ability to help our what is cathy doing with her hand after she introduces her fiance to ted ?an she is image doing sign language . video QA model understand better for answering questions. st0 st1 3.1 stj Features We follow the same approach of Lei et al. (2020)’s work to obtain features from video, questionanswer pairs, and subtitle input and encode them. We sample frames at 0.5 fps and extract object B C each D .... frame via Faster R-CNN (GirfeaturesAfrom E shick,F 2015). Then we use PCA to get features of G 300 dimension from top-20 object proposals. We . also create five hypotheses by concatenating a ques. tion feature with each of five answer features, and we pair each visual frame feature with temporally neighboring subtitles. We encode all the features using convolutional encoder.  x00 = Epos (x)      xit = fi,t (xit−1 ) + xit−1 , φen ("
2020.acl-main.435,P19-1351,1,0.795909,"Missing"
2020.acl-main.435,2021.ccl-1.108,0,0.201843,"Missing"
2020.acl-main.435,D18-1084,0,0.0309419,"context and multimodal data. svk ... svT sd0 sd1 ... sdl ... sdT Softmax ... ... Softmax VID ... Softmax Dense Image Captioning Image captioning is another direction of understanding visual and language information jointly. Single-sentence capSoftmax tions (Karpathy and Fei-Fei, 2015; Anderson et al., 2018) capture the main concept of an image to describe it in a single sentence. However, an image could contain multiple aspects that are important/useful in different ways. Dense captions (Johnson et al., 2016; Yang et al., 2017) and paragraph captions (Krause et al., 2017; Liang et al., 2017; Melas-Kyriazi et al., 2018) have been introduced to densely and broadly capture the diverse aspects and salient regions of an image. Especially, dense caption describes an image in object level and gives useful salient regional information about objects such as attributes and actions. In this paper, we take advantage of this dense caption’s ability to help our what is cathy doing with her hand after she introduces her fiance to ted ?an she is image doing sign language . video QA model understand better for answering questions. st0 st1 3.1 stj Features We follow the same approach of Lei et al. (2020)’s work to obtain fea"
2020.acl-main.435,D14-1162,0,0.0826836,"Missing"
2020.acl-main.435,P19-1348,0,0.336359,"Missing"
2020.acl-main.441,D15-1075,0,0.487871,"chmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-theart models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. 1 Introduction Progress in AI has been driven by, among other things, the development of challenging large-scale benchmarks like ImageNet (Russakovsky et al., 2015) in computer vision, and SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), and others in natural language processing (NLP). Recently, for natural language understanding (NLU) in particular, the focus has shifted to combined benchmarks like SentEval (Conneau and Kiela, 2018) and GLUE (Wang et al., 2018), which track model performance on multiple tasks and provide a unified platform for analysis. With the rapid pace of advancement in AI, however, NLU benchmarks struggle to keep up with model improvement. Whereas it took around 15 years to achieve “near-human performance” on MNIST (LeCun et al., 1998; Cires¸an et al., 2012; Wan et al.,"
2020.acl-main.441,P17-1171,1,0.856888,"Missing"
2020.acl-main.441,W19-2008,0,0.0318535,"Missing"
2020.acl-main.441,L18-1269,1,0.839755,"es. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. 1 Introduction Progress in AI has been driven by, among other things, the development of challenging large-scale benchmarks like ImageNet (Russakovsky et al., 2015) in computer vision, and SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), and others in natural language processing (NLP). Recently, for natural language understanding (NLU) in particular, the focus has shifted to combined benchmarks like SentEval (Conneau and Kiela, 2018) and GLUE (Wang et al., 2018), which track model performance on multiple tasks and provide a unified platform for analysis. With the rapid pace of advancement in AI, however, NLU benchmarks struggle to keep up with model improvement. Whereas it took around 15 years to achieve “near-human performance” on MNIST (LeCun et al., 1998; Cires¸an et al., 2012; Wan et al., 2013) and approximately 7 years to surpass humans on ImageNet (Deng et al., 2009; Russakovsky et al., 2015; He et al., 2016), the GLUE benchmark did not last as long as we would have hoped after the advent of BERT (Devlin et al., 201"
2020.acl-main.441,D17-1070,1,0.886955,"Missing"
2020.acl-main.441,D19-1461,1,0.933183,"fy examples and make splits Verifier Agree Step 4: Retrain model for next round Figure 1: Adversarial NLI data collection via human-and-model-in-the-loop enabled training (HAMLET). The four steps make up one round of data collection. In step 3, model-correct examples are included in the training set; development and test sets are constructed solely from model-wrong verified-correct examples. forts that gamify collaborative training of machine learning agents over multiple rounds (Yang et al., 2017) and pit “builders” against “breakers” to learn better models (Ettinger et al., 2017). Recently, Dinan et al. (2019) showed that such an approach can be used to make dialogue safety classifiers more robust. Here, we focus on natural language inference (NLI), arguably the most canonical task in NLU. We collected three rounds of data, and call our new dataset Adversarial NLI (ANLI). Our contributions are as follows: 1) We introduce a novel human-and-model-in-the-loop dataset, consisting of three rounds that progressively increase in difficulty and complexity, that includes annotator-provided explanations. 2) We show that training models on this new dataset leads to state-of-the-art performance on a variety of"
2020.acl-main.441,W17-5401,0,0.115146,"Missing"
2020.acl-main.441,N18-1074,0,0.0577718,"Missing"
2020.acl-main.491,D18-1128,0,0.454434,"on: given an input and an “explanation,” users must predict what a model would output for the given input. The second is counterfactual simulation: users are given an input, a model’s output for that input, and an “explanation” of that output, and then they must predict what the model will output when given a perturbation of the original input. The explanation itself is algorithmically generated by a method for interpreting or explaining a model. Simulation tests have been carried out before, but no study to date has isolated the effect of explanations on simulatability (Ribeiro et al., 2018; Chandrasekaran et al., 2018; Nguyen, 2018; Bang et al., 2019). We carry out simulation tests that are the first to incorporate all of the following design choices: (1) separating explained instances from test instances, so explanations do not give away the answers, (2) evaluating the effect of explanations against a baseline of unexplained examples, (3) balancing data by model correctness, so users cannot succeed by guessing the true label, and (4) forcing user predictions on all inputs, so performance is not biased toward overly specific explanations. We display our study design in Figure 1. We provide results from hig"
2020.acl-main.491,N16-1082,0,0.0545162,"hods may improve the interpretability of a model, in the sense that an interpretable model is simulatable. 2.2 Explanation Methods Several taxonomies have been proposed for categorizing methods for interpretability. We organize methods below into the categories of: feature importance estimation, case-based reasoning, and latent space traversal. Feature Importance Estimation. Feature importance estimates provide information about how the model uses certain features. Most prominent among these methods are the gradient-based approaches first introduced for vision by Simonyan et al. (2014), which Li et al. (2016) show may 5541 be translated for use with text data. These approaches have since been demonstrated to sometimes behave in counterintuitive ways (Adebayo et al., 2018; Kim et al., 2018). A number of alternative methods have been proposed for quantifying feature importance across data domains (Kim et al., 2018; Lundberg and Lee, 2017; Sundararajan et al., 2017). In our study, we choose to evaluate two domain-agnostic approaches, LIME and Anchor (Ribeiro et al., 2016, 2018). These methods use simple models, i.e. sparse linear models and rule lists, to approximate complex model behavior locally ar"
2020.acl-main.491,N18-1097,0,0.523414,"planation,” users must predict what a model would output for the given input. The second is counterfactual simulation: users are given an input, a model’s output for that input, and an “explanation” of that output, and then they must predict what the model will output when given a perturbation of the original input. The explanation itself is algorithmically generated by a method for interpreting or explaining a model. Simulation tests have been carried out before, but no study to date has isolated the effect of explanations on simulatability (Ribeiro et al., 2018; Chandrasekaran et al., 2018; Nguyen, 2018; Bang et al., 2019). We carry out simulation tests that are the first to incorporate all of the following design choices: (1) separating explained instances from test instances, so explanations do not give away the answers, (2) evaluating the effect of explanations against a baseline of unexplained examples, (3) balancing data by model correctness, so users cannot succeed by guessing the true label, and (4) forcing user predictions on all inputs, so performance is not biased toward overly specific explanations. We display our study design in Figure 1. We provide results from high-quality huma"
2020.acl-main.491,W02-1011,0,0.0257439,"Missing"
2020.acl-main.491,N16-1174,0,\N,Missing
2020.acl-main.491,N16-3020,0,\N,Missing
2020.acl-main.730,D16-1241,1,0.818827,"predict QA relevant temporal spans, then combines the global and local (span localized) video information to answer the questions. In the following, we describe STAGE in detail. 4.1 Formulation In our tasks, the inputs are: (1) a question with 5 candidate answers; (2) a 60-second long video; (3) a set of subtitle sentences. Our goal is to predict the answer and ground it both spatially and temporally. Given the question, q, and the answers, {ak }5k=1 , we first formulate them as 5 hypotheses (QA-pair) hk = [q, ak ] and predict their correctness scores based on the video and subtitle context (Onishi et al., 2016). We denote the ground-truth (GT) answer index as y ans and thus the GT hypothesis as hyans . We then extract video frames {vt }Tt=1 at 0.5 FPS (T is the number of frames for each video). Subtitle sentences are then temporally aligned with the video frames. Specifically, for each frame vt , we pair it with two neighboring sentences based on the subtitle timestamps. We choose two neighbors since this keeps most of the sentences at our current frame rate, and also avoids severe misalignment between the frames and the sentences. The set of aligned subtitle sentences are denoted as {st }Tt=1 . We"
2020.acl-main.730,D14-1162,0,0.0828042,"Missing"
2020.acl-main.730,D18-1167,1,0.906716,"sed questions. Such methods are useful in many scenarios, such as natural language guided spatiotemporal localization, and adding explainability to video question answering, which is potentially useful for decision making and model debugging. To enable this line of research, we also collect new joint spatio-temporal annotations for an existing video QA dataset. In the past few years, several video QA datasets have been proposed, e.g., MovieFIB (Maharaj et al., 2017), MovieQA (Tapaswi et al., 2016), TGIF-QA (Jang et al., 2017), PororoQA (Kim et al., 2017), MarioQA (Mun et al., 2017), and TVQA (Lei et al., 2018). TVQA is one of the largest video QA datasets, providing a large video QA dataset built on top of 6 famous TV series. Be8211 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211–8225 c July 5 - 10, 2020. 2020 Association for Computational Linguistics cause TVQA was collected on television shows, it is built on natural video content with rich dynamics and complex social interactions, where questionanswer pairs are written by people observing both videos and their accompanying dialogues, encouraging the questions to require both vision and language"
2020.acl-main.730,D19-1514,1,0.828122,"Missing"
2020.acl-main.773,S19-1028,0,0.0373782,"Missing"
2020.acl-main.773,P19-1084,0,0.0744063,"Missing"
2020.acl-main.773,D19-6115,0,0.192235,"2019a,b) showed that adversarial removal methods can be effective for the hypothesisonly NLI bias. Our focus is on two different lexical biases and our results are complementary to theirs.2 Recently, Wang et al. (2019a) proposed HEX projection to force the orthogonality between the target model and a superficial model to improve domain generalization for image classification tasks. Here, to make the model less lexically biased, we apply the HEX projection with specially-designed NLP model architectures to regularize the representation in our models. Even more recently, Clark et al. (2019) and He et al. (2019) propose to robustify the task model with the help of an additional simple model, using ensembling to encourage cooperation of the two models. On the other hand, our main motivation to compare the advantages/limitations of dataset vs. embedding vs. classifier debiasing methods (against two different types of problematic lexical biases in NLI), and also our classifier debiasing method forces the task model to capture orthogonal information via HEX projection. Removing Gender Bias in NLP Models. There 2 We have tried a similar approach via gradient reversal w.r.t. BoW sub-model in preliminary ex"
2020.acl-main.773,D17-1215,0,0.0551327,"rd distance (Hamers et al., 1989) of words). Next, since the number of these unbiased samples may not be large enough, we repeatedly add those selected samples to make the training set more balanced. The results from adding 500 new samples to 50,000 new samples are shown in Sec. 6.1. 8761 Data Augmentation by Adding Synthetic Data. Researchers have been using synthetic rules to generate harder or perturbed samples to fool the model. Here, besides using these datasets only as the evaluation set, we also add these samples back to the training set, similar to the concept of adversarial training (Jia and Liang, 2017; Wang et al., 2019c; Niu and Bansal, 2018) where the adversarial examples are added back to the training set so that the resulting model will be more robust to similar adversarial attacks. In our experiments, we follow Naik et al. (2018) to append meaningless sentences at the end of the hypothesis sentence like in Table 1 to create additional new samples. The detailed construction of these samples can be seen in Appendix. By learning from these augmented datasets, the model should also be more robust to certain types of perturbations/biases of the data. In Sec. 6.1, our experiments showed tha"
2020.acl-main.773,P18-2005,0,0.0255496,". Naik et al. (2018) constructed bias-revealing datasets by modifying the development set of MNLI. In our evaluation, besides using the datasets from Naik et al. (2018), we also extract new datasets from the original MNLI dataset to maintain the consistency of input text distribution. Adversarial Removal Methods. Adversarial removal techniques are used to control the content of representations. They were first used to do unsupervised domain adaptation in Ganin and Lempitsky (2015). Xie et al. (2017) later generalized this approach to control specific information learned by the representation. Li et al. (2018) used a similar approach to learn privacy-preserving representations. However, Elazar and Goldberg (2018) showed that such adversarial approach fails to completely remove demographic information. Minervini and Riedel (2018) generate adversarial examples and regularize models based on first-order logic rules. Belinkov et al. (2019a,b) showed that adversarial removal methods can be effective for the hypothesisonly NLI bias. Our focus is on two different lexical biases and our results are complementary to theirs.2 Recently, Wang et al. (2019a) proposed HEX projection to force the orthogonality be"
2020.acl-main.773,S14-2001,0,0.0283128,"rmance and perform almost equally well on these harder parts of the data. Hence, we focus on maintaining the accuracy on the whole dataset and improving the Acc hr metric. All training details and hyperparameter settings are presented in Appendix. 6 Results 6.1 Data-Level Debiasing Results We first show our baseline’s performance on the CWB biases in the first row of Table 2. Since we ob6 One may wonder if biases can also be evaluated simply using generalization performance. However, good generalization to current datasets (e.g., SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), etc.) is different from being bias-free. As shown in Gururangan et al. (2018), similar annotation artifacts can appear in multiple different datasets. So by overfitting to common lexical biases across multiple datasets, biased models might still reach higher generalization accuracy. 7 Another metric on NLI-stress can be checking the portion of model predictions on the hard data that is correct both before and after adding the extra words. We empirically verified that this metric shows the same result trends as Acc hard. serve similar performance for CWB and WOB, we leave the results for WOB"
2020.acl-main.773,P19-1334,0,0.0606717,"Missing"
2020.acl-main.773,K18-1007,0,0.0341861,"inal MNLI dataset to maintain the consistency of input text distribution. Adversarial Removal Methods. Adversarial removal techniques are used to control the content of representations. They were first used to do unsupervised domain adaptation in Ganin and Lempitsky (2015). Xie et al. (2017) later generalized this approach to control specific information learned by the representation. Li et al. (2018) used a similar approach to learn privacy-preserving representations. However, Elazar and Goldberg (2018) showed that such adversarial approach fails to completely remove demographic information. Minervini and Riedel (2018) generate adversarial examples and regularize models based on first-order logic rules. Belinkov et al. (2019a,b) showed that adversarial removal methods can be effective for the hypothesisonly NLI bias. Our focus is on two different lexical biases and our results are complementary to theirs.2 Recently, Wang et al. (2019a) proposed HEX projection to force the orthogonality between the target model and a superficial model to improve domain generalization for image classification tasks. Here, to make the model less lexically biased, we apply the HEX projection with specially-designed NLP model ar"
2020.acl-main.773,P16-2022,0,0.0213707,"baseline since they are more controllable, and because the interaction of sentences only appears at the top classifier, which makes it easier to compare the different effects of different regularization.3 Our baseline structures can be divided into three stages. The first stage is to embed the words into word embeddings. The second stage is to get the representations for each sentence. We use three layers of BiLSTM to get the representation. We also added residual and skip-connections as Nie et al. (2019), and find that it leads to better performance. For the final stage, our baseline follows Mou et al. (2016); Conneau et al. (2017) to concatenate these two sentence embeddings, their difference, and their element-wise product as follows: m = [h1 ; h2 ; h1 − h2 ; h1 h2 ] (1) The resulting vector is passed through another multi-layer perceptron (MLP) to get the final classification result.4 Next, we will describe two different methods to directly debias the model. 4.2 Debiasing Embeddings Word embeddings are an important component in all neural NLP models. They contain the most basic semantics of words. Recent studies have shown that removing gender bias from word embeddings can lead to less biased m"
2020.acl-main.773,C18-1198,0,0.111948,"ference (NLI), where the target of the model is to classify the relations between a pair of sentences into three categories: entailment, neutral and contradiction. With the release of large-scale standard datasets (Bowman et al., 2015; Williams et al., 2018), significant success has been made on 1 Our code and data are available at: https://github. com/owenzx/LexicalDebias-ACL2020 this task, and recent state-of-the-art neural models have already reached competitive performance even compared to humans. However, a number of papers (Gururangan et al., 2018; Poliak et al., 2018; Nie et al., 2019; Naik et al., 2018) have shown that despite the high accuracy on these datasets, these models are far from mastering the required nature of natural language inference. Instead of deeply understanding the sentences in the correct semantic way, these models tend to exploit shortcuts or annotation artifacts in the dataset and actually overfit to these datasets to predict the label using simple patterns. However, most shortcuts are only valid within the datasets and fail to hold for general natural language. Hence, these models fail to generalize to other datasets for the same task (Talman and Chatzikyriakidis, 2019"
2020.acl-main.773,N18-1101,0,0.579834,"models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.1 1 Introduction In this work, we focus on investigating and reducing biases in the task of Natural Language Inference (NLI), where the target of the model is to classify the relations between a pair of sentences into three categories: entailment, neutral and contradiction. With the release of large-scale standard datasets (Bowman et al., 2015; Williams et al., 2018), significant success has been made on 1 Our code and data are available at: https://github. com/owenzx/LexicalDebias-ACL2020 this task, and recent state-of-the-art neural models have already reached competitive performance even compared to humans. However, a number of papers (Gururangan et al., 2018; Poliak et al., 2018; Nie et al., 2019; Naik et al., 2018) have shown that despite the high accuracy on these datasets, these models are far from mastering the required nature of natural language inference. Instead of deeply understanding the sentences in the correct semantic way, these models ten"
2020.acl-main.773,K18-1047,1,0.677665,"s). Next, since the number of these unbiased samples may not be large enough, we repeatedly add those selected samples to make the training set more balanced. The results from adding 500 new samples to 50,000 new samples are shown in Sec. 6.1. 8761 Data Augmentation by Adding Synthetic Data. Researchers have been using synthetic rules to generate harder or perturbed samples to fool the model. Here, besides using these datasets only as the evaluation set, we also add these samples back to the training set, similar to the concept of adversarial training (Jia and Liang, 2017; Wang et al., 2019c; Niu and Bansal, 2018) where the adversarial examples are added back to the training set so that the resulting model will be more robust to similar adversarial attacks. In our experiments, we follow Naik et al. (2018) to append meaningless sentences at the end of the hypothesis sentence like in Table 1 to create additional new samples. The detailed construction of these samples can be seen in Appendix. By learning from these augmented datasets, the model should also be more robust to certain types of perturbations/biases of the data. In Sec. 6.1, our experiments showed that while this approach can lead to less bias"
2020.acl-main.773,D14-1162,0,0.0948902,"Missing"
2020.acl-main.773,S18-2023,0,0.106671,"Missing"
2020.acl-main.773,N16-3020,0,0.0463383,"ll only slightly hurt the model. In 8766 conclusion, this approach significantly robustifies the model against CWB and WOB while maintaining competitive overall performance. In comparison to the debiasing embeddings results, we can see that instead of regularizing the content in the word embeddings, regularizing the model’s compositionality at the upper interaction level is a more promising direction for debiasing lexical biases. We have also tried combining this method with the data-level debiasing approach above but get no further improvement.10 6.3 Qualitative Feature Analysis We use LIME (Ribeiro et al., 2016) to qualitatively visualize how orthogonal projection w.r.t. BoW sub-model changes the features used by the model. We selected one example from the CWB Bal dataset to see how applying the BoW model with HEX corrects previous mistakes. From Fig. 3, we can see that before applying the BoW submodel (the upper part of the figure), the model predicts the contradiction label almost solely based on the existence of the word “no” in the hypothesis. However, after applying our BoW sub-model with HEX projection, our model can give higher importance to other useful features (e.g., the match of the two “b"
2020.acl-main.773,W19-4810,0,0.105255,"Missing"
2020.acl-main.773,D17-1323,0,0.0307739,"n NLI), and also our classifier debiasing method forces the task model to capture orthogonal information via HEX projection. Removing Gender Bias in NLP Models. There 2 We have tried a similar approach via gradient reversal w.r.t. BoW sub-model in preliminary experiments and observed less effectiveness (than HEX-projection), which hints that different types of biases can lead to different behaviors. is also a line of work in NLP on analyzing and reducing gender bias in NLP models. Bolukbasi et al. (2016); Caliskan et al. (2017); Zhao et al. (2018a) studied the bias problem in word embeddings. Zhao et al. (2017) reduced gender bias in visual recognition using corpus-level constraints. Zhao et al. (2018b) discussed the gender bias problem in co-reference resolution. These problems are related to our work, but lexical biases are more complex. Multiple inseparable lexical dataset biases can influence one single example and the same word can have different lexical biases in different contexts. Later in our experiments, we show that these two problems behave differently and we present the need for different solutions. 3 Data-Level Debiasing Models naturally learn the biases from the dataset they are train"
2020.acl-main.773,N18-2003,0,0.318104,"hods (against two different types of problematic lexical biases in NLI), and also our classifier debiasing method forces the task model to capture orthogonal information via HEX projection. Removing Gender Bias in NLP Models. There 2 We have tried a similar approach via gradient reversal w.r.t. BoW sub-model in preliminary experiments and observed less effectiveness (than HEX-projection), which hints that different types of biases can lead to different behaviors. is also a line of work in NLP on analyzing and reducing gender bias in NLP models. Bolukbasi et al. (2016); Caliskan et al. (2017); Zhao et al. (2018a) studied the bias problem in word embeddings. Zhao et al. (2017) reduced gender bias in visual recognition using corpus-level constraints. Zhao et al. (2018b) discussed the gender bias problem in co-reference resolution. These problems are related to our work, but lexical biases are more complex. Multiple inseparable lexical dataset biases can influence one single example and the same word can have different lexical biases in different contexts. Later in our experiments, we show that these two problems behave differently and we present the need for different solutions. 3 Data-Level Debiasing"
2020.acl-main.773,D18-1521,0,0.236003,"hods (against two different types of problematic lexical biases in NLI), and also our classifier debiasing method forces the task model to capture orthogonal information via HEX projection. Removing Gender Bias in NLP Models. There 2 We have tried a similar approach via gradient reversal w.r.t. BoW sub-model in preliminary experiments and observed less effectiveness (than HEX-projection), which hints that different types of biases can lead to different behaviors. is also a line of work in NLP on analyzing and reducing gender bias in NLP models. Bolukbasi et al. (2016); Caliskan et al. (2017); Zhao et al. (2018a) studied the bias problem in word embeddings. Zhao et al. (2017) reduced gender bias in visual recognition using corpus-level constraints. Zhao et al. (2018b) discussed the gender bias problem in co-reference resolution. These problems are related to our work, but lexical biases are more complex. Multiple inseparable lexical dataset biases can influence one single example and the same word can have different lexical biases in different contexts. Later in our experiments, we show that these two problems behave differently and we present the need for different solutions. 3 Data-Level Debiasing"
2020.acl-main.773,D15-1075,0,\N,Missing
2020.acl-main.773,P17-1152,0,\N,Missing
2020.acl-main.773,P18-2103,0,\N,Missing
2020.acl-main.773,D18-1002,0,\N,Missing
2020.acl-main.773,N19-1423,0,\N,Missing
2020.emnlp-main.162,2020.acl-main.234,0,0.0213113,"nd Bansal, 2019; Chen et al., 2019; Su et al., 2020; Zhou et al., 2020) aims to build joint cross-modal representations and focuses on vision-and-language tasks. Due to particularity of grounded language, these models are not able to improve pure language tasks as shown in Sec. 5.1. Visually-Aided Language Learning Previous works use visual information to improve specific language tasks such as coreference resolution (Kong et al., 2014), machine translation (Elliott et al., 2016; Ive et al., 2019; Wu et al., 2019; Zhang et al., 2020), semantic parsing (Christie et al., 2016; Shi et al., 2019; Kojima et al., 2020), and bilingual lexicon learning (Kiela et al., 2015; Vuli´c et al., 2016). Our work has a focus on building a visuallysupervised language pre-training frameworks to improve general language understanding. Similar to our work, Frome et al. (2013); Lazaridou et al. (2015); Collell et al. (2017); Kiela et al. (2018); Bordes et al. (2019) aim to improve language representation with visual information; however, most of these works focus on grounded language and hence might suffer from the large discrepancy that we discuss in Sec. 2.3. 7 Conclusion In this paper, we explored the possibility of util"
2020.emnlp-main.162,N15-1016,0,0.25143,"Missing"
2020.emnlp-main.162,2021.ccl-1.108,0,0.0872368,"Missing"
2020.emnlp-main.162,D14-1162,0,0.0846693,"erstand the language and leads to the improvement in Table 2. On the other hand, some tokens are not faithfully grounded (e.g., “writing” in Example 1) and we also observe a shift in alignment (e.g., the relevant image for the phrase “my love” in Example 2 is aligned to “my” instead of “love”). These misalignments are possibly caused by the limitations of sentence-image weak supervision in our training data since the strong token-image annotations are not available. 6 Related Work Language (Model) Pre-training Language pre-training has moved from token-level pretraining (Mikolov et al., 2013; Pennington et al., 2014) to sentence-level pre-training (Le and Mikolov, 2014; Kiros et al., 2015; Conneau et al., 2017; Dai and Le, 2015). Recently, a Vision-and-Language Pre-training Since language models are trained with self-supervision without knowing the connection to the visual world, vision-and-language pre-training (Li et al., 2019; Lu et al., 2019; Tan and Bansal, 2019; Chen et al., 2019; Su et al., 2020; Zhou et al., 2020) aims to build joint cross-modal representations and focuses on vision-and-language tasks. Due to particularity of grounded language, these models are not able to improve pure language ta"
2020.emnlp-main.162,N18-1202,0,0.265321,"hese images vokens (visualized tokens) and develop a vokenization process to contextually generate them. Introduction Most humans learn language understanding from multiple modalities rather than only from the text and audio, especially using the visual modality. As claimed in Bloom (2002), visual pointing is an essential step for most children to learn meanings of words. However, existing language pretraining frameworks are driven by contextual learning which only takes the language context as selfsupervision. For example, word2vec (Mikolov et al., 2013) takes surrounding bag-of-words; ELMo (Peters et al., 2018) and GPT (Radford et al., 1 Code and pre-trained models publicly available at: https://github.com/airsplay/vokenization. 2018) take succeeding contexts; and BERT (Devlin et al., 2019) takes randomly masked tokens. Although these self-supervised frameworks have achieved strong progress towards understanding human language, they did not borrow grounding information from the external visual world (see related motivations in recent work by Bender and Koller (2020) and Bisk et al. (2020)). In this paper, we introduce the visuallysupervised language model that simulates human language learning with"
2020.emnlp-main.162,P17-1099,0,0.0535611,"Missing"
2020.emnlp-main.162,P18-1238,0,0.101682,"Missing"
2020.emnlp-main.162,P19-1180,0,0.0244447,"t al., 2019; Tan and Bansal, 2019; Chen et al., 2019; Su et al., 2020; Zhou et al., 2020) aims to build joint cross-modal representations and focuses on vision-and-language tasks. Due to particularity of grounded language, these models are not able to improve pure language tasks as shown in Sec. 5.1. Visually-Aided Language Learning Previous works use visual information to improve specific language tasks such as coreference resolution (Kong et al., 2014), machine translation (Elliott et al., 2016; Ive et al., 2019; Wu et al., 2019; Zhang et al., 2020), semantic parsing (Christie et al., 2016; Shi et al., 2019; Kojima et al., 2020), and bilingual lexicon learning (Kiela et al., 2015; Vuli´c et al., 2016). Our work has a focus on building a visuallysupervised language pre-training frameworks to improve general language understanding. Similar to our work, Frome et al. (2013); Lazaridou et al. (2015); Collell et al. (2017); Kiela et al. (2018); Bordes et al. (2019) aim to improve language representation with visual information; however, most of these works focus on grounded language and hence might suffer from the large discrepancy that we discuss in Sec. 2.3. 7 Conclusion In this paper, we explored t"
2020.emnlp-main.162,D13-1170,0,0.00278225,"and Fine-tuning Tasks We train our model on English Wikipedia 5 and its featured subset Wiki103 (Merity et al., 2017). We use our vokenizer to generate vokens for these two datasets as well. The pre-trained models are then fine-tuned on GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016, 2018), and SWAG (Zellers et al., 2018) to assess the pretraining performance. Since some smaller tasks in GLUE are reported as unstable (Dodge et al., 2020), recent papers (e.g., Li et al. (2020b)) only report on selected tasks. We follow this trend and evaluate on the four largest datasets (i.e., SST-2 (Socher et al., 2013), QNLI (Rajpurkar et al., 2016), QQP (Iyer et al., 2017), MNLI (Williams et al., 2018)).6 . 4.2 et al., 2019) and ResNeXt (Xie et al., 2017) are not fine-tuned. We set the hinge loss margin M to 0.5. During the vokenization process of English Wikipedia and Wiki103, we use the faiss (Johnson et al., 2019) library to speed up the nearest neighbor search. The vokens are retrieved from the Visual Genome images that are not used in MS COCO. We fix a voken size of 50000. When pre-training the model on pure language corpus, we unify the training protocols to avoid possible side effects. We follow pre"
2020.emnlp-main.162,D19-1514,1,0.935765,". SQuAD results are “exact match”/“F1”. The results which significantly outperform the second-best ones are marked in bold. The averages of metrics (denoted as “Avg.”) show improvement from voken supervisions. Model Init. with BERT? Diff. to BERT Weight SST-2 QNLI QQP MNLI Yes Yes Yes Yes No 0.0e-3 6.4e-3 6.5e-3 41.6e-3 42.0e-3 90.3 90.1 90.3 87.3 82.4 89.6 89.5 88.9 50.5 50.5 88.4 88.6 88.4 86.6 79.8 82.4 82.9 82.4 77.3 31.8 - 0.0e-3 6.5e-3 90.3 89.9 89.6 89.9 88.4 88.4 82.4 82.3 ViLBERT (Lu et al., 2019) VL-BERT (Su et al., 2020) VisualBERT (Li et al., 2019) Oscar (Li et al., 2020a) LXMERT (Tan and Bansal, 2019) BERTBASE (Devlin et al., 2019) BERTBASE + Weight Noise Table 3: Results of vision-and-language pre-trained models on GLUE tasks. We also provide BERT models w/ and w/o weight noise as baselines. Pre-trained on SST-2 QNLI QQP MNLI MS COCO Wiki103* No Pre-train 83.7 85.8 77.1 60.6 77.9 50.5 82.1 84.8 31.6 69.3 73.9 31.8 Table 4: Results of BERT models pre-trained on captions in MS COCO and a reduced version of Wiki103 dataset (denoted as Wiki103*). Models without pretraining are taken as a baseline. task consistently improves the downstream tasks’ performance and achieves large average gains. W"
2020.emnlp-main.162,D16-1264,0,0.266775,"ted images when considering its context, e.g., the abstract word “angry” in the sentence “an angry cat lies on my leg”. This observation is realized by our contextual token-image matching model (defined in Sec. 3.2) inside our vokenization processor, where we map tokens to images by viewing the sentence as the context. Using our proposed vokenizer with a contextualized token-image matching model, we generate vokens for English Wikipedia. Supervised by these generated vokens, we show consistent improvements upon a BERT model on several diverse NLP tasks such as GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018). We also show the transferability of our vokens to other frameworks (i.e., RoBERTa). 2 Visually-Supervised Language Models Contextual language representation learning is driven by self-supervision without considering explicit connections (grounding) to the external world. In this section, we illustrate the idea of a visuallysupervised language model and discuss the challenges of creating its visual supervision. 2.1 Vokens: Visualized Tokens To provide visual supervision to the language model, we assume a text corpus where each token is aligned with a related i"
2020.emnlp-main.162,N18-1101,0,0.0123311,"set Wiki103 (Merity et al., 2017). We use our vokenizer to generate vokens for these two datasets as well. The pre-trained models are then fine-tuned on GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016, 2018), and SWAG (Zellers et al., 2018) to assess the pretraining performance. Since some smaller tasks in GLUE are reported as unstable (Dodge et al., 2020), recent papers (e.g., Li et al. (2020b)) only report on selected tasks. We follow this trend and evaluate on the four largest datasets (i.e., SST-2 (Socher et al., 2013), QNLI (Rajpurkar et al., 2016), QQP (Iyer et al., 2017), MNLI (Williams et al., 2018)).6 . 4.2 et al., 2019) and ResNeXt (Xie et al., 2017) are not fine-tuned. We set the hinge loss margin M to 0.5. During the vokenization process of English Wikipedia and Wiki103, we use the faiss (Johnson et al., 2019) library to speed up the nearest neighbor search. The vokens are retrieved from the Visual Genome images that are not used in MS COCO. We fix a voken size of 50000. When pre-training the model on pure language corpus, we unify the training protocols to avoid possible side effects. We follow previous works to conduct two simplifications: 1. Removing the nextsentence-prediction ta"
2020.emnlp-main.162,Q14-1006,0,0.0337035,"n the right side of Fig. 2): X LVOKEN - CLS (s, sˆ) = − log pi (v(wi ; s) |s  sˆ) wi ∈s The visually-supervised masked language model takes the sum of these two losses with a ratio λ. LVLM (s, sˆ) = LVOKEN - CLS (s, sˆ) + λLMLM (s, sˆ) (1) 2.3 pi (v |s) = softmaxv {W hi + b} l X LMLM (s, sˆ) = − Two Challenges in Creating Vokens Previous sections illustrate the potential external supervision by assuming the existence of vokens. However, we are currently lacking the dense annotations from tokens to images. The most similar concept to vokens is phrase localization (e.g., in Flickr30K entities (Young et al., 2014; Plummer et al., 2017)). Because the process of collecting phrase localization is costly, the coverage and the amount of annotations cannot meet our requirements.3 Apart from phrase localization, the most promising data source is image captioning datasets with sentence-to-image mappings (or discovered from multimodal documents, as in Hessel et al. (2019)). Image captions belong to a specific type of language called grounded language (Roy and Pentland, 2002; Hermann et al., 2017), which has an explicit grounding to external existence or physical actions. However, grounded language has a large"
2020.emnlp-main.162,D18-1009,0,0.0843955,"text, e.g., the abstract word “angry” in the sentence “an angry cat lies on my leg”. This observation is realized by our contextual token-image matching model (defined in Sec. 3.2) inside our vokenization processor, where we map tokens to images by viewing the sentence as the context. Using our proposed vokenizer with a contextualized token-image matching model, we generate vokens for English Wikipedia. Supervised by these generated vokens, we show consistent improvements upon a BERT model on several diverse NLP tasks such as GLUE (Wang et al., 2019), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018). We also show the transferability of our vokens to other frameworks (i.e., RoBERTa). 2 Visually-Supervised Language Models Contextual language representation learning is driven by self-supervision without considering explicit connections (grounding) to the external world. In this section, we illustrate the idea of a visuallysupervised language model and discuss the challenges of creating its visual supervision. 2.1 Vokens: Visualized Tokens To provide visual supervision to the language model, we assume a text corpus where each token is aligned with a related image (although these voken annota"
2020.emnlp-main.43,2020.lrec-1.325,0,0.0850336,"Missing"
2020.emnlp-main.625,P16-1154,0,0.0661125,"Missing"
2020.emnlp-main.625,C16-1105,0,0.0406471,"Missing"
2020.emnlp-main.625,P16-1014,0,0.0234734,"oaches perform statistically significantly better (based on human evaluation) than strong singlereward based RL models as well as non-bandits based multi-reward methods such as the multi-task approach of Pasunuru and Bansal (2018). We further present various interpretable analyses of our bandit progress and learned rewards curriculum over different bandit approaches. 2 Related Works Policy Gradient and Generative Models: Neural sequence to sequence models with cross-entropy optimization, potentially with attention mechanism (Bahdanau et al., 2015) and pointer-copy mechanism (See et al., 2017; Gulcehre et al., 2016; Vinyals et al., 2015a; Merity et al., 2018), are widely used in language generation tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), abstractive summarization (Chopra et al., 2016; Nallapati et al., 2016), question generation (Du et al., 2017; Zhang and Bansal, 2019), video/image captioning (Xu et al., 2015; Vinyals et al., 2015b; Pasunuru and Bansal, 2017a; Zhou et al., 2018), as well as sentence simplification (Zhang and Lapata, 2017; Guo et al., 2018). However, often the final metrics of interest are not differentiable, and thus not compatible with the stand"
2020.emnlp-main.625,C18-1039,1,0.789634,"tentially with attention mechanism (Bahdanau et al., 2015) and pointer-copy mechanism (See et al., 2017; Gulcehre et al., 2016; Vinyals et al., 2015a; Merity et al., 2018), are widely used in language generation tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), abstractive summarization (Chopra et al., 2016; Nallapati et al., 2016), question generation (Du et al., 2017; Zhang and Bansal, 2019), video/image captioning (Xu et al., 2015; Vinyals et al., 2015b; Pasunuru and Bansal, 2017a; Zhou et al., 2018), as well as sentence simplification (Zhang and Lapata, 2017; Guo et al., 2018). However, often the final metrics of interest are not differentiable, and thus not compatible with the standard maximum-likelihood based training. Motivated by this, recently there has been a surge in applications of reinforcement learning techniques to language generation (Ranzato et al., 2016), in which the gradients of non-differentiable metrics are approximated using the scoring function (REINFORCE (Williams, 1992)). A few successful examples include image captioning (Rennie et al., 2017; Ren et al., 2017), abstractive summarization (Paulus et al., 2018; Chen and Bansal, 2018; Pasunuru an"
2020.emnlp-main.625,N19-1355,1,0.912126,"Some widely used bandit algorithms include -greedy (Sutton and Barto, 2018), Boltzmann exploration (Kaelbling et al., 1996), UCB (Auer et al., 2002a), Thompson sampling (Chapelle and Li, 2011), contextual bandit (Sharaf and Daum´e III, 2019), as well as Exp3 adversarial bandit (Auer et al., 2002b). In this work, we use Exp3, and the hierarchical version of it, for the problem of optimizing multiple rewards.2 Multi-armed bandit algorithms have been used in a wide range of applications, such as online advertising (Chen et al., 2013), recommendation (Li et al., 2010), multi-task task selection (Guo et al., 2019a), and hyper-parameter optimization (Li et al., 2018; Merentitis et al., 2018). Recently, Graves et al. (2017) apply a non-stationary multi-armed bandit (in particular, the Exp3.S algorithm) to select an adaptive policy (curriculum) that a neural network follows to maximize the learning efficiency. Sharma and Ravindran (2017) use multiarmed bandit sampling to choose which domain data (harder vs. easier) to feed as input to a single model (using different Atari games). To our knowledge, we are the first ones to apply a multi-armed bandit to optimize multiple rewards in the context of text gene"
2020.emnlp-main.625,Q19-1019,0,0.106743,"Some widely used bandit algorithms include -greedy (Sutton and Barto, 2018), Boltzmann exploration (Kaelbling et al., 1996), UCB (Auer et al., 2002a), Thompson sampling (Chapelle and Li, 2011), contextual bandit (Sharaf and Daum´e III, 2019), as well as Exp3 adversarial bandit (Auer et al., 2002b). In this work, we use Exp3, and the hierarchical version of it, for the problem of optimizing multiple rewards.2 Multi-armed bandit algorithms have been used in a wide range of applications, such as online advertising (Chen et al., 2013), recommendation (Li et al., 2010), multi-task task selection (Guo et al., 2019a), and hyper-parameter optimization (Li et al., 2018; Merentitis et al., 2018). Recently, Graves et al. (2017) apply a non-stationary multi-armed bandit (in particular, the Exp3.S algorithm) to select an adaptive policy (curriculum) that a neural network follows to maximize the learning efficiency. Sharma and Ravindran (2017) use multiarmed bandit sampling to choose which domain data (harder vs. easier) to feed as input to a single model (using different Atari games). To our knowledge, we are the first ones to apply a multi-armed bandit to optimize multiple rewards in the context of text gene"
2020.emnlp-main.625,D16-1128,0,0.0300828,"Missing"
2020.emnlp-main.625,D16-1127,0,0.031791,"single sample along with a bias estimator ˆb to reduce variance: ∇θ LRL = −(r(ws ) − ˆb)∇θ log pθ (ws ) (2) There are several ways to calculate the baseline estimator, and in this work we use the SCST mechanism (Rennie et al., 2017). Need for a better multi-reward optimization. Often, an RL agent can improve the policy pθ via multiple reward sources. However, efficient ways of optimizing multiple rewards in a policy gradient-based reinforcement learning setup have been less explored. Previous works have either explored using a weighted combination of multiple rewards (Zhang and Lapata, 2017; Li et al., 2016) or alternate fashion of optimizing multiple rewards inspired via multi-task learning setup (Pasunuru and Bansal, 2018). However, these approaches have a disadvantage of tuning the weights of the rewards combination or using a static tunable mixing ratio while optimizing in an alternate fashion. To this end, we explore multi-reward optimization via a multi-armed bandit approach (Bubeck et al., 2012; Lattimore and Szepesv´ari, 2019; Burtini et al., 2015). During the training, the bandit explores/exploits the choice of reward functions in order to improve the overall performance of the model. In"
2020.emnlp-main.625,P09-1011,0,0.0147506,"examples from the training set, as the SQuAD test set is not open. For pre-processing, we do standard tokenization. We report on evaluation metrics including BLEU-4, METEOR, ROUGE-L, QBLEU1 (Nema and Khapra, 2018), as well as QPP and QAP (Zhang and Bansal, 2019). 4.2 Data-to-Text Generation Data-to-text is the task of expressing the components (attributes and values) of meaning representation (MR) as human-readable natural sentences. Previous work in this area include templates (Reiter, 1995), rules (Reiter et al., 2005), pipelines (Reiter, 2007; Reiter and Dale, 1997), probabilistic models (Liang et al., 2009) and more recently end-to-end as well as neural-based methods (Wen et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Lampouras and Vlachos, 2016; Duˇsek et al., 2020; Wiseman et al., 2017; Gong, 2018; Chen and Mooney, 2008; Reiter, 2017; Lebret et al., 2016; Distiawan et al., 2018; Gehrmann et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019b; Zhao et al., 2020). In our work, we use the state-of-the-art model from Zhao et al. (2020) as our baseline. Baseline. Given a set of Resource Description Framework (RDF) triples,4 the task is to generate a natural language t"
2020.emnlp-main.625,W04-1013,0,0.0150841,"ubtasks like referring expression generation, aggregation, lexicalization, sentence segmentation, and surface realization. It contains 9,674 unique RDF triple-sets and 25,298 text references, which is divided into train, dev, and test sets.6 We report all our results on the ‘seen’ part of the test set. For each sample, the input is a set of up to 7 RDF triples from DBPedia, and the output is their text descriptions. The standard evaluation metrics for this dataset include METEOR7 (Denkowski and Lavie, 2014), BLEU (Papineni et al., 2002), and TER8 (Snover et al., 2006). We also report ROUGE-L (Lin, 2004) and Entailment-Score (Pasunuru and Bansal, 2018). 4.3 Training Details All the hyperparameters are tuned on the validation set for both question generation and data-to-text tasks. We use TITAN X and GeForce GTX 1080 GPUs for all our experiments. For the question generation task, we use two layers for both encoder and decoder. We set the hidden size of LSTM-RNN to 600 and use BERT-based contextual embeddings as input. We use a batch size of 32, encoder maximum length of 512 and decoder maximum length of 50, and maximum gradient clipping of 5. We use Adam optimizer (Kingma and Ba, 2015) with a"
2020.emnlp-main.625,2021.ccl-1.108,0,0.122262,"Missing"
2020.emnlp-main.625,D15-1166,0,0.757227,"proaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable"
2020.emnlp-main.625,W18-6501,0,0.0139128,"e components (attributes and values) of meaning representation (MR) as human-readable natural sentences. Previous work in this area include templates (Reiter, 1995), rules (Reiter et al., 2005), pipelines (Reiter, 2007; Reiter and Dale, 1997), probabilistic models (Liang et al., 2009) and more recently end-to-end as well as neural-based methods (Wen et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Lampouras and Vlachos, 2016; Duˇsek et al., 2020; Wiseman et al., 2017; Gong, 2018; Chen and Mooney, 2008; Reiter, 2017; Lebret et al., 2016; Distiawan et al., 2018; Gehrmann et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019b; Zhao et al., 2020). In our work, we use the state-of-the-art model from Zhao et al. (2020) as our baseline. Baseline. Given a set of Resource Description Framework (RDF) triples,4 the task is to generate a natural language text describing the facts in the RDF data. Following Zhao et al. (2020), we serialize and reorder the RDF data as an intermediate planning setup, and feed the plan into a seq2seq model with attention and copy mechanism. Rewards. We use BLEU, ROUGE-L, and Entailment-Score (Pasunuru and Bansal, 2018) as rewards. Entailment-Score is calculated based on the"
2020.emnlp-main.625,N16-1086,1,0.809698,"rd tokenization. We report on evaluation metrics including BLEU-4, METEOR, ROUGE-L, QBLEU1 (Nema and Khapra, 2018), as well as QPP and QAP (Zhang and Bansal, 2019). 4.2 Data-to-Text Generation Data-to-text is the task of expressing the components (attributes and values) of meaning representation (MR) as human-readable natural sentences. Previous work in this area include templates (Reiter, 1995), rules (Reiter et al., 2005), pipelines (Reiter, 2007; Reiter and Dale, 1997), probabilistic models (Liang et al., 2009) and more recently end-to-end as well as neural-based methods (Wen et al., 2015; Mei et al., 2016; Duˇsek and Jurcicek, 2016; Lampouras and Vlachos, 2016; Duˇsek et al., 2020; Wiseman et al., 2017; Gong, 2018; Chen and Mooney, 2008; Reiter, 2017; Lebret et al., 2016; Distiawan et al., 2018; Gehrmann et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019b; Zhao et al., 2020). In our work, we use the state-of-the-art model from Zhao et al. (2020) as our baseline. Baseline. Given a set of Resource Description Framework (RDF) triples,4 the task is to generate a natural language text describing the facts in the RDF data. Following Zhao et al. (2020), we serialize and reorder"
2020.emnlp-main.625,K16-1028,0,0.16677,"empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable automatic evaluation metrics that measure the quality of the generated sequence. Recent introduction of policy gradient-based r"
2020.emnlp-main.625,D18-1429,0,0.0418866,"Missing"
2020.emnlp-main.625,J16-1001,0,0.0194837,"tiable metrics are approximated using the scoring function (REINFORCE (Williams, 1992)). A few successful examples include image captioning (Rennie et al., 2017; Ren et al., 2017), abstractive summarization (Paulus et al., 2018; Chen and Bansal, 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018), machine translation (Wu et al., 2016; Gu et al., 2017), sentence simplification (Zhang and Lapata, 2017), as well as video captioning (Pasunuru and Bansal, 2017b; Wang et al., 2018). Previous works have explored the problem of optimizing multiple rewards in the context of machine translation (Neubig and Watanabe, 2016). For example, the works of Duh et al. (2012) and Sankaran et al. (2013) are based on the theory of Pareto Optimality. Our approach, instead, dynamically decides the trade-off among metrics, rather than exploring the set of static Pareto-optimal hypotheses. The most related work on this line is Pasunuru and Bansal (2018), which simultaneously optimizes multiple rewards in alternate fashion for abstractive summarization. In our work, we use a multi-armed bandit framework to dynamically switch among multiple diverse reward optimizations in the context of policy-gradient-based generative models.1"
2020.emnlp-main.625,P02-1040,0,0.117129,"for data-to-text generation which focuses on micro-planning involving several subtasks like referring expression generation, aggregation, lexicalization, sentence segmentation, and surface realization. It contains 9,674 unique RDF triple-sets and 25,298 text references, which is divided into train, dev, and test sets.6 We report all our results on the ‘seen’ part of the test set. For each sample, the input is a set of up to 7 RDF triples from DBPedia, and the output is their text descriptions. The standard evaluation metrics for this dataset include METEOR7 (Denkowski and Lavie, 2014), BLEU (Papineni et al., 2002), and TER8 (Snover et al., 2006). We also report ROUGE-L (Lin, 2004) and Entailment-Score (Pasunuru and Bansal, 2018). 4.3 Training Details All the hyperparameters are tuned on the validation set for both question generation and data-to-text tasks. We use TITAN X and GeForce GTX 1080 GPUs for all our experiments. For the question generation task, we use two layers for both encoder and decoder. We set the hidden size of LSTM-RNN to 600 and use BERT-based contextual embeddings as input. We use a batch size of 32, encoder maximum length of 512 and decoder maximum length of 50, and maximum gradien"
2020.emnlp-main.625,P17-1117,1,0.943275,"rtant NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable automatic evaluation metrics that measure the quality of the generated sequence. Recent introduction of policy gradient-based reinforcement learning approaches address these issues for sequence generation tasks by directly optimizing the non-diff"
2020.emnlp-main.625,D17-1103,1,0.933605,"rtant NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable automatic evaluation metrics that measure the quality of the generated sequence. Recent introduction of policy gradient-based reinforcement learning approaches address these issues for sequence generation tasks by directly optimizing the non-diff"
2020.emnlp-main.625,N18-2102,1,0.0519723,"n-differentiable evaluation metrics (Zaremba and Sutskever, 2015; Ranzato et al., 2016; Rennie et al., 2017). However, optimizing for a particular metric/reward via policy gradient-based approaches often leads to improvement in mostly that specific metric, suggesting that this approach is gaming the metrics (Paulus et al., 2018). The weighted average of multiple metrics or surrogate rewards have been explored (Liu et al., 2017), but these approaches have to deal with finding the optimal scale balance across different metrics. One can alternatively optimize multiple metrics via a mixing ratio (Pasunuru and Bansal, 2018), but this still needs careful tuning of the mixing ratio. Moreover, all these reward approaches are fixed and do not change over training, and all the metrics may not be important over every stage of the training. Thus, it might be useful to consider using a dynamic combination of metrics, which rewards to use early vs. later, or which rewards might be useful to come back later in training, and consider the context of the full history of rewards, as well as the models current state and the nature of the metric. 7766 Proceedings of the 2020 Conference on Empirical Methods in Natural Language P"
2020.emnlp-main.625,D16-1264,0,0.0239623,"Missing"
2020.emnlp-main.625,W07-2315,0,0.0712965,"Missing"
2020.emnlp-main.625,D15-1044,0,0.0493087,"dit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable automatic evaluation metrics that measure the quality of the generated sequence. Recent introduction of"
2020.emnlp-main.625,N13-1115,0,0.00954373,"ams, 1992)). A few successful examples include image captioning (Rennie et al., 2017; Ren et al., 2017), abstractive summarization (Paulus et al., 2018; Chen and Bansal, 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018), machine translation (Wu et al., 2016; Gu et al., 2017), sentence simplification (Zhang and Lapata, 2017), as well as video captioning (Pasunuru and Bansal, 2017b; Wang et al., 2018). Previous works have explored the problem of optimizing multiple rewards in the context of machine translation (Neubig and Watanabe, 2016). For example, the works of Duh et al. (2012) and Sankaran et al. (2013) are based on the theory of Pareto Optimality. Our approach, instead, dynamically decides the trade-off among metrics, rather than exploring the set of static Pareto-optimal hypotheses. The most related work on this line is Pasunuru and Bansal (2018), which simultaneously optimizes multiple rewards in alternate fashion for abstractive summarization. In our work, we use a multi-armed bandit framework to dynamically switch among multiple diverse reward optimizations in the context of policy-gradient-based generative models.1 Multi-Armed Bandit: Many control problems can be cast as multi-armed ba"
2020.emnlp-main.625,P17-1099,0,0.133238,"fectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable automatic evaluation metrics that measure the quality of the generated sequence. Recent introduction of policy gradient-based reinforcement learni"
2020.emnlp-main.625,2006.amta-papers.25,0,0.0641503,"focuses on micro-planning involving several subtasks like referring expression generation, aggregation, lexicalization, sentence segmentation, and surface realization. It contains 9,674 unique RDF triple-sets and 25,298 text references, which is divided into train, dev, and test sets.6 We report all our results on the ‘seen’ part of the test set. For each sample, the input is a set of up to 7 RDF triples from DBPedia, and the output is their text descriptions. The standard evaluation metrics for this dataset include METEOR7 (Denkowski and Lavie, 2014), BLEU (Papineni et al., 2002), and TER8 (Snover et al., 2006). We also report ROUGE-L (Lin, 2004) and Entailment-Score (Pasunuru and Bansal, 2018). 4.3 Training Details All the hyperparameters are tuned on the validation set for both question generation and data-to-text tasks. We use TITAN X and GeForce GTX 1080 GPUs for all our experiments. For the question generation task, we use two layers for both encoder and decoder. We set the hidden size of LSTM-RNN to 600 and use BERT-based contextual embeddings as input. We use a batch size of 32, encoder maximum length of 512 and decoder maximum length of 50, and maximum gradient clipping of 5. We use Adam opt"
2020.emnlp-main.625,N18-2090,0,0.0265309,"Missing"
2020.emnlp-main.625,D18-1427,0,0.0254755,"Missing"
2020.emnlp-main.625,P17-1018,0,0.0606509,"Missing"
2020.emnlp-main.625,D15-1199,0,0.0744632,"Missing"
2020.emnlp-main.625,N18-1101,0,0.0117542,"atural language text describing the facts in the RDF data. Following Zhao et al. (2020), we serialize and reorder the RDF data as an intermediate planning setup, and feed the plan into a seq2seq model with attention and copy mechanism. Rewards. We use BLEU, ROUGE-L, and Entailment-Score (Pasunuru and Bansal, 2018) as rewards. Entailment-Score is calculated based on the probability that the generated sentence is classified as an entailment w.r.t. the ground truth.5 4 Each triple contains a subject, a predicate, and an object. We use a RoBERTa classifier (Liu et al., 2019b) trained on MultiNLI (Williams et al., 2018) as entailment scorer. 7771 5 Models BLEU-4 METEOR ROUGE-L Q-BLEU1 QPP QAP 46.39 46.64 46.68 46.65 49.01 49.52 49.50 49.72 28.83 29.09 30.10 30.03 54.25 55.07 55.50 57.60 46.75 46.80 46.84 49.66 50.02 50.01 30.03 30.15 30.07 56.51 56.92 56.78 BASELINES Cross-Entropy (Zhang and Bansal, 2019) ROUGE-RL QPP-RL QAP-RL 17.88 18.03 17.90 18.22 22.38 22.55 22.55 22.69 M ULTI -R EWARD M ODELS † Pasunuru and Bansal (2018) Our SM-Bandit† Our HM-Bandit† 18.36 18.68 18.55 22.55 22.88 22.82 Table 1: Performance of our baselines and multi-armed bandit-based models on question generation task. † denotes that"
2020.emnlp-main.625,W17-2603,0,0.036151,"Missing"
2020.emnlp-main.625,D19-1253,1,0.856162,"terpretable analyses of the learned bandit curriculum over the optimized rewards. 1 Introduction Recent advancements in end-to-end neural networks-based approaches have shown wide success in various sequence generation tasks: machine translation (Sutskever et al., 2014; Luong et al., 2015), dialogue systems (Vinyals and Le, 2015; Serban et al., 2016), textual summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), image/video captioning (Bahdanau et al., 2015; Venugopalan et al., 2015; Pasunuru and Bansal, 2017a), question generation (Du et al., 2017; Du and Cardie, 2018; Zhang and Bansal, 2019), etc. In all of these tasks, cross-entropy loss optimization has been widely used as a standard optimization approach (Sutskever et al., 2014), but this approach suffers from exposure-bias issue (Ranzato et al., 2016) and does not optimize for the non-differentiable automatic evaluation metrics that measure the quality of the generated sequence. Recent introduction of policy gradient-based reinforcement learning approaches address these issues for sequence generation tasks by directly optimizing the non-differentiable evaluation metrics (Zaremba and Sutskever, 2015; Ranzato et al., 2016; Renn"
2020.emnlp-main.625,D17-1062,0,0.11312,"entropy optimization, potentially with attention mechanism (Bahdanau et al., 2015) and pointer-copy mechanism (See et al., 2017; Gulcehre et al., 2016; Vinyals et al., 2015a; Merity et al., 2018), are widely used in language generation tasks such as machine translation (Sutskever et al., 2014; Luong et al., 2015), abstractive summarization (Chopra et al., 2016; Nallapati et al., 2016), question generation (Du et al., 2017; Zhang and Bansal, 2019), video/image captioning (Xu et al., 2015; Vinyals et al., 2015b; Pasunuru and Bansal, 2017a; Zhou et al., 2018), as well as sentence simplification (Zhang and Lapata, 2017; Guo et al., 2018). However, often the final metrics of interest are not differentiable, and thus not compatible with the standard maximum-likelihood based training. Motivated by this, recently there has been a surge in applications of reinforcement learning techniques to language generation (Ranzato et al., 2016), in which the gradients of non-differentiable metrics are approximated using the scoring function (REINFORCE (Williams, 1992)). A few successful examples include image captioning (Rennie et al., 2017; Ren et al., 2017), abstractive summarization (Paulus et al., 2018; Chen and Bansal"
2020.emnlp-main.625,D18-1424,0,0.0255266,"Missing"
2020.emnlp-main.659,D15-1075,0,\N,Missing
2020.emnlp-main.659,P18-2103,0,\N,Missing
2020.emnlp-main.659,K18-1007,0,\N,Missing
2020.emnlp-main.659,N18-1101,0,\N,Missing
2020.emnlp-main.659,D19-1333,0,\N,Missing
2020.emnlp-main.659,D19-1456,0,\N,Missing
2020.emnlp-main.659,W13-3502,0,\N,Missing
2020.emnlp-main.661,2020.acl-main.768,0,0.0622894,"ody of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean “and”, allowing them to assign labels automatically through simple templates. Also, their goal is to get BERT to master semantic fragments, which, as they mention, is achieved with a few minutes of additional finetuning on th"
2020.emnlp-main.661,D17-1215,0,0.0527754,"ions, and captures non-boolean usages. Adversarial Methods in NLP. Adversarial training for robustifying neural models has been 8242 Conjunctive Sentence Selection Conjuncts Identification “He is a Worcester resident and a member of the Democratic Family.” “a Worcester resident”, “a member of the Democratic Family” NLI Pair Creation (“He is a Worcester resident and a member of the Democratic Family.”, “He is a member of the Democratic Family.”) Manual Validation + Expert Annotation Entailment Figure 1: Flow diagram of C ONJ NLI dataset creation. proposed in many NLP tasks, most notably in QA (Jia and Liang, 2017; Wang and Bansal, 2018) and NLI (Nie et al., 2019). Nie et al. (2020) improve existing NLI stress tests using adversarially collected NLI data (ANLI) and Kaushik et al. (2020) use counter-factually augmented data for making models robust to spurious patterns. Following Jia and Liang (2017), we also create adversarial training data by performing all data creation steps except for the expensive human annotation. Our iterative adversarial fine-tuning method adapts adversarial training in a fine-tuning setup for BERT-style models and improves results on C ONJ NLI while maintaining performance on"
2020.emnlp-main.661,P17-1152,0,0.092706,"Missing"
2020.emnlp-main.661,N19-1423,0,0.0248108,"many conjunctions, each conjoining two or more conjuncts of varied syntactic categories such as noun phrases, verb phrases, prepositional phrases, clauses, etc. Besides syntax, conjunctions in English have a lot of semantics associated to them and different conjunctions (“and” vs “or”) affect the meaning of a sentence differently. Recent years have seen significant progress in the task of Natural Language Inference (NLI) through the development of large-scale datasets like SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). Although large-scale pre-trained language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have achieved super-human performances on these datasets, there have been concerns raised about these models exploiting idiosyncrasies in the data using tricks like pattern matching (McCoy et al., 2019). Thus, various stress-testing datasets have been proposed that probe NLI models for simple lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), numerical reasoning, antonymy and negation (Naik et al., 2018). However, despite the heavy usage of conjunctions in English, there is no specific NLI dataset that tests their understanding in de"
2020.emnlp-main.661,P16-1079,0,0.014383,"association to NLI. Conjunctions in English. There is a long history of analyzing the nuances of coordinating conjunctions in English and how these compare to boolean and non-boolean semantics (Gleitman, 1965; Keenan and Faltz, 2012). Linguistic studies have shown that noun phrase conjuncts in “and” do not always behave in a boolean manner (Massey, 1976; Hoeksema, 1988; Krifka, 1990). In the NLP community, studies on conjunctions have mostly been limited to treating it as a syntactic phenomenon. One of the popular tasks is that of conjunct boundary identification (Agarwal and Boggess, 1992). Ficler and Goldberg (2016a) show that state-of-the-art parsers often make mistakes in identifying conjuncts correctly and develop neural models to accomplish this (Ficler and Goldberg, 2016b; Teranishi et al., 2019). Saha and Mausam (2018) also identify conjuncts to break conjunctive sentences into simple ones for better downstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first"
2020.emnlp-main.661,D16-1003,0,0.0134438,"association to NLI. Conjunctions in English. There is a long history of analyzing the nuances of coordinating conjunctions in English and how these compare to boolean and non-boolean semantics (Gleitman, 1965; Keenan and Faltz, 2012). Linguistic studies have shown that noun phrase conjuncts in “and” do not always behave in a boolean manner (Massey, 1976; Hoeksema, 1988; Krifka, 1990). In the NLP community, studies on conjunctions have mostly been limited to treating it as a syntactic phenomenon. One of the popular tasks is that of conjunct boundary identification (Agarwal and Boggess, 1992). Ficler and Goldberg (2016a) show that state-of-the-art parsers often make mistakes in identifying conjuncts correctly and develop neural models to accomplish this (Ficler and Goldberg, 2016b; Teranishi et al., 2019). Saha and Mausam (2018) also identify conjuncts to break conjunctive sentences into simple ones for better downstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first"
2020.emnlp-main.661,P18-2103,0,0.0812352,"also identify conjuncts to break conjunctive sentences into simple ones for better downstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. Howeve"
2020.emnlp-main.661,N19-1225,0,0.132721,"Missing"
2020.emnlp-main.661,2021.ccl-1.108,0,0.158044,"Missing"
2020.emnlp-main.661,J93-2004,0,0.0706132,"Missing"
2020.emnlp-main.661,P19-1334,0,0.421273,"ng non-boolean usages of “and”, “or” and “but” in English. In the fifth example, the total time is a single entity and cannot be separated in an entailed hypothesis. In the sixth example, “or” is used as “exclusive-or” because the person began recording in either 1889 or 1890. We observe that state-of-the-art models such as BERT and RoBERTa, trained on existing datasets like SNLI and MNLI, often fail to make these inferences for our dataset. For example, BERT predicts entailment for the non-boolean “and” example #5 in Table 1 as well. This relates to the lexical overlap issue in these models (McCoy et al., 2019), since all the words in the hypothesis are also part of the 8241 premise for the example. Conjunctions are also challenging in the presence of negations. For example, a sentence of the form “not A or B” translates to “not A and not B”, as shown in example #8 of Table 1. Finally, a sentence may contain multiple conjunctions (with quantifiers), further adding to the complexity of the task (example #7 in Table 1). Thus, our C ONJ NLI dataset presents a new and interesting real-world challenge task for the community to work on and allow development of deeper NLI models. We also present some initi"
2020.emnlp-main.661,C18-1198,0,0.0317806,"zing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean “and”, allowing them to assign labels automatically through simple templates. Also, their goal is to get BERT to master semantic fragments, which, as they mention, is achieved with"
2020.emnlp-main.661,2020.acl-main.441,1,0.76003,"sarial training for robustifying neural models has been 8242 Conjunctive Sentence Selection Conjuncts Identification “He is a Worcester resident and a member of the Democratic Family.” “a Worcester resident”, “a member of the Democratic Family” NLI Pair Creation (“He is a Worcester resident and a member of the Democratic Family.”, “He is a member of the Democratic Family.”) Manual Validation + Expert Annotation Entailment Figure 1: Flow diagram of C ONJ NLI dataset creation. proposed in many NLP tasks, most notably in QA (Jia and Liang, 2017; Wang and Bansal, 2018) and NLI (Nie et al., 2019). Nie et al. (2020) improve existing NLI stress tests using adversarially collected NLI data (ANLI) and Kaushik et al. (2020) use counter-factually augmented data for making models robust to spurious patterns. Following Jia and Liang (2017), we also create adversarial training data by performing all data creation steps except for the expensive human annotation. Our iterative adversarial fine-tuning method adapts adversarial training in a fine-tuning setup for BERT-style models and improves results on C ONJ NLI while maintaining performance on existing datasets. 3 Data Creation Creation of C ONJ NLI involves four"
2020.emnlp-main.661,D18-1191,0,0.0630437,"Missing"
2020.emnlp-main.661,W18-5441,0,0.0498023,"Missing"
2020.emnlp-main.661,S18-2023,0,0.091725,"Missing"
2020.emnlp-main.661,K19-1033,0,0.0944565,"through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean “and”, allowing them to assign labels automatically through simple templates. Also, their goal is to get BERT to master semantic fragments, w"
2020.emnlp-main.661,N18-1179,0,0.161316,"ownstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean “and”, allowing them to assign labels automaticall"
2020.emnlp-main.661,N19-1343,0,0.0122414,"s (Gleitman, 1965; Keenan and Faltz, 2012). Linguistic studies have shown that noun phrase conjuncts in “and” do not always behave in a boolean manner (Massey, 1976; Hoeksema, 1988; Krifka, 1990). In the NLP community, studies on conjunctions have mostly been limited to treating it as a syntactic phenomenon. One of the popular tasks is that of conjunct boundary identification (Agarwal and Boggess, 1992). Ficler and Goldberg (2016a) show that state-of-the-art parsers often make mistakes in identifying conjuncts correctly and develop neural models to accomplish this (Ficler and Goldberg, 2016b; Teranishi et al., 2019). Saha and Mausam (2018) also identify conjuncts to break conjunctive sentences into simple ones for better downstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing"
2020.emnlp-main.661,N18-2091,1,0.806762,"n-boolean usages. Adversarial Methods in NLP. Adversarial training for robustifying neural models has been 8242 Conjunctive Sentence Selection Conjuncts Identification “He is a Worcester resident and a member of the Democratic Family.” “a Worcester resident”, “a member of the Democratic Family” NLI Pair Creation (“He is a Worcester resident and a member of the Democratic Family.”, “He is a member of the Democratic Family.”) Manual Validation + Expert Annotation Entailment Figure 1: Flow diagram of C ONJ NLI dataset creation. proposed in many NLP tasks, most notably in QA (Jia and Liang, 2017; Wang and Bansal, 2018) and NLI (Nie et al., 2019). Nie et al. (2020) improve existing NLI stress tests using adversarially collected NLI data (ANLI) and Kaushik et al. (2020) use counter-factually augmented data for making models robust to spurious patterns. Following Jia and Liang (2017), we also create adversarial training data by performing all data creation steps except for the expensive human annotation. Our iterative adversarial fine-tuning method adapts adversarial training in a fine-tuning setup for BERT-style models and improves results on C ONJ NLI while maintaining performance on existing datasets. 3 Dat"
2020.emnlp-main.661,I17-1100,0,0.0410693,"Missing"
2020.emnlp-main.661,N18-1101,0,0.361351,"ub.com/swarnaHub/ConjNLI. tences more realistic and challenging. A sentence can have many conjunctions, each conjoining two or more conjuncts of varied syntactic categories such as noun phrases, verb phrases, prepositional phrases, clauses, etc. Besides syntax, conjunctions in English have a lot of semantics associated to them and different conjunctions (“and” vs “or”) affect the meaning of a sentence differently. Recent years have seen significant progress in the task of Natural Language Inference (NLI) through the development of large-scale datasets like SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). Although large-scale pre-trained language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have achieved super-human performances on these datasets, there have been concerns raised about these models exploiting idiosyncrasies in the data using tricks like pattern matching (McCoy et al., 2019). Thus, various stress-testing datasets have been proposed that probe NLI models for simple lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), numerical reasoning, antonymy and negation (Naik et al., 2018). However, despite the heavy usage of conjunctions"
2020.emnlp-main.661,D19-1228,0,0.0646246,"owever, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean “and”, allowing them to assign labels automatically through simple templates. Also, their goa"
2020.emnlp-main.661,2020.acl-main.543,0,0.0825053,"models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean “and”, allowing them to assign labels automatically through simple templates. Also, their goal is to get BERT to master semantic fragments, which, as they mention, is achieved with a few minutes of additional finetuning on their templated data. C ONJ NLI, however, is more diverse"
2020.emnlp-main.661,C18-1194,1,0.927729,"and Faltz, 2012). Linguistic studies have shown that noun phrase conjuncts in “and” do not always behave in a boolean manner (Massey, 1976; Hoeksema, 1988; Krifka, 1990). In the NLP community, studies on conjunctions have mostly been limited to treating it as a syntactic phenomenon. One of the popular tasks is that of conjunct boundary identification (Agarwal and Boggess, 1992). Ficler and Goldberg (2016a) show that state-of-the-art parsers often make mistakes in identifying conjuncts correctly and develop neural models to accomplish this (Ficler and Goldberg, 2016b; Teranishi et al., 2019). Saha and Mausam (2018) also identify conjuncts to break conjunctive sentences into simple ones for better downstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI. Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences ("
2020.emnlp-main.661,2020.emnlp-main.659,1,0.669565,"Ta-style models to learn the heuristics used to create the adversarial data. We experiment with the IAFT model on C ONJ NLI dev and linearly increase the data size from 6k to 18k, comprising of an equal amount of “and”, “or” and “but” examples. Figure 3 shows the accuracy curve. We obtain maximum improvements with the first 12k examples (4 points), marginal improvement with the next 3k and a slight drop in performance with the next 3k. Early saturation shows that RoBERTa learns the rules using a small number of examples only and also exposes the hardness of C ONJ NLI. 6.5 Instability Analysis Zhou et al. (2020) perform an in-depth analysis of the various NLI stress tests like HANS (McCoy et al., 2019), BREAK-NLI (Glockner et al., 2018), etc and find that different random initializa8247 70 69.18 68.53 CONJ DEV ACCURACY 69 68.86 67.41 68 RoBERTa PA IAFT 66.45 67 66 65 64.68 And Or But 65.36 66.29 67.59 59.87 60.93 62.20 81.48 81.48 80.00 Multiple 65.93 66.81 62.88 All 65.60 66.30 67.90 64 63 Table 9: Comparison of all models on the subset of each conjunction type of C ONJ NLI. 62 61 60 0 6000 9000 12000 15000 18000 NUMBER OF TRAINING EXAMPLES Figure 3: Effect of amount of adversarial training data. ti"
2020.emnlp-main.706,2020.acl-main.233,1,0.916225,"rial data collection and adversarial matching. This helps mitigate potential annotation artifacts and biases in the dataset. A detailed analysis of VLEP is provided. (3) We present a strong baseline method to benchmark the proposed dataset, and show that incorporating commonsense knowledge improves performance, indicating future directions for this new task (with a large model-human performance gap). 2 Related Work Video-and-Language Understanding. Various datasets and tasks have been introduced in this area, such as video captioning (Xu et al., 2016; Rohrbach et al., 2017; Wang et al., 2019; Lei et al., 2020c), video QA (Tapaswi et al., 2016; Jang et al., 2017; Lei et al., 2018), and moment retrieval (Hendricks et al., 2017; Gao et al., 2017; Lei et al., 2020c). Recently, Liu et al. (2020) propose the video-andlanguage inference task where a model needs to infer whether a statement is entailed or contradicted by a video. While this task requires judging a statement’s verification w.r.t. existing events, our task requires predicting future events. Commonsense Reasoning. Recently, commonsense reasoning has emerged as an important topic in both the language (Zellers et al., 2018, 2019b; Sap et al.,"
2020.emnlp-main.706,D18-1167,1,0.810875,"ntial annotation artifacts and biases in the dataset. A detailed analysis of VLEP is provided. (3) We present a strong baseline method to benchmark the proposed dataset, and show that incorporating commonsense knowledge improves performance, indicating future directions for this new task (with a large model-human performance gap). 2 Related Work Video-and-Language Understanding. Various datasets and tasks have been introduced in this area, such as video captioning (Xu et al., 2016; Rohrbach et al., 2017; Wang et al., 2019; Lei et al., 2020c), video QA (Tapaswi et al., 2016; Jang et al., 2017; Lei et al., 2018), and moment retrieval (Hendricks et al., 2017; Gao et al., 2017; Lei et al., 2020c). Recently, Liu et al. (2020) propose the video-andlanguage inference task where a model needs to infer whether a statement is entailed or contradicted by a video. While this task requires judging a statement’s verification w.r.t. existing events, our task requires predicting future events. Commonsense Reasoning. Recently, commonsense reasoning has emerged as an important topic in both the language (Zellers et al., 2018, 2019b; Sap et al., 2019) and vision (Vedantam et al., 2015b; Zellers et al., 2019a; Zadeh e"
2020.emnlp-main.706,2020.acl-main.730,1,0.8664,"rial data collection and adversarial matching. This helps mitigate potential annotation artifacts and biases in the dataset. A detailed analysis of VLEP is provided. (3) We present a strong baseline method to benchmark the proposed dataset, and show that incorporating commonsense knowledge improves performance, indicating future directions for this new task (with a large model-human performance gap). 2 Related Work Video-and-Language Understanding. Various datasets and tasks have been introduced in this area, such as video captioning (Xu et al., 2016; Rohrbach et al., 2017; Wang et al., 2019; Lei et al., 2020c), video QA (Tapaswi et al., 2016; Jang et al., 2017; Lei et al., 2018), and moment retrieval (Hendricks et al., 2017; Gao et al., 2017; Lei et al., 2020c). Recently, Liu et al. (2020) propose the video-andlanguage inference task where a model needs to infer whether a statement is entailed or contradicted by a video. While this task requires judging a statement’s verification w.r.t. existing events, our task requires predicting future events. Commonsense Reasoning. Recently, commonsense reasoning has emerged as an important topic in both the language (Zellers et al., 2018, 2019b; Sap et al.,"
2020.emnlp-main.706,W04-1013,0,0.0584468,"Missing"
2020.emnlp-main.706,2020.acl-main.441,1,0.722692,"te more reasonable future event that ground to the premise event,2 we also require them to provide a rationale as to why it is more or less likely. As it is not the focus of this work, we will release these rationales to support research on textual explanation generation/classification tasks (Huk Park et al., 2018; Zellers et al., 2019a). Each collected example is verified by three human verifiers, by ranking the future events conditioned on the premise event. We only accept an example if at least three out of four (one writer + three verifiers) reach an agreement, as Hendricks et al. (2017); Nie et al. (2020). In addition, we also discard examples if one of the verifiers thinks the events are against our instructions (e.g., wrong person reference). In total, we collected 6,458 veri2 Otherwise, workers sometimes write random events that are not related to the given premise. Type: Negation Premise Summary: Amy picks up her phone and reads a text message. More-likely: Amy tells her friends what the text message says. Less-likely: Amy says nothing at all to her friends. Type: Impolite Actions Premise Summary: Chandler finds out that Joey used his toothbrush. More-likely: Chandler starts arguing with J"
2020.emnlp-main.706,P02-1040,0,0.105947,"Missing"
2020.emnlp-main.706,L18-1239,0,0.0282337,"019a; Zadeh et al., 2019; Fang et al., 2020) communities. Zellers et al. (2018, 2019b) 8770 build multiple-choice QA datasets for commonsense inference with text context, Zellers et al. (2019a); Park et al. (2020) propose datasets for commonsense-based QA and captioning on still images. In this work, we focus on commonsense reasoning with a more complex type of context: video with dialogue, posing challenges for both video understanding and commonsense reasoning. Bias in Datasets. It is known that biases or annotation artifacts (Goyal et al., 2017; Gururangan et al., 2018; McCoy et al., 2019; Tsuchiya, 2018; Poliak et al., 2018; Zellers et al., 2019a) exist in standard human annotated datasets (Bowman et al., 2015; Williams et al., 2018; Antol et al., 2015; Tapaswi et al., 2016; Jang et al., 2017; Kim et al., 2017; Lei et al., 2018). For example, negation words such as nobody, no and never are strong indicators of contradictions (Gururangan et al., 2018) in MNLI (Williams et al., 2018). Such superficial patterns are easy for models to exploit, resulting in an overestimate of task performance (Goyal et al., 2017; Gururangan et al., 2018). Zellers et al. (2019a) propose Adversarial Matching to mit"
2020.emnlp-main.706,N18-1101,0,0.196486,"for commonsense inference with text context, Zellers et al. (2019a); Park et al. (2020) propose datasets for commonsense-based QA and captioning on still images. In this work, we focus on commonsense reasoning with a more complex type of context: video with dialogue, posing challenges for both video understanding and commonsense reasoning. Bias in Datasets. It is known that biases or annotation artifacts (Goyal et al., 2017; Gururangan et al., 2018; McCoy et al., 2019; Tsuchiya, 2018; Poliak et al., 2018; Zellers et al., 2019a) exist in standard human annotated datasets (Bowman et al., 2015; Williams et al., 2018; Antol et al., 2015; Tapaswi et al., 2016; Jang et al., 2017; Kim et al., 2017; Lei et al., 2018). For example, negation words such as nobody, no and never are strong indicators of contradictions (Gururangan et al., 2018) in MNLI (Williams et al., 2018). Such superficial patterns are easy for models to exploit, resulting in an overestimate of task performance (Goyal et al., 2017; Gururangan et al., 2018). Zellers et al. (2019a) propose Adversarial Matching to mitigate biases in QA, where positive answers are recycled to serve as negatives for other questions. Nie et al. (2020) propose a Human"
2020.emnlp-main.706,S18-2023,0,0.0333603,"Missing"
2020.emnlp-main.706,P18-2124,0,0.0692574,"Missing"
2020.emnlp-main.706,D18-1009,0,0.0219149,"2017; Wang et al., 2019; Lei et al., 2020c), video QA (Tapaswi et al., 2016; Jang et al., 2017; Lei et al., 2018), and moment retrieval (Hendricks et al., 2017; Gao et al., 2017; Lei et al., 2020c). Recently, Liu et al. (2020) propose the video-andlanguage inference task where a model needs to infer whether a statement is entailed or contradicted by a video. While this task requires judging a statement’s verification w.r.t. existing events, our task requires predicting future events. Commonsense Reasoning. Recently, commonsense reasoning has emerged as an important topic in both the language (Zellers et al., 2018, 2019b; Sap et al., 2019) and vision (Vedantam et al., 2015b; Zellers et al., 2019a; Zadeh et al., 2019; Fang et al., 2020) communities. Zellers et al. (2018, 2019b) 8770 build multiple-choice QA datasets for commonsense inference with text context, Zellers et al. (2019a); Park et al. (2020) propose datasets for commonsense-based QA and captioning on still images. In this work, we focus on commonsense reasoning with a more complex type of context: video with dialogue, posing challenges for both video understanding and commonsense reasoning. Bias in Datasets. It is known that biases or annotat"
2020.emnlp-main.706,P19-1472,0,0.0130675,"t al., 2017; Lei et al., 2018), and moment retrieval (Hendricks et al., 2017; Gao et al., 2017; Lei et al., 2020c). Recently, Liu et al. (2020) propose the video-andlanguage inference task where a model needs to infer whether a statement is entailed or contradicted by a video. While this task requires judging a statement’s verification w.r.t. existing events, our task requires predicting future events. Commonsense Reasoning. Recently, commonsense reasoning has emerged as an important topic in both the language (Zellers et al., 2018, 2019b; Sap et al., 2019) and vision (Vedantam et al., 2015b; Zellers et al., 2019a; Zadeh et al., 2019; Fang et al., 2020) communities. Zellers et al. (2018, 2019b) 8770 build multiple-choice QA datasets for commonsense inference with text context, Zellers et al. (2019a); Park et al. (2020) propose datasets for commonsense-based QA and captioning on still images. In this work, we focus on commonsense reasoning with a more complex type of context: video with dialogue, posing challenges for both video understanding and commonsense reasoning. Bias in Datasets. It is known that biases or annotation artifacts (Goyal et al., 2017; Gururangan et al., 2018; McCoy et al., 2019; Tsu"
2020.emnlp-main.734,N13-1062,0,0.0299804,"ALBERT, DistilBERT, and BART. 3 We measure the Jensen-Shannon Distance (JSD) and the Kullback–Leibler (KL) divergence between model softmax outputs and the estimated distribution over human annotations. The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ ChaosNLI 2 Related Work Uncertainty of Annotations. Past discussions of human disagreement on semantic annotation tasks were mostly focused on the uncertainty of individual annotators and the noisiness of the data collection process. These tasks include word sense disambiguation (Erk and McCarthy, 2009; Jurgens, 2013), coreference (Versley, 2008), frame corpus collection (Dumitrache et al., 2019), anaphora resolution (Poesio and Artstein, 2005; Poesio et al., 2019), entity linking (Reidsma and op den Akker, 2008), tagging and parsing (Plank et al., 2014; Alonso et al., 2015), and veridicality (De Marneffe et al., 2012; Karttunen et al., 2014). These works focused on studying the ambiguity of annotations, how the design of the annotation setup might affect the inter-annotator-agreement, and how to make the annotations reliable. However, we consider the disagreements and subjectivity to be an intrinsic prope"
2020.emnlp-main.734,2020.acl-main.703,0,0.0136153,"frequency of the label. Majority labels were marked in bold. 5.1 Models and Setup Following the pretraining-then-finetuning trend, we focus our experiments on large-scale language pretraining models. We studied BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) since they are considered to be the state-of-the-art models for learning textual representations and have been used for a variety of downstream tasks. We experimented on both the base and the large versions of these models, in order to analyze the parameter size factor. Additionally, we include BART (Lewis et al., 2020), ALBERT (Lan et al., 2019), and DistilBERT (Sanh et al., 2019) in the experiments. ALBERT is designed to reduce parameters of BERT by crosslayer parameter sharing and decomposing embedding. DistilBERT aims to compress BERT with knowledge distillation. BART is a denoising autoencoder for pretraining seq-to-seq models. For NLI, we trained the models on a combined training set of SNLI and MNLI which contains over 900k NLI pairs. We used the best hyper-parameters chosen by their original authors. For αNLI, we trained the models on αNLI training set (169,654 Evaluation and Metrics As formulated in"
2020.emnlp-main.734,2021.ccl-1.108,0,0.0648402,"Missing"
2020.emnlp-main.734,D17-2014,0,0.0347694,"about some examples, just choose the best category you believe the statement should be in. Data Collection Our goal is to gather annotations from multiple annotators to estimate the distribution over human opinions. Section 3.1 and 3.2 state some details of the collection. More importantly, Section 3.3 explains the challenges of such data collection and how our designs ensure data quality. 3.1 Annotation Interface To collect multiple labels for each example, we employed crowdsourced workers from Mechanical Turker with qualifications. The annotation interface is implemented using the ParlAI5 (Miller et al., 2017) framework. The collection is embodied in a multi-round interactive environment where at each round annotators are instructed to do one single multi-choice selection. This reduces the annotators’ mental load and helps them focus on the human intelligence tasks (HITs). The compressed versions of instructions are shown in Figure 1. Screenshots of Turker interfaces are attached in Appendix A. 3.3 Given two observations (O-Beginning and O-Ending), and two hypotheses (H1 and H2), your goal is to choose one of the hypotheses that is more likely to cause O-Beginning to turn into O-Ending. An automati"
2020.emnlp-main.734,P19-1449,0,0.0390676,"Missing"
2020.emnlp-main.734,2020.acl-main.441,1,0.732563,"1 Introduction Natural Language Understanding (NLU) evaluation plays a key role in benchmarking progress in natural language processing (NLP) research. With the recent advance in language representative learning (Devlin et al., 2019), results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including 1 The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ChaosNLI Natural Language Inference (e.g., SNLI, MNLI and ANLI) (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020), Grounded Commonsense Inference (Zellers et al., 2018), Commonsense QA (Talmor et al., 2019), Social Interactions Reasoning (Sap et al., 2019), Abductive Commonsense Reasoning (αNLI) (Bhagavatula et al., 2020), etc. One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are s"
2020.emnlp-main.734,Q19-1043,0,0.373381,"sumption might be true in human educational settings where prescriptivism is preferred over descriptivism because the goal is to test humans with well-defined knowledge or norms (Trask, 1999). However, it is not true for many NLP tasks due to their pragmatic nature where the meaning of the same sentence might differ depending on the context or background knowledge. Specifically for the NLI task, Manning (2006) advocate that annotation tasks should be “natural” for untrained annotators, and the role of NLP should be to model the inferences that humans make in practical settings. Previous work (Pavlick and Kwiatkowski, 2019) that uses a graded labeling schema on NLI, showed that there are inherent disagreements in inference tasks. All these discussions challenge the commonly used majority “gold-label” practice in most prior data collections and evaluations. Intuitively, such disagreements among humans 9131 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9131–9143, c November 16–20, 2020. 2020 Association for Computational Linguistics should be allowed because different annotators might have different subjective views of the world and might think differently when they"
2020.emnlp-main.734,E14-1078,0,0.0307154,"ntal scripts are available at https://github.com/easonnie/ ChaosNLI 2 Related Work Uncertainty of Annotations. Past discussions of human disagreement on semantic annotation tasks were mostly focused on the uncertainty of individual annotators and the noisiness of the data collection process. These tasks include word sense disambiguation (Erk and McCarthy, 2009; Jurgens, 2013), coreference (Versley, 2008), frame corpus collection (Dumitrache et al., 2019), anaphora resolution (Poesio and Artstein, 2005; Poesio et al., 2019), entity linking (Reidsma and op den Akker, 2008), tagging and parsing (Plank et al., 2014; Alonso et al., 2015), and veridicality (De Marneffe et al., 2012; Karttunen et al., 2014). These works focused on studying the ambiguity of annotations, how the design of the annotation setup might affect the inter-annotator-agreement, and how to make the annotations reliable. However, we consider the disagreements and subjectivity to be an intrinsic property of the populations. Our work discusses the disagreements among a large group of individuals, and further examines the relation between the annotation disagreement and the model performance. Disagreements in NLI Annotations. Our work is"
2020.emnlp-main.734,W05-0311,0,0.060868,"between model softmax outputs and the estimated distribution over human annotations. The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ ChaosNLI 2 Related Work Uncertainty of Annotations. Past discussions of human disagreement on semantic annotation tasks were mostly focused on the uncertainty of individual annotators and the noisiness of the data collection process. These tasks include word sense disambiguation (Erk and McCarthy, 2009; Jurgens, 2013), coreference (Versley, 2008), frame corpus collection (Dumitrache et al., 2019), anaphora resolution (Poesio and Artstein, 2005; Poesio et al., 2019), entity linking (Reidsma and op den Akker, 2008), tagging and parsing (Plank et al., 2014; Alonso et al., 2015), and veridicality (De Marneffe et al., 2012; Karttunen et al., 2014). These works focused on studying the ambiguity of annotations, how the design of the annotation setup might affect the inter-annotator-agreement, and how to make the annotations reliable. However, we consider the disagreements and subjectivity to be an intrinsic property of the populations. Our work discusses the disagreements among a large group of individuals, and further examines the relati"
2020.emnlp-main.734,N19-1176,0,0.0249302,"uts and the estimated distribution over human annotations. The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ ChaosNLI 2 Related Work Uncertainty of Annotations. Past discussions of human disagreement on semantic annotation tasks were mostly focused on the uncertainty of individual annotators and the noisiness of the data collection process. These tasks include word sense disambiguation (Erk and McCarthy, 2009; Jurgens, 2013), coreference (Versley, 2008), frame corpus collection (Dumitrache et al., 2019), anaphora resolution (Poesio and Artstein, 2005; Poesio et al., 2019), entity linking (Reidsma and op den Akker, 2008), tagging and parsing (Plank et al., 2014; Alonso et al., 2015), and veridicality (De Marneffe et al., 2012; Karttunen et al., 2014). These works focused on studying the ambiguity of annotations, how the design of the annotation setup might affect the inter-annotator-agreement, and how to make the annotations reliable. However, we consider the disagreements and subjectivity to be an intrinsic property of the populations. Our work discusses the disagreements among a large group of individuals, and further examines the relation between the annotat"
2020.emnlp-main.734,N18-1179,0,0.0242205,"et al., 2018), Commonsense QA (Talmor et al., 2019), Social Interactions Reasoning (Sap et al., 2019), Abductive Commonsense Reasoning (αNLI) (Bhagavatula et al., 2020), etc. One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are studied and then weaknesses are diagnosed (Sanchez et al., 2018). Despite the straightforwardness of this formalization, one assumption behind most prior benchmark data sourcing is that there exists a single prescriptive ground truth label for each example. The assumption might be true in human educational settings where prescriptivism is preferred over descriptivism because the goal is to test humans with well-defined knowledge or norms (Trask, 1999). However, it is not true for many NLP tasks due to their pragmatic nature where the meaning of the same sentence might differ depending on the context or background knowledge. Specifically for the NLI task, M"
2020.emnlp-main.734,D19-1454,0,0.0184842,"research. With the recent advance in language representative learning (Devlin et al., 2019), results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including 1 The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ChaosNLI Natural Language Inference (e.g., SNLI, MNLI and ANLI) (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020), Grounded Commonsense Inference (Zellers et al., 2018), Commonsense QA (Talmor et al., 2019), Social Interactions Reasoning (Sap et al., 2019), Abductive Commonsense Reasoning (αNLI) (Bhagavatula et al., 2020), etc. One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are studied and then weaknesses are diagnosed (Sanchez et al., 2018). Despite the straightforwardness of this formalization, one assumption behind m"
2020.emnlp-main.734,N19-1421,0,0.0269907,"arking progress in natural language processing (NLP) research. With the recent advance in language representative learning (Devlin et al., 2019), results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including 1 The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ChaosNLI Natural Language Inference (e.g., SNLI, MNLI and ANLI) (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020), Grounded Commonsense Inference (Zellers et al., 2018), Commonsense QA (Talmor et al., 2019), Social Interactions Reasoning (Sap et al., 2019), Abductive Commonsense Reasoning (αNLI) (Bhagavatula et al., 2020), etc. One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are studied and then weaknesses are diagnosed (Sanchez et al., 2018). Despite the straightforwardn"
2020.emnlp-main.734,N18-1101,0,0.373083,"ective human opinions.1 1 Introduction Natural Language Understanding (NLU) evaluation plays a key role in benchmarking progress in natural language processing (NLP) research. With the recent advance in language representative learning (Devlin et al., 2019), results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including 1 The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ChaosNLI Natural Language Inference (e.g., SNLI, MNLI and ANLI) (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020), Grounded Commonsense Inference (Zellers et al., 2018), Commonsense QA (Talmor et al., 2019), Social Interactions Reasoning (Sap et al., 2019), Abductive Commonsense Reasoning (αNLI) (Bhagavatula et al., 2020), etc. One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors"
2020.emnlp-main.734,D18-1009,0,0.0321553,") evaluation plays a key role in benchmarking progress in natural language processing (NLP) research. With the recent advance in language representative learning (Devlin et al., 2019), results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including 1 The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ChaosNLI Natural Language Inference (e.g., SNLI, MNLI and ANLI) (Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020), Grounded Commonsense Inference (Zellers et al., 2018), Commonsense QA (Talmor et al., 2019), Social Interactions Reasoning (Sap et al., 2019), Abductive Commonsense Reasoning (αNLI) (Bhagavatula et al., 2020), etc. One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are studied and then weaknesses are diagnosed (Sanchez et al"
2020.emnlp-main.9,D19-1608,0,0.115628,"Missing"
2020.emnlp-main.9,P19-1176,0,0.0232416,"ain of reasoning, starting from language. We use a maxflow ILP formulation for checking proof graph connectivity (Even and Tarjan, 1975). Multiple approaches for NLP tasks such as sentiment analysis and content selection (Pang and Lee, 2004; Barzilay and Lapata, 2005; Bansal et al., 2008) have been framed as optimal flow problems on graphs. Program Synthesis with Transformers: Existing works show that transformers already capture some knowledge from pre-training for algorithm emulation (Talmor et al., 2019) or can be fine-tuned for tasks like semantic parsing (He and Choi, 2020), translation (Wang et al., 2019), symbolic integration (Lample and Charton, 2020) and mathematics (Saxton et al., 2019). In our work, we also employ a transformer-based pre-trained language model (RoBERTa (Liu et al., 2019b)) but for the downstream task of rule-based reasoning. 3 {T rue, F alse} and generates a proof P. 3.1 Proof Representation A proof, P = (N , E), is a directed graph with nodes n ∈ N and edges e ∈ E. Each node is either a fact f ∈ F , a rule r ∈ R or a special NAF node (Negation As Failure, as described below). Edges in the proof are directed either from a fact (or NAF ) to a rule or from a rule to another"
2020.emnlp-main.9,P19-1618,0,0.0952676,"Missing"
2020.fever-1.1,P17-1001,0,0.0203097,"nd the Multi-Genre Natural Language Inference Corpus (Williams et al., 2018). This task can be formalized as a semantic sequence matching task, which bears resemblance to both the sentence retrieval and claim verification tasks. 2.4 Multi-Task Learning Multi-task learning (MTL) (Caruana, 1997) has been successfully used to merge Natural Language Processing tasks (Luong et al., 2016; Hashimoto et al., 2017; Dong et al., 2015) for improved performance. Parameter sharing, in particular sharing of certain structures such as label spaces, has been used widely in several NLP tasks for this purpose (Liu et al., 2017; Søgaard and Goldberg, 2016). Zhao et al. (2018) used a multi-task learning setup for FEVER that shared certain layers between sentence selection and claim verification modules. Augenstein et al. (2018) used shared label spaces in MTL for sequence classification. Following this work, Augenstein et al. (2019) used shared label spaces for automatic fact checking. However, the labels involved in this work were limited to claim verification labels only, and did not incorporate sentence selection as we do in this paper. Related Work Previous FEVER Systems Many of the top performing FEVER 1.0 syste"
2020.fever-1.1,D19-1475,0,0.0437011,"Missing"
2020.fever-1.1,N18-1172,0,0.0365967,"Missing"
2020.fever-1.1,D19-1258,1,0.884231,"Missing"
2020.fever-1.1,D15-1075,0,0.020969,"functional state much faster than the pipeline-trained system. We argue that the aforementioned design simplification and training acceleration are valuable especially during time-sensitive application development. 2 2.1 Natural Language Inference (NLI) requires a system to classify the logical relationship between two sentences in which one is the premise and one is the hypothesis. This classifier decides whether the relationship is entailment, contradiction, or neutral. Several large-scale datasets have been created for this purpose, including the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre Natural Language Inference Corpus (Williams et al., 2018). This task can be formalized as a semantic sequence matching task, which bears resemblance to both the sentence retrieval and claim verification tasks. 2.4 Multi-Task Learning Multi-task learning (MTL) (Caruana, 1997) has been successfully used to merge Natural Language Processing tasks (Luong et al., 2016; Hashimoto et al., 2017; Dong et al., 2015) for improved performance. Parameter sharing, in particular sharing of certain structures such as label spaces, has been used widely in several NLP tasks for this purpose"
2020.fever-1.1,C18-1287,0,0.0453031,"Missing"
2020.fever-1.1,D19-1261,0,0.0235746,"up. 2.2 Natural Language Inference 2.5 Fake News Detection In addition to the FEVER shared task, other recent work in fake news detection has focused on several aspects of data collection and statement verification. Shu et al. (2019b) looked into the role of social context in fake news detection. Additionally, Shu et al. (2019a) also explored creating explainable fake news detection. Information Retrieval Neural networks have been successfully applied to information retrieval tasks in Natural Language Processing (Huang et al., 2013; Guo et al., 2016; Mitra et al., 2017; Dehghani et al., 2017; Qi et al., 2019; Nie et al., 2019b) with a focus on relevant retrieval. Information retrieval is generally a relevance-matching task whereas claim verification is a more semantics-intensive task. We consider using a single semantics-focused model to conduct both sentence retrieval and claim verification. 3 3.1 Model Sequence Matching Model Sentence selection and claim verification can be easily structured as the same sequence matching problem in which the input is a pair of textual sequences and the output is a semantic relationship label for the pair. Nie et al. (2019a) proposed using 2 claim textual sequen"
2020.fever-1.1,P15-1166,0,0.0893742,"ip is entailment, contradiction, or neutral. Several large-scale datasets have been created for this purpose, including the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre Natural Language Inference Corpus (Williams et al., 2018). This task can be formalized as a semantic sequence matching task, which bears resemblance to both the sentence retrieval and claim verification tasks. 2.4 Multi-Task Learning Multi-task learning (MTL) (Caruana, 1997) has been successfully used to merge Natural Language Processing tasks (Luong et al., 2016; Hashimoto et al., 2017; Dong et al., 2015) for improved performance. Parameter sharing, in particular sharing of certain structures such as label spaces, has been used widely in several NLP tasks for this purpose (Liu et al., 2017; Søgaard and Goldberg, 2016). Zhao et al. (2018) used a multi-task learning setup for FEVER that shared certain layers between sentence selection and claim verification modules. Augenstein et al. (2018) used shared label spaces in MTL for sequence classification. Following this work, Augenstein et al. (2019) used shared label spaces for automatic fact checking. However, the labels involved in this work were"
2020.fever-1.1,P16-2038,0,0.0245761,"Natural Language Inference Corpus (Williams et al., 2018). This task can be formalized as a semantic sequence matching task, which bears resemblance to both the sentence retrieval and claim verification tasks. 2.4 Multi-Task Learning Multi-task learning (MTL) (Caruana, 1997) has been successfully used to merge Natural Language Processing tasks (Luong et al., 2016; Hashimoto et al., 2017; Dong et al., 2015) for improved performance. Parameter sharing, in particular sharing of certain structures such as label spaces, has been used widely in several NLP tasks for this purpose (Liu et al., 2017; Søgaard and Goldberg, 2016). Zhao et al. (2018) used a multi-task learning setup for FEVER that shared certain layers between sentence selection and claim verification modules. Augenstein et al. (2018) used shared label spaces in MTL for sequence classification. Following this work, Augenstein et al. (2019) used shared label spaces for automatic fact checking. However, the labels involved in this work were limited to claim verification labels only, and did not incorporate sentence selection as we do in this paper. Related Work Previous FEVER Systems Many of the top performing FEVER 1.0 systems, all achieving greater tha"
2020.fever-1.1,C18-1158,0,0.118053,"erification Yixin Nie∗ Lisa Bauer∗ Mohit Bansal UNC Chapel Hill {yixin1, lbauer6, mbansal}@cs.unc.edu Abstract 2019a,b). The Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018a) is the most recent large-scale dataset that enables the development of data-driven neural approaches to the automatic fact checking task. Additionally, the FEVER Shared Task (Thorne et al., 2018b) introduced a benchmark, the first of this kind, that is capable of evaluating both evidence retrieval and claim verification. Several top-ranked approaches on FEVER (Nie et al., 2019a; Yoneda et al., 2018; Hanselowski et al., 2018b) decompose the task into 3 subtasks: document retrieval, sentence selection, and claim verification, and follow a similar pipeline training setup where sub-components are developed and trained sequentially. Although achieving higher scores on benchmarks, pipeline training is timeconsuming and imposes difficulty for fast application development since downstream training relies on data provided by a fully-converged upstream component. The impossibility of parallelization also causes data-inefficiency as training the same input sentence for both sentence selection and claim verification require"
2020.fever-1.1,N18-1074,0,0.0537085,"Missing"
2020.fever-1.1,W18-5516,0,0.0857292,"erification Yixin Nie∗ Lisa Bauer∗ Mohit Bansal UNC Chapel Hill {yixin1, lbauer6, mbansal}@cs.unc.edu Abstract 2019a,b). The Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018a) is the most recent large-scale dataset that enables the development of data-driven neural approaches to the automatic fact checking task. Additionally, the FEVER Shared Task (Thorne et al., 2018b) introduced a benchmark, the first of this kind, that is capable of evaluating both evidence retrieval and claim verification. Several top-ranked approaches on FEVER (Nie et al., 2019a; Yoneda et al., 2018; Hanselowski et al., 2018b) decompose the task into 3 subtasks: document retrieval, sentence selection, and claim verification, and follow a similar pipeline training setup where sub-components are developed and trained sequentially. Although achieving higher scores on benchmarks, pipeline training is timeconsuming and imposes difficulty for fast application development since downstream training relies on data provided by a fully-converged upstream component. The impossibility of parallelization also causes data-inefficiency as training the same input sentence for both sentence selection and claim verification require"
2020.fever-1.1,W18-5501,0,0.176838,"Missing"
2020.fever-1.1,N18-1101,0,0.0176287,"the aforementioned design simplification and training acceleration are valuable especially during time-sensitive application development. 2 2.1 Natural Language Inference (NLI) requires a system to classify the logical relationship between two sentences in which one is the premise and one is the hypothesis. This classifier decides whether the relationship is entailment, contradiction, or neutral. Several large-scale datasets have been created for this purpose, including the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre Natural Language Inference Corpus (Williams et al., 2018). This task can be formalized as a semantic sequence matching task, which bears resemblance to both the sentence retrieval and claim verification tasks. 2.4 Multi-Task Learning Multi-task learning (MTL) (Caruana, 1997) has been successfully used to merge Natural Language Processing tasks (Luong et al., 2016; Hashimoto et al., 2017; Dong et al., 2015) for improved performance. Parameter sharing, in particular sharing of certain structures such as label spaces, has been used widely in several NLP tasks for this purpose (Liu et al., 2017; Søgaard and Goldberg, 2016). Zhao et al. (2018) used a mul"
2020.fever-1.1,W18-5515,0,0.0121975,"Fact Extraction and Verification Yixin Nie∗ Lisa Bauer∗ Mohit Bansal UNC Chapel Hill {yixin1, lbauer6, mbansal}@cs.unc.edu Abstract 2019a,b). The Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018a) is the most recent large-scale dataset that enables the development of data-driven neural approaches to the automatic fact checking task. Additionally, the FEVER Shared Task (Thorne et al., 2018b) introduced a benchmark, the first of this kind, that is capable of evaluating both evidence retrieval and claim verification. Several top-ranked approaches on FEVER (Nie et al., 2019a; Yoneda et al., 2018; Hanselowski et al., 2018b) decompose the task into 3 subtasks: document retrieval, sentence selection, and claim verification, and follow a similar pipeline training setup where sub-components are developed and trained sequentially. Although achieving higher scores on benchmarks, pipeline training is timeconsuming and imposes difficulty for fast application development since downstream training relies on data provided by a fully-converged upstream component. The impossibility of parallelization also causes data-inefficiency as training the same input sentence for both sentence selection and"
2020.fever-1.1,W18-5523,0,0.0492213,"Missing"
2020.findings-emnlp.258,S17-2001,0,0.0208959,"nguage modeling benchmark dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs 2871 64.0 55.8 63.1 58.6 55.7 55.5 56.8 GLUE benchmark (Wang et al., 2019). Metrics For the language modeling tasks, we report the perplexity (PPL) as the performance measure. For GLUE tasks, we report the accuracy for MNLI, QNLI,"
2020.findings-emnlp.258,I05-5002,0,0.0117291,"dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs 2871 64.0 55.8 63.1 58.6 55.7 55.5 56.8 GLUE benchmark (Wang et al., 2019). Metrics For the language modeling tasks, we report the perplexity (PPL) as the performance measure. For GLUE tasks, we report the accuracy for MNLI, QNLI, RTE, WNLI, and SST-2, accuracy a"
2020.findings-emnlp.258,W07-1401,0,0.0593146,"putationally very efficient, its WS approach does not converge to local optima. 4 Experimental Setup 4.1 Datasets Penn Treebank. The Penn Treebank (PTB) is a standard English language modeling benchmark dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs 2871 64.0 55.8 63.1 58.6 55.7 55.5 56.8 GLUE benchmark (Wang"
2020.findings-emnlp.258,N18-1101,0,0.0128675,"erge to local optima. 4 Experimental Setup 4.1 Datasets Penn Treebank. The Penn Treebank (PTB) is a standard English language modeling benchmark dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs 2871 64.0 55.8 63.1 58.6 55.7 55.5 56.8 GLUE benchmark (Wang et al., 2019). Metrics For the language modeling tasks,"
2020.findings-emnlp.258,D16-1264,0,0.0178985,"reasoning discussed in Sciuto et al. (2020) that even though ENAS is computationally very efficient, its WS approach does not converge to local optima. 4 Experimental Setup 4.1 Datasets Penn Treebank. The Penn Treebank (PTB) is a standard English language modeling benchmark dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Qu"
2020.findings-emnlp.258,D13-1170,0,0.00325711,"TB) is a standard English language modeling benchmark dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs 2871 64.0 55.8 63.1 58.6 55.7 55.5 56.8 GLUE benchmark (Wang et al., 2019). Metrics For the language modeling tasks, we report the perplexity (PPL) as the performance measure. For GLUE tasks, we report the"
2020.findings-emnlp.258,Q19-1040,0,0.0169505,"Treebank. The Penn Treebank (PTB) is a standard English language modeling benchmark dataset (Marcus et al., 1994). We use the standard pre-processing steps following Zaremba et al. (2014); Pham et al. (2018), which include lowercase, removing numbers and punctuation. The vocabulary size is capped at 10,000 unique tokens. GLUE Tasks. We choose all the 9 tasks from GLUE benchmark (Wang et al., 2019):2 QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; BarHaim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), MNLI (Williams et al., 2018), WNLI (Levesque et al., 2012), CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), STSB (Cer et al., 2017), MRPC (Dolan and Brockett, 2005), and QQP.3 We use the standard splits from 1 We achieved a test perplexity score of 59.2 with RL search on PTB, while our evolution search (ES) based approach achieved a better test perplexity score of 56.8 (see Table 1). 2 https://gluebenchmark.com/tasks 3 https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs 2871 64.0 55.8 63.1 58.6 55.7 55.5 56.8 GLUE benchmark (Wang et al., 2019). Metrics For the language modeling tasks, we report the perplexity (PPL) as the performance measure."
2020.findings-emnlp.309,N18-1101,0,0.0593303,"Missing"
2020.findings-emnlp.309,W05-1206,0,0.118402,"Missing"
2020.findings-emnlp.333,D18-1316,0,0.265046,"ary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and crosslingual (German, Russian, Turkish) generalization.1 1 Introduction There has been growing interest in understanding NLP systems and exposing their vulnerabilities through maliciously designed inputs (Iyyer et al., 2018; Belinkov and Bisk, 2018; Nie et al., 2019; 1 We will publicly release all our code, adversarial policy data, and models on our webpage. Gurevych and Miyao, 2018). Adversarial examples are generated using search (Alzantot et al., 2018), heuristics (Jia and Liang, 2017) or gradient (Ebrahimi et al., 2018) based techniques to fool the model into giving the wrong outputs. Often, the model is further trained on those adversarial examples to make it robust to similar attacks. In the domain of reading comprehension (RC), adversaries are QA samples with distractor sentences that have significant overlap with the question and are randomly inserted into the context. By having a fixed template for creating the distractors and training on them, the model identifies learnable biases and overfits to the template instead of being robust"
2020.findings-emnlp.333,2020.acl-main.421,0,0.0238503,"rds improving generalization of RC model, we calculate the F1 score of task model (trained on source domain) on out-of-domain or cross-lingual development datasets, and feed it as the reward to the optimizer. 4.4 Datasets We use SQuAD v2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017) for adversarial evaluation and in-domain policy-search experiments. Futher, we measure generalization from SQuAD v2.0 to NewsQA and TriviaQA (Joshi et al., 2017), and from SQuAD v1.1 (Rajpurkar et al., 2016) to German dataset from MLQA (Lewis et al., 2020) and Russian, Turkish datasets from XQuAD (Artetxe et al., 2020).4 See Appendix for more details on datasets and training. 4.5 Reading Comprehension Models We use RoBERTaBASE as the primary RC model for all our experiments. For fair baseline evaluation on out-of-domain and cross-lingual datasets, we also use the development set of the target task to select the best checkpoint. Search algorithms like AutoAugment require a downstream model that can be trained and evaluated fast, in order to reduce training time. So, we use distilRoBERTaBASE (Sanh et al., 2019) for AutoAugment training loops. BayesAugment is trained for fewer iterations than AutoAugment and h"
2020.findings-emnlp.333,D19-5801,0,0.0181793,"y the legislation. PerturbQuestion Syntacting paraphrasing network is used to generate the source question with a different syntax. Q: In what country is Normandy located? P: Where does Normany exist? Table 1: Demonstration of the various adversary functions used in our experiments (Q=Question, D=Distractor, A=Answer, P=Paraphrase). Words that have been modified using adversarial methods are italicized in the distractor. self-training method to reduce domain distribution discrepancy. Lee et al. (2019); Wang et al. (2019) use a discriminator to enforce domain-invariant representation learning (Fisch et al., 2019); Chen et al. (2018) and Zhang et al. (2017) learn languageinvariant representations for cross-lingual tasks. We show that heuristics-based adversaries can be used for augmentation as well as generalization. Policy Search: Cubuk et al. (2019) present the AutoAugment algorithm which uses reinforcement learning to find the best augmentation policies in a large search space, and then follow-up with RandAugment (Cubuk et al., 2020) which reduces the task to simple grid-search. Niu and Bansal (2019) use AutoAugment to discover perturbation policies for dialogue generation. Ho et al. (2019) use popu"
2020.findings-emnlp.333,P18-2103,0,0.047766,"Missing"
2020.findings-emnlp.333,P18-1000,0,0.220963,"Missing"
2020.findings-emnlp.333,2020.lrec-1.663,0,0.0224324,"sing network to introduce syntactic variance in adversaries. Augmentation and Generalization: Goodfellow et al. (2015) and Miyato et al. (2018) use adversarial training to demonstrate improvement in image recognition. Xie et al. (2020) improve the adversarial training scheme with auxiliary batch normalization modules. Back-translation (Yu et al., 2018), pre-training with other QA datasets (Devlin et al., 2019; Lewis et al., 2019; Talmor and Berant, 2019) and virtual adversarial training (Miyato et al., 2017; Yang et al., 2019) are shown to be effective augmentation techniques for RC datasets. Cao et al. (2020) propose a conditional adversarial 3724 Adversary Method Description Original Question/Sentence and Corresponding Distractor AddSentDiverse (Jia and Liang, 2017; Wang and Bansal, 2018) Q: In what country is Normandy located? D: D-Day is located in the country of Sri Lanka. AddKSentDiverse Multiple AddSentDiverse distractors are inserted randomly in the context. Q: Which county is developing its business center? D1: The county of Switzerland is developing its art periphery. D2: The county of Switzerland is developing its home center. AddAnswerPosition Answer span is preserved in this distractor"
2020.findings-emnlp.333,Q18-1039,0,0.0239231,"rturbQuestion Syntacting paraphrasing network is used to generate the source question with a different syntax. Q: In what country is Normandy located? P: Where does Normany exist? Table 1: Demonstration of the various adversary functions used in our experiments (Q=Question, D=Distractor, A=Answer, P=Paraphrase). Words that have been modified using adversarial methods are italicized in the distractor. self-training method to reduce domain distribution discrepancy. Lee et al. (2019); Wang et al. (2019) use a discriminator to enforce domain-invariant representation learning (Fisch et al., 2019); Chen et al. (2018) and Zhang et al. (2017) learn languageinvariant representations for cross-lingual tasks. We show that heuristics-based adversaries can be used for augmentation as well as generalization. Policy Search: Cubuk et al. (2019) present the AutoAugment algorithm which uses reinforcement learning to find the best augmentation policies in a large search space, and then follow-up with RandAugment (Cubuk et al., 2020) which reduces the task to simple grid-search. Niu and Bansal (2019) use AutoAugment to discover perturbation policies for dialogue generation. Ho et al. (2019) use population-based augment"
2020.findings-emnlp.333,P19-1484,0,0.01649,"(2019) and Alzantot et al. (2018) use a synonymsubstitution strategy while Ebrahimi et al. (2018) create gradient-based perturbations. Iyyer et al. (2018) construct a syntactic paraphrasing network to introduce syntactic variance in adversaries. Augmentation and Generalization: Goodfellow et al. (2015) and Miyato et al. (2018) use adversarial training to demonstrate improvement in image recognition. Xie et al. (2020) improve the adversarial training scheme with auxiliary batch normalization modules. Back-translation (Yu et al., 2018), pre-training with other QA datasets (Devlin et al., 2019; Lewis et al., 2019; Talmor and Berant, 2019) and virtual adversarial training (Miyato et al., 2017; Yang et al., 2019) are shown to be effective augmentation techniques for RC datasets. Cao et al. (2020) propose a conditional adversarial 3724 Adversary Method Description Original Question/Sentence and Corresponding Distractor AddSentDiverse (Jia and Liang, 2017; Wang and Bansal, 2018) Q: In what country is Normandy located? D: D-Day is located in the country of Sri Lanka. AddKSentDiverse Multiple AddSentDiverse distractors are inserted randomly in the context. Q: Which county is developing its business center?"
2020.findings-emnlp.333,2020.acl-main.653,0,0.0404426,"Missing"
2020.findings-emnlp.333,D16-1230,0,0.0899199,"Missing"
2020.findings-emnlp.333,2021.ccl-1.108,0,0.0571477,"Missing"
2020.findings-emnlp.333,W19-5333,0,0.0295211,"use the modified Translate-Test method as outlined in Lewis et al. (2020); Asai et al. (2018). QA samples in languages other than English are first translated to English and sent as input to RoBERTaBASE finetuned on SQuAD v1.1. The predicted answer spans within English context are then mapped back to the context in original language using alignment scores from the translation model. We use the top-ranked German→English and Russian→English models in WMT19 shared news translation task, and train a Turkish→English model using a similar architecture, to generate translations and alignment scores (Ng et al., 2019).6 5 Results First, in Sec. 5.1, we perform adversarial evaluation of baseline RC models for various categories of adversaries. Next, in Sec. 5.2, we train the RC 5 NewsQA 61.13 62.31 56.90 67.57 44.99 60.74 58.08 Table 3: Adversarial evaluation after training RoBERTaBASE with the original dataset augmented with equally sampled adversarial data. Compare to corresponding rows in Table 2 to observe difference in performance after adversarial training. Results (F1 score) are shown on dev set. Table 2: Adversarial evaluation of baseline RoBERTaBASE trained on SQuAD v2.0 and NewsQA. Compare to corr"
2020.findings-emnlp.333,K18-1047,1,0.904156,"Missing"
2020.findings-emnlp.333,D19-1132,1,0.917989,"l. (2019); Wang et al. (2019) use a discriminator to enforce domain-invariant representation learning (Fisch et al., 2019); Chen et al. (2018) and Zhang et al. (2017) learn languageinvariant representations for cross-lingual tasks. We show that heuristics-based adversaries can be used for augmentation as well as generalization. Policy Search: Cubuk et al. (2019) present the AutoAugment algorithm which uses reinforcement learning to find the best augmentation policies in a large search space, and then follow-up with RandAugment (Cubuk et al., 2020) which reduces the task to simple grid-search. Niu and Bansal (2019) use AutoAugment to discover perturbation policies for dialogue generation. Ho et al. (2019) use population-based augmentation (PBA) techniques (Jaderberg et al., 2017) and significantly reduce the compute time required by AutoAugment. We are the first to adapt RandAugment style techniques for NLP via our BayesAugment method. RandAugment enforces uniform transformation probability on all augmentation methods and collapses the augmentation policy search space to two global parameters. BayesAugment eliminates the need to choose between adversarial methods and optimizes only for their transformat"
2020.findings-emnlp.333,D14-1162,0,0.0856515,"Missing"
2020.findings-emnlp.333,P18-2124,0,0.0170088,"task model in the training loop. For example, the controllers in Cubuk et al. (2019) were trained for 15,000 samples or more. To circumvent this computational issue, we frame our adversarial policy Rewards The F1 score of downstream task model on development set is used as reward during policy search. To discover augmentation policies which are geared towards improving generalization of RC model, we calculate the F1 score of task model (trained on source domain) on out-of-domain or cross-lingual development datasets, and feed it as the reward to the optimizer. 4.4 Datasets We use SQuAD v2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017) for adversarial evaluation and in-domain policy-search experiments. Futher, we measure generalization from SQuAD v2.0 to NewsQA and TriviaQA (Joshi et al., 2017), and from SQuAD v1.1 (Rajpurkar et al., 2016) to German dataset from MLQA (Lewis et al., 2020) and Russian, Turkish datasets from XQuAD (Artetxe et al., 2020).4 See Appendix for more details on datasets and training. 4.5 Reading Comprehension Models We use RoBERTaBASE as the primary RC model for all our experiments. For fair baseline evaluation on out-of-domain and cross-lingual datasets, we also u"
2020.findings-emnlp.333,D16-1264,0,0.0141545,"on development set is used as reward during policy search. To discover augmentation policies which are geared towards improving generalization of RC model, we calculate the F1 score of task model (trained on source domain) on out-of-domain or cross-lingual development datasets, and feed it as the reward to the optimizer. 4.4 Datasets We use SQuAD v2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017) for adversarial evaluation and in-domain policy-search experiments. Futher, we measure generalization from SQuAD v2.0 to NewsQA and TriviaQA (Joshi et al., 2017), and from SQuAD v1.1 (Rajpurkar et al., 2016) to German dataset from MLQA (Lewis et al., 2020) and Russian, Turkish datasets from XQuAD (Artetxe et al., 2020).4 See Appendix for more details on datasets and training. 4.5 Reading Comprehension Models We use RoBERTaBASE as the primary RC model for all our experiments. For fair baseline evaluation on out-of-domain and cross-lingual datasets, we also use the development set of the target task to select the best checkpoint. Search algorithms like AutoAugment require a downstream model that can be trained and evaluated fast, in order to reduce training time. So, we use distilRoBERTaBASE (Sanh"
2020.findings-emnlp.333,P19-1103,0,0.031974,"and cross-lingual evaluation respectively. Overall, the goal of our paper is to make reading comprehension models robust to adversarial attacks as well as out-of-distribution data in cross-domain and cross-lingual scenarios. 2 Related Work Adversarial Methods in NLP: Following the introduction of adversarial evaluation for RC models by Jia and Liang (2017); Wang and Bansal (2018), several methods have been developed for probing the sensitivity and stability of NLP models (Nie et al., 2019; Glockner et al., 2018). Zhao et al. (2018) employ GANS to generate semantically meaningful adversaries. Ren et al. (2019) and Alzantot et al. (2018) use a synonymsubstitution strategy while Ebrahimi et al. (2018) create gradient-based perturbations. Iyyer et al. (2018) construct a syntactic paraphrasing network to introduce syntactic variance in adversaries. Augmentation and Generalization: Goodfellow et al. (2015) and Miyato et al. (2018) use adversarial training to demonstrate improvement in image recognition. Xie et al. (2020) improve the adversarial training scheme with auxiliary batch normalization modules. Back-translation (Yu et al., 2018), pre-training with other QA datasets (Devlin et al., 2019; Lewis e"
2020.findings-emnlp.333,P19-1485,0,0.019768,"et al. (2018) use a synonymsubstitution strategy while Ebrahimi et al. (2018) create gradient-based perturbations. Iyyer et al. (2018) construct a syntactic paraphrasing network to introduce syntactic variance in adversaries. Augmentation and Generalization: Goodfellow et al. (2015) and Miyato et al. (2018) use adversarial training to demonstrate improvement in image recognition. Xie et al. (2020) improve the adversarial training scheme with auxiliary batch normalization modules. Back-translation (Yu et al., 2018), pre-training with other QA datasets (Devlin et al., 2019; Lewis et al., 2019; Talmor and Berant, 2019) and virtual adversarial training (Miyato et al., 2017; Yang et al., 2019) are shown to be effective augmentation techniques for RC datasets. Cao et al. (2020) propose a conditional adversarial 3724 Adversary Method Description Original Question/Sentence and Corresponding Distractor AddSentDiverse (Jia and Liang, 2017; Wang and Bansal, 2018) Q: In what country is Normandy located? D: D-Day is located in the country of Sri Lanka. AddKSentDiverse Multiple AddSentDiverse distractors are inserted randomly in the context. Q: Which county is developing its business center? D1: The county of Switzerl"
2020.findings-emnlp.333,W17-2623,0,0.0129118,"r example, the controllers in Cubuk et al. (2019) were trained for 15,000 samples or more. To circumvent this computational issue, we frame our adversarial policy Rewards The F1 score of downstream task model on development set is used as reward during policy search. To discover augmentation policies which are geared towards improving generalization of RC model, we calculate the F1 score of task model (trained on source domain) on out-of-domain or cross-lingual development datasets, and feed it as the reward to the optimizer. 4.4 Datasets We use SQuAD v2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017) for adversarial evaluation and in-domain policy-search experiments. Futher, we measure generalization from SQuAD v2.0 to NewsQA and TriviaQA (Joshi et al., 2017), and from SQuAD v1.1 (Rajpurkar et al., 2016) to German dataset from MLQA (Lewis et al., 2020) and Russian, Turkish datasets from XQuAD (Artetxe et al., 2020).4 See Appendix for more details on datasets and training. 4.5 Reading Comprehension Models We use RoBERTaBASE as the primary RC model for all our experiments. For fair baseline evaluation on out-of-domain and cross-lingual datasets, we also use the development set of the target"
2020.findings-emnlp.333,N18-2091,1,0.817943,"t improvement using policy search methods. 3.1 Adversary Transformations We present two types of adversaries, namely positive perturbations and negative perturbations (or attacks) (Figure 1). Positive perturbations are adversaries generated using methods that have been traditionally used for data augmentation in NLP i.e., semantic and syntactic transformations. Negative perturbations are distractor sentences based on the classic AddSent model (Jia and Liang, 2017) that exploits the RC model’s shallow language understanding to mislead it to incorrect answers. We use 3725 the method outlined by Wang and Bansal (2018) for AddSentDiverse to generate a distractor sentence (see Table 1) and insert it randomly within the context of a QA sample. We introduce more variance to adversaries with AddKSentDiverse, wherein multiple distractor sentences are generated using AddSentDiverse and are inserted at independently sampled random positions within the context. For AddAnswerPosition, the original answer span is retained within the distractor sentence and the model is penalized for incorrect answer span location. We remove the sentence containing the answer span from the context and introduce a distractor sentence t"
2020.findings-emnlp.333,P17-1179,0,0.0314455,"g paraphrasing network is used to generate the source question with a different syntax. Q: In what country is Normandy located? P: Where does Normany exist? Table 1: Demonstration of the various adversary functions used in our experiments (Q=Question, D=Distractor, A=Answer, P=Paraphrase). Words that have been modified using adversarial methods are italicized in the distractor. self-training method to reduce domain distribution discrepancy. Lee et al. (2019); Wang et al. (2019) use a discriminator to enforce domain-invariant representation learning (Fisch et al., 2019); Chen et al. (2018) and Zhang et al. (2017) learn languageinvariant representations for cross-lingual tasks. We show that heuristics-based adversaries can be used for augmentation as well as generalization. Policy Search: Cubuk et al. (2019) present the AutoAugment algorithm which uses reinforcement learning to find the best augmentation policies in a large search space, and then follow-up with RandAugment (Cubuk et al., 2020) which reduces the task to simple grid-search. Niu and Bansal (2019) use AutoAugment to discover perturbation policies for dialogue generation. Ho et al. (2019) use population-based augmentation (PBA) techniques ("
2020.findings-emnlp.333,P18-2006,0,\N,Missing
2020.findings-emnlp.333,N19-1423,0,\N,Missing
2020.findings-emnlp.333,P18-5000,0,\N,Missing
2020.findings-emnlp.390,N19-1423,0,0.0196169,"they provide an LSTM-based model that jointly predicts labels and generates explanations, shown by MT-R A in Figure 1. Rajani et al. (2019) propose the CoS-E dataset, collecting human explanations for C OM MON S ENSE QA (Talmor et al., 2019), and they introduce the CAGE model, depicted as ST-R E in Figure 1. We build on these works by evaluating both ST-R E and MT-R A as well as models we introduce, ST-R A and MT-R E. We implement each graphical model with strong pretrained-T5 models, and for completeness, we also test methods with GPT2 and BERT (results in Appendix C) (Radford et al., 2019; Devlin et al., 2019). Evaluating Explanations. There is now a wealth of work on evaluating explanations of machine learning models (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017; Hooker et al., 2019; Jacovi and Goldberg, 2020b). For NLP tasks, past works focused on extractive rather than generative explanations (Nguyen, 2018; DeYoung et al., 2020). Such methods extract parts of the model input that are important to the output according to some criterion. However, they are not suited to evaluate NL explanations that are not part of the input, which motivates our new simulatability metric. Measures of similarity"
2020.findings-emnlp.390,2020.acl-main.491,1,0.635951,"ns of explanation faithfulness rather than plausibility. To resolve this evaluation problem, we introduce the leakage-adjusted simulatability (LAS) metric, which is better suited for identifying when explanations actually support model behavior. LAS scores combine two key mechanisms: they measure simulatability, which reflects how well an observer can use model explanations to predict the model’s output, while controlling for explanation leakage, which occurs when explanations directly leak the output. This metric is inspired by prior work on model interpretability (Doshi-Velez and Kim, 2017; Hase and Bansal, 2020), but to date no simulatability analysis has been carried out for NL explanations. We automate our evaluation by using a pretrained language model as the observer, serving as a proxy for a human. Using LAS scores, we evaluate model-generated as well as human explanations for C OMMON S ENSE QA (CQA) (Talmor et al., 2019; Rajani et al., 2019) and SNLI (Bowman et al., 2015; Camburu et al., 2018) tasks. We provide two human evaluations to validate our model-based approach. The first is an expert simulatability evaluation, where we manually play the role of the simulator in our LAS metric computati"
2020.findings-emnlp.390,2020.acl-main.386,0,0.170053,"ns, past works have suggested their models produce “justifications of its classification decisions” (Camburu et al., 2018) and “explanations to justify its predictions” (Rajani et al., 2019). While useful starting points, we argue that these evaluations are insufficient, because they do not necessarily indicate anything about a model’s true internal reasoning. For example, suppose the ground-truth label is A, while a model predicts B; a higher BLEU score will be observed when the model gives an explanation to support human label A, instead of model prediction B. This point is substantiated by Jacovi and Goldberg (2020b), who advocate for evaluations of explanation faithfulness rather than plausibility. To resolve this evaluation problem, we introduce the leakage-adjusted simulatability (LAS) metric, which is better suited for identifying when explanations actually support model behavior. LAS scores combine two key mechanisms: they measure simulatability, which reflects how well an observer can use model explanations to predict the model’s output, while controlling for explanation leakage, which occurs when explanations directly leak the output. This metric is inspired by prior work on model interpretabilit"
2020.findings-emnlp.390,2020.acl-main.409,0,0.0819824,"relationship with overall accuracy, 1[ˆ y |x, eˆ]. Together, these results help explain why models may struggle to learn from human explanations, since models may focus on label leakage in human explanations at the expense of other information. They may also suggest that to collect human ratings that do not correlate with label leakage, a highly controlled environment for human ratings may be required. 6.3 Accuracy-Interpretability Trade-off Past works on model interpretability have observed trade-offs between accuracy and model constraints for interpretation purposes (Bastings et al., 2019; Jain et al., 2020). Yet, Rudin (2018) and Jacovi and Goldberg (2020a) argue that we need not always face such a trade-off. Our findings provide quantitative evidence supporting these prior qualitative arguments. We observe consistently small changes in accuracy for our four models, and the largest changes, -.47 (p = .3124) for SNLI and -2.10 for CQA (p = .3272), are not statistically significant. We also test methods using human explanations purely for improving accuracy, e.g., through Masked Language Modeling objectives Table 6: Human ratings broken down by dataset and simulator prediction, shown alongside reg"
2020.findings-emnlp.390,2020.acl-main.771,0,0.056363,"latability primarily captures causal attribution of explanations and not necessarily social attribution. Simulatability-based evaluations have been conducted before (Ribeiro et al., 2018; Hase and Bansal, 2020), but we are the first to consider NL explanations and employ model-based controls for label leakage. Two contemporaneous works also explore relevant topics. Narang et al. (2020) train a T5 model to generate explanations in a set-up analogous to our MT-R A setting. They also notice the shortcomings of BLEU and collect binary human ratings of whether explanations “support” model outputs. Kumar and Talukdar (2020) introduce label-specific versions of the method in Rajani et al. (2019), one of which shares the graphical structure of our ST-R A model. However, their evaluation focuses on whether humans can recover ground truth labels from generated explanations alone, which they term “explanation accuracy.” Given these interesting concurrent works, our contributions are still distinguished by our joint focus on (1) simulatability-based evaluation, (2) controls for explanation label leakage, and (3) comparison of several distinct graphical models. Multi-Agent Communication. The most relevant work to our m"
2020.findings-emnlp.390,2020.acl-main.685,0,0.028196,"hich they term “explanation accuracy.” Given these interesting concurrent works, our contributions are still distinguished by our joint focus on (1) simulatability-based evaluation, (2) controls for explanation label leakage, and (3) comparison of several distinct graphical models. Multi-Agent Communication. The most relevant work to our multi-agent game concerns discrete communication policies with natural language or artificial protocols grounded in NL. Lazaridou et al. (2017) ground a communication protocol in natural language via an auxiliary image classification task. In concurrent work, Lazaridou et al. (2020) learn NL protocols for an image-based reference game by pretraining with image captions. While our approach shares the premise that language use is goal-oriented, we optimize full explanations of model outputs rather than descriptions of images in reference games. Another contemporaneous work optimizes for simulatability in a multi-agent setting, but they use extractive rather than generative explanations (Treviso and Martins, 2020). 3 Modeling With Explanations In this section, we delineate our baseline model and the four graphical models we study. The graph4353 The answer is: hen house task"
2020.findings-emnlp.390,P17-1015,0,0.122882,"trade-off between interpretability and accuracy, though this also means that existing methods struggle to learn from human explanations. 5. In a multi-agent game, we show that optimizing explanations for simulatability and penalizing trivial explanations can improve LAS scores in some settings. 4352 2 Related Work Generating Natural Language Explanations. Early work on this topic proposes to generate explanations for images that are descriptive as captions and discriminative as labels (Hendricks et al., 2016). However, they seek to explain the image’s label rather than a classifier’s output. Ling et al. (2017) introduce induction approaches for solving math problems and generating explanations of solutions. Two works focus on multi-modal problems, explaining visual question answering (Park et al., 2018) and self-driving car decisions (Kim et al., 2018). A few recent works focus on explanations for language understanding tasks. Camburu et al. (2018) introduce e-SNLI, extending the SNLI dataset (Bowman et al., 2015) with free-form human explanations, and they provide an LSTM-based model that jointly predicts labels and generates explanations, shown by MT-R A in Figure 1. Rajani et al. (2019) propose"
2020.findings-emnlp.390,N18-1097,0,0.0282767,"ild on these works by evaluating both ST-R E and MT-R A as well as models we introduce, ST-R A and MT-R E. We implement each graphical model with strong pretrained-T5 models, and for completeness, we also test methods with GPT2 and BERT (results in Appendix C) (Radford et al., 2019; Devlin et al., 2019). Evaluating Explanations. There is now a wealth of work on evaluating explanations of machine learning models (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017; Hooker et al., 2019; Jacovi and Goldberg, 2020b). For NLP tasks, past works focused on extractive rather than generative explanations (Nguyen, 2018; DeYoung et al., 2020). Such methods extract parts of the model input that are important to the output according to some criterion. However, they are not suited to evaluate NL explanations that are not part of the input, which motivates our new simulatability metric. Measures of similarity between model-generated and human explanations are used to evaluate nearly every method introduced above, with BLEU being the most common (Hendricks et al., 2016; Ling et al., 2017; Park et al., 2018; Kim et al., 2018; Camburu et al., 2018; Rajani et al., 2019). In a few cases, human evaluations are employe"
2020.findings-emnlp.390,P02-1040,0,0.108242,"16 - 20, 2020. 2020 Association for Computational Linguistics Training Phase 1 Training Phase 2 Figure 1: Graphical models representing varying roles of explanations, where the task input is denoted by x, task output by y, and explanation by e. We introduce a new rationalizing model, ST-R A, while also testing a reasoning multi-task model, MT-R E, and two other methods from past works (Camburu et al., 2018; Rajani et al., 2019). Generated explanations have typically been evaluated by automatic measures of similarity with human explanations. Most commonly, phrasematching metrics such as BLEU (Papineni et al., 2002) are used. In a few cases, human evaluations have been employed, also primarily to assess the similarity of explanations to what humans would say. On the basis of these evaluations, past works have suggested their models produce “justifications of its classification decisions” (Camburu et al., 2018) and “explanations to justify its predictions” (Rajani et al., 2019). While useful starting points, we argue that these evaluations are insufficient, because they do not necessarily indicate anything about a model’s true internal reasoning. For example, suppose the ground-truth label is A, while a m"
2020.findings-emnlp.390,W18-6319,0,0.0488811,"Missing"
2020.findings-emnlp.390,N19-1421,0,0.240321,"onalizing (R A) mode, where rationalizing models explicitly condition explanations on a label and reasoning models condition only on the input. Approaches further differ by whether they use explanations as inputs to a task model (ST) or as additional supervision in a multitask framework (MT). Two of these models are drawn from prior works: MT-R A (Camburu et al., 2018) and ST-R E (Rajani et al., 2019). We introduce ST-R A and also test MT-R E as the reasoning counterpart to MT-R A. To fairly compare the approaches, we implement each graphical model with a state-of-the-art pretrained T5 model (Raffel et al., 2019) (details in Section 3). 4351 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4351–4367 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Training Phase 1 Training Phase 2 Figure 1: Graphical models representing varying roles of explanations, where the task input is denoted by x, task output by y, and explanation by e. We introduce a new rationalizing model, ST-R A, while also testing a reasoning multi-task model, MT-R E, and two other methods from past works (Camburu et al., 2018; Rajani et al., 2019). Generated explanations have typically b"
2020.findings-emnlp.390,2020.blackboxnlp-1.10,0,0.043651,"protocols grounded in NL. Lazaridou et al. (2017) ground a communication protocol in natural language via an auxiliary image classification task. In concurrent work, Lazaridou et al. (2020) learn NL protocols for an image-based reference game by pretraining with image captions. While our approach shares the premise that language use is goal-oriented, we optimize full explanations of model outputs rather than descriptions of images in reference games. Another contemporaneous work optimizes for simulatability in a multi-agent setting, but they use extractive rather than generative explanations (Treviso and Martins, 2020). 3 Modeling With Explanations In this section, we delineate our baseline model and the four graphical models we study. The graph4353 The answer is: hen house task: Where would I not want a fox? The choices are hen house, England, and mountains. My commonsense tells me that the fox would eat the hens Context for reasoning T5 Human explanation The answer is: hen house explain: Where would I not want a fox? The choices are hen house, England, and mountains. The answer is hen house because the fox would eat the hens Context for rationalizing Human explanation Figure 2: Inputs and outputs for the"
2021.acl-demo.33,2020.wmt-1.116,0,0.323979,"that the translations will be poor in most cases when used by Internet users. Therefore, QE is essential for avoiding misuse and warning users of potential risks. Existing best performance QE models are usually trained under supervision with quality ratings from professional 272 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 272–279, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics translators (Fomicheva et al., 2020a). However, we are unable to easily collect a lot of human rat ings for Cherokee, due to its state of endanger ment. Nonetheless, we test both supervised and unsupervised QE methods: (1) Supervised: we use BLEU (Papineni et al., 2002) as the quality rat ing proxy and train a BLEU regressor; (2) Unsu pervised: following the uncertain estimation lit erature (Lakshminarayanan et al., 2017), we use the ensemble model’s output probability as the es timation of quality. Furthermore, to evaluate how well the QE models perform, we collect 200 human quality ratings (50 ratings for SMT ChrEn, SM"
2021.acl-demo.33,P03-1021,0,0.114482,"heloop model development. Additionally, our website can be easily adapted to any other low resource translation pairs. 2 System Description 2.1 Translation Models As shown in Figure 1, our system allows users to choose statistical or neural model (SMT or NMT). SMT is more effective for outofdomain transla tion between Cherokee and English (Zhang et al., 2020). We implement phrasebased SMT model via Moses (Koehn et al., 2007), where we train a 3gram KenLM (Heafield et al., 2013) and learn word alignment by GIZA++ (Och and Ney, 2003). Model weights are tuned on a development set by MERT (Och, 2003). NMT has better indomain performance and can generate more fluent texts. We implement the global attentional model proposed by Luong et al. (2015). Detailed hyperparameters can be found in Section 3.1. Note that we do not use Trans former because it empirically works worse (Zhang et al., 2020). And we find that the multilingual techniques we explored only significantly improve indomain performance when using multilingual Bible texts, so we suspect that it biases to Bible style texts. Hence, we also do not apply multilin gual techniques and just train the backbone models with our Cherokee"
2021.acl-demo.33,D19-1632,0,0.0883769,"2: Two user feedback interfaces of our demonstration system. (b) shows the feedback given by an expert. NMT. Note that we normalize the output probabil ity by the sentence length. Similarly, we rescale the normalized probability (01) to 05 by multi plying it by 5. Human Quality Rating. So far, our QE devel opment and evaluation are all based on BLEU. To better evaluate QE performance, we collect 200 hu man ratings (all rated by Prof. Benjamin Frey3 ), 50 ratings for ChrEn SMT, EnChr SMT, ChrEn NMT, and EnChr NMT, respectively. We fol low the direct assessment setup used by FLoRes (Guzmán et al., 2019),4 and thus each translated sentence receives a 0100 quality rating. 3 Benjamin Frey is a proficient secondlanguage Cherokee speaker and a citizen of the Eastern Band of Cherokee Indians. 4 0–10: represents a translation that is completely incorrect and inaccurate; 11–29 represents a translation with a few cor rect keywords, but the overall meaning is different from the source; 30–50 represents a translation that contains translated fragments of the source string, with major mistakes; 51–69 represents a translation that is understandable and conveys the overall meaning of source string but"
2021.acl-demo.33,J03-1002,0,0.0238189,"hich, in the long run, expands Cherokee data corpus and allows human intheloop model development. Additionally, our website can be easily adapted to any other low resource translation pairs. 2 System Description 2.1 Translation Models As shown in Figure 1, our system allows users to choose statistical or neural model (SMT or NMT). SMT is more effective for outofdomain transla tion between Cherokee and English (Zhang et al., 2020). We implement phrasebased SMT model via Moses (Koehn et al., 2007), where we train a 3gram KenLM (Heafield et al., 2013) and learn word alignment by GIZA++ (Och and Ney, 2003). Model weights are tuned on a development set by MERT (Och, 2003). NMT has better indomain performance and can generate more fluent texts. We implement the global attentional model proposed by Luong et al. (2015). Detailed hyperparameters can be found in Section 3.1. Note that we do not use Trans former because it empirically works worse (Zhang et al., 2020). And we find that the multilingual techniques we explored only significantly improve indomain performance when using multilingual Bible texts, so we suspect that it biases to Bible style texts. Hence, we also do not apply multilin gu"
2021.acl-demo.33,P13-2121,0,0.0221546,"s or Cherokee learners; (2) doc umenting human feedback, which, in the long run, expands Cherokee data corpus and allows human intheloop model development. Additionally, our website can be easily adapted to any other low resource translation pairs. 2 System Description 2.1 Translation Models As shown in Figure 1, our system allows users to choose statistical or neural model (SMT or NMT). SMT is more effective for outofdomain transla tion between Cherokee and English (Zhang et al., 2020). We implement phrasebased SMT model via Moses (Koehn et al., 2007), where we train a 3gram KenLM (Heafield et al., 2013) and learn word alignment by GIZA++ (Och and Ney, 2003). Model weights are tuned on a development set by MERT (Och, 2003). NMT has better indomain performance and can generate more fluent texts. We implement the global attentional model proposed by Luong et al. (2015). Detailed hyperparameters can be found in Section 3.1. Note that we do not use Trans former because it empirically works worse (Zhang et al., 2020). And we find that the multilingual techniques we explored only significantly improve indomain performance when using multilingual Bible texts, so we suspect that it biases to Bibl"
2021.acl-demo.33,W18-6319,0,0.0120065,"ength. Model ChrEn EnChr SMT 17.0 12.9 NMT (single) NMT (ensemble) 18.1 19.9 13.8 14.8 to strong (γ ≥ 0.5) (Cohen, 1988). Therefore, we use the trained XGBoost for SMT model’s QE and use the length normalized probability (i.e., Exp(LogProbability / length)) for NMT model’s QE in our online demonstration system. Table 2: The performance of translation models. 3.3 Qualitative Results (5, 0.1, 40) for EnChr NMT. Lastly, the backend of our demonstration website is based on the Flask framework. Metrics. We evaluate translation systems by BLEU (Papineni et al., 2002) calculated via Sacre BLEU7 (Post, 2018). Supervised QE models are developed by minimizing the mean square error of predicting BLEU, but all QE models are evaluated by the correlation with BLEU on development set and the correlation with human ratings. We use Pearson correlation (Benesty et al., 2009). 3.2 Quantitative Results Translation. Table 2 shows the translation per formance on our 1K development set, which are significantly better than the singlemodel in domain translation performance reported in our previous work (Zhang et al., 2020) and thus achieves the stateoftheart results. In addition, the 3model NMT ensemble fu"
2021.acl-demo.33,P17-4012,0,0.0657279,"14K parallel data collected by our previous work (Zhang et al., 2020) plus 3K newly complied par allel texts. We randomly sample 1K as our devel opment set and treat the rest as the training set. The data is opensourced at ChrEn/data/demo. To col lect human quality ratings, we randomly sample 50 examples from the development set, and for each of them, we collect 4 ratings for ChrEn/EnChr SMT and ChrEn/EnChr NMT, respectively. Setup. We implement SMT models via Moses (Koehn et al., 2007). After training and tuning, we run it as a server process.5 We develop our NMT models via OpenNMT (Klein et al., 2017). For both ChrEn and EnChr NMT models , we use 2layer LSTM encoder and decoder, general attention (Luong et al., 2015), hidden size=1024, label smoothing (Szegedy et al., 2016) equals to 0.2, dynamic batching with 1000 tokens. Differ ently, the ChrEn NMT model uses dropout=0.3, BPE tokenizer (Sennrich et al., 2016), and mini mum word frequency=10; the EnChr NMT model uses dropout=0.5, Moses tokenizer, and minimum word frequency=0. We train each NMT model with three random seeds (7, 77, 777) and use the 3model ensemble as the final translation model, and we use beam search (beam size=5)"
2021.acl-demo.33,P07-2045,0,0.072033,"Missing"
2021.acl-demo.33,D15-1166,0,0.0312972,"on 2.1 Translation Models As shown in Figure 1, our system allows users to choose statistical or neural model (SMT or NMT). SMT is more effective for outofdomain transla tion between Cherokee and English (Zhang et al., 2020). We implement phrasebased SMT model via Moses (Koehn et al., 2007), where we train a 3gram KenLM (Heafield et al., 2013) and learn word alignment by GIZA++ (Och and Ney, 2003). Model weights are tuned on a development set by MERT (Och, 2003). NMT has better indomain performance and can generate more fluent texts. We implement the global attentional model proposed by Luong et al. (2015). Detailed hyperparameters can be found in Section 3.1. Note that we do not use Trans former because it empirically works worse (Zhang et al., 2020). And we find that the multilingual techniques we explored only significantly improve indomain performance when using multilingual Bible texts, so we suspect that it biases to Bible style texts. Hence, we also do not apply multilin gual techniques and just train the backbone models with our CherokeeEnglish parallel texts. We use a 3model ensemble as our final working model. Our code is hosted at ChrEnTranslate and our 273 2.2 Quality Estimati"
2021.acl-demo.33,P02-1040,0,0.116479,"n with quality ratings from professional 272 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 272–279, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics translators (Fomicheva et al., 2020a). However, we are unable to easily collect a lot of human rat ings for Cherokee, due to its state of endanger ment. Nonetheless, we test both supervised and unsupervised QE methods: (1) Supervised: we use BLEU (Papineni et al., 2002) as the quality rat ing proxy and train a BLEU regressor; (2) Unsu pervised: following the uncertain estimation lit erature (Lakshminarayanan et al., 2017), we use the ensemble model’s output probability as the es timation of quality. Furthermore, to evaluate how well the QE models perform, we collect 200 human quality ratings (50 ratings for SMT ChrEn, SMT EnChr, NMT ChrEn, and NMT EnChr, respec tively). We show that our methods obtain mod erate to strong correlations with human judgment (Pearson correlation coefficient γ ≥ 0.44). One main purpose of our system is to allow humanin"
2021.acl-demo.33,P16-1162,0,0.0638072,"e development set, and for each of them, we collect 4 ratings for ChrEn/EnChr SMT and ChrEn/EnChr NMT, respectively. Setup. We implement SMT models via Moses (Koehn et al., 2007). After training and tuning, we run it as a server process.5 We develop our NMT models via OpenNMT (Klein et al., 2017). For both ChrEn and EnChr NMT models , we use 2layer LSTM encoder and decoder, general attention (Luong et al., 2015), hidden size=1024, label smoothing (Szegedy et al., 2016) equals to 0.2, dynamic batching with 1000 tokens. Differ ently, the ChrEn NMT model uses dropout=0.3, BPE tokenizer (Sennrich et al., 2016), and mini mum word frequency=10; the EnChr NMT model uses dropout=0.5, Moses tokenizer, and minimum word frequency=0. We train each NMT model with three random seeds (7, 77, 777) and use the 3model ensemble as the final translation model, and we use beam search (beam size=5) to gener ate translations. We implement the supervised QE model with XGBoost.6 XGBoost has three impor tant hyperparameters: max depth, eta, the number of rounds. Tuned on the development set, we set them as (5, 0.1, 100) for ChrEn SMT, (3, 0.1, 80) for EnChr SMT, (4, 0.5, 40) for ChrEn NMT, and 276 5 http://www.s"
2021.acl-demo.33,2020.emnlp-main.43,1,0.827036,"Missing"
2021.acl-long.133,P15-1034,0,0.0223896,"ccuracy on fake documents. A third of the fake news documents were predicted incorrectly by over half of the human subjects. This indicates that our automatically generated fake documents are also very hard for humans to detect. The most common clues humans used to detect fake news include linguistic style, topic coherence, specific event details and novel entities. 6 Related Work Fake News Detection. Traditional approaches to fake news detection are largely based on factchecking, text-style, or context from a single modality (Ciampaglia et al., 2015; Shi and Weninger, 2016; Pan et al., 2018; Angeli et al., 2015). Other approaches include detecting previously factchecked claims (Shaar et al., 2020), retrieving sentences that explain fact-checking (Nadeem et al., 2019; Atanasova et al., 2020), and leveraging context and discourse information (Nakov et al., 2019). 1689 Image Caption Body Text Misinformative KEs Aerial view of Fort McHenry. The battle of Fort McHenry, which took place in September of 1814, was a pivotal moment in the U.S. War of Independence...When the British finally left, they left behind a trail of destruction, including the destruction of the twin towers of the World Trade Center ..."
2021.acl-long.133,2020.findings-emnlp.89,0,0.011227,"stan, the Taliban released to the media this picture, which it said shows the suicide bombers who attacked the army base in Mazar-i-Sharif, April 21, 2017 Fake caption: On 21 April 2017 the Taliban released this picture to the army in Afghanistan which they said was a suicide bomber hiding at a media base in the city of Mazar-i-Sharif Figure 4: Example of AMR-to-text fake caption generation. The roles of army and media (in blue) are switched and the node corresponding to the event trigger (in red) attacked is negated. To obtain the AMR graphs, we use the stacktransformer based AMR parser from Astudillo et al. (2020) and train it on AMR 3.03 . Given the AMR graph, we vary the manipulation as follows: (1) Role switching - we randomly select two entity mentions that are present in different argument subgraphs of the AMR root node and interchange their positions in the AMR graph. (2) Predicate negation - we randomly pick predicates in the AMR graph corresponding to event triggers and other verbs, and replace them with their antonyms, which we obtain from WordNet (Fellbaum, 1998). This manipulation also includes reverting nodes with negative polarity, thereby negating the sentence. 1687 3 https://catalog.ldc."
2021.acl-long.133,2020.acl-main.656,0,0.0166883,"ocuments are also very hard for humans to detect. The most common clues humans used to detect fake news include linguistic style, topic coherence, specific event details and novel entities. 6 Related Work Fake News Detection. Traditional approaches to fake news detection are largely based on factchecking, text-style, or context from a single modality (Ciampaglia et al., 2015; Shi and Weninger, 2016; Pan et al., 2018; Angeli et al., 2015). Other approaches include detecting previously factchecked claims (Shaar et al., 2020), retrieving sentences that explain fact-checking (Nadeem et al., 2019; Atanasova et al., 2020), and leveraging context and discourse information (Nakov et al., 2019). 1689 Image Caption Body Text Misinformative KEs Aerial view of Fort McHenry. The battle of Fort McHenry, which took place in September of 1814, was a pivotal moment in the U.S. War of Independence...When the British finally left, they left behind a trail of destruction, including the destruction of the twin towers of the World Trade Center ... <British, Conflict.Attack, twin towers> Table 4: An example fake document which Tan et al. (2020) misses, but InfoSurgeon successfully detects. Text Features P´erez-Rosas et al. (20"
2021.acl-long.133,D18-1389,0,0.0330051,"Missing"
2021.acl-long.133,W13-2322,0,0.0137395,"Missing"
2021.acl-long.133,D14-1162,0,0.0850423,"mbeddings: We define an attribute function, A : Nt , Er|a ) F , that transforms each of the nodes and edges to its initial representation by concatenating the following features: • Background Embeddings - For the entity nodes Nt that can be linked to Freebase, we use data dump from Google Developers resources2 to map them to their respective Wikipedia pages, which serve as a rich source of established background knowledge. Background node embedding features are initialized from passing a Long Short Term Memory networks (LSTM) based architecture (Gers et al., 2000) through the word embeddings (Pennington et al., 2014) of the first paragraph in the Wikipedia page, which usually starts with a mention of the Wiki page’s title. Background edge embedding features are initialized from passing the LSTM through the paragraphs that contain the mentions of both the head and tail nodes. These embeddings are set to a default zero vector for unlinkable nodes. 1685 2 https://developers.google.com/freebase/ • News Embeddings - These are the surface-level features circumstantial to the entities, relations, and events extracted. News-based node features are initialized from passing an LSTM through the word embeddings of th"
2021.acl-long.133,C18-1287,0,0.040838,"Missing"
2021.acl-long.133,N18-1119,0,0.0250419,"Missing"
2021.acl-long.133,D19-1216,0,0.0412103,"Missing"
2021.acl-long.133,2020.emnlp-main.163,0,0.565405,"the node features. We feed the body text and each caption through the summarization-based BERT encoder from Liu and Lapata (2019), which averages the encoded token embeddings across sentences through a weighted mechanism. For metadata, we run the text encoder on a string containing the article domain, publication date, author, and headline. For images, we concatenate object-based (Anderson et al., 2018) and event-based (Pratt et al., 2020) visual features. Features for the edges between global context nodes are initialized by the attention-based semantic similarity between the node features (Tan et al., 2020). 3.3 Local KG Representation Constructing a KG from each Multimedia News Article: We leverage a publicly available multimedia Information Extraction (IE) system (Li et al., 2020; Lin et al., 2020) to construct a withindocument knowledge graph KG = (Nt , Er|a ) for each multimedia article. The IE system can extract 197 types of entities, 61 types of relations, and 144 types of events from text and images. Then, it performs entity linking (Pan et al., 2017) to map entities extracted from both text and images to a background knowledge base e.g. Freebase (Bollacker et al., 2008) and NIL (unlinkab"
2021.acl-long.133,N18-1074,0,0.0540313,"Missing"
2021.acl-long.133,P17-2067,0,0.0247961,"guage processing (Zellers et al., 2019) and computer vision (Choi et al., 2018) have become the frontier for malicious actors to controllably generate misinformation at scale. These realistic-looking AI-generated “fake news” have been shown to easily deceive humans, and it is, thus, critical for us to develop robust verification techniques against machine-generated fake news (Tan et al., 2020; Zellers et al., 2019; Kaliyar et al., 2020). Current misinformation detection approaches mainly focus on document-level fake news detection using lexical features and semantic embedding representations (Wang, 2017; Karimi et al., 2018; Tan et al., 2020). However, fake news is often generated based on manipulating (misusing, exaggerating, or falsifying) only a small part of the true information, namely the knowledge 1 The code, data and resources related to the misinformation detector are made publicly available at https://github. com/yrf1/InfoSurgeon for research purposes. elements (KEs, including entities, relations and events). Moreover, recent news oftentimes makes claims that do not have verified evidence yet, and evaluating the truthfulness of these real-time claims depends more on their consisten"
2021.acl-long.134,D15-1075,0,0.308314,"stand what they are saying at all (Marcus, 2018). From a listener’s perspective, such inconsistent bots fail to gain user trust and their long-term communication confidence. From a speaker’s perspective, it violates the maxim of quality in Grice’s cooperative principles (Grice, 1975) —”Do not say what you believe to be false.” Hence, efforts on reducing contradicting or inconsistent conversations by open-domain chatbots are imperative. Prior works (Welleck et al., 2019) characterized the modeling of persona-related consistency as a natural language inference (NLI) problem (Dagan et al., 2005; Bowman et al., 2015), and constructed a dialog NLI dataset based on Persona-Chat (Zhang et al., 2018), but so far state-of-the-art chatbots (Roller et al., 2020) have not been able to make use of NLI techniques in improving dialogue consistency. Overall, the challenge remains that we are still unable to answer the simple yet important question—“how good are we at modeling consistency (including persona, logic, causality, etc.) in a general conversation?”. The inability to measure this obscures to what degree building new modules or techniques can in turn help prevent contradicting responses during generation. See"
2021.acl-long.134,N19-1423,0,0.164167,"se aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and outof-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. 1 Introduction Recent progress on neural approaches to natural language processing (Devlin et al., 2019; Brown et al., 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020) have triggered a resurgent interest on building intelligent open-domain chatbots. Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al., 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation. While the success is ind"
2021.acl-long.134,W19-4103,0,0.0269653,"roving the consistency of state-of-the-art generative chatbots. 2 Related Work Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as Blen"
2021.acl-long.134,W19-3646,0,0.160375,"roving the consistency of state-of-the-art generative chatbots. 2 Related Work Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as Blen"
2021.acl-long.134,P18-1082,0,0.0525208,"Missing"
2021.acl-long.134,2020.acl-main.428,1,0.895103,"iction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as BlenderBot with up to 9.4B parameter Transformers (Roller et al., 2020). More similar to our work is utilizing NLI models in dialogue consistency. Dziri et al. (2019b) attempted to use entailment models trained on syn1700 thetic datasets for dialogue topic coherence evaluation. Particularly, Welleck et al. (2019) constructed the dialogue NLI dataset and (Li et al., 2020) utilized it to try to reduce inconsistency in generative models via unlikelihood training in a preliminary study that reports perplexity results, but did not measure actual generations or contradiction rates. We note that the dialogue NLI dataset is only semi-automatically generated, with limited coverage of only Persona-chat data (Zhang et al., 2018), whereas our DECODE is human-written and across multiple domains. Our task also involves logical and context-related reasoning beyond personal facts. We show that transfer of DECODE is subsequently more robust than dialogue NLI on both human-hum"
2021.acl-long.134,2021.ccl-1.108,0,0.0674875,"Missing"
2021.acl-long.134,D17-2014,1,0.889652,"Missing"
2021.acl-long.134,2020.acl-main.441,1,0.83802,"ANLI-R3, DECODE. “All DNLI” denotes all the datasets with DNLI removed. BART (Lewis et al., 2020). They represent the start-of-the-art language representation models and have yielded successes in many NLU tasks. The input format of fθ follows how these models handle sequence-pairs (C and u) for classification tasks with padding, separator and other special tokens such as position embeddings and segment features inserted at designated locations accordingly. We fine-tune fθ on different combinations of NLI training data including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), ANLIR3 (Nie et al., 2020a)4 , DNLI (Welleck et al., 2019), as well as our DECODE Main training set. We convert the 3-way labels of the examples in existing NLI datasets to 2-way, as described before, and θ is optimized using cross-entropy loss. When training fθU B in the utterance-based approach using the DECODE training set, the input sequences 4 ANLI data is collected in three rounds resulting in three subsets (R1, R2, R3). We only used training data in R3 since it contains some dialogue-related examples. 1703 5 5.1 Results and Analysis Performance on Constructed Dataset Our main results comparing various detectors"
2021.acl-long.134,2020.emnlp-main.734,1,0.806327,"Missing"
2021.acl-long.355,K16-1002,0,0.0952852,"Missing"
2021.acl-long.355,N19-1423,0,0.0736232,"Missing"
2021.acl-long.355,W07-0734,0,0.0767854,"n predicting correct answers when original context is replaced by its generated counterpart. 4 Experimental Setup We follow Zhang and Bansal (2019) to split the development set of SQuADv1.1 (Rajpurkar et al., 2016) into two halves and show the result on the test split. We generally follow previous work on evaluation metrics. For density estimation, we use negative log-likelihood (NLL) for comparison and bits per dimension to regularize the negative logL likelihood loss, formulated as M log(2) , where M represents the dimension of input. We evaluate QG by BLEU4 (Papineni et al., 2002), Meteor (Lavie and Agarwal, 2007), Rouge-L (Lin, 2004), and Amazon MTurk human evaluation. We use the BLEU score to evaluate NMT. We use accuracy to evaluate the TVQA QA model and EM (exact match) and F1 score to evaluate the SQuAD QA model. We replicate Zhang and Bansal (2019)’s baseline QG model. We use the STAGE model with Model TVQA Subtitle SQuAD Article Bi-LSTM -7.31 -1.27 Att-C RNN-C 0.68 0.50 -2.01 -0.37 Att-S RNN-S -8.02 -8.35 -17.12 -17.17 Att-AR RNN-AR -9.62 -9.63 -17.12 -17.26 Table 4: The NLL results of flow models and an LSTM baseline on the validation split of TVQA subtitles and test split of SQuAD articles. Th"
2021.acl-long.355,D18-1149,0,0.022561,"to the pure encoder-decoder baseline as well as the FlowSeq model (Ma et al., 2019) in human evaluation. As shown in the last rows in Table 5 and Table 6, humans favor our model more than the baseline in both tasks, which indicates our flow model indeed provides useful latent features for better generation. Plus, our model also always outperforms FlowSeq. We conjecture that it is because FlowSeq is non-autoregressive whereas our QG model is autoregressive. Neural Machine Translation. We also test the effectiveness of our approach on a neural machine translation (NMT) task. We first replicate Lee et al. (2018)’s transformer autoregressive model baseline, and then we add our flow architecture on top of it. As shown in Table 7, our proposed flow-aided MT model can improve the machine translation performance over the strong transformer baseline on the WMT16 (Cettolo et al., 2012) Romanian to English translation task. See A.7 for more details. We hope 5 Statistical significance is computed using the bootstrap test (Efron and Tibshirani, 1994). 6 We exclude those examples where the two models generate identical questions. Models BLEU4 Rouge-L Meteor FlowSeq 12.19 41.02 18.51 QG baseline + Lang-Flow + LS"
2021.acl-long.355,D18-1167,1,0.828154,"inverse stage (decoding the latent feature to the input ) with an uni-directional (i.e., the left-to-right direction) structure, We evaluate the density estimation ability of our language flow models as well as their effectiveness for three downstream tasks: (1) sequenceto-sequence (Seq-to-Seq) generation that includes question generation (QG) and neural machine translation (NMT) and (2) data augmentation for Question Answering (QA). We test QG and QA data augmentation on two large-scale QA datasets: (a) SQuAD (Rajpurkar et al., 2016), a widely explored textual QA and QG dataset and (b) TVQA (Lei et al., 2018), a large-scale multimodal videodialogue QA task. We test machine translation on WMT16 (Cettolo et al., 2012), a commonly used NMT dataset. For density estimation, we compare the negative likelihoods of our models against a baseline LSTM model. For QG, we use the non-autoregressive flow model to provide extra input features for a standard encoder-decoder text generation model. We show that it can significantly improve a baseline QG model for both SQuAD and TVQA on both automatic and human evaluation metrics. Aided by our flow model, we achieve strong improvements over a transformer baseline in"
2021.acl-long.355,2020.acl-main.730,1,0.883749,"l TVQA Subtitle SQuAD Article Bi-LSTM -7.31 -1.27 Att-C RNN-C 0.68 0.50 -2.01 -0.37 Att-S RNN-S -8.02 -8.35 -17.12 -17.17 Att-AR RNN-AR -9.62 -9.63 -17.12 -17.26 Table 4: The NLL results of flow models and an LSTM baseline on the validation split of TVQA subtitles and test split of SQuAD articles. The difference between C (e.g., Att-C) and S (e.g., Att-S) is whether the affine coupling/permutation is based on channel-dim (C) or time-dim (S). AR means autoregressive architecture. Att- refers to transformer nonlinear mapping, and RNNrefers to RNN nonlinear mapping. GloVe embeddings developed by Lei et al. (2020) as the TVQA QA baseline and use BERT as the SQuAD QA baseline. See appendix A for more experiment/reproducibility details. 5 Results 5.1 Negative Log-Likelihood Results First of all, to evaluate the density estimation ability, we compare the negative log-likelihood (NLL, Eq.6)4 of our different flow models on the context data of SQuAD and TVQA against a baseline model (a 3-layer bidirectional LSTM-RNN model with hidden size 300). As shown in Table 4, the flow model of time-dim coupling/permutation 4 Note that since our p(x) is over continuous word embeddings, so it is the probability density"
2021.acl-long.355,D19-1370,0,0.0148385,"ion fails at the latter part of the sentence. It might because the nonautoregressive structure has a weaker ability to model the temporal dependency during generation, which is consistent with the observations from previous works (Ren et al., 2020). To show that our model generates samples from a continuous space, we generate interpolation samples from our autoregressive flow model shown in Table 2. Those samples are mostly grammatically sound and correctly reflect the intermediate content of the two interpolated sentences. While variational autoencoder has the issue of ignoring latent space (Li et al., 2019), our models do not suffer from this issue. We introduced two types of language generation models in the paper: (1) the autoregressive flow model (used in data augmentation tasks) and (2) the model that uses flow latent features as extra input (e.g., for QG tasks). Our autoregressive flow model’s decoder is the inverted version of its encoder with the same weights, so it ensures the decoder uses the latent features. When we use flow latent features as extra inputs, it significantly improves QA performance (Table 5 and Table 6), which implies the latent features are usefully involved in generat"
2021.acl-long.355,D15-1166,0,0.0607317,"anslation (NMT), and data augmentation for Question Answering (QA). We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16. We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA.1 1 Introduction Several generative models have been proposed for language generation, including sequence-tosequence models based on RNNs (Luong et al., 2015) and transformers (Vaswani et al., 2017), as well as variational autoencoders (VAEs) to generate diverse texts (Bowman et al., 2016; Jain 1 Our code and models are available at: https:// github.com/zinengtang/ContinuousFlowNLG et al., 2017), plus generative adversarial networks (GANs) (Yu et al., 2017) to improve intended semantic fidelity. Another line of the generative model, normalizing flow (Rezende and Mohamed, 2015), is widely explored in computer vision and representation learning but less explored for NLG tasks. Flow models have been shown to be capable of improving probability density"
2021.acl-long.355,D19-1437,0,0.296625,"r language flow model learn language generation in a stronger autoregressive manner, we change the flow model’s affine coupling and permutation to a uni-directional structure, i.e., each timestep can only attend to previous timesteps. In this way, we enable our model to perform text generation autoregressively. Some recent works have developed density estimation models targeted on character-level discrete data (DiscreteFlow (Tran et al., 2019)) and explored using the flow architecture as an extra data encoder that provides latent features to support nonautoregressive text generation (FlowSeq (Ma et al., 2019)). While our work shares some similar characteristics, we explore different directions: (1) DiscreteFlow develops a modulus calculation method to process discrete data. Instead, we use word embedding to transform the discrete input tokens to continuous features, which is simple yet effective. (2) FlowSeq essentially leverages the flow architecture in a typical encoder-decoder model to support non-autoregressive generation, whereas our models follow the standard generative flow framework and can directly generate texts via their invertible structure in both non-autoregressive or autoregressive"
2021.acl-long.355,K18-1047,1,0.836017,"hrasing) + Context (Language Flow) + Context (Language Flow+) Valid-Accuracy F1 81.34 81.02 81.65 82.28 82.49 88.76 88.79 88.92 89.22 89.44 Table 9: QA results on SQuAD test split. The augmented data (new articles) significantly improves a strong SQuAD QA baseline. 69.42 69.52 69.98 70.45 70.86 Table 8: QA results on TVQA dev split. Language Flow refers to the autoregressive language flow we propose, and Language Flow+ refers to the model with a non-autoregressive flow model encoder plus an LSTM autoregressive decoder. while, we compare to two other data augmentation techniques: paraphrasing (Niu and Bansal, 2018) and back-translation (Sennrich et al., 2016). Note that for a fair comparison, we apply the same data filter and training schema for all data augmentation methods. It can be seen that both methods perform worse than our Language Flow or Language Flow+ models. 6 EM Discussion We show some sample questions generated by our non-autoregressive and autoregressive flow models in Table 1. The autoregressive samples are better organized and grammatically sound, while non-autoregressive generation fails at the latter part of the sentence. It might because the nonautoregressive structure has a weaker a"
2021.acl-long.355,P02-1040,0,0.111042,"ut if the model performs worse on predicting correct answers when original context is replaced by its generated counterpart. 4 Experimental Setup We follow Zhang and Bansal (2019) to split the development set of SQuADv1.1 (Rajpurkar et al., 2016) into two halves and show the result on the test split. We generally follow previous work on evaluation metrics. For density estimation, we use negative log-likelihood (NLL) for comparison and bits per dimension to regularize the negative logL likelihood loss, formulated as M log(2) , where M represents the dimension of input. We evaluate QG by BLEU4 (Papineni et al., 2002), Meteor (Lavie and Agarwal, 2007), Rouge-L (Lin, 2004), and Amazon MTurk human evaluation. We use the BLEU score to evaluate NMT. We use accuracy to evaluate the TVQA QA model and EM (exact match) and F1 score to evaluate the SQuAD QA model. We replicate Zhang and Bansal (2019)’s baseline QG model. We use the STAGE model with Model TVQA Subtitle SQuAD Article Bi-LSTM -7.31 -1.27 Att-C RNN-C 0.68 0.50 -2.01 -0.37 Att-S RNN-S -8.02 -8.35 -17.12 -17.17 Att-AR RNN-AR -9.62 -9.63 -17.12 -17.26 Table 4: The NLL results of flow models and an LSTM baseline on the validation split of TVQA subtitles an"
2021.acl-long.355,D14-1162,0,0.0848655,"we conjecture that the flow model should also have strong potential to be adapted for language generation tasks. Therefore, in this paper, we introduce a continuous language generative flow model that can deal with discrete language data in continuous latent space. We propose two variants, the non-autoregressive and autoregressive models, and show that they both can perform well on density estimation tasks. We follow the architecture of one previous generative flow model, Glow (Kingma and Dhariwal, 2018), but make adaptions for language generation tasks. We first employ GloVe word embeddings (Pennington et al., 2014) to map the discrete token sequence to a continuous embedding matrix. Furthermore, we utilize two components: time-dimension permutation and affine coupling with RNN or Transformer non-linearity functions, which allow interaction between words in a se4609 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4609–4622 August 1–6, 2021. ©2021 Association for Computational Linguistics quence and better contextualizes language semantics. Overall, these proposed components help gener"
2021.acl-long.355,2020.acl-main.15,0,0.118033,"a se4609 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4609–4622 August 1–6, 2021. ©2021 Association for Computational Linguistics quence and better contextualizes language semantics. Overall, these proposed components help generate texts in a non-autoregressive manner. However, even though the non-autoregressive model has attracted a lot of research attention because of its fast generation speed, it still hardly surpasses the generation quality of autoregressive models (Ren et al., 2020). Therefore, to make our language flow model learn language generation in a stronger autoregressive manner, we change the flow model’s affine coupling and permutation to a uni-directional structure, i.e., each timestep can only attend to previous timesteps. In this way, we enable our model to perform text generation autoregressively. Some recent works have developed density estimation models targeted on character-level discrete data (DiscreteFlow (Tran et al., 2019)) and explored using the flow architecture as an extra data encoder that provides latent features to support nonautoregressive tex"
2021.acl-long.355,P16-1009,0,0.0284169,"(Language Flow+) Valid-Accuracy F1 81.34 81.02 81.65 82.28 82.49 88.76 88.79 88.92 89.22 89.44 Table 9: QA results on SQuAD test split. The augmented data (new articles) significantly improves a strong SQuAD QA baseline. 69.42 69.52 69.98 70.45 70.86 Table 8: QA results on TVQA dev split. Language Flow refers to the autoregressive language flow we propose, and Language Flow+ refers to the model with a non-autoregressive flow model encoder plus an LSTM autoregressive decoder. while, we compare to two other data augmentation techniques: paraphrasing (Niu and Bansal, 2018) and back-translation (Sennrich et al., 2016). Note that for a fair comparison, we apply the same data filter and training schema for all data augmentation methods. It can be seen that both methods perform worse than our Language Flow or Language Flow+ models. 6 EM Discussion We show some sample questions generated by our non-autoregressive and autoregressive flow models in Table 1. The autoregressive samples are better organized and grammatically sound, while non-autoregressive generation fails at the latter part of the sentence. It might because the nonautoregressive structure has a weaker ability to model the temporal dependency durin"
2021.acl-long.355,D19-1253,1,0.927766,"learning method that can provide new features for downstream tasks. Concretely, while the original QG model3 is forˆ i = G(ui ); the new QG mulated as ui = E(xi ), q model with flow is formulated as: ˆ i = G(hatt (ui , zi )) ui = E(xi ), q (19) where E refers to encoder and G decoder. zi refers to latent features of the non-autoregressive flow model. hatt is essentially a MLP with sigmoid activation. The loss function has two parts: Lgen = N 1 X − log p(qi ) N (20) L = λLnll + Lgen (21) i=1 where qi represents the target questions and λ is a hyperparameter for NLL loss (Eq. 6) 3 We replicate Zhang and Bansal (2019)’s standard encoderdecoder attention QG model with BERT features as input embeddings. 3.3 Context Generation for Data Augmentation Context Generation. We propose to use flow to generate diverse contexts for data augmentation as both TVQA and SQuAd are question answering tasks with textual context. We generate new context (video subtitles for TVQA; articles for SQuAD) by injecting noise to the hidden vector of the original context, zi , and reconstructing it to new sentences, ˆ i . Note that, we can also do the same thing for x questions, however, we find that changing one word in the question"
2021.acl-long.355,D18-1424,0,0.0419214,"Missing"
2021.acl-long.537,P18-1063,1,0.834288,"; then the PageRank algorithm is applied to obtain the rank 5 We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email. scores for each sentence, and top-rank sentences are selected as the summary. BertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM. 3.2 Abstractive Fast Abs RL. As the simple non-pretrained abstractive baseline, we use Chen and Bansal (2018), which is a hybrid model that first extracts sentences from the source document, then rewrites the extracted sentences by an abstractive rewriter. They pair summary sentences with the extracted sentences to train the abstractive rewriter. Adapting their model to our email thread summarization task, we make two adjustments: (1) We extract emails instead of sentences, which is a natural unit for email thread; (2) Since summary sentences usually follow the temporal order of the emails, we enhance this pairing procedure by using the Neeleman-Wunsch algorithm (Needleman and Wunsch, 1970; Rameshkum"
2021.acl-long.537,2021.ccl-1.108,0,0.0342117,"Missing"
2021.acl-long.537,2020.acl-main.173,0,0.0179186,"ely. for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020). 6902 5.3 Correlation with Human Judgement ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and human judgments. Specifically, we evaluate"
2021.acl-long.537,2020.acl-main.450,0,0.0235029,"ort and E MAIL S UMlong tasks, respectively. for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020). 6902 5.3 Correlation with Human Judgement ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and"
2021.acl-long.537,W04-3252,0,0.0233489,"old summary. “Ext-Oracle-R1” in Table 2 is computed from an oracle summary that maximizes ROUGE-1 (Lin, 2004). Lead. This model simply picks the first sentence from the source document as the summary, which has surprisingly good performance on CNN/DM dataset (Narayan et al., 2018). We test two variants by selecting: (1) the first sentence of the email thread, which is usually the subject (see the example in Table 1), referred as Lead-1; (2) the first sentence of the email thread (the subject) plus the first sentences of every email, named Lead-1-Email.5 TextRank. This is a graph-based method (Mihalcea and Tarau, 2004). It first builds a graph between sentences by their embedding similarities; then the PageRank algorithm is applied to obtain the rank 5 We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email. scores for each sentence, and top-rank sentences are selected as the summary. BertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM. 3.2 Abstract"
2021.acl-long.537,2020.acl-main.459,0,0.294098,"rayan et al., 2018). However, living in an information era, we are facing with diverse content 1 Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum in different structures. The summarization need is varied along with different application scenarios. Recently, there is an increasing research interest in diverse summarization tasks (Gao et al., 2020), e.g., timeline (Allan et al., 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al., 2018), meeting (Carletta et al., 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc. Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (E MAIL S UM) dataset. Email threads are widely used at work. An email thread is a special type of dialogue that usually has a specific structure (sender, receiver, greeting line, main body, and the signature), contains technical information, and involves multiple speakers. Unlike a conversational dialog turn, an email in a 6895 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan"
2021.acl-long.537,2020.findings-emnlp.19,0,0.0342911,"T5) to generate summaries for unlabelled threads, then mix the model-labeled and human-labeled data to finetune T5 again, referred as SemiSupx (x stands for the unlabelled data source we use, i.e., W3C, Avocado, or together). Hierarchical T5. Hierarchical summarization models have been shown to improve the performance of multi-document summarization task (Liu and Lapata, 2019a). Although an email thread can be treated as a single document due to the temporal dependency between consecutive emails, it also has a clear turn structure that encourages using of the hierarchical encoders. Recently, Zhu et al. (2020) proposed a hierarchical model (HMNet) for meeting summarization. Inspired by their work, we propose a hierarchical model that is similar to HMNet in structure but uses T5 as the backbone, therefore, it can take advantage of both the hierarchical structure and the pre-trained knowledge. As shown in Figure 1, this model contains two encoders: the token-level encodes the whole email Experiments Evaluation Metrics ROUGE (Lin, 2004) is a commonly used automatic metric for summarization tasks. It has several variants: (1) ROUGE-1 (R1) measures the unigram overlap between the generated and reference"
2021.acl-short.92,2020.findings-emnlp.58,0,0.142828,"and effectiveness, we apply encoder parameter sharing and neighborhood constraints (Wang et al., 2018; Kim et al., 2020) which encourages the model to better utilize multilingual data to improve monolingual task performance while maintaining smaller model size. Query and Context Representations. We represent videos using ResNet-152 (He et al., 2016) and I3D (Carreira and Zisserman, 2017) features extracted every 1.5 seconds. We extract language features using pre-trained, then finetuned (on our queries and subtitles) RoBERTa-base (Liu et al., 2019), for English (Liu et al., 2019) and Chinese (Cui et al., 2020), respectively. For queries, we use token-level features. For subtitles, we max3 The differences might be due to the different morphemes in the languages. E.g., the Chinese word 长发 (‘long hair’) is labeled as a single noun, but as an adjective (‘long’) and a noun (‘hair’) in English (Wang et al., 2019b). Video Encoder Encoders and Parameter Sharing. We follow Lei et al. (2020) to use Self-Encoder as our main component for query and context encoding. A Self-Encoder consists of a self-attention (Vaswani et al., 2017) layer, a linear layer, and a residual (He et al., 2016) connection followed by"
2021.acl-short.92,P15-1166,0,0.0285294,"l datasets (Krishna et al., 2017; Hendricks et al., 2017; Gao et al., 2017) because TVR is the largest moment retrieval dataset, and also has the advantage of having dialogues (in the form of subtitle text) as additional context for retrieval, in contrast to pure video context in the other datasets. We further propose mXML, a compact, multilingual model that learns jointly from both English and Chinese data for moment retrieval. Specifically, on top of the state-of-the-art monolingual moment retrieval model XML (Lei et al., 2020), we enforce encoder parameter sharing (Sachan and Neubig, 2018; Dong et al., 2015) where the queries and subtitles from the two languages are encoded using shared encoders. We also incorporate a language neighborhood constraint (Wang et al., 2018; Kim et al., 2020) to the output query and subtitle embeddings. It encourages sentences of the same meaning in different languages to lie close to each other in the embedding space. Compared to separately trained monolingual models, mXML substantially reduces the total model size while improving retrieval performance (over monolingual models) as we show in Section 4. Detailed dataset analyses and model ablations are provided. 2 Dat"
2021.acl-short.92,D18-1167,1,0.83512,"Missing"
2021.acl-short.92,Q13-1003,0,0.0956337,"Missing"
2021.acl-short.92,W18-6327,0,0.0203468,"ver other moment retrieval datasets (Krishna et al., 2017; Hendricks et al., 2017; Gao et al., 2017) because TVR is the largest moment retrieval dataset, and also has the advantage of having dialogues (in the form of subtitle text) as additional context for retrieval, in contrast to pure video context in the other datasets. We further propose mXML, a compact, multilingual model that learns jointly from both English and Chinese data for moment retrieval. Specifically, on top of the state-of-the-art monolingual moment retrieval model XML (Lei et al., 2020), we enforce encoder parameter sharing (Sachan and Neubig, 2018; Dong et al., 2015) where the queries and subtitles from the two languages are encoded using shared encoders. We also incorporate a language neighborhood constraint (Wang et al., 2018; Kim et al., 2020) to the output query and subtitle embeddings. It encourages sentences of the same meaning in different languages to lie close to each other in the embedding space. Compared to separately trained monolingual models, mXML substantially reduces the total model size while improving retrieval performance (over monolingual models) as we show in Section 4. Detailed dataset analyses and model ablations"
2021.blackboxnlp-1.1,D15-1075,0,0.317011,"portions of the input people expected to be important in influencing models’ decisions. We then use Integrated Gradients (IG, Sundararajan et al. 2017) to determine which portions actually influenced the models’ decisions. We term the alignment between them as Importance Alignment. We formulate three different methods of using human-generated natural language explanations to quantify human expectations of model behavior, resulting in three different methods for calculating importance alignment. As a case study, we applied our method to the Natural Language Inference task (Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) in which models are tasked with classifying pairs of sentences according to whether the first sentence entails, contradicts, or is neutral with respect to the second. Concretely, we measured the extent to which the inference decisions of eight models (six state-of-the-art transformers, an LSTM model and a bag-of-words model) aligned with humangenerated explanations from the Adversarial NLI dataset (ANLI, Nie et al. 2020). In all three methods for calculating Importance Alignment, BERT-base had the highest importance alignment score. We also found that the smaller, ‘bas"
2021.blackboxnlp-1.1,N18-2017,0,0.054916,"Missing"
2021.blackboxnlp-1.1,P19-1487,0,0.0223535,"Selvaraju et al., 2020), such as answering “Is the rose red?” with no, but then “What color is the rose?” In this paper, we measure how well model decisions are aligned with human expectations ∗ The work was conducted during an internship at Facebook AI Research. 1 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 1–14 Online, November 11, 2021. ©2021 Association for Computational Linguistics about those decisions. Building on work that aims to extract or generate interpretable or faithful descriptions of model behavior (Lipton, 2018; Rajani et al., 2019; Kalouli et al., 2020; Silva et al., 2020; Jacovi and Goldberg, 2020; Zhao and Vydiswaran, 2020), we use human-generated natural language explanations to determine which portions of the input people expected to be important in influencing models’ decisions. We then use Integrated Gradients (IG, Sundararajan et al. 2017) to determine which portions actually influenced the models’ decisions. We term the alignment between them as Importance Alignment. We formulate three different methods of using human-generated natural language explanations to quantify human expectations of model behavior, resu"
2021.blackboxnlp-1.1,2020.acl-main.441,1,0.902958,"nt methods for calculating importance alignment. As a case study, we applied our method to the Natural Language Inference task (Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) in which models are tasked with classifying pairs of sentences according to whether the first sentence entails, contradicts, or is neutral with respect to the second. Concretely, we measured the extent to which the inference decisions of eight models (six state-of-the-art transformers, an LSTM model and a bag-of-words model) aligned with humangenerated explanations from the Adversarial NLI dataset (ANLI, Nie et al. 2020). In all three methods for calculating Importance Alignment, BERT-base had the highest importance alignment score. We also found that the smaller, ‘base’ versions of transformers tended to have higher importance alignment scores than the corresponding large versions. However, being smaller doesn’t always result in higher importance alignment, since both small non-transformer models had lower importance alignment scores than ‘base’ transformers. Finally, we demonstrate that more accurate models (both for classic test accuracy and on the diagnostic dataset HANS; McCoy et al. 2019) do not necessa"
2021.blackboxnlp-1.1,2020.inlg-1.43,0,0.0159291,"rsions. However, being smaller doesn’t always result in higher importance alignment, since both small non-transformer models had lower importance alignment scores than ‘base’ transformers. Finally, we demonstrate that more accurate models (both for classic test accuracy and on the diagnostic dataset HANS; McCoy et al. 2019) do not necessarily have higher importance alignment, suggesting that accuracy and alignment with human expectations are orthogonal dimensions along which models should be evaluated. 2 behaviour with normative notions of human ethics (“value alignment”; Russell et al. 2015; Peng et al. 2020), alignment between tokens from source to target in machine translation, alignment between images and text in image-caption alignment models, etc. In this paper, we propose a new type of alignment, Importance Alignment: we want models to not only generate accurate outputs, but also to generate these accurate outputs for reasons that align with human expectations. High importance alignment can be valuable because prior work has demonstrated that when people form correct mental models of AI decision boundaries, they make better AI-assisted decisions (Bansal et al., 2019a,b). For example, when an"
2021.blackboxnlp-1.1,D19-6601,0,0.0115275,"dels (finetuned on MNLI + SNLI + ANLI + re-cast FEVER) and models used as the soft oracle (finetuned on 6-way NLI and reason classification on a subset of ANLI).  X Target models Experimental details Models Target models. We measured the importance alignment for six pretrained Transformer language models: BERT base and large (Devlin et al., 2019); RoBERTa base and large (Liu et al., 2019); and ELECTRA base and large (Clark et al., 2020). We fine-tuned these models on the combination of the following NLI datasets: SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), NLI-recast FEVER (Thorne et al., 2019) and ANLI rounds 1–3 (Nie et al., 2020). We used the hyperparameters in the ANLI codebase for finetuning.4 We used the same datasets to also train two non-transformer models: a bag-of-words (BOW) 4 https://github.com/facebookresearch/ anli/blob/master/script/example_scripts/ 5 Model Importance Alignment ∆AH ∆AS Acc. ANLI Model Importance Alignment ∆AH ∆AS ∆AE BERT-Base RoBERTa-Base ELECTRA-Base BERT-Large RoBERTa-Large ELECTRA-Large 0.21*** 0.11*** 0.17*** 0.18*** 0.04* 0.07 0.11*** 0.02* 0.06*** -0.02 0.01 0.01 48.02 50.47 52.33 49.24 55.37 58.06 BERT-Base RoBERTa-Base ELECTRA-Base BERT-Large"
2021.blackboxnlp-1.1,L18-1239,0,0.0282887,"Missing"
2021.blackboxnlp-1.1,D19-1221,0,0.0423921,"Missing"
2021.blackboxnlp-1.1,D19-1002,0,0.0283059,"e current example or about some other example. This task requires the model to perform not only the target task (e.g., NLI) but also requires the model to establish a relationship between the provided explanation and the input, thereby incorporating information from the natural language explanation. Why Integrated Gradients? We use Integrated Gradients because they are axiomatically both interpretable and faithful (Sundararajan et al., 2017), unlike attention based methods which have been argued are not faithful explanations of models’ decision making processes (Jain and Wallace 2019, but see Wiegreffe and Pinter 2019 for a counterpoint). Other perturbation methods which are more faithful than attention such as LIME (Ribeiro et al., 2016), SHAP (Lundberg and Lee, 2017), and their variants can be used, although these methods have been argued to be unreliable (Camburu et al., 2019; Slack et al., 2020; Camburu et al., 2021). No current consensus exists on which methods should be employed (Hase and Bansal, 2020; Ghorbani et al., 2019). Although we use IG, crucially, our method is not dependent on it; IG 2 We used the list of stop words from NLTK (Bird et al., 2009) 3 Formally, for some input x with gold label"
2021.conll-1.25,D19-5408,0,0.019312,"wo aligners becomes. As the average summary length of CNN/Daily Mail is 3.8 sentences, the advantage of the SuperPALsent in those lengths stresses its benefit over ROUGE. Moreover, the SuperPALsent data achieved the highest global result across all summary lengths. 7 Conclusion and Discussion CNN/DailyMail (Fabbri et al., 2019)), albeit somewhat less coherent. An example of such potential summary, illustrated by an oracle-system summary derived from our supervised aligner predictions on CNN/DailyMail, is shown in Table 9. Alternatively, our data can contribute to the recent highlighting task (Arumae et al., 2019; Cho et al., 2020), where salient information fragments are marked inside a document, thus circumventing the need to generate coherent text. Further, propositions may be fused together to generate a coherent abstractive summary. Recently, such a cascaded approach (Lebanoff et al., 2020), consisting of text fragment selection followed by a generation step, exhibited comparable or improved results over end-to-end systems. We advocate the potential of summary-source proposition-level alignment to extract cleaner and more accurate alignments than through the comOverall, we suggest that our releas"
2021.conll-1.25,P18-1063,1,0.930179,"derive (noisy) training sets for sum- mid procedure, using Open IE (Yang et al., 2016; marization subtasks (Zhang et al., 2018; Cho et al., Peyrard and Eckle-Kohler, 2017) or Elementary 2019). For example, training datasets for sentence Discourse Units (EDU) (Hirao et al., 2018), to exsalience detection were derived from reference- tract proposition-level units. As propositional units source alignments, by marking as salient those (like SCUs) may share their arguments (such as source sentences that were aligned with a sum- in conjunctions and other constructions, e.g. “The mary sentence (e.g. Chen and Bansal (2018)). As boy went home and ate dinner”), and may be disanother example, Lebanoff et al. (2019) leveraged contiguous (“The boy...ate dinner”), Open IE out1 put, which satisfies these requirements, is best suitAll corresponding datasets and code are publicly available at https://github.com/oriern/SuperPAL. able for extracting such units (while EDU format 311 Summary Sentence The BBC reports 56-year-old Allan Matthews pleaded guilty Wednesday to removing another man’s left hand at an Australian motel despite not being qualified to practice medicine. The lawsuit, which also alleges a hostile work env"
2021.conll-1.25,P19-1098,0,0.0191218,"the tion task, notably for generating training data full sentence level. Then, (noisy) training data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate alignment methods were for proposition-level alignment, derived autoneither optimized nor evaluated explicitly, making matically from available summarization evaluthem difficult to compare a"
2021.conll-1.25,2020.emnlp-main.509,0,0.0240252,"As the average summary length of CNN/Daily Mail is 3.8 sentences, the advantage of the SuperPALsent in those lengths stresses its benefit over ROUGE. Moreover, the SuperPALsent data achieved the highest global result across all summary lengths. 7 Conclusion and Discussion CNN/DailyMail (Fabbri et al., 2019)), albeit somewhat less coherent. An example of such potential summary, illustrated by an oracle-system summary derived from our supervised aligner predictions on CNN/DailyMail, is shown in Table 9. Alternatively, our data can contribute to the recent highlighting task (Arumae et al., 2019; Cho et al., 2020), where salient information fragments are marked inside a document, thus circumventing the need to generate coherent text. Further, propositions may be fused together to generate a coherent abstractive summary. Recently, such a cascaded approach (Lebanoff et al., 2020), consisting of text fragment selection followed by a generation step, exhibited comparable or improved results over end-to-end systems. We advocate the potential of summary-source proposition-level alignment to extract cleaner and more accurate alignments than through the comOverall, we suggest that our released resources mon se"
2021.conll-1.25,P19-1102,0,0.343819,"position spans (§3), extracted automatically using Open IE (§5). 3 Dev and Test Alignment Datasets This section presents the manually-annotated development and test datasets for reference-source alignments, including their structure (§3.1), source data (§3.2) and annotation process (§3.3). These datasets allow direct tuning and evaluation of alignment algorithms, lacking in prior work (§1). 3.1 Dataset Structure In the typical MDS setting, a summarization instance consists of a set of topically-related documents, often termed a topic, and corresponding gold reference summary(ies) (NIST, 2014; Fabbri et al., 2019). For such an instance, we collect all alignments between each proposition span in the reference summary and the corresponding propositions, conveying the same information, in the source documents. We choose the proposition-span level, termed information unit (IU), as the basis for alignment following the rationale of similar SCU-level alignments in the well-established Pyramid evaluation method (§2). To facilitate crowdsourcing, we adapt a somewhat looser definition for our IUs (Task 1 below). Figure 1 illustrates some IU spans and their alignments. 3.2 Source Data 3.3 Annotation Our annotati"
2021.conll-1.25,D18-1443,0,0.027366,"documents. with their counterparts in source documents Such alignments were generated automatically over was shown as a useful auxiliary summarizalarge summarization datasets, most typically at the tion task, notably for generating training data full sentence level. Then, (noisy) training data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate"
2021.conll-1.25,D18-1450,0,0.0187465,"was favored over the more coarse sentence level, since a sys2 Background and Related Work tem summary sentence may include some propoAs mentioned above, several methods leveraged sitions that match the reference and some that automatically generated reference-source sentence don’t. Later works attempted to automate the Pyraalignments, to derive (noisy) training sets for sum- mid procedure, using Open IE (Yang et al., 2016; marization subtasks (Zhang et al., 2018; Cho et al., Peyrard and Eckle-Kohler, 2017) or Elementary 2019). For example, training datasets for sentence Discourse Units (EDU) (Hirao et al., 2018), to exsalience detection were derived from reference- tract proposition-level units. As propositional units source alignments, by marking as salient those (like SCUs) may share their arguments (such as source sentences that were aligned with a sum- in conjunctions and other constructions, e.g. “The mary sentence (e.g. Chen and Bansal (2018)). As boy went home and ate dinner”), and may be disanother example, Lebanoff et al. (2019) leveraged contiguous (“The boy...ate dinner”), Open IE out1 put, which satisfies these requirements, is best suitAll corresponding datasets and code are publicly ava"
2021.conll-1.25,2020.aacl-main.52,0,0.0181809,"scussion CNN/DailyMail (Fabbri et al., 2019)), albeit somewhat less coherent. An example of such potential summary, illustrated by an oracle-system summary derived from our supervised aligner predictions on CNN/DailyMail, is shown in Table 9. Alternatively, our data can contribute to the recent highlighting task (Arumae et al., 2019; Cho et al., 2020), where salient information fragments are marked inside a document, thus circumventing the need to generate coherent text. Further, propositions may be fused together to generate a coherent abstractive summary. Recently, such a cascaded approach (Lebanoff et al., 2020), consisting of text fragment selection followed by a generation step, exhibited comparable or improved results over end-to-end systems. We advocate the potential of summary-source proposition-level alignment to extract cleaner and more accurate alignments than through the comOverall, we suggest that our released resources mon sentence-level approach. To that end, we establish a new proposition-level alignment task by would encourage appealing future research on proposition-based summarization approaches, as releasing high-quality dev and test datasets, and an well as on developing improved al"
2021.conll-1.25,P19-1209,0,0.267409,"matically over was shown as a useful auxiliary summarizalarge summarization datasets, most typically at the tion task, notably for generating training data full sentence level. Then, (noisy) training data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate alignment methods were for proposition-level alignment, derived autoneither optimized nor e"
2021.conll-1.25,W04-1013,0,0.0517351,"ners, which in turn may trigger appealing research on proposition-based summarization methods. alignments to create a sentence fusion dataset: the input for each fusion instance consists of a pair of source sentences that are aligned to the same summary sentence, while the aligned summary sentence is regarded as the output fused sentence. The underlying sentence alignments, from which the above training datasets were derived, were extracted automatically from large summarization datasets. Alignments were detected using unsupervised sentence similarity measures, typically based on ROUGE score (Lin, 2004) (see §6 for more details). Typically, models trained over the alignmentbased datasets were evaluated only on the final summary. Yet, alignment quality, which determines the quality of the utilized training datasets, was never optimized or assessed explicitly, as we do in this paper. Notably, alignment of matching pieces of information provided the basis for the prominent Pyramid method for summarization evaluation (Nenkova and Passonneau, 2004), capturing information overlap between system summaries and reference summaries. Alignments were performed at the level of individual propositions, te"
2021.conll-1.25,2021.ccl-1.108,0,0.059214,"Missing"
2021.conll-1.25,N04-1019,0,0.748837,"nce Norodom Ranariddh and Sam Rainsy are out of the country following threats of arrest from strongman Hun Sen. Figure 1: Aligning IUs between a summary sentence (top) and sentences from documents (bottom). For fuller example see Appendix B. proposition-level alignment, we first developed an elaborate crowdsourcing methodology and created high-quality development and test datasets (§3). Next, we automatically derive a larger-scale training dataset consisting of 23K alignment instances from available Multi-Document Summarization (MDS) evaluation data, available as reliable Pyrmaid annotations (Nenkova and Passonneau, 2004) (§4). This data is utilized to train a supervised alignment baseline model (§5), which outperforms traditional unsupervised alignment approaches.1 Moreover, thanks to this novel training dataset, we show (§5.4) that our baseline aligner is capable of producing “abstractive” alignments, where there is almost no lexical overlap, while traditional aligners fail to do so. We further show intrinsically that our proposition-level aligner extracts better salient sentences than common sentence-level aligners. Notably, while our datasets are derived from MDS sources, the data and model are applicable"
2021.conll-1.25,P17-1100,0,0.0236324,"Content Units (SCUs) (similar to the information units marked in Fig. 1). Matching information at the proposition level was favored over the more coarse sentence level, since a sys2 Background and Related Work tem summary sentence may include some propoAs mentioned above, several methods leveraged sitions that match the reference and some that automatically generated reference-source sentence don’t. Later works attempted to automate the Pyraalignments, to derive (noisy) training sets for sum- mid procedure, using Open IE (Yang et al., 2016; marization subtasks (Zhang et al., 2018; Cho et al., Peyrard and Eckle-Kohler, 2017) or Elementary 2019). For example, training datasets for sentence Discourse Units (EDU) (Hirao et al., 2018), to exsalience detection were derived from reference- tract proposition-level units. As propositional units source alignments, by marking as salient those (like SCUs) may share their arguments (such as source sentences that were aligned with a sum- in conjunctions and other constructions, e.g. “The mary sentence (e.g. Chen and Bansal (2018)). As boy went home and ate dinner”), and may be disanother example, Lebanoff et al. (2019) leveraged contiguous (“The boy...ate dinner”), Open IE ou"
2021.conll-1.25,2020.acl-main.626,1,0.883925,"Missing"
2021.conll-1.25,N18-1081,1,0.823045,"Missing"
2021.conll-1.25,Q14-1018,0,0.0289173,"tage apa reference summary sentence is aligned with one proach, where we first align a summary IU with or two source sentences which are most similar the three source sentences most similar to it. The to it. We adjust this approach to the IU level, similarity score at this stage was a tuned combidenoted ROUGEIU . Accordingly, each summary nation of ROUGE, RoBERTa-MNLI (Liu et al., IU is matched with the k document IUs of highest 2019) and BERTscore (Zhang et al., 2019). Then, ROUGE similarity, choosing k = 2, which worked to find aligned spans, we applied a word aligner best on the dev set.4 (Sultan et al., 2014) to align words between a document sentence and the summary IU, and aligned 4 Our ROUGE similarity is an average of recall R-1, R-2, the consecutive text spans between the first and last and R-L, where the summary-IU is considered the reference. aligned words on each side (filtering pairs with too We also experimented with setting a threshold over the similarity score, but the common top-k approach worked best. few word alignments). 315 5.1 ROUGE-based Lexical Model 5.3 Supervised Model This model is a binary classifier, deciding whether two given IUs align. We follow the standard usage of RoB"
2021.conll-1.25,D18-1088,0,0.120249,"data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate alignment methods were for proposition-level alignment, derived autoneither optimized nor evaluated explicitly, making matically from available summarization evaluthem difficult to compare and improve. ation data. In addition, we crowdsourced dev In this paper, we establish summary-sou"
2021.deelio-1.7,2020.acl-main.698,0,0.0351452,"Missing"
2021.deelio-1.7,H05-1079,0,0.160223,"the embedding space. Wang et al. (2014) used entity names and Wikipedia anchors to align the embeddings of entities and words in the same space. In our work, we focus on converting knowledge relations from different knowledge sources to relations that are tailored to the NLI task. We then use this knowledge to illustrate the impact that both knowledge content and representation have on model performance. Related Work Natural Language Inference Early work in Natural Language Inference, also known as Textual Entailment (Dagan et al., 2005), exploited different features including logical rules (Bos and Markert, 2005), dependency parsers (Iftene and Balahur, 2007), and semantics (MacCartney and Manning, 2009), etc. With the development of large human annotated corpus such as the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre NLI Corpus (Williams et al., 2018), most recent work has explored various neural models. Different encoders have been studied to represent sentences, including LSTM (Bowman et al., 2016), tree-based CNN (Mou et al., 2015), TreeLSTM (Choi et al., 2018), etc. Previous work has explored using dynamic self-attention (Yoon et al., 2018), distance-based"
2021.deelio-1.7,2020.coling-main.118,0,0.0627268,"Missing"
2021.deelio-1.7,C16-2015,0,0.0158219,"ar, coffee) Premise: Lalley also is enthused about other bar efforts on behalf of the poor, most notably the Legal Assistance Center will operate out of the new courthouse. Hypothesis: Lalley is enthusiastic about the bar’s initiative to help the poor. Label: entailment External Knowledge Pair: ReverseEntailment(efforts, initiative) NLI Knowledge Pair: pos(efforts, initiative) Table 1: NLI & Knowledge Pair Example. NLI pair. For the i-th NLI pair, with premise P and hypothesis H, we first identify all the concepts (single word or key phrase) in P and H using Python Keyphrase Extraction (PKE) (Boudin, 2016). We then extract each NLI knowledge pair y(c1i , c2i ) where c1i ⊆ P (a concept in the premise), c2i ⊆ H (a concept in the hypothesis) and where there exists an NLI knowledge relation y between c1i and c2i . Considering Example (A) in Table 1, we see that c1i = ‘sugar’, c2i = ‘cream’, and y = pos(). There may be multiple NLI knowledge pairs in the i-th NLI pair of premise and hypothesis. the example in Table 1, Example (A). External knowledge pair refers to a pair of two concepts from external knowledge sources, connected by an external knowledge relation, for example RelatedTo(sugar, cream)."
2021.deelio-1.7,D15-1075,0,0.0477054,"ces to relations that are tailored to the NLI task. We then use this knowledge to illustrate the impact that both knowledge content and representation have on model performance. Related Work Natural Language Inference Early work in Natural Language Inference, also known as Textual Entailment (Dagan et al., 2005), exploited different features including logical rules (Bos and Markert, 2005), dependency parsers (Iftene and Balahur, 2007), and semantics (MacCartney and Manning, 2009), etc. With the development of large human annotated corpus such as the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre NLI Corpus (Williams et al., 2018), most recent work has explored various neural models. Different encoders have been studied to represent sentences, including LSTM (Bowman et al., 2016), tree-based CNN (Mou et al., 2015), TreeLSTM (Choi et al., 2018), etc. Previous work has explored using dynamic self-attention (Yoon et al., 2018), distance-based self-attention (Im and Cho, 2017) and reinforced self-attention (Shen et al., 2018) to enhance sentence encoders. Ensemble methods that combine multiple models have also shown improvements (Wang et al., 2017; Peters et al., 2018;"
2021.deelio-1.7,P16-1139,0,0.0119682,"Inference Early work in Natural Language Inference, also known as Textual Entailment (Dagan et al., 2005), exploited different features including logical rules (Bos and Markert, 2005), dependency parsers (Iftene and Balahur, 2007), and semantics (MacCartney and Manning, 2009), etc. With the development of large human annotated corpus such as the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre NLI Corpus (Williams et al., 2018), most recent work has explored various neural models. Different encoders have been studied to represent sentences, including LSTM (Bowman et al., 2016), tree-based CNN (Mou et al., 2015), TreeLSTM (Choi et al., 2018), etc. Previous work has explored using dynamic self-attention (Yoon et al., 2018), distance-based self-attention (Im and Cho, 2017) and reinforced self-attention (Shen et al., 2018) to enhance sentence encoders. Ensemble methods that combine multiple models have also shown improvements (Wang et al., 2017; Peters et al., 2018; Kim et al., 2019). Sun et al. (2019) improved masked language modeling with knowledge masking strategies, via entity-level and phrase-level masking, which showed improvement on NLI. Sun et al. (2020) then e"
2021.deelio-1.7,D15-1082,0,0.022258,"t are important for increased performance. • We perform extensive analysis and experimentation to support our findings (e.g., classification change analysis, adding knowledge incrementally, adding unseen knowledge, etc). 2 2.1 2.2 Knowledge Embeddings Using knowledge embeddings that represent the relations between entities has been useful in various downstream NLP tasks. Bordes et al. (2013) proposed TransE, a method which modeled relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. To address the issue of complex relation embeddings, Lin et al. (2015b) proposed CTransR in which the entity pairs are clustered into different groups and where the pairs in the same group share the same relation vector. Xiao et al. (2016) developed TransG, a generative Bayesian non-parametric infinite mixture embedding model, to handle multiple relation semantics of an entity pair. Further, Wang et al. (2019) integrated logic rules into a translation based knowledge graph embedding model. Their method automatically mined logic rules from triples in a knowledge graph. Previous work has also introduced external knowledge to learn better knowledge embeddings. Lin"
2021.deelio-1.7,P18-1224,0,0.021933,"018; Kim et al., 2019). Sun et al. (2019) improved masked language modeling with knowledge masking strategies, via entity-level and phrase-level masking, which showed improvement on NLI. Sun et al. (2020) then expanded this work to continual pre-training, which incrementally learns pretraining tasks through constant multi-task learning. Peters et al. (2019) investigated embedding knowledge bases into large-scale models in a multitask setup, seeing improvements on relationship extraction, entity typing, and word sense disambiguation. Using external knowledge to enhance NLI models specifically, Chen et al. (2018) obtained the semantic relations between words from WordNet and calculated the relation embeddings using pretrained TransE embeddings. Additionally, previ2.3 Language Model Challenges Pre-trained language models face several challenges and previous work has analyzed and illustrated their strenghts and weaknesses. Ettinger (2020) constructed a series of tests for language models and applied these to BERT to study strengths and weakness. Kassner and Schütze 60 KGs CN PPDB WN MNLI concepts 2. Knowledge Training extracted pairs BERT knowledge embedding 1. Knowledge Extraction Knowledge Classifier"
2021.deelio-1.7,P19-1441,0,0.0468435,"Missing"
2021.deelio-1.7,N19-1423,0,0.205483,"sport” is an entailment whereas the hypothesis “Old men are playing a sport” is a contradiction. Language modeling is a very common and important approach when considering the NLI task. The NLI state-of-the-art utilizes different language modeling techniques to learn the relations between the hypothesis and the premise. Yoon et al. (2018) used Dynamic Self-Attention (DSA) to learn sentence embeddings, Liu et al. (2019) proposed multi-task deep neural network (MT-DNN) for learning language representations in multiple NLU tasks, and Zhang et al. (2019b) combined semantic role labeling and BERT (Devlin et al., 2019) to explicitly absorb contextual semantics over a BERT framework. However, these approaches limit the source of information available for representing both the premise and hypothesis. Consider the following premise and hypothesis: People cut their expenses for the Golden years. People decrease their expenses for retirement. It is challenging to know that “Golden years” entails “retirement” if we rely only on the context within the two sentences. To illustrate how common this problem is, we conduct a manual analysis of BERT classification errors on the NLI task (specifically on the MNLI corpus"
2021.deelio-1.7,D15-1191,0,0.0627648,"Missing"
2021.deelio-1.7,2020.tacl-1.3,0,0.0199127,"ning. Peters et al. (2019) investigated embedding knowledge bases into large-scale models in a multitask setup, seeing improvements on relationship extraction, entity typing, and word sense disambiguation. Using external knowledge to enhance NLI models specifically, Chen et al. (2018) obtained the semantic relations between words from WordNet and calculated the relation embeddings using pretrained TransE embeddings. Additionally, previ2.3 Language Model Challenges Pre-trained language models face several challenges and previous work has analyzed and illustrated their strenghts and weaknesses. Ettinger (2020) constructed a series of tests for language models and applied these to BERT to study strengths and weakness. Kassner and Schütze 60 KGs CN PPDB WN MNLI concepts 2. Knowledge Training extracted pairs BERT knowledge embedding 1. Knowledge Extraction Knowledge Classifier Relation Label Space knowledge embedding ERNIE MNLI data 3. ERNIE-NLI Figure 1: Components of the setup: (1) Knowledge Extraction Phase: Extracts knowledge content from external knowledge sources; (2) Knowledge Training Phase: Learns knowledge embeddings adapted to the NLI task; and (3) ERNIE-NLI: Trains NLI model with the integ"
2021.deelio-1.7,W09-3714,0,0.0323806,"ign the embeddings of entities and words in the same space. In our work, we focus on converting knowledge relations from different knowledge sources to relations that are tailored to the NLI task. We then use this knowledge to illustrate the impact that both knowledge content and representation have on model performance. Related Work Natural Language Inference Early work in Natural Language Inference, also known as Textual Entailment (Dagan et al., 2005), exploited different features including logical rules (Bos and Markert, 2005), dependency parsers (Iftene and Balahur, 2007), and semantics (MacCartney and Manning, 2009), etc. With the development of large human annotated corpus such as the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre NLI Corpus (Williams et al., 2018), most recent work has explored various neural models. Different encoders have been studied to represent sentences, including LSTM (Bowman et al., 2016), tree-based CNN (Mou et al., 2015), TreeLSTM (Choi et al., 2018), etc. Previous work has explored using dynamic self-attention (Yoon et al., 2018), distance-based self-attention (Im and Cho, 2017) and reinforced self-attention (Shen et al., 2018) to enhanc"
2021.deelio-1.7,N13-1092,0,0.0694894,"Missing"
2021.deelio-1.7,P15-1009,0,0.0321712,"ifferent groups and where the pairs in the same group share the same relation vector. Xiao et al. (2016) developed TransG, a generative Bayesian non-parametric infinite mixture embedding model, to handle multiple relation semantics of an entity pair. Further, Wang et al. (2019) integrated logic rules into a translation based knowledge graph embedding model. Their method automatically mined logic rules from triples in a knowledge graph. Previous work has also introduced external knowledge to learn better knowledge embeddings. Lin et al. (2015a) and Luo et al. (2015) utilized relation paths and Guo et al. (2015) integrated additional semantic information and enforced the embedding space to be semantically smooth so that entities in the same semantic category were close to each other in the embedding space. Wang et al. (2014) used entity names and Wikipedia anchors to align the embeddings of entities and words in the same space. In our work, we focus on converting knowledge relations from different knowledge sources to relations that are tailored to the NLI task. We then use this knowledge to illustrate the impact that both knowledge content and representation have on model performance. Related Work N"
2021.deelio-1.7,N18-1202,0,0.0554276,"Bowman et al., 2015) and the Multi-Genre NLI Corpus (Williams et al., 2018), most recent work has explored various neural models. Different encoders have been studied to represent sentences, including LSTM (Bowman et al., 2016), tree-based CNN (Mou et al., 2015), TreeLSTM (Choi et al., 2018), etc. Previous work has explored using dynamic self-attention (Yoon et al., 2018), distance-based self-attention (Im and Cho, 2017) and reinforced self-attention (Shen et al., 2018) to enhance sentence encoders. Ensemble methods that combine multiple models have also shown improvements (Wang et al., 2017; Peters et al., 2018; Kim et al., 2019). Sun et al. (2019) improved masked language modeling with knowledge masking strategies, via entity-level and phrase-level masking, which showed improvement on NLI. Sun et al. (2020) then expanded this work to continual pre-training, which incrementally learns pretraining tasks through constant multi-task learning. Peters et al. (2019) investigated embedding knowledge bases into large-scale models in a multitask setup, seeing improvements on relationship extraction, entity typing, and word sense disambiguation. Using external knowledge to enhance NLI models specifically, Che"
2021.deelio-1.7,W07-1421,0,0.0734714,"d entity names and Wikipedia anchors to align the embeddings of entities and words in the same space. In our work, we focus on converting knowledge relations from different knowledge sources to relations that are tailored to the NLI task. We then use this knowledge to illustrate the impact that both knowledge content and representation have on model performance. Related Work Natural Language Inference Early work in Natural Language Inference, also known as Textual Entailment (Dagan et al., 2005), exploited different features including logical rules (Bos and Markert, 2005), dependency parsers (Iftene and Balahur, 2007), and semantics (MacCartney and Manning, 2009), etc. With the development of large human annotated corpus such as the Stanford Natural Language Inference Corpus (Bowman et al., 2015) and the Multi-Genre NLI Corpus (Williams et al., 2018), most recent work has explored various neural models. Different encoders have been studied to represent sentences, including LSTM (Bowman et al., 2016), tree-based CNN (Mou et al., 2015), TreeLSTM (Choi et al., 2018), etc. Previous work has explored using dynamic self-attention (Yoon et al., 2018), distance-based self-attention (Im and Cho, 2017) and reinforce"
2021.deelio-1.7,D19-1005,0,0.0187697,"(Yoon et al., 2018), distance-based self-attention (Im and Cho, 2017) and reinforced self-attention (Shen et al., 2018) to enhance sentence encoders. Ensemble methods that combine multiple models have also shown improvements (Wang et al., 2017; Peters et al., 2018; Kim et al., 2019). Sun et al. (2019) improved masked language modeling with knowledge masking strategies, via entity-level and phrase-level masking, which showed improvement on NLI. Sun et al. (2020) then expanded this work to continual pre-training, which incrementally learns pretraining tasks through constant multi-task learning. Peters et al. (2019) investigated embedding knowledge bases into large-scale models in a multitask setup, seeing improvements on relationship extraction, entity typing, and word sense disambiguation. Using external knowledge to enhance NLI models specifically, Chen et al. (2018) obtained the semantic relations between words from WordNet and calculated the relation embeddings using pretrained TransE embeddings. Additionally, previ2.3 Language Model Challenges Pre-trained language models face several challenges and previous work has analyzed and illustrated their strenghts and weaknesses. Ettinger (2020) constructe"
2021.deelio-1.7,D19-1250,0,0.048763,"pts 2. Knowledge Training extracted pairs BERT knowledge embedding 1. Knowledge Extraction Knowledge Classifier Relation Label Space knowledge embedding ERNIE MNLI data 3. ERNIE-NLI Figure 1: Components of the setup: (1) Knowledge Extraction Phase: Extracts knowledge content from external knowledge sources; (2) Knowledge Training Phase: Learns knowledge embeddings adapted to the NLI task; and (3) ERNIE-NLI: Trains NLI model with the integration of our learned knowledge embeddings. 3.2 (2020) added a component that focused on negation to the LAMA (LAnguage Model Analysis) evaluation framework (Petroni et al., 2019), showing that BERT failed on most negated statements. Talmor et al. (2019) designed eight reasoning tasks and illustrated that reasoning abilities are strongly context-dependent. Specific to NLI, Richardson et al. (2019) constructed challenging NLI datasets with new semantic fragments and showed that language models, though trained on NLI benchmark datasets, did not perform well on the new fragments. This previous work has shown that when applying pre-trained language models to a new task, a new domain, or new data variations, these models do not always perform well and additional knowledge m"
2021.deelio-1.7,P19-1139,0,0.133708,"s are playing soccer”, the hypothesis “Young men are playing a sport” is an entailment whereas the hypothesis “Old men are playing a sport” is a contradiction. Language modeling is a very common and important approach when considering the NLI task. The NLI state-of-the-art utilizes different language modeling techniques to learn the relations between the hypothesis and the premise. Yoon et al. (2018) used Dynamic Self-Attention (DSA) to learn sentence embeddings, Liu et al. (2019) proposed multi-task deep neural network (MT-DNN) for learning language representations in multiple NLU tasks, and Zhang et al. (2019b) combined semantic role labeling and BERT (Devlin et al., 2019) to explicitly absorb contextual semantics over a BERT framework. However, these approaches limit the source of information available for representing both the premise and hypothesis. Consider the following premise and hypothesis: People cut their expenses for the Golden years. People decrease their expenses for retirement. It is challenging to know that “Golden years” entails “retirement” if we rely only on the context within the two sentences. To illustrate how common this problem is, we conduct a manual analysis of BERT classi"
2021.deelio-1.7,2020.tacl-1.48,0,0.0682875,"Missing"
2021.deelio-1.7,D14-1167,0,0.0356869,"ation semantics of an entity pair. Further, Wang et al. (2019) integrated logic rules into a translation based knowledge graph embedding model. Their method automatically mined logic rules from triples in a knowledge graph. Previous work has also introduced external knowledge to learn better knowledge embeddings. Lin et al. (2015a) and Luo et al. (2015) utilized relation paths and Guo et al. (2015) integrated additional semantic information and enforced the embedding space to be semantically smooth so that entities in the same semantic category were close to each other in the embedding space. Wang et al. (2014) used entity names and Wikipedia anchors to align the embeddings of entities and words in the same space. In our work, we focus on converting knowledge relations from different knowledge sources to relations that are tailored to the NLI task. We then use this knowledge to illustrate the impact that both knowledge content and representation have on model performance. Related Work Natural Language Inference Early work in Natural Language Inference, also known as Textual Entailment (Dagan et al., 2005), exploited different features including logical rules (Bos and Markert, 2005), dependency parse"
2021.deelio-1.7,N18-1101,0,0.312075,"wledge relations are mapped to NLI knowledge relations (Section 4.2). In this step, we not only represent external knowledge from different sources in a unified way, but also convert external knowledge content to the NLI task. Second, the polarity is learned (Section 4.3): NLI knowl• We propose a knowledge analysis framework, ERNIE-NLI, that allow us to directly control and analyze adapted knowledge input, to investigate 59 the characteristics of knowledge that result in a performance increase on the NLI task. ous work has explored injecting lexical knowledge into pre-trained models for MNLI (Williams et al., 2018), among other tasks (Lauscher et al., 2020; Levine et al., 2020). Zhang et al. (2019a) adopted a knowledgeable encoder to inject the knowledge information into language representation. However, in contrast to our work, their external knowledge was not trained specifically for the NLI task. • We present findings that show strong correlations between knowledge polarity and downstream performance, illustrating the knowledge features that are important for increased performance. • We perform extensive analysis and experimentation to support our findings (e.g., classification change analysis, addin"
2021.deelio-1.7,P16-1219,0,0.0568781,"Missing"
2021.eacl-main.192,D18-1454,1,0.787841,"nstructs probes from existing knowledge sources. Commonsense Reasoning: Recent commonsense reasoning datasets (Bhagavatula et al., 2020; Zellers et al., 2018; Zhou et al., 2019; Sap et al., 2019b; Bisk et al., 2020; Lin et al., 2019b; Zellers et al., 2019; Ostermann et al., 2019) have motivated research in several domains of commonsense: abductive, grounded, temporal, social, and physical. Commonsense reasoning can be learned either by KGs pre-training (Bosselut et al., 2019; Bosselut and Choi, 2019; Ye et al., 2019) or by integrating explicit knowledge (Chen et al., 2017; Mitra et al., 2018; Bauer et al., 2018; Lin et al., 2019a; Zhang et al., 2019; Xiong et al., 2019). We show how finding nuanced knowledge for successful commonsense reasoning can be quantitatively examined. Commonsense Knowledge Analysis: Zhang et al. (2020) presented a categorization of essential knowledge for the Winograd Schema Challenge (Levesque et al., 2012) via human annotation to identify what knowledge was required for better commonsense reasoning. Ma et al. (2019) investigated how KG integration methods affected model performance on different tasks and found that the degree of domain overlap between the KG and the task p"
2021.eacl-main.192,C18-1321,0,0.0122877,"and automatically constructs probes from existing knowledge sources. Commonsense Reasoning: Recent commonsense reasoning datasets (Bhagavatula et al., 2020; Zellers et al., 2018; Zhou et al., 2019; Sap et al., 2019b; Bisk et al., 2020; Lin et al., 2019b; Zellers et al., 2019; Ostermann et al., 2019) have motivated research in several domains of commonsense: abductive, grounded, temporal, social, and physical. Commonsense reasoning can be learned either by KGs pre-training (Bosselut et al., 2019; Bosselut and Choi, 2019; Ye et al., 2019) or by integrating explicit knowledge (Chen et al., 2017; Mitra et al., 2018; Bauer et al., 2018; Lin et al., 2019a; Zhang et al., 2019; Xiong et al., 2019). We show how finding nuanced knowledge for successful commonsense reasoning can be quantitatively examined. Commonsense Knowledge Analysis: Zhang et al. (2020) presented a categorization of essential knowledge for the Winograd Schema Challenge (Levesque et al., 2012) via human annotation to identify what knowledge was required for better commonsense reasoning. Ma et al. (2019) investigated how KG integration methods affected model performance on different tasks and found that the degree of domain overlap between t"
2021.eacl-main.192,S19-1012,0,0.346807,"t an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KGto-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHowbased KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation.1 1 Introduction Recently, several datasets (Sap et al., 2019b; Huang et al., 2019; Bhagavatula et al., 2020; Ta"
2021.eacl-main.192,D19-1332,0,0.0179874,"such as comparison, conjunction, and composition. Zhou et al. (2020a) created logically equivalent probes to evaluate robustness on commonsense tasks to syntax. Kwon et al. (2019) proposed tests based on ConceptNet to measure what types of commonsense MLMs understand. Our work instead focuses on probing models for causal, social commonsense in both the MLM and QA setup before and after KG integration and fine-tuning, and automatically constructs probes from existing knowledge sources. Commonsense Reasoning: Recent commonsense reasoning datasets (Bhagavatula et al., 2020; Zellers et al., 2018; Zhou et al., 2019; Sap et al., 2019b; Bisk et al., 2020; Lin et al., 2019b; Zellers et al., 2019; Ostermann et al., 2019) have motivated research in several domains of commonsense: abductive, grounded, temporal, social, and physical. Commonsense reasoning can be learned either by KGs pre-training (Bosselut et al., 2019; Bosselut and Choi, 2019; Ye et al., 2019) or by integrating explicit knowledge (Chen et al., 2017; Mitra et al., 2018; Bauer et al., 2018; Lin et al., 2019a; Zhang et al., 2019; Xiong et al., 2019). We show how finding nuanced knowledge for successful commonsense reasoning can be quantitatively"
2021.eacl-main.192,N19-1421,0,0.509393,"9) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHowbased KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation.1 1 Introduction Recently, several datasets (Sap et al., 2019b; Huang et al., 2019; Bhagavatula et al., 2020; Talmor et al., 2019b) have been released to tackle the challenge of commonsense reasoning. While deep pretrained 1 Our code and commonsense probes will be publicly available on our webpage. language-models (LMs) (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Yang et al., 2019) have been at the top of most leaderboards, they still have shortcomings when it comes to commonsense reasoning (Sap et al., 2019b; Rajani et al., 2019; Mitra et al., 2019). Thus, incorporating knowledge graph (KG) information into these models is an active area of research (Lin et al., 2019a; Sun et al., 2019; Mitra et al.,"
2021.eacl-main.192,P19-1417,0,0.0239191,"e Reasoning: Recent commonsense reasoning datasets (Bhagavatula et al., 2020; Zellers et al., 2018; Zhou et al., 2019; Sap et al., 2019b; Bisk et al., 2020; Lin et al., 2019b; Zellers et al., 2019; Ostermann et al., 2019) have motivated research in several domains of commonsense: abductive, grounded, temporal, social, and physical. Commonsense reasoning can be learned either by KGs pre-training (Bosselut et al., 2019; Bosselut and Choi, 2019; Ye et al., 2019) or by integrating explicit knowledge (Chen et al., 2017; Mitra et al., 2018; Bauer et al., 2018; Lin et al., 2019a; Zhang et al., 2019; Xiong et al., 2019). We show how finding nuanced knowledge for successful commonsense reasoning can be quantitatively examined. Commonsense Knowledge Analysis: Zhang et al. (2020) presented a categorization of essential knowledge for the Winograd Schema Challenge (Levesque et al., 2012) via human annotation to identify what knowledge was required for better commonsense reasoning. Ma et al. (2019) investigated how KG integration methods affected model performance on different tasks and found that the degree of domain overlap between the KG and the task plays a crucial role in performance. We further investigate t"
2021.eacl-main.211,W18-5513,0,0.0167894,"Researchers have also collected datasets for various related topics, such as rumor detection (Kwon et al., 2017; Ma et al., 2016), and propaganda detection (Da San Martino et al., 2020; Barr´on-Cedeno et al., 2019). Besides classifying the veracity of news articles, researchers have also explored related problems, such as predicting the reliability of news sites (Baly et al., 2018), identifying factcheck worthy sentences (Wright and Augenstein, 2020), among other tasks. Several recent papers also focus on measuring the trustworthiness of single statements (Wang, 2017; Pomerleau and Rao, 2017; Alhindi et al., 2018). In this work, we focus on article-level classification because of its relevance to applications, like news feeds, that operate at the article level. 2483 Dataset 5 NELA FakeNewsNet6 r/Fakeddit7 Size Article Source Label Type 136K/713K/1.12M 603K 1.06M News outlets Fact-checking websites Social Media (Reddit) Site-level Article-level Site-level Table 2: Statistics and properties of three recent large-scale unreliable news datasets. The three statistics of NELA dataset sizes correspond to its three versions released in 2017, 2018 and 2019, respectively. Pitfalls in Data Collection. Datasets co"
2021.eacl-main.211,D18-1389,0,0.118617,"uthor(s) is beyond the scope of this work. To mitigate the problem of surfacing unreliable news content, various websites (e.g., PolitiFact2 , Media Bias/Fact Check (MBFC)3 , GossipCop4 , etc.) determine the reliability of news by manually fact-checking the important claims in given news articles. Beyond requiring investigative expertise, manual fact-checking is time-consuming and is thus limited to only a small set of selected news articles. Recent research has explored automating this process using machine learning methods to automatically determine news veracity (P´erez-Rosas et al., 2018; Baly et al., 2018; Nie et al., 2019; Wright and Augenstein, 2020). These efforts were made possible due to the availability of large-scale unreliable news detection datasets (Horne et al., 2018b; Shu et al., 2017; Wang, 2017). In our work, we examine if these datasets accurately reflect the real difficulty of this task or if there are any hidden biases in the datasets. Specifically, we study different methods of dataset construction (e.g., how the data was collected, how the data was split, etc.) and show that the assessed difficulty of the task is sensitive to how carefully different factors are considered wh"
2021.eacl-main.211,N19-1423,0,0.0444739,"Missing"
2021.eacl-main.211,D19-1107,0,0.0476436,"Missing"
2021.eacl-main.211,2021.ccl-1.108,0,0.0290589,"Missing"
2021.eacl-main.211,N18-2017,0,0.153708,"le Source Label Type 136K/713K/1.12M 603K 1.06M News outlets Fact-checking websites Social Media (Reddit) Site-level Article-level Site-level Table 2: Statistics and properties of three recent large-scale unreliable news datasets. The three statistics of NELA dataset sizes correspond to its three versions released in 2017, 2018 and 2019, respectively. Pitfalls in Data Collection. Datasets collected through crowd-sourcing or scraping the Internet have the advantage of much better scalability compared to expert-annotated datasets. However, these automatic processes are prone to hidden pitfalls. Gururangan et al. (2018); Poliak et al. (2018) show that crowd-sourcing “Natural Language Inference” datasets leads to various dataset biases. Similar observations have been made for “Fact Verification” datasets (Schuster et al., 2019). Splitting data– —for training, testing, and validation—–is another important procedure in creating datasets that can lead to several problems. For example, Geva et al. (2019) show that models may just learn the patterns of certain annotators in a random split. Lewis et al. (2020b) demonstrated a significant overlap in current open-domain QA datasets. When present, these unexpected bia"
2021.eacl-main.211,2020.lrec-1.755,0,0.106117,"related tasks have been receiving an increasing focus as news sources have become more accessible in recent years. A lot of effort has been put into collecting high-quality datasets. Wang (2017); Shu et al. (2017) collected manually labeled statements or news articles from fact-checking websites. The NELA datasets (Horne et al., 2018b; Nørregaard et al., 2019; Gruppi et al., 2020) scrape news articles directly from news outlets and use the manually annotated labels from Media Bias/Fact Check (MBFC) as site-level annotations. Social media is also a popular resource for collecting news stories (Nakamura et al., 2020; Santia and Williams, 2018; Mitra and Gilbert, 2015). Researchers have also collected datasets for various related topics, such as rumor detection (Kwon et al., 2017; Ma et al., 2016), and propaganda detection (Da San Martino et al., 2020; Barr´on-Cedeno et al., 2019). Besides classifying the veracity of news articles, researchers have also explored related problems, such as predicting the reliability of news sites (Baly et al., 2018), identifying factcheck worthy sentences (Wright and Augenstein, 2020), among other tasks. Several recent papers also focus on measuring the trustworthiness of s"
2021.eacl-main.211,C18-1287,0,0.0406838,"Missing"
2021.eacl-main.211,S18-2023,0,0.060534,"Missing"
2021.eacl-main.211,D19-1341,0,0.196842,"le news datasets. The three statistics of NELA dataset sizes correspond to its three versions released in 2017, 2018 and 2019, respectively. Pitfalls in Data Collection. Datasets collected through crowd-sourcing or scraping the Internet have the advantage of much better scalability compared to expert-annotated datasets. However, these automatic processes are prone to hidden pitfalls. Gururangan et al. (2018); Poliak et al. (2018) show that crowd-sourcing “Natural Language Inference” datasets leads to various dataset biases. Similar observations have been made for “Fact Verification” datasets (Schuster et al., 2019). Splitting data– —for training, testing, and validation—–is another important procedure in creating datasets that can lead to several problems. For example, Geva et al. (2019) show that models may just learn the patterns of certain annotators in a random split. Lewis et al. (2020b) demonstrated a significant overlap in current open-domain QA datasets. When present, these unexpected biases or overlaps in datasets can significantly undermine the utility of a dataset and lead to deceptively promising results that are in part due to artifacts of flaws in the dataset rather than successfully model"
2021.eacl-main.211,W14-2508,0,0.0327325,"t overlap in current open-domain QA datasets. When present, these unexpected biases or overlaps in datasets can significantly undermine the utility of a dataset and lead to deceptively promising results that are in part due to artifacts of flaws in the dataset rather than successfully modeling the intended task. Automated Fact Checking for Statements. Automated fact checking is an important task closely related to unreliable news detection, yet is constructed in a more controlled manner. This task focuses on strictly judging the factuality of one single statement instead of an entire article. Vlachos and Riedel (2014) first constructed a dataset with 106 claims from fact-checking websites with paired labels. FEVER (Thorne et al., 2018) is currently the largest scale fact-verification dataset, where 185,445 claims were generated by modifying sentences from Wikipedia. Both the altered claims and the ground truth supporting evidence are included in the dataset. Existing effective approaches for fact-verification include self-attention based networks (Nie et al., 2019), large-scale pretrained transformers (Soleimani et al., 2020), neural retrieval methods (Lewis et al., 2020a), and reasoning 5 dataverse.harvar"
2021.eacl-main.211,P17-2067,0,0.175914,"ility of news by manually fact-checking the important claims in given news articles. Beyond requiring investigative expertise, manual fact-checking is time-consuming and is thus limited to only a small set of selected news articles. Recent research has explored automating this process using machine learning methods to automatically determine news veracity (P´erez-Rosas et al., 2018; Baly et al., 2018; Nie et al., 2019; Wright and Augenstein, 2020). These efforts were made possible due to the availability of large-scale unreliable news detection datasets (Horne et al., 2018b; Shu et al., 2017; Wang, 2017). In our work, we examine if these datasets accurately reflect the real difficulty of this task or if there are any hidden biases in the datasets. Specifically, we study different methods of dataset construction (e.g., how the data was collected, how the data was split, etc.) and show that the assessed difficulty of the task is sensitive to how carefully different factors are considered when building and using these datasets. Our investigation begins with data collection procedures: we look at the source of news stories (news outlets, social media, fact-checking websites, etc.) as well as the"
2021.eacl-main.211,2020.emnlp-demos.6,0,0.0565504,"Missing"
2021.eacl-main.211,2020.findings-emnlp.43,0,0.0342344,"work. To mitigate the problem of surfacing unreliable news content, various websites (e.g., PolitiFact2 , Media Bias/Fact Check (MBFC)3 , GossipCop4 , etc.) determine the reliability of news by manually fact-checking the important claims in given news articles. Beyond requiring investigative expertise, manual fact-checking is time-consuming and is thus limited to only a small set of selected news articles. Recent research has explored automating this process using machine learning methods to automatically determine news veracity (P´erez-Rosas et al., 2018; Baly et al., 2018; Nie et al., 2019; Wright and Augenstein, 2020). These efforts were made possible due to the availability of large-scale unreliable news detection datasets (Horne et al., 2018b; Shu et al., 2017; Wang, 2017). In our work, we examine if these datasets accurately reflect the real difficulty of this task or if there are any hidden biases in the datasets. Specifically, we study different methods of dataset construction (e.g., how the data was collected, how the data was split, etc.) and show that the assessed difficulty of the task is sensitive to how carefully different factors are considered when building and using these datasets. Our invest"
2021.eacl-main.211,2020.acl-main.549,0,0.0152515,"l., 2018) is currently the largest scale fact-verification dataset, where 185,445 claims were generated by modifying sentences from Wikipedia. Both the altered claims and the ground truth supporting evidence are included in the dataset. Existing effective approaches for fact-verification include self-attention based networks (Nie et al., 2019), large-scale pretrained transformers (Soleimani et al., 2020), neural retrieval methods (Lewis et al., 2020a), and reasoning 5 dataverse.harvard.edu/dataverse/nela github.com/KaiDMML/FakeNewsNet 7 github.com/entitize/Fakeddit 6 on semantic-level graphs (Zhong et al., 2020). 3 Unreliable News Datasets Collecting high-quality datasets plays an important role in automatic unreliable news detection research. Here we review dataset collection strategies used in constructing recent datasets and point out some hidden pitfalls in these procedures. 3.1 Data Collection Strategies Unreliable news detection is usually formalized as a classification task. Accordingly, constructing a dataset requires collecting pairs of news articles and labels. News Articles: Each individual news outlet has its own website where news articles are published. The easiest way to collect a larg"
2021.eacl-main.211,C16-1038,0,0.0467257,"Missing"
2021.eacl-main.211,N18-1074,1,0.905065,"Missing"
2021.emnlp-demo.33,P19-1409,1,0.902456,"Missing"
2021.emnlp-demo.33,P99-1071,0,0.178254,"grasping the topic, and render an intuitive medium for navigating through the information. The abstractive summaries generated at real-time expose concise details for any combination of sub-topics of choice. Furthermore, we innovatively employ coreference resolution and proposition alignment to generate fine-grained opendomain facets. Attaining information of interest from large document sets has been approached with different techniques. A vast amount of research has been conducted on multi-document summarization, as a method for presenting the central aspects of a target set of texts (e.g. Barzilay et al., 1999; Haghighi and Vanderwende, 2009; Bing et al., 2015; Yasunaga et al., 2017), where query-focused summarization (Dang, 2005) biases the output summary around a given query (e.g. Daumé III and Marcu, 2006; Baumel et al., 2018; Xu and Lapata, 2020). Recognizing the need for dynamically acquiring a broader or deeper scope of the source texts, exploratory search (Marchionini, 2006; White and Roth, 2009) was coined as an umbrella term for allowing more dynamic interactive exploration of in- 6 Conclusion and Future Work formation. Adapting the summarization paradigm to the exploratory setting, intera"
2021.emnlp-demo.33,P14-1086,0,0.0278025,"chionini, 2006; White and Roth, 2009) was coined as an umbrella term for allowing more dynamic interactive exploration of in- 6 Conclusion and Future Work formation. Adapting the summarization paradigm to the exploratory setting, interactive summariza- In this paper, we presented iFACET S UM, a novel tion enables a user to refine or expand on a sum- text exploration approach and tool over large docmary via different modes of interaction. For exam- ument sets, which incorporates faceted search ple, Shapira et al. (2021), Avinesh et al. (2018) and into interactive summarization. Its faceted naviBaumel et al. (2014) provide a limited (or no) initial gation design provides a user with an overview of summary on the document set, and support iterative the topic and the ability to gradually investigate interaction, via queries or preference highlights, to subtopics of interest, communicating concise inforupdate the summary. However, the succinct initial mation via multi-facet abstractive summarization. summary, possibly accompanied by few suggested Fine-grained facet-values are generated from the queries, do not display the full scope of the source source texts based on cross-document coreference texts, whic"
2021.emnlp-demo.33,2021.findings-emnlp.225,1,0.70029,"ted using crossdocument (CD) coreference resolution pipelines, while Statements via a proposition alignment pipeline, described next.2 Concepts. We found that identifying and grouping together significant co-occurring events within the source document collection helps to expose and emphasize the notable concepts in the topic. To that end, we employ CD event coreference resolution which detects these concepts. CD coreference resolution (Lee et al., 2012) clusters text mentions that refer to the same event or entity across multiple documents. Presently, the Cross-Document Language Model (CDLM) (Caciularu et al., 2021) is the state-of-the-art for CD coreference resolution. This model is pretrained on multiple related documents via cross-document masking, encouraging the model to learn crossdocument and long-range relationships. Specifically, we employ the CDLM version fine-tuned for coreference on the ECB+ corpus (Cybulska and Vossen, 2014). This model does not include a mention detection component, but rather expects relevant mentions to be marked within the input texts. We therefore leverage the mention detection ability of the model by Cattan et al. (2021). Once we have obtained the coreference clusters"
2021.emnlp-demo.33,2021.findings-acl.453,1,0.82721,"ly, the Cross-Document Language Model (CDLM) (Caciularu et al., 2021) is the state-of-the-art for CD coreference resolution. This model is pretrained on multiple related documents via cross-document masking, encouraging the model to learn crossdocument and long-range relationships. Specifically, we employ the CDLM version fine-tuned for coreference on the ECB+ corpus (Cybulska and Vossen, 2014). This model does not include a mention detection component, but rather expects relevant mentions to be marked within the input texts. We therefore leverage the mention detection ability of the model by Cattan et al. (2021). Once we have obtained the coreference clusters from CDLM, events whose mentions are predominantly verbs are filtered out,3 since those usually present specific actions that tend to be less informative compared to nominal types that refer to more generic events (e.g., “said”, “found” “increase” compared to “unemployment”, “poverty”, “crash”). CD event coreference resolution separates specific event instances, hence differentiating between clusters of similar event types with different arguments (e.g., “unemployment” in Navajo vs. “unemployment” in Cayuga). Since generic event types, like “une"
2021.emnlp-demo.33,2020.tacl-1.5,0,0.0130802,"architecture. CD = cross-document, WD = within-document. Entities. The Entities facet-values help the user focus on entities such as people (e.g., ""Clinton""), locations (e.g., ""New York""), organizations (e.g., ""FBI"") and others (e.g., ""the casino""). We created a separate pipeline for CD entity coreference resolution, since we observed subpar performance when applying the above CD coreference pipeline for entity coreference.5 Unlike event coreference, mostly studied in the CD setting, entity coreference has recently seen impressive progress in the within-document (WD) setting (Wu et al., 2020; Joshi et al., 2020). Hence, we leverage WD entity coreference in our entity recognition pipeline, which comprises three main steps. (1) We use SpanBERT6 (Joshi et al., 2020), a state-of-the-art transformer-based LM for WD entity coreference resolution, to detect and cluster coreferring entity mentions within each separate document. (2) The entity mentions detected in the first step are marked as input for a CD entity coreference reolution model. To overcome ECB+ entity scarcity referred earlier, we use an alternative model that is trained on the WEC-Eng dataset (Eirew et al., 2021).7 (3) Finally, we apply agglom"
2021.emnlp-demo.33,S18-2001,0,0.0394528,"Missing"
2021.emnlp-demo.33,2020.acl-srw.26,0,0.0190972,"ils in Appendix A.2). This text is then given as input to BART (Lewis et al., 2020), a denoising sequence-to-sequence model fine-tuned on the single-document abstractive summarization task.8 iFACET S UM presents abstractive rather than extractive summaries due to their enhanced readability, particularly when summarizing a set of related sentences. This choice follows prior work, which 8 We use the huggingface model from https://hugg ingface.co/facebook/bart-large-cnn. 286 showed that fusing sentences with shared points of coreference potentially facilitates coherence of abstractive summaries (Lebanoff et al., 2020). Indeed, in an internal manual assessment of 30 random individual summaries produced by iFACET S UM, with 5 readability measures (Dang, 2006), testers found overall that the summaries are highly readable. To verify that factuality is not compromised, an additional inspection found that these summaries were also factually consistent to the input text, with 28 out of 30 sampled sentences marked as consistent. See Appendix B.3 for scores and more details on these assessments. 4 System Experiments iFACET S UM aims to provide an effective means of information seeking in scenarios that require lear"
2021.emnlp-demo.33,D12-1045,0,0.0413212,"e 5 in Appendix). 3 Backend Algorithms 3.1 Coreference-based Facet Formation As described in §2.1, there are three main facets. Concepts and Entities are extracted using crossdocument (CD) coreference resolution pipelines, while Statements via a proposition alignment pipeline, described next.2 Concepts. We found that identifying and grouping together significant co-occurring events within the source document collection helps to expose and emphasize the notable concepts in the topic. To that end, we employ CD event coreference resolution which detects these concepts. CD coreference resolution (Lee et al., 2012) clusters text mentions that refer to the same event or entity across multiple documents. Presently, the Cross-Document Language Model (CDLM) (Caciularu et al., 2021) is the state-of-the-art for CD coreference resolution. This model is pretrained on multiple related documents via cross-document masking, encouraging the model to learn crossdocument and long-range relationships. Specifically, we employ the CDLM version fine-tuned for coreference on the ECB+ corpus (Cybulska and Vossen, 2014). This model does not include a mention detection component, but rather expects relevant mentions to be ma"
2021.emnlp-demo.33,2020.acl-main.703,0,0.0214699,"e graph (more details in Appendix A.2). 3.2 Abstractive Facet Summarization In the standard summarization setting, a system receives a single or multiple documents as input, as well as a query in the query-focused task. In our case, the input is a set of sentences that have one or more selected facet-values in common, effectively providing a multi-facet summary. Given the set of sentences that correspond to the facet-value selection(s), these sentences are concatenated, ordered by their position in their source document (more details in Appendix A.2). This text is then given as input to BART (Lewis et al., 2020), a denoising sequence-to-sequence model fine-tuned on the single-document abstractive summarization task.8 iFACET S UM presents abstractive rather than extractive summaries due to their enhanced readability, particularly when summarizing a set of related sentences. This choice follows prior work, which 8 We use the huggingface model from https://hugg ingface.co/facebook/bart-large-cnn. 286 showed that fusing sentences with shared points of coreference potentially facilitates coherence of abstractive summaries (Lebanoff et al., 2020). Indeed, in an internal manual assessment of 30 random indiv"
2021.emnlp-demo.33,2020.findings-emnlp.440,1,0.849023,"Missing"
2021.emnlp-demo.33,Q19-1016,0,0.0587726,"Missing"
2021.emnlp-demo.33,2021.naacl-main.54,1,0.804173,"Missing"
2021.emnlp-demo.33,N18-1081,1,0.765441,"7 Fine-tuning CDLM on WEC-Eng is computationally infeasible, and therefore we use the model by Eirew et al. (2021). no NER label is assigned to a cluster, it is tagged as “Miscellaneous” (more details in Appendix A.2). Statements. Key statements benefit a user by presenting information about specific facts. To generate these statements, we group together coreferring propositions (rather than words) that describe the same fact within the source documents, as seen in §2.1. Following Ernst et al. (2020), our pipeline consists of three steps. (1) Proposition candidates are extracted with OpenIE (Stanovsky et al., 2018). (2) Pairs of propositions expressing the same statement are matched using the SuperPAL model (Ernst et al., 2020), considering proposition pairs whose alignment score is above 0.5 as matched. (3) A propositions graph is created by connecting pairs of nodes that represent similar propositions, and proposition clusters are matched for the connected components in the graph (more details in Appendix A.2). 3.2 Abstractive Facet Summarization In the standard summarization setting, a system receives a single or multiple documents as input, as well as a query in the query-focused task. In our case,"
2021.emnlp-main.407,2020.acl-main.194,0,0.0215537,"nt, they use ∼9K unlabeled examples in addition to 32 labeled examples per task. ADAPET uses just 32 labeled examples, and performs better than iP ET. Introduction Pre-trained language models (LMs) have shown significant gains across a wide variety of natural language processing (NLP) tasks in recent years (Devlin et al., 2019; Radford et al., 2018; Raffel et al., 2020). Most of these gains are obtained by fine-tuning language models on labeled data for a particular task. However, performance can suffer when there is very limited labeled data available for a downstream task (Xie et al., 2020; Chen et al., 2020). Recently, GPT-3 (Brown et al., 2020) demonstrated how language models, when scaled to hundreds of billions of parameters, can learn well when primed with only a few labeled examples. However, the scale of GPT-3 (175B parameters) makes it impractical to study. There is, therefore, a need to develop smaller language models that can work equally well with limited labeled data. Pattern-Exploiting Training (P ET; Schick and Schütze, 2021a,b) reformulates natural language understanding tasks as cloze-style questions and ∗ Equal contribution performs gradient-based fine-tuning. In doing so, P ET ou"
2021.emnlp-main.407,2021.naacl-main.185,0,0.471218,"odels on labeled data for a particular task. However, performance can suffer when there is very limited labeled data available for a downstream task (Xie et al., 2020; Chen et al., 2020). Recently, GPT-3 (Brown et al., 2020) demonstrated how language models, when scaled to hundreds of billions of parameters, can learn well when primed with only a few labeled examples. However, the scale of GPT-3 (175B parameters) makes it impractical to study. There is, therefore, a need to develop smaller language models that can work equally well with limited labeled data. Pattern-Exploiting Training (P ET; Schick and Schütze, 2021a,b) reformulates natural language understanding tasks as cloze-style questions and ∗ Equal contribution performs gradient-based fine-tuning. In doing so, P ET outperforms GPT-3 with few labeled examples using ALBERT (Lan et al., 2020). However, P ET uses additional task-specific unlabeled data. We propose ADAPET (A Densely-supervised Approach to Pattern Exploiting Training) that uses more supervision by decoupling the losses for the label tokens and a label-conditioned masked language modeling (MLM) objective over the full original input. On SuperGLUE (Wang et al., 2019) with 32 labeled examp"
2021.emnlp-main.460,2021.ccl-1.108,0,0.0672137,"Missing"
2021.emnlp-main.460,P11-1015,0,0.558576,"end, we propose a benchmark suite and evaluation protocol for continual few-shot learning (CFL) on text classification tasks. Our benchmark suite consists of both existing and newly created datasets. More precisely, we use the dataset with several linguistic categories annotated by Williams et al. (2020) from ANLI Round-3 (Nie et al., 2020); and also provide two new datasets with linguistic categories that we annotated using the counterfactual augmented data provided by Kaushik et al. (2020) on SNLI natural language inference dataset (Bowman et al., 2015) and IMDB sentiment analysis dataset (Maas et al., 2011). We discuss several methods as important promising baselines for CFL, borrowing from the literature of few-shot learning and continual learning. We classify these baselines into parameter correction methods (e.g., MAS (Aljundi et al., 2018)) and non-parametric feature matching methods (e.g., Prototypical networks (PN) (Snell et al., 2017)). We compare these methods on our benchmark suite in a traditional few-shot setup and observe that non-parametric feature matching methods perform surprisingly better than other methods. Next, we test the same methods in a continual few-shot setup and observ"
2021.emnlp-main.460,2020.acl-main.441,1,0.915651,"me original task, but target examples that can be considered new because they require solving a linguistic phenomenon, an error category, or a new domain. Unlike few-shot learning, we also require models that can maintain or improve performance on the existing data. To this end, we propose a benchmark suite and evaluation protocol for continual few-shot learning (CFL) on text classification tasks. Our benchmark suite consists of both existing and newly created datasets. More precisely, we use the dataset with several linguistic categories annotated by Williams et al. (2020) from ANLI Round-3 (Nie et al., 2020); and also provide two new datasets with linguistic categories that we annotated using the counterfactual augmented data provided by Kaushik et al. (2020) on SNLI natural language inference dataset (Bowman et al., 2015) and IMDB sentiment analysis dataset (Maas et al., 2011). We discuss several methods as important promising baselines for CFL, borrowing from the literature of few-shot learning and continual learning. We classify these baselines into parameter correction methods (e.g., MAS (Aljundi et al., 2018)) and non-parametric feature matching methods (e.g., Prototypical networks (PN) (Sne"
2021.emnlp-main.460,2021.eacl-main.20,0,0.0407213,"Optimization approaches aim to learn to optimize model parameters based on the gradients computed from limited labeled examples (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017). In the language domain, Yu et al. (2018) proposed to use a weighted combination of multiple metrics obtained from meta-training tasks for inferring on a newly-seen few-shot task. On the dataset side, Han et al. (2018) introduce a few-shot relation classification dataset. Recently, large-scale pretrained language models have been used for fewshot learning of downstream tasks (Brown et al., 2020; Schick and Schütze, 2021). Yin et al. (2020) used pre-trained entailment system for generalizing across more domains or new tasks when there are only a handful of labeled examples. All of the above-mentioned approaches focus on few-shot learning for new tasks. In contrast, we consider the same original task, but target examples that can be considered new because they require solving a linguistic phenomenon, an error category, or a new domain. Unlike few-shot learning, we also require models that can maintain or improve performance on the existing data. To this end, we propose a benchmark suite and evaluation protocol"
2021.emnlp-main.460,Q19-1040,0,0.0597546,"Missing"
2021.emnlp-main.460,N18-1101,0,0.0396846,"the feature representation space fθ .4 We use the final encoder hidden representations before softmax layer as fθ . As with Prototypical networks, support sets can be either the original training data, the few-shot training examples, or both. 5 Results In this section, we report the performances of various baselines discussed in Sec. 4 on our benchmark suite. We refer to Appendix for training details. 5.1 Results on Few-Shot Learning ANLI R3 Categories. Table 4 shows the results on the 6 categories from the Round-3 of the ANLI dataset. The base model, is trained on the combined data of MNLI (Williams et al., 2018), ANLI Round-1 (R1), and ANLI Round-2 (R2). On average, we observe that using the few-shot training examples for each of the categories improves the performance (comparing zero-shot vs. rest of the models), while maintaining the performance on MNLI matched (MNLI-m) and mis-matched (MNLI-mm) datasets. More importantly, we also observe that Supervised Contrastive Learning (SCL): Gunel et al. (2021) proposed supervised contrastive learning for better generalizability, where they jointly optimize the cross-entropy loss and supervised contrastive loss that captures the similarity between examples b"
2021.emnlp-main.460,N18-1109,0,0.0224652,"roaches and optimization-based approaches. Metric-based approaches learn generalizable metrics and corresponding matching functions from multiple training tasks with limited labels (Vinyals et al., 2016). For example, Snell et al. (2017) proposed to build representations for each class using supporting examples and then comparing the test instances by Euclidean distances. Optimization approaches aim to learn to optimize model parameters based on the gradients computed from limited labeled examples (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017). In the language domain, Yu et al. (2018) proposed to use a weighted combination of multiple metrics obtained from meta-training tasks for inferring on a newly-seen few-shot task. On the dataset side, Han et al. (2018) introduce a few-shot relation classification dataset. Recently, large-scale pretrained language models have been used for fewshot learning of downstream tasks (Brown et al., 2020; Schick and Schütze, 2021). Yin et al. (2020) used pre-trained entailment system for generalizing across more domains or new tasks when there are only a handful of labeled examples. All of the above-mentioned approaches focus on few-shot learn"
2021.emnlp-main.460,2002.tmi-tutorials.2,0,0.0653624,"actical, real-world applications of NLP require coreference, etc. Our solutions to complex such mistakes to be corrected on the fly as the problems are still far from perfect, so it is imsystem operates. For example, when a translation portant to create systems that can learn to corsystem makes a harmful mistake (e.g., translates rect mistakes quickly, incrementally, and with “EMNLP” to “ICML”), a phrase-based system can little training data. In this work, we propose be corrected by finding and modifying the respona continual few-shot learning (CFL) task, in sible entries in the phrase table (Zens et al., 2002), which a system is challenged with a difficult whereas there is no equivalent way to correct that phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training exin an end-to-end neural MT system. Similarly, sysamples. To this end, we first create benchtems have been shown to exhibit bias (e.g., gender marks based on previously annotated data: two or racial stereotypes) toward certain inputs of text, NLI (ANLI and SNLI) and one sentiment analwhich we want to correct via few examples on the ysis (IMDB) datasets. Next, we present varfly. ious baselines from diverse parad"
2021.emnlp-main.505,2021.acl-long.258,0,0.0300976,"y models the procedure of recognizing a symbolic function and applying this function via two separate models. These two models make discrete predictions and are jointly trained with Hierarchical Reinforcement Learning. Guo et al. (2021) explored the semisupervised learning with pseudo-parallel dev/test data and showed the efficacy of iterative backtranslation. Our method differs from these two works as (1) it induces the compositional rules implicitly from a general, seq2seq Transformer architecture; (2) it doesn’t require peeking into the novel commands of dev/test data. A contemporary work (Conklin et al., 2021) proposed to construct meta-train and meta-test sets that consist of similar input sequence and used meta-learning to encourage the model to learn generalizable features. Many early works have explored the compositionality of neural networks, like RNNs, for systematic behavior (Wong and Wang, 2007; Brakel and Frank, 2009) in language learning and compositional counting ability (Wiles, 1998; Weiss et al., 2018). In a study of sensitivity to hierarchical structure (Linzen et al., 2016), the authors argued that sequential language modeling signal is insufficient 7 Conclusion for capturing syntax-"
2021.emnlp-main.505,P19-1381,0,0.0386798,"Missing"
2021.emnlp-main.505,2020.emnlp-main.731,0,0.0240989,"ists of natural language commands paired with action sequences and is consisted of multiple splits that test the generalization of different compositional elements. Keysers et al. (2020) proposed a method to maximize compound divergence while guaranteeing a small atom divergence between train and test sets and created three M CD splits for SCAN. They also constructed the CFQ semantic parsing dataset of natural language questions paired with SPARQL output using this method. It was later expanded to ∗ -CFQ (Tsarkov et al., 2021), a large suite of benchmarks based on the original CFQ task. COGS (Kim and Linzen, 2020) is a semantic parsing dataset with multiple systematic gaps that can only be addressed by compositional generalization. More related tasks (Loula et al., 2018; Liška et al., 2018; Bastings et al., 2018) are proposed on top of these original datasets to better evaluate the compositional generalization ability. 6.2 Compositional Generalization Methods gram synthesis with a predefined meta-grammar. Data augmentation (Andreas, 2020; Akyürek et al., 2021) is also a natural method in promoting the generalization by automatically creating extra data that could resemble the test-set distribution. Mos"
2021.emnlp-main.505,D19-1438,0,0.180196,"nts that neural networks are associative devices that cannot capture systematic compositionality (Fodor and Pylyshyn, 1988; Marcus, 1998; Fodor and Lepore, 2002; Marcus, 2003; Calvo and Symons, 2014). Supporting this view, it has been shown that general neural models, like RNNs and Transformers (Vaswani et al., 2017), generalize poorly to the development set’s unseen combination of components seen in training set (Lake and Baroni, 2018; Liu et al., 2020). However, recent works have equipped recurrent neural networks (RNNs) with separate primitive and functional embeddings of the input tokens (Li et al., 2019; Russin et al., 2020). On the SCAN dataset (Lake and Baroni, 2018) that requires parsing a command into actions, these models can effectively parse “jump thrice"" when only trained on how to “walk thrice”, “walk”, and “jump”. In this work, we first show that this dual embedding method from CGPS-RNN (Li et al., 2019) can be transferred to the Transformer architecture to achieve nearly perfect results in substituting new primitives. Our CGPS-Transformer encoder maintains a functional/syntactic embedding and a primitive/semantic embedding for every word in the vo1 Introduction cabulary. This sepa"
2021.emnlp-main.505,Q16-1037,0,0.0317488,"er architecture; (2) it doesn’t require peeking into the novel commands of dev/test data. A contemporary work (Conklin et al., 2021) proposed to construct meta-train and meta-test sets that consist of similar input sequence and used meta-learning to encourage the model to learn generalizable features. Many early works have explored the compositionality of neural networks, like RNNs, for systematic behavior (Wong and Wang, 2007; Brakel and Frank, 2009) in language learning and compositional counting ability (Wiles, 1998; Weiss et al., 2018). In a study of sensitivity to hierarchical structure (Linzen et al., 2016), the authors argued that sequential language modeling signal is insufficient 7 Conclusion for capturing syntax-sensitive dependencies and called for more direct supervision. In this work, we propose two auxiliary sequence Recently, because of the publication of these prediction tasks to induce the compositional genpopular benchmarks, multiple works have come up eralization ability in a Transformer model. On the with promising methods that achieved better but challenging L ENGTH and M CD splits of the SCAN still limited compositional generalization. Dessì dataset, our method achieves the perfe"
2021.emnlp-main.505,W18-5413,0,0.0654933,"Missing"
2021.emnlp-main.518,N19-1423,0,0.131647,"scan S, the goal region G where the target object is located in, multiple turns of utterances between the oracle and navigator, and the navigator’s corresponding navigation trajectories after interacting with the oracle. Vision-Language Pre-Training. There have been 3.2 Navigation from Dialogue History (NDH) significant improvements in natural language proNDH Overview. Based on the CVDN dataset, cessing applications since large-scale pre-training Thomason et al. (2019) defines the task of Navlanguage models were introduced (Radford et al., igation from Dialogue History (NDH). In the NDH 2018; Devlin et al., 2019). The trend has spread to vision-language applications (Sun et al., 2019; task, the navigation path is the sub-path of the full Lu et al., 2019; Tan and Bansal, 2019; Chen et al., navigation path in the CVDN dataset. As shown in Figure 2, the start point for this NDH instance is p1 . 2020; Li et al., 2020). Recently, the pre-training The dialogue before this start point is recorded as approach has shown promising results in visionthe dialogue history. The red path is what a human and-language navigation tasks as well (Majumdar et al., 2020; Hao et al., 2020; Hong et al., 2021). navigator trave"
2021.emnlp-main.518,P19-1181,0,0.0180389,"go past the stairs and straight into a hallway. N: To the right or outside? O: Go to the right, which leads to a bedroom, and that should be the goal room. (b) NDH-F ULL task setup (a) NDH task setup Figure 2: Comparison between the NDH task setup and the NDH-F ULL task setup. Each sub-path corresponds to the sub-dialogue with same color. Dotted orange line in NDH task setup indicates shortest path between p1 and goal region G. N indicates Navigator and O indicates Oracle in the dialogue. et al., 2019; Thomason et al., 2019; Nguyen et al., 2019; Nguyen and Daumé III, 2019; Chen et al., 2019; Jain et al., 2019; Shridhar et al., 2020; Qi et al., 2020; Hermann et al., 2020; Berg et al., 2020; Zhu et al., 2020a; Ku et al., 2020; Anderson et al., 2020). Especially, Jain et al. (2019) introduces a new dataset, called Room-for-Room by combining short paths from Room-to-Room (Anderson et al., 2018) for evaluating instruction fidelity. Vision-and-Dialogue Navigation extends the one-way instruction-following navigation to the two-way multi-round dialogue setup in which agents could ask oracle guidance when they are lost. However, the current NDH task setup, which is built from the CVDN dataset (Thomason et"
2021.emnlp-main.518,2020.emnlp-main.356,0,0.220682,"exploring the potential of its application in several tasks. Vision-and-Language Navigation (VLN) is one of the tasks in which agents have to navigate to a goal location in the indoor or outdoor environment by following natural language instructions (MacMahon et al., 2006; Tellex et al., 2011; Mei et al., 2016; Hermann et al., 2017; Brahmbhatt and Hays, 2017; Mirowski et al., 2018; Anderson et al., 2018; Misra et al., 2018; Blukis et al., 2019; Thomason et al., 2019; Nguyen and Daumé III, 2019; Chen et al., 2019; Shridhar et al., 2020; Qi et al., 2020; Hermann et al., 2020; Berg et al., 2020; Ku et al., 2020). While most VLN datasets only provide instructions from the oracle without considering the navigator’s response, the useful Cooperative Vision-and-Dialogue Navigation (CVDN) (Thomason et al., 2019) dataset extends this one-way com1 Introduction munication to two-way multi-turn dialogue (EnWith the increased number of intelligent agents glish) interaction between the oracle and the navbeing deployed in our daily lives, effective commu- igator. The dataset simulates a situation in which nication between humans and agents is becoming agents navigate through indoor environments tomore important."
2021.emnlp-main.518,D18-1287,0,0.044418,"Missing"
2021.emnlp-main.518,D19-1063,0,0.0404882,"Missing"
2021.emnlp-main.518,N19-1268,1,0.783971,"g. We divide one data instance into multiple instances so that each result- Data Augmentation. The data size of NDH-F ULL ing data point has a different number of dialogue shrinks after combining all sub-paths and dialogue rounds and a corresponding sub-path (i.e., 2, 3, and rounds (7415 vs. 1653, see Table 1). To com4 or more than 4 dialogue rounds) and train the pensate for the loss, we try data augmentation by model on the subset of the data and move on to the generating the oracle’s instruction with the speaker longer dialogue/path ones (starting from the 2 dia- model (Fried et al., 2018; Tan et al., 2019). We 6439 modify their speaker model to take the context (i.e., dialogue history) as well as view trajectory to fit to the CVDN dataset. We replace the oracle’s instruction in a round of dialogue with the newly generated ones to give the model more diverse forms of instructions. But, we do not see an improvement from training the model on this augmented data possibly because NDH-F ULL requires accurate instructions to navigate quite long paths and the quality of the current speaker model could not meet the criteria. This allows future work on more effective generation methods. 6.4 Trajectory C"
2021.emnlp-main.518,2020.acl-main.229,0,0.265142,"ch leads to a bedroom, and that should be the goal room. (b) NDH-F ULL task setup (a) NDH task setup Figure 2: Comparison between the NDH task setup and the NDH-F ULL task setup. Each sub-path corresponds to the sub-dialogue with same color. Dotted orange line in NDH task setup indicates shortest path between p1 and goal region G. N indicates Navigator and O indicates Oracle in the dialogue. et al., 2019; Thomason et al., 2019; Nguyen et al., 2019; Nguyen and Daumé III, 2019; Chen et al., 2019; Jain et al., 2019; Shridhar et al., 2020; Qi et al., 2020; Hermann et al., 2020; Berg et al., 2020; Zhu et al., 2020a; Ku et al., 2020; Anderson et al., 2020). Especially, Jain et al. (2019) introduces a new dataset, called Room-for-Room by combining short paths from Room-to-Room (Anderson et al., 2018) for evaluating instruction fidelity. Vision-and-Dialogue Navigation extends the one-way instruction-following navigation to the two-way multi-round dialogue setup in which agents could ask oracle guidance when they are lost. However, the current NDH task setup, which is built from the CVDN dataset (Thomason et al., 2019), does not provide enough supervision for agents’ learning and does not evaluate agents’"
2021.emnlp-main.518,D19-1514,1,0.928384,"igation trajectories after interacting with the oracle. Vision-Language Pre-Training. There have been 3.2 Navigation from Dialogue History (NDH) significant improvements in natural language proNDH Overview. Based on the CVDN dataset, cessing applications since large-scale pre-training Thomason et al. (2019) defines the task of Navlanguage models were introduced (Radford et al., igation from Dialogue History (NDH). In the NDH 2018; Devlin et al., 2019). The trend has spread to vision-language applications (Sun et al., 2019; task, the navigation path is the sub-path of the full Lu et al., 2019; Tan and Bansal, 2019; Chen et al., navigation path in the CVDN dataset. As shown in Figure 2, the start point for this NDH instance is p1 . 2020; Li et al., 2020). Recently, the pre-training The dialogue before this start point is recorded as approach has shown promising results in visionthe dialogue history. The red path is what a human and-language navigation tasks as well (Majumdar et al., 2020; Hao et al., 2020; Hong et al., 2021). navigator traverses based on target information, dialogue history, the current round of the dialogue, Following this trend, we also apply pre-training and navigation history from p"
2021.emnlp-main.531,N18-2108,0,0.0606873,"Missing"
2021.emnlp-main.531,2020.acl-main.703,0,0.0107301,"ndix A.2. 5 PyrEval is the latest automatic Pyramid metric and is shown to be better than Yang et al. (2016). 6 https://github.com/serenayj/PyrEval/ issues/11 7 We find that the exhaustive set based computation (replacing fNLI in Equation 2 by gold labels) has close to perfect correlation with TAC’s official scores. REALSumm also use this computation as reflected by the gold score in Figure 1. 8 Fast Abs RL (Chen and Bansal, 2018), PtGen (See et al., 2017), ConvS2S and T-ConvS2S (Narayan et al., 2018), TransAbs and BertAbs and BertExtAbs (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART (Lewis et al., 2020), PEGASUS (Zhang et al., 2020) We use Pearson r or Spearman ρ as the correlation measure K. Pearson measures linear correlation while Spearman measures ranking correlation. 6622 Models. We use the pretrained RoBERTa-large (Liu et al., 2019) based NLI model released by Nie et al. (2020), which has been trained on multiple NLI datasets. We continually finetune this model with the gold SCUs plus SCU-presence labels always for 2 epochs. For SRL, Coreference Resolution, and Constituency Tree Parser, we use the out-of-the-box tools provided by AllenNLP (Gardner et al., 2018; Shi and Lin, 2019; Lee e"
2021.emnlp-main.531,N06-1059,0,0.0821309,"hey automatically decompose based. Some metrics measure the n-gram overlap both system summary and reference(s) into seman(Papineni et al., 2002; Lin, 2004), out of which tic units and then ask humans to match/align the ROUGE (Lin, 2004) is the most widely adopted two lists of units. In contrast, our semi-automatic metric till today. Some other works compute the Lite2 Pyramid retains the reusable SCUs while autosimilarity over n-gram graphs (Giannakopoulos matically judges the SCUs’ presence in the system and Karkaletsis, 2011; Giannakopoulos et al., 2008) summary (via NLI). or distributions (Lin et al., 2006). Since exact n-gram matching is too rigid, METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) 3 Our Method provides flexibility by stemming, synonyms, etc., and recently, a few metrics enable “soft” matching 3.1 Lite2 Pyramid through contextualized word embeddings (Zhao et al., 2019; Clark et al., 2019; Zhang et al., 2019). Lite2 Pyramid is a semi-automatic metric that However, Deutsch and Roth (2021) point out that retains human-labeled Summary Content Units the n-gram based metrics indicate more topic sim- (SCUs) to represent reference summaries of a data i ilarity than informatio"
2021.emnlp-main.531,D19-1387,0,0.0141506,"nt”. See more data collection details of PyrXum in Appendix A.2. 5 PyrEval is the latest automatic Pyramid metric and is shown to be better than Yang et al. (2016). 6 https://github.com/serenayj/PyrEval/ issues/11 7 We find that the exhaustive set based computation (replacing fNLI in Equation 2 by gold labels) has close to perfect correlation with TAC’s official scores. REALSumm also use this computation as reflected by the gold score in Figure 1. 8 Fast Abs RL (Chen and Bansal, 2018), PtGen (See et al., 2017), ConvS2S and T-ConvS2S (Narayan et al., 2018), TransAbs and BertAbs and BertExtAbs (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART (Lewis et al., 2020), PEGASUS (Zhang et al., 2020) We use Pearson r or Spearman ρ as the correlation measure K. Pearson measures linear correlation while Spearman measures ranking correlation. 6622 Models. We use the pretrained RoBERTa-large (Liu et al., 2019) based NLI model released by Nie et al. (2020), which has been trained on multiple NLI datasets. We continually finetune this model with the gold SCUs plus SCU-presence labels always for 2 epochs. For SRL, Coreference Resolution, and Constituency Tree Parser, we use the out-of-the-box tools provided by All"
2021.emnlp-main.531,2021.ccl-1.108,0,0.037685,"Missing"
2021.emnlp-main.531,D18-1206,0,0.337784,"sor to predict the “simulation easiness” of each reference sentence: if a sentence is too complex to be well represented by STUs, we will ask humans to annotate SCUs for it; otherwise, we can apply automatic SRL. We call this method as Lite2.x Pyramid, since it provides a smooth, flexible transition from Lite2 Pyramid to Lite3 Pyramid and balances reliability with cost. To comprehensively evaluate the quality of metrics, we not only use 3 existing meta-evaluation datasets (TAC2008 (DBL, 2008), TAC2009 (DBL, 2009), REALSumm (Bhandari et al., 2020)) but also newly collect PyrXSum with 100 XSum (Narayan et al., 2018) test examples plus summaries produced by 10 systems. Next, we compare our new metrics to 15 existing automatic metrics on these 4 meta-evaluation setups for both system-level and summary-level correlations with human Pyramid scores. We find that Lite2 Pyramid consistently has the best summary-level correlations and is reliable as an out-of-the-box metric. Lite3 Pyramid also mostly performs better or competitively. Lastly, the regressor-based Lite2.x Pyramid can help substantially reduce annotation efforts for only small correlation drops, e.g., on TAC2008, TAC2009, it trades off only 0.01 abs"
2021.emnlp-main.531,N04-1019,0,0.532257,"n, 2004; Tratz and Hovy, 2008; Giannakopoulos and Karkaletsis, 2011; Yang et al., 2016; Zhang et al., 2019; Deutsch et al., 2021). However, most of them cannot reliably substitute human evaluation due to the unstable performance across datasets (Bhandari et al., 2020), weak to moderate correlations with human judgment (Fabbri et al., 2021), or more indication of topic similarity than information overlap (Deutsch and Roth, 2021). In this work, we want to combine human and automatic evaluations and find a balance between reliability and reproducibility (plus expense). Recall the Pyramid method (Nenkova and Passonneau, 2004), where these SCUs for reference summaries only need to be annotated once, then they can be 1 Introduction fixed. It means SCUs can come with the datasets Evaluating the quality of summaries is a challeng- and are reusable for evaluating different systems. ing task. Human evaluation is usually regarded as Hence, what hinders this method from being rethe gold standard. Out of different human evalua- producible is its second step of asking humans to tion methods, Pyramid (Nenkova and Passonneau, judge the presence of SCUs in system summaries. 1 Whenever we have a new summarizer, we need Our code"
2021.emnlp-main.531,2020.acl-main.455,0,0.0242324,"Units) and compare the two list of units. Differently, our Lite3 Pyramid only decomposes the reference summaries to semantic triplet units (STUs), and we use NLI to judge the presence of each STU in the system summary, which is closer to the original Pyramid’s procedure and leads to better correlations with human scores (refer to Section 5). Peyrard et al. (2017) propose a learned metric, S3, that is trained to directly predict human Pyramid or Responsiveness scores based on ROUGE, FrameNet features, etc., which is similar to how we finetune the NLI model with human labels of SCUs’ presence. Xu et al. (2020) is distantly related to us in the way of representing texts by SRL, but it is used to weigh the content in the source document(s). Besides, some reference-free metrics are introduced for summary quality estimation (Xenouleas et al., 2019; Gao et al., 2020; Vasilyev et al., 2020) or faithfulness evaluation (Durmus et al., 2020; Wang et al., 2020). Automatic metrics trade off the reliability of human evaluation for reproducibility, low cost, and fast speed. Many automatic metrics have been Semi-automatic evaluation is introduced by introduced, the majority of which are reference- Zhou et al. (2"
2021.emnlp-main.543,D17-1168,0,0.0260178,"ns. Our work is based on exploring structured inputs and outputs for conditional image synthesis which has been largely unexplored in text-to-image synthesis and story visualization. ato and Koyama, 2018). Zhang et al. (2021) combine inter-modality and intra-modality contrastive losses and observe complementary improvements. We adapt inter-modal loss for story visualization by sampling negatives from adjacent frames. Story Understanding & Commonsense. Iyyer et al. (2016) introduced Relationship Modelling Networks to extract evolving relationship trajectories between two characters in a novel. Chaturvedi et al. (2017) use latent variables to weigh predefined semantic aspects like topical consistency to improve encoding for story completion. Guan et al. (2019); Chen et al. (2019) augment story encodings with structured commonsense knowledge to improve story ending generation. We focus on the use of structured commonsense as well as grammatical trees to improve story encoding for the end goal of visualization. 3.1 Tree Encoder. Tree structures have traditionally been encoded using Tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016; Yang et al., 2017b,a). In recent work, Wang et al. (2019) enforce a hierarch"
2021.emnlp-main.543,2020.acl-main.435,1,0.841177,"force a hierarchical prior in the self-attention layer of Transformer (Vaswani et al., 2017) and Harer et al. (2019) use a parent-sibling tree convolution block to perform structure-aware encoding. Nguyen et al. (2020) use sub-tree masking and hierarchical accumulation to improve machine translation. We propose a simpler Tree-Transformer architecture, augmented with memory units (Lei et al., 2020) for recurrence. Dense Captioning. Dense captioning jointly localizes semantic regions and describes these regions with short phrases in natural language (Johnson et al., 2016). Wu et al. (2019) and (Kim et al., 2020) use dense captions for visual and video question answering respectively. We use a pretrained dense captioning model to first annotate our target dataset and then use it within a dual learning framework to improve image synthesis for story visualization. 3 Methods Background Given a sequence of sentences S = [s1 , s2 , ..., sT ], story visualization is the task of generating a correˆ = [ˆ sponding sequence of images X x1 , x ˆ2 , ..., x ˆT ]. The sentences form a coherent story with recurring plot and characters. The generative model for this task has two main modules: story encoder and image"
2021.emnlp-main.543,P18-1249,0,0.0219284,"Missing"
2021.emnlp-main.543,N19-1238,0,0.0599742,"Missing"
2021.emnlp-main.543,2020.acl-main.233,1,0.935973,"the end goal of visualization. 3.1 Tree Encoder. Tree structures have traditionally been encoded using Tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016; Yang et al., 2017b,a). In recent work, Wang et al. (2019) enforce a hierarchical prior in the self-attention layer of Transformer (Vaswani et al., 2017) and Harer et al. (2019) use a parent-sibling tree convolution block to perform structure-aware encoding. Nguyen et al. (2020) use sub-tree masking and hierarchical accumulation to improve machine translation. We propose a simpler Tree-Transformer architecture, augmented with memory units (Lei et al., 2020) for recurrence. Dense Captioning. Dense captioning jointly localizes semantic regions and describes these regions with short phrases in natural language (Johnson et al., 2016). Wu et al. (2019) and (Kim et al., 2020) use dense captions for visual and video question answering respectively. We use a pretrained dense captioning model to first annotate our target dataset and then use it within a dual learning framework to improve image synthesis for story visualization. 3 Methods Background Given a sequence of sentences S = [s1 , s2 , ..., sT ], story visualization is the task of generating a cor"
2021.emnlp-main.543,2021.naacl-main.194,1,0.397452,"ry contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information.1 Petty asks whether it is because of cookies. Eddy denies with his hands. Petty hands her cookies to Eddy. Petty gives her cookies to Loopy and Crong. Crong sighs. VLC Abstract Captions Adyasha Maharana Mohit Bansal Department of Computer Science University of North Carolina at Chapel Hill {adyasha, mbansal}@cs.unc.edu Figure 1: Example of Generated Images from our model VLC-S TORY GAN and Duco-StoryGAN (Maharana et al., 2021) for the PororoSV dataset. images. The goal of the task is to reproduce the images given the captions. It is more challenging than conventional text-to-image generation (Reed et al., 2016) because the generative model needs to identify the narrative structure expressed in the sequence of captions and translate it into a story of images. Some critical features of a good story include consistent character and background appearances, relevance to individual captions as well as overall story, and coherent narrative. While recent text-to-image models (Ramesh et al., 2021; Cho et al., 2020; Li et al"
2021.emnlp-main.543,P16-1105,1,0.885285,"Missing"
2021.emnlp-main.543,D14-1162,0,0.084212,"Missing"
2021.emnlp-main.543,P15-1150,0,0.0331643,"ng Networks to extract evolving relationship trajectories between two characters in a novel. Chaturvedi et al. (2017) use latent variables to weigh predefined semantic aspects like topical consistency to improve encoding for story completion. Guan et al. (2019); Chen et al. (2019) augment story encodings with structured commonsense knowledge to improve story ending generation. We focus on the use of structured commonsense as well as grammatical trees to improve story encoding for the end goal of visualization. 3.1 Tree Encoder. Tree structures have traditionally been encoded using Tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016; Yang et al., 2017b,a). In recent work, Wang et al. (2019) enforce a hierarchical prior in the self-attention layer of Transformer (Vaswani et al., 2017) and Harer et al. (2019) use a parent-sibling tree convolution block to perform structure-aware encoding. Nguyen et al. (2020) use sub-tree masking and hierarchical accumulation to improve machine translation. We propose a simpler Tree-Transformer architecture, augmented with memory units (Lei et al., 2020) for recurrence. Dense Captioning. Dense captioning jointly localizes semantic regions and describes these regions"
2021.emnlp-main.543,D19-1098,0,0.10106,"StoryGan. for additional position and semantic information, 6772 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6772–6786 c November 7–11, 2021. 2021 Association for Computational Linguistics and (3) is trained using intra-story contrastive loss for maximizing global semantic alignment between input captions and generated visual stories. Grammatical structures like constituency parse trees are potentially rich sources of information for visualizing relations between objects (or characters), their actions, and their attribute (property) modifiers. Wang et al. (2019); Nguyen et al. (2020); Xiao et al. (2017); Cirik et al. (2018) demonstrate that inducing such tree-structures within the encoder guides words to compose the meaning of longer phrases hierarchically and improves various tasks like masked language modeling, translation, visual grounding of language etc., suggesting potential gains in other tasks. Most text-to-image synthesis as well as story visualization models (Li et al., 2019c; Maharana et al., 2021) perform flat processing over free-text captions using LSTM or Transformer-based encoders. Hence, in order to leverage the grammatical informati"
2021.emnlp-main.543,P19-1348,0,0.0932142,"al outputs, dealing either with im- Story Visualization. The task of story visualizaage or text. In a bid to combine the benefits of tion and the model StoryGAN was introduced by Li learning signals from both visuo-spatial and lan- et al. (2019c). Zeng et al. (2019) and Li et al. (2020) guage modalities, we propose the use of dense cap- used textual alignment modules and Weighted Actioning as the dual task, which has proven useful as tivation Degrees respectively, to improve perfora source of complementary information for many mance of StoryGAN. Song et al. (2020) add a vision-language tasks (Wu et al., 2019; Kim et al., figure-ground generator and discriminator to pre2020; Li et al., 2019b). Dense captioning models serve the shape of characters. Maharana et al. provide regional bounding boxes for objects in the (2021) demonstrate the effectiveness of video cap6773 tioning as a dual task for story visualization and propose additional evaluation metrics. Notable recent models in the related field of text-to-image generation are large (Brock et al., 2018), trained on gigantic datasets (Ramesh et al., 2021) and are based on Transformer architectures Jiang et al. (2021). Mask-to-image generation modu"
2021.emnlp-main.609,N18-2017,0,0.0266535,"of-the-art commonsense reasoning (CSR) (Davis and Marcus, 2015) models are typically trained and evaluated on discriminative tasks, in which a model answers a multiple-choice question for a certain context (Zellers et al., 2018; Sap et al., 2019b; Bisk et al., 2020). While pretrained language models perform well on these tasks (Lourie et al., 2021), this setup limits the exploration and evaluation of a model’s ability to reason and explain its predictions with relevant commonsense knowledge, thereby allowing models to solve tasks by using shortcuts, statistical biases or annotation artifacts (Gururangan et al., 2018; McCoy et al., 2019). Thus, we emphasize the importance of generative CSR capability, in which a model has to compose and reveal the plausible commonsense knowledge required to solve a reasoning task. Moreover, structured (e.g., graph-based) commonsense explanations, unlike unstructured natural language explanations, can more explicitly explain and evaluate the reasoning structures of the model by visually laying out the relevant context and commonsense knowledge edges, chains, and subgraphs. We propose E XPLAG RAPHS, a new generative and structured CSR task (in English) of explana7716 1 E XP"
2021.emnlp-main.609,P16-1150,0,0.0287544,"erative commonsense tasks. E.g., CommonGen (Lin et al., 2020) generates unstructured commonsense text, and EIGEN (Madaan et al., 2020) considers event influence graph generation. Instead, our work focuses on generating commonsense-augmented explanation graphs. Stance Prediction and Argumentation: Previous stance prediction works have been largely applied to online content, for political, ideological debates, rumor and fake news detection (Mohammad et al., 2016; Derczynski et al., 2017; Hardalov et al., 2021). Other recent works on argumentation deal with convincingness of claims and arguments Habernal and Gurevych (2016); Gleize et al. (2019) and reasons (Hasan and Ng, 2014). However, to the best of our knowledge, our work is the first to explore explicit commonsense-augmented graphbased explanations for stance prediction. Structured Explanations in NLP: Explanation datasets in NLP (Wiegreffe and Marasovi´c, 2021) take three major forms: (1) extractive rationales (Zaidan et al., 2007; Lei et al., 2016; Yu et al., 2019; DeYoung et al., 2020), (2) free-form or natural language explanations (Camburu et al., 2018; Rajani et al., 2019; Narang et al., 2020; Brahman et al., 3 E XPLAG RAPHS Task Definition 2021; Zhan"
2021.emnlp-main.609,2020.acl-main.508,0,0.138953,". 6, Fig. 4), consisting of diverse automatic metrics and human evaluature complex dependencies between facts, while tion. The evaluation framework checks for stance also avoiding redundancy (e.g., “Factory farming causes food and millions desire food” forms a “V- and graph consistency along with the structural structure”), (2) unlike natural language explana- and semantic correctness of explanation graphs, tions (Camburu et al., 2018; Rajani et al., 2019; both locally by evaluating the importance of each edge and globally by the graph’s ability to reNarang et al., 2020; Brahman et al., 2021; Zhang et al., 2020), it is easier to impose task-specific con- veal the target stance. Furthermore, we propose straints on graphs (e.g., connectivity, acyclicity), graph-matching metrics like Graph Edit Distance (Abu-Aisheh et al., 2015) and ones that extend that eventually help in better quality control during data collection (Sec. 4) and designing structural va- text-generation metrics for graphs (based on mullidity metrics for model-evaluation (Sec. 6), and (3) tiple test graphs in our dataset). Lastly, as some unlike semi-structured templates (Ye et al., 2020; strong initial baseline models for this new task"
2021.emnlp-main.808,I11-1038,0,0.02931,"2020), natural language generation (Camburu et al., 2018; Rajani et al., 2019; Hase et al., 2020; Wiegreffe et al., 2020), and recently, kNN-based methods (Papernot and McDaniel, 2018; Rajani et al., 2020). Influence Functions. The use of influence-based diagnostics can be traced back to the seminal papers such as Cook (1977); Cook and Weisberg (1980, 1982); Cook (1986). Recently, Koh and Liang (2017) brought influence functions to largescale deep learning and have been followed up by numerous publications. For example, Koh et al. (2018) used them for data poisoning attacks, Schulam and Saria (2019) for calibrating trust in individual model predictions, Brunet et al. (2019) for tracing biases in word embeddings, Koh et al. (2019) and Basu et al. (2020) for identifying important groups of training data, and Feldman and Zhang (2020) for studying neural networks memorization. In the context of influence functions for NLP, Han et al. (2020) used them to explain model predictions and unveil data artifacts. Yang et al. (2020) used them to estimate the quality of synthetic training samples in the context of dataaugmentation. Meng et al. (2020) explored the combination of gradient-based methods"
2021.findings-acl.384,W13-2322,0,0.0145457,"e (Dong and Lapata, 2016; Yin and Neubig, 2017; Rabinovich et al., 2017; Yin and Neubig, 2018, 2019; Shin et al., 2019; Xu et al., 2020; Sun et al., 2020). While the use of ASTs for code generation has been substantially studied, to the best of our knowledge, the use of input parse tree for code generation is largely unexplored. Semantic Parsing. Several methods have been proposed to parse natural language sentences to formal meaning representations like lambda calculus (Wong and Mooney, 2007), Alexa Meaning Representation Language (Kumar et al., 2017), Abstract Meaning Representations (AMR) (Banarescu et al., 2013), structured queries (Iyer et al., 2017; Yin and Neubig, 2018), etc. Many of the recent works for semantic parsing have focused on sequence-totree models leveraging tree structures like AST as the intermediate representation for target meaning representation (Yin and Neubig, 2018; Sun et al., 2020). Code generation can also be regarded as a form of semantic parsing where the target meaning representation is programming language snippet. Source Trees and Structure-Aware Models. Several structure-aware tree-encoders have also been proposed to process the source trees (Chen et al., 2017a,b; Yang"
2021.findings-acl.384,P17-1177,0,0.0216827,"R) (Banarescu et al., 2013), structured queries (Iyer et al., 2017; Yin and Neubig, 2018), etc. Many of the recent works for semantic parsing have focused on sequence-totree models leveraging tree structures like AST as the intermediate representation for target meaning representation (Yin and Neubig, 2018; Sun et al., 2020). Code generation can also be regarded as a form of semantic parsing where the target meaning representation is programming language snippet. Source Trees and Structure-Aware Models. Several structure-aware tree-encoders have also been proposed to process the source trees (Chen et al., 2017a,b; Yang et al., 2017; Nguyen et al., 2020). While many of the tree-encoders are dependent on recurrent mechanism and hence are unparallelizable, Nguyen et al. (2020) propose a Transformer-based structure-aware model that is parallelizable. Concurrently, several tree-to-seq models have been proposed that leverage source syntactic trees for NLP tasks like machine translation (Eriguchi et al., 2016; Yang et al., 2017; Eriguchi et al., 2017; Chen et al., 2017b) and sentence modeling (Shi et al., 2018). There has been some work on leveraging hybrid tree - a joint treelike representation of the NL"
2021.findings-acl.384,D17-1304,0,0.0299746,"Missing"
2021.findings-acl.384,N19-1423,0,0.0271654,"he standard Transformer is not designed to preserve the tree structure of the input parse trees. Hence, to better encode the trees, we modify a structure-aware Tree Transformer model (Nguyen et al., 2020) for the tree-to-tree code generation task. We focus on constituency-based parse trees in this paper because of space constraints as this is a short paper. Moreover, as pointed out by Nguyen et al. (2020), there is little evidence of constituency structures being learned implicitly in language models, whereas dependency structures have been shown to be implicitly embedded in models like BERT (Devlin et al., 2019; Hewitt and Manning, 2019). We evaluate our models on the CoNaLa dataset (Yin et al., 2018) and find that incorporating constituency parse trees in input using structure-aware encoders improves the quality of generated code. We further evaluate our models on the ATIS dataset (Hemphill et al., 1990), which translates natural language sentences into their lambda calculus logical forms and show that a structure-aware Transformer significantly improves performance over a standard Transformer. We also focus on analyzing the input parse trees to find the aspects that benefit code generation. Our an"
2021.findings-acl.384,P16-1004,0,0.0331727,"ies in input sentence, and the complexity of input trees. We find that the structure-aware model improves performance when such identifiers and variables are present towards the end of the input sentences and when the input sentences are short in length. 2 Related Work Code Generation. Code generation for generalpurpose programming languages is a recent phenomenon, earlier works being focused on domainspecific languages (Gulwani and Marron, 2014; Raza et al., 2015). Recent works have mainly applied sequence-to-tree models for code generation, with the tree being the AST of target source code (Dong and Lapata, 2016; Yin and Neubig, 2017; Rabinovich et al., 2017; Yin and Neubig, 2018, 2019; Shin et al., 2019; Xu et al., 2020; Sun et al., 2020). While the use of ASTs for code generation has been substantially studied, to the best of our knowledge, the use of input parse tree for code generation is largely unexplored. Semantic Parsing. Several methods have been proposed to parse natural language sentences to formal meaning representations like lambda calculus (Wong and Mooney, 2007), Alexa Meaning Representation Language (Kumar et al., 2017), Abstract Meaning Representations (AMR) (Banarescu et al., 2013),"
2021.findings-acl.384,P16-1078,0,0.110192,"here the target meaning representation is programming language snippet. Source Trees and Structure-Aware Models. Several structure-aware tree-encoders have also been proposed to process the source trees (Chen et al., 2017a,b; Yang et al., 2017; Nguyen et al., 2020). While many of the tree-encoders are dependent on recurrent mechanism and hence are unparallelizable, Nguyen et al. (2020) propose a Transformer-based structure-aware model that is parallelizable. Concurrently, several tree-to-seq models have been proposed that leverage source syntactic trees for NLP tasks like machine translation (Eriguchi et al., 2016; Yang et al., 2017; Eriguchi et al., 2017; Chen et al., 2017b) and sentence modeling (Shi et al., 2018). There has been some work on leveraging hybrid tree - a joint treelike representation of the NL sentence and corresponding meaning representation - for semantic parsing (Lu et al., 2008; Jie and Lu, 2018), while Harer et al. (2019) made use of source tree structures for code correction. However, the same is unexplored in the context of code generation. We study the use of tree-to-tree models for code generation and provide analysis of its various modules. 3 3.1 Our Models Baseline (Sequence"
2021.findings-acl.384,P17-2012,0,0.0139582,"programming language snippet. Source Trees and Structure-Aware Models. Several structure-aware tree-encoders have also been proposed to process the source trees (Chen et al., 2017a,b; Yang et al., 2017; Nguyen et al., 2020). While many of the tree-encoders are dependent on recurrent mechanism and hence are unparallelizable, Nguyen et al. (2020) propose a Transformer-based structure-aware model that is parallelizable. Concurrently, several tree-to-seq models have been proposed that leverage source syntactic trees for NLP tasks like machine translation (Eriguchi et al., 2016; Yang et al., 2017; Eriguchi et al., 2017; Chen et al., 2017b) and sentence modeling (Shi et al., 2018). There has been some work on leveraging hybrid tree - a joint treelike representation of the NL sentence and corresponding meaning representation - for semantic parsing (Lu et al., 2008; Jie and Lu, 2018), while Harer et al. (2019) made use of source tree structures for code correction. However, the same is unexplored in the context of code generation. We study the use of tree-to-tree models for code generation and provide analysis of its various modules. 3 3.1 Our Models Baseline (Sequence-to-Tree Model) We use a standard Transfor"
2021.findings-emnlp.298,N19-1423,0,0.052343,"an study) to reveal that, non-monotonic orders. We follow the training interestingly, our policy in general follows an outer- procedure on the En↔De task from Mansimov to-inner order, predicting the left-most and right- et al. (2019), where they first initialize from most positions first and move toward the middle. In a Transformer-based cross-lingual language Table 5, we show that when decoding from left to model (Lample and Conneau, 2019) pre-trained on 3514 the large monolingual corpus in both English and German. They then train the Transformer as a conditional masked language model (MLM) (Devlin et al., 2019), where it is trained to predict some randomly masked tokens from the target sequence, given the complete source sequence and other non-masked tokens of the target sequence. The input is constructed by concatenating the German and English sequences separated by a special token. At each iteration, the sentence in one language is randomly selected as the source and the corresponding sentence in the other language is the target and is masked in random positions. Instead of masking the tokens with a fixed probability (e.g., 0.15 in BERT (Devlin et al., 2019)), they randomly vary the masking maskin"
2021.findings-emnlp.298,D19-1633,0,0.161852,"ted Sequence Generation Models. Most generation models nowadays are directed as they are trained and decode exclusively from left to right. The widely-used Transformer (Vaswani et al., 2017) decoder employs a causal self-attention layer to mask out the attention to the future tokens during the training. At the test time, the decoder takes the previously generated tokens as the input and predicts the next token from left to right. Undirected Sequence Generation Models. Other than training the auto-regressive Transformer model with causal self-attention in a monotonic direction, previous works (Ghazvininejad et al., 2019; Mansimov et al., 2019) have Next, we focus on a detailed and important qual- tried to enable a decoder to generate tokens in itative analysis (and human study) to reveal that, non-monotonic orders. We follow the training interestingly, our policy in general follows an outer- procedure on the En↔De task from Mansimov to-inner order, predicting the left-most and right- et al. (2019), where they first initialize from most positions first and move toward the middle. In a Transformer-based cross-lingual language Table 5, we show that when decoding from left to model (Lample and Conneau, 2019) pre"
2021.findings-emnlp.298,D18-1149,0,0.0372232,"Missing"
2021.findings-emnlp.298,2020.emnlp-main.73,0,0.0436295,"Missing"
2021.findings-emnlp.298,2020.acl-main.24,0,0.0142621,"extended on this thread and in- to generate text under specified lexical constraints. troduced Mask-Predict that keeps replacing the to- Similar to the monotonic left-to-right generation, kens with low likelihood. They showed that, after the insertion-based model also operates autoregres10 iterations, the quality of the generated trans- sively and the length of the output is dynamically lations is competitive with the conventional au- decided by predicting an end-of-sentence token. toregressive models on the WMT’14 En↔De and We instead opt for using a masked language model WMT’16 En↔Ro tasks. Liao et al. (2020) also in- and the output sequence length is fixed before the vestigated generation using masked language mod- decoding starts. We further learn the generation els and proposed a probabilistic masking scheme order using Reinforcement Learning instead of re(PMLM). Wang et al. (2018) proposed a semi- lying on pre-defined heuristics. autoregressive generation scheme by predicting a consecutive chunk of tokens in parallel and repeats 7 Conclusion until the entire sequence is predicted. Kreutzer et al. (2020) studied the Mask-Predict process in a In this work, we train a policy network with reinsimi"
2021.findings-emnlp.298,2021.findings-acl.11,0,0.0334848,"Missing"
2021.findings-emnlp.298,D19-1437,0,0.0159024,"eneration process. We with lower BLEU scores (27.68 for full De− →En 3520 dev set vs 27.92 from our policy). Therefore, we believe it is the combination of BLEU and entropy penalty that makes the model fully explore the action space and finally converge to the outer-to-inner pattern. 6 Related Works Undirected Generation with Iterative Refinement in Continuous Vector Space. Other than iteratively refining the output tokens from the previous pass, another line of work used continuous latent variables and the distribution of the target sentence can be factorized over time given these variables (Ma et al., 2019; Shu et al., 2020). Lee et al. (2020) further improve the speed and performance of the EM-like inference procedure by training an inference network using the latent variable only. Most recently, Gu and Kong (2020) improved the single-pass, fully non-autoregressive models by reducing the dependency in the output space. Undirected Generation with Iterative Refinement in Token Space. Wang and Cho (2019) also explored approaches for generating text from a masked language model (MLM), such as BERT (Devlin et al., 2019), by seeing it as a Markov random field language model and samples one token at"
2021.findings-emnlp.298,Q19-1042,0,0.0304448,"ion with Iterative Refinement in Token Space. Wang and Cho (2019) also explored approaches for generating text from a masked language model (MLM), such as BERT (Devlin et al., 2019), by seeing it as a Markov random field language model and samples one token at a time. Their setting differs from ours Insertion-based Generation with Arbitrary Orin that they used a pretrained MLM for uncondi- ders. Another generation scheme that is closely tional language modeling. Other than replacing one related to our work is the insertion-based generatoken at a time that result in a linear-time genera- tion (Gu et al., 2019; Stern et al., 2019). They also tion scheme, multiple previous works have tried to decode one token at a time within a linear-time replace more than one token at time in a constant- generation scheme, and the insertion order is either time, non-autoregressive generation scheme. Lee some human-designed pre-defined order (e.g., leftet al. (2018) proposed a model that replaces the to-right, balanced-tree, etc.) or a searched adaptive tokens at all positions and keeps refining the pre- order found via beam search. Recently, Zhang vious outputs for multiple iteration. Ghazvinine- et al. (2020) pre"
2021.findings-emnlp.298,P02-1040,0,0.109216,"nt target: i , hmaski, y i , X). φlogp = −logp(yj = yji |y&lt;j &gt;j We then select and replace the mask at the position with the largest φlogp . Easy-First. Other than φlogp , we further consider the negative entropy of the generation model’s output distribution at every position: φnegent = i , hmaski, y i , X) Intuitively, we −H(yji+1 |y&lt;j &gt;j want to replace the mask with a new word that the model is highly certain of (low entropy). We then select and replace the mask at the position with the largest αlogp · φlogp + αnegent · φnegent .3 4.3 Main Results We present our evaluation results by BLEU (Papineni et al., 2002) in Table 1. 4 On the De− →En test set, the Easy-First generation order achieves the highest BLEU scores among all orders, while the translations decoded by our learned orders receives higher BLEU scores compared to the ones decoded from left to right and the ones decoded by previous learned orders (Mansimov et al., 2019). On the En− →De test set, the translations generated from left to right outperforms the translation decoded by any other heuristic or learned orders. Here, our learned order again beats the previous learned order and is on par with the Left2Right baseline on the dev set. For"
2021.findings-emnlp.298,P07-2045,0,0.011197,"Missing"
2021.findings-emnlp.298,2020.emnlp-main.465,0,0.0389499,"Missing"
2021.findings-emnlp.298,P16-1162,0,0.0328872,"Missing"
2021.findings-emnlp.298,W19-2304,0,0.0117796,"refining the output tokens from the previous pass, another line of work used continuous latent variables and the distribution of the target sentence can be factorized over time given these variables (Ma et al., 2019; Shu et al., 2020). Lee et al. (2020) further improve the speed and performance of the EM-like inference procedure by training an inference network using the latent variable only. Most recently, Gu and Kong (2020) improved the single-pass, fully non-autoregressive models by reducing the dependency in the output space. Undirected Generation with Iterative Refinement in Token Space. Wang and Cho (2019) also explored approaches for generating text from a masked language model (MLM), such as BERT (Devlin et al., 2019), by seeing it as a Markov random field language model and samples one token at a time. Their setting differs from ours Insertion-based Generation with Arbitrary Orin that they used a pretrained MLM for uncondi- ders. Another generation scheme that is closely tional language modeling. Other than replacing one related to our work is the insertion-based generatoken at a time that result in a linear-time genera- tion (Gu et al., 2019; Stern et al., 2019). They also tion scheme, mult"
2021.findings-emnlp.298,D18-1044,0,0.0369104,"Missing"
2021.findings-emnlp.298,2020.emnlp-main.698,0,0.029321,"Missing"
2021.naacl-demos.6,2020.acl-main.442,0,0.128913,"Missing"
2021.naacl-demos.6,D19-1228,0,0.0215899,"Missing"
2021.naacl-demos.6,N18-1101,0,0.0309914,"Missing"
2021.naacl-demos.6,2020.emnlp-main.661,1,0.821072,"Missing"
2021.naacl-demos.6,N18-1179,0,0.0651353,"Missing"
2021.naacl-demos.6,P19-1073,0,0.200915,"Missing"
2021.naacl-demos.6,2020.acl-main.543,0,0.0245032,"Missing"
2021.naacl-main.193,J82-2003,0,0.34572,"Missing"
2021.naacl-main.193,W14-3348,0,0.0179285,"Missing"
2021.naacl-main.193,N19-1423,0,0.522888,"for a particular task (e.g., text-to-video retrieval). However, manually annotating video and language data is very expensive, hence limiting the scale of such datasets, and consequently also limiting the performance of models trained on the datasets. The self-supervised pretraining then finetuning paradigm offers an easy and generic solution to this dilemma, where models are first pre-trained on large-scale unlabeled data by performing various “proxy tasks”, followed by finetuning the pre-trained model on downstream tasks where data is often limited. Recent advances on language pre-training (Devlin et al., 2019; Liu et al., 2019) demonstrate the effectiveness of this approach, where transformerbased (Vaswani et al., 2017) models pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on"
2021.naacl-main.193,2020.aacl-main.48,0,0.0301136,"is success, image-and-language precaptions) are tokenized and represented as a setraining models (Tan and Bansal, 2019; Lu et al., quence of WordPiece (Wu et al., 2016) tokens. We 2019; Chen et al., 2020; Zhou et al., 2020; Li et al., use a trainable word embedding layer to encode the 2020a) and video-and-language pre-training modtokens into feature representations. We use appearels (Sun et al., 2019; Miech et al., 2019; Zhu and ance and motion features to represent videos. For Yang, 2020; Miech et al., 2020; Li et al., 2020b; appearance, we use a resnet152 (He et al., 2016) Luo et al., 2020; Huang et al., 2020; Stroud et al., model pre-trained on ImageNet (Deng et al., 2009) 2020) have also shown promising results on many to extract 2D video features at 1FPS. Similarly, for vision and language tasks (Antol et al., 2015; Xu motion, we use a 3D ResNeXt (Xie et al., 2017; et al., 2016; Zhou et al., 2017). Hara et al., 2018; Kataoka et al., 2020) to extract For video-and-language pre-training in particu- 3D video features at 1FPS. The temporally aligned lar, most existing work (Sun et al., 2019; Miech appearance and motion features are L2-normalized et al., 2019; Zhu and Yang, 2020; Miech et al., and c"
2021.naacl-main.193,P19-1351,1,0.718477,"eighboring captions, to alleviate the inherent misalignment between the ASR captions and the videos. (iii) Extensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate the effectiveness of our approach. Furthermore, we also provide comprehensive ablation study and visualization to quantitatively and qualitatively examine the effect of using dense captions and the proposed constrained attention loss. 2 Related Work chitecture due to linearly increased computation cost. Inspired by recent work (Kim and Bansal, 2019; Kim et al., 2020) that uses dense captions (Johnson et al., 2016; Yang et al., 2017) to improve image and video QA models, we propose to add dense captions an as auxiliary text input that provide aligned visual cues to ease the difficulties of learning a video-text matching objective from often temporally and semantically misaligned ASR captions. In addition, we also propose a constrained attention loss, which employs an entropy minimizationbased regularization (Tanaka et al., 2018; Yi and Wu, 2019) to the model to encourage higher attention scores from the video to the correct matched capti"
2021.naacl-main.193,2020.acl-main.435,1,0.843888,"o alleviate the inherent misalignment between the ASR captions and the videos. (iii) Extensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate the effectiveness of our approach. Furthermore, we also provide comprehensive ablation study and visualization to quantitatively and qualitatively examine the effect of using dense captions and the proposed constrained attention loss. 2 Related Work chitecture due to linearly increased computation cost. Inspired by recent work (Kim and Bansal, 2019; Kim et al., 2020) that uses dense captions (Johnson et al., 2016; Yang et al., 2017) to improve image and video QA models, we propose to add dense captions an as auxiliary text input that provide aligned visual cues to ease the difficulties of learning a video-text matching objective from often temporally and semantically misaligned ASR captions. In addition, we also propose a constrained attention loss, which employs an entropy minimizationbased regularization (Tanaka et al., 2018; Yi and Wu, 2019) to the model to encourage higher attention scores from the video to the correct matched caption among a pool of"
2021.naacl-main.193,2021.ccl-1.108,0,0.0599785,"Missing"
2021.naacl-main.193,2020.acl-main.233,1,0.823533,"t al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol et al., 2015; Xu 1 Introduction et al., 2016; Yu et al., 2018a; Suhr et al., 2019; Zhou Video and language are ubiquitous in the world et al., 2017; Lei et al., 2020b). The most commonly we live. The ability to understand the interplay of used “proxy tasks” for multimodal pre-training are video and language is thus essential for intelligent masked language modeling (Devlin et al., 2019) agents to operate in real-world scenario. Past suc- (MLM) and cross-modal matching (Tan and Bansal, cess in video-and-language has mostly been driven 2019; Lu et al., 20"
2021.naacl-main.193,D18-1167,1,0.805718,"els pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol et al., 2015; Xu 1 Introduction et al., 2016; Yu et al., 2018a; Suhr et al., 2019; Zhou Video and language are ubiquitous in the world et al., 2017; Lei et al., 2020b). The most commonly we live. The ability to understand the interplay of used “proxy tasks” for multimodal pre-training are video and language is thus essential for intelligent masked language modeling (Devlin et al., 2019) agents to operate in"
2021.naacl-main.193,P02-1040,0,0.108731,"Missing"
2021.naacl-main.193,D16-1264,0,0.0691224,"retraining then finetuning paradigm offers an easy and generic solution to this dilemma, where models are first pre-trained on large-scale unlabeled data by performing various “proxy tasks”, followed by finetuning the pre-trained model on downstream tasks where data is often limited. Recent advances on language pre-training (Devlin et al., 2019; Liu et al., 2019) demonstrate the effectiveness of this approach, where transformerbased (Vaswani et al., 2017) models pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al.,"
2021.naacl-main.193,P18-1238,0,0.0225827,"the effectiveness of this approach, where transformerbased (Vaswani et al., 2017) models pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol et al., 2015; Xu 1 Introduction et al., 2016; Yu et al., 2018a; Suhr et al., 2019; Zhou Video and language are ubiquitous in the world et al., 2017; Lei et al., 2020b). The most commonly we live. The ability to understand the interplay of used “proxy tasks” for multimodal pre-training are video and language is thus essential fo"
2021.naacl-main.193,P19-1644,0,0.0191799,"ining (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol et al., 2015; Xu 1 Introduction et al., 2016; Yu et al., 2018a; Suhr et al., 2019; Zhou Video and language are ubiquitous in the world et al., 2017; Lei et al., 2020b). The most commonly we live. The ability to understand the interplay of used “proxy tasks” for multimodal pre-training are video and language is thus essential for intelligent masked language modeling (Devlin et al., 2019) agents to operate in real-world scenario. Past suc- (MLM) and cross-modal matching (Tan and Bansal, cess in video-and-language has mostly been driven 2019; Lu et al., 2019; Zhu and Yang, 2020) (e.g., 1 video-text matching), where MLM aims to learn a Code and models: https://github.com/ zine"
2021.naacl-main.193,D19-1514,1,0.900455,"unlabeled data by performing various “proxy tasks”, followed by finetuning the pre-trained model on downstream tasks where data is often limited. Recent advances on language pre-training (Devlin et al., 2019; Liu et al., 2019) demonstrate the effectiveness of this approach, where transformerbased (Vaswani et al., 2017) models pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol et al., 2015; Xu 1 Introduction et al., 2016; Yu et al., 2018a; Suhr et al., 2019; Zhou Vid"
2021.naacl-main.193,D18-1009,0,0.106601,"asy and generic solution to this dilemma, where models are first pre-trained on large-scale unlabeled data by performing various “proxy tasks”, followed by finetuning the pre-trained model on downstream tasks where data is often limited. Recent advances on language pre-training (Devlin et al., 2019; Liu et al., 2019) demonstrate the effectiveness of this approach, where transformerbased (Vaswani et al., 2017) models pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol"
2021.naacl-main.193,W18-5446,0,0.0416733,"on to this dilemma, where models are first pre-trained on large-scale unlabeled data by performing various “proxy tasks”, followed by finetuning the pre-trained model on downstream tasks where data is often limited. Recent advances on language pre-training (Devlin et al., 2019; Liu et al., 2019) demonstrate the effectiveness of this approach, where transformerbased (Vaswani et al., 2017) models pre-trained on large-scale unlabeled text corpus has shown to perform remarkably well across a wide range of natural language tasks (Rajpurkar et al., 2016; Williams et al., 2017; Zellers et al., 2018; Wang et al., 2018). Following this momentum, multimodal pre-training (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Su et al., 2019; Cho et al., 2021; Sun et al., 2019; Li et al., 2020c; Zhu and Yang, 2020; Miech et al., 2020; Li et al., 2020b; Lei et al., 2021) on large-scale image-text corpus (Sharma et al., 2018; Chen et al., 2015; Krishna et al., 2017) and video-text corpus (Lei et al., 2018; Miech et al., 2019; Sun et al., 2019) have also shown to outperform existing approaches (Anderson et al., 2018; Yu et al., 2018a; Lei et al., 2020a,b) on vision and language tasks (Antol et al., 2015; Xu 1"
2021.naacl-main.287,D13-1160,0,0.174915,"Missing"
2021.naacl-main.287,P14-1133,0,0.0719736,"Missing"
2021.naacl-main.287,2020.acl-main.771,0,0.04542,"Missing"
2021.naacl-main.287,D16-1011,0,0.0788619,"Missing"
2021.naacl-main.287,D19-5808,0,0.0175865,"for Q1 in Figure 1 have the sub-graph {F10 → R1 } common. Thus, in order to exploit these correlations which Multilabel-MULTI PROVER cannot capture explicitly, we further propose an improved variant of MULTI PROVER, named Iterative-MULTI PROVER, which 1 generates appropriate number of proofs by stacking Our code and models are publicly available at https: multiple node and edge encoders, each of which //github.com/swarnaHub/multiPRover. 3663 2 Related Work The task of rule reasoning (Clark et al., 2020) is related to other recently proposed tasks on QA (Weston et al., 2015; Yang et al., 2018; Lin et al., 2019; Tafjord et al., 2019; Richardson et al., 2020) and NLI (MacCartney and Manning, 2014). However, most of these tasks require implicit reasoning rules as opposed to explicit ones and the focus is either on broad language understanding or on single rule application. Below we discuss MULTI PROVER’s relation to multiple areas of NLP and ML. Structured Explanations: There is useful previous work on developing interpretable and explainable models (Doshi-Velez and Kim, 2017; Rudin, 2019; Hase and Bansal, 2020; Jacovi and Goldberg, 2020) for NLP. Explanations in NLP take three major forms – (1) extra"
2021.naacl-main.287,P19-1487,0,0.0221251,"or on single rule application. Below we discuss MULTI PROVER’s relation to multiple areas of NLP and ML. Structured Explanations: There is useful previous work on developing interpretable and explainable models (Doshi-Velez and Kim, 2017; Rudin, 2019; Hase and Bansal, 2020; Jacovi and Goldberg, 2020) for NLP. Explanations in NLP take three major forms – (1) extractive rationales or highlights (Zaidan et al., 2007; Lei et al., 2016; Yu et al., 2019; DeYoung et al., 2020) where a subset of the input text explain a prediction, (2) free-form or natural language explanations (Camburu et al., 2018; Rajani et al., 2019; Zhang et al., 2020; Kumar and Talukdar, 2020) that are not constrained to the input, and (3) structured explanations that range from semi-structured text (Ye et al., 2020) to chain of facts (Khot et al., 2020; Jhamtani and Clark, 2020; Gontier et al., 2020) to explanation graphs (based on edges between chains of facts) (Jansen et al., 2018; Jansen and Ustalov, 2019; Xie et al., 2020). Machine Learning over Sets: Set-based ML models (Zaheer et al., 2017; Lee et al., 2018; Zhang et al., 2019a; Kosiorek et al., 2020) have a wide range of applications including generating multiple image captions"
2021.naacl-main.287,P19-1177,0,0.0148036,"puts: Generating a set of proofs can be viewed as a task of generating multiple structured outputs (Prasad et al., 2014). Multiple prior studies focus on generating diverse unstructured texts (Gimpel et al., 2013; Dai et al., 2017; Xu et al., 2018; Raffel et al., 2020). which broadly span two categories – (1) using improved decoding techniques like beam search with intersibling ranking penalty (Li et al., 2016), iterative 3.2 Baseline PROVER Model beam search (Kulikov et al., 2018), diverse beam search (Vijayakumar et al., 2018), and sentence PROVER (Saha et al., 2020) builds on top of codes (Shu et al., 2019), (2) varying the hidden RoBERTa (Liu et al., 2019) and consists of a quesrepresentations or using multiple decoders (Dai tion answering (QA) module, a node module and et al., 2017; Jain et al., 2017; Shen et al., 2019). Our an edge module where the node and edge modules baseline, PROVER-top-p, which extends PROVER are used to predict a single proof graph. The into generate top-p proofs during inference falls in put to RoBERTa is the concatenation of the facts, the first category while MULTI PROVER falls in the rules and the question. The QA module takes in second category, where the multiple"
2021.naacl-main.287,D19-1608,0,0.0137845,"have the sub-graph {F10 → R1 } common. Thus, in order to exploit these correlations which Multilabel-MULTI PROVER cannot capture explicitly, we further propose an improved variant of MULTI PROVER, named Iterative-MULTI PROVER, which 1 generates appropriate number of proofs by stacking Our code and models are publicly available at https: multiple node and edge encoders, each of which //github.com/swarnaHub/multiPRover. 3663 2 Related Work The task of rule reasoning (Clark et al., 2020) is related to other recently proposed tasks on QA (Weston et al., 2015; Yang et al., 2018; Lin et al., 2019; Tafjord et al., 2019; Richardson et al., 2020) and NLI (MacCartney and Manning, 2014). However, most of these tasks require implicit reasoning rules as opposed to explicit ones and the focus is either on broad language understanding or on single rule application. Below we discuss MULTI PROVER’s relation to multiple areas of NLP and ML. Structured Explanations: There is useful previous work on developing interpretable and explainable models (Doshi-Velez and Kim, 2017; Rudin, 2019; Hase and Bansal, 2020; Jacovi and Goldberg, 2020) for NLP. Explanations in NLP take three major forms – (1) extractive rationales or hi"
2021.naacl-main.287,2020.lrec-1.671,0,0.0368029,"ights (Zaidan et al., 2007; Lei et al., 2016; Yu et al., 2019; DeYoung et al., 2020) where a subset of the input text explain a prediction, (2) free-form or natural language explanations (Camburu et al., 2018; Rajani et al., 2019; Zhang et al., 2020; Kumar and Talukdar, 2020) that are not constrained to the input, and (3) structured explanations that range from semi-structured text (Ye et al., 2020) to chain of facts (Khot et al., 2020; Jhamtani and Clark, 2020; Gontier et al., 2020) to explanation graphs (based on edges between chains of facts) (Jansen et al., 2018; Jansen and Ustalov, 2019; Xie et al., 2020). Machine Learning over Sets: Set-based ML models (Zaheer et al., 2017; Lee et al., 2018; Zhang et al., 2019a; Kosiorek et al., 2020) have a wide range of applications including generating multiple image captions (Vinyals et al., 2015), generating diverse translations (Cho et al., 2014; Bahdanau et al., 2015), enumerating rules in a logical inference system (Gao et al., 2019). Set problems are challenging because the number of valid solutions for a set of size n are n!, which increases faster than exponential in n and ignoring the set structure produces sub-optimal solutions (Zhang et al., 201"
2021.naacl-main.287,D18-1259,0,0.0311079,", all the 3 proofs for Q1 in Figure 1 have the sub-graph {F10 → R1 } common. Thus, in order to exploit these correlations which Multilabel-MULTI PROVER cannot capture explicitly, we further propose an improved variant of MULTI PROVER, named Iterative-MULTI PROVER, which 1 generates appropriate number of proofs by stacking Our code and models are publicly available at https: multiple node and edge encoders, each of which //github.com/swarnaHub/multiPRover. 3663 2 Related Work The task of rule reasoning (Clark et al., 2020) is related to other recently proposed tasks on QA (Weston et al., 2015; Yang et al., 2018; Lin et al., 2019; Tafjord et al., 2019; Richardson et al., 2020) and NLI (MacCartney and Manning, 2014). However, most of these tasks require implicit reasoning rules as opposed to explicit ones and the focus is either on broad language understanding or on single rule application. Below we discuss MULTI PROVER’s relation to multiple areas of NLP and ML. Structured Explanations: There is useful previous work on developing interpretable and explainable models (Doshi-Velez and Kim, 2017; Rudin, 2019; Hase and Bansal, 2020; Jacovi and Goldberg, 2020) for NLP. Explanations in NLP take three major"
2021.naacl-main.287,2020.acl-main.508,0,0.0284332,"lication. Below we discuss MULTI PROVER’s relation to multiple areas of NLP and ML. Structured Explanations: There is useful previous work on developing interpretable and explainable models (Doshi-Velez and Kim, 2017; Rudin, 2019; Hase and Bansal, 2020; Jacovi and Goldberg, 2020) for NLP. Explanations in NLP take three major forms – (1) extractive rationales or highlights (Zaidan et al., 2007; Lei et al., 2016; Yu et al., 2019; DeYoung et al., 2020) where a subset of the input text explain a prediction, (2) free-form or natural language explanations (Camburu et al., 2018; Rajani et al., 2019; Zhang et al., 2020; Kumar and Talukdar, 2020) that are not constrained to the input, and (3) structured explanations that range from semi-structured text (Ye et al., 2020) to chain of facts (Khot et al., 2020; Jhamtani and Clark, 2020; Gontier et al., 2020) to explanation graphs (based on edges between chains of facts) (Jansen et al., 2018; Jansen and Ustalov, 2019; Xie et al., 2020). Machine Learning over Sets: Set-based ML models (Zaheer et al., 2017; Lee et al., 2018; Zhang et al., 2019a; Kosiorek et al., 2020) have a wide range of applications including generating multiple image captions (Vinyals et al., 20"
2021.naacl-main.324,2020.emnlp-main.393,0,0.0658177,"hoice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likeli"
2021.naacl-main.324,2020.tacl-1.3,0,0.0240418,"med “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,W17-5401,0,0.025796,", 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah a"
2021.naacl-main.324,2020.acl-main.465,0,0.0806274,"solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al.,"
2021.naacl-main.324,2020.insights-1.13,0,0.0263233,"er, e.g., the multiple iterations of SemEval or WMT datasets over the years, we’ve already been handling this quite well—we accept that a model’s BLEU score on WMT16 is not comparable to WMT14. That is, it is perfectly natural for benchmark datasets to evolve as the community makes progress. The only thing Dynabench does differently is that it anticipates dataset saturation and embraces the loop so that we can make faster and more sustained progress. ever, it has also been found that model-in-the-loop counterfactually-augmented training data does not necessarily lead to better generalization (Huang et al., 2020). Given the distributional shift induced by adversarial settings, it would probably be wisest to combine adversarially collected data with nonadversarial data during training (ANLI takes this approach), and to also test models in both scenarios. To get the most useful training and testing data, it seems the focus should be on collecting adversarial data with the best available model(s), preferably with a wide range of expertise, as that will likely be beneficial to future models also. That said, we expect this to be both task and model dependent. Much more research is required, and we encourag"
2021.naacl-main.324,2020.acl-main.768,1,0.846016,"e the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle t"
2021.naacl-main.324,N19-1225,0,0.0738848,"bstantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and that encour- introduce Dynabench, an open-source, web-based age apples-to-apples model comparisons. Bench- research platform for dynamic data collection and marks provide a north star goal for researchers, and model benchmarking. The guiding hypothesis be4110 Proceedings of the 2021 Con"
2021.naacl-main.324,D17-1215,1,0.795486,"ard et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et a"
2021.naacl-main.324,2021.ccl-1.108,0,0.0774911,"Missing"
2021.naacl-main.324,D15-1166,0,0.00793057,"rk tasks, that milestone is now rou- cording to the narrow criteria used to define human performance) nonetheless fail on simple chaltinely reached within just a few years for newer lenge examples and falter in real-world scenarios. datasets (see Figure 1). As with the rest of AI, NLP A substantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and"
2021.naacl-main.324,2020.emnlp-main.154,0,0.0295127,"uperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring"
2021.naacl-main.324,J93-2004,0,0.0749322,"humans? This reveals the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang"
2021.naacl-main.324,C10-1091,0,0.0606954,"Missing"
2021.naacl-main.324,N19-1063,0,0.0241688,"d its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not s"
2021.naacl-main.324,P19-1334,0,0.0218097,"ang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al.,"
2021.naacl-main.324,K18-1007,1,0.843089,"for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing tar"
2021.naacl-main.324,W10-0719,1,0.726488,"collaborative effort, the platform is meant to be a platform technology for humanand-model-in-the-loop evaluation that belongs to the entire community. In the current iteration, the platform is set up for dynamic adversarial data collection, where humans can attempt to find modelfooling examples. This design choice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics."
2021.naacl-main.324,C18-1198,0,0.0239103,"ard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Mo"
2021.naacl-main.324,2020.acl-main.441,1,0.878791,"our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of resource collection and architectural improvements. Similar to Dynabench, recent work seeks to embrace this phenomenon, addressing many of the previously mentioned issues through an iterative human-and-model-in-the-loop annotation process (Yang et al., 2017; Dinan et al., 2019; Chen et al., 2019; Bartolo et al., 2020; Nie et al., 2020), to find “unknown unknowns” (Attenberg et al., 2015) or in a never-ending or life-long learning setting (Silver et al., 2013; Mitchell et al., 2018). The Adversarial NLI (ANLI) dataset (Nie et al., 2020), for example, was collected with an adversarial setting over multiple rounds to yield “a ‘moving post’ dynamic target for NLU systems, rather than a static benchmark that will eventually saturate”. In its few-shot learning mode, GPT-3 barely shows “signs of life” (Brown et al., 2020) (i.e., it is barely above random) on ANLI, which is evidence that we are still far away from human performance"
2021.naacl-main.324,S18-2023,0,0.0556136,"Missing"
2021.naacl-main.324,W12-4501,0,0.0439542,"the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However"
2021.naacl-main.324,P18-2124,1,0.869954,"Missing"
2021.naacl-main.324,D16-1264,0,0.230428,"n use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its lead"
2021.naacl-main.324,2020.acl-main.442,0,0.230761,"performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuri"
2021.naacl-main.324,K19-1019,0,0.019609,"al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models"
2021.naacl-main.324,N18-2002,0,0.0355102,"Missing"
2021.naacl-main.324,2020.emnlp-main.661,1,0.822242,"Missing"
2021.naacl-main.324,P19-1004,0,0.0188742,"put character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally"
2021.naacl-main.324,2020.acl-main.479,0,0.0170779,"ead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Etting"
2021.naacl-main.324,2020.acl-main.222,0,0.0281863,"19) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likelihood ilarly, the paradigm is perfectly compatible with training on i.i.d. train/test splits and human lan- collaborative settings that utilize human feedback, guage (Linzen, 2020; Stiennon et al., 2020). or even negotiation. The crucial aspect of this proposal is the fact that models and humans interact We think there is widespread agreement that something has to change about our standard eval- live “in the loop” for evaluation and data collection. uation paradigm and that we nee"
2021.naacl-main.324,D08-1027,0,0.352191,"Missing"
2021.naacl-main.324,D13-1170,1,0.0131607,"t data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather tha"
2021.naacl-main.324,2020.emnlp-main.746,0,0.0673253,"Missing"
2021.naacl-main.324,N18-1074,0,0.0416359,"Missing"
2021.naacl-main.324,L18-1239,0,0.0544033,"Missing"
2021.naacl-main.324,W19-3509,1,0.836306,"Missing"
2021.naacl-main.324,D19-1221,0,0.0180989,"2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of r"
2021.naacl-main.324,W18-5446,1,0.794698,"Missing"
2021.naacl-main.324,D19-1286,0,0.0188342,"ive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressiv"
2021.naacl-main.324,2020.tacl-1.25,0,0.0303378,"enging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzi"
2021.naacl-main.324,W17-3012,1,0.867269,"Missing"
2021.naacl-main.324,D18-1501,0,0.0349494,"Missing"
2021.naacl-main.324,N18-1101,1,0.774917,"he background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive bod"
2021.naacl-main.324,D18-1259,0,0.0459996,"Missing"
2021.naacl-main.324,2020.emnlp-main.397,0,0.0238252,"was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,2020.emnlp-main.659,1,0.819658,"Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by sim"
2021.naacl-main.380,P13-1100,1,0.854454,"Missing"
2021.naacl-main.380,P19-1102,0,0.243296,"multi-document summarization using a pre-trained encoder-decoder 1 All our code publicly available at: https://github. Transformer model (Lewis et al., 2019), depicted com/amazon-research/BartGraphSumm. in Fig. 1, along with an efficient encoding mech4768 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4768–4779 June 6–11, 2021. ©2021 Association for Computational Linguistics anism to encode longer input texts. To this end, we first provide a strong baseline for MDS on the Multi-News dataset (Fabbri et al., 2019) using a pre-trained encoder-decoder model, called BART (Lewis et al., 2019). Next, we incorporate a Longformer-based approach (Beltagy et al., 2020) into the pre-trained BART model, replacing the quadratic memory growth of the full self-attention mechanism with an efficient context window-based attention mechanism that scales the memory linearly w.r.t. the input length. This enables us to encode longer documents than previous work. This efficient encoding mechanism comprises local and global attention mechanisms that address the challenge of modeling inter-document context. approaches on the"
2021.naacl-main.380,D19-1428,0,0.0836103,"g scores based on relevancy and redundancy. Li et al. (2020) further showed the usefulness of pre-trained language models to improve the performance on MDS. However, this approach lacks a pre-trained decoder, and it also limits the document length that can be encoded by the pre-trained language models. In contrast, our work utilizes the pre-trained seq2seq BART (Lewis et al., 2019) model to improve the performance on MDS. We have also incorporated the Longformerbased attention mechanism (Beltagy et al., 2020) into BART model to encode long documents. To encode graphs into an MDS neural model, Fan et al. (2019) constructed a semantic graph representing key phrases and entities from the documents, as well as their expressed relationships; they used linearized forms of these graphs as inputs to their Transformer model. In contrast, we use dual encoders for encoding both documents text and linearized graph text information. Recently, Li et al. (2020) constructed a similarity graph, topic graph, and discourse graph between input documents and encoded this information directly, rather than in linearized form, into a Transformer. In our work, we build semantic graphs at the sentence level and create a con"
2021.naacl-main.380,D18-1443,0,0.151809,"Missing"
2021.naacl-main.380,N18-1065,0,0.14692,"Missing"
2021.naacl-main.380,D18-1446,0,0.0451338,"Missing"
2021.naacl-main.380,C16-1023,0,0.060189,"Missing"
2021.naacl-main.380,2020.acl-main.555,0,0.348161,"ory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.1 Decoder Figure 1: Illustration of our dual-encoder approach to summarizing multi-document clusters with graph encodings. The truncated concatenated text contains the beginnings of each cluster document; the graphs contain information from the full documents. for external deep context representations. Liu and Lapata (2019) and Li et al. ("
2021.naacl-main.380,C18-1101,0,0.0200875,"add auxiliary graph encodings) leads sentence relation graphs. Baralis et al. (2013) built to significant improvements on the Multi-News graphs over sets of terms, rather than sentences. Li dataset (achieving state-of-the-art), overall leading et al. (2016) built a graph over event mentions and to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). Based on vari- their relationships, in order to summarize news events using sentence extraction techniques. Liu ous automatic evaluation metrics, we show that adding graph encodings can help the model ab- et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs stract away from the specific lexical content of the input and generate summaries that are more ab- and then generate a summary using these graphs. stractive. Further human evaluation shows that More recently, the introduction of larger datasets they are also more informative and factually more for MDS has enabled researchers to train neural consistent with their input documents. We also test models for multi-document summarization. Liu our model with auxiliary graph encodings on the et al. (2018) introduced a large-scale dataset"
2021.naacl-main.380,N15-1114,0,0.075047,"Missing"
2021.naacl-main.380,P19-1500,0,0.234783,"over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.1 Decoder Figure 1: Illustration of our dual-encoder approach to summarizing multi-document clusters with graph encodings. The truncated concatenated text contains the beginnings of each cluster document; the graphs contain information from the full documents. for external deep context representations. Liu and Lapata (2019) and Li et al. (2020) have addressed the inter-document context modeling to some extent with local and global attention, and document-level similarity graphs. Further, Li et al. (2020) have addressed the later part of using external contextual information (large pre-trained language models, e.g., RoBERTa (Liu et al., 2019)) to improve the performance of MDS models. However, these pre-trained language models are (1) not scalable for long documents because of their encoding length limit and quadratic memory growth; and (2) they do not jointly explore alternate auxiliary information, e.g., semant"
2021.naacl-main.380,2021.ccl-1.108,0,0.0654593,"Missing"
2021.naacl-main.380,W04-3252,0,0.429601,"Missing"
2021.naacl-main.380,2020.emnlp-main.748,0,0.021931,"g graphs increases abstractiveness. Next, we concatenate the documents’ text with linearized graph text and give it has input to the BART model (‘BL-Graph-Concat’) which achieves slightly better results over the baseline. However, when we add the linearized graph text as a separate graph encoder (‘BL-Separate-Graph’; same as our ‘BART-Long-Graph’ model in Table 1), we achieve the best results. How abstractive are the summaries? Abstractive summarizers generate surprisingly extractive summaries, copying large fragments unmodified from the input documents into the summaries (Weber et al., 2018; Pilault et al., 2020). We hypothesize that providing graph representations of the input can help the model abstract away from the specific lexical content of the input and generate summaries that are more abstractive. Table 8 shows Different approaches of graph encodings. Tathe lexical overlap between the summaries and their ble 6 presents the results on various graph encoding inputs when truncating the input documents to difmethods. First, we replace the original input with ferent numbers of words, and when adding a graph linearized graph text and we observe a significant representation of the input (truncated to"
2021.naacl-main.380,J98-3005,0,0.387542,"acing the quadratic memory growth of the full self-attention mechanism with an efficient context window-based attention mechanism that scales the memory linearly w.r.t. the input length. This enables us to encode longer documents than previous work. This efficient encoding mechanism comprises local and global attention mechanisms that address the challenge of modeling inter-document context. approaches on the performance of the MDS system. 2 Related Work Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997; Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures – perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeFurther, we build consolidated semantic graph own (1998) summarized multiple documents by representations of the multiple input documents mapping them to abstract template representations, and explore"
2021.naacl-main.380,N18-1081,0,0.0263445,"ns across both documents. Radev and McKeFurther, we build consolidated semantic graph own (1998) summarized multiple documents by representations of the multiple input documents mapping them to abstract template representations, and explore ways to incorporate them into the then generating text from the templates. encoder-decoder model. The semantic graph In the early 2000s, datasets from the Document for a given multi-document cluster is a compact Understanding Conference (DUC), which included representation of subject-predicate-object triplets human-written summaries for multi-document clus(Stanovsky et al., 2018) extracted from the text of ters, sparked increased research interest. In the documents; see Fig. 3 for an example. We pro- LexRank, Erkan and Radev (2004) extracted the pose a dual encoding mechanism that separately en- most salient sentences from a multi-document cluscodes the regular text of a multi-document cluster ter by constructing a graph representing pairwise and a text representation of its graph. The regular sentence similarities and running a PageRank altext is encoded by the pre-trained BART encoder, gorithm on the graph. Subsequent approaches folwhile the graph text is encoded by"
2021.naacl-main.380,D08-1079,0,0.0435598,"ti-document cluster ter by constructing a graph representing pairwise and a text representation of its graph. The regular sentence similarities and running a PageRank altext is encoded by the pre-trained BART encoder, gorithm on the graph. Subsequent approaches folwhile the graph text is encoded by a transformer lowed the same paradigm while improving diverencoder that is not pre-trained. sity of the extracted sentences (Wan and Yang, Empirically, we show that our approach (includ- 2006) or adding document-level information into ing the ability to use longer parts of the input doc- the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their uments and add auxiliary graph encodings) leads sentence relation graphs. Baralis et al. (2013) built to significant improvements on the Multi-News graphs over sets of terms, rather than sentences. Li dataset (achieving state-of-the-art), overall leading et al. (2016) built a graph over event mentions and to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). Based on vari- their relationships, in order to summarize news events using sentence extraction techniques. Liu ous automatic evaluation m"
2021.naacl-main.381,W18-2501,0,0.0190822,"Missing"
2021.naacl-main.381,N18-1065,0,0.0237488,"Missing"
2021.naacl-main.381,N19-1419,0,0.0638963,"Missing"
2021.naacl-main.381,N18-1114,1,0.83673,"We also believe there is a connection between the above two effects, as the structural, syntactic information favors a lower-dimensional or even discrete space while the distributed, semantic information favors a higher-dimensional space. 5 Related Work Explicit TPR Structures in Neural Networks While earlier TPR work based on (Smolensky, 1990) focused on computability rather than learnability questions, recently TPRs have been incorporated into several recurrent deep learning models in order to solve various NLP tasks including Part-of-Speech tagging, constituency parsing, image captioning (Huang et al., 2018, 2019), question answering (Palangi et al., 2018; Schlag and Schmidhuber, 2018), and natural-to-formal language generation (program synthesis) (Chen et al., 2020). Most recently, TPRs have been introduced into Transformer architectures, starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside i"
2021.naacl-main.381,D18-1206,0,0.316024,"and (3) demonstrate the emergent structures in representations by revealing the disentangled syntactic and semantic information encoded in the role and filler spaces. To test the ability of our TP-T RANSFORMER with discrete roles against the standard Transformer and the TP-T RANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequenc"
2021.naacl-main.381,P19-1209,0,0.036398,"Missing"
2021.naacl-main.381,C18-1101,0,0.0419582,"Missing"
2021.naacl-main.381,W04-1013,0,0.0317552,"RMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequence of tokens i = 1, ..., I can be seen as T RANSFORMER are indeed better than the Trans- a 2-dimensional lattice of cells (i, l) where i is the 4781 2.2 Linear TPR Concat Concat Multi-Head Attention Fillers (F) Roles (R) Scaled Dot-Product Attention Role-Attention Linear L"
2021.naacl-main.381,W19-4825,0,0.0481743,"Missing"
2021.naacl-main.381,Q18-1005,0,0.021196,"lag and Schmidhuber, 2018), and natural-to-formal language generation (program synthesis) (Chen et al., 2020). Most recently, TPRs have been introduced into Transformer architectures, starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. only the semantic content (Liu and Lapata, 2018, 2019). To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while Song et al. (2018) introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism, Liao et al. (2018) uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 6 Conclusion In"
2021.naacl-main.381,P19-1500,0,0.033989,"Missing"
2021.naacl-main.381,P14-5010,0,0.00451574,"Missing"
2021.naacl-main.381,2020.acl-main.173,0,0.0184431,"erated by the TP-T RANSFORMER are significantly better in grammar. This corroborates our claim that having the TPR can improve the model’s ability to follow the correct syntax in composing the summary. On the Wikihow dataset, the Transformer receives more votes in regarding the saliency. However, our TP-T RANSFORMER maintains an advantage in grammar and achieves significantly better overall preferences. Unfaithful XSum Examples It is well-known that the XSum dataset contains a portion of unfaithful reference summaries that mention facts not included in the source article (Durmus et al., 2020; Maynez et al., 2020). Therefore, we are interested to find out whether our TP-T RANSFORMER is better than the baseline only at expressing the faithful content or it can also generate some external, “unfaithful"" facts that the baseline can’t cover. To answer this question, we randomly sample 100 examples from the XSum dev set and manually examine the source document, reference summary, and the two generated summaries. Among these 100 examples, we identify 71 examples whose reference summary includes “unfaithful"" facts that are not mentioned in the source. In 21 out of 71 examples, the Transformer baseline manages"
2021.naacl-main.381,K16-1028,0,0.203201,"ability of our TP-T RANSFORMER with discrete roles against the standard Transformer and the TP-T RANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequence of tokens i = 1, ..., I can be seen as T RANSFORMER are indeed better than the Trans- a 2-dimensional lattice of cells (i, l) where i is the 4781 2.2 Linear TPR Concat Concat Mult"
2021.naacl-main.381,N18-2102,1,0.854901,"r summarizing any type of company internal datasets (e.g., internal documents, reports, meetings, legal forms, etc.) to further improve the productivity and efficiency of the users in their daily activities without needing to read long documents. Structured Representations for Abstractive Summarization Compared to the extractive methods, abstractive summarization models usually fail to show extractive properties, and have ten- Failure mode. Even though our models yield facdency to copy text from the source (See et al., 2017; tually consistent summaries, as judged by human Paulus et al., 2018; Pasunuru and Bansal, 2018; Ce- evaluation, they can still generate factually inconlikyilmaz et al., 2018). More recent approaches sistent summaries or sometimes hallucinate inforthat use standard transformers deal with this issue mation that the source document does not include. by introducing hierarchical structures to encode lo- This might be due to the bias or noise in the traincal and global information separately focusing on ing data. Model builders wanting to use our archi4788 tecture to build models on their company internal datasets should build models with consideration of intellectual properties and privacy"
2021.naacl-main.381,P17-1099,0,0.305661,"at least 0.3 advantage are bolded. Arxiv (Cohan et al., 2018) is a long document summarization dataset of scientific publications from arXiv.org (113k). The task is to generate the abstract from the paper body. (2019), while a sentence in an XSum or Wikihow summary usually aggregates information from multiple source sentences. 3.2 CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset contains 93k articles from CNN and 220k articles from the Daily Mail. Every article is accompanied by a few human-written bullet points about its content. We use the nonanonymized version used in See et al. (2017). Experimental Setup The Transformer and the two TP-T RANSFORMERS all have 6 layers, 8 heads per layer, dimension per head dk =64, model dimension dm =512, and feedforward dimension df =2048 for the encoder and decoder. Our TP-T RANSFORMER with discrete roles has Nr =50 role embeddings of dimension dr =64 at every layer. For each dataset above, we train the all three models from scratch using an Adafactor Optimizer (Shazeer and Stern, 2018) with square root learning rate decay and dropout rate of 0.1. We evaluate the models using automatic metrics including ROUGE F1 score and METEOR. Dataset A"
2021.naacl-main.381,C18-1146,0,0.0207274,"starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. only the semantic content (Liu and Lapata, 2018, 2019). To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while Song et al. (2018) introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism, Liao et al. (2018) uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 6 Conclusion In this work, we enrich the Transformer model with the structured Tensor Product Representation for abstractive summarization tasks. We represent every token as a pair of role and fill"
2021.naacl-main.381,P19-1452,0,0.137421,"ely one-hot, thus restricting the role vectors to a highly discrete space. This structural inductive bias encourages the TP-T RANSFORMER to encode the syntactic information in the discrete roles while isolating the semantics in the continuous fillers. former’s generations. Furthermore, to investigate the structural representation that naturally emerges during training and the advantage of having compositional TPR hidden states, we design a suite of decoder probing tasks to explore the information encoded in the role, filler, and TPR space. We adopt the encoder probing task design presented in Tenney et al. (2019b) and create four decoder probing tasks: Part-of-speech tagging (POS), Dependency Labeling (DEP), Semantic Role Labeling (SRL), and Named Entity Labeling (NEL). Our findings collectively show that the decoder’s role vectors encode a wealth of syntactic structures, aiding the decoder in deducing the syntactic features (e.g., being a proper noun, being the object of the root predicate) of the next token to be generated. The decoder’s filler vectors on the other hand encode more semantic information (e.g., being a person’s name). Furthermore, we observe that having the compositional TPR results"
2021.naacl-main.54,D19-1410,0,0.0306572,"Missing"
2021.naacl-main.54,2020.acl-main.626,1,0.812095,"ork are: (1) scores are absolute and comparable from one session/system to another; (2) our framework fundamentally and conveniently extends upon prevailing static summarization evaluation practices and utilizes existing standard MDS dataset reference summaries. 4 Session Collection The evaluation of interactive systems requires real user sessions, as explained in §3. Using a prototype I NT S UMM system, described in §5.1, we conducted several cycles of session collection which uncovered multiple user-related challenges, in line with previous work on user task design (Christmann et al., 2019; Roit et al., 2020; Zuccon et al., 2013). In particular, recruited users may make undue interactions due to insincere or experimental behavior, yielding noisy sessions that do not reflect realistic system use. Additionally, without an objective informational goal, a user interacts with the system according to subjective interests, producing sessions that are objectively incomparable. efficiently filter out insincere workers, and, conversely, discover workers with an ability to apprehend salient information within text. The second stage assigns practice tasks that familiarize the workers to the I NT S UMM system"
2021.naacl-main.54,W17-1003,0,0.06858,"Missing"
2021.naacl-main.54,N19-1072,1,0.829875,"ents, we simulate each of our two systems on scripted query lists. Simulated sessions provide a means for quick development cycles and quality estimation. The first of two query lists, LSug , is constructed fully automatically: it consists of the top-10 ordered phrases in the system’s suggested queries component per topic. This mimics a “lower bound” user who adopts the simplest strategy, namely, clicking the suggested queries in order without using judgment even to choose among these queries. The second list, LOracle , consists of 10 randomly chosen crowdsourced summary content units (SCUs) (Shapira et al., 2019) for each of the topics. Since the SCUs were extracted from the reference summaries of the corresponding topics, they mimic a user who searches for the exact information required to maximize similarity to the same reference summaries which we then evaluate against. While this is not necessarily the optimal query list due to the randomized sampling of SCUs for queries, we consider it our (non-strict) “upper bound” for the sake of experimentation. The two “bounds” are relative to the system on which the simulations are carried on. Also, for fair comparison to real sessions, the simulation initia"
2021.naacl-main.54,D18-1087,1,0.834263,"y snapshots, we would first like to obtain comparable scores for each static summary that will capture the information gained along the session up to the current interaction. Existing static MDS benchmarks provide reference summaries at a single length for the purpose of evaluating a summary at a similar length. This presumably means we would require a series of reference summaries that differ by small length gaps for the sequence of lengthening snapshots, which is difficult and costly to produce. To address this obstacle, We address all of the above-mentioned evalua- we leverage a finding by Shapira et al. (2018) show659 0.47 0.37 0.32 0.27 Upper intersection Lower intersection ROUGE-1 Recall 0.42 0.22 Topic 1 0.17 Topic 2 0.12 70 120 170 220 270 320 Word Length 370 420 470 Figure 2: Example recall-curves of two sessions on an I NT S UMM system. Points plotted per interaction snapshot within a session. Range of intersection between observed summary lengths is bounded by dashed lines. ing that a reference summary of a single length can be used to relatively evaluate varying length summaries on a topic with a recall measure such as ROUGE. Thus, utilizing existing MDS datasets is indeed possible for meas"
2021.naacl-main.54,D11-1124,0,0.0383304,"Missing"
2021.naacl-main.54,K17-1045,0,0.020812,"puts must comply to required interuation framework and demonstrate its utility. As action latency standards (Anderson, 2020; Attig the few existing I NT S UMM systems were not readet al., 2017), e.g., a few seconds for the initial sumily available or suitable for adaptation to our expermary and a few hundred milliseconds for a query imental setup, we developed an I NT S UMM system response. While we experimented with some more of our own, shown in Figure 1, with two different advanced techniques for MDS generation (e.g., algorithmic implementations for comparison. We Christensen et al., 2013; Yasunaga et al., 2017), gathered user sessions with our controlled crowdsentence representation (Reimers and Gurevych, sourcing procedure and evaluated their quality with 2019) and sentence similarity (Zhang* et al., 2020), 3 https://www.mturk.com we found that these are not practical for incorpo662 ration within the interactive low-latency setting, or that they could not handle the relatively large document set inputs. Instead, we developed the two back-end schemas described next (with further details in Appendix A). S1 runs a sentence clustering initial summary algorithm. Query-responses are generated in MMRstyle"
2021.naacl-main.54,N18-1152,0,0.0268761,"e on an earlier set (e.g. Li et al., 2008; Wang and Li, trolled crowdsourcing procedure that overcomes 2010; McCreadie et al., 2014; Zopf et al., 2016). the above obstacles, making the evaluation process Evaluation approaches predominantly include autoreliable and much more accessible for researchers interested in pursuing I NT S UMM research. See §4. matic ROUGE (Lin, 2004) measurement, i.e. word overlap against reference summaries, and manual We demonstrate the use of our full evaluation responsiveness (Dang, 2006) scores or pairwise framework on two I NT S UMM systems that we imcomparison (Zopf, 2018) between summaries. plemented, which apply different algorithms but 1 share a common user interface, with the DUC https://github.com/OriShapira/ InterExp 2006 (Dang, 2006) MDS dataset. Analysis shows 658 In the related QA task (Voorhees et al., 1999), a system extracts an answer for a targeted question. Similarly, in the interactive setting, a conversational QA (Reddy et al., 2019) system extracts answers to a series of interconnected questions with a clear informational goal. To check correctness in both cases, a system answer is simply compared to the true answer via text-comparison. On the"
2021.naacl-main.54,C16-1102,0,0.0244943,"are less replica- and Marcu, 2006; Zhao et al., 2009; Cao et al., ble, not scalable, and not always easily attainable. 2016; Feigenblat et al., 2017; Baumel et al., 2018), and incremental update summarization (Dang and In contrast, standard crowdsourcing induces noise Owczarzak, 2008), generating a summary of a docand overly tolerates subjective behavior, hindering replicability and comparability. We describe a con- ument set with the assumption of prior knowledge on an earlier set (e.g. Li et al., 2008; Wang and Li, trolled crowdsourcing procedure that overcomes 2010; McCreadie et al., 2014; Zopf et al., 2016). the above obstacles, making the evaluation process Evaluation approaches predominantly include autoreliable and much more accessible for researchers interested in pursuing I NT S UMM research. See §4. matic ROUGE (Lin, 2004) measurement, i.e. word overlap against reference summaries, and manual We demonstrate the use of our full evaluation responsiveness (Dang, 2006) scores or pairwise framework on two I NT S UMM systems that we imcomparison (Zopf, 2018) between summaries. plemented, which apply different algorithms but 1 share a common user interface, with the DUC https://github.com/OriShap"
2021.naacl-main.82,W19-8611,0,0.0140344,"tree to explicitly incorporate syntactic information and get syntax-aware instruction representations, hence achieving substantial improvement in generalizing to the unseen environment. Tree-based Language Representations. Dependency tree provides essential syntactic information for understanding a sentence. Tree-LSTM (Tai et al., 2015) has been widely used to encode parsed tree information and shown improvement over multiple tasks, such as relation extraction (Miwa and Bansal, 2016; Geng et al., 2020), machine translation (Su et al., 2020; Choi et al., 2017; Eriguchi et al., 2016), dialogue (Rao et al., 2019), and language inference (Chen et al., 2017). We are novel in incorporating a dependency tree into the visionlanguage navigation task via a Tree-LSTM for better phrase-level alignment between the visual environment and language instructions. 3 Method As illustrated in Figure 2, our base model follows the sequence-to-sequence architecture of previous VLN agents. Our tree-based encoder module is built on top of the strong Environment Drop Agent 2 Related Work (Tan et al., 2019). The main difference is that we Visual and Textual Grounding in VLN. In employ a tree-based language encoder to encode"
2021.naacl-main.82,P15-1150,0,0.134725,"Missing"
2021.naacl-main.82,N19-1268,1,0.907945,"on success rate and achieves the new state-of-the-art (at the time of submission) on the RxR dataset, which contains instruction in three languages (English, Hindi, and Telugu). Moreover, by using structured information from syntax, we are also able to avoid word-level shallow overfitting of the model and hence achieve better generalization in the unseen environment. Our analysis further shows that our syntax-aware agent has better interpretability and learns better cross-modality matching. modal grounding in both the natural language instruction and the visual scene (Wang et al., 2018, 2019; Tan et al., 2019; Landi et al., 2019; Xia et al., 2020; Wang et al., 2020b,a; Xiang et al., 2020; Zhu et al., 2020b). Other works improve vision and language representations (Hu et al., 2019; Li et al., 2019; Huang et al., 2019b,a; Hao et al., 2020; Majumdar et al., 2020) and propose an additional progress monitor module (Ma et al., 2019b,a; Ke et al., 2019) and object and action aware modules (Qi et al., 2020b) that aid co-grounding. The closest work to ours is from Hong et al. (2020), where they use the dependency tree to generate pre-divided sub-instructions, and then propose a shifting module to select an"
2021.naacl-main.82,2020.acl-main.229,0,0.110159,"al environment via syntax-enriched alignment. Recently, several approaches were proposed to solve the Vision-Language Navigation task with better interactions between natural language in1 Introduction structions and visual scenes (Fried et al., 2018; Wang et al., 2019; Landi et al., 2019; Wang et al., Vision-Language Navigation defines the task of 2020a; Huang et al., 2019a; Hu et al., 2019; Marequiring an agent to navigate through a visual environment based on natural language instruc- jumdar et al., 2020; Ma et al., 2019a; Qi et al., tions (Anderson et al., 2018b; Misra et al., 2018; 2020b; Zhu et al., 2020a,c). Some approaches utilize soft attention over individual words for better Chen et al., 2019; Jain et al., 2019; Nguyen and cross-modal grounding, while others improve coDaumé III, 2019; Thomason et al., 2020). This task poses several challenges. To complete this task, grounding with better language and vision reprean embodied agent needs to perceive the surround- sentation and additional alignment module. ing environment, understand the given natural lanAlthough these models achieve significant imguage instructions, and most importantly, ground provement in performance, they do not explici"
2021.repl4nlp-1.29,P19-1595,0,0.0463846,"Missing"
2021.repl4nlp-1.29,D16-1264,0,0.0660158,"Missing"
2021.repl4nlp-1.29,N16-3020,0,0.164231,"Missing"
2021.repl4nlp-1.29,2021.ccl-1.108,0,0.0657923,"Missing"
2021.repl4nlp-1.29,N18-1202,0,0.0292395,"Missing"
2021.repl4nlp-1.29,Q19-1040,0,0.0276579,"Missing"
2021.sdp-1.9,2020.emnlp-main.750,0,0.0987672,"Missing"
2021.sdp-1.9,N18-2097,0,0.028158,"etraining. The best model is selected on the basis of validation ROUGE scores for one-line summaries on the validation set. This is done to select the model with the best &quot;extreme&quot; summarization capability. When evaluating on Pubmed, the number of sentences extracted is set to 6, as reported in (Zhong et al., 2020). For fine-tuning on S CI T LDRA as well as S CI T LDR-AIC, the batch size is set to 100 and the number of extracted sentences to form the final summary is 1. Experimental Setup Summarization Datasets We evaluate the models on two scientific summarization benchmark datasets— Pubmed (Cohan et al., 2018) and S CI T LDR (Cachola et al., 2020). We use the CNN/DM (Hermann et al., 2015) dataset for intermediate pretraining. S CI T LDR. S CI T LDR is a curated corpus containing computer science articles, with each article having one or more reference T LDR’s or one-sentence summaries. The inputs could either be abstractonly (S CI T LDR-A) or the abstract, introduction and conclusion sections of the article (S CI T LDRAIC). We present results for both settings and use the splits specified in (Cachola et al., 2020). Evaluation Metrics. The S CI T LDR tasks have multiple reference summaries for each"
2021.sdp-1.9,2020.emnlp-main.748,0,0.0342064,"in NLP and its benefits on the 6 tasks of classification. The benefits of this in summarization has been shown by Yu et al. (2021) where in they finetune BART (Lewis et al., 2020) on XSUM (Narayan et al., 2018) and show its results on low resource domain adaptation benchmark for summarization . We show the effects of intermediate pretraining in the context of scientific document summarization. Scientific Summarization Cachola et al. (2020) introduce the S CI T LDR task and benchmark a variety of summarization models such as MatchSum and BERTSUM on the task. Impressive results were reported by Pilault et al. (2020), Zaheer et al. (2020) on scientific datasets like Pubmed, arXiv using compute-intensive transformer based models. We report results on Pubmed and S CI T LDR where our models use significantly less compute and achieve superior results on S CI T LDR over BERTSUM and MatchSum. • Intermediate pretraining using labeled summarization datasets (even when containing articles that are very different in domain from scientific articles) is very beneficial to lowresource target tasks like S CI T LDR. We also derive additional benefits by filtering the intermediate pretraining data to only retain a subset"
2021.sdp-1.9,2020.acl-main.703,0,0.184753,".iitb.ac.in 73 Proceedings of the Second Workshop on Scholarly Document Processing, pages 73–82 June 10, 2021. ©2021 Association for Computational Linguistics entific summarization benchmark, S CI T LDR (Cachola et al., 2020). We also make the following key observations: ing summarization on scientific documents is an overlooked area. Intermediate Pretraining Howard and Ruder (2018) first introduced the idea of intermediate pretraining in NLP and its benefits on the 6 tasks of classification. The benefits of this in summarization has been shown by Yu et al. (2021) where in they finetune BART (Lewis et al., 2020) on XSUM (Narayan et al., 2018) and show its results on low resource domain adaptation benchmark for summarization . We show the effects of intermediate pretraining in the context of scientific document summarization. Scientific Summarization Cachola et al. (2020) introduce the S CI T LDR task and benchmark a variety of summarization models such as MatchSum and BERTSUM on the task. Impressive results were reported by Pilault et al. (2020), Zaheer et al. (2020) on scientific datasets like Pubmed, arXiv using compute-intensive transformer based models. We report results on Pubmed and S CI T LDR"
2021.sdp-1.9,2020.emnlp-main.635,0,0.0401932,"Missing"
2021.starsem-1.14,W14-4012,0,0.0775043,"Missing"
2021.starsem-1.14,P15-1150,0,0.41869,"Missing"
2021.starsem-1.14,traum-etal-2004-evaluation,0,0.0113101,"App, are common tools used by people to communicate in groups and in real time. In these venues multiple independent conversations often occur simultaneously with their individual utterances interspersed. It is reasonable to assume the existence of an underlying thread structure partitioning the full conversation into disjoint sets of utterances, which ideally represent independent sub-conversations. ∗ Equal contribution φi The task of identifying these sub-units, disentanglement, is a prerequisite for further downstream tasks among which question answering, summarization, and topic modeling (Traum et al., 2004; Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Additional structure can generally be found in these logs, as a particular utterance could be a response or a continuation of a previous one. Such reply-to relationships implicitly define threads as the connected components of the resulting graph topology, and can then be used for disentanglement (Mehri and Carenini, 2017; Dulceanu, 2016; Wang et al., 2008; Gaoyang Guo et al., 2018). Modeling work on conversation disentanglement spans more than a decade. Elsner and Charniak (2008, 2010) use feature based linear models to"
2021.starsem-1.14,I17-1062,0,0.0130548,"nversations. ∗ Equal contribution φi The task of identifying these sub-units, disentanglement, is a prerequisite for further downstream tasks among which question answering, summarization, and topic modeling (Traum et al., 2004; Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Additional structure can generally be found in these logs, as a particular utterance could be a response or a continuation of a previous one. Such reply-to relationships implicitly define threads as the connected components of the resulting graph topology, and can then be used for disentanglement (Mehri and Carenini, 2017; Dulceanu, 2016; Wang et al., 2008; Gaoyang Guo et al., 2018). Modeling work on conversation disentanglement spans more than a decade. Elsner and Charniak (2008, 2010) use feature based linear models to find 152 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 152–159 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics pairs of utterances belonging to the same thread and heuristic global algorithms to assign posts to threads. Mehri and Carenini (2017) and Jiang et al. (2018), while also adopting similar heuristics, use f"
2021.starsem-1.14,D14-1162,0,0.0870112,"a different approach by modeling the interactions between the predicted reply-to relations as a conditional random field. One challenge in building automatic systems that perform disentanglement is the scarcity of large annotated datasets to be used to train expressive models. A remarkable effort in this direction is the work of Kummerfeld et al. (2019a) and the release of a dataset containing more that 77k utterances from the IRC #Ubuntu channel with annotated reply-to structure. In the same paper, it is shown how a set of simple handcrafted features, pooling of utterances GloVe embeddings (Pennington et al., 2014), and a feed-forward classifier can achieve good performances on the disentanglement task. Most of the follow-up work on the dataset relies on BERT (Devlin et al., 2019) embeddings to generate utterance representations (Zhu et al., 2020; Gu et al., 2020; Li et al., 2020). Zhu et al. (2020) use an additional transformer module to contextualize these representations, while Gu et al. (2020); Li et al. (2020) use an LSTM. Two exceptions are Liu et al. (2020), which models thread membership in an online fashion and discards reply-to relationships, and the recent Yu and Joty (2020a) which uses point"
2021.starsem-1.14,2020.emnlp-main.512,0,0.120137,"embeddings (Pennington et al., 2014), and a feed-forward classifier can achieve good performances on the disentanglement task. Most of the follow-up work on the dataset relies on BERT (Devlin et al., 2019) embeddings to generate utterance representations (Zhu et al., 2020; Gu et al., 2020; Li et al., 2020). Zhu et al. (2020) use an additional transformer module to contextualize these representations, while Gu et al. (2020); Li et al. (2020) use an LSTM. Two exceptions are Liu et al. (2020), which models thread membership in an online fashion and discards reply-to relationships, and the recent Yu and Joty (2020a) which uses pointer networks (Vinyals et al., 2015). In this short paper, we use DAG-structured LSTMs (˙Irsoy et al., 2019) to study disentanglement. As a generalization of Tree-LSTMs (Tai et al., 2015a), DAG-LSTMs allow to faithfully represent the structure of a conversation, which is more properly described as a directed acyclic graph (DAG) than a sequence. Furthermore, DAGLSTMs allow for the systematic inclusion of structured information like user turn and mentions in the learned representation of the conversation context. We enrich the representation learned by the DAG-LSTM by concatenat"
2021.textgraphs-1.5,W09-3208,1,0.803824,"representations of event mentions and its arguments. Graph Representation Methods. Skipgram (Mikolov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as"
2021.textgraphs-1.5,W09-4303,1,0.847544,"averaged contextual representations of event mentions and its arguments. Graph Representation Methods. Skipgram (Mikolov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Josh"
2021.textgraphs-1.5,P10-1143,0,0.369636,"0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summary. For fair comparison, we train the same framework with the following graph representation learning methods. Event Coreference Resolution. Besides existing methods (Bejan and Harabagiu, 2010b; Liu et al., 2014) we implement the model architecture (Lee et al., 2017) that has achieved the current state-of-the-art results in entity coreference resolution (Joshi et al., 2019) and cross-document event Results and Analysis Dataset We construct corpus-level graphs for training, development, and test sets from the English subset of Automatic Content Extraction (ACE) 2005 dataset2 . We follow the pre-processing steps in (Lin et al., 2020) and show the dataset statistics in Table 1. 2 https://www.ldc.upenn.edu/collaborations/ past-projects/ace 47 Node Article ACE train dev test 521 30 40 E"
2021.textgraphs-1.5,P18-1045,0,0.0130917,"18; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger"
2021.textgraphs-1.5,P18-1198,0,0.0302608,"eight sharing between different relation types. Multiple Views. The structure of event networks can be viewed in multiple different perspectives. For example, when entity-entity relations are masked out, an event network degenerates to pieces of isolated events and only local neighborhood will be observed. The advantage of separate modeling 3.3 Topology Learning To capture neighborhood information, we train the graph encoder with relation discrimination loss to 45 learn the graph topology. X X X LT = ( E[log Dr (yi , yj )] i + r∈R j∈Nir X X r∈R linguistic properties (Hewitt and Manning, 2019; Conneau et al., 2018). The task of event network embedding requires the embedded distributional node representations to preserve semantic proximity, local neighborhood and global neighborhood. Accordingly, we intrinsically evaluate the semantics preservation with node typing and assess the local neighborhood preservation with event argument role classification. We also apply the node embeddings to a downstream task, event coreference resolution, to extrinsically evaluate the global neighborhood preservation. Node Typing and Event Argument Role Classification are conducted under the same evaluation setting: given t"
2021.textgraphs-1.5,W16-1004,1,0.882797,"Missing"
2021.textgraphs-1.5,N19-1423,0,0.208811,"work denoted as G = {V, E}, where V and E are node and edge sets, respectively. Each node vi = hai , bi , si , li i ∈ V represents an event or entity mention, where ai and bi are the start and end word indices in sentence si , and li is the node type label. Each edge eij = hi, j, lij i ∈ E represents an event-entity or entity-entity relation, where i and j are indices of the involved nodes and lij is the edge type label. In this work, we initialize the semantic representation of each node vi with an m-dimensional attribute vector xi derived from sentence context using a pretrained BERT model (Devlin et al., 2019). Semantic Proximity (Gao and Huang, 2018). Given an event network G = {V, E}, the semantic proximity of node vi and node vj is determined by We design Event Network Structural Probes, an evaluation framework including a series of structural probing tasks, to check the model’s capability to implicitly incorporate event network structures. In this work, the learned node embeddings are intrinsically evaluated with node typing and event argument role classification tasks, and applied to the downstream task of event coreference resolution. Experimental results on the augmented Automatic Content Ex"
2021.textgraphs-1.5,P14-2082,0,0.027139,"ntity relation. In this example, Execution event and Set Fire event are connected through two paths, which tell the story of angry protesters revenge the death of Nimral-Nimr against Saudi Arabia by attacking its embassy. event network. In Figure 1, a good representation of the Set Fire event should involve the Execution event because the latter clarifies the grievance motivating the former. We further enrich event representations by introducing more context from the entire event network. Compared with other methods to connect events (e.g., with eventevent relations (Pustejovsky et al., 2003; Cassidy et al., 2014; Hong et al., 2016; Ikuta et al., 2014; O’Gorman et al., 2016)), our representation of each event grounded in an event network is semantically richer. the event representations. GENE and its variants significantly outperform the baseline methods on various tasks. In summary, our contributions are: • We formalize the task of event network embedding and accordingly propose a novel unsupervised learning framework, which trains the multi-view graph encoder with topology and semantics learning losses. • We design a series of incrementally structural probing tasks, including node typing, argument r"
2021.textgraphs-1.5,W16-1701,1,0.893696,"Missing"
2021.textgraphs-1.5,D19-1495,0,0.0237863,"d event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns distributed event representations using supervised multi-task le"
2021.textgraphs-1.5,Y00-1012,0,0.398674,"ncluding trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety"
2021.textgraphs-1.5,I17-1010,0,0.023199,"models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns di"
2021.textgraphs-1.5,W14-2903,0,0.0300513,"Missing"
2021.textgraphs-1.5,2020.tacl-1.5,0,0.285549,"2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj"
2021.textgraphs-1.5,D19-1588,0,0.0127311,"i et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is"
2021.textgraphs-1.5,W19-3311,0,0.0233777,"oint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns distributed event repr"
2021.textgraphs-1.5,N19-1419,0,0.0184702,"can be seen as a way of weight sharing between different relation types. Multiple Views. The structure of event networks can be viewed in multiple different perspectives. For example, when entity-entity relations are masked out, an event network degenerates to pieces of isolated events and only local neighborhood will be observed. The advantage of separate modeling 3.3 Topology Learning To capture neighborhood information, we train the graph encoder with relation discrimination loss to 45 learn the graph topology. X X X LT = ( E[log Dr (yi , yj )] i + r∈R j∈Nir X X r∈R linguistic properties (Hewitt and Manning, 2019; Conneau et al., 2018). The task of event network embedding requires the embedded distributional node representations to preserve semantic proximity, local neighborhood and global neighborhood. Accordingly, we intrinsically evaluate the semantics preservation with node typing and assess the local neighborhood preservation with event argument role classification. We also apply the node embeddings to a downstream task, event coreference resolution, to extrinsically evaluate the global neighborhood preservation. Node Typing and Event Argument Role Classification are conducted under the same eval"
2021.textgraphs-1.5,2021.naacl-main.274,1,0.730225,"lov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2"
2021.textgraphs-1.5,D12-1045,0,0.0247054,"et al., 2019) jointly model nodes and edges. Attributed network embedding approaches (Gao and Huang, 2018; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (To"
2021.textgraphs-1.5,K16-1008,0,0.01633,"y features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) proposes a tensor-based event composition approach to combine a trigger and arguments to represent each event. We extend the definition of scenario to multiple inter-connected events. (Modi, 2016) captures statistical dependencies between events but limits to script data sets where the events are naturally organized in sequential temporal order. Our approach captures a rich variety of explicit semantic connections among complex events. (Hong et al., 2018) learns distributed event representations using supervised multi-task learning, while our framework is based on unsupervised learning. Network Embedding. Our work falls into the scope of unsupervised learning for heterogeneous attributed network embeddings. Heterogeneous network embedding methods (Chang et al., 2015; 7 Conclusions and"
2021.textgraphs-1.5,D17-1018,0,0.042921,"by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summary. For fair comparison, we train the same framework with the following graph representation learning methods. Event Coreference Resolution. Besides existing methods (Bejan and Harabagiu, 2010b; Liu et al., 2014) we implement the model architecture (Lee et al., 2017) that has achieved the current state-of-the-art results in entity coreference resolution (Joshi et al., 2019) and cross-document event Results and Analysis Dataset We construct corpus-level graphs for training, development, and test sets from the English subset of Automatic Content Extraction (ACE) 2005 dataset2 . We follow the pre-processing steps in (Lin et al., 2020) and show the dataset statistics in Table 1. 2 https://www.ldc.upenn.edu/collaborations/ past-projects/ace 47 Node Article ACE train dev test 521 30 40 Event Entity Event-Entity 4,353 494 424 3,688 667 750 7,888 938 897 Edge Ent"
2021.textgraphs-1.5,W16-5706,0,0.0359262,"Missing"
2021.textgraphs-1.5,P17-1178,1,0.809683,"out of 238 labels. Each label consists of an event type and an argument role type as defined in ACE. For example, the argument role label “Justice:Arrest-Jail:Agent"" can only be correctly selected when the event node implies the type “Justice:Arrest-Jail"" and the entity node implies its role being the “Agent"". Compared to the traditional argument role labeling procedure, this setting skips the step of mention identification, which has been done in network construction process. The performance is reported with multi-label classification Micro F1 score. 4.3 We perform automatic entity linking (Pan et al., 2017) to link entities to Wikipedia. Entity nodes linked to the same Wikipedia entity are merged into one node. We further retrieve entity-entity relations from Wikidata and enrich the event network with these connections, such as the part-whole relation between Tehran and Iran in Figure 1. We also add narrative event-event relations by connecting every pair of events within one document as edges in the graph. 5.2 Non-Graph Event Representation Methods. Mention-based method represents events with contextual representations inferred by BERT (Devlin et al., 2019). Tuple-based method uses the averaged"
2021.textgraphs-1.5,2020.emnlp-main.50,1,0.79706,"Missing"
2021.textgraphs-1.5,D16-1038,0,0.0360844,"Missing"
2021.textgraphs-1.5,2020.acl-main.713,1,0.736969,"n, we train the same framework with the following graph representation learning methods. Event Coreference Resolution. Besides existing methods (Bejan and Harabagiu, 2010b; Liu et al., 2014) we implement the model architecture (Lee et al., 2017) that has achieved the current state-of-the-art results in entity coreference resolution (Joshi et al., 2019) and cross-document event Results and Analysis Dataset We construct corpus-level graphs for training, development, and test sets from the English subset of Automatic Content Extraction (ACE) 2005 dataset2 . We follow the pre-processing steps in (Lin et al., 2020) and show the dataset statistics in Table 1. 2 https://www.ldc.upenn.edu/collaborations/ past-projects/ace 47 Node Article ACE train dev test 521 30 40 Event Entity Event-Entity 4,353 494 424 3,688 667 750 7,888 938 897 Edge Entity-Entity Original Wiki* 6,856 7,040 723 853 796 1,543 Event-Event Narrative* Coref 70,992 912 12,572 144 6,154 121 Table 1: Statistics for the enhanced ACE 2005 dataset. Wiki and Narrative are enriched event-event relations. GENE w/ LT ) have a significant drop of performance on this task, while our proposed model has the best performance because of the similarity and"
2021.textgraphs-1.5,liu-etal-2014-supervised,0,0.0315778,"Missing"
2021.textgraphs-1.5,L16-1631,0,0.0153208,"s arguments. Graph Representation Methods. Skipgram (Mikolov et al., 2013) learns graph topology by increasing the predicted similarity of adjacent node embeddings and decreasing the similarity of irrelevant node embeddings with random negative sampling: Event Coreference Resolution The goal of event coreference resolution is to determine which event mentions refer to the same real-world event. The features for similarity computation used in previous work are typically limited to event triggers, arguments and sentence-level contexts (Chen et al., 2009; Chen and Ji, 2009; Sammons et al., 2015; Lu and Ng, 2016; Chen and Ng, 2016; Duncan et al., 2017; Lai et al., 2021). However, event arguments are often distributed across the content of an article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The"
2021.textgraphs-1.5,P17-1009,0,0.0172825,"Attributed network embedding approaches (Gao and Huang, 2018; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introducing arguments (Levin, 1993; Goldberg, 1995; Ritter and Rosen, 2000; Huang and Ahrens, 2000; Iwata, 2005; Goldberg, 2006; Xu and Huang, 2013; Bies et al., 2016; Do et al., 2017; Kalm et al., 2019), intent and sentiment (Ding et al., 2019), and temporal information (Tong et al., 2008). (Weber et al., 2018) propo"
2021.textgraphs-1.5,H05-1004,0,0.206904,"event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summary. For fair compari"
2021.textgraphs-1.5,M95-1005,0,0.392176,"article. Therefore a global event network can ground event mentions into a wider context with related events and help cluster coreferential mentions more accurately. In this task we evaluate the impact of applying event network embedding as additional features on enhancing event coreference resolution. We concatenate the event embeddings learned by the event network and by a fine-tuned SpanBERT model (Joshi et al., 2020) as the input for the scoring function. The training procedure is the same as that in (Joshi et al., 2019). We report F1 scores in terms of B3 (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995), CEAFe (Luo, 2005), BLANC (Recasens and Hovy, 2011) metrics, and also their averaged results (AVG). 5 5.1 Baseline LG = X X X log σ(yjT yi )+ ( log σ(−yjT0 yi )) i j 0 ∈N / i j∈Ni Deep Graph Infomax (Velickovic et al., 2019) captures graph topology by maximizing the mutual information between patch representations and higher-level subgraph summary: LD = X X ( E[log D(yi , s)] i + j∈Ni X E[log(1 − D(yj 0 , s))]) j 0 ∈N / i where the subgraph summary s is read out as the average of node embeddings and D is the discriminator deciding the probability score for node’s being contained in the summar"
2021.textgraphs-1.5,W13-5408,0,0.184046,"ing, argument role classification, and event coreference resolution. 1 1 Introduction Understanding events is a fundamental human activity. Our minds represent events at various granularity and abstraction levels, which allows us to quickly access and reason about related scenarios. A typical event mention includes an event trigger (the word or phrase that most clearly expresses an event occurrence) and its arguments (i.e., participants in events). The lexical embedding of a trigger is usually not sufficient, because the type of an event often depends on its arguments (Ritter and Rosen, 2000; Xu and Huang, 2013; Weber et al., 2018). For example, the support verb “get” may indicate a Transfer.Ownership event (“Ellison to spend $10.3 billion to get his company.”) or a Movement.Transport event (“Airlines are getting flyers to destinations on time more often.”). In Figure 1, the event type triggered by “execution” is Life.Die instead of project implementation. However, such kind of atomic event representation is 1 Our code is released at https://github.com/ pkuzengqi/GENE 42 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 42–53 June 11,"
2021.textgraphs-1.5,D15-1020,1,0.831804,"imited ability in entity coreference resolution. In some failing cases, GENE model does not link two events because some of their connecting arguments are expressed as pronouns. This limitation is inherited from the upstream event extraction. 6 Dong et al., 2017; Wang et al., 2019) jointly model nodes and edges. Attributed network embedding approaches (Gao and Huang, 2018; Yang et al., 2015) on the other hand put focus on preserving node attributes when encoding the networks. Event Coreference Resolution. Most existing methods (Chen et al., 2009; Chen and Ji, 2009; Bejan and Harabagiu, 2010a; Zhang et al., 2015; Peng et al., 2016; Lai et al., 2021) only exploit local features including trigger, argument and sentence context matching. To prevent error propagation, some models perform joint inference between event extraction and event coreference resolution (Lee et al., 2012; Araki and Mitamura, 2015; Lu and Ng, 2017) or incorporate document topic structures (Choubey and Huang, 2018). To the best of our knowledge our method is the first to leverage the entire event networks to compute similarity features. Related Work Event Representation. Some previous efforts enrich event representations by introduc"
C08-2004,H05-1042,0,0.0133662,"es of the minimum-cut framework. 1 Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs — henceforth minimum cuts or min cuts — have been successfully employed in vision, computational biology, and natural language processing. Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005). 2 Method 2.1 Min-cut classification framework c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Binary classification problems are usually approached by considering each classification decision in isolation. More formally, let Xtest = 15 Coling 2008: Companion volume – Posters and Demonstrations, pages 15–18 Manchester, August 2008 {x1 , x2 , . . . , xn } be the test instances, drawn from some universe X, and let C = {c1 , c2 } be the two possible classes. Then, the"
C08-2004,P04-1035,1,0.0626428,"abel-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework. 1 Claire Cardie and Lillian Lee Dept. of Computer Science Cornell University {cardie,llee}@cs.cornell.edu Introduction Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs — henceforth minimum cuts or min cuts — have been successfully employed in vision, computational biology, and natural language processing. Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005). 2 Method 2.1 Min-cut classification framework c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Binary classification problems are usually approached by considering each classification decision in isolation. More formally, let Xtest = 15 Coling 2008: Companion volume – Posters and Demonstrations, pages 15–18 Manchester, August 2008 {x1 , x2 ,"
C08-2004,W06-1639,1,0.632297,"edge weights, respectively; but in general, the inclusion of even a relatively small number of negative edge weights makes finding a minimum cut NP-hard (McCormick et al., 2003). To avoid this computational issue, we propose several heuristics that encode disagreement information with non-negative edge weights. We instantiate our approach on a sentiment-polarity classification task — determining whether individual conversational turns in U.S. Congressional floor debates support or oppose some given legislation. Our preliminary results demonstrate promising improvements over the prior work of Thomas et al. (2006), who considered only the use of agreement information in this domain. Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications. The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way. Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it. Here, we report on work in progress that examines several novel heuristics for incorporating"
C18-1039,W14-1214,0,0.131994,"Missing"
C18-1039,P01-1008,0,0.255745,"on, and information retrieval (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Neural network models (Bowman et al., 2015; Parikh et al., 2016) and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled recent strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful for downstream applications such as question answering, semantic parsing, and information extraction (Fader et al., 2013; 463 Berant and Liang, 2014; Zhang et al., 2015). Wieting and Gimpel (2017a) recently introduced a large sentential paraphrase dataset via back-translation, and showed promising results when applied to learning sentence embeddings. In this work, we use this paraphrase dataset as an auxiliary generation task to improve our sentence sim"
C18-1039,P17-1080,0,0.022612,"ose to relax the priors in two ways: (1) we share the model parameters in a finer-grained scale, i.e. layer-specific sharing, by keeping some of their parameters private, while sharing related representations; and (2) we encourage shared parameters to be close in certain distance metrics with a penalty term instead of hard-parameter-tying (Luong et al., 2015). Multi-Level Sharing Mechanism Fig. 1 shows our multi-task model with parallel training of three tasks: sentence simplification (primary task), entailment generation (auxiliary task), and paraphrase generation (auxiliary task). Recently, Belinkov et al. (2017) observed that different layers in a sequence-tosequence model (trained on translation) exhibit different functionalities: lower-layers (closer to inputs) of the encoder learn to represent word structure while higher layers (farther from inputs) are more focused on semantics and meanings (Zeiler and Fergus (2014) observed similar findings for convolutional image features). Based on these findings, we share the higher-level layers2 between the entailment generation and sentence simplification tasks, since they share higher semantic-level language inference 2 We found that sharing higher-level s"
C18-1039,P14-1133,0,0.0362863,"ly, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful for downstream applications such as question answering, semantic parsing, and information extraction (Fader et al., 2013; 463 Berant and Liang, 2014; Zhang et al., 2015). Wieting and Gimpel (2017a) recently introduced a large sentential paraphrase dataset via back-translation, and showed promising results when applied to learning sentence embeddings. In this work, we use this paraphrase dataset as an auxiliary generation task to improve our sentence simplification model by teaching it about paraphrasing in a multi-task setting. Many control problems can be cast as a multi-armed bandits algorithm, where the goal of the agent is to select the arm/action from one of the M choices that gives the maximum expected future reward (Bubeck et al.,"
C18-1039,D15-1075,0,0.31498,"s optimization. In this work, we explore a multi-level (i.e., taskspecific higher-level semantic versus lower-level lexico-syntactic layer sharing) and soft-sharing mechanism for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the task of predicting entailment, contradiction, or neutral relationships, and is useful for many downstream tasks like Q&A, summarization, and information retrieval (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Neural network models (Bowman et al., 2015; Parikh et al., 2016) and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled recent strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful f"
C18-1039,E99-1042,0,0.520351,"Missing"
C18-1039,C96-2183,0,0.143553,"s, and our model’s learned entailment and paraphrasing skills. 1 Introduction Sentence simplification is the task of improving the readability and understandability of an input text. This challenging task has been the subject of research interest because it can address automatic ways of improving reading aids for people with limited language skills, or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It also has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models a"
C18-1039,P17-1152,0,0.0706285,"Missing"
C18-1039,P06-1048,0,0.0193945,"as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It also has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy gradient approaches (Zhang and Lapata, 2017). Our baseline is a novel application of the pointer-copy mechanism (See et al., 2017) for the sentence simplification task, which allows the model to directly copy words and phrases from the input to the output. We further improve this strong bas"
C18-1039,W11-1601,0,0.0983617,"r) with auxiliary tasks, hard versus soft sharing, dynamic mixing ratio sampling, as well as our model’s learned entailment and paraphrasing skills. 2 Related Work Previous approaches to sentence simplification systems range from hand-designed rules (Siddharthan, 2006), to syntactic and lexical simplification via synonyms and paraphrases (Siddharthan, 2014; Kaji et ˇ al., 2002; Horn et al., 2014; Glavaˇs and Stajner, 2015), as well as treating simplification as a monolingual MT task, where operations are learned from examples of complex-simple sentence pairs (Specia, 2010; Koehn et al., 2007; Coster and Kauchak, 2011; Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014). Recently, Xu et al. (2016) trained a syntax-based MT model using the newly proposed SARI as a simplification-specific objective. Further, Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving"
C18-1039,W14-1215,0,0.100566,"and human evaluation. Further, we present several ablation analyses on alternative layer sharing methods, soft versus hard sharing, dynamic multi-armed bandit sampling approaches, and our model’s learned entailment and paraphrasing skills. 1 Introduction Sentence simplification is the task of improving the readability and understandability of an input text. This challenging task has been the subject of research interest because it can address automatic ways of improving reading aids for people with limited language skills, or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It also has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and par"
C18-1039,P13-1158,0,0.0439054,"strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful for downstream applications such as question answering, semantic parsing, and information extraction (Fader et al., 2013; 463 Berant and Liang, 2014; Zhang et al., 2015). Wieting and Gimpel (2017a) recently introduced a large sentential paraphrase dataset via back-translation, and showed promising results when applied to learning sentence embeddings. In this work, we use this paraphrase dataset as an auxiliary generation task to improve our sentence simplification model by teaching it about paraphrasing in a multi-task setting. Many control problems can be cast as a multi-armed bandits algorithm, where the goal of the agent is to select the arm/action from one of the M choices that gives the maximum expected fu"
C18-1039,W08-1105,0,0.0536011,", 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It also has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy gradient approaches (Zhang and Lapata, 2017). Our baseline is a novel application of the pointer-copy mechanism (See et al., 2017) for the sentence simplification task, which allows the model to directly copy words and phrases from the input to the output. We further improve this strong baseline by bringing in auxiliar"
C18-1039,N13-1092,0,0.203985,"Missing"
C18-1039,P15-2011,0,0.121965,"Missing"
C18-1039,P18-1064,1,0.839431,"for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the task of predicting entailment, contradiction, or neutral relationships, and is useful for many downstream tasks like Q&A, summarization, and information retrieval (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Neural network models (Bowman et al., 2015; Parikh et al., 2016) and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled recent strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful for downstream applications such as question answering, semantic parsing, and information extraction (Fader et al., 2013; 463 Berant and Liang, 2014; Zhang et al., 2015). Wie"
C18-1039,P06-1114,0,0.0586522,"is similar to Luong et al. (2015), where different tasks share some common model parameters with alternating mini-batches optimization. In this work, we explore a multi-level (i.e., taskspecific higher-level semantic versus lower-level lexico-syntactic layer sharing) and soft-sharing mechanism for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the task of predicting entailment, contradiction, or neutral relationships, and is useful for many downstream tasks like Q&A, summarization, and information retrieval (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Neural network models (Bowman et al., 2015; Parikh et al., 2016) and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled recent strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; W"
C18-1039,D17-1206,0,0.0318516,"rics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving the generalization performance of a task with related tasks, has successful application to many domains of machine learning (Caruana, 1998; Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Pasunuru and Bansal, 2017; Pasunuru et al., 2017). Although there are many variants of multi-task learning (Ruder et al., 2017; Hashimoto et al., 2017; Luong et al., 2015), our approach is similar to Luong et al. (2015), where different tasks share some common model parameters with alternating mini-batches optimization. In this work, we explore a multi-level (i.e., taskspecific higher-level semantic versus lower-level lexico-syntactic layer sharing) and soft-sharing mechanism for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the task of predicting entailment, contradiction, or neutral relationships, and is useful for many downstream tasks like Q&A, summar"
C18-1039,P14-2075,0,0.037768,"bandit based switching of tasks during training improves over the traditional manually-tuned static mixing ratio. Lastly, we show several ablation studies based on different layer-sharing approaches (higher versus lower) with auxiliary tasks, hard versus soft sharing, dynamic mixing ratio sampling, as well as our model’s learned entailment and paraphrasing skills. 2 Related Work Previous approaches to sentence simplification systems range from hand-designed rules (Siddharthan, 2006), to syntactic and lexical simplification via synonyms and paraphrases (Siddharthan, 2014; Kaji et ˇ al., 2002; Horn et al., 2014; Glavaˇs and Stajner, 2015), as well as treating simplification as a monolingual MT task, where operations are learned from examples of complex-simple sentence pairs (Specia, 2010; Koehn et al., 2007; Coster and Kauchak, 2011; Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014). Recently, Xu et al. (2016) trained a syntax-based MT model using the newly proposed SARI as a simplification-specific objective. Further, Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce th"
C18-1039,N15-1022,0,0.11992,"Missing"
C18-1039,W03-1602,0,0.154781,"al., 1999). It also has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy gradient approaches (Zhang and Lapata, 2017). Our baseline is a novel application of the pointer-copy mechanism (See et al., 2017) for the sentence simplification task, which allows the model to directly copy words and phrases from the input to the output. We further improve this strong baseline by bringing in auxiliary entailment and paraphrasing knowledge via soft an"
C18-1039,S14-2131,0,0.0730812,"Missing"
C18-1039,P02-1028,0,0.11914,"o has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy gradient approaches (Zhang and Lapata, 2017). Our baseline is a novel application of the pointer-copy mechanism (See et al., 2017) for the sentence simplification task, which allows the model to directly copy words and phrases from the input to the output. We further improve this strong baseline by bringing in auxiliary entailment and paraphrasing knowledge via soft and dynamic multi-leve"
C18-1039,P13-1151,0,0.247986,"Missing"
C18-1039,P07-2045,0,0.00480034,"(higher versus lower) with auxiliary tasks, hard versus soft sharing, dynamic mixing ratio sampling, as well as our model’s learned entailment and paraphrasing skills. 2 Related Work Previous approaches to sentence simplification systems range from hand-designed rules (Siddharthan, 2006), to syntactic and lexical simplification via synonyms and paraphrases (Siddharthan, 2014; Kaji et ˇ al., 2002; Horn et al., 2014; Glavaˇs and Stajner, 2015), as well as treating simplification as a monolingual MT task, where operations are learned from examples of complex-simple sentence pairs (Specia, 2010; Koehn et al., 2007; Coster and Kauchak, 2011; Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014). Recently, Xu et al. (2016) trained a syntax-based MT model using the newly proposed SARI as a simplification-specific objective. Further, Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task lea"
C18-1039,S14-2055,0,0.014919,"ent tasks share some common model parameters with alternating mini-batches optimization. In this work, we explore a multi-level (i.e., taskspecific higher-level semantic versus lower-level lexico-syntactic layer sharing) and soft-sharing mechanism for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the task of predicting entailment, contradiction, or neutral relationships, and is useful for many downstream tasks like Q&A, summarization, and information retrieval (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Neural network models (Bowman et al., 2015; Parikh et al., 2016) and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled recent strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods"
C18-1039,P14-1041,0,0.693873,"ratio sampling, as well as our model’s learned entailment and paraphrasing skills. 2 Related Work Previous approaches to sentence simplification systems range from hand-designed rules (Siddharthan, 2006), to syntactic and lexical simplification via synonyms and paraphrases (Siddharthan, 2014; Kaji et ˇ al., 2002; Horn et al., 2014; Glavaˇs and Stajner, 2015), as well as treating simplification as a monolingual MT task, where operations are learned from examples of complex-simple sentence pairs (Specia, 2010; Koehn et al., 2007; Coster and Kauchak, 2011; Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014). Recently, Xu et al. (2016) trained a syntax-based MT model using the newly proposed SARI as a simplification-specific objective. Further, Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving the generalization performance of a task with related tasks, has s"
C18-1039,P02-1040,0,0.101451,"Missing"
C18-1039,D16-1244,0,0.0743507,"Missing"
C18-1039,P17-1117,1,0.935878,", Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving the generalization performance of a task with related tasks, has successful application to many domains of machine learning (Caruana, 1998; Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Pasunuru and Bansal, 2017; Pasunuru et al., 2017). Although there are many variants of multi-task learning (Ruder et al., 2017; Hashimoto et al., 2017; Luong et al., 2015), our approach is similar to Luong et al. (2015), where different tasks share some common model parameters with alternating mini-batches optimization. In this work, we explore a multi-level (i.e., taskspecific higher-level semantic versus lower-level lexico-syntactic layer sharing) and soft-sharing mechanism for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the tas"
C18-1039,W17-4504,1,0.854422,"sed reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving the generalization performance of a task with related tasks, has successful application to many domains of machine learning (Caruana, 1998; Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Pasunuru and Bansal, 2017; Pasunuru et al., 2017). Although there are many variants of multi-task learning (Ruder et al., 2017; Hashimoto et al., 2017; Luong et al., 2015), our approach is similar to Luong et al. (2015), where different tasks share some common model parameters with alternating mini-batches optimization. In this work, we explore a multi-level (i.e., taskspecific higher-level semantic versus lower-level lexico-syntactic layer sharing) and soft-sharing mechanism for improving sentence simplification via related tasks of entailment and paraphrase generation. Recognizing Textual Entailment (RTE) is the task of predicting entailme"
C18-1039,P17-1099,0,0.489398,"long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy gradient approaches (Zhang and Lapata, 2017). Our baseline is a novel application of the pointer-copy mechanism (See et al., 2017) for the sentence simplification task, which allows the model to directly copy words and phrases from the input to the output. We further improve this strong baseline by bringing in auxiliary entailment and paraphrasing knowledge via soft and dynamic multi-level, multi-task learning.1 Apart from the three simplification operations discussed above, we also ensure that the simplified output is a directed logical entailment w.r.t. the input text, i.e., does not generate any contradictory or unrelated information. We incorporate this entailment skill via multi-task learning (Luong et al., 2015) Th"
C18-1039,P15-2135,0,0.0687322,"Missing"
C18-1039,P08-1040,0,0.118632,"fication is the task of improving the readability and understandability of an input text. This challenging task has been the subject of research interest because it can address automatic ways of improving reading aids for people with limited language skills, or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It also has wide applications in NLP tasks as a preprocessing step, for example, to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy grad"
C18-1039,P17-1190,0,0.312446,"6; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Neural network models (Bowman et al., 2015; Parikh et al., 2016) and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled recent strong progress. Recently, Pasunuru et al. (2017) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful for downstream applications such as question answering, semantic parsing, and information extraction (Fader et al., 2013; 463 Berant and Liang, 2014; Zhang et al., 2015). Wieting and Gimpel (2017a) recently introduced a large sentential paraphrase dataset via back-translation, and showed promising results when applied to learning sentence embeddings. In this work, we use this paraphrase dataset as an auxiliary generation task to improve our sentence simplification model by teaching it about paraphrasing i"
C18-1039,D11-1038,0,0.214379,"Missing"
C18-1039,P12-1107,0,0.671964,"Missing"
C18-1039,Q15-1021,0,0.242642,"s equal or better than the traditional static mixing ratio (see Table 3). Also, we further show ablation study in Sec. 6 to show that this switching approach is better than the alternative approach of first using multi-armed bandits for finding an optimal ‘final’ mixing ratio and then re-training the model based on this bandits-selected mixing ratio. 4 Evaluation Setup Datasets We first describe the three standard sentence simplification datasets we evaluate on: Newsela, WikiSmall, and WikiLarge; next, we describe datasets for our auxiliary entailment and paraphrase generation tasks. Newsela (Xu et al., 2015) is acknowledged as a higher-quality dataset for studying sentence simplifications, as opposed to Wikipedia-based datasets which automatically align complex-simple sentence pairs and have generalization issues (Zhang and Lapata, 2017; Xu et al., 2015; Amancio and Specia, ˇ 2014; Hwang et al., 2015; Stajner et al., 2015). Newsela consists of 1, 130 news articles, and we follow previous work (Zhang and Lapata, 2017) to use the first 1, 070 documents for training, and 30 documents each for development and test. WikiSmall (Zhu et al., 2010) contains automatically-aligned complexsimple sentences fr"
C18-1039,Q16-1029,0,0.731926,"s learned entailment and paraphrasing skills. 2 Related Work Previous approaches to sentence simplification systems range from hand-designed rules (Siddharthan, 2006), to syntactic and lexical simplification via synonyms and paraphrases (Siddharthan, 2014; Kaji et ˇ al., 2002; Horn et al., 2014; Glavaˇs and Stajner, 2015), as well as treating simplification as a monolingual MT task, where operations are learned from examples of complex-simple sentence pairs (Specia, 2010; Koehn et al., 2007; Coster and Kauchak, 2011; Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014). Recently, Xu et al. (2016) trained a syntax-based MT model using the newly proposed SARI as a simplification-specific objective. Further, Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving the generalization performance of a task with related tasks, has successful application to man"
C18-1039,D17-1062,0,0.31047,"pata, 2014). Several sentence simplification systems focus on operations such as splitting a long sentence into shorter sentences (Siddharthan, 2006; Petersen and Ostendorf, 2007), deletion of less important words/phrases (Knight and Marcu, 2002; Clarke and Lapata, 2006; Filippova and Strube, 2008), and paraphrasing (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Inspired from machine translation based neural models, recent work has built end-to-end sentence simplification models along with attention mechanism, and further improved it with reinforcement-based policy gradient approaches (Zhang and Lapata, 2017). Our baseline is a novel application of the pointer-copy mechanism (See et al., 2017) for the sentence simplification task, which allows the model to directly copy words and phrases from the input to the output. We further improve this strong baseline by bringing in auxiliary entailment and paraphrasing knowledge via soft and dynamic multi-level, multi-task learning.1 Apart from the three simplification operations discussed above, we also ensure that the simplified output is a directed logical entailment w.r.t. the input text, i.e., does not generate any contradictory or unrelated information"
C18-1039,Q15-1009,0,0.0149139,"7) and Guo et al. (2018) presented results using entailment generation as an auxiliary task for abstractive summarization; however, we use entailment as well as paraphrasing knowledge in a soft and multi-level layer sharing setup to improve sentence simplification. Previous work (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; Wieting and Gimpel, 2017a) has developed methods and datasets for generating paraphrase pairs which can be useful for downstream applications such as question answering, semantic parsing, and information extraction (Fader et al., 2013; 463 Berant and Liang, 2014; Zhang et al., 2015). Wieting and Gimpel (2017a) recently introduced a large sentential paraphrase dataset via back-translation, and showed promising results when applied to learning sentence embeddings. In this work, we use this paraphrase dataset as an auxiliary generation task to improve our sentence simplification model by teaching it about paraphrasing in a multi-task setting. Many control problems can be cast as a multi-armed bandits algorithm, where the goal of the agent is to select the arm/action from one of the M choices that gives the maximum expected future reward (Bubeck et al., 2012). Optimal contro"
C18-1039,C10-1152,0,0.617701,"ard versus soft sharing, dynamic mixing ratio sampling, as well as our model’s learned entailment and paraphrasing skills. 2 Related Work Previous approaches to sentence simplification systems range from hand-designed rules (Siddharthan, 2006), to syntactic and lexical simplification via synonyms and paraphrases (Siddharthan, 2014; Kaji et ˇ al., 2002; Horn et al., 2014; Glavaˇs and Stajner, 2015), as well as treating simplification as a monolingual MT task, where operations are learned from examples of complex-simple sentence pairs (Specia, 2010; Koehn et al., 2007; Coster and Kauchak, 2011; Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014). Recently, Xu et al. (2016) trained a syntax-based MT model using the newly proposed SARI as a simplification-specific objective. Further, Zhang and Lapata (2017) used reinforcement learning in a sequence-to-sequence approach to directly optimize simplification metrics. In this work, we first introduce the pointer-copy mechanism (See et al., 2017) as a novel application to sentence simplification, and then use multi-task learning to bring in auxiliary entailment and paraphrasing skills. Multi-task learning, known for improving the generalizatio"
C18-1039,P18-1042,0,\N,Missing
D14-1139,afonso-etal-2002-floresta,0,0.00852921,"led to high POS tagging accuracy metrics. We call this voting scheme A LIGN. To see the benefit of A LIGN, we also compare to a simple scheme (NA¨I VE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our π function assigns identities to tags (e.g., tag 1 is assumed to"
D14-1139,N10-1083,0,0.0490183,"Missing"
D14-1139,D10-1117,0,0.123885,"adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent struct"
D14-1139,P11-1087,0,0.245259,"rove learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out f"
D14-1139,erjavec-etal-2010-jos,0,0.0151633,"cheme A LIGN. To see the benefit of A LIGN, we also compare to a simple scheme (NA¨I VE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our π function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training"
D14-1139,D13-1205,0,0.0118045,"resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1 . To map inputs to outputs, we start by building a model of the joint probability distribution pθ (x, y). We use a log-linear parameterization with feature vector f and weigh"
D14-1139,J92-4003,0,0.351774,"Missing"
D14-1139,P09-1042,0,0.0183092,"ributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn l"
D14-1139,W06-2920,0,0.00887404,"t Functions The output cost π should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a sufficient number of times until they were a similar size as the largest treebank. Then we counted gold POS tag unigrams and bigrams from the concatenation. tag unigram X NUM PRT ADV CONJ PRON DET ADJ ADP VERB . NOUN tag bigram DET PRT DET CONJ NUM ADV NOUN NOUN DET NOUN NOUN . count 50783 174613 179131 330210 436649 461880 615284 694685 906922 1018989 104266"
D14-1139,W11-2208,0,0.0341673,"Missing"
D14-1139,N09-1009,0,0.0296754,"over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1"
D14-1139,D11-1005,0,0.0132415,"to posterior regularization. The difference is that we specify preferences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of Grac¸a et al. (2011) in our experiments, improving over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work."
D14-1139,W12-1909,0,0.0241758,"Missing"
D14-1139,N10-1112,1,0.837539,"ammaticality. To address this, we introduce an observation cost function ∆ : X × X → R≥0 that indicates how much two observations differ. Using ∆, we define the following gain function γCCE1 (x(i) ) = n o X log exp θ > f (x(i) , y) − y∈Y(x(i) ) log X X n o exp θ > f (x0 , y 0 ) + ∆(x(i) , x0 ) x0 ∈Ni y 0 ∈Y(x0 ) The function ∆ inflates the score of neighborhood entries with larger differences from the observed x(i) . This gain function is inspired by ideas from structured large-margin learning (Taskar et al., 2003; Tsochantaridis et al., 2005), specifically softmax-margin (Povey et al., 2008; Gimpel and Smith, 2010). Softmax-margin extends conditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γCCE1 uses a cost function ∆ to specify how two inputs differ. But the motivations are similar: since poor structures have their scores artificially inflated by ∆, learning pays more attention to them, choosing weights that penalize them more than the low"
D14-1139,N12-1023,1,0.81247,"AT LM. 5 Expressing Structural Preferences Our second modification to CE allows us to specify structural preferences for outputs y. We first note that there exist objective functions for supervised structure prediction that never require computing the feature vector for the true output y (i) . Examples include Bayes risk (Kaiser et al., 2000; Povey and Woodland, 2002) and structured ramp loss (Do et al., 2008). These two objectives do, however, need to compute a cost function cost(y (i) , y), which requires the true output y (i) . We start with the following form of structured ramp loss from Gimpel and Smith (2012), transformed here to a gain function:   max θ > f (x(i) , y) − cost(y (i) , y) − y∈Y(x(i) )   θ > f (x(i) , y 0 ) + cost(y (i) , y 0 ) (1) max y 0 ∈Y(x(i) ) Maximizing this gain function for supervised learning corresponds to increasing the model score 1332 of outputs that have both high model score (θ > f ) and low cost, while decreasing the model score of outputs with high model score and high cost. For unsupervised learning, we do not have y (i) , so we simply drop y (i) from the cost function. The result is an output cost function π : Y → R≥0 which captures our a priori knowledge abou"
D14-1139,P07-1094,0,0.027513,"d CE as well as strong baselines from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised me"
D14-1139,P11-1061,0,0.0818384,"2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why th"
D14-1139,N06-1041,0,0.0842256,"z Z(θ) } The difficulty is the final term, log Z(θ), which requires summing over all possible inputs and all valid outputs for them. This summation is typically intractable for structured problems, and may even diverge. For this reason, EM is typically only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training = log (i) y∈Y(x(i) ) pθ (x , y) P P 0 0 x0 ∈Ni y 0 ∈Y(x0 ) pθ (x , y ) n o exp θ > f (x(i) , y) − X y∈Y(x(i) ) log X X n o exp θ > f (x0 , y 0 ) x0 ∈Ni y 0 ∈Y(x0 ) Two log Z(θ) terms cancel out, leaving the summation over input/output pairs in the neighborhood instead of the full summation over pairs. Two desiderata govern the choice of N. One is to make the summation over its elements computationally tractable. If N(x) = X for all x ∈ X, we obtain EM, so a smaller neighborhood typically must be used in practice. The second consideration is to t"
D14-1139,D07-1031,0,0.164084,"SCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Sn"
D14-1139,P11-1042,0,0.025385,"Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1 . To map inputs to outputs, we start by building a model of the joint probability distribution pθ (x, y). We use a log-linear parameterization with feature vector f and weight vector θ:  exp θ > f (x, y)  > pθ (x, y) = P 0 0 x0 ∈X,y 0 ∈Y(x0 ) exp θ f (x , y ) data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i) ), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE (x(i) ) = log Pr(x(i) |Ni ) P = log where the sum in the denominator ranges over all possible inputs and all valid outputs for them. In this paper, we consider ways of learning the paramet"
D14-1139,P04-1061,0,0.0631652,"e setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly"
D14-1139,2005.mtsummit-papers.11,0,0.00892963,"which we also compare to. When using both M AT LM and U NIV simultaneously, we first choose the best two α values by the LL criterion and the best two β values by the CE criterion when using only those individual costs. This gives us 4 pairs of values; we run experiments with these pairs and choose the pair to report using each of the model selection criteria. For system combination, we use the 4 system outputs resulting from these 4 pairs. For training bigram language models for the M AT LM cost, we use the language’s POS training data concatenated with its portion of the Europarl v7 corpus (Koehn, 2005) and the text of its type. For unknown words at test time, we use the UNK emission feature, the Brown cluster features with the special UNK cluster identifiers, and the word’s actual spelling features. 8 In subsequent experiments we tried C ∈ {0.01, 0.001} for the baseline CE setting and found minimal differences. 1336 neighborhood S HUFF 10 T RANS 1 mod. sel. none N/A CE M ATCH LL CE M AT LM LL none N/A CE M ATCH LL CE M AT LM LL cost DA M-1 1-1 45.0 38.0 48.9 31.5 49.9 34.4 49.1 34.3 50.2 40.0 58.5 42.7 58.5 42.5 58.8 42.8 59.4 43.5 58.7 42.8 NL M-1 1-1 55.1 45.7 56.5 46.4 56.5 46.4 59.6 50."
D14-1139,C10-2075,0,0.0258577,"ather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be im"
D14-1139,D12-1127,0,0.0254019,"Missing"
D14-1139,N09-1069,0,0.0766411,"Missing"
D14-1139,N06-1014,0,0.0530489,"of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typicall"
D14-1139,P05-1044,0,0.572934,"ion of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives. In this paper, we present a new objective function for weakly-supe"
D14-1139,J94-2001,0,0.481462,"ng structural preferences on the latent variable used to explain the observations. They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the ter"
D14-1139,P06-1072,0,0.50841,"text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 201"
D14-1139,D09-1086,0,0.0197016,"sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; th"
D14-1139,D10-1120,0,0.505272,"ion, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an output cost function in our framework. Posterior regularization (PR; Ganchev et al., 2010) is a general framework for declaratively specifying preferences on model outputs. Naseem et al. (2010) proposed universal syntactic rules for unsupervised dependency parsing and used them in a PR regime; we use analogous universal tag sequences in our cost function. Our output cost is similar to posterior regularization. The difference is that we specify preferences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of Grac¸a et al. (2011) in our experiments, improving over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging e"
D14-1139,P09-1009,0,0.460881,"07; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that"
D14-1139,nivre-etal-2006-talbanken05,0,0.0104745,"scheme (NA¨I VE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our π function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training with the π cost function. But we use M-1 and 1-1 accuracy to enable e"
D14-1139,N10-1116,0,0.100382,"for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t"
D14-1139,W10-2902,0,0.125561,"for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t"
D14-1139,petrov-etal-2012-universal,0,0.123056,"erm, defining our new gain function γCCE2 (x(i) ) = n o X log exp θ > f (x(i) , y) − π(y) − y∈Y(x(i) ) log X X n o exp θ > f (x0 , y 0 ) + π(y 0 ) x0 ∈Ni y 0 ∈Y(x0 ) Gimpel (2012) found that using such “softened” versions of the ramp losses worked better than the original versions (e.g., Eq. 1) when training machine translation systems. 5.1 Output Cost Functions The output cost π should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a suff"
D14-1139,N09-1024,0,0.19186,"upervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by sw"
D14-1139,P09-1057,0,0.125057,"d task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das"
D14-1139,P04-1062,0,0.0392678,"9; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2."
D14-1139,P10-1130,0,0.103689,"for dependency parsing (Smith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t"
D14-1139,D11-1117,0,0.0681351,"mith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives."
D14-1139,W11-0303,0,0.0711019,"mith and Eisner, 2006), 1 We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives."
D14-1139,D13-1204,0,0.0138648,"into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired o"
D14-1139,Q13-1001,0,0.0298322,"Missing"
D14-1139,D09-1071,0,0.0607765,"Missing"
D14-1139,P10-2039,0,0.0204087,"Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the sam"
D14-1139,Q14-1005,0,0.0187655,"Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i) }N i=1 . To map inputs to outputs, we start by build"
D14-1139,D11-1081,0,0.0239983,"e the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an outp"
D14-1139,D07-1096,0,\N,Missing
D16-1090,N12-1094,0,0.0271058,"e idea is to avoid a highly specific prediction if there is a chance of being wrong, and instead make a more generic prediction that is more likely to be right (Deng et al., 2012). Malinowski and Fritz (2014) use semantic segmentations in their approach to question answering, where they reason that objects not present in the segmentations should not be part of the answer. To the best of our knowledge, our work is the first to study the relevance of questions in VQA. Chen et al. (2012) classify users’ intention of questions for community question answering services. Most related to our work is Dodge et al. (2012). They extract visual text from within Flickr photo captions to be used as supervisory signals for training image captioning systems. Our motivation is to endow VQA systems the ability to detect non-visual questions to respond in a human-like fashion. Moreover, we also detect a more fine-grained notion of question relevance via true- and false-premise. 3 Datasets For the task of detecting visual vs. non-visual questions, we assume all questions in the VQA dataset (Antol et al., 2015) are visual, since the Amazon Mechanical Turk (AMT) workers were specifically instructed to ask questions about"
D16-1090,D15-1162,0,0.050685,"Missing"
D16-1090,N04-4022,0,0.0555937,", how well an image matches a caption (Feng and Lapata, 2013; Xu et al., 2015; Ordonez et al., 2011; Karpathy and Fei-Fei, 2015; Fang et al., 2015), and how well a video matches a description (Donahue et al., 2015; Lin et al., 2014a). 920 In our work, if a question is deemed irrelevant, the VQA model says so, as opposed to answering the question anyway. This is related to perception systems that do not respond to an input where the system is likely to fail. Such failure prediction systems have been explored in vision (Zhang et al., 2014; Devarakota et al., 2007) and speech (Zhao et al., 2012; Sarma and Palmer, 2004; Choularton, 2009; Voll et al., 2008). Others attempt to provide the most meaningful answer instead of suppressing the output of a model that is expected to fail for a given input. One idea is to avoid a highly specific prediction if there is a chance of being wrong, and instead make a more generic prediction that is more likely to be right (Deng et al., 2012). Malinowski and Fritz (2014) use semantic segmentations in their approach to question answering, where they reason that objects not present in the segmentations should not be part of the answer. To the best of our knowledge, our work is"
D16-1091,P16-1167,0,0.110126,"Missing"
D16-1091,P08-1090,0,0.115886,"Missing"
D16-1091,D14-1082,0,0.00571172,"Missing"
D16-1091,E95-1035,0,0.463411,"Missing"
D16-1091,P06-1095,0,0.0475025,"Missing"
D16-1091,W14-1606,0,0.0524619,"Missing"
D16-1091,K16-1008,0,0.00600807,"Missing"
D16-1091,N16-1098,1,0.102369,"Missing"
D16-1091,J88-2005,0,0.491123,"Missing"
D16-1091,D13-1020,0,0.0114345,"Missing"
D16-1091,N16-1008,0,0.0129676,"Missing"
D16-1091,J88-2006,0,0.231184,"Missing"
D16-1157,S12-1051,0,0.0144193,"the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages witho"
D16-1157,S13-1004,0,0.0200536,"sks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any m"
D16-1157,S14-2010,0,0.0216753,"ir similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the official task descriptions (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Gan"
D16-1157,N06-2001,0,0.0153082,"morphology, and word choice. Inspection of embeddings of particular character ngrams reveals etymological links; e.g., die is close to mort. We release our resources to the community in the hope that CHARAGRAM can provide a strong baseline for subword-aware text representation. GRAM 2 Related Work We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memor"
D16-1157,D15-1041,0,0.0132114,"l architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (C"
D16-1157,D15-1075,0,0.00575546,"sequence. We were interested to see whether CHARAGRAM - PHRASE could handle negation, since it does model limited information about word order (via character n-grams that span multiple words). We made a list of “not” bigrams that could be represented by a single word, then embedded each bigram using both models and did a nearest-neighbor search over a working vocabulary.6 The results, in Table 8, show how the CHARAGRAM - PHRASE embeddings model negation. In all cases but one, the near6 This has all words in PPDB-XXL, our evaluations, and two other datasets: SST (Socher et al., 2013) and SNLI (Bowman et al., 2015), resulting in 93,217 unique (up-to-casing) tokens. Word vehicals serious-looking near-impossible growths litered journeying babyyyyyy adirty Nearest Neighbors vehical, vehicles, vehicels, vehicular, cars, vehicle, automobiles, car serious, grave, acute, serious-minded, seriousness, gravity, serious-faced impossible, hard/impossible, audacious-impossible, impractical, unable growth, grow, growing, increases, grows, increase, rise, growls, rising liter, litering, lited, liters, literate, literature, literary, literal, lite, obliterated journey, journeys, voyage, trip, roadtrip, travel, tourney,"
D16-1157,P16-1139,0,0.00399984,"s outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.1 1 Introduction Representing textual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Zhu et al., 2015; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015). Some prior work has found benefit from using character-based compositional models that encode 1 Trained models and code are available at http://ttic. uchicago.edu/˜wieting. Our approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded int"
D16-1157,P14-2111,0,0.047939,") used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (Chrupała, 2014). CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss`a and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (J´ozefowicz et al., 2016). Most closely-related to our approach is the DSSM (instantiated variously as “deep semantic similarity model” or “deep structured semantic"
D16-1157,P16-1160,0,0.0220592,"models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (Chrupała, 2014). CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015"
D16-1157,P16-2058,0,0.0518,"Missing"
D16-1157,W15-3904,0,0.0617296,"Missing"
D16-1157,D13-1146,0,0.0519918,"ing and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character-by-character (Chung et al., 2016). Sutskever et al. (2011) and Graves (2013) used character-level RNNs for language modeling. Others trained character-level RNN language models to provide features for NLP tasks, including tokenization and segmentation (Chrupała, 2013; Evang et al., 2013), and text normalization (Chrupała, 2014). CNNs with character n-gram filters have been used to embed arbitrary word types for several tasks, including language modeling (Kim et al., 2015), part-of-speech tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss`a and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (J´ozefowicz et al., 2016). Most closely-related to our approach is the DSSM (instantiated variously as “deep semantic simil"
D16-1157,P15-2076,0,0.048898,"Missing"
D16-1157,ganitkevitch-callison-burch-2014-multilingual,0,0.0247593,"014; Agirre et al., 2015). 4.1.2 Preliminaries For training data, we use pairs from PPDB. For word similarity experiments, we train on word pairs and for sentence similarity, we train on phrase pairs. PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Ganitkevitch and Callison-Burch, 2014). Before training the CHARAGRAM model, we need to populate V , the vocabulary of character n-grams included in the model. We obtain these from the training data used for the final models in each setting, which is either the lexical or phrasal section of PPDB XXL. We tune over whether to include the full sets of character n-grams in these datasets or only those that appear more than once. When extracting n-grams, we include spaces and add an extra space before and after each word or phrase in the training and evaluation data to ensure that the beginning and end of each word is represented. We n"
D16-1157,N13-1092,0,0.0407243,"Missing"
D16-1157,D15-1181,1,0.802006,"xtual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Zhu et al., 2015; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015). Some prior work has found benefit from using character-based compositional models that encode 1 Trained models and code are available at http://ttic. uchicago.edu/˜wieting. Our approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded into a low-dimensional space using a single nonlinear transformation. This can be interpreted as learning embeddings of character n-grams, which are learned so as to produce effective sequence embeddings when a s"
D16-1157,J15-4004,0,0.0694913,"containing counts of character n-grams, inspired by Huang et al. (2013). This vector is embedded into a low-dimensional space using a single nonlinear transformation. This can be interpreted as learning embeddings of character n-grams, which are learned so as to produce effective sequence embeddings when a summation is performed over the character n-grams in the sequence. We consider three evaluations: word similarity, sentence similarity, and part-of-speech tagging. On multiple word similarity datasets, CHARAGRAM outperforms RNNs and CNNs, achieving state-ofthe-art performance on SimLex-999 (Hill et al., 2015). When evaluated on a large suite of sentencelevel semantic textual similarity tasks, CHARA GRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al. (2015a). The three architectures reach similar performance, though CHARAGRAM converges fastest to high accuracy. 1504 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504–1515, c Austin, Texas, November 1-5,"
D16-1157,N16-1162,0,0.112481,"Missing"
D16-1157,P15-1162,0,0.0128306,"Missing"
D16-1157,D14-1181,0,0.00208649,"populating a vector of length |V |with counts of character ngrams followed by a nonlinear transformation. We compare the CHARAGRAM model to two other models. First we consider LSTM architectures (Hochreiter and Schmidhuber, 1997) over the character sequence x, using the version from Gers et al. (2003). We use a forward LSTM over the characters in x, then take the final LSTM hidden vector as the representation of x. Below we refer to this model as “charLSTM.” We also compare to convolutional neural network (CNN) architectures, which we refer to below as “charCNN.” We use the architecture from Kim (2014) with a single convolutional layer followed by an optional fully-connected layer. We use filters of varying lengths of character n-grams, using two primary configurations of filter sets, one of which is identical to that used by Kim et al. (2015). Each filter operates over the entire sequence of character n-grams in x and we use max pooling for each filter. We tune over the choice of nonlinearity for both the convolutional filters and for the optional fullyconnected layer. We give more details below about filter sets, n-gram lengths, and nonlinearities. We note that using character n-gram conv"
D16-1157,P13-1149,0,0.0308746,"ord-aware text representation. GRAM 2 Related Work We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-le"
D16-1157,D15-1176,0,0.152769,"grams in the sequence. We consider three evaluations: word similarity, sentence similarity, and part-of-speech tagging. On multiple word similarity datasets, CHARAGRAM outperforms RNNs and CNNs, achieving state-ofthe-art performance on SimLex-999 (Hill et al., 2015). When evaluated on a large suite of sentencelevel semantic textual similarity tasks, CHARA GRAM embeddings again outperform the RNN and CNN architectures as well as the PARAGRAM PHRASE embeddings of Wieting et al. (2016). We also consider English part-of-speech (POS) tagging using the bidirectional long short-term memory tagger of Ling et al. (2015a). The three architectures reach similar performance, though CHARAGRAM converges fastest to high accuracy. 1504 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1504–1515, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We perform extensive analysis of our CHARA embeddings. We find large gains in performance on rare words, showing the empirical benefit of subword modeling. We also compare performance across different character n-gram vocabulary sizes, finding that the semantic tasks benefit far more from large vo"
D16-1157,P16-1100,0,0.0442846,"Missing"
D16-1157,W13-3512,0,0.235027,"tting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translation, whether for representing source or target words (Ling et al., 2015b; Luong and Man1505 ning, 2016), or for generating entire translations character"
D16-1157,S14-2001,0,0.0315977,"us on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013). For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or similar datasets from older tasks. Further details are provided in the offic"
D16-1157,P15-2070,0,0.0506303,"Missing"
D16-1157,D14-1162,0,0.10436,"Missing"
D16-1157,P15-1094,0,0.058996,"Missing"
D16-1157,C14-1015,0,0.230275,"k We first review work on using subword information in word embedding models. The simplest approaches append subword features to word embeddings, letting the model learn how to use the subword information for particular tasks. Some added knowledge-based morphological features to word representations (Alexandrescu and Kirchhoff, 2006; El-Desoky Mousa et al., 2013). Others learned embeddings jointly for subword units and words, defining simple compositional architectures (often based on addition) to create word embeddings from subword embeddings (Lazaridou et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Chen et al., 2015). A recent trend is to use richer functional architectures to convert character sequences into word embeddings. Luong et al. (2013) used recursive models to compose morphs into word embeddings, using unsupervised morphological analysis. Ling et al. (2015a) used a bidirectional long shortterm memory (LSTM) RNN on characters to embed arbitrary word types, showing strong performance for language modeling and POS tagging. Ballesteros et al. (2015) used this model to represent words for dependency parsing. Several have used character-level RNN architectures for machine translati"
D16-1157,K15-1026,0,0.0160069,"s are shown in Table 1. The CHARAGRAM model outperforms both the charLSTM and charCNN models, and also outperforms recent strong results on SL999. We also found that the charCNN and charLSTM models take far more epochs to converge than the CHARAGRAM model. We noted this trend across experiments and explore it further in Section 4.3. Comparison to Prior Work We found that performance of CHARAGRAM on word similarity tasks can be improved by using more character n-grams. This is explored in Section 4.4. Our best result from these experiments was obtained with the largest Model Hill et al. (2014) Schwartz et al. (2015) Faruqui and Dyer (2015) Wieting et al. (2015) CHARAGRAM (large) SL999 52 56 58 66.7 70.6 Table 2: Spearman’s ρ × 100 on SL999. refers to the CHARAGRAM CHARAGRAM (large) model described in Section 4.4. This model contains 173,881 character n-grams, more than the 100,283 in the CHARAGRAM model used in Table 1. model we considered, which contains 173,881 ngram embeddings. When using WS353 for model selection and training for 25 epochs, this model achieves 70.6 on SL999. To our knowledge, this is the best result reported on SL999 in this setting; Table 2 shows comparable recent results. Note that"
D16-1157,D13-1170,0,0.00784337,"mply averages the words in the sequence. We were interested to see whether CHARAGRAM - PHRASE could handle negation, since it does model limited information about word order (via character n-grams that span multiple words). We made a list of “not” bigrams that could be represented by a single word, then embedded each bigram using both models and did a nearest-neighbor search over a working vocabulary.6 The results, in Table 8, show how the CHARAGRAM - PHRASE embeddings model negation. In all cases but one, the near6 This has all words in PPDB-XXL, our evaluations, and two other datasets: SST (Socher et al., 2013) and SNLI (Bowman et al., 2015), resulting in 93,217 unique (up-to-casing) tokens. Word vehicals serious-looking near-impossible growths litered journeying babyyyyyy adirty Nearest Neighbors vehical, vehicles, vehicels, vehicular, cars, vehicle, automobiles, car serious, grave, acute, serious-minded, seriousness, gravity, serious-faced impossible, hard/impossible, audacious-impossible, impractical, unable growth, grow, growing, increases, grows, increase, rise, growls, rising liter, litering, lited, liters, literate, literature, literary, literal, lite, obliterated journey, journeys, voyage, t"
D16-1157,N15-1186,0,0.0973462,"am embeddings. When using WS353 for model selection and training for 25 epochs, this model achieves 70.6 on SL999. To our knowledge, this is the best result reported on SL999 in this setting; Table 2 shows comparable recent results. Note that a higher SL999 number is reported by Mrkˇsi´c et al. (2016), but the setting is not comparable to ours as they started with embeddings tuned on SL999. Lastly, we evaluated our model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013), using SL999 for model selection. We obtained a Spearman’s ρ of 47.1, which outperforms the 41.8 result from Soricut and Och (2015) and is competitive with the 47.8 reported by Pennington et al. (2014), which used a 42B-token corpus for training. 4.1.4 Sentence Embedding Experiments Training and Tuning We did initial training of our models using one pass through PPDB XL, which consists of 3,033,753 unique phrase pairs. Following Wieting et al. (2016), we use the annotated phrase pairs developed by Pavlick et al. (2015) as our validation set, using Spearman’s ρ to rank the models. We then take the highest performing models and train on the 9,123,575 unique phrase pairs in the phrasal section of PPDB XXL for 10 epochs. For"
D16-1157,W13-3204,0,0.0443586,"tagging (dos Santos and Zadrozny, 2014), named entity recognition (dos Santos and Guimar˜aes, 2015), text classification (Zhang et al., 2015), and machine translation (Costa-Juss`a and Fonollosa, 2016). Combinations of CNNs and RNNs on characters have also been explored (J´ozefowicz et al., 2016). Most closely-related to our approach is the DSSM (instantiated variously as “deep semantic similarity model” or “deep structured semantic model”) developed by Huang et al. (2013). For an information retrieval task, they represented words using feature vectors containing counts of character n-grams. Sperr et al. (2013) used a very similar technique to represent words in neural language models for machine translation. Our CHARAGRAM embeddings are based on this same idea. We show this strategy to be extremely effective when applied to both words and sentences, outperforming character LSTMs like those used by Ling et al. (2015a) and character CNNs like those from Kim et al. (2015). 3 Models We now describe models that embed textual sequences using their characters, including our CHARAGRAM model and the baselines that we compare to. We denote a character-based textual sequence by x = hx1 , x2 , ..., xm i, which"
D16-1157,P15-1150,0,0.0145383,"HARAGRAM embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.1 1 Introduction Representing textual sequences such as words and sentences is a fundamental component of natural language understanding systems. Many functional architectures have been proposed to model compositionality in word sequences, ranging from simple averaging (Mitchell and Lapata, 2010; Iyyer et al., 2015) to functions with rich recursive structure (Socher et al., 2011; Zhu et al., 2015; Tai et al., 2015; Bowman et al., 2016). Most work uses words as the smallest units in the compositional architecture, often using pretrained word embeddings or learning them specifically for the task of interest (Tai et al., 2015; He et al., 2015). Some prior work has found benefit from using character-based compositional models that encode 1 Trained models and code are available at http://ttic. uchicago.edu/˜wieting. Our approach, CHARAGRAM, uses a much simpler functional architecture. We represent a character sequence by a vector containing counts of character n-grams, inspired by Huang et al. (2013). This"
D16-1157,Q15-1025,1,0.846538,"is to produce embeddings for textual sequences such that the embeddings for paraphrases have high cosine similarity. Our third evaluation (Section 4.2) is a classification task, and follows the setup of the English part-of-speech tagging experiment from Ling et al. (2015a). 4.1 Word and Sentence Similarity We compare the ability of our models to capture semantic similarity for both words and sentences. We train on noisy paraphrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) with an L2 regularized contrastive loss objective function, following the training procedure of Wieting et al. (2015) and Wieting et al. (2016). More details are provided in the supplementary material. 4.1.1 Datasets For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013). For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to"
D16-1157,S15-2001,0,0.0627307,"the supplementary material. 4.1.1 Datasets For word similarity, we focus on two of the most commonly used datasets for evaluating semantic similarity of word embeddings: WordSim-353 (WS353) (Finkelstein et al., 2001) and SimLex-999 (SL999) (Hill et al., 2015). We also evaluate our best model on the Stanford Rare Word Similarity Dataset (Luong et al., 2013). For sentence similarity, we evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014). Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Most submissions for these tasks use supervised models that are trained and tuned on provided training data or s"
D16-1216,P13-1025,0,0.158414,"of the success of these neural networks. 1 Introduction Politeness theories (Brown and Levinson, 1987; Gu, 1990; Bargiela-Chiappini, 2003) include key components such as modality, indirection, deference, and impersonalization. Positive politeness strategies focus on making the hearer feel good through offers, promises, and jokes. Negative politeness examples include favor seeking, orders, and requests. Differentiating among politeness types is a highly nontrivial task, because it depends on factors such as a context, relative power, and culture. Mohit Bansal UNC Chapel Hill mbansal@cs.unc.edu Danescu-Niculescu-Mizil et al. (2013) proposed a useful computational framework for predicting politeness in natural language requests by designing various lexical and syntactic features about key politeness theories, e.g., first or second person start vs. plural. However, manually identifying such politeness features is very challenging, because there exist several complex theories and politeness in natural language is often realized via subtle markers and non-literal cues. Neural networks have been achieving high performance in sentiment analysis tasks, via their ability to automatically learn short and long range spatial relat"
D16-1216,P14-2009,0,0.0129601,"resented one of the first useful datasets and computational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; K´ad´ar and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks. There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting for sentiment compositionality across R"
D16-1216,C14-1008,0,0.0125649,"st useful datasets and computational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; K´ad´ar and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks. There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting for sentiment compositionality across RNN variants. Similarly, K´ad"
D16-1216,P14-1062,0,0.0486546,"omputational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; K´ad´ar and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks. There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting for sentiment compositionality across RNN variants. Similarly, K´ad´ar et al. (2016) 2036 analy"
D16-1216,N16-1082,0,0.243512,"istic features, while still performing better than such featurized systems. More importantly, we next present an intuitive interpretation of what these successful neural networks are learning, using the challenging politeness task as a testbed. To this end, we present several visualization strategies: activation clustering, first derivative saliency, and embedding space transformations, some of which are inspired by similar strategies in computer vision (Erhan et al., 2009; Simonyan et al., 2014; Girshick et al., 2014), and have also been recently adopted in NLP for recurrent neural networks (Li et al., 2016; K´ad´ar et al., 2016). The neuron activation clustering method not only rediscovers and extends several manually defined features from politeness theories, but also uncovers multiple novel strategies, whose importance we measure quantitatively. The first derivative saliency technique allows us to identify the impact of each phrase 2035 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2035–2041, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the final politeness prediction score via heatmaps, revealing useful"
D16-1216,P04-1035,0,0.0221077,"the featurized system and the neural model, thus providing a clear, quantitative interpretation of the success of these neural networks in automatically learning useful features. 2 Related Work Danescu-Niculescu-Mizil et al. (2013) presented one of the first useful datasets and computational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; K´ad´ar and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks. There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Sam"
D16-1216,W02-1011,0,0.0293985,"curacy gap between the featurized system and the neural model, thus providing a clear, quantitative interpretation of the success of these neural networks in automatically learning useful features. 2 Related Work Danescu-Niculescu-Mizil et al. (2013) presented one of the first useful datasets and computational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; K´ad´ar and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks. There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler a"
D16-1216,D13-1170,0,0.00875804,"rk Danescu-Niculescu-Mizil et al. (2013) presented one of the first useful datasets and computational approaches to politeness theories (Brown and Levinson, 1987; Goldsmith, 2007; K´ad´ar and Haugh, 2013; Locher and Watts, 2005), using manually defined lexical and syntactic features. Substantial previous work has employed machine learning models for other sentiment analysis style tasks (Pang et al., 2002; Pang and Lee, 2004; Kennedy and Inkpen, 2006; Go et al., 2009; Ghiassi et al., 2013). Recent work has also applied neural network based models to sentiment analysis tasks (Chen et al., 2011; Socher et al., 2013; Moraes et al., 2013; Dong et al., 2014; dos Santos and Gatti, 2014; Kalchbrenner et al., 2014). However, none of the above methods focused on visualizing and understanding the inner workings of these successful neural networks. There have been a number of visualization techniques explored for neural networks in computer vision (Krizhevsky et al., 2012; Simonyan et al., 2014; Zeiler and Fergus, 2014; Samek et al., 2016; Mahendran and Vedaldi, 2015). Recently in NLP, Li et al. (2016) successfully adopt computer vision techniques, namely first-order saliency, and present representation plotting"
D16-1216,P06-4018,0,\N,Missing
D16-1241,P05-1045,0,0.0209727,"Missing"
D16-1241,P16-1086,0,0.230218,"Missing"
D16-1241,P03-1054,0,0.00625259,"Missing"
D16-1241,D13-1020,0,0.155425,"Missing"
D16-1241,P15-2115,1,0.45212,"Missing"
D16-1241,D15-1237,0,0.0518536,"Missing"
D17-1101,D16-1091,1,0.836056,"photos using soft attention. ¯ t = GRUselect (pt−1 , h ¯ t−1 ), h ¯ t , vi ])), p(ya (t) = 1) = σ(MLP([h i 1 While the pointer network requires grounding labels, we regard the labels as latent variables 967 Figure 1: Model: the album encoder is a bi-directional GRU-RNN that encodes all album photos; the photo selector computes the probability of each photo being the tth album-summary photo; and finally, the story generator outputs a sequence of sentences that combine to tell a story for the album. straint to order the sequence of sentences within a story (related to the story-sorting idea in Agrawal et al. (2016)). For each story S we randomly shuffle its 5 sentences to generate negative story instances S 0 . We then apply a max-margin ranking loss to encourage correctly-ordered stories: Lrank (S, S 0 ) = max(0, m−log p(S 0 )+log p(S)). The final loss is then a combination of the generation and ranking losses: At each summarization step, t, the GRU takes the previous pt−1 and previous hidden state as input, ¯ t. h ¯ t is fused and outputs the next hidden state h with each photo representation vi to compute the ith photo’s attention pit = p(yai (t) = 1). At test time, we simply pick the photo with the"
D17-1101,P16-1046,0,0.0226335,"roaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions. For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description. Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014). Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Pre"
D17-1101,D15-1044,0,0.0338766,"ng: Most recent approaches to image captioning (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions. For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description. Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014). Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story"
D17-1101,P10-1058,0,0.0609546,"., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions. For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description. Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014). Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works include storyline graph modeling ("
D17-1101,N16-1147,0,0.612783,"oral dynamics given a large amount of albums, and (Kim and Xing, 2014) formulate the photo selection as a sparse time-varying directed graph. However, these visual summaries tend to be difficult to evaluate and selected photos may not agree with human selections. For languagebased approaches, a sequence of natural language sentences are generated to describe a set of photos. To drive this work (Park and Kim, 2015) collected a dataset mined from Blog Posts. However, this kind of data often contains contextual information or loosely related language. A more direct dataset was recently released (Huang et al., 2016), where multi-sentence stories are collected describing photo albums via Amazon Mechanical Turk. In this paper, we make use of the Visual Storytelling Dataset (Huang et al., 2016). While the authors provide a seq2seq baseline, they only deal with the task of generating stories given 5representative (summary) photos hand-selected by people from an album. Instead, we focus on the more challenging and realistic problem of end-toend generation of stories from entire albums. This requires us to either generate a story from all of the album’s photos or to learn selection mechanisms to identify repre"
D17-1101,N16-1086,1,0.220897,"ing (Vinyals et al., 2015b; Xu et al., 2015) have used CNN-LSTM structures to generate descriptions. For captioning video or movie content (Venugopalan et al., 2015; Pan et al., 2016), sequence-to-sequence models are widely applied, where the first sequence encodes video frames and the second sequence decodes the description. Attention techniques (Xu et al., 2015; Yu et al., 2016; Yao et al., 2015) are commonly incorporated for both tasks to localize salient temporal or spatial information. Video Summarization: Similar to documentation summarization (Rush et al., 2015; Cheng and Lapata, 2016; Mei et al., 2016; Woodsend and Lapata, 2010) which extracts key sentences and words, video summarization selects key frames or shots. While some approaches use unsupervised learning (Lu and Grauman, 2013; Khosla et al., 2013) or intuitive criteria to pick salient frames, recent models learn from human-created summaries (Gygli et al., 2015; Zhang et al., 2016b,a; Gong et al., 2014). Recently, to better exploit semantics, (Choi et al., 2017) proposed textually customized summaries. Visual Storytelling: Visual storytelling tries to tell a coherent visual or textual story about an image set. Previous works includ"
D17-1102,D14-1086,0,0.0234952,"bly useful language models that help disambiguate difficult cases for vision models, and that combining the two sources is the most effective model (across multiple languages). 2 Related Work We briefly discuss a small sample of the related work on language and vision datasets, summarization, and highlight prediction. There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al., 2016), or referring expressions (Kazemzadeh et al., 2014), or on the broader context (Huang et al., 2016). For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015). Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015). This approach is extended to movies in Tapaswi et al. (2016). 3 Data Collection Our dataset covers 218 videos from NALCS and 103 from LMS for a total of 321 videos from week 1 to week 9 in 2017 spring series from each tournament. Each week"
D17-1102,P11-1020,0,0.019273,"t effective model (across multiple languages). 2 Related Work We briefly discuss a small sample of the related work on language and vision datasets, summarization, and highlight prediction. There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al., 2016), or referring expressions (Kazemzadeh et al., 2014), or on the broader context (Huang et al., 2016). For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015). Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015). This approach is extended to movies in Tapaswi et al. (2016). 3 Data Collection Our dataset covers 218 videos from NALCS and 103 from LMS for a total of 321 videos from week 1 to week 9 in 2017 spring series from each tournament. Each week there are 10 matches for NALCS and 6 matches for LMS. Matches are best of 3, so consist of two games or three games. The first"
D17-1102,N16-1086,1,0.807349,"mark, September 7–11, 2017. 2017 Association for Computational Linguistics The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al., 2011) from short videos. For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch. Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to our work, Song (2"
D17-1102,N16-1012,0,0.0192935,"978 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al., 2011) from short videos. For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch. Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to"
D17-1102,K16-1028,0,0.0187834,"11, 2017. 2017 Association for Computational Linguistics The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al., 2011) from short videos. For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch. Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to our work, Song (2016) collects videos for"
D17-1102,D13-1155,0,0.0134302,"icles/ 2016-league-legends-world-championship-numbers 972 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 972–978 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al., 2011) from short videos. For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch. Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporti"
D17-1102,W10-0721,0,0.0528181,"combinations of language and vision models. Our results indicate that while surprisingly the visual models generally outperform language-based models, we can still build reasonably useful language models that help disambiguate difficult cases for vision models, and that combining the two sources is the most effective model (across multiple languages). 2 Related Work We briefly discuss a small sample of the related work on language and vision datasets, summarization, and highlight prediction. There has been a surge of vision and language datasets focusing on captions over the last few years, (Rashtchian et al., 2010; Ordonez et al., 2011; Lin et al., 2014), followed by efforts to focus on more specific parts of images (Krishna et al., 2016), or referring expressions (Kazemzadeh et al., 2014), or on the broader context (Huang et al., 2016). For video, similar efforts have collected descriptions (Chen and Dolan, 2011), while others use existing descriptive video service (DVS) sources (Rohrbach et al., 2015; Torabi et al., 2015). Beyond descriptions, other datasets use questions to relate images and language (Antol et al., 2015; Yu et al., 2015). This approach is extended to movies in Tapaswi et al. (2016)."
D17-1102,D16-1044,0,0.0222404,"es every 10th frame from 30fps video). The hidden state from the last cell is used as the V-CNNLSTM feature. This process is shown in Figure 3b. Joint lv-LSTM Model Our final lv-LSTM model combines the best vision and language models: V-CNN-LSTM and L-Char-LSTM. For the vision and language models, we can extract features Fv and Fl from V-CNN-LSTN and LChar-LSTM, respectively. Then we concatenate Fv and Fl , and feed it into a 2-layer MLP. The completed model is shown in Figure 3d. We expect there is room to improve this approach, by using more involved representations, e.g., Bilinear Pooling (Fukui et al., 2016), Memory Networks (Xiong et al., 2016), and Attention Models (Lu et al., 2016); this is future work. L-Word-LSTM and L-Char-LSTM Next, we discuss our language-based models using the audience chat text. Word-level LSTM-RNN models (Sutskever et al., 2014) are a common approach to embedding sentences. Unfortunately, this does not fit our Internet-slang style language with irregularities, “mispelled” words (hapy, happppppy), emojis (ˆ ˆ), abbreviations (LOL), marks (?!?!?!?!), or onomatopoeic cases 6 7 The number of these stop characters is then an encoding of the number of chats in the window. Th"
D17-1102,P17-1099,0,0.0127135,"on for Computational Linguistics The related problem of visually summarizing videos (as opposed to finding the highlights) has produced datasets of holiday and sports events with multiple users making summary videos (Gygli et al., 2014) and multiple users selecting summary key-frames (de Avila et al., 2011) from short videos. For language-based summarization, Extractive models (Filippova and Altun, 2013; Filippova et al., 2015) generate summaries by selecting important sentences and then assembling these, while Abstractive models (Chopra et al., 2016; Mei et al., 2016; Nallapati et al., 2016; See et al., 2017) generate/rewrite the summaries from scratch. Closer to our setting, there has been work on highlight prediction in football (soccer) and basketball based on audio of broadcasts (Cheng and Hsu, 2006) (Wang et al., 2004) where commentators may have an outsized impact or visual features (Bertini et al., 2005). In the spirit of our study, there has been work looking at tweets during sporting events (Hsieh et al., 2012), but the tweets are not as immediate or as well aligned with the games as the eSports comments. More closely related to our work, Song (2016) collects videos for Heroes of the Stor"
D17-1103,D15-1075,0,0.0197135,"nt for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.1 Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup. Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt¨aschel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation. ∗ log p(wt∗ |w1:t−1 , f1:n ) (1) where p(wt |w1:t−1 , f1:n ) = sof tmax(W T hdt ), W T is the projection matrix, and wt and hdt are the generated word and the RNN decoder hidden state at time step t, computed using the standard RNN recursion and attention-based context vector ct . De"
D17-1103,P11-1020,0,0.0486973,"ovel CIDEnt-reward RL model (‘CIDEnt-RL’). This model achieves statistically significant6 improvements on top of the strong CIDEr-RL model, on all automatic metrics (as well as human evaluation). Note that in Table 2, we also report the CIDEnt reward scores, and the CIDEnt-RL model strongly outperforms CIDEr and baseline models on this entailmentcorrected measure. Overall, we are also the new Rank1 on the MSR-VTT leaderboard, based on their ranking criteria. Experimental Setup Datasets We use 2 datasets: MSR-VTT (Xu et al., 2016) has 10, 000 videos, 20 references/video; and YouTube2Text/MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references/video. Standard splits and other details in supp. Automatic Evaluation We use several standard automated evaluation metrics: METEOR, BLEU4, CIDEr-D, and ROUGE-L (from MS-COCO evaluation server (Chen et al., 2015)). Human Evaluation We also present human evaluation for comparison of baseline-XE, CIDEr-RL, and CIDEnt-RL models, esp. because the automatic metrics cannot be trusted solely. Relevance measures how related is the generated caption w.r.t, to the video content, whereas coherence measures readability of the generated caption. Human Evaluation We also perf"
D17-1103,D16-1244,0,0.0416453,"Missing"
D17-1103,P17-1117,1,0.270665,"16) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup. Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt¨aschel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation. ∗ log p(wt∗ |w1:t−1 , f1:n ) (1) where p(wt |w1:t−1 , f1:n ) = sof tmax(W T hdt ), W T is the projection matrix, and wt and hdt are the generated word and the RNN decoder hidden state at time step t, computed using the standard RNN recursion and attention-based context vector ct . Details of the attention model are in the supplementary (due to space constraints). Reinforcement Learning (Policy Gradient) In order to directly optimize the sentence-level test metrics (as opposed to the cross-entropy loss a"
D17-1103,D16-1053,0,0.0269652,"sing a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.1 Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup. Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt¨aschel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation. ∗ log p(wt∗ |w1:t−1 , f1:n ) (1) where p(wt |w1:t−1 , f1:n ) = sof tmax(W T hdt ), W T is the projection matrix, and wt and hdt are the generated word and the RNN decoder hidden state at time step t, computed using the standard RNN recursion and attention-based context vector ct . Details of the attention model are in the supplementary (due to space cons"
D17-1103,P14-2074,0,0.0410379,"tion of the end-of-sequence token results in a reward r to the agent. Our training objective is to minimize the negative expected reward function: L(θ) = −Ews ∼pθ [r(ws )] (2) where ws is the word sequence sampled from the model. Based on the REINFORCE algorithm (Williams, 1992), the gradients of this nondifferentiable, reward-based loss function are: ∇θ L(θ) = −Ews ∼pθ [r(ws ) · ∇θ log pθ (ws )] (3) 1 Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014). We follow Ranzato et al. (2016) approximating the above gradients via a single sampled word 980 Ground-truth caption a man is spreading some butter in a pan a panda is eating some bamboo a monkey pulls a dogs tail a man is cutting the meat the dog is jumping in the snow a man and a woman is swimming in the pool Generated (sampled) caption puppies is melting butter on the pan a panda is eating some fried a monkey pulls a woman a man is cutting meat into potato a dog is jumping in cucumbers a man and a whale are swimming in a pool CIDEr 140.5 256.8 116.4 114.3 126.2 192.5 Ent 0.07 0.14 0.04 0."
D17-1103,S14-2131,0,0.0380765,"Missing"
D17-1103,D16-1204,0,0.0264109,"de input frame level video features {f1:n } via a bi-directional LSTM-RNN and then generate the caption w1:m using an LSTM-RNN with an attention mechanism. Let θ be the model ∗ parameters and w1:m be the ground-truth caption, then the cross entropy loss function is: Related Work L(θ) = − m X t=1 Past work has presented several sequence-tosequence models for video captioning, using attention, hierarchical RNNs, 3D-CNN video features, joint embedding spaces, language fusion, etc., but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016). Policy gradient for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.1 Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup. Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently"
D17-1103,S14-2055,0,0.0072339,"(Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016). Policy gradient for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.1 Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup. Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt¨aschel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation. ∗ log p(wt∗ |w1:t−1 , f1:n ) (1) where p(wt |w1:t−1 , f1:n ) = sof tmax(W T hdt ), W T is the projection matrix, and wt and hdt are the generated word and the RNN decoder hidden stat"
D17-1103,D16-1230,0,0.190281,"aption, then the cross entropy loss function is: Related Work L(θ) = − m X t=1 Past work has presented several sequence-tosequence models for video captioning, using attention, hierarchical RNNs, 3D-CNN video features, joint embedding spaces, language fusion, etc., but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016). Policy gradient for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.1 Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup. Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt¨aschel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh e"
D17-1103,N15-1173,0,0.424925,"d truth. We introduce CIDEnt, which penalizes the phrase-matching metric (CIDEr) based reward, when the entailment score is low. This ensures that a generated caption gets a high reIntroduction The task of video captioning (Fig. 1) is an important next step to image captioning, with additional modeling of temporal knowledge and action sequences, and has several applications in online content search, assisting the visuallyimpaired, etc. Advancements in neural sequenceto-sequence learning has shown promising improvements on this task, based on encoderdecoder, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a). However, these models are still trained using a wordlevel cross-entropy loss, which does not correlate well with the sentence-level metrics that the task is finally evaluated on (e.g., CIDEr, BLEU). Moreover, these models suffer from exposure bias (Ran979 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 979–985 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ... XENT Ent LSTM LSTM LSTM ... LSTM LSTM ... CIDEr ... CIDEnt Reward RL Figure 2: Reinforced (mixed-loss) video captioning usi"
D17-1103,1983.tc-1.13,0,0.412574,"Missing"
D17-1103,P02-1040,0,\N,Missing
D17-1103,W14-3348,0,\N,Missing
D17-1103,W04-1013,0,\N,Missing
D18-1012,P93-1008,0,0.0696984,"Missing"
D18-1012,W17-4402,0,0.0298614,"game on twitch.tv with concurrent user chat. dialogue models need to generate the next response in the sequence of chats, conditioned both on the raw video features as well as the previous textual chat history. Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW) (Janin et al., 2003; Waibel et al., 2001; Schmidt and Bannon, 1992). In the live video stream direction, Fu et al. (2017) and Ping and Chen (2017) used real-time comments to predict the frame highlights in a video, and Barbieri et al. (2017) presented emotes and troll prediction. 3 3.1 Twitch-FIFA Dataset Dataset Collection and Processing For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA18) games along with the users’ live chat conversations about the game. This dataset has videos involving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towards multimodal video-based dialogue data. An example is shown in Fig. 1 (and an original screenshot example in Fig. 2), w"
D18-1012,D14-1223,0,0.0261911,"Foster et al., 2008). Additionally, dialogue systems for digital personal assistants are also well explored (Myers et al., 2007; Sarikaya et al., 2016; Damacharla et al., 2018). In the visual modality direction, some important recent attempts have been made to use static image based context in dialogue systems (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017), who proposed the ‘visual dialog’ task, where the human can ask questions on a static image, and an agent interacts by answering these questions based on the previous chat context and the image’s visual features. Also, Celikyilmaz et al. (2014) used visual display information for on-screen item resolution in utterances for improving personal digital assistants. In contrast, we propose to employ dynamic video-based information as visual context knowledge in dialogue models, so as to move towards video-grounded intelligent assistant applications. In the video+language direction, previous work has looked at video captioning (Venugopalan et al., 2015) as well as Q&A and fill-inthe-blank tasks on videos (Tapaswi et al., 2016; Jang et al., 2017; Maharaj et al., 2017) and interactive 3D environments (Das et al., 2018; Yan et al., 2018; Gor"
D18-1012,D17-1102,1,0.737214,"ng, we also discourage frequent responses (top-20 most-frequent Figure 2: Sample page of live broadcast of FIFA-18 game on twitch.tv with concurrent user chat. dialogue models need to generate the next response in the sequence of chats, conditioned both on the raw video features as well as the previous textual chat history. Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW) (Janin et al., 2003; Waibel et al., 2001; Schmidt and Bannon, 1992). In the live video stream direction, Fu et al. (2017) and Ping and Chen (2017) used real-time comments to predict the frame highlights in a video, and Barbieri et al. (2017) presented emotes and troll prediction. 3 3.1 Twitch-FIFA Dataset Dataset Collection and Processing For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA18) games along with the users’ live chat conversations about the game. This dataset has videos involving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towar"
D18-1012,C04-1110,0,0.0498975,"deo and textual chat context encoders, which then decodes the response. We evaluate these models via retrieval ranking-recall, phrasematching metrics, as well as human evaluation studies. We also present dataset analysis as well as model ablations and attention visualizations to understand the contribution of the video vs. chat modalities and the model components. 2 In addition to the focus on textual dialogue context, using multimodal context brings more potential for having real-world grounded conversations. For example, spoken dialogue systems have been widely explored (Singh et al., 2000; Gurevych and Strube, 2004; Georgila et al., 2006; Eckert et al., 1997; Young, 2000; Janin et al., 2003; De Mori, 2007; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016; Hori et al., 2016; Celikyilmaz et al., 2015, 2017), as well as gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). Additionally, dialogue systems for digital personal assistants are also well explored (Myers et al., 2007; Sarikaya et al., 2016; Damacharla et al., 2018). In the visual modality direction, some important recent attempts have been made to use static image based context in dialogue systems"
D18-1012,I17-1047,0,0.118949,"rbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is limIntroduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includWe release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue 125 Pro"
D18-1012,P02-1048,0,0.485288,"; Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b). Current dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is limIntroduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades (W"
D18-1012,P17-1163,0,0.0408936,"Missing"
D18-1012,P16-1094,0,0.127404,"drunk bet? S5: @S11 thanks mate S6: could have passed one more S7: Pass that S1: record now! S8: !record S9: done a nother pass there Figure 1: Sample example from our many-speaker, video-context dialogue dataset, based on live soccer game chat. The task is to predict the response (bottomright) using the video context (left) and the chat context (top-right). ing recent work on introduction of large textualdialogue datasets (e.g., Lowe et al. (2015); Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b). Current dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). In order to address the additional advantage of using visually-grounded context knowledge in dialogue,"
D18-1012,W04-1013,0,0.0349749,"Missing"
D18-1012,W17-4501,0,0.0315439,"e frequent responses (top-20 most-frequent Figure 2: Sample page of live broadcast of FIFA-18 game on twitch.tv with concurrent user chat. dialogue models need to generate the next response in the sequence of chats, conditioned both on the raw video features as well as the previous textual chat history. Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW) (Janin et al., 2003; Waibel et al., 2001; Schmidt and Bannon, 1992). In the live video stream direction, Fu et al. (2017) and Ping and Chen (2017) used real-time comments to predict the frame highlights in a video, and Barbieri et al. (2017) presented emotes and troll prediction. 3 3.1 Twitch-FIFA Dataset Dataset Collection and Processing For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA18) games along with the users’ live chat conversations about the game. This dataset has videos involving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towards multimodal video-based"
D18-1012,H01-1055,0,0.16351,"In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is limIntroduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includWe release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue 125 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 125–136 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics generation units have been extended with seq2seq neural network models (Vinyals and Le, 2015; Serban et al., 2016; Luan et al., 2016). ited to one static image. Moreover, the interactions are between t"
D18-1012,D16-1230,0,0.0831148,"so as to move towards video-grounded intelligent assistant applications. In the video+language direction, previous work has looked at video captioning (Venugopalan et al., 2015) as well as Q&A and fill-inthe-blank tasks on videos (Tapaswi et al., 2016; Jang et al., 2017; Maharaj et al., 2017) and interactive 3D environments (Das et al., 2018; Yan et al., 2018; Gordon et al., 2017; Anderson et al., 2017). There has also been early related work on generating sportscast commentaries from simulation (RoboCup) soccer videos represented as non-visual state information (Chen and Mooney, 2008). Also, Liu et al. (2016a) presented some initial ideas on robots learning grounded task representations by watching and interacting with humans performing the task (i.e., by converting human demonstration videos to Causal And-Or graphs). On the other hand, we propose a new video-chat dataset where the Related Work Early dialogue systems had components of natural language (NL) understanding unit, dialogue manager, and NL generation unit (Bates, 1995). Statistical learning methods were used for automatic feature extraction (Dowding et al., 1993; Mikolov et al., 2013), dialogue managers incorporated reward-driven reinf"
D18-1012,2005.sigdial-1.11,0,0.142081,"Missing"
D18-1012,W15-4640,0,0.515566,"del ablations, and visualizations to understand the contribution of different modalities and model components. 1 S3: suprised you didn&apos;t do the extra pass S4: @S10 a drunk bet? S5: @S11 thanks mate S6: could have passed one more S7: Pass that S1: record now! S8: !record S9: done a nother pass there Figure 1: Sample example from our many-speaker, video-context dialogue dataset, based on live soccer game chat. The task is to predict the response (bottomright) using the video context (left) and the chat context (top-right). ing recent work on introduction of large textualdialogue datasets (e.g., Lowe et al. (2015); Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b). Current dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016), as well as work on gesture and haptics based dialogue ("
D18-1012,P08-1073,0,0.02104,"rounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is limIntroduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includWe release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue 125 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 125–136 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics generation units have been extended with seq2seq neural network models (Vinyals and Le, 2015; Serban et al., 2016; Luan et al., 2016). ited to one static image. Moreover, the interactions are between two speakers with fixed roles (one asks questions and the other answe"
D18-1012,D15-1166,0,0.131191,"Missing"
D18-1012,D11-1054,0,0.039585,"e in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is limIntroduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includWe release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue 125 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 125–136 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics generation units have been extended with seq2seq neural network models (Vinyals and Le, 2015; Serban et al., 2016; Luan et al., 2016). ited to one static image. Moreover, the interactions are between two speakers with fixed roles (one asks questions and the other answers). Several situation"
D18-1012,D15-1199,0,0.0750838,"t context (top-right). ing recent work on introduction of large textualdialogue datasets (e.g., Lowe et al. (2015); Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b). Current dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is limIntroduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of art"
D18-1012,N15-1020,0,0.0609853,"Missing"
D18-1012,P16-1230,0,0.166616,"didn&apos;t do the extra pass S4: @S10 a drunk bet? S5: @S11 thanks mate S6: could have passed one more S7: Pass that S1: record now! S8: !record S9: done a nother pass there Figure 1: Sample example from our many-speaker, video-context dialogue dataset, based on live soccer game chat. The task is to predict the response (bottomright) using the video context (left) and the chat context (top-right). ing recent work on introduction of large textualdialogue datasets (e.g., Lowe et al. (2015); Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b). Current dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkˇsi´c et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). In order to address the additional advantage of using visually-grou"
D18-1167,D14-1162,0,0.0805892,"on cross-validation, we find K=6 to perform best. labels as input to an image captioning system gave comparable performance to using CNN features directly. Inspired by this work, we also experiment with using detected labels as visual inputs. As shown in Fig. 5, we are able to detect rich visual concepts, including both objects and attributes, e.g. ”white basket”, which could be used to answer “What is Sheldon holding in his hand when everyone is at the door”. We first gather detected concepts over all the frames to represent concept presence. After removing duplicate concepts, we use GloVe (Pennington et al., 2014) to embed the words. The resulting video representation is denoted as V cpt ∈ Rncpt ×300 , where ncpt is the number of unique concepts. ImageNet Features: We extract the pooled 2048D feature of the last block of ResNet101. Features from the same video clip are L2 normalized and stacked, denoted as V img ∈ Rnimg ×2048 , where nimg is the number of frames extracted from the video clip. 4.2 LSTM Encoders for Video and Text We use a bi-directional LSTM (BiLSTM) to encode both textual and visual sequences. A subtitle S, which contains a set of sentences, is flattened into a long sequence of words a"
D18-1167,D13-1020,0,0.0787188,"al and language understanding are required to answer a large portion of questions, either due to unrealistic video sources (PororoQA, MarioQA) or data collection strategy being more focused on either visual (MovieFIB, VideoQA, TGIF-QA) or language (MovieQA) sources. In comparison, our TVQA collection strategy takes a directly multimodal approach to construct a large-scale, realvideo dataset by letting humans ask and answer questions while watching TV-show videos with associated dialogues. Text Question Answering: The related task of text-based question answering has been extensively explored (Richardson et al., 2013; Weston et al., 2015; Rajpurkar et al., 2016; Hermann et al., 2015; Hill et al., 2015). Richardson et al. (2013) collected MCTest, a multiple choice QA dataset intended for open-domain reading comprehension. 1370 With the same goal in mind, Rajpurkar et al. (2016) introduced the SQuAD dataset, but their answers are specific spans from long passages. Weston et al. (2015) designed a set of tasks with automatically generated QAs to evaluate the textual reasoning ability of artificial agents and Hermann et al. (2015); Hill et al. (2015) constructed the cloze dataset on top of an existing corpus."
D18-1167,D17-1017,0,0.0707448,"Missing"
D18-1167,D16-1264,0,\N,Missing
D18-1303,D16-1216,1,0.859591,"ssibility of using descriptors of offenders as a classification tool. Figure 4 (right) is an incorrectly classified example. We see that the word “body”, followed by “language”, had the most influence on the classification of this exam2808 ple as “commenting”. Our model identifies synonyms and hyponyms like the word “language” in relation to the category of commenting. However, the true label was “non-commenting”, as the word was not used in a context of sexual language, but rather as “vague language” and “body language”. 6.4 Activation Clustering Activation clustering (Girshick et al., 2014; Aubakirova and Bansal, 2016) accesses the activation values of all n neurons and treats the activation values per input as coordinates in ndimensional space. K-means clustering was performed to group activation clusters and find common themes in these reports. Activation clustering is distinct from both LIME analysis and first derivative saliency in that it finds patterns and clusterings at a description-level. Circumstances of Harassment: One of the clusters was classified as “ogling”: {‘a group of boys was standing near us and were making weird expressions and as we moved away they started following’; ‘a group of guys"
D18-1303,N16-1082,0,0.0351243,"ance in the classification, suggesting that this may be a frequent location in which groping takes place. In Figure 3 (middle), the words with the most importance are “comments” and “staring”, indicating that ogling may coincide with commenting very frequently. In Figure 3 (right), the words “ogling”, “sexual”, and “commenting” had the most importance, which further supports the notion that ogling and commenting often occur together. As verified by the data, ogling and commenting together is more common than ogling alone. 6.3 First Derivative Saliency Saliency heatmaps (Simonyan et al., 2014; Li et al., 2016) illustrate which words of an input have the biggest impact on the final classification by taking the gradient of the final scores outputted by the neural network (S) with respect to the embedding (E), given the true label (L), giving ∂SL (E) ∂E . While LIME analysis and first derivative saliency are both used to find word-level contributions, first derivative saliency is model-dependent and gives reasoning behind classification based on the whole model, in contrast to the locally-faithful, model-agnostic LIME analysis technique. In Figure 4 (left), the word “commenting” and the words “one boy"
D18-1303,N16-3020,0,0.0358563,"ing clustering. 6.1 Word Embedding Visualization We selected seed words that corresponded to class labels and found the nearest neighbors of each seed word’s vector by reducing the dimensionality of the word embeddings using t-SNE (see Table 3) (Maaten and Hinton, 2008). This form of visualization not only ensures that our model has learned appropriate word embeddings, but also demonstrates that each form of sexual harassment has a unique and distinct context. Furthermore, this shows that our model learns related words and concepts for each type of harassment. 6.2 LIME Analysis LIME analysis (Ribeiro et al., 2016), or Local Interpretable Model-Agnostic Explanation, interprets the local reasoning of a model around an instance. Results of LIME (ξ) are found by taking the minimum of L, which is the measure of how unfaithful the interpretable model (g) is to approximating the probability that an input (x) belongs to a certain class (f ) in the locally defined area (πx ) summed with complexity measures Ω, giving ξ(x) = argmin L(f, g, πx ) + Ω(g). In Figure 3 (left), the words “touch”, “man”, and the collective words “indecently till pushed away” are the most important to the local classification of “groping"
D18-1303,D15-1309,0,0.355324,"ich usually requires the victim to detail each form of sexual harassment that took place. The act of partially filling out the report 2805 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2805–2811 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Analyzing personal sexual harassment stories from online social forums is fairly unexplored, to the best of our knowledge. However, recent works in a similar vein include detecting the presence of domestic abuse stories on social media sites (Schrading et al., 2015a; Schrading, 2015; Schrading et al., 2015b). In more distantly related work, NLP has been used for various sociallydriven tasks, such as detecting the presence of cyberbullying or incivility (Ziegele et al., 2018; Founta et al., 2018; Chen et al., 2012; Zhao et al., 2016; Agrawal and Awekar, 2018; Van Hee et al., 2018), and detecting and providing aid for signs of depression or suicidal thoughts (Pestian et al., 2010; Yazdavar et al., 2017; Stepanov et al., 2017; Fitzpatrick et al., 2017). Output Thresholding P (Commenting) P (Staring) (by our classifier) in itself makes it more likely for th"
D18-1303,N15-1139,0,0.0266539,"ich usually requires the victim to detail each form of sexual harassment that took place. The act of partially filling out the report 2805 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2805–2811 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Analyzing personal sexual harassment stories from online social forums is fairly unexplored, to the best of our knowledge. However, recent works in a similar vein include detecting the presence of domestic abuse stories on social media sites (Schrading et al., 2015a; Schrading, 2015; Schrading et al., 2015b). In more distantly related work, NLP has been used for various sociallydriven tasks, such as detecting the presence of cyberbullying or incivility (Ziegele et al., 2018; Founta et al., 2018; Chen et al., 2012; Zhao et al., 2016; Agrawal and Awekar, 2018; Van Hee et al., 2018), and detecting and providing aid for signs of depression or suicidal thoughts (Pestian et al., 2010; Yazdavar et al., 2017; Stepanov et al., 2017; Fitzpatrick et al., 2017). Output Thresholding P (Commenting) P (Staring) (by our classifier) in itself makes it more likely for th"
D18-1433,P16-1014,0,0.0345829,"objects into captions (Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel Knowledgeaware Video Description"
D18-1433,W04-3250,0,0.0136134,"se and diverse descriptions, though it negatively affects the entity incorporation performance. The video alone is insufficient to generate the correct entities (Table 2). In Figure 5a, the VD baseline generates the correct event, but generates the incorrect location “Kabul”. We observe that when the visual evidence is ambiguous, this model may fail to generate the correct events and entities. For example, if a video depicts the destruction of buildings after a hurricane, then the VD baseline 7 8 This criterion is used for computing precision and recall. Found via paired bootstrap resampling (Koehn, 2004). may mistakenly describe the video as an explosion since the visual evidence is similar. The article-only baseline tends to mention the correct entities as shown in Figure 5a, where the description is generally on topic but provides some irrelevant information. Indeed, this model can generate descriptions unrelated to the video itself. In Figure 5b, the article-only baseline’s description contains some correct entities (e.g., “Colombia”), but is not focused on the announcement depicted in the video. As See et al. (2017) discuss, this model can be more extractive than abstractive, copying many"
D18-1433,P13-1008,1,0.940267,"events that appear in the video’s description, but these may not be specific to the video content. For example, in Figure 2b, the video discusses the “heightened security” and does not depict the arrest directly. Topically related news documents capture background knowledge about the attack that led to the “heightened security” as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures. Thus, we propose to retrieve topically related news documents from which we seek to extract named entities (Pan et al., 2017) and events (Li et al., 2013) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted entities into the description, and through a new knowledge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchical attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these generation components the Know"
D18-1433,W04-1013,0,0.0415132,"Missing"
D18-1433,D16-1031,0,0.0534703,"ns by encoding information from the preceding and subsequent frames (Yao et al., 2015). We use a LSTM decoder, which applies a temporal attention (Bahdanau et al., 2015) to the frame representations at each step. To generate each word, the decoder computes its hidden state, adjusts this hidden state with the knowledge gate output at the current time step, and determines the most probable word by utilizing the entity pointer network to decide whether to generate a named entity or vocabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences (Miao and Blunsom, 2016; See et al., 2017). In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity. Encoder. The input to the encoder is a sequence of video frames, {F1 , ..., FN }. First, we extract frame-level features by applying a Convolutional Neural Network (CNN) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Ioffe and Szegedy, 2015;"
D18-1433,K16-1028,0,0.0223229,"(Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel Knowledgeaware Video Description network, which can utili"
D18-1433,N04-1019,0,0.0681349,"beddings and compute entity embeddings. For visual features, we use the Conv3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the"
D18-1433,P11-1020,0,0.281605,"Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model’s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions. 1 a) Description (Chen and Dolan, 2011): A man is talking. b) Human Description: Senior army ofﬁcer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. Figure 1: Comparison of machine (a) and human (b) generated descriptions.1 Introduction Video captioning is a challenging task that seeks to automatically generate a natural language description of the content of a video. Many video captioning efforts focus on learning video representations that model the spatial and temporal dynamics of the videos"
D18-1433,D17-1238,0,0.0476198,"Missing"
D18-1433,P17-1178,1,0.924882,"contain the named entities or events that appear in the video’s description, but these may not be specific to the video content. For example, in Figure 2b, the video discusses the “heightened security” and does not depict the arrest directly. Topically related news documents capture background knowledge about the attack that led to the “heightened security” as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures. Thus, we propose to retrieve topically related news documents from which we seek to extract named entities (Pan et al., 2017) and events (Li et al., 2013) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted entities into the description, and through a new knowledge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchical attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these g"
D18-1433,W14-3348,0,0.0367102,"Missing"
D18-1433,P16-1154,0,0.0754805,"., 2017), incorporate novel objects into captions (Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel K"
D18-1433,P17-1117,1,0.841562,"em mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et al., 2015; Yu et al., 2017). We move towards using datasets with captions that have specific knowledge rather than generic 3998 Model Article-only VD VD+Entity Pointer VD+Knowledge Gate Entity Pointer+Knowledge Gate KaVD METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1 8.6 9.1 9.7 9.8 10."
D18-1433,D17-1103,1,0.825738,"em mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et al., 2015; Yu et al., 2017). We move towards using datasets with captions that have specific knowledge rather than generic 3998 Model Article-only VD VD+Entity Pointer VD+Knowledge Gate Entity Pointer+Knowledge Gate KaVD METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1 8.6 9.1 9.7 9.8 10."
D18-1433,N18-2102,1,0.823313,"v3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the generated description compared to"
D18-1433,N15-1173,0,0.0413973,"te with this metric. We observe discrepancies between the manual and automatic event metrics, in part, due to errors in the automated extraction and the addition of more test points. For example, in the generated sentence, “Hundreds of people are to take to the streets of...”, the event extraction system mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et"
D18-1433,D15-1199,0,0.025523,"Missing"
D18-1433,D17-1239,0,0.0255935,"atures, we use the Conv3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the genera"
D18-1433,P17-1099,0,0.383734,"on from the preceding and subsequent frames (Yao et al., 2015). We use a LSTM decoder, which applies a temporal attention (Bahdanau et al., 2015) to the frame representations at each step. To generate each word, the decoder computes its hidden state, adjusts this hidden state with the knowledge gate output at the current time step, and determines the most probable word by utilizing the entity pointer network to decide whether to generate a named entity or vocabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences (Miao and Blunsom, 2016; See et al., 2017). In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity. Encoder. The input to the encoder is a sequence of video frames, {F1 , ..., FN }. First, we extract frame-level features by applying a Convolutional Neural Network (CNN) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Ioffe and Szegedy, 2015; Szegedy et al., 201"
D18-1433,K17-1044,0,0.0998129,"ding, xt−1 . The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that contain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism (Tu et al., 2016). For example, given the retrieved event types in Figure 3, the knowledge gate encourages the decoder to generate the event trigger “coup” due to the presence of the “Attack” event type. Inspired by the gating mechanisms from natural language generation (Wen et al., 2015; Tran and Nguyen, 2017), the knowledge gate, gt , is given by gt = σ (Wg,v [xt−1 , vt ] + Wg,sˆst ) (7) kt = gt kt−1 (8) where all W are learned parameters and [xt−1 , vt ] is the concatenation of these two vectors. This gating step determines the amount of the entity and event type features contained in kt−1 to carry to the next step. With the updated kt , we compute the decoder hidden state, st , as st = ˆst + (ot tanh (Ws,k kt )) (9) where ot is the output gate of the LSTM and Ws,k is a learned parameter. Our next step is to generate the next word. The model needs to produce named entities (e.g., “S. B. Moyo” and"
D18-1433,N16-1174,0,0.0505958,"0 corresponds to an event or entity type (e.g., “Arrest-Jail” event type or “President” entity type), so the j th element, k (j) , is 1 if the entity or event type is found in the related documents and 0 otherwise. k0 serves as the initial knowledge gate vector of the decoder (Section 2.2). The entity embeddings give the model access to semantic representations of the entities, while the knowledge gate vector aids the generation process by providing the model with the event and entity types. 2.2 KaVD Network Our model learns video representations using hierarchical, or multi-level, attention (Yang et al., 2016; Qin et al., 2017). The encoder is comprised of a spatial attention (Xu et al., 2015) and bidirectional Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) temporal encoder. The spatial attention allows the model to attend to different locations of each frame (Figure 4), yielding frame representations 3994 S. B. Moyo g Output Distribution gin p genP v + (1 − p gen)P e st p gen vt ct Temporal Attention gin vt S. Mu ga be st Vocab Distribution B. M Zim oyo ba bw e as su r froes m Spatial Attention g hN sta h1 Entity Distribution Knowledge Gate Vector S. B. Moyo sta General"
D18-1433,P16-1008,0,0.0352559,"where st−1 is the previous decoder hidden state and atime is another scoring function. This yields a single, spatio-temporally attentive video representation, vt . We then compute an intermediate hidden state, ˆst , by applying the decoder LSTM 3995 to st−1 , vt , and previous word embedding, xt−1 . The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that contain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism (Tu et al., 2016). For example, given the retrieved event types in Figure 3, the knowledge gate encourages the decoder to generate the event trigger “coup” due to the presence of the “Attack” event type. Inspired by the gating mechanisms from natural language generation (Wen et al., 2015; Tran and Nguyen, 2017), the knowledge gate, gt , is given by gt = σ (Wg,v [xt−1 , vt ] + Wg,sˆst ) (7) kt = gt kt−1 (8) where all W are learned parameters and [xt−1 , vt ] is the concatenation of these two vectors. This gating step determines the amount of the entity and event type features contained in kt−1 to carry to the n"
D18-1433,D16-1204,0,0.0955565,"b) Human Description: Senior army ofﬁcer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. Figure 1: Comparison of machine (a) and human (b) generated descriptions.1 Introduction Video captioning is a challenging task that seeks to automatically generate a natural language description of the content of a video. Many video captioning efforts focus on learning video representations that model the spatial and temporal dynamics of the videos (Yao et al., 2015; Venugopalan et al., 2016; Yu et al., 2017). Although the language generation component within this task is of great importance, less work has been done to enhance the contextual knowledge conveyed by the descriptions. The descriptions generated by previous methods tend to be “generic”, describing To address this problem, we collect a news video dataset, where each video is accompanied by meta-data (e.g., tags and date) and a natural language description of the content in, and/or context around, the video. We create an approach to this task that is motivated by two observations. First, the video content alone is insuf"
D18-1440,D14-1085,0,0.0256043,"monstrates our 2-decoder model’s enhanced ability to memorize and recover important information from the input document, which is our main contribution in this paper. 2 Related Work Extractive and Abstractive Summarization: Early models for automatic text summarization were usually extractive (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015). For abstractive summarization, different early non-neural approaches were applied, based on graphs (Giannakopoulos, 2009; Ganesan et al., 2010), discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and a combination of linguistic compression and topic detection (Zajic et al., 2004). Recent neuralnetwork models have tackled abstractive summarization using methods such as hierarchical encoders and attention, coverage, and distraction (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Takase et al., 2016) as well as various initial large-scale, shortlength summarization datasets like DUC-2004 and Gigaword. Nallapati et al. (2016) adapted the CNN/Daily Mail (Hermann et al., 2015) dataset for long-text summarization, and provided an abstr"
D18-1440,N16-1012,0,0.356766,"nhanced by attention mechanism (Bahdanau et al., 2015) and pointer network (Vinyals et al., 2015), such that the decoder can refer to (and weigh) all the encod4067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067–4077 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing steps’ hidden states or directly copy words from the source text, instead of relying solely on encoder’s final memory state for all information about the source passage. Recent studies (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Zeng et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; See et al., 2017) have demonstrated success with such seq-attention-seq and pointer models in summarization tasks. While the advantage of attention and pointer models compared to vanilla sequence-to-sequence models in summarization is well supported by previous studies, these models still struggle to find the most salient information in the source text when generating summaries. This is because summarization, being different from other textto-text generation tasks (where there is an almost one-to-one correspondence between input and"
D18-1440,W14-3348,0,0.0187569,"ntage over the closed-book decoder, similar to the situation of student B being able to refer back to the passage during the test for best performance (but is still trained hard to do well in both situations). Fig. 1 shows an example of our 2-decoder summarizer generating a summary that covers the original passage with more saliency than the baseline model. Empirically, we test our 2-decoder architecture on the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), and our model surpasses the strong pointer-generator baseline significantly on both ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) metrics, as well as based on human evaluation. This holds true both for a cross-entropy baseline as well as a stronger, policy-gradient based reinforcement learning setup (Williams, 1992). Moreover, our 2-decoder models (both cross-entropy and reinforced) also achieve reasonable improvements on a test-only generalizability/transfer setup on the DUC-2002 dataset. We further present a series of numeric and qualitative analysis to understand whether the improvements in these automatic metric scores are in fact due to the enhanced memory and saliency strengths of our encoder. First, by evaluating"
D18-1440,D15-1042,0,0.11592,"Missing"
D18-1440,C10-1039,0,0.0122721,"s, while maintaining similar length and better avoiding repetitions. This directly demonstrates our 2-decoder model’s enhanced ability to memorize and recover important information from the input document, which is our main contribution in this paper. 2 Related Work Extractive and Abstractive Summarization: Early models for automatic text summarization were usually extractive (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015). For abstractive summarization, different early non-neural approaches were applied, based on graphs (Giannakopoulos, 2009; Ganesan et al., 2010), discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and a combination of linguistic compression and topic detection (Zajic et al., 2004). Recent neuralnetwork models have tackled abstractive summarization using methods such as hierarchical encoders and attention, coverage, and distraction (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Takase et al., 2016) as well as various initial large-scale, shortlength summarization datasets like DUC-2004 and Gigaword. Nallapati et al. (2016) adapted the CNN/Daily M"
D18-1440,D14-1168,0,0.0204105,"better avoiding repetitions. This directly demonstrates our 2-decoder model’s enhanced ability to memorize and recover important information from the input document, which is our main contribution in this paper. 2 Related Work Extractive and Abstractive Summarization: Early models for automatic text summarization were usually extractive (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015). For abstractive summarization, different early non-neural approaches were applied, based on graphs (Giannakopoulos, 2009; Ganesan et al., 2010), discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and a combination of linguistic compression and topic detection (Zajic et al., 2004). Recent neuralnetwork models have tackled abstractive summarization using methods such as hierarchical encoders and attention, coverage, and distraction (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Takase et al., 2016) as well as various initial large-scale, shortlength summarization datasets like DUC-2004 and Gigaword. Nallapati et al. (2016) adapted the CNN/Daily Mail (Hermann et al., 2015) dataset for"
D18-1440,P16-1154,0,0.0831772,"et al., 2015) and pointer network (Vinyals et al., 2015), such that the decoder can refer to (and weigh) all the encod4067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067–4077 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing steps’ hidden states or directly copy words from the source text, instead of relying solely on encoder’s final memory state for all information about the source passage. Recent studies (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Zeng et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; See et al., 2017) have demonstrated success with such seq-attention-seq and pointer models in summarization tasks. While the advantage of attention and pointer models compared to vanilla sequence-to-sequence models in summarization is well supported by previous studies, these models still struggle to find the most salient information in the source text when generating summaries. This is because summarization, being different from other textto-text generation tasks (where there is an almost one-to-one correspondence between input and output words, e.g., machine transla"
D18-1440,P16-1014,0,0.116945,"pointer network (Vinyals et al., 2015), such that the decoder can refer to (and weigh) all the encod4067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067–4077 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing steps’ hidden states or directly copy words from the source text, instead of relying solely on encoder’s final memory state for all information about the source passage. Recent studies (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Zeng et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; See et al., 2017) have demonstrated success with such seq-attention-seq and pointer models in summarization tasks. While the advantage of attention and pointer models compared to vanilla sequence-to-sequence models in summarization is well supported by previous studies, these models still struggle to find the most salient information in the source text when generating summaries. This is because summarization, being different from other textto-text generation tasks (where there is an almost one-to-one correspondence between input and output words, e.g., machine translation), requires the sequ"
D18-1440,A00-2024,0,0.509407,"or that little girl , ' she added . Figure 1: Baseline model repeats itself twice (italic), and fails to find all salient information (highlighted in red in the original text) from the source text that is covered by our 2-decoder model. The summary generated by our 2-decoder model also recovers most of the information mentioned in the reference summary (highlighted in blue in the reference summary). Introduction Text summarization is the task of condensing a long passage to a shorter version that only covers the most salient information from the original text. Extractive summarization models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015) directly pick words, phrases, and sentences from the source text to form a summary, while an abstractive model generates (samples) words from a fixed-size vocabulary instead of copying from text directly. The last few years have seen significant progress on both extractive and abstractive approaches, of which a large number of studies are fueled by neural sequence-to-sequence models (Sutskever et al., 2014). One popular formulation of such models is an RNN/LSTM encoder that encodes the source passage to a fixed-size mem"
D18-1440,W04-1013,0,0.0503055,"due to its copying advantage over the closed-book decoder, similar to the situation of student B being able to refer back to the passage during the test for best performance (but is still trained hard to do well in both situations). Fig. 1 shows an example of our 2-decoder summarizer generating a summary that covers the original passage with more saliency than the baseline model. Empirically, we test our 2-decoder architecture on the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), and our model surpasses the strong pointer-generator baseline significantly on both ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) metrics, as well as based on human evaluation. This holds true both for a cross-entropy baseline as well as a stronger, policy-gradient based reinforcement learning setup (Williams, 1992). Moreover, our 2-decoder models (both cross-entropy and reinforced) also achieve reasonable improvements on a test-only generalizability/transfer setup on the DUC-2002 dataset. We further present a series of numeric and qualitative analysis to understand whether the improvements in these automatic metric scores are in fact due to the enhanced memory and saliency strengt"
D18-1440,D16-1031,0,0.0261837,"tractive baseline using attentional sequence-tosequence model. Pointer Network for Summarization: Pointer networks (Vinyals et al., 2015) are useful for summarization models because summaries often need to copy/contain a large number of words that have appeared in the source text. This provides the advantages of both extractive and abstractive approaches, and usually includes a gating function to model the distribution for the extended vocabulary including the pre-set vocabulary and words from the source text (Zeng et al., 2016; Nallapati et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; Miao and Blunsom, 2016). See et al. (2017) used a soft gate to control model’s behavior of copying versus generating. They further applied coverage mechanism and achieved the state-of-the-art results on CNN/Daily Mail dataset. Memory Enhancement: Some recent works (Wang et al., 2016; Xiong et al., 2018; Gu et al., 2016a) have studied enhancing the memory capacity of sequence-to-sequence models. They studied this problem in Neural Machine Translation by keeping an external memory state analogous to data in the Von Neumann architecture, while the instructions are represented by the sequenceto-sequence model. Our work"
D18-1440,K16-1028,0,0.136683,"Missing"
D18-1440,N18-2102,1,0.86121,"Missing"
D18-1440,D16-1264,0,0.116511,"Missing"
D18-1440,D15-1044,0,0.404671,"from this memory state. This paradigm is enhanced by attention mechanism (Bahdanau et al., 2015) and pointer network (Vinyals et al., 2015), such that the decoder can refer to (and weigh) all the encod4067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067–4077 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing steps’ hidden states or directly copy words from the source text, instead of relying solely on encoder’s final memory state for all information about the source passage. Recent studies (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Zeng et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; See et al., 2017) have demonstrated success with such seq-attention-seq and pointer models in summarization tasks. While the advantage of attention and pointer models compared to vanilla sequence-to-sequence models in summarization is well supported by previous studies, these models still struggle to find the most salient information in the source text when generating summaries. This is because summarization, being different from other textto-text generation tasks (where there is an almos"
D18-1440,P17-1099,0,0.173403,"ls et al., 2015), such that the decoder can refer to (and weigh) all the encod4067 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4067–4077 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing steps’ hidden states or directly copy words from the source text, instead of relying solely on encoder’s final memory state for all information about the source passage. Recent studies (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Zeng et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; See et al., 2017) have demonstrated success with such seq-attention-seq and pointer models in summarization tasks. While the advantage of attention and pointer models compared to vanilla sequence-to-sequence models in summarization is well supported by previous studies, these models still struggle to find the most salient information in the source text when generating summaries. This is because summarization, being different from other textto-text generation tasks (where there is an almost one-to-one correspondence between input and output words, e.g., machine translation), requires the sequence-attention-sequ"
D18-1440,D16-1112,0,0.0298178,"8; Filippova et al., 2015). For abstractive summarization, different early non-neural approaches were applied, based on graphs (Giannakopoulos, 2009; Ganesan et al., 2010), discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and a combination of linguistic compression and topic detection (Zajic et al., 2004). Recent neuralnetwork models have tackled abstractive summarization using methods such as hierarchical encoders and attention, coverage, and distraction (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Takase et al., 2016) as well as various initial large-scale, shortlength summarization datasets like DUC-2004 and Gigaword. Nallapati et al. (2016) adapted the CNN/Daily Mail (Hermann et al., 2015) dataset for long-text summarization, and provided an abstractive baseline using attentional sequence-tosequence model. Pointer Network for Summarization: Pointer networks (Vinyals et al., 2015) are useful for summarization models because summaries often need to copy/contain a large number of words that have appeared in the source text. This provides the advantages of both extractive and abstractive approaches, and usua"
D18-1440,P13-1136,0,0.0536857,"r model’s enhanced ability to memorize and recover important information from the input document, which is our main contribution in this paper. 2 Related Work Extractive and Abstractive Summarization: Early models for automatic text summarization were usually extractive (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015). For abstractive summarization, different early non-neural approaches were applied, based on graphs (Giannakopoulos, 2009; Ganesan et al., 2010), discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and a combination of linguistic compression and topic detection (Zajic et al., 2004). Recent neuralnetwork models have tackled abstractive summarization using methods such as hierarchical encoders and attention, coverage, and distraction (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Takase et al., 2016) as well as various initial large-scale, shortlength summarization datasets like DUC-2004 and Gigaword. Nallapati et al. (2016) adapted the CNN/Daily Mail (Hermann et al., 2015) dataset for long-text summarization, and provided an abstractive baseline usin"
D18-1440,D16-1027,0,0.0204708,"he source text. This provides the advantages of both extractive and abstractive approaches, and usually includes a gating function to model the distribution for the extended vocabulary including the pre-set vocabulary and words from the source text (Zeng et al., 2016; Nallapati et al., 2016; Gu et al., 2016b; Gulcehre et al., 2016; Miao and Blunsom, 2016). See et al. (2017) used a soft gate to control model’s behavior of copying versus generating. They further applied coverage mechanism and achieved the state-of-the-art results on CNN/Daily Mail dataset. Memory Enhancement: Some recent works (Wang et al., 2016; Xiong et al., 2018; Gu et al., 2016a) have studied enhancing the memory capacity of sequence-to-sequence models. They studied this problem in Neural Machine Translation by keeping an external memory state analogous to data in the Von Neumann architecture, while the instructions are represented by the sequenceto-sequence model. Our work is novel in that we aim to improve the internal long-term memory of the encoder LSTM by adding a closed-book decoder that has no attention layer, yielding a more efficient internal memory that encodes only important information from the source text, which is c"
D18-1454,W05-0909,0,0.108953,"y incorporate commonsense at every step of inference. 4 Experimental Setup Datasets: We report results on two multi-hop reasoning datasets: generative NarrativeQA (Koˇcisk`y et al., 2018) (summary subtask) and extractive QAngaroo WikiHop (Welbl et al., 2018). For multiple-choice WikiHop, we rank candidate responses by their generation probability. Similar to previous works (Dhingra et al., 2018), we use the non-oracle, unmasked and not-validated dataset. Evaluation Metrics: We evaluate NarrativeQA on the metrics proposed by its original authors: Bleu-1, Bleu-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and RougeL (Lin, 2004). We also evaluate on CIDEr (Vedantam et al., 2015) which emphasizes annotator consensus. For WikiHop, we evaluate on accuracy.5 More dataset, metric, and all other training details are in the supplementary. 5 Due to the 2-week evaluation wait-time on the nonpublic test set, we instead train our model on a sub-section of the training set, pick hyperparameters based on a small 4226 Model BLEU-1 BLEU-4 METEOR Rouge-L CIDEr Seq2Seq (Koˇcisk`y et al., 2018) ASR (Koˇcisk`y et al., 2018) BiDAF† (Koˇcisk`y et al., 2018) BiAttn + MRU-LSTM† (Tay et al., 2018) 15.89 23.20 33.72 36"
D18-1454,P17-1168,0,0.0553249,"Missing"
D18-1454,C16-1236,0,0.036252,"ommonsense knowledge as relation triples or features from external databases. Recently, largescale graphical commonsense databases such as ConceptNet (Speer and Havasi, 2012) use graphical structure to express intricate relations between concepts, but effective goal-oriented graph traversal has not been extensively used in previous commonsense incorporation efforts. Knowledgebase QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs (Bollacker et al., 2008). Knowledge path extraction has been shown to be effective at the task (Bordes et al., 2014; Bao et al., 2016). We apply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet (Speer and Havasi, 2012). Incorporation of External Knowledge: There have been several attempts at using external knowledge to boost model performance on a variety of tasks: Chen et al. (2018) showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; Xu et al. (2017) used a gated recall-LSTM mechanism to incorporate commonsense information into to4221 ken representations in dia"
D18-1454,D14-1067,0,0.0326324,"hese approaches add commonsense knowledge as relation triples or features from external databases. Recently, largescale graphical commonsense databases such as ConceptNet (Speer and Havasi, 2012) use graphical structure to express intricate relations between concepts, but effective goal-oriented graph traversal has not been extensively used in previous commonsense incorporation efforts. Knowledgebase QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs (Bollacker et al., 2008). Knowledge path extraction has been shown to be effective at the task (Bordes et al., 2014; Bao et al., 2016). We apply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet (Speer and Havasi, 2012). Incorporation of External Knowledge: There have been several attempts at using external knowledge to boost model performance on a variety of tasks: Chen et al. (2018) showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; Xu et al. (2017) used a gated recall-LSTM mechanism to incorporate commonsense information into to4221 ken rep"
D18-1454,P18-1224,0,0.0377915,"fforts. Knowledgebase QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs (Bollacker et al., 2008). Knowledge path extraction has been shown to be effective at the task (Bordes et al., 2014; Bao et al., 2016). We apply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet (Speer and Havasi, 2012). Incorporation of External Knowledge: There have been several attempts at using external knowledge to boost model performance on a variety of tasks: Chen et al. (2018) showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; Xu et al. (2017) used a gated recall-LSTM mechanism to incorporate commonsense information into to4221 ken representations in dialogue. In MRC, Weissenborn et al. (2017) integrated external background knowledge into an NLU model by using contextually-refined word embeddings which integrated information from ConceptNet (single-hop relations mapped to unstructured text) via a single layer bidirectional LSTM. Concurrently to our work, Mihaylov and Frank (2018) showed improvements on a cloz"
D18-1454,D16-1053,0,0.0886787,"Missing"
D18-1454,P18-1078,0,0.0335287,"pying from) the context. At decoding step t, the decoder receives the input xt (embedded representation of last timestep’s output), the last time step’s hidden state st−1 and context vector at−1 . The decoder computes the current hidden state st as: where ; is concatenation, ct is the cell’s output. The initial input of the reasoning layer is the embedded context representation, i.e., c0 = eC , and the final output of the reasoning layer is the output of the last cell, ck . Self-Attention Layer: As the final layer before answer generation, we utilize a residual static selfattention mechanism (Clark and Gardner, 2018) to help the model process long contexts with longterm dependencies. The input of this layer is the output of the last reasoning cell, ck . We first pass this representation through a fully-connected layer and then a bi-directional LSTM to obtain another representation of the context cSA . We obtain the self attention representation c0 : SA SA Sij = W4 cSA + W5 cSA cSA i j + W6 (ci j ) SA ) exp(Sij pSA ij = Pn SA k=1 exp(Sik ) st = LSTM([xt ; at−1 ], st−1 ) This hidden state is then used to compute a probability distribution over the generative vocabulary: Pgen = softmax(Wgen st + bgen ) We em"
D18-1454,N18-2007,0,0.127304,"oning in very long texts. However, QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context; hence, this is more focused on fact finding and linking, and does not require models to synthesize and generate new information. We focus on the recently published NarrativeQA generative dataset (Koˇcisk`y et al., 2018) that contains questions requiring multi-hop reasoning for long, complex stories and other narratives, which requires the model to go beyond fact linking and to synthesize non-span answers. Hence, models that perform well on previous reasoning tasks (Dhingra et al., 2018) have had limited success on this dataset. In this paper, we first propose the Multi-Hop Pointer-Generator Model (MHPGM), a strong baseline model that uses multiple hops of bidirectional attention, self-attention, and a pointer-generator decoder to effectively read and reason within a long passage and synthesize a coherent response. Our model achieves 41.49 Rouge-L and 17.33 METEOR on the summary 4220 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4220–4230 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Ling"
D18-1454,P16-1086,0,0.0377446,"is task, it has been shown that iteratively updating the query representation with information from the context can effectively emulate multi-step reasoning (Sukhbaatar et al., 2015). More recently, there has been an increase in multi-paragraph, multi-hop inference QA datasets such as QAngaroo (Welbl et al., 2018) and NarrativeQA (Koˇcisk`y et al., 2018). These datasets have much longer contexts than previous datasets, and answering a question often requires the synthesis of multiple discontiguous pieces of evidence. It has been shown that models designed for previous tasks (Seo et al., 2017; Kadlec et al., 2016) have limited success on these new datasets. In our work, we expand upon Gated Attention Network (Dhingra et al., 2017) to create a baseline model better suited for complex MRC datasets such as NarrativeQA by improving its attention and gating mechanisms, expanding its generation capabilities, and allowing access to external commonsense for connecting implicit relations. Commonsense/Background Knowledge: Commonsense or background knowledge has been used for several tasks including opinion mining (Cambria et al., 2010), sentiment analysis (Poria et al., 2015, 2016), handwritten text recognition"
D18-1454,Q18-1023,0,0.0838808,"Missing"
D18-1454,P18-1076,0,0.0597293,"l performance on a variety of tasks: Chen et al. (2018) showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; Xu et al. (2017) used a gated recall-LSTM mechanism to incorporate commonsense information into to4221 ken representations in dialogue. In MRC, Weissenborn et al. (2017) integrated external background knowledge into an NLU model by using contextually-refined word embeddings which integrated information from ConceptNet (single-hop relations mapped to unstructured text) via a single layer bidirectional LSTM. Concurrently to our work, Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention, where commonsense relations were extracted as triples. This work represented commonsense relations as keyvalue pairs and combined context representation and commonsense via a static gate. Differing from previous works, we employ multi-hop commonsense paths (multiple connected edges within ConceptNet graph that give us information beyond a single relationship triple) to help with our MRC model. Moreover, we use this in tandem with our multi-hop reasoning architecture to inco"
D18-1454,P02-1040,0,0.100639,"ng cell with NOIC, we selectively incorporate commonsense at every step of inference. 4 Experimental Setup Datasets: We report results on two multi-hop reasoning datasets: generative NarrativeQA (Koˇcisk`y et al., 2018) (summary subtask) and extractive QAngaroo WikiHop (Welbl et al., 2018). For multiple-choice WikiHop, we rank candidate responses by their generation probability. Similar to previous works (Dhingra et al., 2018), we use the non-oracle, unmasked and not-validated dataset. Evaluation Metrics: We evaluate NarrativeQA on the metrics proposed by its original authors: Bleu-1, Bleu-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and RougeL (Lin, 2004). We also evaluate on CIDEr (Vedantam et al., 2015) which emphasizes annotator consensus. For WikiHop, we evaluate on accuracy.5 More dataset, metric, and all other training details are in the supplementary. 5 Due to the 2-week evaluation wait-time on the nonpublic test set, we instead train our model on a sub-section of the training set, pick hyperparameters based on a small 4226 Model BLEU-1 BLEU-4 METEOR Rouge-L CIDEr Seq2Seq (Koˇcisk`y et al., 2018) ASR (Koˇcisk`y et al., 2018) BiDAF† (Koˇcisk`y et al., 2018) BiAttn + MRU-LSTM† (Tay"
D18-1454,N18-1202,0,0.200025,"ections, an effective generative QA model needs to be able to perform several hops of reasoning over long and complex passages. It would also need to be able to generate coherent statements to answer complex questions while having the ability to copy rare words such as specific entities from the reading context. With these in mind, we propose the Multi-Hop Pointer-Generator Model (MHPGM) baseline, a novel combination of previous works with the following major components: • Embedding Layer: The tokens are embedded into both learned word embeddings and pretrained context-aware embeddings (ELMo (Peters et al., 2018)). • Reasoning Layer: The embedded context is then passed through k reasoning cells, each of which iteratively updates the context representation with information from the query via BiDAF attention (Seo et al., 2017), emulating a single reasoning step within the multi-step reasoning process. • Self-Attention Layer: The context representation is passed through a layer of self-attention (Cheng et al., 2016) to resolve long-term dependencies and co-reference within the context. • Pointer-Generator Decoding Layer: A attention-pointer-generator decoder (See et al., 2017) that attends on and potenti"
D18-1454,Q18-1021,0,0.318138,"the combination of multiple disjoint pieces of evidence in the context. However, due to its synthetic nature, bAbI evidences have smaller lexicons and simpler passage structures when compared to humangenerated text. There also have been several attempts at the MRC-QA task on human-generated text. Large scale datasets such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) have made the training of end-to-end neural models possible. However, these datasets are fact-based and do not place heavy emphasis on multi-hop reasoning capabilities. More recent datasets such as QAngaroo (Welbl et al., 2018) have prompted a strong focus on multi-hop reasoning in very long texts. However, QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context; hence, this is more focused on fact finding and linking, and does not require models to synthesize and generate new information. We focus on the recently published NarrativeQA generative dataset (Koˇcisk`y et al., 2018) that contains questions requiring multi-hop reasoning for long, complex stories and other narratives, which requires the model to go beyond fact linking and to synthesize non-span answers. Hence, models"
D18-1454,D16-1264,0,0.0956714,"ta at: https://github.com/yicheng-w/CommonSenseMultiHopQA based on a passage of relevant content. Much progress has been made in reasoning-based MRCQA on the bAbI dataset (Weston et al., 2016), which contains questions that require the combination of multiple disjoint pieces of evidence in the context. However, due to its synthetic nature, bAbI evidences have smaller lexicons and simpler passage structures when compared to humangenerated text. There also have been several attempts at the MRC-QA task on human-generated text. Large scale datasets such as CNN/DM (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016) have made the training of end-to-end neural models possible. However, these datasets are fact-based and do not place heavy emphasis on multi-hop reasoning capabilities. More recent datasets such as QAngaroo (Welbl et al., 2018) have prompted a strong focus on multi-hop reasoning in very long texts. However, QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context; hence, this is more focused on fact finding and linking, and does not require models to synthesize and generate new information. We focus on the recently published NarrativeQA generative dataset"
D18-1454,P17-1099,0,0.360278,"-aware embeddings (ELMo (Peters et al., 2018)). • Reasoning Layer: The embedded context is then passed through k reasoning cells, each of which iteratively updates the context representation with information from the query via BiDAF attention (Seo et al., 2017), emulating a single reasoning step within the multi-step reasoning process. • Self-Attention Layer: The context representation is passed through a layer of self-attention (Cheng et al., 2016) to resolve long-term dependencies and co-reference within the context. • Pointer-Generator Decoding Layer: A attention-pointer-generator decoder (See et al., 2017) that attends on and potentially copies from the context is used to create the answer. The overall model is illustrated in Fig. 1, and the layers are described in further detail below. Embedding layer: We embed each word from the context and question with a learned embedding space of dimension d. We also obtain contextaware embeddings for each word via the pretrained embedding from language models (ELMo) (1024 dimensions). The embedded representation for each word in the context or question, eC i or Q d+1024 ei ∈ R , is the concatenation of its learned word embedding and ELMo embedding. Reason"
D18-1454,speer-havasi-2012-representing,0,0.567933,"2018 Conference on Empirical Methods in Natural Language Processing, pages 4220–4230 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics subtask of NarrativeQA, substantially better than the performance of previous generative models. Next, to address the issue that understanding human-generated text and performing longdistance reasoning on it often involves intermittent access to missing hops of external commonsense (background) knowledge, we present an algorithm for selecting useful, grounded multi-hop relational knowledge paths from ConceptNet (Speer and Havasi, 2012) via a pointwise mutual information (PMI) and term-frequency-based scoring function. We then present a novel method of inserting these selected commonsense paths between the hops of document-context reasoning within our model, via the Necessary and Optional Information Cell (NOIC), which employs a selectivelygated attention mechanism that utilizes commonsense information to effectively fill in gaps of inference. With these additions, we further improve performance on the NarrativeQA dataset, achieving 44.16 Rouge-L and 19.03 METEOR (also verified via human evaluation). We also provide manual a"
D19-1132,E09-1026,0,0.214786,"Missing"
D19-1132,C18-1105,0,0.0554924,"that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et al., 2017; Sixt et al., 2018; Antoniou et al., 2017; Zhu et al., 2017; Mun et al., 2017; Wang and Perez, 2017), or generate augmentation strategies (Ratner et al., 2017). These a"
D19-1132,P16-1002,0,0.0178896,"d may need more epochs to expose the model to the many diverse policies it generates. We also present selected best policies to demonstrate that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et al., 2017; Sixt et al., 2018; Antoniou et"
D19-1132,W17-3529,0,0.0325206,"icies to demonstrate that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et al., 2017; Sixt et al., 2018; Antoniou et al., 2017; Zhu et al., 2017; Mun et al., 2017; Wang and Perez, 2017), or generate augmentation strategies (Ratner et a"
D19-1132,W15-4640,0,0.128836,"s into several smaller ones (such as Grammar Errors divided into Singular/Plural Errors and Verb Inflection Errors) and also add a new operation Stammer (word repetition). This modification provides a much larger space of operation combinations for the model to explore from, so that it could potentially learn more complex and nuanced augmentation policies. Figure 1 shows a sub-policy containing two operations. It first paraphrases 2 tokens with probability 0.7 and then introduces 1 grammar error with probability 0.4.1 We choose the dialogue generation task based on the Ubuntu Dialogue Corpus (Lowe et al., 2015) because as opposed to Natural Language Inference and Question Answering tasks, realworld dialogue datasets more naturally afford such perturbation-style human errors (i.e., contain more noise), and thus are inherently compatible with 1 Our code and sampled augmented data is publicly available at: https://github.com/WolfNiu/ AutoAugDialogue. The learned policies are presented in Table 4. 1317 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1317–1323, c Hong Kong, China, Novem"
D19-1132,K18-1047,1,0.69379,"andomly during training, and each sub-policy consists of 2 operations applied in sequence. These operations are semantic-preserving image processing functions such as translation and rotation. We adapt AutoAugment to NLP tasks, where the operations are subtle, semantic-preserving text perturbations. To collect a pool of such operations, the first challenge we face is that the discrete nature of text makes it less straightforward to come up with semantic-preserving perturbations. We thus employ as a starting point Should-NotChange strategies (equivalent to operations in this paper) proposed by Niu and Bansal (2018) which are shown to improve their dialogue task performance when trained on data perturbed by these strategies. Importantly, we next divide their operations into several smaller ones (such as Grammar Errors divided into Singular/Plural Errors and Verb Inflection Errors) and also add a new operation Stammer (word repetition). This modification provides a much larger space of operation combinations for the model to explore from, so that it could potentially learn more complex and nuanced augmentation policies. Figure 1 shows a sub-policy containing two operations. It first paraphrases 2 tokens w"
D19-1132,P16-1009,0,0.0167677,"e the controller outputs do not depend on the source inputs), and may need more epochs to expose the model to the many diverse policies it generates. We also present selected best policies to demonstrate that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augm"
D19-1132,D16-1139,0,0.0399138,"s to expose the model to the many diverse policies it generates. We also present selected best policies to demonstrate that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et al., 2017; Sixt et al., 2018; Antoniou et al., 2017; Zhu et al"
D19-1132,K17-2010,0,0.0376898,"present selected best policies to demonstrate that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et al., 2017; Sixt et al., 2018; Antoniou et al., 2017; Zhu et al., 2017; Mun et al., 2017; Wang and Perez, 2017), or generate augmentation str"
D19-1132,D15-1306,0,0.0757932,"s do not depend on the source inputs), and may need more epochs to expose the model to the many diverse policies it generates. We also present selected best policies to demonstrate that the seq2seq controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et a"
D19-1132,D18-1100,0,0.0305242,"controller can sometimes successfully attend to the source inputs. 2 Related Works There has been extensive work that employs data augmentation in both computer vision (Simard et al., 2003; Krizhevsky et al., 2012; Cires¸an et al., 2012; Wan et al., 2013; Sato et al., 2015; DeVries and Taylor, 2017; Tran et al., 2017; Lemley et al., 2017) and NLP (F¨urstenau and Lapata, 2009; Sennrich et al., 2016; Wang and Yang, 2015; Zhang et al., 2015; Jia and Liang, 2016; Kim and Rush, 2016; Hu et al., 2017; Xu et al., 2017; Xia et al., 2017; Silfverberg et al., 2017; Kafle et al., 2017; Hou et al., 2018; Wang et al., 2018). Automatic data augmentation is addressed via the AutoAugment algorithm proposed by Cubuk et al. (2019), which uses a hypernetwork (in our case, a controller) to train the target model, an approach inspired by neural architecture search (Zoph et al., 2017). Previous works have also adopted Generative Adversarial Networks (Goodfellow et al., 2014) to either directly generate augmented data (Tran et al., 2017; Sixt et al., 2018; Antoniou et al., 2017; Zhu et al., 2017; Mun et al., 2017; Wang and Perez, 2017), or generate augmentation strategies (Ratner et al., 2017). These approaches produce pe"
D19-1253,P17-1123,0,0.500747,"y be used to augment QA datasets (Duan et al., 2017), but can also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), the task of Question Generation (QG) remains understudied and challenging. However, as an important dual 1 Code and models publicly available at: https:// github.com/ZhangShiyue/QGforQA Current QG systems follow an attentionbased sequence-to-sequence struct"
D19-1253,D17-1090,0,0.409069,"e also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.1 1 Gt: who was an advocate of separation of powers? Base: who opposed the principle of enlightenment? Ours: who advocated the principle in the age of enlightenment? Table 1: An examples of the “semantic drift” issue in Question Generation (“Gt” is short for “ground truth”). task to QA, QG can not only be used to augment QA datasets (Duan et al., 2017), but can also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-"
D19-1253,N10-1086,0,0.783657,"batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.1 1 Gt: who was an advocate of separation of powers? Base: who opposed the principle of enlightenment? Ours: who advocated the principle in the age of enlightenment? Table 1: An examples of the “semantic drift” issue in Question Generation (“Gt” is short for “ground truth”). task to QA, QG can not only be used to augment QA datasets (Duan et al., 2017), but can also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018;"
D19-1253,N19-1237,0,0.0473286,"-of-the-art performance (Dong et al., 2019). However, the above models were trained with teacher forcing only. To address the exposure bias problem, some works applied reinforcement learning taking evaluation metrics (e.g., BLEU) as rewards (Song et al., 2017; Kumar et al., 2018). Yuan et al. (2017) proposed to use a language model’s perplexity (RP P L ) and a QA model’s accuracy (RQA ) as two rewards but failed to get significant improvement. Their second reward is similar to our QAP reward except that we use QA probability rather than accuracy as the probability distribution is more smooth. Hosking and Riedel (2019) compared a set of different rewards, including RP P L and RQA , and claimed none of them improved the quality of generated questions. For QG evaluation, even though some previous works conducted human evaluations, most of them still relied on traditional metrics (e.g., BLEU). However, Nema and Khapra (2018) pointed out the existing metrics do not correlate with human judgment about answerability, so they proposed “Q-metrics” that mixed traditional metrics with an “answerability” score. In our work, we will show QG results on traditional metrics, Q-metrics, as well as human evaluation, and als"
D19-1253,D17-1091,0,0.0217307,"tions, most of them still relied on traditional metrics (e.g., BLEU). However, Nema and Khapra (2018) pointed out the existing metrics do not correlate with human judgment about answerability, so they proposed “Q-metrics” that mixed traditional metrics with an “answerability” score. In our work, we will show QG results on traditional metrics, Q-metrics, as well as human evaluation, and also propose a QA-based QG evaluation. Question Generation for QA As the dual task of QA, QG has been often proposed for improving QA. Some works have directly used QG in QA models’ pipeline (Duan et al., 2017; Dong et al., 2017; Lewis and Fan, 2019). Some other works enabled semi-supervised QA with the help of QG. Tang et al. (2017) applied the “dual learning” algorithm (He et al., 2016) to learn QA and QG jointly with unlabeled texts. Yang et al. (2017) and Tang et al. (2018) followed the GAN (Goodfellow et al., 2014) paradigm, taking QG as a generator and QA as a discriminator, to utilize unlabeled data. Sachan and Xing (2018) proposed a self-training cycle between QA and QG. However, these works either reduced the ground-truth data size or simplified the span-prediction QA task to answer sentence selection. Dhing"
D19-1253,D17-1215,0,0.0340037,"es.1 1 Gt: who was an advocate of separation of powers? Base: who opposed the principle of enlightenment? Ours: who advocated the principle in the age of enlightenment? Table 1: An examples of the “semantic drift” issue in Question Generation (“Gt” is short for “ground truth”). task to QA, QG can not only be used to augment QA datasets (Duan et al., 2017), but can also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 201"
D19-1253,P18-1177,0,0.209978,"Missing"
D19-1253,P17-1147,0,0.0312586,"he task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), the task of Question Generation (QG) remains understudied and challenging. However, as an important dual 1 Code and models publicly available at: https:// github.com/ZhangShiyue/QGforQA Current QG systems follow an attentionbased sequence-to-sequence structure, taking the paragraph-level context and answer as inputs and outputting the question. However, we observed that these QG models often generate questions that semantically drift away from the given context and answer; we call this the “semantic drift” problem. As shown in Table 1, the baseline QG model generates a qu"
D19-1253,P15-1086,0,0.0901636,"ng mini-batch training” strategy that always regularizes the training signal with the ground-truth data. Experiments show that our method improves both BiDAF (Seo et al., 2016; Clark and Gardner, 2018) and BERT (Devlin et al., 2018) QA baselines by 1.69/1.27 and 1.19/0.56 absolute points on EM/F1, respectively; even without introducing new articles, it can bring 1.51/1.13 and 0.95/0.13 absolute improvement, respectively. 2 Related Works Question Generation Early QG studies focused on using rule-based methods to transform statements to questions (Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). Recent works adopted the attention-based sequenceto-sequence neural model (Bahdanau et al., 2014) for QG tasks, taking answer sentence as input and outputting the question (Du et al., 2017; Zhou et al., 2017), which proved to be better than rulebased methods. Since human-labeled questions are often relevant to a longer context, later works leveraged information from the whole paragraph for QG, either by extracting additional information from the paragraph (Du and Cardie, 2018; Song et al., 2018; Liu et al., 2019) or by directly taking the whole paragraph as input (Zhao et al., 2018; Kim et a"
D19-1253,W13-2114,0,0.4385,"ly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.1 1 Gt: who was an advocate of separation of powers? Base: who opposed the principle of enlightenment? Ours: who advocated the principle in the age of enlightenment? Table 1: An examples of the “semantic drift” issue in Question Generation (“Gt” is short for “ground truth”). task to QA, QG can not only be used to augment QA datasets (Duan et al., 2017), but can also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our"
D19-1253,D18-1429,0,0.260045,"swer. We regularize the generation with these two rewards via reinforcement learning. Experiments show that these two rewards can significantly improve the question generation quality separately or jointly, and achieve the new state-of-the-art performance on the SQuAD QG task. Next, in terms of QG evaluation, previous works have mostly adopted popular automatic evaluation metrics, like BLEU, METEOR, etc. However, we observe that these metrics often fall short in properly evaluating the quality of generated questions. First, they are not always correlated to human judgment about answerability (Nema and Khapra, 2018). Second, since multiple questions are valid but only one reference exists in the dataset, these traditional metrics fail to appropriately score question paraphrases and novel generation (shown in Table 2). Therefore, we introduce a QA-based evaluation method that directly measures the QG model’s ability to mimic human annotators in generating QA training data, because ideally, we hope that the QG model can act like a human to ask questions. We compare different QG systems using this evaluation method, which shows that our semantics-reinforced QG model performs best. However, this improvement"
D19-1253,P02-1040,0,0.10442,"Missing"
D19-1253,N18-2102,1,0.845416,"b) 5θ logpθ (q s ) (5) We follow the effective SCST strategy (Rennie et al., 2017) to take the reward of greedy search result q g as the baseline, i.e. b = r(q g ). However, only using this objective to train QG will result in poor readability, so we follow the mixed loss setting (Paulus et al., 2017): Lmixed = γLRL + (1 − γ)LM L . In practice, we find the mixing ratio γ for QAP reward should be lower, i.e., it needs more regularization from teacher forcing, so that it can avoid the undesirable cheating issue mentioned above. Furthermore, we also apply the multi-reward optimization strategy (Pasunuru and Bansal, 2018) to train the model with two mixed losses alternately with an alternate rate n : m, i.e., train with Lqpp mixed for n mini-batches, then train with Lqap mixed for m mini-batches, repeat until convergence. n and m are two hyper-parameters. qpp qpp Lqpp LRL + (1 − γ qpp )LM L mixed = γ qap qap Lqap LRL + (1 − γ qap )LM L mixed = γ (6) Experiments show that these two rewards can significantly improve the QG performance separately or jointly, and we achieve new state-of-the-art QG performances, see details in Section 6. 3.3 QA-Based QG Evaluation Inspired by the idea that “a perfect QG model can r"
D19-1253,N18-1202,0,0.0258168,"ith significant margins. Our base model architecture is shown in the upper box in Figure 1 and described as follow. If we have a paragraph p = {xi }M i=1 and an answer a which is a sub-span of p, the target of the QG task is to generate a question q = {yj }N j=1 that can be answered by a based on the information in p. Embedding The model first concatenates four word representations: word vector, answer tag embedding, Part-of-Speech (POS) tag embedding, and Name Entity (NER) tag embedding, i.e., ei = [wi , ai , pi , ni ]. For word vectors, we use the deep contextualized word vectors from ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018). The answer tag follows the BIO2 tagging scheme. Encoder The output of the embedding layer is then encoded by a two-layer bi-directional LSTMRNNs, resulting in a list of hidden representations H. At any time step i, the representation hi is the → − ← − concatenation of hi and hi . → − → − −−−−→ h i = LST M ([ei ; h i−1 ]) ← − ← − ←−−−− h i = LST M ([ei ; h i+1 ]) → − ← − H = [ hi , hi ]M i=1 2 (1) “B”, for “Begin”, tags the start token of the answer span; “I”, for “Inside”, tags other tokens in the answer span; “O”, for “Other”, tags other tokens in the paragraph"
D19-1253,D16-1264,0,0.805125,"(Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), the task of Question Generation (QG) remains understudied and challenging. However, as an important dual 1 Code and models publicly available at: https:// github.com/ZhangShiyue/QGforQA Current QG systems follow an attentionbased sequence-to-sequence structure, taking the paragraph-level context and answer as inputs and outputting the question. However, we observed that these QG models often generate questions that semantically drift away from the given context and answer; we call this the “semantic drift” problem. As shown in Table 1, the baseline QG"
D19-1253,E17-1036,0,0.0216961,"also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), the task of Question Generation (QG) remains understudied and challenging. However, as an important dual 1 Code and models publicly available at: https:// github.com/ZhangShiyue/QGforQA Current QG systems follow an attentionbased sequence-to-sequence structure, taking the paragraph-level context and answer as inputs and"
D19-1253,N18-1058,0,0.188196,"QA-based QG evaluation. Question Generation for QA As the dual task of QA, QG has been often proposed for improving QA. Some works have directly used QG in QA models’ pipeline (Duan et al., 2017; Dong et al., 2017; Lewis and Fan, 2019). Some other works enabled semi-supervised QA with the help of QG. Tang et al. (2017) applied the “dual learning” algorithm (He et al., 2016) to learn QA and QG jointly with unlabeled texts. Yang et al. (2017) and Tang et al. (2018) followed the GAN (Goodfellow et al., 2014) paradigm, taking QG as a generator and QA as a discriminator, to utilize unlabeled data. Sachan and Xing (2018) proposed a self-training cycle between QA and QG. However, these works either reduced the ground-truth data size or simplified the span-prediction QA task to answer sentence selection. Dhingra et al. (2018) collected 3.2M cloze-style QA pairs to pre-train a QA model, then fine-tune with the full groundtruth data which improved a BiDAF-QA baseline. In our paper, we follow the back-translation (Sennrich et al., 2016) strategy to generate new QA pairs by our best QG model to augment SQuAD training set. Further, we introduce a data filter to remove poorly generated examples and a mixing mini-batc"
D19-1253,P16-1009,0,0.612913,"erent QG systems using this evaluation method, which shows that our semantics-reinforced QG model performs best. However, this improvement is relatively minor compared to our improvement on other QG metrics, which indicates improvement on typical QG metrics does not always lead to better question annotation by QG models for generating QA training set. Further, we investigate how to use our best QG system to enrich QA datasets and perform semi-supervised QA on SQuADv1.1 (Rajpurkar et al., 2016). Following the back-translation strategy that has been shown to be effective in Machine Translation (Sennrich et al., 2016) and Natural Language Navigation (Fried et al., 2018; Tan et al., 2019), we propose two methods to collect synthetic data. First, since multiple questions can be asked for one answer while there is only one human-labeled ground-truth, we make our QG model generate new questions for existing context-answer pairs in SQuAD training set, so as to enrich it with paraphrased and other novel but valid questions. Second, we use our QG model to label new context-answer pairs from new Wikipedia articles. However, directly mixing synthetic QA pairs with ground-truth data will not lead to improvement. Hen"
D19-1253,W18-2609,0,0.0192542,"ng and different facts in the context that can be used to ask the question. Therefore, without introducing new Wikipedia articles, we make our QG model generate diverse questions for the existing contextanswer pairs in SQuAD training set by keeping the all beam search outputs for each example. Generate from New Articles To use unlabeled Wikipedia articles for data augmentation, an automatic answer extractor is indispensable. Some previous works have proposed methods to detect key phrases from a paragraph and automatically extract potential answer spans (Yang et al., 2017; Du and Cardie, 2018; Subramanian et al., 2018). Instead of building up our answer extractor, we directly take advantage of the released HarvestingQA dataset. It contains 1.2M synthetic QA pairs, in which both the answer extractor and the QG model were proposed by Du and Cardie (2018). We use their paragraphs with answer span labels but generate questions with our QG models, and only use their questions for comparison. 4.2 Synthetic Data Usage In practice, we find that directly mixing the synthetic data with the ground-truth data does not improve QA performance. We conjecture the reason is that some poor-quality synthetic examples mislead"
D19-1253,D18-1427,0,0.137196,"orks adopted the attention-based sequenceto-sequence neural model (Bahdanau et al., 2014) for QG tasks, taking answer sentence as input and outputting the question (Du et al., 2017; Zhou et al., 2017), which proved to be better than rulebased methods. Since human-labeled questions are often relevant to a longer context, later works leveraged information from the whole paragraph for QG, either by extracting additional information from the paragraph (Du and Cardie, 2018; Song et al., 2018; Liu et al., 2019) or by directly taking the whole paragraph as input (Zhao et al., 2018; Kim et al., 2018; Sun et al., 2018). A very recent concurrent work applied the large-scale language model pre-training strategy for QG and 2496 also achieved a new state-of-the-art performance (Dong et al., 2019). However, the above models were trained with teacher forcing only. To address the exposure bias problem, some works applied reinforcement learning taking evaluation metrics (e.g., BLEU) as rewards (Song et al., 2017; Kumar et al., 2018). Yuan et al. (2017) proposed to use a language model’s perplexity (RP P L ) and a QA model’s accuracy (RQA ) as two rewards but failed to get significant improvement. Their second rewar"
D19-1253,N19-1268,1,0.836397,"s-reinforced QG model performs best. However, this improvement is relatively minor compared to our improvement on other QG metrics, which indicates improvement on typical QG metrics does not always lead to better question annotation by QG models for generating QA training set. Further, we investigate how to use our best QG system to enrich QA datasets and perform semi-supervised QA on SQuADv1.1 (Rajpurkar et al., 2016). Following the back-translation strategy that has been shown to be effective in Machine Translation (Sennrich et al., 2016) and Natural Language Navigation (Fried et al., 2018; Tan et al., 2019), we propose two methods to collect synthetic data. First, since multiple questions can be asked for one answer while there is only one human-labeled ground-truth, we make our QG model generate new questions for existing context-answer pairs in SQuAD training set, so as to enrich it with paraphrased and other novel but valid questions. Second, we use our QG model to label new context-answer pairs from new Wikipedia articles. However, directly mixing synthetic QA pairs with ground-truth data will not lead to improvement. Hence, we introduce two empirically effective strategies: one is a data fi"
D19-1253,N18-1141,0,0.0825648,"Missing"
D19-1253,P17-1018,0,0.0506819,"2 tagging scheme. Encoder The output of the embedding layer is then encoded by a two-layer bi-directional LSTMRNNs, resulting in a list of hidden representations H. At any time step i, the representation hi is the → − ← − concatenation of hi and hi . → − → − −−−−→ h i = LST M ([ei ; h i−1 ]) ← − ← − ←−−−− h i = LST M ([ei ; h i+1 ]) → − ← − H = [ hi , hi ]M i=1 2 (1) “B”, for “Begin”, tags the start token of the answer span; “I”, for “Inside”, tags other tokens in the answer span; “O”, for “Other”, tags other tokens in the paragraph. 2497 Agent Self-attention A gated self-attention mechanism (Wang et al., 2017) is applied to H to aggregate the long-term dependency within the paragraph. αi is an attention vector between hi and each element in H; ui is the self-attention context vector for hi ; hi is then updated to fi using ui ; a soft gate gi decides how much the update is apˆ i ]M is the output of this layer. ˆ = [h plied. H i=1 ... 1 2 reward (QPP & QAP) ui = Hαi , αi = sof tmax(H W hi ) f gi = sigmoid(W g [hi ; ui ]) ˆ i = gi ∗ fi + (1 − gi ) ∗ hi h (2) ˆ j , αj = sof tmax(H ˆ T W a sj ) cj = Hα (3) sj+1 = LST M ([yj ; s˜j ]) The probability of the target word yj is computed by a maxout neural ne"
D19-1253,P17-1096,0,0.0773686,"ed traditional metrics with an “answerability” score. In our work, we will show QG results on traditional metrics, Q-metrics, as well as human evaluation, and also propose a QA-based QG evaluation. Question Generation for QA As the dual task of QA, QG has been often proposed for improving QA. Some works have directly used QG in QA models’ pipeline (Duan et al., 2017; Dong et al., 2017; Lewis and Fan, 2019). Some other works enabled semi-supervised QA with the help of QG. Tang et al. (2017) applied the “dual learning” algorithm (He et al., 2016) to learn QA and QG jointly with unlabeled texts. Yang et al. (2017) and Tang et al. (2018) followed the GAN (Goodfellow et al., 2014) paradigm, taking QG as a generator and QA as a discriminator, to utilize unlabeled data. Sachan and Xing (2018) proposed a self-training cycle between QA and QG. However, these works either reduced the ground-truth data size or simplified the span-prediction QA task to answer sentence selection. Dhingra et al. (2018) collected 3.2M cloze-style QA pairs to pre-train a QA model, then fine-tune with the full groundtruth data which improved a BiDAF-QA baseline. In our paper, we follow the back-translation (Sennrich et al., 2016) st"
D19-1253,D18-1259,0,0.0397368,"usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), the task of Question Generation (QG) remains understudied and challenging. However, as an important dual 1 Code and models publicly available at: https:// github.com/ZhangShiyue/QGforQA Current QG systems follow an attentionbased sequence-to-sequence structure, taking the paragraph-level context and answer as inputs and outputting the question. However, we observed that these QG models often generate questions that semantically drift away from the given context and answer; we call this the “semantic drift” problem. As shown in Table 1, the baseline QG model generates a question that has almo"
D19-1253,W17-2603,0,0.0585528,"from the paragraph (Du and Cardie, 2018; Song et al., 2018; Liu et al., 2019) or by directly taking the whole paragraph as input (Zhao et al., 2018; Kim et al., 2018; Sun et al., 2018). A very recent concurrent work applied the large-scale language model pre-training strategy for QG and 2496 also achieved a new state-of-the-art performance (Dong et al., 2019). However, the above models were trained with teacher forcing only. To address the exposure bias problem, some works applied reinforcement learning taking evaluation metrics (e.g., BLEU) as rewards (Song et al., 2017; Kumar et al., 2018). Yuan et al. (2017) proposed to use a language model’s perplexity (RP P L ) and a QA model’s accuracy (RQA ) as two rewards but failed to get significant improvement. Their second reward is similar to our QAP reward except that we use QA probability rather than accuracy as the probability distribution is more smooth. Hosking and Riedel (2019) compared a set of different rewards, including RP P L and RQA , and claimed none of them improved the quality of generated questions. For QG evaluation, even though some previous works conducted human evaluations, most of them still relied on traditional metrics (e.g., BLEU"
D19-1253,D18-1424,0,0.408529,"ent QA datasets (Duan et al., 2017), but can also be applied in conversation and education systems (Heilman and Smith, 2010; Lindberg et al., 2013). Furthermore, given that existing QA models often fall short by doing simple word/phrase matching rather than true comprehension (Jia and Liang, 2017), the task of QG, which usually needs complicated semantic reasoning and syntactic variation, should be another way to encourage true machine comprehension (Lewis and Fan, 2019). Recently, we have seen an increasing interest in the QG area, with mainly three categories: Textbased QG (Du et al., 2017; Zhao et al., 2018), Knowledge-Base-based QG (Reddy et al., 2017; Serban et al., 2016), and Image-based QG (Li et al., 2018; Jain et al., 2017). Our work focuses on the Text-based QG branch. Introduction In contrast to the rapid progress shown in Question Answering (QA) tasks (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), the task of Question Generation (QG) remains understudied and challenging. However, as an important dual 1 Code and models publicly available at: https:// github.com/ZhangShiyue/QGforQA Current QG systems follow an attentionbased sequence-to-sequence structure, taking the para"
D19-1258,1989.mtsummit-1.1,0,0.177844,"Missing"
D19-1455,N16-1181,0,0.70596,"ructured reasoning path where the model first locates two entities on the leaves and then compares them to get the answer. The difference in the required reasoning skills makes it hard for a single static model to handle all types of questions. To automatically discover the multiple elaborated reasoning steps as required in HotpotQA, a model needs to dynamically adopt a sequence of different reasoning behaviors based on specific questions, which is still unexplored in the field of large-scale text-based QA. In our work, we present a highly-interpretable self-assembling Neural Modular Network (Andreas et al., 2016a; Hu et al., 2017) with three novel modules designed for multi-hop NLP tasks: Find, Relocate, Compare, where each module embodies a unique type of reasoning behavior. The Find module is similar to the previously-introduced 1-hop biattention model (Seo et al., 2017; Xiong et al., 2017), which produces an attention map over the context words given the context and the question representation. For the first example in Fig. 1, a Find module is used to find the answer (“Shirley Temple”) to the sub-question (“who portrayed Corliss Archer ...”). The Relocate module takes the intermediate answer to th"
D19-1455,N19-1405,0,0.0245941,"ntribution of this work is that we adapt neural modular networks (NMN) (Andreas et al., 2016a; Hu et al., 2017, 2018), which were designed for visual-domain QA, to text-domain QA by rebuilding every reasoning module. We substitute convolution and multiplication between question vectors and context features with biattention as the basic reasoning component in the Find and Relocate. Moreover, our model maintains a stack of attention outputs before it is projected down to 1-d, thus enabling skip connections when predicting the answer span. As shown Adversarial Evaluation Multiple previous works (Chen and Durrett, 2019; Min et al., 2019a) have shown that models performing strongly on HotpotQA are not necessarily capable of compositional reasoning. Jiang and Bansal (2019) proposed to construct adversarial distractor documents to eliminate the reasoning shortcut and necessitate compositional reasoning on HotpotQA dataset. To test whether our modular network can perform robust multi-hop reasoning against such adversaries, we evaluate our models on the adversarial dev set. The second column of Table 4 shows that our NMN outperforms the baseline significantly (+10 points in EM score) on the adversarial evaluatio"
D19-1455,P18-1078,0,0.0235169,"the antonyms back to their comparative form (3rd row in Table 1). Finally, if the question has a comparative adj./adv., we flip the order of the two entities compared (4th row in Table 1). In all three cases, the answer to the mutated question is also flipped. Training Details We use 300-d GloVe pretrained embeddings (Pennington et al., 2014). The model is supervised to predict either the start and end index of a span or “Yes/No” for specific questions. The entire model (controller + modular netResults Baseline Our baseline is the bi-attention (Seo et al., 2017) + self-attention model as in (Clark and Gardner, 2018; Yang et al., 2018), which was shown to be able to achieve strong performance on single-hop QA tasks like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). Our baseline shares the preprocessing and encoding layer with the modular network. 5.2 Primary NMN Results We first present our model’s performance on the HotpotQA (Yang et al., 2018) dev and test set of our split. As shown in the first three rows of Table 2, our modular network achieves significant improvements over both the baseline and the convolution-based NMN (Hu et al., 2018) on our test set. In Table 3, we further br"
D19-1455,P19-1262,1,0.904603,"predicts the modular layout and the network assembled with the selected modules. We further apply intermediate supervision to the first produced attention map to encourage finding the bridge entity (“Shirley Temple” in Fig. 1). The entire model is trained end-to-end using cross entropy loss for all supervision. Overall, our self-assembling controller-based Neural Modular Network achieves statistically significant improvements over both the single-hop bi-attention baseline and the original modular network (Hu et al., 2018) designed for visual-domain QA. We also present adversarial evaluation (Jiang and Bansal, 2019), where single-hop reasoning shortcuts are eliminated and compositional reasoning is enforced; and here our NMN again outperforms the BiDAF baseline significantly in the 4475 EM score (as well as after adversarial training). We further demonstrate the interpretability of our modular network with three analyses. First, the controller understands the multi-hop semantics of the question and can provide accurate step-bystep sub-questions to the module to lead the main network to follow the reasoning path. Second, the controller can successfully predict layouts that conform to the layouts designed"
D19-1455,P17-1147,0,0.126005,"ply self-assembling modular networks to text-based QA; 2) We design three novel modules to handle multi-hop questions in HotpotQA; 3) The resulting network is interpretable in terms of the model’s intermediate outputs and the assembled layout. 2 Related Works Multi-hop Reading Comprehension The last few years have witnessed significant progress on large-scale QA datasets including clozestyle tasks (Hermann et al., 2015), open-domain QA (Yang et al., 2015), and more (Rajpurkar et al., 2016, 2018). However, all of the above datasets are confined to a single-document context per question domain. Joshi et al. (2017) introduced a multi-document QA dataset with some questions requiring cross-sentence inferences to answer. The bAbI dataset (Weston et al., 2016) requires the model to combine multiple pieces of evidence in the synthetic text. Welbl et al. (2017) uses Wikipedia articles as the context and a subject-relation pair as the query, and constructs the multi-hop QAngaroo dataset by traversing a directed bipartite graph so that the evidence required to answer a query could be spread across multiple documents. HotpotQA (Yang et al., 2018) is a more recent multi-hop QA dataset that has crowd-sourced ques"
D19-1455,P14-5010,0,0.00294294,"s from HotpotQA training set. Dev work) is trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.001. Test EM F1 EM F1 BiDAF Baseline NMN Our NMN + Data aug. 44.68 31.04 50.67 50.63 57.19 40.28 63.35 63.29 42.7 30.87 49.58 49.46 55.81 39.90 62.71 62.59 - Bridge sup. - Relocate - Compare - NoOp 46.56 47.81 50.29 49.11 58.60 60.22 63.30 61.79 45.91 46.75 48.87 48.56 57.22 59.23 62.52 62.10 5 5.1 Table 2: EM and F1 scores on HotpotQA dev set and test set. All models are tuned on dev set. part-of-speech tags and NER tag of the question using Corenlp (Manning et al., 2014). For each question whose answer is either “yes” or “no”, we generate a new question by randomly sampling two titles of the same type (based on POS and NER) from the training set to substitute the original entities in the question and corresponding supporting documents (1st row in Table 1). We then employ three strategies to generate 5,342 extra questions by mutating original questions. First, if the question contains the word “same” and the answer is yes or no, we substitute “same” with “different” and vice versa (2nd row in Table 1). Second, we detect the comparative and superlative adjectiv"
D19-1455,P19-1416,0,0.142751,"at the evidence required to answer a query could be spread across multiple documents. HotpotQA (Yang et al., 2018) is a more recent multi-hop QA dataset that has crowd-sourced questions with more diverse syntactic and semantic features compared to QAngaroo. It includes four types of questions, each requiring a different reasoning paradigm. Some examples require inferring the bridge entity from the question (Type I in Yang et al. (2018)), while others demand fact-checking or comparing subjects’ properties from two different documents (Type II and comparison question). Concurrently to our work, Min et al. (2019b) also tackle HotpotQA by decomposing its multi-hop questions into singlehop sub-questions to achieve better performance and interpretability. However, their system approaches the question decomposition by having a decomposer model trained via human labels, while our controller accomplishes this task automatically with the soft attention over the questionwords’ representation and is only distantly supervised by the answer and bridge-entity supervision, with no extra human labels. Moreover, they propose a pipeline system with the decomposers, an answer-prediction model, and a decomposition sco"
D19-1455,P19-1613,0,0.317887,"at the evidence required to answer a query could be spread across multiple documents. HotpotQA (Yang et al., 2018) is a more recent multi-hop QA dataset that has crowd-sourced questions with more diverse syntactic and semantic features compared to QAngaroo. It includes four types of questions, each requiring a different reasoning paradigm. Some examples require inferring the bridge entity from the question (Type I in Yang et al. (2018)), while others demand fact-checking or comparing subjects’ properties from two different documents (Type II and comparison question). Concurrently to our work, Min et al. (2019b) also tackle HotpotQA by decomposing its multi-hop questions into singlehop sub-questions to achieve better performance and interpretability. However, their system approaches the question decomposition by having a decomposer model trained via human labels, while our controller accomplishes this task automatically with the soft attention over the questionwords’ representation and is only distantly supervised by the answer and bridge-entity supervision, with no extra human labels. Moreover, they propose a pipeline system with the decomposers, an answer-prediction model, and a decomposition sco"
D19-1455,D14-1162,0,0.0821358,"n controllers instead of being manually-designed. However, Neural Architecture Search aims to learn the structure of the individual CNN/RNN cell with fixed inter-connections between the cells, while Modular Networks have preset individual modules but learns the way to assemble them into a larger network. Moreover, Modular Networks’ architectures are predicted dynamically on each data point, while previous NAS methods learn a single cell structure independent of the example. Encoding We use a Highway Network (Srivastava et al., 2015) to merge the character embeddings and GloVe word embeddings (Pennington et al., 2014), building word representations for the context and the question as x ∈ RS×v and q ∈ RJ×v respectively, where S and J are the lengths of the context and the question. We then apply a bidirectional LSTM-RNN (Hochreiter and Schmidhuber, 1997) of d hidden units to get the contextualized word representations for the context and question: h = BiLSTM(x); u = BiLSTM(q) so that h ∈ RS×2d and u ∈ RJ×2d . We also use a self attention layer (Zhong et al., 2019) to get qv, a fixed-sized vector representation of the question. 3.2 Model Layout Controller In a modular network, the controller reads the questi"
D19-1455,P18-2124,0,0.0551847,"Missing"
D19-1455,D16-1264,0,0.58624,"ies (second question in Fig. 1). For the first question, it is necessary to first find the person “who portrayed Corliss Archer in the film Kiss and Tell”, and then find out the “government position” she held. For the second question, one may arrive at the answer by finding the country where Scott and Ed are from, and then comparing the two nationalities to conclude whether they are the same. Multi-hop QA is more challenging than singlehop QA for two main reasons. First, the techniques used for single-hop QA are not sufficient to answer a multi-hop question. In single-hop QA tasks like SQuAD (Rajpurkar et al., 2016), the evidence necessary to answer the question is concentrated in a short context (Q: “What is the color of the grass”, Context: “The grass is green.”, An4474 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4474–4484, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics swer: “Green”). Such datasets emphasize the role of matching the information between the question and the short context surrounding the answer (“sky→sky, color→blue”), and ca"
D19-1455,D15-1237,0,0.0943424,"Missing"
D19-1455,D18-1259,0,0.256172,"s two distantly-located supporting facts by addressing the sub-question from the controller.1 1 Figure 1: Two HotpotQA examples and the modular network layout predicted by the controller. Introduction The task of multi-hop question answering (QA) requires the model to answer a natural language question by finding multiple relevant information 1 Our code is publicly available at: https://github. com/jiangycTarheel/NMN-MultiHopQA pieces scattered in a given natural language context. It has attracted more attention recently and multiple datasets have been proposed, including the recent HotpotQA (Yang et al., 2018) that is comprised of questions embodying four different multi-hop reasoning paradigms: inferring the bridge entity to complete the 2nd-hop question (first question in Fig. 1), inferring the answer through a bridge entity, checking multiple properties to select the answer, and comparing two entities (second question in Fig. 1). For the first question, it is necessary to first find the person “who portrayed Corliss Archer in the film Kiss and Tell”, and then find out the “government position” she held. For the second question, one may arrive at the answer by finding the country where Scott and"
D19-1514,N19-1423,0,0.311904,"(Simonyan and Zisserman, 2014; Szegedy et al., 2015; He et al., 2016) and shown their effectiveness on large vision datasets (Deng et al., 2009; Lin et al., 2014; Krishna et al., 2017). Pioneering works (Girshick et al., 2014; Xu et al., 2015) also show the generalizability of these pretrained (especially on ImageNet) backbone models by fine-tuning them on different tasks. In terms of language understanding, last year, we witnessed strong progress towards building a universal backbone model with large-scale contextualized language model pre-training (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), which has improved performances on various tasks (Rajpurkar et al., 2016; Wang et al., 2018) to significant levels. Despite these influential singlemodality works, large-scale pretraining and finetuning studies for the modality-pair of vision and language are still under-developed. Therefore, we present one of the first works in building a pre-trained vision-and-language crossmodality framework and show its strong performance on several datasets. We name this framework “LXMERT: Learning Cross-Modality Encoder Representations from Transformers” (pronounced: ‘leksmert’). This framework is mode"
D19-1514,N18-1202,0,0.433369,"ple have developed several backbone models (Simonyan and Zisserman, 2014; Szegedy et al., 2015; He et al., 2016) and shown their effectiveness on large vision datasets (Deng et al., 2009; Lin et al., 2014; Krishna et al., 2017). Pioneering works (Girshick et al., 2014; Xu et al., 2015) also show the generalizability of these pretrained (especially on ImageNet) backbone models by fine-tuning them on different tasks. In terms of language understanding, last year, we witnessed strong progress towards building a universal backbone model with large-scale contextualized language model pre-training (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), which has improved performances on various tasks (Rajpurkar et al., 2016; Wang et al., 2018) to significant levels. Despite these influential singlemodality works, large-scale pretraining and finetuning studies for the modality-pair of vision and language are still under-developed. Therefore, we present one of the first works in building a pre-trained vision-and-language crossmodality framework and show its strong performance on several datasets. We name this framework “LXMERT: Learning Cross-Modality Encoder Representations from Transformers” (pro"
D19-1514,D16-1264,0,0.0573489,"d shown their effectiveness on large vision datasets (Deng et al., 2009; Lin et al., 2014; Krishna et al., 2017). Pioneering works (Girshick et al., 2014; Xu et al., 2015) also show the generalizability of these pretrained (especially on ImageNet) backbone models by fine-tuning them on different tasks. In terms of language understanding, last year, we witnessed strong progress towards building a universal backbone model with large-scale contextualized language model pre-training (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019), which has improved performances on various tasks (Rajpurkar et al., 2016; Wang et al., 2018) to significant levels. Despite these influential singlemodality works, large-scale pretraining and finetuning studies for the modality-pair of vision and language are still under-developed. Therefore, we present one of the first works in building a pre-trained vision-and-language crossmodality framework and show its strong performance on several datasets. We name this framework “LXMERT: Learning Cross-Modality Encoder Representations from Transformers” (pronounced: ‘leksmert’). This framework is modeled after recent BERT-style innovations while further adapted to useful cr"
D19-1514,P19-1644,0,0.125153,"Missing"
D19-1514,W18-5446,0,0.155289,"er encoding layers. Word-Level Sentence Embeddings A sentence is first split into words {w1 , . . . , wn } with length of n by the same WordPiece tokenizer (Wu et al., 2016) in Devlin et al. (2019). Next, as shown in Fig. 1, the word wi and its index i (wi ’s absolute position in the sentence) are projected to vectors by embedding sub-layers, and then added to the index-aware word embeddings: w ˆi = WordEmbed (wi ) u ˆi = IdxEmbed (i) hi = LayerNorm (w ˆi + u ˆi ) Object-Level Image Embeddings Instead of using the feature map output by a convolutional neural network, we follow Anderson et al. (2018) in taking the features of detected objects as the embeddings of images. Specifically, the object detector detects m objects {o1 , . . . , om } from the image (denoted by bounding boxes on the image in Fig. 1). Each object oj is represented by its position feature (i.e., bounding box coordinates) pj and its 2048-dimensional region-of-interest (RoI) feature fj . Instead of directly using the RoI feature fj without considering its position pj in Anderson et al. (2018), we learn a position-aware embedding vj by adding outputs of 2 fully-connected layers: Model Architecture fˆj = LayerNorm (WF fj"
D19-1514,1983.tc-1.13,0,0.162856,"Missing"
K18-1047,D16-1127,0,0.199886,"e sequence that should not change the response), we demonstrate that it is equally valuable to consider ShouldChange strategies (i.e., semantics-changing, intentional perturbations to the source sequence that should change the response). We investigate three state-of-the-art models on two task-oriented dialogue datasets. Concretely, we propose and evaluate five naturally motivated and increasingly complex Should-NotChange and five Should-Change adversarial strategies on the VHRED (Variational Hierarchical Encoder-Decoder) model (Serban et al., 2017b) and the RL (Reinforcement Learning) model (Li et al., 2016) with the Ubuntu Dialogue CorWe present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as ShouldChange strategies that test if a model is overstable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a maxmargin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial"
K18-1047,D16-1230,0,0.0371789,"ntext has not changed much in meaning. Secondly, for each Should-NotChange strategy, the cosine similarity of context is much higher than that of response, indicating that responses change more significantly in meaning than their corresponding contexts. Lastly, The high semantic similarity for Generative Paraphrasing also partly shows that the Pointer-Generator model in general produces faithful paraphrases. Semantic Similarity In addition to F1, we also follow Serban et al. (2017a) and employ cosine similarity between average embeddings of normal and adversarial inputs/responses (proposed by Liu et al. (2016)) to evaluate how much the inputs/responses change in semantic meaning (Table 4). This metric is useful in three ways. Firstly, by comparing the two columns of context similarity, we can get a general idea of how much change is perceived by each model. For example, we can see that Stopword Dropout leads to more evident changes from VHRED’s perspective than from Reranking-RL’s. This also agrees with the F1 results in Table 2 and 3, which indicate Human Evaluation As introduced in Section 5, we performed two human studies on adversarial training and Generative Paraphrasing. For the first study,"
K18-1047,W15-4640,0,0.114542,"ecently, there has been substantial work on adversarial attacks in computer vision and NLP. Unlike vision, where one can simply add in imperceptible perturbations without changing an image’s meaning, carrying out such subtle changes in text is harder since text is discrete in nature (Jia and We publicly release all our code and data at https: //github.com/WolfNiu/AdversarialDialogue 486 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 486–496 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics pus (Lowe et al., 2015), and Dynamic Knowledge Graph Network with the Collaborative Communicating Agents (CoCoA) dataset (He et al., 2017). On the Should-Not-Change side for the Ubuntu task, we introduce adversarial strategies of increasing linguistic-unit complexity – from shallow word-level errors, to phrase-level paraphrastic changes, and finally to syntactic perturbations. We first propose two rule-based perturbations to the source dialogue context, namely Random Swap (randomly transposing neighboring tokens) and Stopword Dropout (randomly removing stopwords). Next, we propose two data-level strategies that leve"
K18-1047,P17-1162,0,0.0739075,"can simply add in imperceptible perturbations without changing an image’s meaning, carrying out such subtle changes in text is harder since text is discrete in nature (Jia and We publicly release all our code and data at https: //github.com/WolfNiu/AdversarialDialogue 486 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 486–496 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics pus (Lowe et al., 2015), and Dynamic Knowledge Graph Network with the Collaborative Communicating Agents (CoCoA) dataset (He et al., 2017). On the Should-Not-Change side for the Ubuntu task, we introduce adversarial strategies of increasing linguistic-unit complexity – from shallow word-level errors, to phrase-level paraphrastic changes, and finally to syntactic perturbations. We first propose two rule-based perturbations to the source dialogue context, namely Random Swap (randomly transposing neighboring tokens) and Stopword Dropout (randomly removing stopwords). Next, we propose two data-level strategies that leverage existing parallel datasets in order to simulate more realistic, diverse noises: namely, Data-Level Paraphrasin"
K18-1047,N18-1170,0,0.136673,"(ti |si )) i where LML is the maximum likelihood loss, LMM is the max-margin loss, α is the weight of the maxmargin loss (set to 1.0 following Yu et al. (2017)), M is the margin (tuned be to 0.1), and ti , si and ai are the target sequence, normal input, and adversarial input, respectively.7 Adversarial Training for Should-Not-Change Strategies 5 For each Should-Not-Change strategy, we take an already trained model from a certain checkpoint,6 and train it on the adversarial inputs with maximum likelihood loss for K epochs (Shalyminov et al., 2017; Belinkov and Bisk, 2018; Jia and Liang, 2017; Iyyer et al., 2018). By feeding “adversarial source sequence + ground-truth response pairs” as regular positive data, we teach the model that these pairs are also valid examples despite the added perturbations. 4.2 Activity F1 1.18 4.34 4.63 5.94 5.67 Experimental Setup In addition to datasets, tasks, models and evaluation methods introduced in Section 2, we present training details in this section (see Appendix for a comprehensive version). Models on Ubuntu: We implemented VHRED and Reranking-RL in TensorFlow (Abadi et al., 2016) and employed greedy search for inference. As shown in Table 1, for both models we"
K18-1047,P15-2070,0,0.0521186,"Missing"
K18-1047,D18-1316,0,0.0319072,"es not have any access to the model outputs when creating adversarial inputs, and is thus more generalizable across models/tasks. We adopt the pure-model-agnostic approach, only drawing inspiration from real-world noise, and testing them on the target model. Adversarial in NLP: Text-based adversarial works have targeted both classification models (Weston et al., 2016; Jia and Liang, 2017; Wong, 2017; Liang et al., 2017; Samanta and Mehta, 2017; Shalyminov et al., 2017; Gao et al., 2018; Iyyer et al., 2018) and generative models (Hosseini et al., 2017; Henderson et al., 2017; Mironenco et al.; Zhao et al., 2018; Belinkov and Bisk, 2018). To our best knowledge, our work is the first to target generative goal-oriented dialogue systems with several new adversarial strategies in both Should-Not-Change and Should-Change categories, and then to fix the broken models through adversarial training (esp. using max-margin loss for Should-Change), and also achieving model robustness without using any adversarial data. Table 9: Activity, Entity F1 results of VHRED model vs. BPE-VHRED model tested on normal inputs. erative dialogue task. Moreover, BPE-VHRED achieved (5.86, 3.54) on Grammar Errors based adversaria"
K18-1047,P17-1099,0,0.0395832,"their paraphrases (e.g., “She bought a bike” to “She purchased a bicycle”). (4) Generative-level Paraphrasing: Although Data-level Paraphrasing provides us with semantic-preserving inputs most of the time, it still suffers from the fact that the validity of a paraphrase depends on the context, especially for words with multiple meanings. In addition, simply replacing word-by-word does not lead to new compositional sentence-level paraphrases, e.g., “How old are you” to “What’s your age”. We thus also experiment with generative-level paraphrasing, where we employ the PointerGenerator Networks (See et al., 2017), and train it on the recently published paraphrase dataset ParaNMT-5M (Wieting and Gimpel, 2017) which contains 5 millions paraphrase pairs. (5) Grammar Errors: We repurpose the AESW dataset (Daudaravicius, 2015), text extracted from 9, 919 published journal articles with data before/after language editing. This dataset was used for training models that identify and correct grammar errors. Based on the corrections in the edits, we build a look-up table to replace each correct word/phrase with a wrong one (e.g., “He doesn’t like cakes” to “He don’t like cake”). We applied all the above success"
K18-1047,P16-1162,0,0.0378476,"Pair-Encoding VHRED Although we have shown that adversarial training on most strategies makes the dialogue model more robust, generating such perturbed data is not always straightforward for diverse, complex strategies. For example, our data-level and generativelevel strategies all leverage datasets that are not always available to a language. We are thus motivated to also address the robustness task on the model-level, and explore an extension to the VHRED model that makes it robust to Grammar Errors even without adversarial training. Model Description: We performed Byte Pair Encoding (BPE) (Sennrich et al., 2016) on the Ubuntu dataset.13 This algorithm encodes rare/unknown words as sequences of subword units, which helps segmenting words with the same lemma but different inflections (e.g., “showing” to “show + ing”, and “cakes” to “cake + s”), making the model more likely to be robust to grammar errors such as verb tense or plural/singular noun confusion. We experimented BPE with 5K merging operations, and obtained a vocabulary size of 5121. Results: As shown in Table 9, BPE-VHRED achieved F1’s (5.99, 3.66), which is stat. equal to (5.94, 3.52) obtained without BPE. To our best knowledge, we are the f"
N09-1026,J98-2004,0,0.0963594,"Missing"
N09-1026,P05-1033,0,0.212091,"Missing"
N09-1026,D07-1079,0,0.0420296,"Missing"
N09-1026,N04-1035,0,0.470513,"Missing"
N09-1026,P06-1121,0,0.278456,"Missing"
N09-1026,P07-1019,0,0.0526295,"nts. These parameters were tuned on a development set. 233 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems. Recent innovations have managed the complexity of LM integration using multi-pass architectures. Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM. Petrov et al. (2008) describes an additional coarse-to-fine hierarchy over language projections. Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search. As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest. In this section, we show that the coarse-to-fine idea of constraining each pass using marginal predictions of the previous pass also applies effectively to cube growing. Max marginal predictions from the parse can substantially reduce LM integration time. 6.1 Language Model Forest Reranking Parsing produces a forest of derivations, where each edge in the forest holds its Viterbi (or one-best) derivation under"
N09-1026,P03-1054,1,0.0267897,"quires. (b) Chomsky S ! NNPnormal no dabaform, una bofetada DT NN verde Because transducer rules are very flat and contain speS cific lexical items, binarization introduces a large number of intermediate grammar symbols. Rule size affectDT parsing complexity whether (c) and lexicalization NNP NN the grammar is binarized explicitly (Zhang et al., did not binarized slap the green witch 2006) orMary implicitly using Early-style intermediate symbols (Zollmann et al., 2006). Moreover, no daba una bofetada a la bruja verde theMaria resulting binary rules cannot be Markovized to merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the Right-branching 8,095 grammar, a common technique in syntactic machine Left-branching translation. For instance, Zollmann 5,871 et al. (2006) follow Chiang (2005) in disallowing adjacent nonGreedy 1,101 terminals. Optimal (ILP) Watanabe 443 et al. (2006) limit grammars to Griebach-Normal form. However, general tree 0 3,000 excellent 6,000 translation 9,000 transducer grammars provide performance (Galley et al., 2006), and so we focus on parsing with all availabl"
N09-1026,D07-1104,0,0.0305308,"mic program is organized into spans Sij and computes the Viterbi score w(i, j, X) for each edge Sij [X], the weight of the maximum parse over words i+1 to j, rooted at symbol X. For each Sij , computation proceeds in three phases: binary, lexical, and unary. 4.1 w(i, j, X) = max(wl (i, j, X), wb (i, j, X)). To efficiently compute mappings, we store lexical rules in a trie (or suffix array) – a searchable graph that indexes rules according to their sequence of lexical items and non-terminals. This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007). To find all rules that map onto a span, we traverse the trie using depth-first search. 4.3 max r=X→X1 X2 w(i, j, X) = j−1 ωr max w(i, k, X1 ) · w(k, j, X2 ). k=i+1 The quantities w(i, k, X1 ) and w(k, j, X2 ) will have already been computed by the dynamic program. The work in this phase is cubic in sentence length. Applying Lexical Rules On the other hand, lexical rules in LNF can be applied without binarization, because they only apply to particular spans that contain the appropriate lexical items. For a given Sij , we first compute all the legal mappings of each rule onto the span. A mappi"
N09-1026,W06-1606,0,0.0341851,"Missing"
N09-1026,N07-1051,1,0.838206,"Missing"
N09-1026,P06-1055,1,0.723838,"ogram of the size of rules applicable to a typical 30-word sentence appears in Figure 2. The grammar includes 149 grammatical symbols, an augmentation of the Penn Treebank symbol set. To evaluate, we decoded 300 sentences of up to 40 words in length from the NIST05 Arabic-English test set. 3 Efficient Grammar Encodings Monolingual parsing with a source-projected transducer grammar is a natural first pass in multi-pass decoding. These grammars are qualitatively different from syntactic analysis grammars, such as the lexicalized grammars of Charniak (1997) or the heavily state-split grammars of Petrov et al. (2006). 70,000 52,500 35,000 17,500 0 NNP1 did not slap DT2 green NN3 (a) InSthis ! section, we develop an appropriate grammar encoding that NNP daba unaefficient bofetadaparsing. a DT2 NN3 verde 1 noenables 7+ It is problematic to convert these grammars into which aCKY requires. (b) Chomsky S ! NNPnormal no dabaform, una bofetada DT NN verde Because transducer rules are very flat and contain speS cific lexical items, binarization introduces a large number of intermediate grammar symbols. Rule size affectDT parsing complexity whether (c) and lexicalization NNP NN the grammar is binarized explicitly"
N09-1026,D08-1012,1,0.9397,"be a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, an"
N09-1026,D08-1018,0,0.183564,"o ensure that the sequences X1 . . . Xk and Xk+1 . . . Xn can be constructed. As baselines, we used left-branching (where k = 1 always) and right-branching (where k = n − 1) binarizations. We also tested a greedy binarization approach, choosing k to minimize the number of grammar symbols introduced. We first try to select k such that both X1:k and Xk+1:n are already in the grammar. If no such k exists, we select k such that one of the intermediate types generated is already  used. If no such k exists again, we choose k = 21 n . This policy only creates new intermediate types when necessary. Song et al. (2008) propose a similar greedy approach to binarization that uses corpus statistics to select common types rather than explicitly reusing types that have already been introduced. Finally, we computed an optimal binarization that explicitly minimizes the number of symbols in the resulting grammar. We cast the minimization as an integer linear program (ILP). Let V be the set of all base non-terminal symbols in the grammar. We introduce an indicator variable TY for each symbol Y ∈ V + to indicate that Y is used in the grammar. Y can be either a base non-terminal symbol Xi or an intermediate symbol X1:"
N09-1026,N07-1063,0,0.0332326,"Missing"
N09-1026,P06-1098,0,0.0486143,"Missing"
N09-1026,J97-3002,0,0.0777932,"me. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, and each divergence expands the minimal domain of translation rules, so rules are large and flat. Finally, we see most rules very few times,"
N09-1026,P08-1025,0,0.122277,"runing scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, and each divergence expands"
N09-1026,N06-1033,0,0.278249,"Missing"
N09-1026,W06-3119,0,0.0979856,"Missing"
N09-1026,J99-4004,0,\N,Missing
N12-1095,E09-1010,0,0.0205909,"xplored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference s"
N12-1095,P98-1012,0,0.0486869,"of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clustering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. After defining each constraint below, we briefly describe its relevance to the translation sense clustering ta"
N12-1095,J92-4003,0,0.0504551,"anslations of the Spanish source word colocar that appear in our input dataset. Clustering with K-Means 4 In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the featu"
N12-1095,J93-2003,0,0.0638348,"with usage examples. 1 Dekang Lin Google lindek@google.com Figure 1: This excerpt from a bilingual dictionary groups English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses (Vel´azquez de la Cadena et al., 1965). Introduction The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing. Probabilistic word alignment models can induce bilexical distributions over target-language translations of source-language words (Brown et al., 1993). However, word-to-word correspondences do not capture the full structure of a bilingual lexicon. Consider the example bilingual dictionary entry in Figure 1; in addition to enumerating the translations of a word, the dictionary author has grouped those translations into three sense clusters. Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers. ∗ Author was a summer intern with Google Research while conducting this research project. This paper formalizes the task of clustering a set of translation"
N12-1095,P02-1033,0,0.0598201,"k To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised so"
N12-1095,N03-1015,0,0.0600517,"translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means c"
N12-1095,N03-1017,0,0.00542878,"700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Ro"
N12-1095,P09-1116,1,0.590238,"statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional con"
N12-1095,P03-1058,0,0.199405,"sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method"
N12-1095,D07-1043,0,0.115773,"3). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clustering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. Aft"
N12-1095,D11-1095,0,0.0160154,"ons of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt ). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and"
N12-1095,C04-1192,0,0.0406943,"task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively reco"
N12-1095,P08-1086,0,0.114418,"with K-Means 4 In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Li"
N12-1095,C10-1124,0,0.0119987,"of-speech-tag distribution of 38 nouns, 10 verbs and 4 adverbs. The J→E dataset has 369 sourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001)"
N12-1095,W09-0210,0,0.0168583,"d to a list of up to M clusters. To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt ). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The unde"
N12-1095,C96-2141,0,0.30935,"ourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). F"
N12-1095,C98-1012,0,\N,Missing
N15-1028,N09-1003,0,0.0553204,"ing the word aligner in cdec (Dyer et al., 2010) run on the WMT0610 news commentary corpora and Europarl. After training, we apply the learned CCA/DCCA projection mappings to the original English word embeddings (180K words) and use these transformed embeddings for our evaluation tasks. 3.1 Evaluation Tasks We compare our DCCA-based embeddings to the original word vectors and to CCA-based em1 www.statmt.org/wmt11/ beddings on several tasks. We use WordSim353 (Finkelstein et al., 2001), which contains 353 English word pairs with human similarity ratings. It is divided into WS-SIM and WS-REL by Agirre et al. (2009) to measure similarity and relatedness. We also use SimLex-999 (Hill et al., 2014), a new similarity-focused dataset consisting of 666 noun pairs, 222 verb pairs, and 111 adjective pairs. Finally, we use the bigram similarity dataset from Mitchell and Lapata (2010) which has 3 subsets, adjective-noun (AN), noun-noun (NN), and verbobject (VN), and dev and test sets for each. For the bigram task, we simply add the word vectors output by CCA or DCCA to get bigram vectors.2 All task datasets contain pairs with human similarity ratings. To evaluate embeddings, we compute cosine similarity between t"
N15-1028,N12-1095,1,0.859014,"e idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) technique of Andrew et al. (2013) to learn nonline"
N15-1028,P14-2131,1,0.807197,"this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 20"
N15-1028,D13-1167,0,0.00836244,"thout using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data. Acknowledgments We are grateful to Manaal Faruqui for sharing resources, and to Chris Dyer, David Sontag, Lyle Ungar, and anonymous reviewers for helpful input"
N15-1028,P02-1033,0,0.16235,"012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) tec"
N15-1028,P10-4002,0,0.0115371,"CCA and KCCA on a speech recognition task. 3 Experiments We use English and German as our two languages. Our original monolingual word vectors are the same as those used by Faruqui and Dyer (2014). They are 640-dimensional and are estimated via latent semantic analysis on the WMT 2011 monolingual news corpora.1 We use German-English translation pairs as the input to CCA and DCCA, using the same set of 36K pairs as used by Faruqui and Dyer. These pairs contain, for each of 36K English word types, the single most frequently aligned German word. They were obtained using the word aligner in cdec (Dyer et al., 2010) run on the WMT0610 news commentary corpora and Europarl. After training, we apply the learned CCA/DCCA projection mappings to the original English word embeddings (180K words) and use these transformed embeddings for our evaluation tasks. 3.1 Evaluation Tasks We compare our DCCA-based embeddings to the original word vectors and to CCA-based em1 www.statmt.org/wmt11/ beddings on several tasks. We use WordSim353 (Finkelstein et al., 2001), which contains 353 English word pairs with human similarity ratings. It is divided into WS-SIM and WS-REL by Agirre et al. (2009) to measure similarity and r"
N15-1028,E14-1049,0,0.74059,"similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) technique of Andrew et al. (2013) to learn nonlinear transformations of two languages’ embeddings that a"
N15-1028,P08-1088,0,0.0870681,"d´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks"
N15-1028,J14-1004,0,0.012743,"om two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002;"
N15-1028,N13-1056,0,0.0166863,"r and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, co"
N15-1028,C12-1089,0,0.0603216,"ingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and bigram similarity tasks. Future work could compare DCCA to other nonlinear approaches discussed in §5, compare different languages as multiview context, and extend to aligned phrase pairs, and to unaligned data. Acknowledgments We are grateful to Manaal Faruqui for sharing resources, and to Chris Dyer, David Sontag, Lyle Ungar, and anonymous review"
N15-1028,P14-2037,0,0.344264,"Missing"
N15-1028,W02-0902,0,0.103506,"models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embeddings via deep canonical correlation analysis (DCCA). The DCCA embeddings consistently outperform linear CCA embeddings on word and"
N15-1028,P08-1068,0,0.0314638,"example via canonical correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational"
N15-1028,W13-3512,0,0.0293337,"Missing"
N15-1028,N04-1043,0,0.0307805,"e their quality, for example via canonical correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that ad"
N15-1028,N10-1135,0,0.151153,"Missing"
N15-1028,P00-1054,0,0.0867822,"se-like similarity and thereby discourage the similarity of hypernym-hyponym pairs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. 5 Related work Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008);"
N15-1028,N12-1052,0,0.0465392,"Missing"
N15-1028,P10-1040,0,0.140012,"cal correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 1 Introduction Learned word representations are widely used in NLP tasks such as tagging, named entity recognition, and parsing (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012; Huang et al., 2014; Bansal et al., 2014). The idea in such representations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in b"
N15-1028,D13-1168,0,0.091007,"hao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used a supervised learning method with multiple monolingual signals. Finally, other work on CCA and spectral methods has been used in the context of other types of views (Collobert and Weston, 2008; Dhillon et al., 2011; Klementiev et al., 2012; Chang et al., 2013). 6 Conclusion We have demonstrated how bilingual information can be incorporated into word embedd"
N15-1028,P06-2124,0,0.00841971,"irs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. 5 Related work Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens, 2013). This research is also related to early examples of learning bilingual lexicons using monolingual corpora (Koehn and Knight, 2002; Haghighi et al., 2008); the latter used CCA to find matched word pairs. Irvine and Callison-Burch (2013) used"
N15-1028,W05-0804,0,0.0353416,"de them less similar than the original vectors, and DCCA made them less similar still. This matches our intuition that bilingual information should encourage paraphrase-like similarity and thereby discourage the similarity of hypernym-hyponym pairs. Visualizations We visualized several synonymantonym word lists and often found that DCCA more cleanly separated synonyms from antonyms than CCA or the original vectors. An example of the clearest improvement is shown in Fig. 2. 5 Related work Previous work has successfully used translational context for word representations (Diab and Resnik, 2002; Zhao et al., 2005; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Faruqui and Dyer, 2014), including via hand-designed vector space models (Peirsman and Pad´o, 2010; Sumita, 2000) or via unsuper254 vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning approaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest in learning bilingual representations without using word alignments (Chandar et al., 2014; Gouws et al., 2014; Koˇcisk`y et al., 2014; Vulic and Moens,"
N15-1028,D13-1141,0,0.555013,"entations is that words with similar context have similar meaning, and hence should be nearby in a clustering or vector space. Continuous representations are learned with neural language models (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). The context used to learn these representations is typically the set of nearby words of each word occurrence. Prior work has found that adding translational context results in better representations (Diab and Resnik, 2002; T¨ackstr¨om et al., 2012; Bansal et al., 2012; Zou et al., 2013). Recently, Faruqui and Dyer (2014) applied canonical correlation analysis (CCA) to word embeddings of two languages, and found that the resulting embeddings represent word similarities better than the original monolingual embeddings. In this paper, we follow the same intuition as Faruqui and Dyer (2014) but rather than learning linear transformations with CCA, we permit the correlated information to lie in nonlinear subspaces of the original embeddings. We use the recently proposed deep canonical correlation analysis (DCCA) technique of Andrew et al. (2013) to learn nonlinear transformations"
N15-1028,J15-4004,0,\N,Missing
N16-1086,D10-1049,0,0.281464,"two subproblems: content selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks Matthew R. Walter TTI-Chicago mwalter@ttic.edu jointly is challenging due to the uncertainty in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012). However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence. We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize"
N16-1086,H05-1042,0,0.854233,"here we get results competitive with or better than state-of-the-art, despite being extremely data-starved. 2 Related Work Selective generation is a task where a natural language description is produced for a salient subset of a rich world state represented as an over-determined database of event records. A good deal of attention in this area has been paid to the individual content selection and selective realization subproblems. With regards to the former, Barzilay and Lee (2004) model the content structure from unanno721 tated documents and apply it to the application of text summarization. Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trai"
N16-1086,N04-1015,0,0.0103975,"ach. Finally, we demonstrate the generalizability of our model by directly applying it to a benchmark sportscasting dataset (ROBO C UP), where we get results competitive with or better than state-of-the-art, despite being extremely data-starved. 2 Related Work Selective generation is a task where a natural language description is produced for a salient subset of a rich world state represented as an over-determined database of event records. A good deal of attention in this area has been paid to the individual content selection and selective realization subproblems. With regards to the former, Barzilay and Lee (2004) model the content structure from unanno721 tated documents and apply it to the application of text summarization. Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a"
N16-1086,C10-2062,0,0.890756,"ent selection, which involves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks Matthew R. Walter TTI-Chicago mwalter@ttic.edu jointly is challenging due to the uncertainty in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012). However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence. We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains. Furth"
N16-1086,N12-1093,0,0.648326,"nvolves choosing a subset of relevant records to talk about from the exhaustive database, and surface realization, which is concerned with generating natural language descriptions for this subset. Learning to perform these tasks Matthew R. Walter TTI-Chicago mwalter@ttic.edu jointly is challenging due to the uncertainty in deciding which records are relevant, the complex dependencies between selected records, and the multiple ways in which these records can be described. Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2012). However, most approaches solve the two content selection and surface realization subtasks separately, use manual domain-dependent resources (e.g., semantic parsers) and features, or employ template-based generation. This limits domain adaptability and reduces coherence. We take an alternative, neural encoder-aligner-decoder approach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains. Further, our memorybased model c"
N16-1086,D13-1157,0,0.433418,"s on their choice of features for the log-linear model, while the template-based generation further employs some domain-specific features for fluent output. Konstas and Lapata (2012) propose an alternative method that simultaneously optimizes the content selection and surface realization problems. They employ a probabilistic context-free grammar that specifies the structure of the event records, and then treat generation as finding the best derivation tree according to this grammar. However, their method still selects and orders records in a local fashion via a Markovized chaining of records. Konstas and Lapata (2013) improve upon this approach with global document representations. However, this approach also requires alignment during training, which they estimate using the method of Liang et al. (2009). We treat the problem of selective generation as end-to-end learning via a recurrent neural network encoder-aligner-decoder model, which enables us to jointly learn content selection and surface realization directly from database-text pairs, without the need for an external aligner or ground-truth selection labels. The use of LSTM-RNNs enables our model to capture the long-range dependencies that exist amon"
N16-1086,P09-1011,0,0.578163,"age description is produced for a salient subset of a rich world state represented as an over-determined database of event records. A good deal of attention in this area has been paid to the individual content selection and selective realization subproblems. With regards to the former, Barzilay and Lee (2004) model the content structure from unanno721 tated documents and apply it to the application of text summarization. Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for context planning and dialog, relying upon various linguistics features. Soricut and Marcu (20"
N16-1086,D11-1149,0,0.0352458,"es and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for context planning and dialog, relying upon various linguistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDLrepresentation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a loglinear framework (Angeli et al., 2010). Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records usin"
N16-1086,D08-1082,0,0.0419294,"e grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a loglinear framework (Angeli et al., 2010). Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009), and then produces the text based on the generation system of Wong and Mooney (2007). Angeli et al. (2010) propose a unified conceptto-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model. Similar to other work, they train their model using external alignments from Liang et al. (2009). Generation then follows as inference over this model, where they first choose an event record, then the record’s fields (i.e., attributes), and finally a set of templates that they then fill in with word"
N16-1086,D09-1042,0,0.0249623,"r context planning and dialog, relying upon various linguistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDLrepresentation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a loglinear framework (Angeli et al., 2010). Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their corresponding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009), and then produces the text based on the generation system of Wong and Mooney (2007). Angeli et"
N16-1086,2001.mtsummit-papers.68,0,0.0128843,"Missing"
N16-1086,P06-1139,0,0.0800643,". Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for context planning and dialog, relying upon various linguistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDLrepresentation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a loglinear framework (Angeli et al., 2010). Recen"
N16-1086,P04-1011,0,0.101958,"ion. Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for context planning and dialog, relying upon various linguistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDLrepresentation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realiz"
N16-1086,N01-1003,0,0.0278183,"cation of text summarization. Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for context planning and dialog, relying upon various linguistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDLrepresentation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars t"
N16-1086,N07-1022,0,0.136879,"ext sequences into utterances and associates each to the corresponding record. Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for context planning and dialog, relying upon various linguistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDLrepresentation, a formalism used to compactly represent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) employs probabilistic context-free grammars to perform surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a loglinear framework (Angeli et al., 2010). Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their correspondi"
N16-1086,P02-1040,0,\N,Missing
N16-1118,N09-1003,0,0.270505,"Missing"
N16-1118,P14-2131,1,0.523782,"Missing"
N16-1118,S13-2050,0,0.0573775,"Missing"
N16-1118,D14-1082,0,0.025131,"Missing"
N16-1118,D13-1203,0,0.0106208,"Missing"
N16-1118,E14-1049,0,0.125005,"Missing"
N16-1118,P13-2121,0,0.0130981,"Missing"
N16-1118,D15-1242,0,0.263547,"Missing"
N16-1118,Q14-1041,0,0.0675169,"Missing"
N16-1118,P14-2050,0,0.468834,"Missing"
N16-1118,Q15-1016,0,0.310476,"Missing"
N16-1118,D15-1161,0,0.022211,"Missing"
N16-1118,N15-1142,0,0.0221778,"Missing"
N16-1118,N15-1028,1,0.898226,"Missing"
N16-1118,P14-5010,1,0.0192857,"Missing"
N16-1118,W14-1619,1,0.893963,"Missing"
N16-1118,N15-1050,1,0.737216,"Missing"
N16-1118,W15-1501,1,0.534649,"Missing"
N16-1118,D14-1162,0,0.106006,"Missing"
N16-1118,W12-4501,0,0.029725,"Missing"
N16-1118,D15-1036,0,0.148548,"Missing"
N16-1118,D13-1170,0,0.00373521,"Missing"
N16-1118,W03-0419,0,0.105084,"Missing"
N16-1118,D15-1243,0,0.0772053,"Missing"
N16-1118,P10-1040,0,0.172134,"Missing"
N16-1118,D12-1086,0,0.093889,"Missing"
N16-1118,J15-4004,0,\N,Missing
N16-1118,S13-1005,0,\N,Missing
N18-1007,N16-1024,0,0.0388215,"ay have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody. Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline. However, the prosody modeling com"
N18-1007,1993.eamt-1.1,0,0.608238,"Missing"
N18-1007,Q13-1006,0,0.0167593,"an improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016). Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency"
N18-1007,P06-1055,0,0.121059,"Missing"
N18-1007,P06-1021,0,0.716489,"r, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques. The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007). The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation. Our work offers the following contributions. We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure. We demonstrate improvements"
N18-1007,H91-1073,1,0.371999,"rical Engineering, University of Washington 2 Toyota Technological Institute at Chicago 3 Department of Computer Science, UNC Chapel Hill {ttmt001, ostendor}@uw.edu, mbansal@cs.unc.edu, {shtoshni, kgimpel, klivescu}@ttic.edu Abstract Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation. Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991). Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994). However, most speech parsing systems in practice take little advantage of these cues. Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment,"
N18-1007,D13-1013,0,0.587645,"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments. Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model. Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014). ∗ Equal Contribution. 69 Proceedings of NAACL-HLT 2018, pages 69–81 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors. 2 decoder hidden state at time step t, which captures the previous output sequence context y<t . uit = v > tanh(W 1 hi + W 2 dt + ba ) Task and Model Description αt = softmax(ut ) Our model maps a sequence of word-level input fe"
N18-1007,Q14-1011,0,0.616409,"parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments. Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model. Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014). ∗ Equal Contribution. 69 Proceedings of NAACL-HLT 2018, pages 69–81 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors. 2 decoder hidden state at time step t, which captures the previous output sequence context y<t . uit = v > tanh(W 1 hi + W 2 dt + ba ) Task and Model Description αt = softmax(ut ) Our model maps a sequence of word-level input features to a linearized parse"
N18-1007,H05-1030,1,0.848641,"s in a neural parser, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques. The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007). The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation. Our work offers the following contributions. We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure. We demons"
N18-1007,D17-1296,0,0.24349,"dule; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1). However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model). Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017). Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement. a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had> my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal. Further, the"
N18-1007,P15-1113,0,0.0224504,"ror suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody. Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline. However, the"
N18-1007,N04-4032,1,0.423253,"act the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody. Not included in this analysis are sentence boundary errors, which also change the “gold” parse. Thus, prosody may be more useful than results here indicate. 6 It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing. A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007). Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks. The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies. (In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.) Our analyses of the impact of prosody also extends prior work. Related Work Related work on parsing conversational speech has mainly addressed four probl"
N18-1007,D16-1109,0,0.187986,"n interruption point. Repairs often involve parallel grammatical Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012). In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had 76 obtained smaller gains. Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016). Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural"
N18-1007,D12-1096,0,0.0498214,"r benefit from prosody in the fluent set. tachment errors. Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment. Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser. Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word. Types of errors. We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences. The two models differ in the types of error reductions they provide. Including pause information gives largest improvements on PP attachment and Modifier atEffect of transcription errors. The results and analyses so far have assumed that we have reliable transcripts. In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio"
N18-2071,P17-2034,0,0.48256,", a joint-bidirectional attention flow model builds consistent, two-way matchings between the representations in different domains. Finally, since the scores computed by the bidirectional attention are about the three sub-images, a pooling combination layer over the three subimage representations is required to give the final score of the whole image. Introduction Visual Reasoning (Antol et al., 2015; Andreas et al., 2016; Bisk et al., 2016; Johnson et al., 2017) requires a sophisticated understanding of the compositional language instruction and its relationship with the corresponding image. Suhr et al. (2017) recently proposed a challenging new NLVR task and dataset in this direction with natural and complex language statements that have to be classified as true or false given a multi-image set (shown in Fig. 1). Specifically, each task instance consists of an image with three sub-images and a statement which describes the image. The model is asked to answer the question whether the given statement is consistent with the image or not. To solve the task, the designed model needs to fuse the information from two different domains, On the structured-object-representation version of the dataset, our p"
N18-2071,N16-1089,0,0.165081,"based policy gradient method with a reward extracted from the subsequent comprehension model. With these strong representations of the visual objects and the statement units, a joint-bidirectional attention flow model builds consistent, two-way matchings between the representations in different domains. Finally, since the scores computed by the bidirectional attention are about the three sub-images, a pooling combination layer over the three subimage representations is required to give the final score of the whole image. Introduction Visual Reasoning (Antol et al., 2015; Andreas et al., 2016; Bisk et al., 2016; Johnson et al., 2017) requires a sophisticated understanding of the compositional language instruction and its relationship with the corresponding image. Suhr et al. (2017) recently proposed a challenging new NLVR task and dataset in this direction with natural and complex language statements that have to be classified as true or false given a multi-image set (shown in Fig. 1). Specifically, each task instance consists of an image with three sub-images and a statement which describes the image. The model is asked to answer the question whether the given statement is consistent with the image"
N18-2091,P17-1018,0,0.0518394,"Missing"
N18-2091,P05-1014,0,0.016513,"e deeper methods for identifying/discarding the distractors, it only has limited ability in recognizing semantic differences because its current inputs do not capture crucial aspects of lexical semantics such as antonymy (which were inserted by Jia and Liang (2017) when generating the AddSent adversaries; see Sec. 3). Most current models use pretrained word embeddings (e.g., GloVE (Pennington et al., 2014) and ELMo (Peters et al., 2018)) as input, which are usually calculated based on the distributional hypothesis (Harris, 1954), and do not capture lexical semantic relations such as antonymy (Geffet and Dagan, 2005). These shortcomings are reflected by our results in Sec. 4.6, where we see that we can’t resolve all AddSent2 Note that for any fixed n, Y = n − X, but for our purposes it is easier to keep them separate since the length of the paragraph is also a random variable. 577 Training Original-SQuAD AddSent AddSentDiverse Original-SQuAD-Dev 84.65 83.76 83.49 AddSent 42.45 79.55 76.95 AddSentPrepend 41.46 51.96 77.45 AddSentRandom 40.48 59.03 76.02 AddSentMod 41.96 46.85 77.06 Average 50.20 64.23 78.19 Table 1: F1 performance of the BSAE model trained and tested on different regular/adversarial datase"
N18-2091,D17-1215,0,0.514299,"luation while maintaining performance on the regular SQuAD task. 1 Introduction We explore the task of reading comprehension based question answering (Q&A), where we focus on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), in which models answer questions about paragraphs taken from Wikipedia. Significant progress has been made with deep end to end neural-attention models, with some achieving above human level performance on the test set (Wang and Jiang, 2017; Seo et al., 2017; Wang et al., 2017; Huang et al., 2018; Peters et al., 2018). However, as shown recently by Jia and Liang (2017), these models are very fragile when presented with adversarially gener575 Proceedings of NAACL-HLT 2018, pages 575–581 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics cantly higher variance (by varying the locations where the distractors are placed and expanding the set of fake answers), so that the model is punished during training time for making these superficial assumptions about the distractor. We show that an AddSentDiverse-based adversariallytrained model beats an AddSent-trained model across 3 different adversarial test sets, showing an avera"
N18-2091,D14-1162,0,0.0843488,"ted the model from identifying distractors based on superficial clues such as location and fake answer identity by eliminating these correlations within the training data. But even if we force the model to learn some deeper methods for identifying/discarding the distractors, it only has limited ability in recognizing semantic differences because its current inputs do not capture crucial aspects of lexical semantics such as antonymy (which were inserted by Jia and Liang (2017) when generating the AddSent adversaries; see Sec. 3). Most current models use pretrained word embeddings (e.g., GloVE (Pennington et al., 2014) and ELMo (Peters et al., 2018)) as input, which are usually calculated based on the distributional hypothesis (Harris, 1954), and do not capture lexical semantic relations such as antonymy (Geffet and Dagan, 2005). These shortcomings are reflected by our results in Sec. 4.6, where we see that we can’t resolve all AddSent2 Note that for any fixed n, Y = n − X, but for our purposes it is easier to keep them separate since the length of the paragraph is also a random variable. 577 Training Original-SQuAD AddSent AddSentDiverse Original-SQuAD-Dev 84.65 83.76 83.49 AddSent 42.45 79.55 76.95 AddSen"
N18-2102,D14-1085,0,0.0779231,"dataset as well as strong improvements in a testonly transfer setup on DUC-2002. Lastly, we present several analyses of our model’s saliency, entailment, and abstractiveness skills. 2 Related Work 3 Earlier summarization work was based on extraction and compression-based approaches (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015), with more focus on graph-based (Giannakopoulos, 2009; Ganesan et al., 2010) and discourse tree-based (Gerani et al., 2014) models. Recent focus has shifted towards abstractive, rewriting-based summarization based on parse trees (Cheung and Penn, 2014; Wang et al., 2016), Abstract Meaning Representations (Liu et al., 2015; Dohare and Karnick, 2017), and neural network models with pointercopy mechanism and coverage (Rush et al., 2015; Chopra et al., 2016; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017), as well as reinforcebased metric rewards (Ranzato et al., 2015; Paulus et al., 2017). We also use reinforce-based models, but with novel reward functions and better simultaneous multi-reward optimization methods. Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral,"
N18-2102,D17-1223,0,0.0221616,"; Nallapati et al., 2016) made the abstractive summarization task more feasible and accurate, with recent ideas ranging from copypointer mechanism and redundancy coverage, to metric reward based reinforcement learning (Rush 646 Proceedings of NAACL-HLT 2018, pages 646–653 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics spans annotated by humans for important questions, serve as an interesting and effective proxy for keyphrase-style salient information in summarization. Some related previous work has incorporated document topic/subject classification (Isonuma et al., 2017) and webpage keyphrase extraction (Zhang et al., 2004) to improve saliency in summarization. Some recent work Subramanian et al. (2017) has also used answer probabilities in a document to improve question generation. reward combination), inspired from how humans take multiple concurrent types of rewards (feedback) to learn a task. Overall, our methods achieve the new state-of-the-art on the CNN/Daily Mail dataset as well as strong improvements in a testonly transfer setup on DUC-2002. Lastly, we present several analyses of our model’s saliency, entailment, and abstractiveness skills. 2 Related"
N18-2102,N16-1012,0,0.330552,"summarization work was based on extraction and compression-based approaches (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015), with more focus on graph-based (Giannakopoulos, 2009; Ganesan et al., 2010) and discourse tree-based (Gerani et al., 2014) models. Recent focus has shifted towards abstractive, rewriting-based summarization based on parse trees (Cheung and Penn, 2014; Wang et al., 2016), Abstract Meaning Representations (Liu et al., 2015; Dohare and Karnick, 2017), and neural network models with pointercopy mechanism and coverage (Rush et al., 2015; Chopra et al., 2016; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017), as well as reinforcebased metric rewards (Ranzato et al., 2015; Paulus et al., 2017). We also use reinforce-based models, but with novel reward functions and better simultaneous multi-reward optimization methods. Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral, has been used for Q&A and IE tasks (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Recent neural network models and large datasets (Bowman et al., 2015; Wi"
N18-2102,D17-1103,1,0.934156,"n methods. Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral, has been used for Q&A and IE tasks (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Recent neural network models and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled stronger accuracies. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of RTE by modeling graphbased relationships between sentences to select the most non-redundant sentences for summarization. Recently, Pasunuru and Bansal (2017) improved video captioning with entailment-corrected rewards. We instead directly use multi-sentence entailment knowledge (with additional length constraints) as a separate RL reward to improve abstractive summarization, while avoiding their penalty hyperparameter tuning. For our saliency prediction model, we make use of the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), where the answer 3.1 Models Baseline Sequence-to-Sequence Model Our abstractive text summarization model is a simple sequence-to-sequence single-layer bidirectional encoder and unidirectional decoder LSTMRNN, wi"
N18-2102,D16-1264,0,0.578388,"revious work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of RTE by modeling graphbased relationships between sentences to select the most non-redundant sentences for summarization. Recently, Pasunuru and Bansal (2017) improved video captioning with entailment-corrected rewards. We instead directly use multi-sentence entailment knowledge (with additional length constraints) as a separate RL reward to improve abstractive summarization, while avoiding their penalty hyperparameter tuning. For our saliency prediction model, we make use of the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), where the answer 3.1 Models Baseline Sequence-to-Sequence Model Our abstractive text summarization model is a simple sequence-to-sequence single-layer bidirectional encoder and unidirectional decoder LSTMRNN, with attention (Bahdanau et al., 2015), pointer-copy, and coverage mechanism – please refer to See et al. (2017) for details. 3.2 Policy Gradient Reinforce Traditional cross-entropy loss optimization for sequence generation has an exposure bias issue and the model is not optimized for the evaluated metrics (Ranzato et al., 2015). Reinforce-based policy gradient approach addresses both o"
N18-2102,D15-1044,0,0.241677,"ted Work 3 Earlier summarization work was based on extraction and compression-based approaches (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015), with more focus on graph-based (Giannakopoulos, 2009; Ganesan et al., 2010) and discourse tree-based (Gerani et al., 2014) models. Recent focus has shifted towards abstractive, rewriting-based summarization based on parse trees (Cheung and Penn, 2014; Wang et al., 2016), Abstract Meaning Representations (Liu et al., 2015; Dohare and Karnick, 2017), and neural network models with pointercopy mechanism and coverage (Rush et al., 2015; Chopra et al., 2016; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017), as well as reinforcebased metric rewards (Ranzato et al., 2015; Paulus et al., 2017). We also use reinforce-based models, but with novel reward functions and better simultaneous multi-reward optimization methods. Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral, has been used for Q&A and IE tasks (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Recent neural network models and large datasets (Bo"
N18-2102,P17-1099,0,0.338185,"approaches (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015), with more focus on graph-based (Giannakopoulos, 2009; Ganesan et al., 2010) and discourse tree-based (Gerani et al., 2014) models. Recent focus has shifted towards abstractive, rewriting-based summarization based on parse trees (Cheung and Penn, 2014; Wang et al., 2016), Abstract Meaning Representations (Liu et al., 2015; Dohare and Karnick, 2017), and neural network models with pointercopy mechanism and coverage (Rush et al., 2015; Chopra et al., 2016; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017), as well as reinforcebased metric rewards (Ranzato et al., 2015; Paulus et al., 2017). We also use reinforce-based models, but with novel reward functions and better simultaneous multi-reward optimization methods. Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral, has been used for Q&A and IE tasks (Harabagiu and Hickl, 2006; Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014). Recent neural network models and large datasets (Bowman et al., 2015; Williams et al., 2017) enabled stronger accuracies. Some previou"
N18-2102,C10-1039,0,\N,Missing
N18-2102,A00-1043,0,\N,Missing
N18-2102,P06-1114,0,\N,Missing
N18-2102,W14-3348,0,\N,Missing
N18-2102,D14-1168,0,\N,Missing
N18-2102,W04-1013,0,\N,Missing
N18-2102,S14-2131,0,\N,Missing
N18-2102,S14-1010,0,\N,Missing
N18-2102,D15-1042,0,\N,Missing
N18-2102,S14-2055,0,\N,Missing
N18-2102,W13-2117,0,\N,Missing
N18-2102,K16-1028,0,\N,Missing
N18-2102,D16-1244,0,\N,Missing
N18-2102,N15-1114,0,\N,Missing
N18-2110,D16-1216,1,0.841897,"T top of a CNN model. This model has been shown to perform better at sentiment classification than either of its integral parts alone. Visualization Techniques for Neural Models: There have been various visualization techniques proposed for neural networks in both Computer Vision (Krizhevsky et al., 2012; Simonyan et al., 2013; Zeiler and Fergus, 2014; Samek et al., 2017; Mahendran and Vedaldi, 2015) and NLP (Li et al., 2015; K´ad´ar et al., 2017). In this work, we adopt two visualization techniques: Activation Clustering (Girshick et al., 2014) following the politeness interpretation work of Aubakirova and Bansal (2016), which leads to insight on sentencelevel patterns, and First Derivative Saliency (Simonyan et al., 2013) following Li et al. (2015) and Aubakirova and Bansal (2016), which provides insight to the importance of each word in deciding the final classification label. RNN INPUT LAYER OUTPUT and uh mother is washing the dishes P (Alzheimer’s) P (Control) HIDDEN LAYERS CONVOLUTION MAX POOLING Figure 1: Our CNN-LSTM hybrid neural network. der and analyze the performance of our model on each subsample of the data to illustrate that the features we find are not gender-specific. These methods not only h"
N18-2110,J17-4003,0,\N,Missing
N18-2121,P05-3029,0,0.0731986,"Missing"
N18-2121,D16-1126,0,0.0298568,"et al. (2015) and Radev et al. (2015) learn to rank cartoon captions based on their funniness. Unlike typical, boring images in our task, memes and cartoons are images that are already funny or atypical. E.g., “LOL-cats” (funny cat photos), “Bieber-memes” (modified pictures of Justin Bieber), cartoons with talking animals, etc. Chandrasekaran et al. (2016) alter an abstract scene to make it more funny. In comparison, our task is to generate witty natural language remarks for a novel image. Poetry generation. Although our tasks are different, our generation approach is conceptually similar to Ghazvininejad et al. (2016) who produce poetry, given a topic. While they also generate and score a set of candidates, their approach involves many more constraints and utilizes a finite state acceptor unlike our approach which enforces constraints during beam search of the RNN decoder. Related Work Humor theory. General Theory of Verbal Humor (Attardo and Raskin, 1991) characterizes linguistic stimuli that induce humor but implementing computational models of it requires severely restricting its assumptions (Binsted, 1996). Puns. Zwicky and Zwicky (1986) classify puns as perfect (pronounced exactly the same) or imperfe"
N18-2121,N16-1079,0,0.024482,"our approach which enforces constraints during beam search of the RNN decoder. Related Work Humor theory. General Theory of Verbal Humor (Attardo and Raskin, 1991) characterizes linguistic stimuli that induce humor but implementing computational models of it requires severely restricting its assumptions (Binsted, 1996). Puns. Zwicky and Zwicky (1986) classify puns as perfect (pronounced exactly the same) or imperfect (pronounced differently). Similarly, Pepicello and Green (1984) categorize riddles based on the linguistic ambiguity that they exploit – phonological, morphological or syntactic. Jaech et al. (2016) learn phone-edit distances to predict the counterpart, given a pun by drawing from automatic speech recognition techniques. In contrast, we augment a web-scraped list of puns using an existing model of pronunciation similarity. Generating textual humor. JAPE (Binsted and Ritchie, 1997) also uses phonological ambiguity to generate pun-based riddles. While our task involves producing free-form responses to a novel 3 Approach Extracting tags. The first step in producing a contextually witty remark is to identify concepts that are relevant to the context (image). At times, these concepts are dire"
N18-2121,W14-2801,0,0.013069,"ssion, i.e., eliminate captions that are similar5 to a higher-ranked caption to reduce the pool to a smaller, more diverse set. We report results on the 3 top-ranked captions. We describe the effect of design choices in the supplementary. by comparing image tags against a list of puns. We construct the list of puns by mining the web for differently spelled words that sound exactly the same (heterographic homophones). We increase coverage by also considering pairs of words with 0 edit-distance, according to a metric based on finegrained articulatory representations (AR) of word pronunciations (Jyothi and Livescu, 2014). Our list of puns has a total of 1067 unique words (931 from the web and 136 from the AR-based model). The pun list yields a set of puns that are associated with a given image and their phonologically identical counterparts, which together form the pun vocabulary for the image. We evaluate our approach on the subset of images that have nonempty pun vocabularies (about 2 in 5 images). Generating punny image captions. We introduce an incongruity by forcing a vanilla image captioning model (Vinyals et al., 2016) to decode a phonological counterpart of a pun word associated with the image, at a s"
N18-2121,N15-1039,0,0.182884,"Missing"
N18-2121,W16-3608,0,0.0215552,"sconfirmed by the (phonetically similar) word ‘bare’. This incongruity is resolved when the perceiver parses the entire image description. The incongruity followed by resolution can be perceived to be witty.1 Introduction “Wit is the sudden marriage of ideas which before their union were not perceived to have any relation.” – Mark Twain. Witty remarks are often contextual, i.e., grounded in a specific situation. Developing computational models that can emulate rich forms of interaction like contextual humor, is a crucial step towards making human-AI interaction more natural and more engaging (Yu et al., 2016). E.g., witty chatbots could help relieve stress and increase user engagement by being more personable and human-like. Bots could automatically post witty comments (or suggest witty responses) on social media, chat, or messaging. The absence of large scale corpora of witty captions and the prohibitive cost of collecting such a dataset (being witty is harder than just describing 1 Indeed, a perceiver may fail to appreciate wit if the pro770 Proceedings of NAACL-HLT 2018, pages 770–775 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics We build two computa"
N18-2121,W02-0109,0,0.0180108,"rRNN rRNN rRNN of group a <stop> sell sell an Sentence reads this way Figure 2: Our models for generating and retrieving image descriptions containing a pun (see Sec. 3). the first T and last T positions, respectively.3 Retrieving punny image captions. As an alternative to our approach of generating witty remarks for the given image, we also attempt to leverage natural, human-written sentences which are relevant (yet unexpected) in the given context. Concretely, we retrieve natural language sentences4 from a combination of the Book Corpus (Zhu et al., 2015) and corpora from the NLTK toolkit (Loper and Bird, 2002). The retrieved sentences each (a) contains an incongruity (pun) whose counterpart is associated with the image, and (b) has support in the image (contains an image tag). This yields a pool of candidate captions that are perfectly grammatical, a little unexpected, and somewhat relevant to the image (see Sec. 4). Ranking. We rank captions in the candidate pools from both generation and retrieval models, according to their log-probability score under the image captioning model. We observe that the higherranked descriptions are more relevant to the image and grammatically correct. We then perform"
N18-2121,P13-2041,0,0.0708179,"Missing"
N19-1072,N10-1000,0,0.248169,"Missing"
N19-1072,P18-1060,0,0.0492731,"all crowdsourcing scripts, for future evaluations. 1 Introduction Evaluating content quality of summaries is an integral part of summarization research. Measuring the performance of a summarization system can be done through either automatic or manual evaluation. An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing. Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient (Owczarzak et al., 2012; Chaganty et al., 2018). This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment. The Pyramid method (Nenkova and Passonneau, 2004) is a prominent manual evaluation methodology that is considered highly reliable for 682 Proceedings of NAACL-HLT 2019, pages 682–687 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics The Responsiveness method, introduced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a s"
N19-1072,K16-1028,0,0.0422407,"systems with absolute scores, which pairwise comparison does not. Future work may improve correlation with the original Pyramid, or reduce annotation cost, by following our qualitative analysis and by reducing crowdsourcing noise (via qualification tests, enhanced guidelines, and post-processing result normalization (Hovy et al., 2013; Plank et al., 2014; Hosseini et al., 2012)). It would be appealing to investigate applying our methods to additional evaluation datasets, for which original Pyramid evaluations are not available for comparison. For example, addressing the CNN/DailyMail dataset (Nallapati et al., 2016) would involve testing single document summarization, utilizing a single reference summary per source text and addressing varying lengths of reference and system summaries. The Pyramid method is mainly a measurement of recall, which thus also applies to our lightweight Pyramid; but other measurements for summary quality, such as precision, nonredundancy and grammaticality, may also be considered. In particular, it may be possible to extend our design of crowdsourcing tasks to supply indications for these complementary measurements as well. 0.6 0.5 ’05 ρp ’06 ρp 0.4 2 4 6 ’05 ρs ’06 ρs 8 10 12"
N19-1072,I17-1081,0,0.113854,"(NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranked by the average score of their summaries. This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (Grusky et al., 2018; Paulus et al., 2018). Another common crowdsourcable evaluation method is pairwise comparison (Gao et al., 2018; Falke et al., 2017; Fan et al., 2018): an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text. This protocol allows comparing only two systems at a time, where the superior is determined by the total votes over all input texts. The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores. Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (Gillick and Liu, 2010). The lightweight crowdsourcable Pyramid version we propose"
N19-1072,N04-1019,0,0.568019,"easuring the performance of a summarization system can be done through either automatic or manual evaluation. An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing. Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient (Owczarzak et al., 2012; Chaganty et al., 2018). This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment. The Pyramid method (Nenkova and Passonneau, 2004) is a prominent manual evaluation methodology that is considered highly reliable for 682 Proceedings of NAACL-HLT 2019, pages 682–687 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics The Responsiveness method, introduced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranke"
N19-1072,W18-2706,0,0.0348052,"ot require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranked by the average score of their summaries. This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (Grusky et al., 2018; Paulus et al., 2018). Another common crowdsourcable evaluation method is pairwise comparison (Gao et al., 2018; Falke et al., 2017; Fan et al., 2018): an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text. This protocol allows comparing only two systems at a time, where the superior is determined by the total votes over all input texts. The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores. Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (Gillick and Liu, 2010). The lightweight crowdsourcable Pyramid version we propose aims to preserve t"
N19-1072,D18-1445,1,0.65027,"duced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranked by the average score of their summaries. This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (Grusky et al., 2018; Paulus et al., 2018). Another common crowdsourcable evaluation method is pairwise comparison (Gao et al., 2018; Falke et al., 2017; Fan et al., 2018): an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text. This protocol allows comparing only two systems at a time, where the superior is determined by the total votes over all input texts. The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores. Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (Gillick and Liu, 2010). The lightweight crowdsourcable Pyrami"
N19-1072,W12-2601,0,0.0352086,"ContentUnits, along with all crowdsourcing scripts, for future evaluations. 1 Introduction Evaluating content quality of summaries is an integral part of summarization research. Measuring the performance of a summarization system can be done through either automatic or manual evaluation. An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing. Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient (Owczarzak et al., 2012; Chaganty et al., 2018). This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment. The Pyramid method (Nenkova and Passonneau, 2004) is a prominent manual evaluation methodology that is considered highly reliable for 682 Proceedings of NAACL-HLT 2019, pages 682–687 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics The Responsiveness method, introduced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summa"
N19-1072,W10-0722,0,0.657793,"ise comparison, which do not rely on reference summaries and can be attained via crowdsourcing. Yet, these methods are quite subjective, since evaluators need to provide only a single global judgment for the quality of a summary (or a pair of summaries). Such judgments are far more subjective than the Pyramid score, which is derived from many, more objective, local decisions, each judging independently the presence of an individual SCU. Indeed, it was shown that the above subjective crowdsourcing-based evaluation methods are not reliable enough to produce consistent scores across experiments (Gillick and Liu, 2010). We propose a simplified crowdsourcable and reproducible version of the Pyramid method, that suggests appealing advantages over prior crowdsourcable evaluation methods. Like the original Pyramid, our method leverages the strong signal of the reference summaries and similarly bases its score on less subjective SCU judgments. In contrast to the original Pyramid, we rely on statistical sampling rather than exhaustive SCU extraction and testing, lowering overall cost. Empirically, our method correlates with the original PyraConducting a manual evaluation is considered an essential part of summary"
N19-1072,N18-1065,0,0.0569204,"Missing"
N19-1072,D18-1450,0,0.10047,"score is defined as the average Pyramid score over all its evaluated summaries. Although certain normalization variants attempt to weigh in SCU precision, the score is essentially an absolute “recallstyle” interpretation reflecting the system’s ability to cover the content units found in the reference summaries. Such a fairly robust score allows, in principle, system comparison across experiments (Nenkova and Passonneau, 2004). We note that due to the Pyramid method’s reliability, some research has been carried out on simulating the Pyramid method as a fully automatic one (Yang et al., 2016; Hirao et al., 2018). The hope of such a line of work is to find an automatic evaluation method that is more reliable than the commonly used ones, by taking the reference summary semantic content into account. Despite these efforts, automated Pyramid evaluations did not make their way yet to mainstream summary evaluation practices, where variants of the ROUGE metric (Lin, 2004) still prevail. In any case, as this paper focuses on manual evaluation, we compare our results to those of the manual Pyramid. 3 Our Lightweight Pyramid Method Our Lightweight Pyramid method mimics the two phases of the original Pyramid pr"
N19-1072,P14-2083,0,0.0137284,"our knowledge, our method is the first to mimic the reliable Pyramid method as an affordable crowdsourced procedure. Our experiments suggest that this lightweight Pyramid is more reliable than the common Responsiveness method. It also allows comparing multiple systems with absolute scores, which pairwise comparison does not. Future work may improve correlation with the original Pyramid, or reduce annotation cost, by following our qualitative analysis and by reducing crowdsourcing noise (via qualification tests, enhanced guidelines, and post-processing result normalization (Hovy et al., 2013; Plank et al., 2014; Hosseini et al., 2012)). It would be appealing to investigate applying our methods to additional evaluation datasets, for which original Pyramid evaluations are not available for comparison. For example, addressing the CNN/DailyMail dataset (Nallapati et al., 2016) would involve testing single document summarization, utilizing a single reference summary per source text and addressing varying lengths of reference and system summaries. The Pyramid method is mainly a measurement of recall, which thus also applies to our lightweight Pyramid; but other measurements for summary quality, such as pr"
N19-1072,N13-1132,0,0.0257061,"0.7 To the best of our knowledge, our method is the first to mimic the reliable Pyramid method as an affordable crowdsourced procedure. Our experiments suggest that this lightweight Pyramid is more reliable than the common Responsiveness method. It also allows comparing multiple systems with absolute scores, which pairwise comparison does not. Future work may improve correlation with the original Pyramid, or reduce annotation cost, by following our qualitative analysis and by reducing crowdsourcing noise (via qualification tests, enhanced guidelines, and post-processing result normalization (Hovy et al., 2013; Plank et al., 2014; Hosseini et al., 2012)). It would be appealing to investigate applying our methods to additional evaluation datasets, for which original Pyramid evaluations are not available for comparison. For example, addressing the CNN/DailyMail dataset (Nallapati et al., 2016) would involve testing single document summarization, utilizing a single reference summary per source text and addressing varying lengths of reference and system summaries. The Pyramid method is mainly a measurement of recall, which thus also applies to our lightweight Pyramid; but other measurements for summary"
N19-1268,D15-1138,0,0.020378,"swering (Gordon et al., 2018), and task planning (Zhu et al., 2017a). While these tasks are driven by different goals, they all require agents that can perceive their surroundings, understand the goal (either presented visually or in language instructions), and act in a virtual environment. Instruction-based Navigation For instructionbased navigation task, an agent is required to navigate from start viewpoint to end viewpoint according to some given instruction in an environment. This task has been studied by many works (Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015; Mei et al., 2016; Misra et al., 2017) in recent years. Among them, (Anderson et al., 2018b) differs from the others as it introduced a photo-realistic dataset – Room-to-Room (R2R), where all images are real ones taken by Matterport3D (Chang et al., 2017) and the instructions are also natural. In R2R 2 https://evalai.cloudcv.org/web/ challenges/challenge-page/97/overview 2611 environments, the agent’s ability to perceive realworld images and understanding natural language becomes even more crucial. To solve this challenging task, a lot of works (Fried et al., 2018; Wang et al., 2018a, 2019; M"
N19-1268,Q13-1005,0,0.0303205,"018), interactive question answering (Gordon et al., 2018), and task planning (Zhu et al., 2017a). While these tasks are driven by different goals, they all require agents that can perceive their surroundings, understand the goal (either presented visually or in language instructions), and act in a virtual environment. Instruction-based Navigation For instructionbased navigation task, an agent is required to navigate from start viewpoint to end viewpoint according to some given instruction in an environment. This task has been studied by many works (Tellex et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015; Mei et al., 2016; Misra et al., 2017) in recent years. Among them, (Anderson et al., 2018b) differs from the others as it introduced a photo-realistic dataset – Room-to-Room (R2R), where all images are real ones taken by Matterport3D (Chang et al., 2017) and the instructions are also natural. In R2R 2 https://evalai.cloudcv.org/web/ challenges/challenge-page/97/overview 2611 environments, the agent’s ability to perceive realworld images and understanding natural language becomes even more crucial. To solve this challenging task, a lot of works (Fried et al., 2018; Wa"
N19-1268,W18-2703,0,0.0210377,"f works (Fried et al., 2018; Wang et al., 2018a, 2019; Ma et al., 2019) have been proposed and shown some potential. The most relevant work to us is Fried et al. (2018), which proposed to use a speaker to synthesize new instructions and implement pragmatic reasoning. However, we observe there is some performance gap between seen and unseen environments. In this paper, we focus on improving the agent’s generalizability in unseen environment. Back-translation Back translation (Sennrich et al., 2016), a popular semi-supervised learning method, has been well studied in neural machine translation (Hoang et al., 2018; Wang et al., 2018b; Edunov et al., 2018; Prabhumoye et al., 2018). Given paired data of source and target sentences, the model first learns two translators – a forward translator from source to target and a backward translator from target to source. Next, it generates more source sentences using the back translator on an external target-language corpus. The generated pairs are then incorporated into the training data for fine-tuning the forward translator, which proves to improve the translation performance. Recently, this approach (also known as data augmentation) was applied to the task of"
N19-1268,D18-1045,0,0.0196868,", 2018a, 2019; Ma et al., 2019) have been proposed and shown some potential. The most relevant work to us is Fried et al. (2018), which proposed to use a speaker to synthesize new instructions and implement pragmatic reasoning. However, we observe there is some performance gap between seen and unseen environments. In this paper, we focus on improving the agent’s generalizability in unseen environment. Back-translation Back translation (Sennrich et al., 2016), a popular semi-supervised learning method, has been well studied in neural machine translation (Hoang et al., 2018; Wang et al., 2018b; Edunov et al., 2018; Prabhumoye et al., 2018). Given paired data of source and target sentences, the model first learns two translators – a forward translator from source to target and a backward translator from target to source. Next, it generates more source sentences using the back translator on an external target-language corpus. The generated pairs are then incorporated into the training data for fine-tuning the forward translator, which proves to improve the translation performance. Recently, this approach (also known as data augmentation) was applied to the task of instructionbased navigation (Fried et al"
N19-1268,D17-1106,0,0.0612935,"Missing"
N19-1268,P18-1080,0,0.0224801,"al., 2019) have been proposed and shown some potential. The most relevant work to us is Fried et al. (2018), which proposed to use a speaker to synthesize new instructions and implement pragmatic reasoning. However, we observe there is some performance gap between seen and unseen environments. In this paper, we focus on improving the agent’s generalizability in unseen environment. Back-translation Back translation (Sennrich et al., 2016), a popular semi-supervised learning method, has been well studied in neural machine translation (Hoang et al., 2018; Wang et al., 2018b; Edunov et al., 2018; Prabhumoye et al., 2018). Given paired data of source and target sentences, the model first learns two translators – a forward translator from source to target and a backward translator from target to source. Next, it generates more source sentences using the back translator on an external target-language corpus. The generated pairs are then incorporated into the training data for fine-tuning the forward translator, which proves to improve the translation performance. Recently, this approach (also known as data augmentation) was applied to the task of instructionbased navigation (Fried et al., 2018), where the source"
N19-1268,D18-1100,0,0.0937123,"13; Andreas and Klein, 2015; Mei et al., 2016; Misra et al., 2017) in recent years. Among them, (Anderson et al., 2018b) differs from the others as it introduced a photo-realistic dataset – Room-to-Room (R2R), where all images are real ones taken by Matterport3D (Chang et al., 2017) and the instructions are also natural. In R2R 2 https://evalai.cloudcv.org/web/ challenges/challenge-page/97/overview 2611 environments, the agent’s ability to perceive realworld images and understanding natural language becomes even more crucial. To solve this challenging task, a lot of works (Fried et al., 2018; Wang et al., 2018a, 2019; Ma et al., 2019) have been proposed and shown some potential. The most relevant work to us is Fried et al. (2018), which proposed to use a speaker to synthesize new instructions and implement pragmatic reasoning. However, we observe there is some performance gap between seen and unseen environments. In this paper, we focus on improving the agent’s generalizability in unseen environment. Back-translation Back translation (Sennrich et al., 2016), a popular semi-supervised learning method, has been well studied in neural machine translation (Hoang et al., 2018; Wang et al., 2018b; Edunov"
N19-1268,P16-1009,0,0.0410843,"e realworld images and understanding natural language becomes even more crucial. To solve this challenging task, a lot of works (Fried et al., 2018; Wang et al., 2018a, 2019; Ma et al., 2019) have been proposed and shown some potential. The most relevant work to us is Fried et al. (2018), which proposed to use a speaker to synthesize new instructions and implement pragmatic reasoning. However, we observe there is some performance gap between seen and unseen environments. In this paper, we focus on improving the agent’s generalizability in unseen environment. Back-translation Back translation (Sennrich et al., 2016), a popular semi-supervised learning method, has been well studied in neural machine translation (Hoang et al., 2018; Wang et al., 2018b; Edunov et al., 2018; Prabhumoye et al., 2018). Given paired data of source and target sentences, the model first learns two translators – a forward translator from source to target and a backward translator from target to source. Next, it generates more source sentences using the back translator on an external target-language corpus. The generated pairs are then incorporated into the training data for fine-tuning the forward translator, which proves to impro"
N19-1355,E17-2026,0,0.0777721,"relevant auxiliary tasks, where different tasks share some common model parameters with alternating mini-batches optimization, similar to Luong et al. (2015). To address the problem of automatic shared parameter selection, Ruder et al. (2017a) automatically learned the latent multi-task sharing architecture, and Xiao et al. (2018) used a gate mechanism that filters the feature flows between tasks. On the problem of identifying task relatedness, Ben-David and Schuller (2003) provided a formal framework for task relatedness and derived generalization error bounds for learning of multiple tasks. Bingel and Søgaard (2017) explored task relatedness via exhaustively experimenting with all possible two task tuples in a nonautomated multi-task setup. Other related works explored data selection, where the goal is to select or reorder the examples from one or more domains (usually in a single task) to either improve the training efficiency or enable better transfer learning. These approaches have been applied in machine translation (van der Wees et al., 2017), language models (Moore and Lewis, 2010; Duh et al., 2013), dependency parsing (Søgaard, 2011), etc. In particular, Ruder and Plank (2017) used Bayesian optimi"
N19-1355,D17-1070,0,0.212887,"xing ratio w.r.t. the primary task, via a Beta-Bernoulli bandit with Thompson Sampling and a Gaussian Process framework. Task Task Task Soft-Max Fully-Connected Soft-Max Fully-Connected Max-Pooling Soft-Max Fully-Connected Max-Pooling LSTM-RNN Layers ELMo Embeddings Figure 1: Overview of our baseline model where we use different projection layers for each task during MTL, while sharing rest of the model parameters. 3.1 Bi-Text Classification Model Let s1 and s2 be the input sentence pair in our classification task, where we encode these sentences via bidirectional LSTM-RNN, similar to that of Conneau et al. (2017). Next, we do max-pooling on the output hidden states of both encoders where u and v are the outputs from the max-pooing layer for s1 and s2 respectively. Later, we map these two representations (u and v) into a single rich dense representation vector h: h = [u; v; u ? v; |u − v|] (1) where [; ] represents the concatenation and u ? v represents the element-wise multiplication of u and v. We project this final representation h to label space to classify the given sentence pair (see Fig. 1). We also use ELMo (Peters et al., 2018) representations for word embeddings in our model. For this, we ext"
N19-1355,P13-2119,0,0.0259142,"for task relatedness and derived generalization error bounds for learning of multiple tasks. Bingel and Søgaard (2017) explored task relatedness via exhaustively experimenting with all possible two task tuples in a nonautomated multi-task setup. Other related works explored data selection, where the goal is to select or reorder the examples from one or more domains (usually in a single task) to either improve the training efficiency or enable better transfer learning. These approaches have been applied in machine translation (van der Wees et al., 2017), language models (Moore and Lewis, 2010; Duh et al., 2013), dependency parsing (Søgaard, 2011), etc. In particular, Ruder and Plank (2017) used Bayesian optimization to select relevant training instances for transfer learning, and Tsvetkov et al. (2016) applied it to learn a curriculum for training word embeddings via reordering data. Graves et al. (2017) used the bandit approach (Exp3.S algorithm) in the context of automated curriculum learning, but in our work, we have two stages with each stage addressing a different problem (automatic task selection and learning of the training mixing ratio). Recently, Sharma and Ravindran (2017) used multi-armed"
N19-1355,C18-1039,1,0.904256,"Missing"
N19-1355,D17-1206,0,0.161586,"of AUTO S E M and analyze the learned auxiliary task choices. 1 Introduction Multi-task Learning (MTL) (Caruana, 1997) is an inductive transfer mechanism which leverages information from related tasks to improve the primary model’s generalization performance. It achieves this goal by training multiple tasks in parallel while sharing representations, where the training signals from the auxiliary tasks can help improve the performance of the primary task. Multi-task learning has been applied to a wide range of natural language processing problems (Luong et al., 2015; Pasunuru and Bansal, 2017; Hashimoto et al., 2017; Ruder et al., 2017b; Kaiser et al., 2017; McCann et al., 2018). Despite its impressive performance, the design of a multitask learning system is non-trivial. In the context of improving the primary task’s performance using knowledge from other auxiliary tasks (Luong et al., 2015; Pasunuru and Bansal, 2017), two major challenges include selecting the most relevant auxiliary tasks and also learning the balanced mixing ratio for synergized training of these tasks. One can achieve this via manual intuition or hyper-parameter tuning over all combinatorial task choices, but this introduces human i"
N19-1355,D17-1038,0,0.0309618,"f multiple tasks. Bingel and Søgaard (2017) explored task relatedness via exhaustively experimenting with all possible two task tuples in a nonautomated multi-task setup. Other related works explored data selection, where the goal is to select or reorder the examples from one or more domains (usually in a single task) to either improve the training efficiency or enable better transfer learning. These approaches have been applied in machine translation (van der Wees et al., 2017), language models (Moore and Lewis, 2010; Duh et al., 2013), dependency parsing (Søgaard, 2011), etc. In particular, Ruder and Plank (2017) used Bayesian optimization to select relevant training instances for transfer learning, and Tsvetkov et al. (2016) applied it to learn a curriculum for training word embeddings via reordering data. Graves et al. (2017) used the bandit approach (Exp3.S algorithm) in the context of automated curriculum learning, but in our work, we have two stages with each stage addressing a different problem (automatic task selection and learning of the training mixing ratio). Recently, Sharma and Ravindran (2017) used multi-armed bandits (MAB) to learn the choice of hard vs. easy domain data selection as inp"
N19-1355,P10-2041,0,0.0345651,"ded a formal framework for task relatedness and derived generalization error bounds for learning of multiple tasks. Bingel and Søgaard (2017) explored task relatedness via exhaustively experimenting with all possible two task tuples in a nonautomated multi-task setup. Other related works explored data selection, where the goal is to select or reorder the examples from one or more domains (usually in a single task) to either improve the training efficiency or enable better transfer learning. These approaches have been applied in machine translation (van der Wees et al., 2017), language models (Moore and Lewis, 2010; Duh et al., 2013), dependency parsing (Søgaard, 2011), etc. In particular, Ruder and Plank (2017) used Bayesian optimization to select relevant training instances for transfer learning, and Tsvetkov et al. (2016) applied it to learn a curriculum for training word embeddings via reordering data. Graves et al. (2017) used the bandit approach (Exp3.S algorithm) in the context of automated curriculum learning, but in our work, we have two stages with each stage addressing a different problem (automatic task selection and learning of the training mixing ratio). Recently, Sharma and Ravindran (201"
N19-1355,P17-1117,1,0.916099,"nt ablations for each stage of AUTO S E M and analyze the learned auxiliary task choices. 1 Introduction Multi-task Learning (MTL) (Caruana, 1997) is an inductive transfer mechanism which leverages information from related tasks to improve the primary model’s generalization performance. It achieves this goal by training multiple tasks in parallel while sharing representations, where the training signals from the auxiliary tasks can help improve the performance of the primary task. Multi-task learning has been applied to a wide range of natural language processing problems (Luong et al., 2015; Pasunuru and Bansal, 2017; Hashimoto et al., 2017; Ruder et al., 2017b; Kaiser et al., 2017; McCann et al., 2018). Despite its impressive performance, the design of a multitask learning system is non-trivial. In the context of improving the primary task’s performance using knowledge from other auxiliary tasks (Luong et al., 2015; Pasunuru and Bansal, 2017), two major challenges include selecting the most relevant auxiliary tasks and also learning the balanced mixing ratio for synergized training of these tasks. One can achieve this via manual intuition or hyper-parameter tuning over all combinatorial task choices, but"
N19-1355,W17-4504,1,0.852849,"g on the selected tasks. Our 2-stage model performs better than both these ablations, showing that both of our stages are crucial. Further, we also discuss the learned auxiliary task choices in terms of their intuitive relevance w.r.t. the corresponding primary task. 2 Related Work Multi-task learning (Caruana, 1998), known for improving the generalization performance of a task with auxiliary tasks, has successfully been applied to many domains of machine learning, including natural language processing (Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Pasunuru and Bansal, 2017; Pasunuru et al., 2017), computer vision (Misra et al., 2016; Kendall et al., 2017; Dai et al., 2016), and reinforcement learning (Teh et al., 2017; Parisotto et al., 2015; Jaderberg et al., 2016). Although there are many variants of multi-task learning (Ruder et al., 2017b; Hashimoto et al., 2017; Luong et al., 2015; McCann et al., 2018), our goal is to improve the performance of a primary task using a set of relevant auxiliary tasks, where different tasks share some common model parameters with alternating mini-batches optimization, similar to Luong et al. (2015). To address the problem of automatic shared paramet"
N19-1355,N18-1202,0,0.0472857,"de these sentences via bidirectional LSTM-RNN, similar to that of Conneau et al. (2017). Next, we do max-pooling on the output hidden states of both encoders where u and v are the outputs from the max-pooing layer for s1 and s2 respectively. Later, we map these two representations (u and v) into a single rich dense representation vector h: h = [u; v; u ? v; |u − v|] (1) where [; ] represents the concatenation and u ? v represents the element-wise multiplication of u and v. We project this final representation h to label space to classify the given sentence pair (see Fig. 1). We also use ELMo (Peters et al., 2018) representations for word embeddings in our model. For this, we extract the three ELMo layer representations for each of the sentence pair and use their weighted sum as the ELMo output representation, where the weights are trainable. 3.2 Multi-Task Learning In this work, we focus on improving a task (primary task) by allowing it to share parameters with related auxiliary tasks via multi-task learning (MTL). Let {D1 , ..., DN } be a set of N tasks, where we set D1 to be the primary task and the rest of them as auxiliary tasks. We can extend 3522 MR-3 MR-2 MR-1 Task Utility Primary Task Arm1 Arm"
N19-1355,P11-2120,0,0.021931,"lization error bounds for learning of multiple tasks. Bingel and Søgaard (2017) explored task relatedness via exhaustively experimenting with all possible two task tuples in a nonautomated multi-task setup. Other related works explored data selection, where the goal is to select or reorder the examples from one or more domains (usually in a single task) to either improve the training efficiency or enable better transfer learning. These approaches have been applied in machine translation (van der Wees et al., 2017), language models (Moore and Lewis, 2010; Duh et al., 2013), dependency parsing (Søgaard, 2011), etc. In particular, Ruder and Plank (2017) used Bayesian optimization to select relevant training instances for transfer learning, and Tsvetkov et al. (2016) applied it to learn a curriculum for training word embeddings via reordering data. Graves et al. (2017) used the bandit approach (Exp3.S algorithm) in the context of automated curriculum learning, but in our work, we have two stages with each stage addressing a different problem (automatic task selection and learning of the training mixing ratio). Recently, Sharma and Ravindran (2017) used multi-armed bandits (MAB) to learn the choice o"
N19-1355,P16-1013,0,0.0686718,"Missing"
N19-1355,W18-5446,0,0.0454077,"Missing"
N19-1355,D17-1147,0,0.0409674,"Missing"
N19-1355,N18-2114,0,0.0312779,"g (Teh et al., 2017; Parisotto et al., 2015; Jaderberg et al., 2016). Although there are many variants of multi-task learning (Ruder et al., 2017b; Hashimoto et al., 2017; Luong et al., 2015; McCann et al., 2018), our goal is to improve the performance of a primary task using a set of relevant auxiliary tasks, where different tasks share some common model parameters with alternating mini-batches optimization, similar to Luong et al. (2015). To address the problem of automatic shared parameter selection, Ruder et al. (2017a) automatically learned the latent multi-task sharing architecture, and Xiao et al. (2018) used a gate mechanism that filters the feature flows between tasks. On the problem of identifying task relatedness, Ben-David and Schuller (2003) provided a formal framework for task relatedness and derived generalization error bounds for learning of multiple tasks. Bingel and Søgaard (2017) explored task relatedness via exhaustively experimenting with all possible two task tuples in a nonautomated multi-task setup. Other related works explored data selection, where the goal is to select or reorder the examples from one or more domains (usually in a single task) to either improve the training"
P10-1112,E93-1006,0,0.901614,"tantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of sub"
P10-1112,P01-1010,0,0.925161,"avoid selection of fragments, and work with all fragments. Of course, having a grammar that includes all training substructures is only desirable to the extent that those structures can be appropriately weighted. Implicit representations like those used here do not allow arbitrary weightings of fragments. However, we use a simple weighting scheme which does decompose appropriately over the implicit encoding, and which is flexible enough to allow weights to depend not only on frequency but also on fragment size, node patterns, and certain lexical properties. Similar ideas have been explored in Bod (2001), Collins and Duffy (2002), and Goodman (2003). Our model empirically affirms the effectiveness of such a flexible weighting scheme in full-scale experiments. We also investigate parsing without an explicit lexicon. The all-fragments approach has the advantage that parsing down to the character level requires no special treatment; we show that an explicit lexicon is not needed when sentences are considered as strings of characters rather than words. This avoids the need for complex unknown word models and other specialized lexical resources. The main contribution of this work is to show practi"
P10-1112,P05-1022,0,0.121447,"Missing"
P10-1112,W98-1115,0,0.0767748,"d (PT) -13 Figure 5: Effect of coarse-pass pruning on parsing accuracy (WSJ, training ≤ 20 words, tested on dev-set ≤ 20 words). This graph shows that the fortuitous improvement due to pruning is very small and that the peak accuracy is almost equal to the accuracy without pruning (the dotted line). from no pruning to pruning with a −6.2 log posterior threshold.10 Figure 4 depicts the variation in parsing accuracies in response to the amount of pruning done by the coarse-pass. Higher posterior pruning thresholds induce more aggressive pruning. Here, we observe an effect seen in previous work (Charniak et al. (1998), Petrov and Klein (2007), Petrov et al. (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). However, these ‘fortuitous’ search errors give only a small improvement and the peak accuracy is almost equal to the parsing accuracy without any pruning (as seen in Figure 5).11 This outcome suggests that the coarsepass pruning is critical for tractability but not for performance. 10 Unpruned experiments could not be run for 40-word test sentences even with 50GB of memory, therefore we calculated the improv"
P10-1112,N06-1022,0,0.135273,"Missing"
P10-1112,A00-2018,0,0.228742,"Missing"
P10-1112,J98-4004,0,0.583695,"Missing"
P10-1112,J02-1005,0,0.0898002,"by chored local binary tree counts, which are easily directly optimizing parsing F1 on our developcomputed from P (dI |s) and equivalent to those ment set. Because this objective is not easily diffrom P (d|s). Therefore, no additional approximaferentiated, we simply perform a grid search on tions are made in GI over G. the three hyperparameters. The tuned values are As shown in Table 1, our model (an allωBODY = 0.35, ωLEX = 0.25 and asp = 0.018. fragments grammar with the weighting scheme For generalization to a larger parameter space, we would of course need to switch to a learning aptent by Johnson (2002). Later, Zollmann and Sima’an (2005) proach that scales more gracefully in the number presented a statistically consistent estimator, with the basic of tunable hyperparameters.8 insight of optimizing on a held-out set. Our estimator is not 8 Note that there has been a long history of DOP estimators. The generative DOP1 model was shown to be inconsisintended to be viewed as a generative model of trees at all, but simply a loss-minimizing conditional distribution within our parametric family. 1102 88.2 88.0 87.8 Coarse-to-Fine Inference -4.0 Coarse-to-fine inference is a well-established way to"
P10-1112,P03-1054,1,0.223084,"Missing"
P10-1112,N03-1017,0,0.00134302,"oduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in the training data, and it is impractical to explicitly extract them all. On both efficiency and statistical grounds, much recent TSG work has focused on fragment selection (Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009). At the same time,"
P10-1112,P05-1010,0,0.170065,"Missing"
P10-1112,N07-1051,1,0.748561,",i,k,j) P r I(rootr ,0,n) tmax = argmax P r(A→B C,i,k,j) x O(Ax ,i,j)I(Ax ,i,j) tmax = argmax t t P q(c) c∈t P q(e) e∈t Q q(e) e∈t Figure 3: Inference: Different objectives for parsing with posteriors. A, B, C are base symbols, Ax , By , Cz are indexed symbols and i,j,k are between-word indices. Hence, (Ax , i, j) represents a constituent labeled with Ax spanning words i to j. I(Ax , i, j) and O(Ax , i, j) denote the inside and outside scores of this constituent, respectively. For brevity, we write c ≡ (A, i, j) and e ≡ (A → B C, i, k, j). Also, tmax is the highest scoring parse. Adapted from Petrov and Klein (2007). constant ωLEX (see Figure 2). Fractional values of these parameters allow the weight of a fragment to depend on its size and lexical properties. Another parameter we introduce is a ‘switching-penalty’ csp for the END rules (Figure 2). The DOP1 model uses binary values (0 if symbol is intermediate, 1 otherwise) as the END rule weight, which is equivalent to prohibiting fragment switching at intermediate symbols. We learn a fractional constant asp that allows (but penalizes) switching between fragments at annotated symbols through the formulation csp (Xintermediate ) = 1 − asp and csp (Xnon−in"
P10-1112,D08-1091,1,0.651745,"Missing"
P10-1112,P05-1033,0,0.0133267,"Missing"
P10-1112,P06-1055,1,0.842051,"Missing"
P10-1112,N09-1062,0,0.298064,"Missing"
P10-1112,D08-1012,1,0.0957222,"g on parsing accuracy (WSJ, training ≤ 20 words, tested on dev-set ≤ 20 words). This graph shows that the fortuitous improvement due to pruning is very small and that the peak accuracy is almost equal to the accuracy without pruning (the dotted line). from no pruning to pruning with a −6.2 log posterior threshold.10 Figure 4 depicts the variation in parsing accuracies in response to the amount of pruning done by the coarse-pass. Higher posterior pruning thresholds induce more aggressive pruning. Here, we observe an effect seen in previous work (Charniak et al. (1998), Petrov and Klein (2007), Petrov et al. (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). However, these ‘fortuitous’ search errors give only a small improvement and the peak accuracy is almost equal to the parsing accuracy without any pruning (as seen in Figure 5).11 This outcome suggests that the coarsepass pruning is critical for tractability but not for performance. 10 Unpruned experiments could not be run for 40-word test sentences even with 50GB of memory, therefore we calculated the improvement factors using a smaller experiment with f"
P10-1112,P02-1034,0,0.558039,"iable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in the training data, and it is impractical to explicitly extract them al"
P10-1112,P09-2012,0,0.513536,"Missing"
P10-1112,C92-2065,0,0.100557,"ariable modeling, no smoothing, and even no explicit lexicon (hence negligible training overall). These techniques, however, are not limited to the case of monolingual parsing, offering extensions to models of machine translation, semantic interpretation, and other areas in which a similar tension exists between the desire to extract many large structures and the computational cost of doing so. 2 Representation of Implicit Grammars 2.1 All-Fragments Grammars We consider an all-fragments grammar G (see Figure 1(a)) derived from a binarized treebank B. G is formally a tree-substitution grammar (Resnik, 1992; Bod, 1993) wherein each subgraph of each training tree in B is an elementary tree, or fragment f , in G. In G, each derivation d is a tree (multiset) of fragments (Figure 1(c)), and the weight of the derivation is the Q product of the weights of the fragments: ω(d) = f ∈d ω(f ). In the following, the derivation weights, when normalized over a given sentence s, are interpretable as conditional probabilities, so G induces distributions of the form P (d|s). In models like G, many derivations will generally correspond to the same unsegmented tree, and the parsing task is to find the tree whose s"
P10-1112,D09-1076,0,0.0351793,"Missing"
P10-1112,N04-1035,0,0.0110723,"Missing"
P10-1112,C96-2215,0,0.222042,"Missing"
P10-1112,W96-0214,0,0.919889,"ore complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in t"
P10-1112,P00-1008,0,0.441448,"Missing"
P10-1112,P96-1024,0,0.234224,"ore complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding. 1 Introduction Modern NLP systems have increasingly used dataintensive models that capture many or even all substructures from the training data. In the domain of syntactic parsing, the idea that all training fragments1 might be relevant to parsing has a long history, including tree-substitution grammar (data-oriented parsing) approaches (Scha, 1990; Bod, 1993; Goodman, 1996a; Chiang, 2003) and tree kernel approaches (Collins and Duffy, 2002). For machine translation, the key modern advancement has been the ability to represent and memorize large training substructures, be it in contiguous phrases (Koehn et al., 2003) or syntactic trees 1 In this paper, a fragment means an elementary tree in a tree-substitution grammar, while a subtree means a fragment that bottoms out in terminals. (Galley et al., 2004; Chiang, 2005; Deneefe and Knight, 2009). In all such systems, a central challenge is efficiency: there are generally a combinatorial number of substructures in t"
P10-1112,P04-1013,0,0.0153987,"Missing"
P10-1112,D07-1058,0,0.742542,"Missing"
P10-1112,J03-4003,0,\N,Missing
P10-1112,P00-1058,0,\N,Missing
P11-1070,J07-4002,0,0.0225942,"auer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have access to gold-standard knowledge of the competing attachment sites (see Atterer and Schutze (2007)’s criticism of previous work), we must instead compute features for all possible head-argument pairs from our web corpus. Moreover, when there are only two competing attachment options, one can do things like directly compare two count-based heuristics and choose the larger. Integration into a parser requires features to be functions of single attachments, not pairwise comparisons between alternatives. A learning algorithm can then weight features so that they compare appropriately 695 across parses. We employ a collection of affinity features of varying specificity. The basic feature is the"
P11-1070,P05-1022,0,0.142616,"Missing"
P11-1070,J05-1003,0,0.0577981,"Missing"
P11-1070,W02-1001,0,0.108147,"Missing"
P11-1070,P08-1109,0,0.0639056,"Missing"
P11-1070,P08-1067,0,0.0383059,"Missing"
P11-1070,P10-1001,0,0.0108779,"ocal) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6 Their README specifies ‘training-k:5 iters:10 losstype:nopunc decode-type:proj’, which we used for all final experiments; we used the faster ‘training-k:1 iters:5’ setting for most development experiments. 7 Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8 Settings: 6 iterations of split and merge with smoothing. 698 k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different values of k) for the developm"
P11-1070,P08-1068,0,0.185751,"consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‘semantically bad’ parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in 694 addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency"
P11-1070,N04-1016,0,0.395696,"ld edges are in dashed gold and edges common in guess and gold parses are in black. attachment ambiguity where by yesterday afternoon should attach to had already, (b) is an NP-internal ambiguity where half a should attach to dozen and not to newspapers, and (c) is an adverb attachment ambiguity, where just should modify fine and not the verb ’s. Resolving many of these errors requires information that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‘affinity’ is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal atProceedings of the 49th Annua"
P11-1070,P95-1007,0,0.0762287,"ead ?(raising ?($ from) raising arg) $ from) from debt Figure 3: Features factored over head-argument pairs. pairs, as is standard in the dependency parsing literature (see Figure 3). Here, we discuss which webcount based features φ(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely"
P11-1070,P09-1039,0,0.00841255,"using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6 Their README specifies ‘training-k:5 iters:10 losstype:nopunc decode-type:proj’, which we used for all final experiments; we used the faster ‘training-k:1 iters:5’ setting for most development experiments. 7 Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8 Settings: 6 iterations of split and merge with smoothing. 698 k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different value"
P11-1070,E06-1011,0,0.366453,"et al., 2010). For example, in (b), half dozen is more frequent than half newspapers. In this paper, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and Pereira (2006). For constituent parsing, we rerank the output of the Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences"
P11-1070,P05-1012,0,0.126243,"riments Our features are designed to be used in full-sentence parsing rather than for limited decisions about isolated ambiguities. We first integrate our features into a dependency parser, where the integration is more natural and pushes all the way into the underlying dynamic program. We then add them to a constituent parser in a reranking approach. We also verify that our features contribute on top of standard reranking features.3 4.1 Dependency Parsing For dependency parsing, we use the discriminatively-trained MSTParser4 , an implementation of first and second order MST parsing models of McDonald et al. (2005) and McDonald and Pereira (2006). We use the standard splits of Penn Treebank into training (sections 2-21), development (section 22) and test (section 23). We used the ‘pennconverter’5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3 All reported experiments are run on all sentences, i.e. without any length limit. 4 http://sourceforge.net/projects/mstparser 5 This supersedes ‘Penn2Malt’ and is available at htt"
P11-1070,W05-0603,0,0.268752,"formation that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‘affinity’ is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal atProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693–702, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (a) NP NP (b) S QP VP PP had already … PDT … half …Lehman Hutton Inc. (c) NP DT VBZ PDT PDT … ´s dozen newspapers a VP ADVP ADJP ADJP RB JJ just fine by yesterday afternoon Figure 2: Different kinds of attachment errors in the pa"
P11-1070,H05-1105,0,0.325504,"formation that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‘affinity’ is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal atProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693–702, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (a) NP NP (b) S QP VP PP had already … PDT … half …Lehman Hutton Inc. (c) NP DT VBZ PDT PDT … ´s dozen newspapers a VP ADVP ADJP ADJP RB JJ just fine by yesterday afternoon Figure 2: Different kinds of attachment errors in the pa"
P11-1070,P08-1052,0,0.0189483,"stracted versions of these paraphrase features where the context words c are collapsed to their parts-of-speech POS(c), obtained using a unigram-tagger trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. Working with Web n-Grams 3 Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issues of both speed and daily query limits, especially if all possible attachments trigger queries. Such methods also create a dependence on the quality and postprocessing of the search results, limitations of the query process (for instance, search engines can ignore punct"
P11-1070,P06-1055,1,0.0718814,", we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and Pereira (2006). For constituent parsing, we rerank the output of the Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to fi"
P11-1070,C10-1100,0,0.434752,"inguistics (a) NP NP (b) S QP VP PP had already … PDT … half …Lehman Hutton Inc. (c) NP DT VBZ PDT PDT … ´s dozen newspapers a VP ADVP ADJP ADJP RB JJ just fine by yesterday afternoon Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in solid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black. tachment. Still other work has exploited Web counts for other isolated ambiguities, such as NP coordination (Nakov and Hearst, 2005b) and noun-sequence bracketing (Nakov and Hearst, 2005a; Pitler et al., 2010). For example, in (b), half dozen is more frequent than half newspapers. In this paper, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and"
P11-1070,W96-0213,0,0.194628,"Missing"
P11-1070,D08-1016,0,0.0212834,"n rerank this k-best list using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6 Their README specifies ‘training-k:5 iters:10 losstype:nopunc decode-type:proj’, which we used for all final experiments; we used the faster ‘training-k:1 iters:5’ setting for most development experiments. 7 Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8 Settings: 6 iterations of split and merge with smoothing. 698 k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lis"
P11-1070,P07-1031,0,0.0115344,"Missing"
P11-1070,W06-1604,0,0.0237835,"he Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‘semantically bad’ parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in 694 addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (G"
P11-1070,J03-4003,0,\N,Missing
P11-1131,2009.eamt-1.23,0,0.575459,"Missing"
P11-1131,W04-3216,0,0.0424911,"Missing"
P11-1131,W06-3105,0,0.110753,"-word links. ments, such English not to French ne ? pas. Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model. 1.1 Related Work Our first major influence is that of conditional phrase-based models. An early approach by Deng and Byrne (2005) changed the parameterization of the traditional word-based HMM model, modeling subsequent words from the same state using a bigram model. However, this model changes only the parameterization and not the set of possible alignments. More closely related are the approaches of Daum´e III and Marcu (2004) and DeNero et al. (2006), which allow phrase-to-phrase alignments between the source and target domain. As DeNero warns, though, an unconstrained model may overfit using unusual segmentations. Interestingly, the phrase-based hidden semi-Markov model of Andr´es-Ferrer and Juan (2009) does not seem to encounter these problems. We suspect two main causes: first, the model interpolates with Model 1 (Brown et al., 1994), which may help prevent overfitting, and second, the model is monotonic, which screens out many possible alignments. Monotonicity is generally undesirable, though: almost all parallel sentences exhibit som"
P11-1131,H05-1022,0,0.192963,"Figure 2: The model of E given F can represent the phrasal alignment {e1 , e2 } ∼ {f1 }. However, the model of F given E cannot: the probability mass is distributed between {e1 } ∼ {f1 } and {e2 } ∼ {f1 }. Agreement of the forward and backward HMM alignments tends to place less mass on phrasal links and greater mass on word-to-word links. ments, such English not to French ne ? pas. Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model. 1.1 Related Work Our first major influence is that of conditional phrase-based models. An early approach by Deng and Byrne (2005) changed the parameterization of the traditional word-based HMM model, modeling subsequent words from the same state using a bigram model. However, this model changes only the parameterization and not the set of possible alignments. More closely related are the approaches of Daum´e III and Marcu (2004) and DeNero et al. (2006), which allow phrase-to-phrase alignments between the source and target domain. As DeNero warns, though, an unconstrained model may overfit using unusual segmentations. Interestingly, the phrase-based hidden semi-Markov model of Andr´es-Ferrer and Juan (2009) does not see"
P11-1131,P10-1016,0,0.0111129,"h as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discou"
P11-1131,J07-3002,0,0.0790977,"Missing"
P11-1131,N10-1140,0,0.0267577,"ligned sentence pairs. models prevents EM from overfitting, even in the absence of harsh penalties. We also allow gappy (noncontiguous) phrases on the state side, which makes agreement more successful but agreement needs approximation of posterior marginals. Using pruned lists of good phrases, we maintain complexity equal to the baseline word-to-word model. There are several steps forward from this point. Limiting the gap length also prevents combinatorial explosion; we hope to explore this in future work. Clearly a translation system that uses discontinuous mappings at runtime (Chiang, 2007; Galley and Manning, 2010) may make better use of discontinuous alignments. This model can also be applied at the morpheme or character level, allowing joint inference of segmentation and alignment. Furthermore the state space could be expanded and enhanced to include more possibilities: states with multiple gaps might be useful for alignment in languages with template morphology, such as Arabic or Hebrew. More exploration in the model space could be useful – a better distortion model might place a stronger distribution on the likely starting and ending points of phrases. Acknowledgments We would like to thank the anon"
P11-1131,N04-1035,0,0.0383112,"arkov Models, while maintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this proble"
P11-1131,N03-1017,0,0.0294345,"ontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are n"
P11-1131,W06-2402,0,0.0162878,"also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this"
P11-1131,N06-1014,0,0.704607,"ns (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases. This allows agreement to reinforce non-contiguous align1308 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308–1317, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Observations→ e1 e2 e3 f1 f2 f3 States→ f1 e1 ? f2 e2 ? f3 e3 HMM(E|F) HMM(F|E) Figure 2: The model of E given F can represent the phrasal alignment {e1 , e2 } ∼ {f1 }. However"
P11-1131,P07-1039,0,0.0160259,"NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find t"
P11-1131,W02-1018,0,0.0409262,"from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous seq"
P11-1131,J03-1002,0,0.202395,"osoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We"
P11-1131,C96-2141,0,0.958377,"-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases. This allows agreement to reinforce non-contiguous align1308 Proceedings of the 49th Annual Meeting of the Association fo"
P11-1131,J97-3002,0,0.620845,"zation and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the a"
P11-1131,W06-3119,0,0.016468,"aintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for"
P11-1131,J93-2003,0,\N,Missing
P11-1131,J07-2003,0,\N,Missing
P11-2127,P10-1112,1,0.871391,"P The all-fragments grammar (AFG) for a (binarized) treebank is formally the tree-substitution grammar (TSG) (Resnik, 1992; Bod, 1993) that consists of all fragments (elementary trees) of all training trees in the treebank, with some weighting on each fragment. AFGs are too large to fully extract explicitly; researchers therefore either work with a tractable subset of the fragments (Sima’an, 2000; Bod, 2001; Post and Gildea, 2009; Cohn and Blunsom, 2010) or use a PCFG reduction like that of Goodman (1996a), in which each treebank node token Xi is given its own unique grammar symbol. We follow Bansal and Klein (2010) in choosing the latter, both to permit comparison to their results and because SDP is easily phrased as a PCFG reduction. Bansal and Klein (2010) use a carefully paOne guiding intuition in parsing, and data-driven NLP more generally, is that, all else equal, it is advantageous to memorize large fragments of training examples. Taken to the extreme, this intuition suggests shortest derivation parsing (SDP), wherein a test sentence is analyzed in a way which uses as few training fragments as possible (Bod, 2000; Goodman, 2003). SDP certainly has appealing properties: it is simple and parameter f"
P11-2127,E93-1006,0,0.213648,"Missing"
P11-2127,C00-1011,0,0.0817071,"Missing"
P11-2127,P01-1010,0,0.0607119,"Missing"
P11-2127,W98-1115,0,0.0767635,"Missing"
P11-2127,N06-1022,0,0.0487217,"Missing"
P11-2127,P10-2042,0,0.0614055,"Missing"
P11-2127,P05-1039,0,0.0222265,"ns more pruning. These are results without the coarseposterior tie-breaking to illustrate the sole effect of pruning. of Java code, including I/O.7 5.1 Other Treebanks One nice property of the parameter-free, allfragments SDP approach is that we can easily transfer it to any new domain with a treebank, or any new annotation of an existing treebank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7 6 PCFG + SDP SDP 20 # of fragments SDP PCFG PCFG+SDP test (≤ 40) F1 EX 66.9 18.4 84.0 21.6 86.9 31.5 F1 Model dev (≤ 40) F1 EX 66.2 18.0 83.8 20.0 86.4 30.6 These statistics can be further improved with standard parsing micro-optimization. 8 See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. test (≤ 40) F1 EX BROWN Gildea (2001) 84.1 – This Paper (PCFG+SDP) 84.7 34.6 GERMAN Dubey (2005) 76.3 – Petrov and Klein (2007) 80.8 40.8 This Paper (PCFG+SDP) 78.1 39."
P11-2127,W01-0521,0,0.012192,"ank. Table 3 shows domain adaptation performance by the results for training and testing on the Brown and German datasets.8 On Brown, we perform better than the relatively complex lexicalized Model 1 of Collins (1999). For German, our parser outperforms Dubey (2005) and we are not far behind latentvariable parsers, for which parsing is substantially 7 6 PCFG + SDP SDP 20 # of fragments SDP PCFG PCFG+SDP test (≤ 40) F1 EX 66.9 18.4 84.0 21.6 86.9 31.5 F1 Model dev (≤ 40) F1 EX 66.2 18.0 83.8 20.0 86.4 30.6 These statistics can be further improved with standard parsing micro-optimization. 8 See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. test (≤ 40) F1 EX BROWN Gildea (2001) 84.1 – This Paper (PCFG+SDP) 84.7 34.6 GERMAN Dubey (2005) 76.3 – Petrov and Klein (2007) 80.8 40.8 This Paper (PCFG+SDP) 78.1 39.3 Model test (all) F1 EX – 83.1 – 32.6 – 80.1 77.1 – 39.1 38.2 Table 3: Results for training and testing on the Brown and German treebanks. Gildea (2001) uses the lexicalized Collins’ Model 1 (Collins, 1999). Annotation S TAN -A NNOTATION B ERK -A NNOTATION test (≤ 40) F1 EX 88.1 34.3 90.0 38.9 test (all) F1 EX 87.4 32.2 89.5 36.8 Table 4: Resul"
P11-2127,W96-0214,0,0.0701869,"test-derivation parsing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the derivation which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell posteriors P (X, i, j|s) used to build the derivation).4 The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see 4 This is similar to the maximum recall objective for approximate inference (Goodman, 1996b). The product of posteriors also works equally well. 722 Model B&K2010 pruned B&K2010 unpruned dev (≤ 40) F1 EX 88.4 33.7 87.9 32.4 test (≤ 40) F1 EX 88.5 33.0 88.1 31.9 Table 1: Accuracy (F1) and exact match (EX) for Bansal and Klein (2010). The pruned row shows their original results with coarse-to-fine pruning. The unpruned row shows new results for an unpruned version of their parser; these accuracies are very similar to their pruned counterparts. Table 2). In addition, the speed of parsing and memory-requirements improve by more than an order of magnitude over the exact SDP pass alone."
P11-2127,P96-1024,0,0.0646017,"test-derivation parsing (after coarse-pruning), if two derivations have the same cost (i.e., the number of switches), then we break the tie between them by choosing the derivation which has a higher sum of coarse posteriors (i.e., the sum of the coarse PCFG chart-cell posteriors P (X, i, j|s) used to build the derivation).4 The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (see 4 This is similar to the maximum recall objective for approximate inference (Goodman, 1996b). The product of posteriors also works equally well. 722 Model B&K2010 pruned B&K2010 unpruned dev (≤ 40) F1 EX 88.4 33.7 87.9 32.4 test (≤ 40) F1 EX 88.5 33.0 88.1 31.9 Table 1: Accuracy (F1) and exact match (EX) for Bansal and Klein (2010). The pruned row shows their original results with coarse-to-fine pruning. The unpruned row shows new results for an unpruned version of their parser; these accuracies are very similar to their pruned counterparts. Table 2). In addition, the speed of parsing and memory-requirements improve by more than an order of magnitude over the exact SDP pass alone."
P11-2127,J98-4004,0,0.0768119,"Missing"
P11-2127,P03-1054,1,0.107655,"Missing"
P11-2127,N07-1051,1,0.819401,"Missing"
P11-2127,P06-1055,1,0.874824,"Missing"
P11-2127,P09-2012,0,0.168671,"Missing"
P11-2127,C92-2065,0,0.0715887,"Missing"
P11-2127,P00-1008,0,0.64147,"Missing"
P11-2127,J03-4003,0,\N,Missing
P12-1041,P11-1070,1,0.881287,"mentioned otherwise. example, the head of the Palestinian territories is the word territories). Next, we take each headword pair (h1 , h2 ) and compute various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the general co-occurrence group, helped for all mention pair types. The fifth feature group applies only to pairs in which the anaphor is a pronoun but the antecedent is a non-pronoun. Related work for each feature category is discussed inline. 3.1 General co-occurrence These features captu"
P12-1041,D08-1031,0,0.31621,"mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution procedures that already achieve near state-of-the-art results on multiple popular datasets using multiple standard metrics. It includes over 80 core features that exploit 390 various automatically generated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision trees by incrementally maximizi"
P12-1041,P06-1005,0,0.833709,"His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface affinities (i.e., (Obama, president) versus"
P12-1041,P10-1089,0,0.0107131,"and Hanks, 1989)).3 This normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10 , the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we us"
P12-1041,P89-1010,0,0.0434437,"not. Using the n-grams corpus (for n = 1 to 5), we collect co-occurrence Web-counts by allowing a varying number of wildcards between h1 and h2 in the query. The co-occurrence value is:    c12 bin log10 c1 · c2 where c12 = count(“h1 ? h2 ”) + count(“h1 ? ? h2 ”) + count(“h1 ? ? ? h2 ”), c1 = count(“h1 ”), and c2 = count(“h2 ”). We normalize the overall co-occurrence count of the headword pair c12 by the unigram counts of the individual headwords c1 and c2 , so that high-frequency headwords do not unfairly get a high feature value (this is similar to computing scaled mutual information MI (Church and Hanks, 1989)).3 This normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10 , the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al.,"
P12-1041,H05-1013,0,0.587923,"Missing"
P12-1041,1993.eamt-1.1,0,0.160596,"7.5 73.7 (p &lt; 0.005) 76.5 73.2 77.0 77.0 78.8 80.0 1.3 82.1 63.9 83.2 68.4 85.0 65.5 89.4 64.2 81.1 70.8 (p &lt; 0.1) 73.7 71.8 75.1 73.9 74.7 75.6 0.9 54.4 70.5 53.2 73.1 55.4 74.8 80.6 60.5 85.1 60.4 80.7 65.9 (p &lt; 0.001) 61.4 61.6 63.8 69.1 70.6 72.5 1.9 Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically extracted system mentions. B3 here is the B3 All version of Stoyanov et al. (2009). We also report statistical significance of the improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05ALL) has different results from Stoyanov et al. (2009) because their current publicly available code is different from that used in their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3 All nor B3 None). For completeness, our (untuned) B3 None results (DT + Web) on the ACE05-ALL dataset are P=69.9|R=65.9|F1=67.8. class / sens"
P12-1041,D09-1120,1,0.967,"nt amount of noise. Also, we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an"
P12-1041,N10-1061,1,0.570643,"d Dan Klein Computer Science Division University of California, Berkeley {mbansal, klein}@cs.berkeley.edu Abstract other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3 ), resulting in the be"
P12-1041,C92-2082,0,0.0237996,", the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was |were} {a |an |the}? h2 • h1 {and |or} {other |the other |another} h2 2 These clusters are derived form the V2 Google n-grams corpus. The V2 corpus itself is not publicly available, but the clusters are available at http://www.clsp.jhu.edu/ ˜sbergsma/PhrasalClusters 391 • h1 other than {a |an |the}? h2 3 We also tried adding count(“h1 h2”) to c12 but this d"
P12-1041,P11-1079,0,0.40519,"hat fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10 , the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was |were} {a |an |the}? h2 • h1 {and |or} {other |the other |another} h2 2 These clust"
P12-1041,P08-1068,0,0.0391799,"in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. However, we also assume that higher-ranked matches tend to imply closer meanings. To this end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2 . Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the only related work to our knowledge is from Daum´e III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al. (2005). 3.5 Pronoun context Our last feature category specifically addresses pronoun reference, for cases when the anaphoric mention N P2 (and hence its headword h2 ) is a pronoun, while the candidate antecedent mention N P1 (and hence its headword h1 ) is not. For such a headword pair (h1 , h2 ), the idea is to substitute the nonpronoun h1 into h2 ’s position and see"
P12-1041,P09-1116,0,0.0168332,"tributional K-Means clustering (with K = 1000) on phrases, using the n-gram context as features. The cluster data contains almost 10 million phrases and their soft cluster memberships. Up to twenty cluster ids with the highest centroid similarities are included for each phrase in this dataset (Lin et al., 2010). Our cluster-based features assume that if the headwords of the two mentions have matches in their cluster id lists, then they are more compatible for coreference. We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. However, we also assume that higher-ranked matches tend to imply closer meanings. To this end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2 . Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the"
P12-1041,lin-etal-2010-new,0,0.0412494,"e various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the general co-occurrence group, helped for all mention pair types. The fifth feature group applies only to pairs in which the anaphor is a pronoun but the antecedent is a non-pronoun. Related work for each feature category is discussed inline. 3.1 General co-occurrence These features capture co-occurrence statistics of the two headwords, i.e., how often h1 and h2 are seen adjacent or nearly adjacent on the Web. This count can be a"
P12-1041,J05-3004,0,0.15927,"ussed the economy, technology, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface"
P12-1041,P02-1014,0,0.174008,"ould be adapted to entity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution procedures that already achieve near state-of-the-art results on multiple popular datasets using multiple standard metrics. It includes over 80 core features that exploit 390 various automatically generated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision t"
P12-1041,P10-1142,0,0.0915512,"3.2 63.6 64.3 F1 65.9 69.5 69.8 70.0 70.4 70.7 71.3 P 82.2 89.5 88.7 89.1 88.1 87.9 88.0 B3 R 69.9 69.0 69.8 70.1 70.9 71.2 71.6 F1 75.5 77.9 78.1 78.5 78.6 78.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline (on the AC"
P12-1041,P04-1019,0,0.453558,"s, the president discussed the economy, technology, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents"
P12-1041,D09-1101,0,0.637989,"r of gold mentions in the test split; chn: the number of coreference chains in the test split. 4 Experiments 4.1 Data We show results on three popular and comparatively larger coreference resolution data sets – the ACE04, ACE05, and ACE05-ALL datasets from the ACE Program (NIST, 2004). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al. (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are"
P12-1041,P11-1082,0,0.278685,"we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an |the}? y taking seeds"
P12-1041,P05-1077,0,0.017262,"s end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2 . Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the only related work to our knowledge is from Daum´e III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al. (2005). 3.5 Pronoun context Our last feature category specifically addresses pronoun reference, for cases when the anaphoric mention N P2 (and hence its headword h2 ) is a pronoun, while the candidate antecedent mention N P1 (and hence its headword h1 ) is not. For such a headword pair (h1 , h2 ), the idea is to substitute the nonpronoun h1 into h2 ’s position and see whether the result is attested on the Web. If the anaphoric pronominal mention is h2 and its sentential context is l’ l h2 r r’, then the substituted phrase will be l’ l h1 r r’.5 High Web counts of substituted phrases tend to indicate"
P12-1041,P10-1144,0,0.199359,"C R 63.1 61.0 62.1 62.3 63.2 63.6 64.3 F1 65.9 69.5 69.8 70.0 70.4 70.7 71.3 P 82.2 89.5 88.7 89.1 88.1 87.9 88.0 B3 R 69.9 69.0 69.8 70.1 70.9 71.2 71.6 F1 75.5 77.9 78.1 78.5 78.6 78.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline"
P12-1041,J01-4004,0,0.573195,"significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our semantic Web features, we first describe our baseline. The core inference and features come from the Reconcile package (Stoyanov et al., 2009; Stoyanov et al., 2010), with modifications described below. Our baseline differs most substantially from Stoyanov et al. (2009) in using a decision tree classifier rather than an averaged linear perceptron. 2.1 Reconcile Reconcile is one of the best implementations of the mention-pair model (Soon et al., 2001) of coreference resolution. The mention-pair model relies on a pairwise function to determine whether or not two mentions are coreferent. Pairwise predictions are then consolidated by transitive closure (or some other clustering method) to form the final set of coreference clusters (chains). While our Web features could be adapted to entity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution"
P12-1041,P09-1074,0,0.134032,"(2) lexical relations (via Hearst-style hypernymy patterns), (3) similarity of entity-based context (e.g., common values of y for 389 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389–398, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics which h is a y is attested), (4) matches of distributional soft cluster ids, and (5) attested substitutions of candidate antecedents in the context of a pronominal anaphor. We first describe a strong baseline consisting of the mention-pair model of the Reconcile system (Stoyanov et al., 2009; Stoyanov et al., 2010) using a decision tree (DT) as its pairwise classifier. To this baseline system, we add our suite of features in turn, each class of features providing substantial gains. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE 2005) and metrics (MUC and B3 ), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our sem"
P12-1041,M95-1005,0,0.612371,"and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are available in the Reconcile infrastructure.8 MUC measures how many predicted clusters need to be merged to cover the true gold clusters. B3 computes precision and recall for each mention by computing the intersection of its predicted and gold cluster and dividing by the size of the predicted 7 Note that the development set is used only for ACE04, because for ACE05, and ACE05-ALL, we directly test using the features tuned on ACE04. 8 Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al. ("
P12-1041,P07-1067,0,0.117233,"the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an |the}? y taking seeds y in order of decreasing Web count. The corresponding ordered seed list Y = {y} gives us useful information about the headword’s entity type. For example,"
P12-1041,P05-1021,0,0.70583,"ogy, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface affinities (i.e., ("
P12-1041,J90-1003,0,\N,Missing
P12-1041,D08-1067,0,\N,Missing
P14-1098,P11-1070,1,0.486021,"Missing"
P14-1098,P06-1038,0,0.0161687,"mmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one co"
P14-1098,N12-1051,0,0.0313556,"nal probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi"
P14-1098,N03-1011,0,0.0146092,"ring only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most"
P14-1098,P96-1024,0,0.045358,"o et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). 1044 ties or syntactic word classes, which are primary drivers for dependency parsing, are mostly uninformative for taxonomy induction. Instead, inducing taxonomies requires world knowledge to capture the semantic relations between various unseen terms. For this, we use semantic cues to hypernymy and siblinghood via features on simple surface patterns and statistics in large text corpora. We fire features on both the edge and the sibling factors. We first describe all the edge features in detail (Section 3.1 and Section 3.2), and th"
P14-1098,C92-2082,0,0.253921,"gest common substring of xi and xj , and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj |)/2). Length difference: We compute the signed length difference between xj and xi , and create indicator features for rounded-off and binned values of (|xj |− |xi |)/((|xi |+ |xj |)/2). Yang and Callan (2009) use a similar feature. 3.2 3.2.1 Semantic Features Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi , C=xj ) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi , C=xj ), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negative signal).6 Next, for each pattern in this top-k list, we compute its normalized pattern count c, and fire an indicator feature on the tuple (pattern, t), for all thresholds t (in a fixed set) s.t. c ≥ t. Our supervised model then automatically learns which patterns are good in"
P14-1098,D09-1099,0,0.0559525,"et’s vertebrates taxonomy. Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edges with a shared par"
P14-1098,D07-1015,0,0.0118216,". Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief odds ratio: 3 bYij (ON) 5 bYij (OFF) . Features While spanning trees are familiar from nonprojective dependency parsing, features based on the linear order of the words or on lexical identi4 See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al. (2005) for an illustrative example. Also, we constrain the Chu-Liu-Edmonds MST algorithm to output only single-root MSTs, where the (dummy) root has exactly one child (Koo et al., 2007), because multi-root spanning ‘forests’ are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. 5 The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996)"
P14-1098,D10-1108,0,0.852751,"signal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise classifiers on man"
P14-1098,P08-1119,0,0.0317587,"ts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction"
P14-1098,D12-1093,0,0.0195349,"et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related"
P14-1098,C02-1144,0,0.0345728,"sibling factors are symmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our e"
P14-1098,P98-2127,0,0.0361892,"s that the sibling factors are symmetric (in the sense that Sijk is redundant to Sikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats.8 4 Related Work In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typically taking one of two major approaches. The clustering-based approach (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Yamada et al., 2009) discovers relations based on the assumption that similar concepts appear in sim7 One can also add features on the full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the trainin"
P14-1098,H05-1066,0,0.0473065,"Missing"
P14-1098,P10-1134,0,0.0105973,"in and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patterns for Navigli and Velardi (2010), Navigli et al. (2011)) to find intermediate terms and all the attested hypernymy links between them.10 To prune down the resulting tax9 Determining the set of input terms is orthogonal to our work, and our method can be used in conjunction with various term extraction approaches described above. 10 Unlike our system, which assumes a complete set of terms and only attempts to induce the taxonomic structure, 1046 onomy graph, Kozareva and Hovy (2010) use a procedure that iteratively retains the longest paths between root and leaf terms, removing conflicting graph edges as they go. The end resu"
P14-1098,P06-1015,0,0.01721,"fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors."
P14-1098,P06-1100,0,0.0219366,"1 Figure 1: An excerpt of WordNet’s vertebrates taxonomy. Introduction Many tasks in natural language understanding, such as question answering, information extraction, and textual entailment, benefit from lexical semantic information in the form of types and hypernyms. A recent example is IBM’s Jeopardy! system Watson (Ferrucci et al., 2010), which used type information to restrict the set of answer candidates. Information of this sort is present in term taxonomies (e.g., Figure 1), ontologies, and thesauri. However, currently available taxonomies such as WordNet are incomplete in coverage (Pennacchiotti and Pantel, 2006; Hovy et al., 2009), unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lexical taxonomies automatically. In this work, we focus on the task of taking collections of terms as input and predicting a complete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hypernym edg"
P14-1098,W02-1017,0,0.0512144,"full triple (xi , xj , xk ) but most such features will be sparse. 8 All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the abstracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzio"
P14-1098,P10-1031,0,0.0146464,"y maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words (Cimiano et al., 2005; Cimiano and Staab, 2005; Poon and Domingos, 2010; Fountain and Lapata, 2012). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words. We know of two closely-related previous systems, Kozareva and Hovy (2010) and Navigli et al. (2011), that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the final taxonomy tree, typically) and then using relational patterns (hand-selected ones in the case of Kozareva and Hovy (2010), and ones learned separately by a pairwise classifier on manually annotated co-occurrence patte"
P14-1098,W97-0313,0,0.0581972,"otstrapping using syntactic heuristics (Phillips and Riloff, 2002), dependency patterns (Snow et al., 2006), doubly anchored patterns (Kozareva et al., 2008; Hovy et al., 2009), and Web definition classifiers (Navigli et al., 2011). ilar contexts (Harris, 1954). The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster t"
P14-1098,D08-1016,0,0.0246291,"we wish to accomplish: computing expected feature counts and selecting a particular taxonomy tree for a given set of input terms (decoding). As an initial step to each of these procedures, we wish to compute the marginal probabilities of particular edges (and pairs of edges) being on. In a factor graph, the natural inference procedure for computing marginals is belief propagation. Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). Therefore, we will only briefly sketch the procedure here. Belief propagation is a general-purpose inference method that computes marginals via directed messages passed from variables to adjacent factors (and vice versa) in the factor graph. These messages take the form of (possibly unnormalized) distributions over values of the variable. The two types of messages (variable to factor or factor to variable) have mutually recursive definitions. The message from a factor F to an adjacent variable V involves a sum over all possible values of every other variable that F touches. While the E DGE a"
P14-1098,P06-1101,0,0.389767,"like rat is a rodent in large corpora is a strong signal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures exactly these types of intuitions by automatically discovering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental taxonomy induction system of Snow et al. (2006), the longest path approach of Kozareva and Hovy (2010), and the maximum spanning tree (MST) approach of Navigli et al. (2011) (see Section 4 for a more detailed overview). The main contribution of this work is that we present the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees, using a structured inference procedure through both the learning and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were h"
P14-1098,D08-1061,0,0.0139227,"Missing"
P14-1098,N03-1036,0,0.0418323,"relation lists (Phillips and Riloff, 2002; Girju et al., 2003; Pantel and Pennacchiotti, 2006; Suchanek et al., 2007; Ritter et al., 2009; Hovy et al., 2009; Baroni et al., 2010; Ponzetto and Strube, 2011) and semantic classes or classinstance pairs (Riloff and Shepherd, 1997; Katz and Lin, 2003; Pas¸ca, 2004; Etzioni et al., 2005; Talukdar et al., 2008). We focus on the second step of taxonomy induction, namely the structured organization of terms into a complete and coherent tree-like hierarchy.9 Early work on this task assumes a starting partial taxonomy and inserts missing terms into it. Widdows (2003) place unknown words into a region with the most semantically-similar neighbors. Snow et al. (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Yang and Callan (2009) incrementally cluster terms based on a pairwise semantic distance. Lao et al. (2012) extend a knowledge base using a random walk model to learn binary relational inference rules. However, the task of inducing full taxonomies without assuming a substantial initial partial taxonomy is relatively less well studied. There is some prior work on the related task o"
P14-1098,D09-1097,0,0.0196102,"Missing"
P14-1098,P09-1031,0,0.555984,"or not. This captures pairs such as (fish, bony fish) in our data. Contains: Checks if xj contains xi , or not. This captures pairs such as (bird, bird of prey). Suffix match: Checks whether the k-length suffixes of xi and xj match, or not, for k = 1, 2, . . . , 7. LCS: We compute the longest common substring of xi and xj , and create indicator features for rounded-off and binned values of |LCS|/((|xi |+ |xj |)/2). Length difference: We compute the signed length difference between xj and xi , and create indicator features for rounded-off and binned values of (|xj |− |xi |)/((|xi |+ |xj |)/2). Yang and Callan (2009) use a similar feature. 3.2 3.2.1 Semantic Features Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=xi , C=xj ) is often signaled by the presence of surface patterns like C is a P, P such as C in large text corpora, an observation going back to Hearst (1992). For each potential parent-child edge (P=xi , C=xj ), we mine the top k strings (based on count) in which both xi and xj occur (we use k=200). We collect patterns in both directions, which allows us to judge the correct direction of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negat"
P14-1098,C98-2122,0,\N,Missing
P14-2131,P11-2125,0,0.0198849,"Representations Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2 We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 1 We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their B ROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the S KIP DEP (§2.1.2) setting because it performs slightly better without it. 810 Representation B ROWN S ENNA T URIAN H UANG CBOW, w = 2 S KIP, w = 1 S KIP, w = 2 S KIP, w = 5 S KIP, w = 10 S KIP DEP S IM"
P14-2131,W13-3520,0,0.0657293,"Missing"
P14-2131,P14-2133,0,0.431574,"Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (T URIAN) while we compare several and also train our own, tailored to the task. Second, their embedding features are simpler than ours, only using flat (non-hierarchical) cluster IDs and binary strings obtained via sign quantization (1[x &gt; 0]) of the vectors. They also compare to a first-order baseline and only evaluate on the Web treebanks. Concurrently, Andreas and Klein (2014) investigate the use of embeddings in constituent parsing. There are several differences: we work on dependency parsing, use clustering-based features, and tailor our embeddings to dependency-style syntax; their work additionally studies vocabulary expansion and relating in-vocabulary words via embeddings. Web results: Table 6 shows our main Web results.12 Here, we see that the S ENNA, B ROWN, and S KIP DEP embeddings perform the best on average (and are statistically indistinguishable, except S ENNA vs. S KIP DEP on the reviews domain). They yield statistically significant UAS improvements ov"
P14-2131,P12-1092,0,0.796086,"pendency Parsing Mohit Bansal Kevin Gimpel Karen Livescu Toyota Technological Institute at Chicago, IL 60637, USA {mbansal, kgimpel, klivescu}@ttic.edu Abstract tinuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To th"
P14-2131,J14-1004,0,0.184327,"rsing. We compare several popular embeddings to Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and"
P14-2131,J92-4003,0,0.294466,"e of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2011). Koo et al. (2008) found improvement on indomain dependency parsing using features based on discrete Brown clusters. In this paper, we experiment with parsing features derived from con2 Continuous Word Representations There are many ways to train continuous representations; in this paper, we"
P14-2131,W07-2416,0,0.0430778,"s, specifically its secondorder projective model.9 We remove all features that occur only once in the training data. For WSJ parsing, we use the standard train(0221)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use Comparing bucket and bit string features: In Table 4, we find that bucket features based on individual embedding dimensions do not lead to improvements in test accuracy, while bit string features generally do. This is likely because individual embedding dimensions rarely correspond to interpretable or useful distinctions among words, whereas the hierarchical bit strings take into account all dimensions of the representations simultaneously. Their prefixes also naturally define features at multiple levels o"
P14-2131,P08-1068,0,0.907992,"both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on all test sets) in a fraction of the training time. Finally, a simple parser ensemble on all the representations achieves the best results, suggesting their complementarity for dependency parsing. Word representations have proven useful for many NLP tasks, e.g., Brown clusters as features in dependency parsing (Koo et al., 2008). In this paper, we investigate the use of continuous word representations as features for dependency parsing. We compare sever"
P14-2131,P09-1116,0,0.0387301,"n MSTParser, using bit string prefixes of the head, argument, sibling, intermediate words, etc., to augment or replace the POS and lexical identity information. We tried various sets of prefix lengths on the development set and found the best setting to use prefixes of length 4, 6, 8, and 12.5 3.2 Continuous Representation Features We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding 4 A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). 5 See Koo et al. (2008) for the exact feature templates. They used the full string in place of the length-12 prefixes, but that setting worked slightly worse for us. Note that the baseline parser used by Koo et al. (2008) is different from the second-order MSTParser that we use here; their parser allows grandparent interactions in addition to the sibling interactions in ours. We use their clusters, available at http://people. 3 For clustering, we use k-means with k = 1000 and initialize by placing centroids on the 1000 most-frequent words. csail.mit.edu/maestro/papers/bllip-clusters.gz. 811"
P14-2131,J93-2004,0,0.0490206,"ontinuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in terms of context within syntactic parses. To this end, we use two simple modifications to the models of Mikolov et al. (2013a): a smaller context window, and conditioning on syntactic context (dependency links and labels). Interestingly, the Brown clusters of Koo et al. (2008) prove to be difficult to beat, but we find that our syntactic tailoring can lead to embeddings that match the parsing performance of Brown (on al"
P14-2131,E06-1011,0,0.123145,"imilarity (S IM): One widely-used evaluation compares distances in the continuous space to human judgments of word similarity using the 353-pair dataset of Finkelstein et al. (2002). We compute cosine similarity between the two vectors in each word pair, then order the word pairs by similarity and compute Spearman’s rank correlation coefficient (ρ) with the gold similarities. Embeddings with high ρ capture similarity in terms of paraphrase and topical relationships. 3 Dependency Parsing Features We now discuss the features that we add to our baseline dependency parser (second-order MSTParser; McDonald and Pereira, 2006) based on discrete and continuous representations. 3.1 Clustering-based tagging accuracy (M-1): Intuitively, we expect embeddings to help parsing the most if they can tell us when two words are similar syntactically. To this end, we use a metric based on unsupervised evaluation of POS taggers. We perform clustering and map each cluster to one POS tag so as to maximize tagging accuracy, where multiple clusters can map to the same tag. We cluster vectors corresponding to the tokens in PTB WSJ sections 00-21.3 Table 2 shows these metrics for representations used in this paper. The B ROWN clusters"
P14-2131,P08-1109,0,0.0313486,"Missing"
P14-2131,S13-1035,0,0.00746473,"we parse the BLLIP corpus (minus PTB) using our baseline dependency parser, then build a corpus in which each line contains a single child word c, its parent word p, its grandparent g, and the dependency label ` of the hc, pi link: “`&lt;L&gt; g&lt;G&gt; p c `&lt;L&gt; ”, that is, both the dependency label and grandparent word are subscripted with a special token to avoid collision with words.2 We train the S KIP model on this corpus of tuples with window size w = 1, denoting the result S KIP DEP . Note that this approach needs a parsed corpus, but there also already exist such resources (Napoles et al., 2012; Goldberg and Orwant, 2013). Syntactically-tailored Representations We train word embeddings using the continuous bag-of-words (CBOW) and skip-gram (S KIP) models described in Mikolov et al. (2013a; 2013b) as implemented in the open-source toolkit word2vec. These models avoid hidden layers in the neural network and hence can be trained in only minutes, compared to days or even weeks for the others, as shown in Table 1.1 We adapt these embeddings to be more useful for dependency parsing in two ways, described next. 2.1.1 Smaller Context Windows The CBOW model learns vectors to predict a word given its set of surrounding"
P14-2131,N04-1043,0,0.0744049,"Brown clusters, via multiple types of features, in both news and web domains. We find that all embeddings yield significant parsing gains, including some recent ones that can be trained in a fraction of the time of others. Explicitly tailoring the representations for the task leads to further improvements. Moreover, an ensemble of all representations achieves the best results, suggesting their complementarity. 1 Introduction Word representations derived from unlabeled text have proven useful for many NLP tasks, e.g., partof-speech (POS) tagging (Huang et al., 2014), named entity recognition (Miller et al., 2004), chunking (Turian et al., 2010), and syntactic parsing (Koo et al., 2008; Finkel et al., 2008; T¨ackstr¨om et al., 2012). Most word representations fall into one of two categories. Discrete representations consist of memberships in a (possibly hierarchical) hard clustering of words, e.g., via kmeans or the Brown et al. (1992) algorithm. Continuous representations (or distributed representations or embeddings) consist of low-dimensional, real-valued vectors for each word, typically induced via neural language models (Bengio et al., 2003; Mnih and Hinton, 2007) or spectral methods (Deerwester e"
P14-2131,W13-3511,0,0.0352405,"of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2 We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 1 We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their B ROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the S KIP DEP (§2.1.2) setting because it performs slightly better without it. 810 Representation B ROWN S ENNA T URIAN H UANG CBOW, w = 2 S KIP, w = 1 S KIP, w = 2 S KIP, w = 5 S KIP, w = 10 S KIP DEP S IM – 49.8 29.5 62.6 34.7"
P14-2131,W03-3023,0,0.105048,"ith real-valued features (instead of bucketed indicator features) and similar conjunction variants, but these did not perform well. 8 We use prefixes of length 4, 6, 8, 12, 16, 20, and fulllength, again tuned on the development set. 9 We use the recommended MSTParser settings: trainingk:5 iters:10 loss-type:nopunc decode-type:proj 10 Our setup is different from SANCL 2012 (Petrov and McDonald, 2012) because the exact splits and test data were only available to participants. 11 We find similar improvements under labeled attachment score (LAS). We ignore punctuation : , “ ” . in our evaluation (Yamada and Matsumoto, 2003; McDonald et al., 2005). 812 System Baseline B ROWN S ENNA T URIAN H UANG CBOW S KIP S KIP DEP ans 82.6 83.4 83.7 83.0 83.1 82.9 83.1 83.3 eml nwg rev blog Avg 81.2 84.3 83.8 85.5 83.5 81.7 85.2 84.5 86.1 84.2 81.9 85.0 85.0 86.0 84.3 81.5 85.0 84.1 85.7 83.9 81.8 85.1 84.7 85.9 84.1 81.3 85.2 83.9 85.8 83.8 81.1 84.7 84.1 85.4 83.7 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average."
P14-2131,W12-3018,0,0.0120023,"Missing"
P14-2131,N13-1039,1,0.269497,"Missing"
P14-2131,J07-2002,0,0.0898319,"Missing"
P14-2131,W09-1119,0,0.013559,"84.5 86.1 84.2 81.9 85.0 85.0 86.0 84.3 81.5 85.0 84.1 85.7 83.9 81.8 85.1 84.7 85.9 84.1 81.3 85.2 83.9 85.8 83.8 81.1 84.7 84.1 85.4 83.7 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, H"
P14-2131,W96-0213,0,0.172904,"at occur only once in the training data. For WSJ parsing, we use the standard train(0221)/dev(22)/test(23) split and apply the NP bracketing patch by Vadas and Curran (2007). For Web parsing, we still train on WSJ 02-21, but test on the five Web domains (answers, email, newsgroup, reviews, and weblog) of the ‘English Web Treebank’ (LDC2012T13), splitting each domain in half (in original order) for the development and test sets.10 For both treebanks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). To evaluate, we use Comparing bucket and bit string features: In Table 4, we find that bucket features based on individual embedding dimensions do not lead to improvements in test accuracy, while bit string features generally do. This is likely because individual embedding dimensions rarely correspond to interpretable or useful distinctions among words, whereas the hierarchical bit strings take into account all dimensions of the representations simultaneously. Their prefixes also naturally define features at multiple levels of granularity. WSJ results: Table 5 shows our main WSJ results. Alt"
P14-2131,D11-1141,0,0.0144767,"2.9 83.1 83.3 eml nwg rev blog Avg 81.2 84.3 83.8 85.5 83.5 81.7 85.2 84.5 86.1 84.2 81.9 85.0 85.0 86.0 84.3 81.5 85.0 84.1 85.7 83.9 81.8 85.1 84.7 85.9 84.1 81.3 85.2 83.9 85.8 83.8 81.1 84.7 84.1 85.4 83.7 81.5 85.2 84.3 86.0 84.1 Ensemble Results ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 201"
P14-2131,W09-3829,0,0.0230055,"Intrinsic Evaluation of Representations Short of running end-to-end parsing experiments, how can we choose which representations to use for parsing tasks? Several methods have been proposed for intrinsic evaluation of word representa2 We use a subscript on g so that it will be treated differently from c when considering the context of p. We removed all g&lt;G&gt; from the vocabulary after training. We also tried adding information about POS tags. This increases M-1 (§2.2), but harms parsing performance, likely because the embeddings become too tag-like. Similar ideas have been used for clustering (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013), semantic space models (Pad´o and Lapata, 2007), and topic modeling (Boyd-Graber and Blei, 2008). 1 We train both models on BLLIP (LDC2000T43) with PTB removed, the same corpus used by Koo et al. (2008) to train their B ROWN clusters. We created a special vector for unknown words by averaging the vectors for the 50K least frequent words; we did not use this vector for the S KIP DEP (§2.1.2) setting because it performs slightly better without it. 810 Representation B ROWN S ENNA T URIAN H UANG CBOW, w = 2 S KIP, w = 1 S KIP, w = 2 S KIP, w = 5 S KIP,"
P14-2131,D11-1118,0,0.0282083,"Missing"
P14-2131,N12-1052,0,0.0728699,"Missing"
P14-2131,D11-1116,0,0.0124489,"ts ALL–B R 83.9 82.2 85.9 85.0 86.6 84.7 ALL 84.2 82.3 85.9 85.1 86.8 84.9 Table 6: Main UAS test results on Web treebanks. Here, ans=answers, eml=email, nwg=newsgroup, rev=reviews, blog=weblog, B R=B ROWN, Avg=Macro-average. 5 Related Work In addition to work mentioned above, relevant work that uses discrete representations exists for POS tagging (Ritter et al., 2011; Owoputi et al., 2013), named entity recognition (Ratinov and Roth, 2009), supersense tagging (Grave et al., 2013), grammar induction (Spitkovsky et al., 2011), constituency parsing (Finkel et al., 2008), and dependency parsing (Tratz and Hovy, 2011). Continuous representations in NLP have been evaluated for their ability to capture syntactic and semantic word similarity (Huang et al., 2012; Mikolov et al., 2013a; Mikolov et al., 2013b) and used for tasks like semantic role labeling, partof-speech tagging, NER, chunking, and sentiment classification (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013). For dependency parsing, Hisamoto et al. (2013) also used embedding features, but there are several differences between their work and ours. First, they use only one set of pre-trained embeddings (T URIA"
P14-2131,P10-1040,0,0.913842,"loring Continuous Word Representations for Dependency Parsing Mohit Bansal Kevin Gimpel Karen Livescu Toyota Technological Institute at Chicago, IL 60637, USA {mbansal, kgimpel, klivescu}@ttic.edu Abstract tinuous representations. We find that simple attempts based on discretization of individual word vector dimensions do not improve parsing. We see gains only after first performing a hierarchical clustering of the continuous word vectors and then using features based on the hierarchy. We compare several types of continuous representations, including those made available by other researchers (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012), and embeddings we have trained using the approach of Mikolov et al. (2013a), which is orders of magnitude faster than the others. The representations exhibit different characteristics, which we demonstrate using both intrinsic metrics and extrinsic parsing evaluation. We report significant improvements over our baseline on both the Penn Treebank (PTB; Marcus et al., 1993) and the English Web treebank (Petrov and McDonald, 2012). While all embeddings yield some parsing improvements, we find larger gains by tailoring them to capture similarity in te"
P14-2131,P07-1031,0,\N,Missing
P15-2115,P98-1013,0,0.340159,"efine a frame semantic parse as a tuple hT, F, R, Li. We define six features based on two parsed sentences hT 1 , F 1 , R1 , L1 i and hT 2 , F 2 , R2 , L2 i: • f1 : # frame label matches: |{hs, ti : s ∈ F 1 , t ∈ F 2 , s = t}| • f2 : # argument label matches: |{hs, ti : s ∈ L1 , t ∈ L2 , s = t}|. • f3 : # target matches, ignoring frame labels: |{hs, ti : s ∈ T 1 , t ∈ T 2 , s = t}|. Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet (Baker et al., 1998). This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three frames are identified. The target words pulled, all, and shelves have respective frame labels C AUSE MOTION, Q UANTITY, and NATU - • f4 : # argument matches, ignoring arg."
P15-2115,N10-1138,0,0.00955749,"es B+D+F B+D+F+S Bwc + D + Fc + Swc Experiments MCTest splits its stories into train, development, and test sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer λ and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts w and c to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Noce"
P15-2115,P14-2131,1,0.646995,"stion, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector fw+ as the vector summation of all words inside sentence w and fw× as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concate+ and f × . For the nate q and a, then calculate fqa qa bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use + , f + ) and cos(f × , f × ) as features, where cos(fqa w qa w cos is cosine similarity. For syntactic features, where τw is the bag of dependencies of w and τq"
P15-2115,D13-1160,0,0.0111299,"achan et al., 2015). We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic te"
P15-2115,J14-1002,0,0.0681273,"ed with an argument label in the parse. We denote the bag of argument labels in the parse by L. For each phrase r ∈ R, there is an argument label denoted Lr ∈ L. We define a frame semantic parse as a tuple hT, F, R, Li. We define six features based on two parsed sentences hT 1 , F 1 , R1 , L1 i and hT 2 , F 2 , R2 , L2 i: • f1 : # frame label matches: |{hs, ti : s ∈ F 1 , t ∈ F 2 , s = t}| • f2 : # argument label matches: |{hs, ti : s ∈ L1 , t ∈ L2 , s = t}|. • f3 : # target matches, ignoring frame labels: |{hs, ti : s ∈ T 1 , t ∈ T 2 , s = t}|. Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet (Baker et al., 1998). This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three fram"
P15-2115,N10-1145,0,0.0124901,"= a; (2) v = r but u 6= a; and (3) v 6= r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS (u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the"
P15-2115,D14-1067,0,0.019125,"We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroo"
P15-2115,W01-1201,0,0.0210934,"Missing"
P15-2115,P99-1042,0,0.537395,"Missing"
P15-2115,D14-1082,0,0.0279241,"t sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer λ and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts w and c to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The accuracy of different feature sets on DEV is given in Table 1.6 The boldface results correspond D"
P15-2115,J13-4004,0,0.0239521,"Missing"
P15-2115,W14-2416,0,0.0265491,"Missing"
P15-2115,P07-1098,0,0.0294712,"ollowing three categories: (1) v = r and u = a; (2) v = r but u 6= a; and (3) v 6= r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS (u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent eac"
P15-2115,P15-1121,0,0.360924,"Missing"
P15-2115,D13-1020,0,0.627704,"t vector θ with an entry for each feature, the prediction a ˆ for a new P and q is given by: Figure 1: Example output from SEMAFOR. a ˆ = arg max max θ&gt; f (P, w, q, a) RAL FEATURES . a∈A w∈W Given triples {hP i , q i , ai i}ni=1 , we minimize an `2 -regularized max-margin loss function: n  X min λ||θ||2 + − max θ&gt; f (P i , w, q i , ai ) θ i=1  + max a∈A &gt; w∈W i 0 i i  max θ f (P , w , q , a) + ∆(a, a ) w0 ∈W where λ is the weight of the `2 term and ∆(a, ai ) = 1 if a 6= ai and 0 otherwise. The latent variable w makes the loss function non-convex. 3 Features We start with two features from Richardson et al. (2013). Our first feature corresponds to their sliding window similarity baseline, which measures weighted word overlap between the bag of words constructed from the question/answer and the bag of words in the window. We call this feature B. The second feature corresponds to their word distance baseline, and is the minimal distance between two word occurrences in the passage that are also contained in the question/answer pair. We call this feature D. Space does not permit a detailed description. 3.1 Each frame has its own set of arguments; e.g., the C AUSE MOTION frame has the labeled Agent, Theme,"
P15-2115,P15-1024,0,0.43581,"Missing"
P15-2115,P10-1040,0,0.0293802,"let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector fw+ as the vector summation of all words inside sentence w and fw× as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concate+ and f × . For the nate q and a, then calculate fqa qa bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use + , f + ) and cos(f × , f × ) as features, where cos(fqa w qa w cos is cosine similarity. For syntactic features, where τw is the"
P15-2115,D07-1003,0,0.00989106,"s: (1) v = r and u = a; (2) v = r but u 6= a; and (3) v 6= r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS (u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS (r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dim"
P15-2115,C98-1013,0,\N,Missing
P16-1105,P11-1056,0,0.017295,"ettings as Li and Ji (2014). We report the primary micro F1-scores as well as micro precision and recall on both entity and relation extraction to better explain model performance. We treat an entity as correct when its type and the region of its head are correct. We treat a relation as correct when its type and argument entities are correct; we thus treat all non-negative relations on wrong entities as false positives. ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types. We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014), and the preprocessing and evaluation metrics of ACE05. SemEval-2010 Task 8 defines 9 relation types between nominals and a tenth type Other when two nouns have none of these relations (Hendrickx et al., 2010). We treat this Other type as a negative relation type, and no direction is considered. The dataset consists of 8,000 training and 2,717 test sentences, and each sentence is annotated with a relation between two given nominals. We randomly selected 800 sentences from the training set as our development set. We followed the official task setting, and report the offici"
P16-1105,D14-1082,0,0.0567617,"rection is considered. The dataset consists of 8,000 training and 2,717 test sentences, and each sentence is annotated with a relation between two given nominals. We randomly selected 800 sentences from the training set as our development set. We followed the official task setting, and report the official macro-averaged F1-score (Macro-F1) on the 9 relation types. For more details of the data and task settings, please refer to the supplementary material. 4.2 Experimental Settings We implemented our model using the cnn library.6 We parsed the texts using the Stanford neural dependency parser7 (Chen and Manning, 2014) with the original Stanford Dependencies. Based on preliminary tuning, we fixed embedding dimensions nw to 200, np , nd , ne to 25, and dimensions of intermediate layers (nls , nlt of LSTM-RNNs and nhe , nhr of hidden layers) to 100. We initialized word vectors via word2vec (Mikolov et al., 2013) trained on Wikipedia8 and randomly initialized all other parameters. We tuned hyper-parameters using development sets for ACE05 and SemEval2010 Task 8 to achieve high primary (Micro- and Macro-) F1-scores.9 For ACE04, we directly employed the best parameters for ACE05. The hyperparameter settings are"
P16-1105,W06-1670,0,0.0133803,"ee), FullTree performs significantly worse than other structures regardless of whether or not we distinguish the nodes in the shortest paths from the other nodes, which hints that the information outside of the shortest path significantly hurts the performance (p&lt;0.05). We also compare our treestructured LSTM-RNN (SPTree) with sequencebased LSTM-RNNs (SPSeq and SPXu) and treestructured LSTM-RNNs (Child-Sum). All these LSTM-RNNs perform slightly worse than our SP12 When incorporating WordNet information into our model, we prepared embeddings for WordNet hypernyms extracted by SuperSenseTagger (Ciaramita and Altun, 2006) and concatenated the embeddings to the input vector (the concatenation of word and POS embeddings) of the sequence LSTM. We tuned the dimension of the WordNet embeddings and set it to 15 using the development dataset. 1112 Settings SPTree −Hidden layer −Sequence layer −Pair −Pair, Sequence layer Stanford PCFG +WordNet Left-to-right candidates Neg. sampling (Xu et al., 2015a) Macro-F1 0.851 0.839 0.840 0.844 0.827∗ 0.844 0.854 0.843 0.848 mance, but the difference was small and hence the creation of right-to-left candidates was not critical. Treating the inverse relation candidate as a negativ"
P16-1105,doddington-etal-2004-automatic,0,0.221885,"ypes and 6 coarse-grained relation types between entities. We use the same data splits, preprocessing, and task settings as Li and Ji (2014). We report the primary micro F1-scores as well as micro precision and recall on both entity and relation extraction to better explain model performance. We treat an entity as correct when its type and the region of its head are correct. We treat a relation as correct when its type and argument entities are correct; we thus treat all non-negative relations on wrong entities as false positives. ACE04 defines the same 7 coarse-grained entity types as ACE05 (Doddington et al., 2004), but defines 7 coarse-grained relation types. We follow the cross-validation setting of Chan and Roth (2011) and Li and Ji (2014), and the preprocessing and evaluation metrics of ACE05. SemEval-2010 Task 8 defines 9 relation types between nominals and a tenth type Other when two nouns have none of these relations (Hendrickx et al., 2010). We treat this Other type as a negative relation type, and no direction is considered. The dataset consists of 8,000 training and 2,717 test sentences, and each sentence is annotated with a relation between two given nominals. We randomly selected 800 sentenc"
P16-1105,P15-1061,0,0.498544,"ing via neural network (NN) based models. There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures. Word sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words 1105 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, p"
P16-1105,W01-0722,0,0.351558,"ities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. Previous joint models have employed feature-based structured learning. An alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models. There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN archit"
P16-1105,W03-0426,0,0.139057,"tion (SemEval-2010 Task 8), our model compares favorably to the state-of-the-art CNNbased model in F1-score. Finally, we also ablate and compare our various model components, which leads to some key findings (both positive and negative) about the contribution and effectiveness of different RNN structures, input dependency relation structures, different parsing models, external resources, and joint learning settings. 2 Related Work LSTM-RNNs have been widely used for sequential labeling, such as clause identification (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003). Recently, Huang et al. (2015) showed that building a conditional random field (CRF) layer on top of bidirectional LSTM-RNNs performs comparably to the state-of-the-art methods in the partof-speech (POS) tagging, chunking, and NER. For relation classification, in addition to traditional feature/kernel-based approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), several neural models have been proposed in the SemEval-2010 Task 8 (Hendrickx et al., 2010), including embedding-based models (Hashimoto et al., 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et"
P16-1105,K15-1027,1,0.788974,"cation (Hammerton, 2001), phonetic labeling (Graves and Schmidhuber, 2005), and NER (Hammerton, 2003). Recently, Huang et al. (2015) showed that building a conditional random field (CRF) layer on top of bidirectional LSTM-RNNs performs comparably to the state-of-the-art methods in the partof-speech (POS) tagging, chunking, and NER. For relation classification, in addition to traditional feature/kernel-based approaches (Zelenko et al., 2003; Bunescu and Mooney, 2005), several neural models have been proposed in the SemEval-2010 Task 8 (Hendrickx et al., 2010), including embedding-based models (Hashimoto et al., 2015), CNN-based models (dos Santos et al., 2015), and RNN-based models (Socher et al., 2012). Recently, Xu et al. (2015a) and Xu et al. (2015b) showed that the shortest dependency paths between relation arguments, which were used in feature/kernel-based systems (Bunescu and Mooney, 2005), are also useful in NN-based models. Xu et al. (2015b) also showed that LSTMRNNs are useful for relation classification, but the performance was worse than CNN-based models. Li et al. (2015) compared separate sequence-based and tree-structured LSTM-RNNs on relation classification, using basic RNN model structures."
P16-1105,S10-1006,0,0.0387369,"Missing"
P16-1105,W10-2924,0,0.0935209,"al such novel model structures and training settings, investigating the simultaneous use of bidirectional sequential and bidirectional tree-structured LSTM-RNNs to jointly capture linear and dependency context for end-toend extraction of relations between entities. As for end-to-end (joint) extraction of relations between entities, all existing models are featurebased systems (and no NN-based model has been proposed). Such models include structured prediction (Li and Ji, 2014; Miwa and Sasaki, 2014), integer linear programming (Roth and Yih, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). Among these, structured prediction methods are state-of-the-art on several corpora. We present an improved, NN-based alternative for the end-to-end relation extraction. 1106 neural net / softmax dropout PHYS Dependency (Relation) softmax LSTM unit tanh embeddings hidden label embeddings Sequence (Entity) B-PER born L-PER in Yates softmax tanh tanh Chicago Bi-TreeLSTM hidden ・・・ ・・・ dependency embeddings Bi-LSTM word/POS embeddings In 1909 nsubjpass , Sidney Yates was pobj prep born in Chicago . Fig. 1: Our incr"
P16-1105,P14-1038,0,0.849273,"8). Finally, we present an extensive ablation analysis of several model components. 1 Introduction Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information. For instance, to learn that Toefting and Bolton have an OrganizationAffiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. Previous joint models have employed feature-based structured learning. An alternative approach to this end-to-end relation extra"
P16-1105,D15-1278,0,0.177916,"xtraction task is to employ automatic feature learning via neural network (NN) based models. There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures. Word sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words 1105 Proceedings of the 54th Annual Meet"
P16-1105,D15-1102,0,0.0159106,"Missing"
P16-1105,D14-1200,1,0.90077,"resent an extensive ablation analysis of several model components. 1 Introduction Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information. For instance, to learn that Toefting and Bolton have an OrganizationAffiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relation. Previous joint models have employed feature-based structured learning. An alternative approach to this end-to-end relation extraction task is to employ"
P16-1105,W09-1119,0,0.477485,"n ACE2005 and ACE2004, respectively. We also show that our LSTMRNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components. 1 Introduction Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information. For instance, to learn that Toefting and Bolton have an OrganizationAffiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the"
P16-1105,D12-1110,0,0.494793,"Missing"
P16-1105,P15-1150,0,0.663587,"context words transferred to, which indicate an employment relation. Previous joint models have employed feature-based structured learning. An alternative approach to this end-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models. There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic st"
P16-1105,D15-1062,0,0.17918,"-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models. There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures. Word sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words 1105 Proceedings of t"
P16-1105,D15-1206,0,0.240285,"-to-end relation extraction task is to employ automatic feature learning via neural network (NN) based models. There are two ways to represent relations between entities using neural networks: recurrent/recursive neural networks (RNNs) and convolutional neural networks (CNNs). Among these, RNNs can directly represent essential linguistic structures, i.e., word sequences (Hammerton, 2001) and constituent/dependency trees (Tai et al., 2015). Despite this representation ability, for relation classification tasks, the previously reported performance using long short-term memory (LSTM) based RNNs (Xu et al., 2015b; Li et al., 2015) is worse than one using CNNs (dos Santos et al., 2015). These previous LSTM-based systems mostly include limited linguistic structures and neural architectures, and do not model entities and relations jointly. We are able to achieve improvements over state-of-the-art models via endto-end modeling of entities and relations based on richer LSTM-RNN architectures that incorporate complementary linguistic structures. Word sequence and tree structure are known to be complementary information for extracting relations. For instance, dependencies between words 1105 Proceedings of t"
P16-1105,P13-1161,0,0.0106233,"dependency tree information. We propose several such novel model structures and training settings, investigating the simultaneous use of bidirectional sequential and bidirectional tree-structured LSTM-RNNs to jointly capture linear and dependency context for end-toend extraction of relations between entities. As for end-to-end (joint) extraction of relations between entities, all existing models are featurebased systems (and no NN-based model has been proposed). Such models include structured prediction (Li and Ji, 2014; Miwa and Sasaki, 2014), integer linear programming (Roth and Yih, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). Among these, structured prediction methods are state-of-the-art on several corpora. We present an improved, NN-based alternative for the end-to-end relation extraction. 1106 neural net / softmax dropout PHYS Dependency (Relation) softmax LSTM unit tanh embeddings hidden label embeddings Sequence (Entity) B-PER born L-PER in Yates softmax tanh tanh Chicago Bi-TreeLSTM hidden ・・・ ・・・ dependency embeddings Bi-LSTM word/POS embeddings In 1909 nsubjpass , Sidney Yates wa"
P16-1105,C10-2160,0,0.0278642,"the simultaneous use of bidirectional sequential and bidirectional tree-structured LSTM-RNNs to jointly capture linear and dependency context for end-toend extraction of relations between entities. As for end-to-end (joint) extraction of relations between entities, all existing models are featurebased systems (and no NN-based model has been proposed). Such models include structured prediction (Li and Ji, 2014; Miwa and Sasaki, 2014), integer linear programming (Roth and Yih, 2007; Yang and Cardie, 2013), card-pyramid parsing (Kate and Mooney, 2010), and global probabilistic graphical models (Yu and Lam, 2010; Singh et al., 2013). Among these, structured prediction methods are state-of-the-art on several corpora. We present an improved, NN-based alternative for the end-to-end relation extraction. 1106 neural net / softmax dropout PHYS Dependency (Relation) softmax LSTM unit tanh embeddings hidden label embeddings Sequence (Entity) B-PER born L-PER in Yates softmax tanh tanh Chicago Bi-TreeLSTM hidden ・・・ ・・・ dependency embeddings Bi-LSTM word/POS embeddings In 1909 nsubjpass , Sidney Yates was pobj prep born in Chicago . Fig. 1: Our incrementally-decoded end-to-end relation extraction model, with"
P16-1105,P05-1053,0,0.960797,"d model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components. 1 Introduction Extracting semantic relations between entities in text is an important and well-studied task in information extraction and natural language processing (NLP). Traditional systems treat this task as a pipeline of two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007; Ratinov and Roth, 2009) and relation extraction (Zelenko et al., 2003; Zhou et al., 2005), but recent studies show that end-to-end (joint) modeling of entity and relation is important for high performance (Li and Ji, 2014; Miwa and Sasaki, 2014) since relations interact closely with entity information. For instance, to learn that Toefting and Bolton have an OrganizationAffiliation (ORG-AFF) relation in the sentence Toefting transferred to Bolton, the entity information that Toefting and Bolton are Person and Organization entities is important. Extraction of these entities is in turn encouraged by the presence of the context words transferred to, which indicate an employment relati"
P16-1105,W09-2415,0,\N,Missing
P16-1105,H05-1091,0,\N,Missing
P17-1117,D15-1075,0,0.242411,"quence-to-sequence models (Luong et al., 2016) that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms. The unsupervised video prediction task, i.e., video-to-video generation (adapted from Srivastava et al. (2015)), shares its encoder with the video captioning task’s encoder, and helps it learn richer video representations that can predict their temporal context and action sequence. The entailment generation task, i.e., premise-to-entailment generation (based on the image caption domain SNLI corpus (Bowman et al., 2015)), shares its decoder with the video captioning decoder, and helps it learn better video-entailing caption representations, since the caption is essentially an entailment of the video, i.e., it describes subsets of objects and events that are logically implied by or follow from the full video content). The overall many-tomany multi-task model combines all three tasks. Our three novel multi-task models show statistically significant improvements over the state-ofthe-art, and achieve the best-reported results (and rank) on multiple datasets, based on several automatic and human evaluations. We a"
P17-1117,P11-1020,0,0.0780676,"ions of the video captioning model. We train the multi-task model by alternately optimizing each task in mini-batches based on a mixing ratio. Let αv , αf , and αe be the number of mini-batches optimized alternately from each of these three tasks – video captioning, unsupervised video future frames prediction, and entailment generation, resp. Then the mixing ratio is deα αv fined as (αv +α : (αv +αff +αe ) : (αv +ααfe +αe ) . f +αe ) 4 Experimental Setup 4.1 Datasets Video Captioning Datasets We report results on three popular video captioning datasets. First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con2 Empirically, logical entailment helped captioning more than simple fusion with language modeling (i.e., partial sentence completion with no logical implication), because a caption also entails a video in a logically-directed sense and hence the entailment generation task matches the video captioning task better than language modeling. Moreover, a multi-task setup is more suitable to add directed information such as entailment (as opposed to pretraining or fusion with only the decoder). Details in Sec. 5.1. tains 1970 YouTube videos in the wild with several"
P17-1117,W14-3348,0,0.114211,"These sampled frames are then converted into features using several stateof-the-art pre-trained models on ImageNet (Deng et al., 2009) – VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016). Details of these feature dimensions and layer positions are in the supplementary. 4.2 Evaluation (Automatic and Human) For our video captioning as well as entailment generation results, we use four diverse automatic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004). Particularly, METEOR and CIDEr-D have been justified to be better for generation tasks, because CIDEr-D uses consensus among the (large) number of references and METEOR uses soft matching based on stemming, paraphrasing, and WordNet synonyms. We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.3 We also present human evaluation results based 3 We use avg. of these four metrics on validation se"
P17-1117,S14-2131,0,0.092149,"Missing"
P17-1117,S14-2055,0,0.0498629,"ce or predict the future sequence. We model video generation with an attention-enhanced encoder-decoder and harness it to improve video captioning. The task of recognizing textual entailment (RTE) is to classify whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (neutral), which is helpful for several downstream NLP tasks. The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014). However, directly generating the entailed hypothesis sentences given a premise sentence would be even more beneficial than retrieving or reranking sentence pairs, because most downstream generation tasks only come with the source sentence and not pairs. Recently, Kolesnyk et al. (2016) tried a sequenceto-sequence model for this on the original SNLI dataset, which is a single-reference setting and hence restricts automatic evaluation. We modify the SNLI corpus to a new multi-reference (and a more challenging zero train-test premise overlap) setting, and present a novel"
P17-1117,W04-1013,0,0.0440574,"ImageNet (Deng et al., 2009) – VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016). Details of these feature dimensions and layer positions are in the supplementary. 4.2 Evaluation (Automatic and Human) For our video captioning as well as entailment generation results, we use four diverse automatic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004). Particularly, METEOR and CIDEr-D have been justified to be better for generation tasks, because CIDEr-D uses consensus among the (large) number of references and METEOR uses soft matching based on stemming, paraphrasing, and WordNet synonyms. We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.3 We also present human evaluation results based 3 We use avg. of these four metrics on validation set to choose the best model, except for single-reference M-VAD dataset where we only report"
P17-1117,P02-1040,0,0.100231,"rted into features using several stateof-the-art pre-trained models on ImageNet (Deng et al., 2009) – VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016). Details of these feature dimensions and layer positions are in the supplementary. 4.2 Evaluation (Automatic and Human) For our video captioning as well as entailment generation results, we use four diverse automatic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004). Particularly, METEOR and CIDEr-D have been justified to be better for generation tasks, because CIDEr-D uses consensus among the (large) number of references and METEOR uses soft matching based on stemming, paraphrasing, and WordNet synonyms. We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.3 We also present human evaluation results based 3 We use avg. of these four metrics on validation set to choose the best model, exce"
P17-1117,C14-1115,0,0.0198208,"ribes subsets of objects and events that are logically implied by or follow from the full video content). The overall many-tomany multi-task model combines all three tasks. Our three novel multi-task models show statistically significant improvements over the state-ofthe-art, and achieve the best-reported results (and rank) on multiple datasets, based on several automatic and human evaluations. We also demonstrate that video captioning, in turn, gives mutual improvements on the new multi-reference entailment generation task. 2 Related Work Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder. To harness the important frame sequence temporal ordering, Venugopalan et al. (2015a) proposed a sequence-to-sequence model with video encoder and language decoder RNNs. More recently, Venugopalan et al. (2016) explored linguistic improvements to the captio"
P17-1117,D16-1204,0,0.727875,"nt of powerful image processing features as well as sequence-to-sequence LSTM models. It is also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the temporal across-frame action dynamics and the logical storyline language dynamics. Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video. A sequence-to-sequence model is then used to ‘translate’ the video to a caption. Venugopalan et al. (2016) showed linguistic improvements over this by fusing the decoder with external language models. Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a). More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016). Despite these recent improvements, video captioning models still suffer from the lack of sufficient temporal and logical supervision to"
P17-1117,N15-1173,0,0.760068,"lications such as assistance to a visually impaired person and improving the quality of online video search or retrieval. This task has gained recent momentum in the natural language processing and computer vision communities, esp. with the advent of powerful image processing features as well as sequence-to-sequence LSTM models. It is also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the temporal across-frame action dynamics and the logical storyline language dynamics. Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video. A sequence-to-sequence model is then used to ‘translate’ the video to a caption. Venugopalan et al. (2016) showed linguistic improvements over this by fusing the decoder with external language models. Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a). More recently, hierarchical two-level RNNs were proposed to allow for longer inpu"
P18-1063,J10-3005,0,0.326988,"tochastic policy. We set the reward for the agent performing EOE to ROUGE-1F1 ([{g(djt )}t ], [{st }t ]); whereas for any extraneous, unwanted extraction step, the agent receives zero reward. The model is therefore encouraged to extract when there are still remaining ground-truth summary sentences (to accumulate intermediate reward), and learn to stop by optimizing a global ROUGE and avoiding extra extraction.8 Overall, this modification allows dy4 Related Work Early summarization works mostly focused on extractive and compression based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep th"
P18-1063,W14-3348,0,0.222585,"further define another latent function f : X → Dn that satisfies f (x) = {dt }nj=1 and y = h(x) = [g(d1 ), g(d2 ), . . . , g(dn )], where [, ] denotes sentence concatenation. This latent function f can be seen as an extractor that chooses the salient (ordered) sentences in a given document for the abstracting function g to rewrite. Our overall model consists of these two submodules, the extractor agent and the abstractor network, to approximate the above-mentioned f and g, respectively. Empirically, our approach is the new state-ofthe-art on all ROUGE metrics (Lin, 2004) as well as on METEOR (Denkowski and Lavie, 2014) of the CNN/Daily Mail dataset, achieving statistically significant improvements over previous models that use complex long-encoder, copy, and coverage mechanisms (See et al., 2017). The test-only DUC-2002 improvement also shows our model’s better generalization than this strong abstractive system. In addition, we surpass the popular lead-3 baseline on all ROUGE scores with an abstractive model. Moreover, our sentence-level abstractive rewriting module also produces substantially more (3x) novel N -grams that are not seen in the input document, as compared to the strong flat-structured model o"
P18-1063,P00-1041,0,0.286023,"training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC2002 dataset, where we achieve higher scores than a state-of-the-art model. 1 Introduction The task of document summarization has two main paradigms: extractive and abstractive. The former method directly chooses and outputs the salient sentences (or phrases) in the original document (Jing and McKeown, 2000; Knight and Marcu, 2000; Martins and Smith, 2009; BergKirkpatrick et al., 2011). The latter abstractive approach involves rewriting the summary (Banko et al., 2000; Zajic et al., 2004), and has seen substantial recent gains due to neural sequence-to675 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 675–686 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics sentence-pairs between the document and ground truth summary. Next, our model achieves the new state-of-the-art on all metrics of multiple versions of a popular summarization dataset (as well as a test-only dataset) both extractively and abstractively, without loss in language fluency (also demonstrat"
P18-1063,D15-1042,0,0.130138,"Missing"
P18-1063,P11-1049,0,0.126368,"the reward for the agent performing EOE to ROUGE-1F1 ([{g(djt )}t ], [{st }t ]); whereas for any extraneous, unwanted extraction step, the agent receives zero reward. The model is therefore encouraged to extract when there are still remaining ground-truth summary sentences (to accumulate intermediate reward), and learn to stop by optimizing a global ROUGE and avoiding extra extraction.8 Overall, this modification allows dy4 Related Work Early summarization works mostly focused on extractive and compression based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep the abstractor parameters fixed."
P18-1063,W09-1802,0,0.131596,"nt small networks on extractive QA, resulting a scalable, parallelizable model. Fan et al. (2017) added controlling parameters to adapt the summary to length, style, and entity preferences. However, none of these used RL to bridge the non-differentiability of neural models. Our model shares some high-level intuition with extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please ref"
P18-1063,D17-1210,0,0.0246894,"et al. (2015) use Q-learning based RL for extractive summarization. Paulus et al. (2018) use RL policy gradient methods for abstractive summarization, utilizing sequence-level metric rewards with curriculum learning (Ranzato et al., 2016) or weighted ML+RL mixed loss (Paulus et al., 2018) for stability and language fluency. We use sentence-level rewards to optimize the extractor while keeping our ML trained abstractor decoder fixed, so as to achieve the best of both worlds. Training a neural network to use another fixed network has been investigated in machine translation for better decoding (Gu et al., 2017a) and real-time translation (Gu et al., 2017b). They used a fixed pretrained translator and applied policy gradient techniques to train another task-specific network. In question answering (QA), Choi et al. (2017) extract one sentence and then generate the answer from the sentence’s vector representation with RL bridging. Another recent work attempted a new coarse-to-fine attention approach on summarization (Ling and Rush, 2017) and found desired sharp focus properties for scaling to larger inputs (though without metric improvements). Very recently (concurrently), Narayan et al. (2018) use RL"
P18-1063,P15-1153,0,0.0251805,"l shares some high-level intuition with extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please refer to the supplementary for full training details (all hyperparameter tuning was performed on the validation set). We use the CNN/Daily Mail dataset (Hermann et al., 2015) modified for summarization (Nallapati et al., 2016). Because there are two versions of the dataset, origin"
P18-1063,P16-1154,0,0.0562934,"ion based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep the abstractor parameters fixed. Because the input sentences for the abstractor are extracted by an intermediate stochastic policy of the extractor, it is impossible to find the correct target summary for the abstractor to fit g with ML objective. Though it is possible to optimize the abstractor with RL, in out preliminary experiments we found that this does not improve the overall ROUGE, most likely because this RL optimizes at a sentence-level and can add across-sentence redundancy. We achieve SotA results without this"
P18-1063,N18-1150,0,0.197457,"trained translator and applied policy gradient techniques to train another task-specific network. In question answering (QA), Choi et al. (2017) extract one sentence and then generate the answer from the sentence’s vector representation with RL bridging. Another recent work attempted a new coarse-to-fine attention approach on summarization (Ling and Rush, 2017) and found desired sharp focus properties for scaling to larger inputs (though without metric improvements). Very recently (concurrently), Narayan et al. (2018) use RL for ranking sentences in pure extraction-based summarization and C ¸ elikyilmaz et al. (2018) investigate multiple communicating encoder agents 5.1 Evaluation Metrics For all the datasets, we evaluate standard ROUGE1, ROUGE-2, and ROUGE-L (Lin, 2004) on fulllength F1 (with stemming) following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018). Following See et al. (2017), we also evaluate on METEOR (Denkowski and Lavie, 2014) for a more thorough analysis. 5.2 Modular Extractive vs. Abstractive Our hybrid approach is capable of both extractive and abstractive (i.e., rewriting every sentence) summarization. The extractor alone performs extractive summarizatio"
P18-1063,E17-1099,0,0.0200634,"et al. (2015) use Q-learning based RL for extractive summarization. Paulus et al. (2018) use RL policy gradient methods for abstractive summarization, utilizing sequence-level metric rewards with curriculum learning (Ranzato et al., 2016) or weighted ML+RL mixed loss (Paulus et al., 2018) for stability and language fluency. We use sentence-level rewards to optimize the extractor while keeping our ML trained abstractor decoder fixed, so as to achieve the best of both worlds. Training a neural network to use another fixed network has been investigated in machine translation for better decoding (Gu et al., 2017a) and real-time translation (Gu et al., 2017b). They used a fixed pretrained translator and applied policy gradient techniques to train another task-specific network. In question answering (QA), Choi et al. (2017) extract one sentence and then generate the answer from the sentence’s vector representation with RL bridging. Another recent work attempted a new coarse-to-fine attention approach on summarization (Ling and Rush, 2017) and found desired sharp focus properties for scaling to larger inputs (though without metric improvements). Very recently (concurrently), Narayan et al. (2018) use RL"
P18-1063,P16-1046,0,0.316411,"extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please refer to the supplementary for full training details (all hyperparameter tuning was performed on the validation set). We use the CNN/Daily Mail dataset (Hermann et al., 2015) modified for summarization (Nallapati et al., 2016). Because there are two versions of the dataset, original text and entity anonymized, we show resul"
P18-1063,P17-1020,0,0.0249583,"ning (Ranzato et al., 2016) or weighted ML+RL mixed loss (Paulus et al., 2018) for stability and language fluency. We use sentence-level rewards to optimize the extractor while keeping our ML trained abstractor decoder fixed, so as to achieve the best of both worlds. Training a neural network to use another fixed network has been investigated in machine translation for better decoding (Gu et al., 2017a) and real-time translation (Gu et al., 2017b). They used a fixed pretrained translator and applied policy gradient techniques to train another task-specific network. In question answering (QA), Choi et al. (2017) extract one sentence and then generate the answer from the sentence’s vector representation with RL bridging. Another recent work attempted a new coarse-to-fine attention approach on summarization (Ling and Rush, 2017) and found desired sharp focus properties for scaling to larger inputs (though without metric improvements). Very recently (concurrently), Narayan et al. (2018) use RL for ranking sentences in pure extraction-based summarization and C ¸ elikyilmaz et al. (2018) investigate multiple communicating encoder agents 5.1 Evaluation Metrics For all the datasets, we evaluate standard ROU"
P18-1063,N16-1012,0,0.224566,"reward. The model is therefore encouraged to extract when there are still remaining ground-truth summary sentences (to accumulate intermediate reward), and learn to stop by optimizing a global ROUGE and avoiding extra extraction.8 Overall, this modification allows dy4 Related Work Early summarization works mostly focused on extractive and compression based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep the abstractor parameters fixed. Because the input sentences for the abstractor are extracted by an intermediate stochastic policy of the extractor, it is impossible to find the corr"
P18-1063,D13-1158,0,0.108243,"Missing"
P18-1063,D15-1166,0,0.110327,"Missing"
P18-1063,A00-2024,0,0.569421,"and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC2002 dataset, where we achieve higher scores than a state-of-the-art model. 1 Introduction The task of document summarization has two main paradigms: extractive and abstractive. The former method directly chooses and outputs the salient sentences (or phrases) in the original document (Jing and McKeown, 2000; Knight and Marcu, 2000; Martins and Smith, 2009; BergKirkpatrick et al., 2011). The latter abstractive approach involves rewriting the summary (Banko et al., 2000; Zajic et al., 2004), and has seen substantial recent gains due to neural sequence-to675 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 675–686 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics sentence-pairs between the document and ground truth summary. Next, our model achieves the new state-of-the-art on all metrics of multiple"
P18-1063,W09-1801,0,0.528542,"oding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC2002 dataset, where we achieve higher scores than a state-of-the-art model. 1 Introduction The task of document summarization has two main paradigms: extractive and abstractive. The former method directly chooses and outputs the salient sentences (or phrases) in the original document (Jing and McKeown, 2000; Knight and Marcu, 2000; Martins and Smith, 2009; BergKirkpatrick et al., 2011). The latter abstractive approach involves rewriting the summary (Banko et al., 2000; Zajic et al., 2004), and has seen substantial recent gains due to neural sequence-to675 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 675–686 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics sentence-pairs between the document and ground truth summary. Next, our model achieves the new state-of-the-art on all metrics of multiple versions of a popular summarization dataset (as"
P18-1063,P14-2052,0,0.0172048,"ty preferences. However, none of these used RL to bridge the non-differentiability of neural models. Our model shares some high-level intuition with extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please refer to the supplementary for full training details (all hyperparameter tuning was performed on the validation set). We use the CNN/Daily Mail dataset (Hermann et al., 2015)"
P18-1063,D16-1031,0,0.0264231,"extractive and compression based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep the abstractor parameters fixed. Because the input sentences for the abstractor are extracted by an intermediate stochastic policy of the extractor, it is impossible to find the correct target summary for the abstractor to fit g with ML objective. Though it is possible to optimize the abstractor with RL, in out preliminary experiments we found that this does not improve the overall ROUGE, most likely because this RL optimizes at a sentence-level and can add across-sentence redundancy. We achieve SotA resu"
P18-1063,D14-1181,0,0.00547816,"r3 r4 h0 h1 h4 CONV r1 r1 r2 r2 r1 r3 r3 r2 r1 r4 r4 r1 r3 r2 h0 h0 r2 r4 r3 h1 h1 r3 h0 r4 h4 Context-aware Sent. Reps. Encoded Sentence Representations h4 (from previous extraction step) r4 h1 h0 h0 h4 h1 Figure 1: Our extractor agent: the convolutional encoder computes representation rj for each sentence. h1 h4 The RNN encoder (blue) computes context-aware representation hj and then the RNN decoder (green) h4 selects sentence jt at time step t. With jt selected, hjt will be fed into the decoder at time t + 1. 2.1.1 Hierarchical Sentence Representation We use a temporal convolutional model (Kim, 2014) to compute rj , the representation of each individual sentence in the documents (details in supplementary). To further incorporate global context of the document and capture the long-range semantic dependency between sentences, a bidirec- d1 d1 d2 tional LSTM-RNN (Hochreiter and Schmidhuber, 1997; Schuster et al., 1997) is applied on the convolutional output. This enables learning a strong representation, denoted as hj for the j-th sentence in the document, that takes into account the context of all previous and future sentences in the same document. 2.1.2 Sentence Selection Next, to select t"
P18-1063,D14-1076,0,0.055084,"models. Our model shares some high-level intuition with extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please refer to the supplementary for full training details (all hyperparameter tuning was performed on the validation set). We use the CNN/Daily Mail dataset (Hermann et al., 2015) modified for summarization (Nallapati et al., 2016). Because there are two versions of"
P18-1063,N18-1158,0,0.35629,"er decoding (Gu et al., 2017a) and real-time translation (Gu et al., 2017b). They used a fixed pretrained translator and applied policy gradient techniques to train another task-specific network. In question answering (QA), Choi et al. (2017) extract one sentence and then generate the answer from the sentence’s vector representation with RL bridging. Another recent work attempted a new coarse-to-fine attention approach on summarization (Ling and Rush, 2017) and found desired sharp focus properties for scaling to larger inputs (though without metric improvements). Very recently (concurrently), Narayan et al. (2018) use RL for ranking sentences in pure extraction-based summarization and C ¸ elikyilmaz et al. (2018) investigate multiple communicating encoder agents 5.1 Evaluation Metrics For all the datasets, we evaluate standard ROUGE1, ROUGE-2, and ROUGE-L (Lin, 2004) on fulllength F1 (with stemming) following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018). Following See et al. (2017), we also evaluate on METEOR (Denkowski and Lavie, 2014) for a more thorough analysis. 5.2 Modular Extractive vs. Abstractive Our hybrid approach is capable of both extractive and abstractive"
P18-1063,W04-1013,0,0.729607,"r. Under this assumption, we can further define another latent function f : X → Dn that satisfies f (x) = {dt }nj=1 and y = h(x) = [g(d1 ), g(d2 ), . . . , g(dn )], where [, ] denotes sentence concatenation. This latent function f can be seen as an extractor that chooses the salient (ordered) sentences in a given document for the abstracting function g to rewrite. Our overall model consists of these two submodules, the extractor agent and the abstractor network, to approximate the above-mentioned f and g, respectively. Empirically, our approach is the new state-ofthe-art on all ROUGE metrics (Lin, 2004) as well as on METEOR (Denkowski and Lavie, 2014) of the CNN/Daily Mail dataset, achieving statistically significant improvements over previous models that use complex long-encoder, copy, and coverage mechanisms (See et al., 2017). The test-only DUC-2002 improvement also shows our model’s better generalization than this strong abstractive system. In addition, we surpass the popular lead-3 baseline on all ROUGE scores with an abstractive model. Moreover, our sentence-level abstractive rewriting module also produces substantially more (3x) novel N -grams that are not seen in the input document,"
P18-1063,D13-1156,0,0.0245052,"ge the non-differentiability of neural models. Our model shares some high-level intuition with extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please refer to the supplementary for full training details (all hyperparameter tuning was performed on the validation set). We use the CNN/Daily Mail dataset (Hermann et al., 2015) modified for summarization (Nallapati et al., 2016)."
P18-1063,W17-4505,0,0.0186516,"r fixed, so as to achieve the best of both worlds. Training a neural network to use another fixed network has been investigated in machine translation for better decoding (Gu et al., 2017a) and real-time translation (Gu et al., 2017b). They used a fixed pretrained translator and applied policy gradient techniques to train another task-specific network. In question answering (QA), Choi et al. (2017) extract one sentence and then generate the answer from the sentence’s vector representation with RL bridging. Another recent work attempted a new coarse-to-fine attention approach on summarization (Ling and Rush, 2017) and found desired sharp focus properties for scaling to larger inputs (though without metric improvements). Very recently (concurrently), Narayan et al. (2018) use RL for ranking sentences in pure extraction-based summarization and C ¸ elikyilmaz et al. (2018) investigate multiple communicating encoder agents 5.1 Evaluation Metrics For all the datasets, we evaluate standard ROUGE1, ROUGE-2, and ROUGE-L (Lin, 2004) on fulllength F1 (with stemming) following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018). Following See et al. (2017), we also evaluate on METEOR (D"
P18-1063,W10-4327,0,0.0244657,"t the summary to length, style, and entity preferences. However, none of these used RL to bridge the non-differentiability of neural models. Our model shares some high-level intuition with extract-then-compress methods. Earlier attempts in this paradigm used Hidden Markov Models and rule-based systems (Jing and McKeown, 2000), statistical models based on parse trees (Knight and Marcu, 2000), and integer linear programming based methods (Martins and Smith, 2009; Gillick and Favre, 2009; Clarke and Lapata, 2010; BergKirkpatrick et al., 2011). Recent approaches investigated discourse structures (Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Wang et al., 2015), graph cuts (Qian and Liu, 2013), and parse trees (Li et al., 2014; Bing et al., 2015). For neural models, Cheng and Lapata (2016) used a second neural net to select words from an extractor’s output. Our abstractor does not merely ‘compress’ the sentences but generatively produce novel words. Moreover, our RL bridges the extractor and the abstractor for end-to-end training. 5 Experimental Setup Please refer to the supplementary for full training details (all hyperparameter tuning was performed on the validation set). We use the CNN"
P18-1063,D15-1044,0,0.521476,"agent receives zero reward. The model is therefore encouraged to extract when there are still remaining ground-truth summary sentences (to accumulate intermediate reward), and learn to stop by optimizing a global ROUGE and avoiding extra extraction.8 Overall, this modification allows dy4 Related Work Early summarization works mostly focused on extractive and compression based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep the abstractor parameters fixed. Because the input sentences for the abstractor are extracted by an intermediate stochastic policy of the extractor, it is impos"
P18-1063,P17-1099,0,0.0716063,"ction f can be seen as an extractor that chooses the salient (ordered) sentences in a given document for the abstracting function g to rewrite. Our overall model consists of these two submodules, the extractor agent and the abstractor network, to approximate the above-mentioned f and g, respectively. Empirically, our approach is the new state-ofthe-art on all ROUGE metrics (Lin, 2004) as well as on METEOR (Denkowski and Lavie, 2014) of the CNN/Daily Mail dataset, achieving statistically significant improvements over previous models that use complex long-encoder, copy, and coverage mechanisms (See et al., 2017). The test-only DUC-2002 improvement also shows our model’s better generalization than this strong abstractive system. In addition, we surpass the popular lead-3 baseline on all ROUGE scores with an abstractive model. Moreover, our sentence-level abstractive rewriting module also produces substantially more (3x) novel N -grams that are not seen in the input document, as compared to the strong flat-structured model of See et al. (2017). This empirically justifies that our RL-guided extractor has learned sentence saliency, rather than benefiting from simply copying longer sentences. We also show"
P18-1063,P17-1108,0,0.147136,"Early summarization works mostly focused on extractive and compression based methods (Jing and McKeown, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Filippova et al., 2015). Recent large-sized corpora attracted neural methods for abstractive summarization (Rush et al., 2015; Chopra et al., 2016). Some of the recent success in neural abstractive models include hierarchical attention (Nallapati et al., 2016), coverage (Suzuki and Nagata, 2016; Chen et al., 2016; See et al., 2017), RL based metric optimization (Paulus et al., 2018), graph-based attention (Tan et al., 2017), and the copy mechanism (Miao and Blunsom, 2016; Gu et al., 2016; See et al., 2017). 7 During this RL training of the extractor, we keep the abstractor parameters fixed. Because the input sentences for the abstractor are extracted by an intermediate stochastic policy of the extractor, it is impossible to find the correct target summary for the abstractor to fit g with ML objective. Though it is possible to optimize the abstractor with RL, in out preliminary experiments we found that this does not improve the overall ROUGE, most likely because this RL optimizes at a sentence-level and can add"
P18-1063,P17-1101,0,0.057558,"found that this does not improve the overall ROUGE, most likely because this RL optimizes at a sentence-level and can add across-sentence redundancy. We achieve SotA results without this abstractor-level RL. 8 We use ROUGE-1 for terminal reward because it is a better measure of bag-of-words information (i.e., has all the important information been generated); while ROUGE-L is used as intermediate rewards since it is known for better measurement of language fluency within a local sentence. 679 to enhance the copying abstractive summarizer. Finally, there are some loosely-related recent works: Zhou et al. (2017) proposed selective gate to improve the attention in abstractive summarization. Tan et al. (2018) used an extract-thensynthesis approach on QA, where an extraction model predicts the important spans in the passage and then another synthesis model generates the final answer. Swayamdipta et al. (2017) attempted cascaded non-recurrent small networks on extractive QA, resulting a scalable, parallelizable model. Fan et al. (2017) added controlling parameters to adapt the summary to length, style, and entity preferences. However, none of these used RL to bridge the non-differentiability of neural mo"
P18-1063,E17-2025,0,\N,Missing
P18-1064,D17-1320,0,0.0249593,"n evaluation studies as well as detailed quantitative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster. 688 task like summarization. Our pointer mechanism approach is similar to See et al. (2017), who use a sof"
P18-1064,P17-1080,0,0.0172293,"k learning details. 4.1 Layer-Specific Sharing Mechanism Simply sharing all parameters across the related tasks is not optimal, because models for different tasks have different input and output distributions, esp. for low-level vs. high-level parameters. Therefore, related tasks should share some common representations (e.g., high-level information), as well as need their own individual task-specific representations (esp. low-level information). To this end, we allow different components of model parameters of related tasks to be shared vs. unshared, as described next. Encoder Layer Sharing: Belinkov et al. (2017) observed that lower layers (i.e., the layers closer to the input words) of RNN cells in a seq2seq 2 We also tried to generate all the questions at once from the full document, but we obtained low accuracy because of this task’s challenging nature and overall less training data. 690 θs and ψs represent the shared parameter subset between the primary and auxiliary tasks. machine translation model learn to represent word structure, while higher layers (farther from input) are more focused on high-level semantic meanings (similar to findings in the computer vision community for image features (Ze"
P18-1064,D15-1075,0,0.631721,"nes, which significantly speeds up our experiment cycle1 ; (5) For evaluation, we show diverse improvements of our soft, layer-specific MTL model (over state-of-the-art pointer+coverage baselines) on the CNN/DailyMail, Gigaword, as well as DUC datasets; we also report human evaluation plus analysis examples of learned saliency and entailment skills; we also report improvements on the auxiliary question and entailment generation tasks over their respective previous state-of-the-art. by Du et al. (2017). Our entailment generation task is based on the recent SNLI classification dataset and task (Bowman et al., 2015), converted to a generation task (Pasunuru and Bansal, 2017). Further, we also present novel multi-task learning architectures based on multi-layered encoder and decoder models, where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks, while keeping the lower-level (lexico-syntactic) layers unshared. We also explore different ways to optimize the shared parameters and show that ‘soft’ parameter sharing achieves higher performance than hard sharing. Empirically, our soft, layer-specific sharing model with the quest"
P18-1064,C10-1039,0,0.0729329,"nally, we present human evaluation studies as well as detailed quantitative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster. 688 task like summarization. Our pointer mechanism approach is similar to See e"
P18-1064,D14-1168,0,0.0273796,"tative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster. 688 task like summarization. Our pointer mechanism approach is similar to See et al. (2017), who use a soft switch based on the generation probabili"
P18-1064,P17-1152,0,0.0855673,"Missing"
P18-1064,S14-1010,0,0.0758903,"Using the SQuAD dataset (Rajpurkar et al., 2016), we learn to generate a question given the sentence containing the answer span in the comprehension (similar to Du et al. (2017)). For the second auxiliary task of entailment generation, we use the generation version of the RTE classification task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014; Bowman et al., 2015). Some previous work has explored the use of RTE for redundancy detection in summarization by modeling graph-based relationships between sentences to select the most non-redundant sentences (Mehdad et al., 2013; Gupta et al., 2014), whereas our approach is based on multi-task learning. 3 Coverage Mechanism Following previous work (See et al., 2017), coverage helps alleviate the issue of word repetition while generating long summaries. We maintain a coverage vector P α that sums over all of the previous cˆt = t−1 t=0 t time steps attention distributions αt , and this is added as input to the attention mechanism. CovP P erage loss is Lcov (θ) = ˆt,i ). t i min(αt,i , c Finally, the total loss is a weighted combination of cross-entropy loss and coverage loss: Models First, we introduce our pointer+coverage baseline model a"
P18-1064,D17-1206,0,0.0424581,"et al., 2016; Tan et al., 2017). Paulus et al. (2018) and Henß et al. (2015) incorporated recent advances from reinforcement learning. Also, See et al. (2017) further improved results via pointercopy mechanism and addressed the redundancy with coverage mechanism. Multi-task learning (MTL) is a useful paradigm to improve the generalization performance of a task with related tasks while sharing some common parameters/representations (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum´e III, 2012). Several recent works have adopted MTL in neural models (Luong et al., 2016; Misra et al., 2016; Hashimoto et al., 2017; Pasunuru and Bansal, 2017; Ruder et al., 2017; Kaiser et al., 2017). Moreover, some of the above works have investigated the use of shared vs unshared sets of parameters. On the other hand, we investigate the importance of soft parameter sharing and highlevel versus low-level layer-specific sharing. Our previous workshop paper (Pasunuru et al., 2017) presented some preliminary results for multi-task learning of textual summarization with entailment generation. This current paper has several major differences: (1) We present question generation as an additional effective auxiliary task to enh"
P18-1064,D14-1085,0,0.0196063,"e improved saliency detection and logical inference skills learned by our multi-task model. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster. 688 task like summarization. Our pointer mechanism approach is similar to See et al. (2017), who use a soft switch based on the generation probability pg = σ(Wg ct +Ug st +Vg ewt−1 +bg ), where"
P18-1064,N16-1012,0,0.609986,"er-Specific Multi-Task Summarization with Entailment and Question Generation Han Guo∗ Ramakanth Pasunuru∗ Mohit Bansal UNC Chapel Hill {hanguo, ram, mbansal}@cs.unc.edu Abstract Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Henß et al., 2015), abstractive summaries are based on rewriting as opposed to selecting. Recent end-to-end, neural sequence-tosequence models and larger datasets have allowed substantial progress on the abstractive task, with ideas ranging from copy-pointer mechanism and redundancy coverage, to metric reward based reinforcement learning (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017). Despite these strong recent advancements, there is still a lot of scope for improving the summary quality generated by these models. A good rewritten summary is one that contains all the salient information from the document, is logically followed (entailed) by it, and avoids redundant information. The redundancy aspect was addressed by coverage models (Suzuki and Nagata, 2016; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017), but we still need to teach these models about how to better detect salient information from the input document,"
P18-1064,S14-2131,0,0.100768,"Missing"
P18-1064,W14-3348,0,0.0603391,", 2015) and the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) datasets for our entailment and question generation tasks, resp. We also show generalizability/transfer results on DUC-2002 with our CNN/DM trained models. Supplementary contains dataset details. Evaluation Metrics: We use the standard ROUGE evaluation package (Lin, 2004) for reporting the results on all of our summarization models. Following previous work (Chopra et al., 2016; Nallapati et al., 2016), we use ROUGE full-length F1 variant for all our results. Following See et al. (2017), we also report METEOR (Denkowski and Lavie, 2014) using the MS-COCO evaluation script (Chen et al., 2015). Human Evaluation Criteria: We used Amazon MTurk to perform human evaluation of summary relevance and readability. We selected human annotators that were located in the US, had an apSoft vs. Hard Parameter Sharing Hard-sharing: In the most common multi-task learning hard-sharing approach, the parameters to be shared are forced to be the same. As a result, gradient information from multiple tasks will directly pass through shared parameters, hence forcing a common space representation for all the related tasks. Soft-sharing: In our soft-s"
P18-1064,A00-2024,0,0.372309,", we achieve statistically significant improvements over the state-ofthe-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model’s learned saliency and entailment skills. 1 Introduction Abstractive summarization is the challenging NLG task of compressing and rewriting a document into a short, relevant, salient, and coherent summary. It has numerous applications such as summarizing storylines, event understanding, etc. As compared to extractive or compressive summarization (Jing and McKeown, 2000; Knight and ∗ Equal contribution. 687 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 687–697 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics in hierarchical, distractive, saliency, and graphattention modeling (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Tan et al., 2017). Paulus et al. (2018) and Henß et al. (2015) incorporated recent advances from reinforcement learning. Also, See et al. (2017) further improved results via pointercopy mechanism and a"
P18-1064,P17-1117,1,0.947306,"., 2017). Paulus et al. (2018) and Henß et al. (2015) incorporated recent advances from reinforcement learning. Also, See et al. (2017) further improved results via pointercopy mechanism and addressed the redundancy with coverage mechanism. Multi-task learning (MTL) is a useful paradigm to improve the generalization performance of a task with related tasks while sharing some common parameters/representations (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum´e III, 2012). Several recent works have adopted MTL in neural models (Luong et al., 2016; Misra et al., 2016; Hashimoto et al., 2017; Pasunuru and Bansal, 2017; Ruder et al., 2017; Kaiser et al., 2017). Moreover, some of the above works have investigated the use of shared vs unshared sets of parameters. On the other hand, we investigate the importance of soft parameter sharing and highlevel versus low-level layer-specific sharing. Our previous workshop paper (Pasunuru et al., 2017) presented some preliminary results for multi-task learning of textual summarization with entailment generation. This current paper has several major differences: (1) We present question generation as an additional effective auxiliary task to enhance the important compleme"
P18-1064,N18-2102,1,0.77283,"Missing"
P18-1064,P15-1155,0,0.0247079,"significantly decrease the training time of the multitask models by initializing the individual tasks from their pretrained baseline models. Finally, we present human evaluation studies as well as detailed quantitative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the communi"
P18-1064,W17-4504,1,0.943642,"of a task with related tasks while sharing some common parameters/representations (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum´e III, 2012). Several recent works have adopted MTL in neural models (Luong et al., 2016; Misra et al., 2016; Hashimoto et al., 2017; Pasunuru and Bansal, 2017; Ruder et al., 2017; Kaiser et al., 2017). Moreover, some of the above works have investigated the use of shared vs unshared sets of parameters. On the other hand, we investigate the importance of soft parameter sharing and highlevel versus low-level layer-specific sharing. Our previous workshop paper (Pasunuru et al., 2017) presented some preliminary results for multi-task learning of textual summarization with entailment generation. This current paper has several major differences: (1) We present question generation as an additional effective auxiliary task to enhance the important complementary aspect of saliency detection; (2) Our new high-level layer-specific sharing approach is significantly better than alternative layer-sharing approaches (including the decoder-only sharing by Pasunuru et al. (2017)); (3) Our new soft sharing parameter approach gives stat. significant improvements over hard sharing; (4) We"
P18-1064,D16-1264,0,0.17125,"pecific multi-task learning with two relevant auxiliary tasks. The first is that of document-toquestion generation, which teaches the summarization model about what are the right questions to ask, which in turn is directly related to what the salient information in the input document is. The second auxiliary task is a premise-to-entailment generation task to teach it how to rewrite a summary which is a directed-logical subset of (i.e., logically follows from) the input document, and contains no contradictory or unrelated information. For the question generation task, we use the SQuAD dataset (Rajpurkar et al., 2016), where we learn to generate a question given a sentence containing the answer, similar to the recent work An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logica"
P18-1064,S14-2055,0,0.0156273,")·Pc (y), where Pv vocabulary distribution is as shown in Eq. 1, and copy distribution Pc is based on the attention distribution over source document words. In our work, we use a question generation task to improve the saliency of abstractive summarization in a multi-task setting. Using the SQuAD dataset (Rajpurkar et al., 2016), we learn to generate a question given the sentence containing the answer span in the comprehension (similar to Du et al. (2017)). For the second auxiliary task of entailment generation, we use the generation version of the RTE classification task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014; Bowman et al., 2015). Some previous work has explored the use of RTE for redundancy detection in summarization by modeling graph-based relationships between sentences to select the most non-redundant sentences (Mehdad et al., 2013; Gupta et al., 2014), whereas our approach is based on multi-task learning. 3 Coverage Mechanism Following previous work (See et al., 2017), coverage helps alleviate the issue of word repetition while generating long summaries. We maintain a coverage vector P α that sums over all of the previous cˆt = t−1 t=0 t time steps attention distributio"
P18-1064,D15-1044,0,0.54295,"he challenging NLG task of compressing and rewriting a document into a short, relevant, salient, and coherent summary. It has numerous applications such as summarizing storylines, event understanding, etc. As compared to extractive or compressive summarization (Jing and McKeown, 2000; Knight and ∗ Equal contribution. 687 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 687–697 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics in hierarchical, distractive, saliency, and graphattention modeling (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Tan et al., 2017). Paulus et al. (2018) and Henß et al. (2015) incorporated recent advances from reinforcement learning. Also, See et al. (2017) further improved results via pointercopy mechanism and addressed the redundancy with coverage mechanism. Multi-task learning (MTL) is a useful paradigm to improve the generalization performance of a task with related tasks while sharing some common parameters/representations (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum´e III, 2012). Several recent works have adopted MTL in neur"
P18-1064,W04-1013,0,0.0297619,"parameters. 5 Experimental Setup Datasets: We use CNN/DailyMail dataset (Hermann et al., 2015; Nallapati et al., 2016) and Gigaword (Rush et al., 2015) datasets for summarization, and the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) datasets for our entailment and question generation tasks, resp. We also show generalizability/transfer results on DUC-2002 with our CNN/DM trained models. Supplementary contains dataset details. Evaluation Metrics: We use the standard ROUGE evaluation package (Lin, 2004) for reporting the results on all of our summarization models. Following previous work (Chopra et al., 2016; Nallapati et al., 2016), we use ROUGE full-length F1 variant for all our results. Following See et al. (2017), we also report METEOR (Denkowski and Lavie, 2014) using the MS-COCO evaluation script (Chen et al., 2015). Human Evaluation Criteria: We used Amazon MTurk to perform human evaluation of summary relevance and readability. We selected human annotators that were located in the US, had an apSoft vs. Hard Parameter Sharing Hard-sharing: In the most common multi-task learning hard-sh"
P18-1064,P17-1099,0,0.0587076,"ailment and Question Generation Han Guo∗ Ramakanth Pasunuru∗ Mohit Bansal UNC Chapel Hill {hanguo, ram, mbansal}@cs.unc.edu Abstract Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Henß et al., 2015), abstractive summaries are based on rewriting as opposed to selecting. Recent end-to-end, neural sequence-tosequence models and larger datasets have allowed substantial progress on the abstractive task, with ideas ranging from copy-pointer mechanism and redundancy coverage, to metric reward based reinforcement learning (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017). Despite these strong recent advancements, there is still a lot of scope for improving the summary quality generated by these models. A good rewritten summary is one that contains all the salient information from the document, is logically followed (entailed) by it, and avoids redundant information. The redundancy aspect was addressed by coverage models (Suzuki and Nagata, 2016; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017), but we still need to teach these models about how to better detect salient information from the input document, as well as about better logicallydirected n"
P18-1064,N15-1114,0,0.0196244,"del. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster. 688 task like summarization. Our pointer mechanism approach is similar to See et al. (2017), who use a soft switch based on the generation probability pg = σ(Wg ct +Ug st +Vg ewt−1 +bg ), where σ(·) is a sigmoid function, Wg , Ug , Vg and bg are parameters learned during trai"
P18-1064,P17-1108,0,0.132466,"ant, salient, and coherent summary. It has numerous applications such as summarizing storylines, event understanding, etc. As compared to extractive or compressive summarization (Jing and McKeown, 2000; Knight and ∗ Equal contribution. 687 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 687–697 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics in hierarchical, distractive, saliency, and graphattention modeling (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Chen et al., 2016; Tan et al., 2017). Paulus et al. (2018) and Henß et al. (2015) incorporated recent advances from reinforcement learning. Also, See et al. (2017) further improved results via pointercopy mechanism and addressed the redundancy with coverage mechanism. Multi-task learning (MTL) is a useful paradigm to improve the generalization performance of a task with related tasks while sharing some common parameters/representations (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum´e III, 2012). Several recent works have adopted MTL in neural models (Luong et al., 2016; Misra et al., 2016; Hashimoto et al., 2017; Pasunuru"
P18-1064,P14-5010,0,0.00623179,"Missing"
P18-1064,P13-1136,0,0.048763,"ection and logical inference skills learned by our multi-task model. 2 Related Work Automatic text summarization has been progressively improving over the time, initially more focused on extractive and compressive models (Jing and McKeown, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015; Kedzie et al., 2015), and moving more towards compressive and abstractive summarization based on graphs and concept maps (Giannakopoulos, 2009; Ganesan et al., 2010; Falke and Gurevych, 2017) and discourse trees (Gerani et al., 2014), syntactic parse trees (Cheung and Penn, 2014; Wang et al., 2013), and Abstract Meaning Representations (AMR) (Liu et al., 2015; Dohare and Karnick, 2017). Recent work has also adopted machine translation inspired neural seq2seq models for abstractive summarization with advances 1 About 4-5 days for Pasunuru et al. (2017) approach vs. only 10 hours for us. This will allow the community to try many more multi-task training and tuning ideas faster. 688 task like summarization. Our pointer mechanism approach is similar to See et al. (2017), who use a soft switch based on the generation probability pg = σ(Wg ct +Ug st +Vg ewt−1 +bg ), where σ(·) is a sigmoid fu"
P18-1064,W13-2117,0,0.0544505,"a multi-task setting. Using the SQuAD dataset (Rajpurkar et al., 2016), we learn to generate a question given the sentence containing the answer span in the comprehension (similar to Du et al. (2017)). For the second auxiliary task of entailment generation, we use the generation version of the RTE classification task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014; Bowman et al., 2015). Some previous work has explored the use of RTE for redundancy detection in summarization by modeling graph-based relationships between sentences to select the most non-redundant sentences (Mehdad et al., 2013; Gupta et al., 2014), whereas our approach is based on multi-task learning. 3 Coverage Mechanism Following previous work (See et al., 2017), coverage helps alleviate the issue of word repetition while generating long summaries. We maintain a coverage vector P α that sums over all of the previous cˆt = t−1 t=0 t time steps attention distributions αt , and this is added as input to the attention mechanism. CovP P erage loss is Lcov (θ) = ˆt,i ). t i min(αt,i , c Finally, the total loss is a weighted combination of cross-entropy loss and coverage loss: Models First, we introduce our pointer+cove"
P18-1064,D15-1042,0,\N,Missing
P19-1182,D16-1125,0,0.0208753,"., 2016; Johnson et al., 2017). In terms of model progress, recent years witnessed strong research progress in generating natural language sentences to describe visual contents, such as Vinyals et al. (2015); Xu et al. (2015); Ranzato et al. (2016); Anderson et al. (2018) in single image captioning, Venugopalan et al. (2015); Pan et al. (2016); Pasunuru and Bansal (2017) in video captioning, Mao et al. (2016); Liu et al. (2017a); Yu et al. (2017); Luo and Shakhnarovich (2017) in referring expressions, Jain et al. (2017); Li et al. (2018); Misra et al. (2018) in visual question generation, and Andreas and Klein (2016); CohnGordon et al. (2018); Luo et al. (2018); Vedantam et al. (2017) in other setups. Single image captioning is the most relevant problem to the two-images captioning. Vinyals et al. (2015) created a powerful encoder-decoder (i.e., CNN to LSTM) framework in solving the captioning problem. Xu et al. (2015) further equipped it with an attention module to handle the memorylessness of fixed-size vectors. Ranzato et al. (2016) used reinforcement learning to eliminate exposure bias. Recently, Anderson et al. (2018) brought the information from object detection system to further boost the performan"
P19-1182,W05-0909,0,0.384545,", we compare the performance of our models on all three datasets with various automated metrics. Results on the test sets are reported. Following the setup in Jhamtani and Berg-Kirkpatrick (2018), we takes CIDEr (Vedantam et al., 2015) as the main metric in evaluating the Spot-the-Diff and NLVR2 datasets. However, CIDEr is known as its problem in up-weighting unimportant details (Kilickaya et al., 2017; Liu et al., 2017b). In our dataset, we find that instructions generated from a small set of short phrases could get a high CIDEr score. We thus change the main metric of our dataset to METEOR (Banerjee and Lavie, 2005), which is manually verified to be aligned with human judgment on the validation set in our dataset. To avoid over-fitting, the model is 1879 Ours(IEdit) Spot-the-Diff NLVR2 Basic 11 22 24 Full 24 37 37 Both Good 5 6 17 Both Not 60 35 22 Table 3: Human evaluation on 100 examples. Image pair and two captions generated by our basic model and full model are shown to the user. The user chooses one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both Good’, or ‘Both Not’. Better model marked in bold font. we do not explicitly model the pixel-level differences; however, we still find that the model cou"
P19-1182,N18-1150,0,0.0223056,"e also report the BLEU-4 (Papineni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansal (2017). Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually. We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model. We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality. The Image Editing Request dataset is specifically annotated by the image editing expert. The winning rate of our full model (dynamic relation attention) versus the"
P19-1182,N18-2070,0,0.0416472,"Missing"
P19-1182,D18-1436,0,0.345133,"Missing"
P19-1182,D14-1086,0,0.420249,"n social 1 Our data and code are publicly available at: https://github.com/airsplay/ VisualRelationships media, etc. This task has drawn significant attention in the research community with numerous studies (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018), and recent state of the art methods have achieved promising results on large captioning datasets, such as MS COCO (Lin et al., 2014). Besides single image captioning, the community has also explored other visual captioning problems such as video captioning (Venugopalan et al., 2015; Xu et al., 2016), and referring expressions (Kazemzadeh et al., 2014; Yu et al., 2017). However, the problem of two-image captioning, especially the task of describing the relationships and differences between two images, is still underexplored. In this paper, we focus on advancing research in this challenging problem by introducing a new dataset and proposing novel neural relational-speaker models.2 To the best of our knowledge, Jhamtani and Berg-Kirkpatrick (2018) is the only public dataset aimed at generating natural language descriptions for two real images. This dataset is about ‘spotting the difference’, and hence focuses more on describing exhaustive di"
P19-1182,N18-2120,0,0.029005,"e DDLA (Difference Description with Latent Alignment) method proposed in Jhamtani and Berg-Kirkpatrick (2018) learns the alignment between descriptions and visual differences. It relies on the nature of the particular dataset and thus could not be easily transferred to other dataset where the visual relationship is not obvious. The two-images captioning could also be considered as a two key-frames video captioning problem, and our sequential multi-heads attention is a modified version of the seq-to-seq model (Venugopalan et al., 2015). Some existing work (Chen et al., 2018; Wang et al., 2018; Manjunatha et al., 2018) also learns how to modify images. These datasets and methods focus on the image colorization and adjustment tasks, while our dataset aims to study the general image editing request task. 6 Conclusion In this paper, we explored the task of describing the visual relationship between two images. We collected the Image Editing Request dataset, which contains image pairs and human annotated editing instructions. We designed novel relational speaker models and evaluate them on our collected and other public existing dataset. Based on automatic and human evaluations, our relational speaker model imp"
P19-1182,P02-1040,0,0.103959,"The user chooses one from ‘Basic’ model wins, ‘Full’ model wins, ‘Both Good’, or ‘Both Not’. Better model marked in bold font. we do not explicitly model the pixel-level differences; however, we still find that the model could learn these differences in the Spot-the-Diff dataset. Since the descriptions in Spot-the-Diff is relatively simple, the errors mostly come from wrong entities or undetected differences as shown in Fig. 5. Our model is also sensitive to the image contents as shown in the NLVR2 dataset. 5 early-stopped based on the main metric on validation set. We also report the BLEU-4 (Papineni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansa"
P19-1182,D17-1103,1,0.908355,"neni et al., 2002) and ROUGE-L (Lin, 2004) scores. The results on various datasets shows the gradual improvement made by our novel neural components, which are designed to better describe the relationship between 2 images. Our full model has a significant improvement in result over baseline. The improvement on the NLVR2 dataset is limited because the comparison of two images was not forced to be considered when generating instructions. 4.3 Human Evaluation and Qualitative Analysis We conduct a pairwise human evaluation on our generated sentences, which is used in Celikyilmaz et al. (2018) and Pasunuru and Bansal (2017). Agarwala (2018) also shows that the pairwise comparison is better than scoring sentences individually. We randomly select 100 examples from the test set in each dataset and generate captions via our full speaker model. We ask users to choose a better instruction between the captions generated by our full model and the basic model, or alternatively indicate that the two captions are equal in quality. The Image Editing Request dataset is specifically annotated by the image editing expert. The winning rate of our full model (dynamic relation attention) versus the basic model is shown in Table 3"
P19-1182,P19-1644,0,0.199322,"further extend it by designing a dynamic relational attention module to combine the advantages of these two components, which finds the relationship between two images while decoding. The computation of dynamic relational attention is mathematically equivalent to attention over all visual “relationships”. Thus, our method provides a direct way to model visual relationships in language. To show the effectiveness of our models, we evaluate them on three datasets: our new dataset, the ”Spot-the-Diff” dataset (Jhamtani and BergKirkpatrick, 2018), and the two-image visual reasoning NLVR2 dataset (Suhr et al., 2019) (adapted for our task). We train models separately on each dataset with the same hyper-parameters and evaluate them on the same test set across all methods. Experimental results demonstrate that our model outperforms all the baselines and existing methods. The main contributions of our paper are: (1) We create a novel human language guided image editing dataset to boost the study in describing visual relationships; (2) We design novel relationalspeaker models, including a dynamic relational attention module, to handle the problem of twoimage captioning by focusing on all their visual relation"
P19-1185,N16-1181,0,0.0338971,"l-task cells in terms of the number of activation functions, which intuitively relates to better generalizability. 2 Related Work Neural architecture search (NAS) has been recently introduced for automatic learning of the model structure for the given dataset/task (Zoph and Le, 2017; Zoph et al., 2018), and has shown good improvements on image classification and language modeling. NAS shares some similarity to program synthesis and inductive programming (Summers, 1986; Biermann, 1978), and it has been successfully applied to some simple Q&A tasks (Liang et al., 2010; Neelakantan et al., 2015; Andreas et al., 2016; Lake et al., 2015). NAS was made more computationally feasible via tree-structured search space or Q-learning with -greedy exploration strategy and experience replay (Negrinho and Gordon, 2017; Baker et al., 2017), or a weight-sharing strategy among search space parameters called Efficient Neural Architecture Search (ENAS) (Pham et al., 2018). We explore architecture search for text classification and video caption generation tasks and their integration to two transfer learning paradigms of continual learning and multi-task learning. The major problem in continual learning is catastrophic f"
P19-1185,N18-1172,0,0.0259082,"architecture search algorithms that dynamically allocate more resources for promising architecture candidates, but these works are different from us in that they do not consider the case where we have continual incoming-data from different data sources, but instead focus on the continual evolution of the model search for efficiency purposes. 1912 Multi-task learning (MTL) is primarily used to CONTROLLER improve the generalization performance of a task by leveraging knowledge from related tasks (Caruana, 1998; Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Ruder et al., 2017; Augenstein et al., 2018; Guo et al., 2018; Oh et al., 2017; Ruder and Plank, 2017). In similar generalization spirit of multi-task learning, we present multi-task architecture learning based on performance rewards from multiple tasks, so as to find a single cell structure which can generalize well to a new unseen task. 3 ReLU 1 ReLU 2 tanh 1 ReLU 0 ReLU 1 ReLU 2 tanh 1 Sentence 1 Max-pooling label distribution (a) Text classification ENAS. CONTROLLER In this section, we first discuss how we adapt ENAS (Pham et al., 2018) for modeling our bitext classification and multimodal video captioning tasks. Next, we introduce"
P19-1185,P11-1020,0,0.0206914,"erall, we present two generalization approaches: CAS learns generalizable model parameters over sequential training of multiple tasks (continual learning), whereas MAS learns a generalizable cell structure which performs well across multiple tasks. For empirical evaluation of our two approaches of continual and multi-task cell learning, we choose three domains of natural language inference (NLI) bi-text classification tasks from the GLUE benchmark (Wang et al., 2018): QNLI, RTE, and WNLI, and three domains of multimodal-generation based video captioning tasks: MSR-VTT (Xu et al., 2016), MSVD (Chen and Dolan, 2011), and DiDeMo (Hendricks et al., 2017). Note that we are the first ones to use the architecture search approach for text classification tasks as well as multimodal conditionedgeneration tasks, which achieves improvements on the strong GLUE and video captioning baselines. Next, for continual learning, we train the three tasks sequentially for both text classification and video captioning (through our continual architecture search method) and show that this approach tightly maintains the performance on the previously-learned domain (also verified via human evaluation), while also significantly ma"
P19-1185,D17-1070,0,0.148462,"controller to update its own parameters. We repeat this alternate training procedure until the model converges. Later, we select the DAG structure with the best performance and use it to retrain the model from scratch. 3.2 (h) ·W1 ) + (1 − c1 ) hN (t) ReLU (b) Video captioning ENAS. ENAS Algorithm (t) Max-pooling Concatenation Architecture Search for Text Classification and Generation 3.1 Sentence 2 ENAS for Bi-Text Classification For our NLI text classification tasks, we are given the sentence pair as input, and we have to classify it as entailment or not. For a strong base model, we follow Conneau et al. (2017) model, and use bidirectional LSTM-RNN encoders to encode both the sentences and then we do max-pooling on the outputs from these encoders. Let v represent the maxpooling output from the first sentence encoder and 1913 u represent the max-pooling output from the second sentence encoding. The joint representation h is defined as h = [u; v; |u − v|; u v]. The final representation is linearly projected to the label classes, and then fed through softmax to get the final class distribution. Fig. 1a presents an overview of our text classification model along with ENAS controller for sampling an RNN"
P19-1185,W14-3348,0,0.0151134,"ning set, 100 as validation set, and 670 as test set. DiDeMo Dataset: Distinct Describable Moments (DiDeMo) is traditionally a video localization task w.r.t. given description query (Hendricks et al., 2017). In this work, we use it as a video description task where given the video as input we have to generate the caption. We use the standard splits as provided by Hendricks et al. (2017). 6.3 Evaluation For GLUE tasks, we use accuracy as an evaluation metric following the previous work (Wang et al., 2018). For video captioning tasks, we report four diverse automatic evaluation metrics: METEOR (Denkowski and Lavie, 2014), 1916 Models QNLI P REVIOUS W ORK BiLSTM+ELMo (2018) 69.4 BiLSTM+ELMo+Attn (2018) 61.1 BASELINES Baseline (with ELMo) 73.2 ENAS (Architecture Search) 74.5 CAS R ESULTS CAS Step-1 (QNLI training) 73.8 CAS Step-2 (RTE training) 73.6 CAS Step-3 (WNLI training) 73.3 CIDEr (Vedantam et al., 2015), BLEU-4 (Papineni et al., 2002), and ROUGE-L (Lin, 2004). We use the standard evaluation code (Chen et al., 2015) to obtain these scores for our generated captions w.r.t. the reference captions. 6.4 Training Details In all our experiments, our hyperparameter choices are based on validation set accuracy fo"
P19-1185,P18-1064,1,0.85076,"rithms that dynamically allocate more resources for promising architecture candidates, but these works are different from us in that they do not consider the case where we have continual incoming-data from different data sources, but instead focus on the continual evolution of the model search for efficiency purposes. 1912 Multi-task learning (MTL) is primarily used to CONTROLLER improve the generalization performance of a task by leveraging knowledge from related tasks (Caruana, 1998; Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Ruder et al., 2017; Augenstein et al., 2018; Guo et al., 2018; Oh et al., 2017; Ruder and Plank, 2017). In similar generalization spirit of multi-task learning, we present multi-task architecture learning based on performance rewards from multiple tasks, so as to find a single cell structure which can generalize well to a new unseen task. 3 ReLU 1 ReLU 2 tanh 1 ReLU 0 ReLU 1 ReLU 2 tanh 1 Sentence 1 Max-pooling label distribution (a) Text classification ENAS. CONTROLLER In this section, we first discuss how we adapt ENAS (Pham et al., 2018) for modeling our bitext classification and multimodal video captioning tasks. Next, we introduce our continual and"
P19-1185,P84-1044,0,0.20494,"Missing"
P19-1185,W04-1013,0,0.0102875,"Hendricks et al. (2017). 6.3 Evaluation For GLUE tasks, we use accuracy as an evaluation metric following the previous work (Wang et al., 2018). For video captioning tasks, we report four diverse automatic evaluation metrics: METEOR (Denkowski and Lavie, 2014), 1916 Models QNLI P REVIOUS W ORK BiLSTM+ELMo (2018) 69.4 BiLSTM+ELMo+Attn (2018) 61.1 BASELINES Baseline (with ELMo) 73.2 ENAS (Architecture Search) 74.5 CAS R ESULTS CAS Step-1 (QNLI training) 73.8 CAS Step-2 (RTE training) 73.6 CAS Step-3 (WNLI training) 73.3 CIDEr (Vedantam et al., 2015), BLEU-4 (Papineni et al., 2002), and ROUGE-L (Lin, 2004). We use the standard evaluation code (Chen et al., 2015) to obtain these scores for our generated captions w.r.t. the reference captions. 6.4 Training Details In all our experiments, our hyperparameter choices are based on validation set accuracy for GLUE tasks and an average of the four automatic evaluation metrics (METEOR, CIDEr, BLEU-4, and ROUGE-L) for video captioning tasks. We use same settings for both normal and architecture search models, unless otherwise specified. More details in appendix. 7 Results and Analysis 7.1 Continual Learning on GLUE Tasks Baseline Models: We use bidirecti"
P19-1185,D17-1038,0,0.0152526,"re resources for promising architecture candidates, but these works are different from us in that they do not consider the case where we have continual incoming-data from different data sources, but instead focus on the continual evolution of the model search for efficiency purposes. 1912 Multi-task learning (MTL) is primarily used to CONTROLLER improve the generalization performance of a task by leveraging knowledge from related tasks (Caruana, 1998; Collobert and Weston, 2008; Girshick, 2015; Luong et al., 2015; Ruder et al., 2017; Augenstein et al., 2018; Guo et al., 2018; Oh et al., 2017; Ruder and Plank, 2017). In similar generalization spirit of multi-task learning, we present multi-task architecture learning based on performance rewards from multiple tasks, so as to find a single cell structure which can generalize well to a new unseen task. 3 ReLU 1 ReLU 2 tanh 1 ReLU 0 ReLU 1 ReLU 2 tanh 1 Sentence 1 Max-pooling label distribution (a) Text classification ENAS. CONTROLLER In this section, we first discuss how we adapt ENAS (Pham et al., 2018) for modeling our bitext classification and multimodal video captioning tasks. Next, we introduce our continual and multi-task approaches of transfer learni"
P19-1185,P02-1040,0,0.103845,"e the standard splits as provided by Hendricks et al. (2017). 6.3 Evaluation For GLUE tasks, we use accuracy as an evaluation metric following the previous work (Wang et al., 2018). For video captioning tasks, we report four diverse automatic evaluation metrics: METEOR (Denkowski and Lavie, 2014), 1916 Models QNLI P REVIOUS W ORK BiLSTM+ELMo (2018) 69.4 BiLSTM+ELMo+Attn (2018) 61.1 BASELINES Baseline (with ELMo) 73.2 ENAS (Architecture Search) 74.5 CAS R ESULTS CAS Step-1 (QNLI training) 73.8 CAS Step-2 (RTE training) 73.6 CAS Step-3 (WNLI training) 73.3 CIDEr (Vedantam et al., 2015), BLEU-4 (Papineni et al., 2002), and ROUGE-L (Lin, 2004). We use the standard evaluation code (Chen et al., 2015) to obtain these scores for our generated captions w.r.t. the reference captions. 6.4 Training Details In all our experiments, our hyperparameter choices are based on validation set accuracy for GLUE tasks and an average of the four automatic evaluation metrics (METEOR, CIDEr, BLEU-4, and ROUGE-L) for video captioning tasks. We use same settings for both normal and architecture search models, unless otherwise specified. More details in appendix. 7 Results and Analysis 7.1 Continual Learning on GLUE Tasks Baseline"
P19-1185,P17-1117,1,0.848331,"cell structures and giving the corresponding validation accuracy as the feedback reward to the controller. In the second stage, we use the best cell structure from the stage-1 to retrain the text classification model from scratch. 3.3 ENAS for Conditioned Generation Next, we go beyond text classification, and look at conditioned text generation with ENAS, where we choose the task of video-conditioned text generation (also known as video captioning) so as to also bring in a multi-modality aspect. For a strong baseline, we use a sequence-to-sequence model with an attention mechanism similar to Pasunuru and Bansal (2017a), where we encode the video frames as a sequence into a bidirectional LSTM-RNN and decode the caption through another LSTM-RNN (see Fig. 1b). Our attention mechanism is similar to Bahdanau et al. (2015), where at each time step t of the decoder, the LSTM hidden state st is a non-linear function of previous time step’s decoder hidden state st−1 and generated word wt−1 , and the context vector ct which is a weighted combination of the encoder hidden states {hi }. These weights αt , are defined as: exp(et,i ) αt,i = Pn k=1 exp(et,k ) (5) The attention function et,i = wT tanh(Wa hi + Ua st−1 + b"
P19-1185,D17-1103,1,0.838106,"cell structures and giving the corresponding validation accuracy as the feedback reward to the controller. In the second stage, we use the best cell structure from the stage-1 to retrain the text classification model from scratch. 3.3 ENAS for Conditioned Generation Next, we go beyond text classification, and look at conditioned text generation with ENAS, where we choose the task of video-conditioned text generation (also known as video captioning) so as to also bring in a multi-modality aspect. For a strong baseline, we use a sequence-to-sequence model with an attention mechanism similar to Pasunuru and Bansal (2017a), where we encode the video frames as a sequence into a bidirectional LSTM-RNN and decode the caption through another LSTM-RNN (see Fig. 1b). Our attention mechanism is similar to Bahdanau et al. (2015), where at each time step t of the decoder, the LSTM hidden state st is a non-linear function of previous time step’s decoder hidden state st−1 and generated word wt−1 , and the context vector ct which is a weighted combination of the encoder hidden states {hi }. These weights αt , are defined as: exp(et,i ) αt,i = Pn k=1 exp(et,k ) (5) The attention function et,i = wT tanh(Wa hi + Ua st−1 + b"
P19-1185,N18-1202,0,0.0235153,"reference captions. 6.4 Training Details In all our experiments, our hyperparameter choices are based on validation set accuracy for GLUE tasks and an average of the four automatic evaluation metrics (METEOR, CIDEr, BLEU-4, and ROUGE-L) for video captioning tasks. We use same settings for both normal and architecture search models, unless otherwise specified. More details in appendix. 7 Results and Analysis 7.1 Continual Learning on GLUE Tasks Baseline Models: We use bidirectional LSTMRNN encoders with max-pooling (Conneau et al., 2017) as our baseline.4 Further, we used the ELMo embeddings (Peters et al., 2018) as input to the encoders, where we allowed to train the weights on each layer of ELMo to get a final representation. Table 1 shows that our baseline models achieve strong results when compared with GLUE benchmark baselines (Wang et al., 2018).5 On top of these strong baselines, we add ENAS approach. ENAS Models: Next, Table 1 shows that our ENAS models (for all three tasks QNLI, RTE, WNLI) perform better or equal than the nonarchitecture search based models.6 Note that we only replace the LSTM-RNN cell with our ENAS cell, rest of the model architecture in ENAS model is same as our baseline mo"
P19-1185,W18-5446,0,0.0293568,"Missing"
P19-1185,D16-1264,0,0.0360789,"Missing"
P19-1191,E17-1060,0,0.0353562,"he contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news"
P19-1191,W14-3348,0,0.040687,"ov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (nati"
P19-1191,W13-0108,0,0.0277008,"d graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator netwo"
P19-1191,P18-1082,0,0.033721,"paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert"
P19-1191,N16-1087,0,0.0455489,"tto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et"
P19-1191,W07-2305,0,0.0841706,"Missing"
P19-1191,P16-1154,0,0.301994,"rk-to-title) follow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite training. We label the words with frequen"
P19-1191,D18-1086,0,0.0190419,"ckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interest"
P19-1191,P15-1067,0,0.0119856,"om biomedical text”, PaperRobot mistakenly extracts “prolog” as a related entity and generates an abstract “In this paper, we present a novel approach to the problem of extracting relationships among the prolog program. We present a system that uses a macromolecular binding relationships to extract the relationships between the abstracts of the entry. The results show that the system is able to extract the most important concepts in the prolog program.”. 4 Related Work Link Prediction. Translation-based approaches (Nickel et al., 2011; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015a) have been widely exploited for link prediction. Compared with these studies, we are the first to incorporate multi-head graph attention (Sukhbaatar et al., 2015; Madotto et al., 2018; Veliˇckovi´c et al., 2018) to encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven"
P19-1191,N18-2101,0,0.303572,"its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published pa"
P19-1191,W13-1302,0,0.0196907,"paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, a"
P19-1191,D16-1128,0,0.0872029,"Missing"
P19-1191,P16-1094,0,0.0146349,"t. 3 3.1 Experiment Data We collect biomedical papers from the PMC Open Access Subset.5 To construct ground truth for new title prediction, if a human written paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We"
P19-1191,W04-1013,0,0.0826554,"Missing"
P19-1191,D16-1230,0,0.03642,"Missing"
P19-1191,W15-4640,0,0.0129704,"nt Data We collect biomedical papers from the PMC Open Access Subset.5 To construct ground truth for new title prediction, if a human written paper A 5 ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/ oa_package/ cites a paper B, we assume the title of A is generated from B’s conclusion and future work session. We construct background knowledge graphs from 1,687,060 papers which include 30,483 entities and 875,698 relations. Tables 2 shows the detailed data statistics. The hyperparameters of our model are presented in the Appendix. 3.2 Automatic Evaluation Previous work (Liu et al., 2016; Li et al., 2016; Lowe et al., 2015) has proven it to be a major challenge to automatically evaluate long text generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our fr"
P19-1191,D18-1435,1,0.843574,"Missing"
P19-1191,D18-1360,1,0.841868,"t data. All of the system generated titles are declarative sentences while human generated titles are often more engaging (e.g., “Does HPV play any role in the initiation or prognosis of endometrial Requirements to Make PaperRobot Work: Case Study on NLP Domain When a cool Natural Language Processing (NLP) system like PaperRobot is built, it’s natural to ask whether she can benefit the NLP community itself. We re-build the system based on 23,594 NLP papers from the new ACL Anthology Network (Radev et al., 2013). For knowledge extraction we apply our previous system trained for the NLP domain (Luan et al., 2018). But the results are much less satisfactory compared to the 1987 biomedical domain. Due to the small size of data, the language model is not able to effectively copy out-of-vocabulary words and thus the output is often too generic. For example, given a title “Statistics based hybrid approach to Chinese base phrase identification”, PaperRobot generates a fluent but uninformative abstract “This paper describes a novel approach to the task of Chinese-base-phrase identification. We first utilize the solid foundation for the Chinese parser, and we show that our tool can be easily extended to meet"
P19-1191,P18-1136,0,0.174988,"n and future work, and conclusion and future work-to-title) follow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite"
P19-1191,P02-1040,0,0.104014,"Missing"
P19-1191,W16-6603,1,0.855329,"encourage the model to capture multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include ge"
P19-1191,P17-1099,0,0.622639,"ow the same architecture. Given a reference title τ = [w1 , ..., wl ], we apply the knowledge extractor (Section 2.2) to extract entities from τ . For each entity, we retrieve a set of related entities from the enriched knowledge e after link prediction. We rank all the regraph K lated entities by confidence scores and select up to 10 most related entities Eτ = [eτ1 , ..., eτv ]. Then we feed τ and Eτ together into the paper generation framework as shown in Figure 2. The framework is based on a hybrid approach of a Mem2seq model (Madotto et al., 2018) and a pointer generator (Gu et al., 2016; See et al., 2017). It allows us to balance three types of sources for each time step during decoding: the probability of generating a token from the entire word vocabulary based on language model, the probability of copying a word from the reference title, such as regulates in Table 1, and the probability of incorporating a related entity, such as Snail in Table 1. The output is a paragraph Y = [y1 , ..., yo ].3 Reference Encoder For each word in the refer3 During training, we truncate both of the input and the output to around 120 tokens to expedite training. We label the words with frequency < 5 as Out-of-vo"
P19-1191,2006.amta-papers.25,0,0.221983,"Missing"
P19-1191,E17-2047,0,0.056946,"Missing"
P19-1191,P18-1151,0,0.0126333,"ure multi-aspect relevance among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts base"
P19-1191,D18-1422,0,0.0136042,"i, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wa"
P19-1191,P16-1008,0,0.060528,"Missing"
P19-1191,D18-2028,0,0.0454461,"Missing"
P19-1191,D18-1112,0,0.0200447,"nce among nodes. Similar to (Wang and Li, 2016; Xu et al., 2017), we enrich entity representation by combining the contextual sentences that include the target entity and its neighbors from the graph structure. This is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for t"
P19-1191,W18-6502,1,0.880435,"t generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one. 6 https://github.com/pytorch/examples/ tree/master/word_language_model 7 The perplexity scores of the language model are in the Appendix. 1985 Task Input Human Title End-to-End System Abstract System Conclusion and Future work System Title Human Abstract Diagnostic Human Conclusion and Future work Output Different Same Different Same Different Same Different Different Same Differen"
P19-1191,P18-2042,1,0.914756,"t generation. Following the story generation work (Fan et al., 2018), we use METEOR (Denkowski and Lavie, 2014) to measure the topic relevance towards given titles and use perplexity to further evaluate the quality of the language model. The perplexity scores of our model are based on the language model6 learned on other PubMed papers (500,000 titles, 50,000 abstracts, 50,000 conclusions and future work) which are not used for training or testing in our experiment.7 The results are shown in Table 3. We can see that our framework outperforms all previous approaches. 3.3 Turing Test Similar to (Wang et al., 2018b), we conduct Turing tests by a biomedical expert (non-native speaker) and a non-expert (native speaker). Each human judge is asked to compare a system output and a human-authored string, and select the better one. 6 https://github.com/pytorch/examples/ tree/master/word_language_model 7 The perplexity scores of the language model are in the Appendix. 1985 Task Input Human Title End-to-End System Abstract System Conclusion and Future work System Title Human Abstract Diagnostic Human Conclusion and Future work Output Different Same Different Same Different Same Different Different Same Differen"
P19-1191,D18-1433,1,0.748118,"eep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic writing of key paper elements for the biomedical domain, especially conclusion and future work, and follow-on paper titles. 5 Conclusions"
P19-1191,D18-1356,0,0.0165433,"his is the first work to incorporate new idea creation via link prediction into automatic paper writing. Knowledge-driven Generation. Deep Neural Networks have been applied to generate natural language to describe structured knowledge bases (Duma and Klein, 2013; Konstas and Lapata, 2013; Flanigan et al., 2016; Hardy and Vlachos, 2018; Pourdamghani et al., 2016; Trisedya et al., 2018; Xu et al., 2018; Madotto et al., 2018; Nie et al., 2018), biographies based on attributes (Lebret et al., 2016; Chisholm et al., 2017; Liu et al., 2018; Sha et al., 2018; Kaffee et al., 2018; Wang et al., 2018a; Wiseman et al., 2018), and image/video captions based on background entities and events (Krishnamoorthy et al., 2013; Wu et al., 2018; Whitehead et al., 2018; Lu et al., 2018). To handle unknown words, we design an architecture similar to pointer-generator networks (See et al., 2017) and copy mechanism (Gu et al., 2016). Some interesting applications include generating abstracts based on titles for the natural language processing domain (Wang et al., 2018b), generating a poster (Qiang et al., 2016) or a science news blog title (Vadapalli et al., 2018) about a published paper. This is the first work on automatic wr"
P19-1261,N19-1032,0,0.0168016,"ear in different documents. On the other hand, our Document Explorer can discover the document with the answer “Loon op Zand” (in Fig. 1a) by iteratively selecting relevant documents and encoding the hinge words “Efteling” and “Kaatsheuvel” in its memory. Recently, Dhingra et al. (2018) leveraged coreference annotations from an external system to connect the entities. Song et al. (2018a) and De Cao et al. (2018) utilized Graph Convolutional Networks (Kipf and Welling, 2017) and Graph Recurrent Networks (Song et al., 2018b; Zhang et al., 2018) to model the relations between entities. Recently, Cao et al. (2019) extended the Graph Convolutional Network in De Cao et al. (2018) by introducing bi-directional attention between the entity graph and query. By connecting the entities, these models learn the inference paths for multihop reasoning. Our work differs in that our system learns the relation implicitly without the need of any human-annotated relation. Recently, Zhong et al. (2019) used hierarchies of co-attention and self-attention to combine evidence from multiple scattered documents. Our novel 3-module architecture is inspired by previous 2-module selection architectures for MRC (Choi et al., 20"
P19-1261,P17-1171,0,0.0229622,"he longest artificial ditch in the Upper Harz in central Germany. Its purpose was to collect surface runoff for the operation of the Upper Harz mining industry from precipitation-heavy regions a long way away (particularly from the Bruchberg and parts of the Brocken massif). ... Final answer: Lower Saxony Answer Proposer Evidence Assembler Figure 4: An example of our 3-stage EPAr system exploring relevant documents, proposing candidate answers, and then assembling extracted evidence to make the final prediction. ulously designed to solve single-document MRC tasks. Clark and Gardner (2018) and Chen et al. (2017) used a simple TF-IDF based documentselection procedure to find the context that is most relevant to the query for multi-document QA. However, this 1-hop, similarity-based selection process would fail on multi-hop readingcomprehension datasets like WikiHop because the query subject and the answer could appear in different documents. On the other hand, our Document Explorer can discover the document with the answer “Loon op Zand” (in Fig. 1a) by iteratively selecting relevant documents and encoding the hinge words “Efteling” and “Kaatsheuvel” in its memory. Recently, Dhingra et al. (2018) lever"
P19-1261,P17-1020,0,0.0230022,"o et al. (2019) extended the Graph Convolutional Network in De Cao et al. (2018) by introducing bi-directional attention between the entity graph and query. By connecting the entities, these models learn the inference paths for multihop reasoning. Our work differs in that our system learns the relation implicitly without the need of any human-annotated relation. Recently, Zhong et al. (2019) used hierarchies of co-attention and self-attention to combine evidence from multiple scattered documents. Our novel 3-module architecture is inspired by previous 2-module selection architectures for MRC (Choi et al., 2017). Similarly, Wang et al. (2018) first selected relevant content by ranking documents and then extracted the answer span. Min et al. (2018) selected relevant sentences from long documents in a singledocument setup and achieved faster speed and robustness against adversarial corruption. However, none of these models are built for multi-hop MRC where our EPAr system shows great effectiveness. 6 Conclusion We presented an interpretable 3-module, multihop, reading-comprehension system ‘EPAr’ which constructs a ‘reasoning tree’, proposes an answer candidate for every root-to-leaf chain, and merges k"
P19-1261,P18-1078,0,0.0176141,"assif). d The Dyke Ditch is the longest artificial ditch in the Upper Harz in central Germany. Its purpose was to collect surface runoff for the operation of the Upper Harz mining industry from precipitation-heavy regions a long way away (particularly from the Bruchberg and parts of the Brocken massif). ... Final answer: Lower Saxony Answer Proposer Evidence Assembler Figure 4: An example of our 3-stage EPAr system exploring relevant documents, proposing candidate answers, and then assembling extracted evidence to make the final prediction. ulously designed to solve single-document MRC tasks. Clark and Gardner (2018) and Chen et al. (2017) used a simple TF-IDF based documentselection procedure to find the context that is most relevant to the query for multi-document QA. However, this 1-hop, similarity-based selection process would fail on multi-hop readingcomprehension datasets like WikiHop because the query subject and the answer could appear in different documents. On the other hand, our Document Explorer can discover the document with the answer “Loon op Zand” (in Fig. 1a) by iteratively selecting relevant documents and encoding the hinge words “Efteling” and “Kaatsheuvel” in its memory. Recently, Dhin"
P19-1261,N19-1240,0,0.0318861,"Missing"
P19-1261,N18-2007,0,0.258147,"he supporting documents and 0 query4 . This gives three matrices: X ∈ RN ×K×d , Qsub ∈ RJs ×d and Qbod ∈ RJb ×d , K, Js , Jb are the lengths of supporting documents, query body, and query subject respectively. We then apply a bi-directional LSTM-RNN (Hochreiter and Schmidhuber, 1997) of v hidden units to get the contextual word representations for the documents H = {h1 , · · · , hN 0 } s.t. hi ∈ RK×2v and the query Usub ∈ RJs ×2v , Ubod ∈ RJb ×2v . Other than the word-level encoding, we also collect compact representations of all the supporting docu4 Unlike previous works (Welbl et al., 2017; Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a) that concatenate supporting documents together to form a large context, we instead maintain the document-level hierarchy and encode each document separately. 2716 ments, denoted as P = {p1 , · · · , pN 0 }, by applying the self-attention mechanism in Zhong et al. (2019) (see details in appendix). We obtain embeddings for each candidate ci ∈ {c1 , c2 , .., cL } using the average-over-word embeddings of the first mention5 of the candidate in H. 2.2 Document Explorer Our Document Explorer (DE, shown in the left part of Fig. 2) is a hierarchical memory ne"
P19-1261,P17-1168,0,0.156895,"e identified in every reasoning chain of the tree to make one final, unified prediction. To do so, the Assembler selects key sentences from each root-to-leaf document path in the ‘reasoning tree’ and forms a new condensed, salient context which is then bidirectionally-matched with the query representation to output the final prediction. Via this procedure, evidence that was originally scattered widely across several documents is now collected concentratedly, hence transforming the task to a scenario where previous standard phrase-matching style QA models (Seo et al., 2017; Xiong et al., 2017; Dhingra et al., 2017) can be effective. Overall, our 3-module, multi-hop, reasoningtree based EPAr (Explore-Propose-Assemble reader) closely mimics the coarse-to-fine-grained reading and reasoning behavior of human readers. We jointly optimize this 3-module system by having the following component working on the outputs from the previous component and minimizing the sum of the losses from all 3 modules. The Answer Proposer and Evidence Assembler are trained with maximum likelihood using ground-truth answers as labels, while the Document Explorer is weakly supervised by heuristic reasoning chains constructed via TF"
P19-1261,N16-2016,0,0.0300485,"t progress on text-based machine reading comprehension and question answering (MRC-QA) including cloze-style blank-filling tasks (Hermann et al., 2015), open-domain QA (Yang et al., 2015), answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a single-document context per question setup. Joshi et al. (2017) extended the task to the multidocument regime, with some examples requiring cross-sentence inference. Earlier attempts in multi-hop MRC focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). QAngaroo WikiHop and MedHop (Welbl et al., 2017), on the other hand, are created as natural language MRC tasks. They are designed in a way such that the evidence required to answer a query could be spread across multiple documents. Thus, finding some evidence requires building a reasoning chain from the query with intermediate inference steps, which poses extra difficulty for MRC-QA systems. HotpotQA (Yang et al., 2018) is another recent multi-hop dataset which focuses on four different reasoning paradigms. The emergence of l"
P19-1261,P17-1147,0,0.0250753,"Finally, the Evidence Assembler selects key sentences from all documents in the constructed document chains and makes the final prediction (“Lower Saxony”). 5 Related Works The last few years have witnessed significant progress on text-based machine reading comprehension and question answering (MRC-QA) including cloze-style blank-filling tasks (Hermann et al., 2015), open-domain QA (Yang et al., 2015), answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a single-document context per question setup. Joshi et al. (2017) extended the task to the multidocument regime, with some examples requiring cross-sentence inference. Earlier attempts in multi-hop MRC focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). QAngaroo WikiHop and MedHop (Welbl et al., 2017), on the other hand, are created as natural language MRC tasks. They are designed in a way such that the evidence required to answer a query could be spread across multiple documents. Thus, finding some evidence requires building a reasoning chain from the query with interm"
P19-1261,D18-1362,0,0.0323051,"e reading comprehension and question answering (MRC-QA) including cloze-style blank-filling tasks (Hermann et al., 2015), open-domain QA (Yang et al., 2015), answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a single-document context per question setup. Joshi et al. (2017) extended the task to the multidocument regime, with some examples requiring cross-sentence inference. Earlier attempts in multi-hop MRC focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). QAngaroo WikiHop and MedHop (Welbl et al., 2017), on the other hand, are created as natural language MRC tasks. They are designed in a way such that the evidence required to answer a query could be spread across multiple documents. Thus, finding some evidence requires building a reasoning chain from the query with intermediate inference steps, which poses extra difficulty for MRC-QA systems. HotpotQA (Yang et al., 2018) is another recent multi-hop dataset which focuses on four different reasoning paradigms. The emergence of large-scale MRC datasets has led to inn"
P19-1261,P18-1160,0,0.0129517,"graph and query. By connecting the entities, these models learn the inference paths for multihop reasoning. Our work differs in that our system learns the relation implicitly without the need of any human-annotated relation. Recently, Zhong et al. (2019) used hierarchies of co-attention and self-attention to combine evidence from multiple scattered documents. Our novel 3-module architecture is inspired by previous 2-module selection architectures for MRC (Choi et al., 2017). Similarly, Wang et al. (2018) first selected relevant content by ranking documents and then extracted the answer span. Min et al. (2018) selected relevant sentences from long documents in a singledocument setup and achieved faster speed and robustness against adversarial corruption. However, none of these models are built for multi-hop MRC where our EPAr system shows great effectiveness. 6 Conclusion We presented an interpretable 3-module, multihop, reading-comprehension system ‘EPAr’ which constructs a ‘reasoning tree’, proposes an answer candidate for every root-to-leaf chain, and merges key information from all reasoning chains to make the final prediction. On WikiHop, our system outperforms all published models on the dev"
P19-1261,D14-1162,0,0.0820935,"rst select one document with the shortest TF-IDF distance to the query. We then rank the remaining documents according to their TF-IDF distances to the first selected document and add the top N 0 −1 documents to form the context with a total of N 0 documents for this query. Adding this preprocessing step is not only helpful in reducing GPU memory consumption but also helps bootstrap the training by reducing the search space of the Document Explorer (Sec. 2.2). We then use a Highway Network (Srivastava et al., 2015) of dimension d, which merges the character embedding and GloVe word embedding (Pennington et al., 2014), to get the word representations for the supporting documents and 0 query4 . This gives three matrices: X ∈ RN ×K×d , Qsub ∈ RJs ×d and Qbod ∈ RJb ×d , K, Js , Jb are the lengths of supporting documents, query body, and query subject respectively. We then apply a bi-directional LSTM-RNN (Hochreiter and Schmidhuber, 1997) of v hidden units to get the contextual word representations for the documents H = {h1 , · · · , hN 0 } s.t. hi ∈ RK×2v and the query Usub ∈ RJs ×2v , Ubod ∈ RJb ×2v . Other than the word-level encoding, we also collect compact representations of all the supporting docu4 Unli"
P19-1261,N18-1202,0,0.110588,"Missing"
P19-1261,P18-2124,0,0.0398778,"Missing"
P19-1261,D16-1264,0,0.420416,"y to answer the question is concentrated in a single sentence or located closely in a single paragraph. Such datasets emphasize the role of locating, matching, and aligning information between the question and the context. However, some recent multi-document, multi-hop reading comprehension datasets, such as WikiHop and MedHop (Welbl et al., 2017), have been proposed to further assess MRC systems’ ability to perform multi-hop reasoning, where the required evidence is scattered in a set of supporting documents. These multi-hop tasks are much more challenging than previous single-hop MRC tasks (Rajpurkar et al., 2016, 2018; Hermann et al., 2015; Nguyen et al., 2016; Yang et al., 2015) for three primary reasons. First, the given context contains a large number of documents (e.g., 14 on average, 64 maximum for WikiHop). Most existing QA models cannot scale to the context of such length, and it is challenging to retrieve a reasoning chain of documents with complete information required to connect the question to the answer in a logical way. Second, given a reasoning chain of documents, it is still necessary for the model to consider evidence loosely distributed in all these documents in order to predict the"
P19-1261,C18-1171,0,0.0325139,"n text-based machine reading comprehension and question answering (MRC-QA) including cloze-style blank-filling tasks (Hermann et al., 2015), open-domain QA (Yang et al., 2015), answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a single-document context per question setup. Joshi et al. (2017) extended the task to the multidocument regime, with some examples requiring cross-sentence inference. Earlier attempts in multi-hop MRC focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). QAngaroo WikiHop and MedHop (Welbl et al., 2017), on the other hand, are created as natural language MRC tasks. They are designed in a way such that the evidence required to answer a query could be spread across multiple documents. Thus, finding some evidence requires building a reasoning chain from the query with intermediate inference steps, which poses extra difficulty for MRC-QA systems. HotpotQA (Yang et al., 2018) is another recent multi-hop dataset which focuses on four different reasoning paradigms. The emergence of large-scale MRC data"
P19-1261,P18-1150,0,0.326972,"gives three matrices: X ∈ RN ×K×d , Qsub ∈ RJs ×d and Qbod ∈ RJb ×d , K, Js , Jb are the lengths of supporting documents, query body, and query subject respectively. We then apply a bi-directional LSTM-RNN (Hochreiter and Schmidhuber, 1997) of v hidden units to get the contextual word representations for the documents H = {h1 , · · · , hN 0 } s.t. hi ∈ RK×2v and the query Usub ∈ RJs ×2v , Ubod ∈ RJb ×2v . Other than the word-level encoding, we also collect compact representations of all the supporting docu4 Unlike previous works (Welbl et al., 2017; Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a) that concatenate supporting documents together to form a large context, we instead maintain the document-level hierarchy and encode each document separately. 2716 ments, denoted as P = {p1 , · · · , pN 0 }, by applying the self-attention mechanism in Zhong et al. (2019) (see details in appendix). We obtain embeddings for each candidate ci ∈ {c1 , c2 , .., cL } using the average-over-word embeddings of the first mention5 of the candidate in H. 2.2 Document Explorer Our Document Explorer (DE, shown in the left part of Fig. 2) is a hierarchical memory network (Chandar et al., 2016). It utilize"
P19-1261,K17-1028,0,0.0308458,"Missing"
P19-1261,D15-1237,0,0.0833686,"Missing"
P19-1261,D18-1259,0,0.0329304,"sentence inference. Earlier attempts in multi-hop MRC focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). QAngaroo WikiHop and MedHop (Welbl et al., 2017), on the other hand, are created as natural language MRC tasks. They are designed in a way such that the evidence required to answer a query could be spread across multiple documents. Thus, finding some evidence requires building a reasoning chain from the query with intermediate inference steps, which poses extra difficulty for MRC-QA systems. HotpotQA (Yang et al., 2018) is another recent multi-hop dataset which focuses on four different reasoning paradigms. The emergence of large-scale MRC datasets has led to innovative neural models such as coattention (Xiong et al., 2017), bi-directional attention flow (Seo et al., 2017), and gated attention (Dhingra et al., 2017), all of which are metic2721 Query subject: Polsterberg Pumphouse The Sperberhai Dyke is in fact an aqueduct which forms part of the Upper Harz Water Regale network of reservoirs, ditches, dams and tunnels ... 1 2 The Harz is the highest mountain range in Northern Germany and its rugged terrain ex"
P19-1261,P18-1030,0,0.0218718,"atasets like WikiHop because the query subject and the answer could appear in different documents. On the other hand, our Document Explorer can discover the document with the answer “Loon op Zand” (in Fig. 1a) by iteratively selecting relevant documents and encoding the hinge words “Efteling” and “Kaatsheuvel” in its memory. Recently, Dhingra et al. (2018) leveraged coreference annotations from an external system to connect the entities. Song et al. (2018a) and De Cao et al. (2018) utilized Graph Convolutional Networks (Kipf and Welling, 2017) and Graph Recurrent Networks (Song et al., 2018b; Zhang et al., 2018) to model the relations between entities. Recently, Cao et al. (2019) extended the Graph Convolutional Network in De Cao et al. (2018) by introducing bi-directional attention between the entity graph and query. By connecting the entities, these models learn the inference paths for multihop reasoning. Our work differs in that our system learns the relation implicitly without the need of any human-annotated relation. Recently, Zhong et al. (2019) used hierarchies of co-attention and self-attention to combine evidence from multiple scattered documents. Our novel 3-module architecture is inspired"
P19-1261,D14-1179,0,\N,Missing
P19-1261,Q18-1021,0,\N,Missing
P19-1262,N16-2016,0,0.0265855,"ing shortcut by mentioning another government position held by a person. 7 Related Works Multi-hop Reading Comprehension The last few years have witnessed significant progress on large-scale QA datasets including cloze-style blank-filling tasks (Hermann et al., 2015), opendomain QA (Yang et al., 2015), QA with answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a singledocument context per question domain. Earlier attempts in multi-hop QA focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). The bAbI dataset (Weston et al., 2016) uses synthetic contextx and requires the model to combine multiple pieces of evidence in the text-based context. 2733 TriviaQA (Joshi et al., 2017) includes a small portion of questions that require cross-sentence inference. Welbl et al. (2017) uses Wikipedia articles as the context and subject-relation pairs as the query, and construct the multi-hop QAngaroo dataset by traversing a directed bipartite graph. It is designed in a way such that the evidence required to answer a query could"
P19-1262,D17-1215,0,0.437023,"contain the answer, we apply A DD D OC twice while alternating the choice of p1 and p2 tle occurrence, for each adversarial document, we additionally find another document from the entire dev set that contains the exact title of our adversarial document and add it to the context.5 Every new document added to the context replaces an original non-supporting document so that the total number of documents in context remains unchanged. Note that A DD D OC adversaries are model-independent, which means that they require no access to the model or any training data, similar to the A D D O NE S ENT in Jia and Liang (2017). 3 3.1 Models Encoding We first describe the pre-processing and encoding steps. We use a Highway Network (Srivastava et al., 2015) of dimension v, which merges the character embedding and GloVe word embedding (Pennington et al., 2014), to get the word representations for the context and the question as x ∈ RJ×v and q ∈ RS×v where J and S are the lengths of the context and question. We then apply a bi-directional LSTM-RNN (Hochreiter and Schmidhuber, 1997) of d hidden units to get the contextualized word representations for the context and question: h = BiLSTM(x); u = BiLSTM(q) so that h ∈ RJ×"
P19-1262,P17-1147,0,0.0443002,"Hermann et al., 2015), opendomain QA (Yang et al., 2015), QA with answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a singledocument context per question domain. Earlier attempts in multi-hop QA focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). The bAbI dataset (Weston et al., 2016) uses synthetic contextx and requires the model to combine multiple pieces of evidence in the text-based context. 2733 TriviaQA (Joshi et al., 2017) includes a small portion of questions that require cross-sentence inference. Welbl et al. (2017) uses Wikipedia articles as the context and subject-relation pairs as the query, and construct the multi-hop QAngaroo dataset by traversing a directed bipartite graph. It is designed in a way such that the evidence required to answer a query could be spread across multiple documents that are not directly related to the query. HotpotQA (Yang et al., 2018) is a more recent multi-hop dataset that has crowd-sourced questions with diverse syntactic and semantic features. HotpotQA and QAngaroo also diffe"
P19-1262,D18-1362,0,0.0315122,"her government position held by a person. 7 Related Works Multi-hop Reading Comprehension The last few years have witnessed significant progress on large-scale QA datasets including cloze-style blank-filling tasks (Hermann et al., 2015), opendomain QA (Yang et al., 2015), QA with answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a singledocument context per question domain. Earlier attempts in multi-hop QA focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). The bAbI dataset (Weston et al., 2016) uses synthetic contextx and requires the model to combine multiple pieces of evidence in the text-based context. 2733 TriviaQA (Joshi et al., 2017) includes a small portion of questions that require cross-sentence inference. Welbl et al. (2017) uses Wikipedia articles as the context and subject-relation pairs as the query, and construct the multi-hop QAngaroo dataset by traversing a directed bipartite graph. It is designed in a way such that the evidence required to answer a query could be spread across multiple documents th"
P19-1262,D14-1162,0,0.0820432,"one-year older than Kate” rarely exist in Wikipedia articles. Therefore, we simply leave these examples unchanged in our adversarial data. 2728 Suppose p2 ∈ P is a document containing the answer a and p1 ∈ P is the other supporting document.4 A DD D OC applies a word/phrase-level perturbation to p2 so that the generated p02 contains a fake answer that satisfies the reasoning shortcut but does not contradict the answer to the entire question (e.g., the adversarial document in Fig. 2). First, for every non-stopword in the answer, we find the substitute within the top-10 closest words in GloVe (Pennington et al., 2014) 100-d vector space that doesn’t have an overlapping substring longer than 3 with the original answer (“Mumbai → Delhi, Goalkeeper → Defender”). If this procedure fails, we randomly sample a candidate from the entire pool of answers in the HotpotQA dev set (e.g., “Rome” for Fig. 2 or “defence of the Cathedral” for Fig. 1). We then replace the original answer in p2 with our generated answer to get p02 . If the original answer spans multiple words, we substitute one non-stopword in the answer with the corresponding sampled answer word to create the fake answer (“World’s Best Goalkeeper → World’s"
P19-1262,P18-2124,0,0.130963,"Missing"
P19-1262,D16-1264,0,0.645651,"1 What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992? necessary to answer the question is concentrated in a single sentence or located closely in a single paragraph (Q: “What’s the color of the sky?”, Context: “The sky is blue.”, Answer: “Blue”). Such datasets emphasize the role of matching and aligning information between the question and the context (“sky→sky, color→blue”). Previous works have shown that models with strong questionaware context representation (Seo et al., 2017; Xiong et al., 2017) can achieve super-human performance on single-hop QA tasks like SQuAD (Rajpurkar et al., 2016, 2018). Recently, several multi-hop QA datasets, such 2726 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2726–2736 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics as QAngaroo (Welbl et al., 2017) and HotpotQA (Yang et al., 2018), have been proposed to further assess QA systems’ ability to perform composite reasoning. In this setting, the information required to answer the question is scattered in the long context and the model has to connect multiple evidence pieces to pinpoint to the final answer. Fi"
P19-1262,P17-1018,0,0.0141267,"cuments that make it necessary to perform multi-hop reasoning in order to find the correct answer. As shown in Fig. 1, we apply phrase-level perturbations to the answer span and the titles in the supporting documents to create the adversary with a new title and a fake answer to confuse the model. With the adversary added to the context, it is no longer possible to locate the correct answer with the single-hop shortcut, which now leads to two possible answers (“World’s Best Goalkeeper” and “World’s Best Defender”). We evaluate the strong “Bi-attention + Self-attention” model (Seo et al., 2017; Wang et al., 2017) from Yang et al. (2018) on our constructed adversarial dev set (adv-dev), and find that its EM score drops significantly. In the example in Fig. 1, the 2 HotpotQA has a fullwiki setting as an open-domain QA task. In this work, we focus on the distractor setting as it provides a less noisy environment to study machine reasoning. model is confused by our adversary and predicts the wrong answer (“World’s Best Defender”). Our experiments further reveal that when strong supervision of the supporting facts that contain the evidence is applied, the baseline achieves a significantly higher score on t"
P19-1262,N18-2091,1,0.870753,"on question). serve words with common semantic meaning to the question so that it can distract models that are exploiting the reasoning shortcut in the context. Adversarial Evaluation and Training Jia and Liang (2017) first applied adversarial evaluation to QA models on the SQuAD (Rajpurkar et al., 2016) dataset by generating a sentence that only resembles the question syntactically and appending it to the paragraph. They report that the performances of state-of-the-art QA models (Seo et al., 2017; Hu et al., 2018; Huang et al., 2018) drop significantly when evaluated on the adversarial data. Wang and Bansal (2018) further improves the AddSent adversary and proposed AddSentDiverse that employs a diverse vocabulary for the question conversion procedure. They show that models trained with such adversarial examples can be robust against a wide range of adversarial evaluation samples. Our paper shares the spirit with these two works as we also try to investigate models’ over-stability to semantics-altering perturbations. However, our study also differs from the previous works (Jia and Liang, 2017; Wang and Bansal, 2018) in two points. First, we generate adversarial documents by replacing the answer and brid"
P19-1262,D15-1237,0,0.100092,"Missing"
P19-1262,D18-1259,0,0.307287,"ng information between the question and the context (“sky→sky, color→blue”). Previous works have shown that models with strong questionaware context representation (Seo et al., 2017; Xiong et al., 2017) can achieve super-human performance on single-hop QA tasks like SQuAD (Rajpurkar et al., 2016, 2018). Recently, several multi-hop QA datasets, such 2726 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2726–2736 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics as QAngaroo (Welbl et al., 2017) and HotpotQA (Yang et al., 2018), have been proposed to further assess QA systems’ ability to perform composite reasoning. In this setting, the information required to answer the question is scattered in the long context and the model has to connect multiple evidence pieces to pinpoint to the final answer. Fig. 1 shows an example from the HotpotQA dev set, where it is necessary to consider information in two documents to infer the hidden reason of soning chain “Kasper Schemeichel −−−−→ Peter voted as Schemeichel −−−−−→ World’s Best Goalkeeper” that leads to the final answer. However, in this example, one may also arrive at t"
P19-1262,C18-1171,0,0.0245291,"by mentioning another government position held by a person. 7 Related Works Multi-hop Reading Comprehension The last few years have witnessed significant progress on large-scale QA datasets including cloze-style blank-filling tasks (Hermann et al., 2015), opendomain QA (Yang et al., 2015), QA with answer span prediction (Rajpurkar et al., 2016, 2018), and generative QA (Nguyen et al., 2016). However, all of the above datasets are confined to a singledocument context per question domain. Earlier attempts in multi-hop QA focused on reasoning about the relations in a knowledge base (Jain, 2016; Zhou et al., 2018; Lin et al., 2018) or tables (Yin et al., 2015). The bAbI dataset (Weston et al., 2016) uses synthetic contextx and requires the model to combine multiple pieces of evidence in the text-based context. 2733 TriviaQA (Joshi et al., 2017) includes a small portion of questions that require cross-sentence inference. Welbl et al. (2017) uses Wikipedia articles as the context and subject-relation pairs as the query, and construct the multi-hop QAngaroo dataset by traversing a directed bipartite graph. It is designed in a way such that the evidence required to answer a query could be spread across mu"
P19-1351,D16-1044,0,0.625382,"answers are given an extra score to enhance the chance of selection (later fusion). Empirical results show that paragraph captions, even when automatically generated (via an RL-based encoderdecoder model), help correctly answer more visual questions. Overall, our joint model, when trained on the Visual Genome dataset, significantly improves the VQA performance over a strong baseline model. 1 Introduction Understanding visual information along with natural language have been studied in different ways. In visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017; Lu et al., 2016; Fukui et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Zhu et al., 2016; Anderson et al., 2018), models are trained to choose the correct answer given a question about an image. On the other hand, in image captioning tasks (Karpathy and Fei-Fei, 2015; Johnson et al., 2016; Anderson et al., 2018; Krause et al., 2017; Liang et al., 2017; Melas-Kyriazi et al., 2018), the goal is to generate sentences which should describe a given image. Similar to the VQA task, image captioning models should also learn the relationship between partial areas in an image and the generated words or phrases. While these two tasks"
P19-1351,D18-1084,0,0.0765758,"the VQA performance over a strong baseline model. 1 Introduction Understanding visual information along with natural language have been studied in different ways. In visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017; Lu et al., 2016; Fukui et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Zhu et al., 2016; Anderson et al., 2018), models are trained to choose the correct answer given a question about an image. On the other hand, in image captioning tasks (Karpathy and Fei-Fei, 2015; Johnson et al., 2016; Anderson et al., 2018; Krause et al., 2017; Liang et al., 2017; Melas-Kyriazi et al., 2018), the goal is to generate sentences which should describe a given image. Similar to the VQA task, image captioning models should also learn the relationship between partial areas in an image and the generated words or phrases. While these two tasks seem to have different directions, they have the same purpose: understanding visual information with language. If their goal is similar, can the tasks help each other? In this work, we propose an approach to improve a VQA model by exploiting textual information from a paragraph captioning model. Suppose you are assembling furniture by looking at a v"
P19-1351,D14-1162,0,0.0820964,"3.2.1 VTQA Model Features Visual Features: We adopt the bottom-up and top-down VQA model from Anderson et al. (2018), which uses visual features from the salient areas in an image (bottom-up) and gives them weights using attention mechanism (top-down) with features from question encoding. Following Anderson et al. (2018), we also use Faster R-CNN (Ren et al., 2015) to get visual features V ∈ RO×d , where O is #objects detected and d is the dimension of each visual feature of the objects. Paragraph Captions: These provide diverse aspects of an image by describing the whole scene. We use GloVe (Pennington et al., 2014) for the word embeddings. The embedded words are sequentially fed into the encoder, for which we use GRU (Cho et al., 2014), to create a sentence representation, si ∈ Rd : si = ENCsent (w0:T ), where T is the number of words. The paragraph feature is a matrix which contains each sentence representation in each row, P ∈ RK×d , where K is the number of sentences in a paragraph. Object Property Sentences: The other text we use is from properties of detected objects in images (name and attribute), which can provide explicit information of the corresponding object to a VQA model. We create simple s"
Q13-1023,P11-1070,1,0.835243,"ams corpus (Brants and Franz, 2006), which contains English n-grams (n = 1 to 5) and their observed frequency counts, generated from nearly 1 trillion word tokens and 95 billion sentences. We consider each pair of words (a1 , a2 ) in the input set in turn. For each pattern p in the two pattern sets (weak-strong Pws and strong-weak Psw ), we insert the word pair into the pattern as p(a1 , a2 ) to get a phrasal query like “big but not huge”. This is done by replacing the two wildcards in the pattern by the two words in order. Finally, we scan the Web ngrams corpus in a batch approach similar to Bansal and Klein (2011) and collect frequencies of all our phrase queries. Table 2 depicts some examples of useful intensity-based phrase queries and their frequencies in the Web-scale corpus. We also collect frequencies for the input word unigrams and the patterns for normalization purposes. Given a word pair (a1 , a2 ) and a corpus count function cnt, we define W1 = 1 X cnt(p1 (a1 , a2 )) P1 p1 ∈Pws 1 X S1 = cnt(p2 (a1 , a2 )) P2 p2 ∈Psw W2 = 1 X cnt(p1 (a2 , a1 )) P1 p1 ∈Pws 1 X cnt(p2 (a2 , a1 )) S2 = P2 (1) p2 ∈Psw with P1 = X cnt(p1 ) p1 ∈Pws P2 = X cnt(p2 ), (2) p2 ∈Psw 281 such that the final overall weak-st"
Q13-1023,W04-3205,0,0.0732836,"pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available (good, great) good , but not great → 24492.0 good , if not great → 1912.0 good , though not great → 504.0 good , or even great → 338.0 not just good but great → 181.0 good , almost great → 156.0 (great, good) not great , just good → 248.0 great or very good → 89.0 not great but still good → 47.0 (small, minute) small , almost minute → 97.0 small , even minute → 41.0 Table 2: Some examples from the Web-scale"
Q13-1023,P08-1079,0,0.0125031,". In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean by Chklovski and Pantel (2004) extracts various fine-grained semantic relations (including the stronger-than relation) between pairs of verbs, using lexico-syntactic patterns over the Web. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the p"
Q13-1023,P10-1018,0,0.204588,"Missing"
Q13-1023,P93-1023,0,0.835131,"ection 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available (good, great) good , but not great → 24492.0 good , if not great → 1912.0 good , though not great → 504.0 good , or even great → 338.0 not just good but great → 181.0 good , almost great → 156.0 (great, good) not great , just good → 248.0 great or very good → 89.0 not great but still good → 47.0 (small, minute) small , almost minute → 97.0 small , even minute → 41.0"
Q13-1023,P97-1023,0,0.752309,"et of words using pairwise evidence is also applicable to the VerbOcean pairs, and should help address similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale sc"
Q13-1023,C00-1044,0,0.111279,"Missing"
Q13-1023,C92-2082,0,0.0511667,"ork (and see Section 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available (good, great) good , but not great → 24492.0 good , if not great → 1912.0 good , though not great → 504.0 good , or even great → 338.0 not just good but great → 181.0 good , almost great → 156.0 (great, good) not great , just good → 248.0 great or very good → 89.0 not great but still good → 47.0 (small, minute) small , almost minu"
Q13-1023,J06-2003,0,0.012551,"se two words are attested together on the Web with respect to the intensity patterns more than with other candidate words. Therefore, we try to respect the order of such word pairs more in the final ranking when we are breaking constraint-violating cycles. 3 Related Work Hatzivassiloglou and McKeown (1993) presented the first step towards automatic identification of adjective scales, thoroughly discussing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, 283 attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an"
Q13-1023,D09-1017,0,0.0273154,"ddress similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partition"
Q13-1023,P06-1101,0,0.02426,"attested together on the Web with respect to the intensity patterns more than with other candidate words. Therefore, we try to respect the order of such word pairs more in the final ranking when we are breaking constraint-violating cycles. 3 Related Work Hatzivassiloglou and McKeown (1993) presented the first step towards automatic identification of adjective scales, thoroughly discussing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, 283 attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an"
Q13-1023,J11-2001,0,0.0785274,"y issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partitioned into connected comp"
Q13-1023,C08-1114,0,0.0522616,"Missing"
Q13-1023,P07-1067,0,0.0763955,"Missing"
Q13-1023,D11-1016,0,0.0252742,"wise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 40 41 27 30 20 12 10 3 3 2 7 8 0 3 4 5 6 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partitioned into connected components such that any adjective"
Q13-1023,J13-3004,0,\N,Missing
Q15-1005,S07-1002,0,0.0410802,"the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1 The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. 61 Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given target word. In an instance, there are Ng global context words (wg ) and N` local context words (w` ), a"
Q15-1005,P14-2131,1,0.815957,"esented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of e"
Q15-1005,S13-2050,0,0.121632,"012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´ero"
Q15-1005,S07-1060,0,0.00951226,"2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context i"
Q15-1005,D07-1109,0,0.00754963,"ge that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element"
Q15-1005,E09-1013,0,0.500978,"lly conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep the local context of each ambiguous word, discarding the global context. However, the topical information contained in the broader context, though it may not determine the sense directly, might still be useful for narrowing down the likely senses of the ambiguous word. Consider the ambi"
Q15-1005,J93-2003,0,0.0664727,"uations. 62 the model. We later include an empirical comparison to justify some of our modeling choices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = (3) 1 Pr(s` = k|d, t` = j) Pr(t` = j|d, s` = k) Zd where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d, t` = j) = Pθs (s` = k|d)Pθs|tj(s` = k|t` = j)Pθst (t` = j, s` = k) Zd,tj Pr(t` = j|d, s` = k) = Pθt(t` = j|d)Pθt|sk(t` = j|s` = k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs , θs|tj , θst , and θt|sk . We use the same idea to factor the word generation dis"
Q15-1005,D07-1108,0,0.0103102,"(HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous"
Q15-1005,P05-1048,0,0.0329534,", expensive, and subject to poor 60 inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it ca"
Q15-1005,D07-1007,0,0.0209944,"se a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional w"
Q15-1005,P11-1098,0,0.0420272,"Missing"
Q15-1005,E03-1020,0,0.0393647,"aseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1 The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. 61 Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the given target word. In an instance, there are Ng global context words"
Q15-1005,D09-1046,0,0.023234,"on (WSI) is the task of automatically discovering all senses of an ambiguous word in a corpus. The inputs to WSI are instances of the ambiguous word with its surrounding context. The output is a grouping of these instances into clusters corresponding to the induced senses. WSI is generally conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep"
Q15-1005,P09-1002,0,0.0144135,"Missing"
Q15-1005,ide-suderman-2004-american,0,0.0506654,", all of which are observed. There is one latent variable (“topic” tg ) for the wg and two latent variables (“topic” t` and “sense” s` ) for the w` . Each instance has topic mixing proportions θt and sense mixing proportions θs . For clarity, not all variables are shown. The complete figure with all variables is given in Appendix A. This is a dependency network, not a directed graphical model, as shown by the directed arrows between t` and s` ; see text for details. dataset released for SemEval-2013 Task 13 (Jurgens and Klapaftis, 2013), collected from the Open American National Corpus (OANC; Ide and Suderman, 2004).2 It includes 50 target words: 20 verbs, 20 nouns, and 10 adjectives. There are a total of 4,664 instances across all target words. Each instance contains only one sentence, with a minimum length of 22 and a maximum length of 100. The gold standard for the dataset was prepared by multiple annotators, where each annotator labeled instances based on the sense inventories in WordNet 3.1. For each instance, they rated all senses of a target word on a Likert scale from one to five. 4 A Sense-Topic Model for WSI We now present our sense-topic model, shown in plate notation in Figure 1. It generates"
Q15-1005,S13-2049,0,0.165256,"arate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language model"
Q15-1005,N13-1062,0,0.0118433,"automatically discovering all senses of an ambiguous word in a corpus. The inputs to WSI are instances of the ambiguous word with its surrounding context. The output is a grouping of these instances into clusters corresponding to the induced senses. WSI is generally conducted as an unsupervised learning task, relying on the assumption that the surrounding context of a word indicates its meaning. Most previous work assumed that each instance is best labeled with a single sense, and therefore, that each instance belongs to exactly one sense cluster. However, recent work (Erk and McCarthy, 2009; Jurgens, 2013) has shown that more than one sense can be used to interpret certain instances, due to context ambiguity and sense relatedness. One approach would be to run LDA on the instances for an ambiguous word, then simply interpret topics as induced senses (Brody and Lapata, 2009). However, while sense and topic are related, they are distinct linguistic phenomena. Topics are assigned to entire documents and are expressed by all word tokens, while senses relate to a single ambiguous word and are expressed through the local context of that word. One possible approach would be to only keep the local conte"
Q15-1005,P02-1017,0,0.0184036,"ices (§5). First, when relating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = (3) 1 Pr(s` = k|d, t` = j) Pr(t` = j|d, s` = k) Zd where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d, t` = j) = Pθs (s` = k|d)Pθs|tj(s` = k|t` = j)Pθst (t` = j, s` = k) Zd,tj Pr(t` = j|d, s` = k) = Pθt(t` = j|d)Pθt|sk(t` = j|s` = k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs , θs|tj , θst , and θt|sk . We use the same idea to factor the word generation distribution: Pr(w` |t` = j, s` = k) = Pψtj(w` |t` = j)Pψsk(w` |s` = k) Ztj ,sk where Ztj ,sk is a normali"
Q15-1005,E12-1060,0,0.237265,"formance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full disc"
Q15-1005,S13-2051,0,0.0614558,"c model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic"
Q15-1005,P10-1116,0,0.00968942,"ly discover the number of senses. Lau et al. (2012) described a model based on an HDP with positional word features; it formed the basis for their submission (unimelb, Lau et al., 2013) to the SemEval2013 WSI task (Jurgens and Klapaftis, 2013). Our sense-topic model is distinct from this prior work in that we model sense and topic as two separate latent variables and learn them jointly. We compare to the performance of unimelb in §5. For word sense disambiguation, there also exist several approaches that use topic models (Cai et al., 2007; Boyd-Graber and Blei, 2007; Boyd-Graber et al., 2007; Li et al., 2010); space does not permit a full discussion. Word Representations for WSI: Another approach to solving WSI is to use word representations built by distributional semantic models (DSMs; Sahlgren, 2006) or neural net language models (NNLMs; Bengio et al., 2003; Mnih and Hinton, 2007). Their assumption is that words with similar distributions have similar meanings. Akkaya et al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplicati"
Q15-1005,D07-1038,0,0.0120413,"lating the sense and topic variables, we avoid making a single decision about generative dependence. Taking inspiration from dependency networks (Heckerman et al., 2001), we use the following factorization: Pr(t` = j, s` = k|d) = (3) 1 Pr(s` = k|d, t` = j) Pr(t` = j|d, s` = k) Zd where Zd is a normalization constant. We factorize further by using redundant probabilistic events, then ignore the normalization constants during learning, a concept commonly called deficiency (Brown et al., 1993). Deficient modeling has been found to be useful for a wide range of NLP tasks (Klein and Manning, 2002; May and Knight, 2007; Toutanova and Johnson, 2007). In particular, we factor the conditional probabilities in Eq. (3) into products of multinomial probabilities: Pr(s` = k|d, t` = j) = Pθs (s` = k|d)Pθs|tj(s` = k|t` = j)Pθst (t` = j, s` = k) Zd,tj Pr(t` = j|d, s` = k) = Pθt(t` = j|d)Pθt|sk(t` = j|s` = k) Zd,sk where Zd,tj and Zd,sk are normalization factors and we have introduced new multinomial parameters θs , θs|tj , θst , and θt|sk . We use the same idea to factor the word generation distribution: Pr(w` |t` = j, s` = k) = Pψtj(w` |t` = j)Pψsk(w` |s` = k) Ztj ,sk where Ztj ,sk is a normalization factor, and we"
Q15-1005,P10-2041,0,0.0212012,"Missing"
Q15-1005,passonneau-etal-2010-word,0,0.0283751,"deling to WSI as well as other approaches that use word embeddings and clustering algorithms. WSD and WSI: WSI is related to but distinct from word sense disambiguation (WSD). WSD seeks to assign a particular sense label to each target word instance, where the sense labels are known and usually drawn from an existing sense inventory like WordNet (Miller et al., 1990). Although extensive research has been devoted to WSD, WSI may be more useful for downstream tasks. WSD relies on sense inventories whose construction is time-intensive, expensive, and subject to poor 60 inter-annotator agreement (Passonneau et al., 2010). Sense inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis,"
Q15-1005,D09-1146,0,0.0473864,"Missing"
Q15-1005,W04-2406,0,0.109124,"external corpora for data enrichment), and in a product-of-embeddings baseline. Baskaya et al. (2013) represent the context of each ambiguous word by using the most likely substitutes according to a 4-gram LM. They pair the ambiguous word with likely substitutes, project the pairs onto a sphere (Maron et al., 2010), and obtain final senses via k-means clustering. We compare to their SemEval-2013 system AI-KU (§5). Other Approaches to WSI: Other approaches include clustering algorithms to partition instances of an ambiguous word into sense-based clusters (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), or graph-based methods to induce senses (Dorow and Widdows, 2003; V´eronis, 2004; Agirre and Soroa, 2007). 3 Problem Setting In this paper, we induce senses for a set of word types, which we refer to as target words. For each target word, we have a set of instances. Each instance provides context for a single occurrence of the target word.1 For our experiments, we use the 1 The target word token may occur multiple times in an instance, but only one occurrence is chosen as the target word occurrence. 61 Figure 1: Proposed sense-topic model in plate notation. There are MD instances for the giv"
Q15-1005,J98-1004,0,0.845845,"Missing"
Q15-1005,P10-1040,0,0.00846388,"al. (2012) use word representations learned from DSMs directly for WSI. Each word is represented by a cooccurrence vector, and the meaning of an ambiguous word in a specific context is computed through element-wise multiplication applied to the vector of the target word and its surrounding words in the context. Then instances are clustered by hierarchical clustering based on their representations. Word representations trained by NNLMs, often called word embeddings, capture information via training criteria based on predicting nearby words. They have been useful as features in many NLP tasks (Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Hisamoto et al., 2013; Bansal et al., 2014). The similarity between two words can be computed using cosine similarity of their embedding vectors. Word embeddings are often also used to build representations for larger units of text, such as sentences, through vector operations (e.g., summation) applied to the vector of each token in the sentence. In our work, we use word embeddings to compute word similarities (for better modeling of our data distribution), to represent sentences (to find similar sentences in external corpora for data enrichment)"
Q15-1005,H05-1097,0,0.0351615,"inventories also impose a fixed sense granularity for each ambiguous word, which may not match the ideal granularity for the task of interest. Finally, they may lack domain-specific senses and are difficult to adapt to low-resource domains or languages. In contrast, senses induced by WSI are more likely to represent the task and domain of interest. Researchers in machine translation and information retrieval have found that predefined senses are often not well-suited for these tasks (Voorhees, 1993; Carpuat and Wu, 2005), while induced senses can lead to improved performance (V´eronis, 2004; Vickrey et al., 2005; Carpuat and Wu, 2007). Topic Modeling for WSI: Brody and Lapata (2009) proposed a topic model that uses a weighted combination of separate LDA models based on different feature sets (e.g. word tokens, parts of speech, and dependency relations). They only used smaller units of text surrounding the ambiguous word, discarding the global context of each instance. Yao and Van Durme (2011) proposed a model based on a hierarchical Dirichlet process (HDP; Teh et al., 2006), which has the advantage that it can automatically discover the number of senses. Lau et al. (2012) described a model based on a"
Q15-1005,W11-1102,0,0.240639,"Missing"
Q15-1025,P05-1074,0,0.0343034,"Mohit Bansal† Kevin Gimpel† Karen Livescu† Dan Roth∗ ∗ University of Illinois at Urbana-Champaign, Urbana, IL, 61801, USA {wieting2,danr}@illinois.edu † Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA {mbansal,kgimpel,klivescu}@ttic.edu Abstract One component of many such systems is a paraphrase table containing pairs of text snippets, usually automatically generated, that have the same meaning. The most recent work in this area is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), a collection of confidence-rated paraphrases created using the pivoting technique of Bannard and Callison-Burch (2005) over large parallel corpora. The PPDB is a massive resource, containing 220 million paraphrase pairs. It captures many short paraphrases that would be difficult to obtain using any other resource. For example, the pair {we must do our utmost, we must make every effort} has little lexical overlap but is present in PPDB. The PPDB has recently been used for monolingual alignment (Yao et al., 2013), for predicting sentence similarity (Bjerva et al., 2014), and to improve the coverage of FrameNet (Rastogi and Van Durme, 2014). The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensi"
Q15-1025,P14-2131,1,0.218757,"46 our code and the trained models.3 2 Related Work There is a vast literature on representing words as vectors. The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddin"
Q15-1025,D10-1115,0,0.0117713,"pularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010). An alternative approach to composition, used by Socher et al. (2011), is to train a"
Q15-1025,P14-1133,0,0.033444,"over, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1 1 Introduction Paraphrase detection2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009). 1 We release our datasets, code, and trained models at http://web.engr.illinois.edu/˜wieting2/. 2 See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases. Though already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size"
Q15-1025,S14-2114,0,0.0312541,"Missing"
Q15-1025,D12-1050,0,0.0101664,"rds have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositiona"
Q15-1025,C04-1051,0,0.17504,"notatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship. 3.1 Annotated-PPDB Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases. Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). To our knowledge, there are no datasets that focus on the paraphrasability of short phrases. Thus, we created Annotated-PPDB so that researchers can focus on local compositional phenomena and measure the performance of models directly—avoiding the need to do so indirectly in a sentence-level task. Models that have strong performance on Annotated-PPDB can be used to provide more accurate confidence scores for the paraphrases in the PPDB as well as reduce the need for large paraphrase tables altogether. Annotated-PPDB was created in a multi-step process (outlined below) in"
Q15-1025,P13-1158,0,0.0201205,"well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1 1 Introduction Paraphrase detection2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009). 1 We release our datasets, code, and trained models at http://web.engr.illinois.edu/˜wieting2/. 2 See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases. Though already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of par"
Q15-1025,N15-1184,0,0.352035,"Missing"
Q15-1025,N13-1092,0,0.295969,"Missing"
Q15-1025,D14-1163,0,0.0687883,"els were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010). An alternative approach to composition, used by Socher et al. (2011), is to train a recursive neural network (RNN) whose structure is defined by a binarized parse tree. In particular, they trained their RNN as an unsupervised autoencoder. The RNN captures the latent structure of composition. Recent work has shown that this model struggles in tasks involving compositionality (Blacoe an"
Q15-1025,J15-4004,0,0.276726,"Missing"
Q15-1025,D14-1181,0,0.00646945,"est results reported to date. We also find that we can train low-dimensional word vectors that exceed the performance of much larger vectors. This is very useful as using large vectors can increase both time and memory consumption in NLP applications. To generate word vectors to use for downstream 10 Hill et al. (2014a) did not report the dimensionality of the vectors that led to their state-of-the-art results. 351 Sentiment Analysis As an extrinsic evaluation of our PARAGRAM word vectors, we used them in a convolutional neural network (CNN) for sentiment analysis. We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013). We used the standard data splits, removing examples with a neutral rating. We trained on all constituents in the training set while only using full sentences from development and test, giving us train/development/test sizes of 67,349/872/1,821. The CNN uses m-gram filters, each of which is an m × n vector. The CNN computes the inner product between an m-gram filter and each m-gram in an example, retaining the maximum match (so-called “max-pooling”). The score of the match is a single dimension in a feature vector"
Q15-1025,P14-5010,0,0.00474436,"rase bigrams, we consider the original bigram similarity task from Mitchell and Lapata (2010) as well as our newlyannotated version of it: ML-Paraphrase. 6.2 Evaluation and Baselines Extracting Training Data Training data for these tasks was extracted from the XL portion of PPDB. The bigram similarity task from Mitchell and Lapata (2010) contains three types of bigrams: adjectivenoun (JN), noun-noun (NN), and verb-noun (VN). We aimed to collect pairs from PPDB that mirrored these three types of bigrams. We found parsing to be unreliable on such short segments of text, so we used a POS tagger (Manning et al., 2014) to tag the tokens in each phrase. We then used the word alignments in PPDB to extract bigrams for training. For JN and NN, we extracted pairs containing aligned, adjacent tokens in the two phrases with the appropriate part-of-speech tag. Thus we extracted pairs like heasy job, simple taski for the JN section and htown meeting, town councili for the NN section. We used a different strategy for extracting training data for the VN subset: we took aligned VN tokens and took the closest noun after the verb. This was done to approximate the direct object that would have been ideally extracted with"
Q15-1025,D09-1040,0,0.011396,"Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1 1 Introduction Paraphrase detection2 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entailment (Bosma and CallisonBurch, 2007), and machine translation (Marton et al., 2009). 1 We release our datasets, code, and trained models at http://web.engr.illinois.edu/˜wieting2/. 2 See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases. Though already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size of the dataset used to build it. In practice, it can become unwieldy to work with as the size of th"
Q15-1025,P08-1028,0,0.0146968,"rs (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices. More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alterna"
Q15-1025,D14-1162,0,0.103607,"evaluating bigram paraphrases. We release the new datasets, complete with annotation instructions and raw annotations, as well as 346 our code and the trained models.3 2 Related Work There is a vast literature on representing words as vectors. The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014). Phrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition us"
Q15-1025,W04-3219,0,0.260201,"Missing"
Q15-1025,W14-2901,0,0.0392114,"Missing"
Q15-1025,W03-1604,0,0.100189,"Missing"
Q15-1025,D13-1170,0,0.00571035,"sional word vectors that exceed the performance of much larger vectors. This is very useful as using large vectors can increase both time and memory consumption in NLP applications. To generate word vectors to use for downstream 10 Hill et al. (2014a) did not report the dimensionality of the vectors that led to their state-of-the-art results. 351 Sentiment Analysis As an extrinsic evaluation of our PARAGRAM word vectors, we used them in a convolutional neural network (CNN) for sentiment analysis. We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013). We used the standard data splits, removing examples with a neutral rating. We trained on all constituents in the training set while only using full sentences from development and test, giving us train/development/test sizes of 67,349/872/1,821. The CNN uses m-gram filters, each of which is an m × n vector. The CNN computes the inner product between an m-gram filter and each m-gram in an example, retaining the maximum match (so-called “max-pooling”). The score of the match is a single dimension in a feature vector for the example, which is then associated with a weight in a linear classifier"
Q15-1025,Q14-1017,0,0.634426,". (2011), is to train a recursive neural network (RNN) whose structure is defined by a binarized parse tree. In particular, they trained their RNN as an unsupervised autoencoder. The RNN captures the latent structure of composition. Recent work has shown that this model struggles in tasks involving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).4 However, we found suc3 http://web.engr.illinois.edu/˜wieting2/ We also replicated this approach and found training to be time-consuming even using low-dimensional word vectors. 4 cess using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations. Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015). 3 New Paraphrase Data"
Q15-1025,D13-1056,0,0.0157861,"Missing"
Q15-1025,P14-2089,0,0.216257,"e also replicated this approach and found training to be time-consuming even using low-dimensional word vectors. 4 cess using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations. Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015). 3 New Paraphrase Datasets We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship. 3.1 Annotated-PPDB Our motivation for creating Annotated-PPDB was to establish a way to evalu"
Q15-1025,Q15-1017,0,0.0125763,"ed setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations. Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015). 3 New Paraphrase Datasets We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship. 3.1 Annotated-PPDB Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases. Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or ent"
Q15-1025,C10-1142,0,0.0109219,"Missing"
Q15-1025,N09-1003,0,\N,Missing
Q18-1027,J08-4004,0,0.0117046,"Missing"
Q18-1027,D16-1216,1,0.427475,"ches do not fully extend to our politeness generation task, because politeness strategies follow complex patterns of grammar, word order, and phrasing (DanescuNiculescu-Mizil et al., 2013). For example, the politeness of please depends on where it occurs in a sentence, and what other politeness markers it cooccurs with (e.g., ‘could/would you’ style counterfactual modals vs. ‘can/will you’ style indicative modals). Therefore, our novel polite dialogue models are based on an accurate neural classifier, which is better at capturing several compositional paralinguistic features (as visualized in Aubakirova and Bansal (2016), whose politeness classifier we extend). Moreover, our LFT and Polite-RL models can generate a continuum of style levels based on the continuously-scaled (by the politeness score) label embedding or reinforcement rewards. Lastly, there have also been style transfer models that rely on the latent representation of text and use variational auto-encoders or cross-alignment to disentangle the representation of content and style 375 in text (Hu et al., 2017; Shen et al., 2017; Zhao et al., 2017; Fu et al., 2018). During inference time, the latent style representation is combined with new content t"
Q18-1027,P13-1025,0,0.568093,"is-conditioning-their-kids-to-be-rude/ 2 The first version of this paper with the three Fusion, Discrete-LFT, and Polite-RL models was submitted on Oct 1, 2017. The two retrieval baselines and the continuous version 373 Transactions of the Association for Computational Linguistics, vol. 6, pp. 373–389, 2018. Action Editor: Colin Cherry. Submission batch: 10/2017; Revision batch: 2/2018; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. alogue responses, using data from separate style and dialogue domains: the Stanford Politeness Corpus (Danescu-Niculescu-Mizil et al., 2013) with Wikipedia and Stack Exchange requests, and the MovieTriples Dialogue Corpus (Serban et al., 2016) with IMSDB movie scripts, respectively. Each of our three models is based on a state-of-the-art politeness classifier and a sequence-to-sequence dialogue model. The first model (Fusion) employs a late fusion technique to merge the response generation decoder of the dialogue model with a language model trained on polite utterances chosen by the politeness classifier. The second label-fine-tuning (LFT) model prepends to the input utterance a single politeness label whose embedding is continuou"
Q18-1027,P15-1166,0,0.024653,"e continuous label in the LFT model, or the RL weight in the Polite-RL model. 2.2 Multi-Task Learning and Style Transfer In order to obtain a persona-based conversational agent, Luan et al. (2017) proposed a multi-task learning (MTL) based approach: they train a Seq2seq model with conversation data and an autoencoder with non-conversational persona-related data from target speakers, and share the decoder parameters of these two models so that the generated responses can be adapted to the style of the target-speaker. This way of incorporating MTL into Seq2seq learning was first investigated by Dong et al. (2015) and Luong et al. (2016) to achieve multilingual NMT. In addition, Sennrich et al. (2016b) also employed MTL to improve NMT models with monolingual (non-parallel) data. These approaches are related to our Fusion model, because we use our classifier to obtain noisy polite target sequences (non-parallel data) that a polite language model trains on; next, during inference, we combine the parameters of the language model with a generative dialogue model trained on parallel data. In general, our models are also related to previous works like Johnson et al. (2017), who adopted labeled sequence trans"
Q18-1027,W17-4912,0,0.0662407,", 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training set. For example, to control how formal the output sequences are in a EN-DE translation task, Sennrich et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honorifics). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated their text with meta-data and keywords/POS tags based heuristics, while Ghosh et al. (2017) also adopted keyword spotting based on a dictionary of emotional words. The basic ideas of their models are similar to that of our LFT model. However, these keyword-spotting approaches do not fully extend to our politeness generation task, because politeness strategies follow complex patterns of grammar, word order, and phrasing (DanescuNiculescu-Mizil et al., 2013). For example, the politeness of please depends on where it occurs in a sentence, and what other politeness markers it cooccurs w"
Q18-1027,P15-2073,0,0.0240614,"nstructions (and a 300-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue quality study uses ‘Very good’, ‘Good’, ‘Acceptable’, ‘Poor’, and ‘Very poor’, instead of the abstract scores 1-5."
Q18-1027,P17-1059,0,0.164163,"and is able to handle a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training set. For example, to control how formal the output sequences are in a EN-DE translation task, Sennrich et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honorifics). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated their text with meta-data and keywords/PO"
Q18-1027,W17-4902,0,0.05796,"vels. Finally, we present several detailed qualitative and quantitative analyses, including positive and negative output examples, automatic metric results on output responses, classifier error analysis, and visualization of the RL rewards. 2 2.1 Related Works Models for Style Transfer Style Transfer with Parallel Data There have been multiple works on style transfer with parallel data. These tasks can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been proposed (Kobus et al., 2017; Yamagishi et al., 2016; Johnson et al., 2017). For example, Kikuchi et al. (2016) are able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates the intended output length in addition to the source input. Our LFT model also adopts this labeling idea, and is able to handle a similar situation but without parallel data, b"
Q18-1027,Q17-1024,0,0.0357337,"Missing"
Q18-1027,D16-1140,0,0.0212679,"Parallel Data There have been multiple works on style transfer with parallel data. These tasks can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been proposed (Kobus et al., 2017; Yamagishi et al., 2016; Johnson et al., 2017). For example, Kikuchi et al. (2016) are able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates the intended output length in addition to the source input. Our LFT model also adopts this labeling idea, and is able to handle a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer wi"
Q18-1027,kobus-etal-2017-domain,0,0.0166189,"e RL rewards. 2 2.1 Related Works Models for Style Transfer Style Transfer with Parallel Data There have been multiple works on style transfer with parallel data. These tasks can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been proposed (Kobus et al., 2017; Yamagishi et al., 2016; Johnson et al., 2017). For example, Kikuchi et al. (2016) are able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates the intended output length in addition to the source input. Our LFT model also adopts this labeling idea, and is able to handle a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Tra"
Q18-1027,N16-1082,0,0.173436,"0-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue quality study uses ‘Very good’, ‘Good’, ‘Acceptable’, ‘Poor’, and ‘Very poor’, instead of the abstract scores 1-5. Note that we did"
Q18-1027,N16-1014,0,0.69871,"0-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue quality study uses ‘Very good’, ‘Good’, ‘Acceptable’, ‘Poor’, and ‘Very poor’, instead of the abstract scores 1-5. Note that we did"
Q18-1027,P16-1094,0,0.51538,"0-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue quality study uses ‘Very good’, ‘Good’, ‘Acceptable’, ‘Poor’, and ‘Very poor’, instead of the abstract scores 1-5. Note that we did"
Q18-1027,D16-1230,0,0.0656146,"teness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue quality study uses ‘Very good’, ‘Good’, ‘Acceptable’, ‘Poor’, and ‘Very poor’, instead of the abstract scores 1-5. Note that we did not adopt pairwise comparisons because first, it will create several independent sets of pairwise res"
Q18-1027,W15-4640,0,0.0204337,"4.5 Retrieval-based Models We employ two retrieval-based baseline models as a sanity check to the proposed approaches’ perfor6 However, to make the reward-based model capable of multitasking, one could also prepend various politeness labels to each of the context in the training set (thus generating several examples out of one context), and encourage the generated response to be consistent with the given label. We will explore this extension in future work. 379 mance: the first with oracle-level fluency, the second with additional oracle-level politeness. Classifier-based Retrieval Following Lowe et al. (2015), for a [X1 , Y, X2 ] triple, our retrieval model treats the context (X1 , Y ) and each response (X2 ) as two documents and converts them to their TF-IDF based vectors (Ramos, 2003) to check for similarity. Specifically, we first obtain all candidate responses in the training set that are polite,7 and calculate their TF-IDF vectors. Then for each context TF-IDF vector in the test set, we calculate its cosine similarity with that of each such polite-classified candidate response, and output the one with the highest value. Intuitively, for each context we are choosing a response that is both pol"
Q18-1027,P17-1103,0,0.0389981,"Missing"
Q18-1027,I17-1061,0,0.55304,"labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training set. For example, to control how formal the output sequences are in a EN-DE translation task, Sennrich et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honorifics). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated their text with meta-data and keywords/POS tags based heuristics, while Ghosh et al. (2017) also adopted keyword spotting"
Q18-1027,P07-1063,0,0.0444645,"task by training a CNN model that directly learns to identify polite requests without using any hand-engineered features, while still improving on prediction accuracy. They also visualized what features the CNN model was learning and discovered some new features along the way. Our classifier mainly extends their work by adding a bi-directional LSTM layer (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) before the CNN layer to capture long-distance relationships in the sentence, which leads to higher cross-domain performance. A related early work in personality-based dialogue is Mairesse and Walker (2007), who studied introvert/extrovert personality language based on templated content and sentence planning (via personality dimensions such as hedges, tag questions, negations, subject implicitness, etc.). Relatedly, Sennrich et al. (2016a) use an English to German translation task to present a model that can generate target sequences that are either formal or informal, specifically based on honorifics-related verbs and pronouns. Our task is more general, taking into account several politeness-related paralinguistic features of Brown and Levinson (1987) and allowing end-to-end trainable stylistic"
Q18-1027,P02-1040,0,0.101301,"versation disconnected or lacking context, and encouraged them to make the best guess when in doubt. Using similar instructions (and a 300-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue qu"
Q18-1027,D11-1054,0,0.312339,"oubt. Using similar instructions (and a 300-sized sample), we also performed a separate 3-way LFT model comparison by setting its target politeness scores to 1.0, 0.5, and 0.0, respectively. Automatic Since there do not exist ground-truth stylized versions of the response to the MovieTriples conversations, we only use automatic evaluation metrics as complementary and trend-verification information to the primary human perception studies in this work: we compute BLEU (a phrase-matching based metric; (Papineni et al., 2002)) as an approximation of dialogue quality as used by some previous work (Ritter et al., 2011; Galley et al., 2015; Li et al., 2016c). Note that we choose to report BLEU scores not to draw any immediate conclusion (Liu et al. (2016) found that BLEU does not correlate well with human studies on dialogue quality), but rather to check for match with the trends from 10 The Likert scale is a bipolar scaling method that maps each score to a text item that describes the score, e.g., our politeness level interface uses ‘Polite’, ‘Slightly Polite’, ‘Neutral’, ‘Slightly Rude’, ‘Rude’; and our dialogue quality study uses ‘Very good’, ‘Good’, ‘Acceptable’, ‘Poor’, and ‘Very poor’, instead of the"
Q18-1027,N16-1005,0,0.321733,"LFT model also adopts this labeling idea, and is able to handle a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training set. For example, to control how formal the output sequences are in a EN-DE translation task, Sennrich et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honorifics). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated the"
Q18-1027,P16-1009,0,0.332661,"LFT model also adopts this labeling idea, and is able to handle a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training set. For example, to control how formal the output sequences are in a EN-DE translation task, Sennrich et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honorifics). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated the"
Q18-1027,D17-1235,0,0.022261,"ion on the performance of our proposed models, we also employ two retrieval-based polite dialogue models toward the end. 4.1 Base Seq2seq Dialogue Model Our base dialogue model is a simple sequence-tosequence (Seq2seq) model that consists of a twolayer bi-directional LSTM-RNN encoder to encode the conversation history turns, and a four-layer LSTM-RNN decoder to generate the response. Additive attention from the output of the encoder is applied to the last layer of the decoder. This architecture is almost identical to that proposed by Bahdanau et al. (2015), except with more layers (similar to Shao et al. (2017)). Our base dialogue model achieves perplexity and word error rate results on par with those reported for the popular hierarchical HRED models in Serban et al. (2016), thus serving as a good base model to incorporate style into. Details will be discussed in Section 6. 4.2 Fusion Model Inspired by the ‘late fusion’ approach in Venugopalan et al. (2016), our Fusion model (Fig. 2) combines the response generation decoder of the base Seq2seq dialogue model with a language model (polite-LM) trained exclusively on polite utterances. These utterances are chosen by feeding the classifier all response"
Q18-1027,D16-1101,0,0.0159335,"sampled response generated by the model and the reward it generates after getting fed into the style classifier. Note that the attention mechanism is not shown here for clarity. fier (see Figure 3), while during test time, we are free to scale the prepended politeness label with different scores of our choice (i.e., when we want the model to generate a polite response, we scale the label’s embedding by a score between 0.5 and 1.0, whereas, to generate a rude response, we scale the embedding by a score between 0.0 and 0.5). This approach is related to the ‘numerically-grounded’ language model (Spithourakis et al., 2016), except that we scale the politeness label embedding by its corresponding politeness score, rather than concatenating the two as input to the LSTM.3 Thus, the LFT model is able to simultaneously produce polite, neutral and rude responses depending on the prepended label, similar to recent multilabel, multi-space, and zero-shot machine translation work using language identity or style labels (Sennrich et al., 2016a; Johnson et al., 2017; Ghosh et al., 2017). Intuitively, this prepended label serves as the prior for the intended style of the generated response sequence, while the source utteran"
Q18-1027,D16-1204,0,0.0204935,"TM-RNN decoder to generate the response. Additive attention from the output of the encoder is applied to the last layer of the decoder. This architecture is almost identical to that proposed by Bahdanau et al. (2015), except with more layers (similar to Shao et al. (2017)). Our base dialogue model achieves perplexity and word error rate results on par with those reported for the popular hierarchical HRED models in Serban et al. (2016), thus serving as a good base model to incorporate style into. Details will be discussed in Section 6. 4.2 Fusion Model Inspired by the ‘late fusion’ approach in Venugopalan et al. (2016), our Fusion model (Fig. 2) combines the response generation decoder of the base Seq2seq dialogue model with a language model (polite-LM) trained exclusively on polite utterances. These utterances are chosen by feeding the classifier all response utterances in the MovieTriples training set, and only keeping those with politeness scores great than a certain threshold (set to 0.8 in our experiments, as will be discussed in Section 4.5). The polite-LM model is a two-layer LSTM-RNN based on Jozefowicz et al. (2016). During inference time, we used the language 377 G3 &lt;end&gt; T2 T3 Target Politeness C"
Q18-1027,D17-1228,0,0.110082,"Missing"
Q18-1027,C12-1177,0,0.0395505,"the continuous LFT model for three different politeness levels. Finally, we present several detailed qualitative and quantitative analyses, including positive and negative output examples, automatic metric results on output responses, classifier error analysis, and visualization of the RL rewards. 2 2.1 Related Works Models for Style Transfer Style Transfer with Parallel Data There have been multiple works on style transfer with parallel data. These tasks can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been proposed (Kobus et al., 2017; Yamagishi et al., 2016; Johnson et al., 2017). For example, Kikuchi et al. (2016) are able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates the intended output length in addition to the source input. Our LFT model also adopts this labeling idea, and i"
Q18-1027,W16-4620,0,0.0220781,"Related Works Models for Style Transfer Style Transfer with Parallel Data There have been multiple works on style transfer with parallel data. These tasks can often be solved by directly applying some variation of translation-based Seq2seq model discussed in the previous section. For example, Xu et al. (2012) use a phrase-based statistical model, and Jhamtani et al. (2017) use a standard Seq2seq model to convert modern language to Shakespeare-style language by treating style transfer as a translation task. Some labeled sequence transduction methods have also been proposed (Kobus et al., 2017; Yamagishi et al., 2016; Johnson et al., 2017). For example, Kikuchi et al. (2016) are able to control the length of the summarization text by feeding to the Seq2seq base model a label that indicates the intended output length in addition to the source input. Our LFT model also adopts this labeling idea, and is able to handle a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel D"
Q18-1027,P17-1061,0,0.149189,"le a similar situation but without parallel data, because by labeling each target sequence in the training set with its politeness classifier score, we are essentially converting nonparallel data to (noisy) parallel data (by using a classifier with high accuracy). Style Transfer without Parallel Data Several previous works have looked at style transfer without parallel data, in both vision (Gatys et al., 2016; Zhu et al., 2017; Liu and Tuzel, 2016; Liu et al., 2017; Taigman et al., 2016; Kim et al., 2017; Yi et al., 2017), and text (Sennrich et al., 2016a; Hu et al., 2017; Ghosh et al., 2017; Zhao et al., 2017; Mueller et al., 2017; Wang et al., 2017; Luan et al., 2017). Among these models, some are bag-of-words based, i.e., they use style-related keywords to annotate the target sequences in the training set. For example, to control how formal the output sequences are in a EN-DE translation task, Sennrich et al. (2016a) labeled each target sequence based on whether it contains formal or informal verbs and pronouns (honorifics). To build a language model that generates utterances with the desired style, Ficler and Goldberg (2017) annotated their text with meta-data and keywords/POS tags based heuris"
W11-1916,N10-1061,1,0.916509,"n Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity. Previously the focus in the field has been on the latter task. Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the head word. This task (Pradhan et al., 2011) has set a harder challenge by only considering exact matches to be correct. Our system uses an unsupervised approach based on a generative model. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). This was necessary for the system to be eligible for the closed task. The system detects mentions by finding the maximal projection of every noun and pronoun. For the OntoNotes corpus this approach posed several problems. First, the annotation scheme explicitly rejects noun phrases in certain constructions. And second, it includes coreference for events as well as things. In preliminary experiments on the development set, we found that spurious mentions were our primary source of error. Using an oracle to exclude all spurious mentions at evaluation time yielded improvements ranging from five"
W11-1916,P06-1055,1,0.631097,"a spurious mention, or because it is not co-referent. Without manually annotating the singletons in the data, these two cases cannot be easily separated. 3.1 Baseline mention detection The standard approach used in the system to detect mentions is to consider each word and its maximal projection, accepting it only if the span is an NP or the word is a pronoun. This approach will introduce spurious mentions if the parser makes a mistake, or if the NP is not considered a mention in the OntoNotes corpus. In this work, we considered the provided parses and parses produced by the Berkeley parser (Petrov et al., 2006) trained on the provided training data. We added a set of filters based on the annotation scheme described by Pradhan et al. (2007). Some filters are applied before coreference resolution and others afterward, as described below. Data Set Dev Test Filters None Pre Post All All P 37.59 39.49 59.05 58.69 56.97 R 76.93 76.83 68.08 67.98 69.77 F 50.50 52.17 63.24 63.00 62.72 Filters None Pre Post All Table 1: Mention detection performance with various subsets of the filters. 3.2 Before Coreference Resolution The pre-resolution filters were based on three reliable features of spurious mentions: • A"
W11-1916,W11-1901,0,0.0603164,"system behavior on the development set. These changes led to improvements in coreference F-score of 10.06, 5.71, 6.78, 6.63 and 3.09 on the MUC, B3 , Ceaf-e, Ceaf-m and Blanc, metrics, respectively, and a final task score of 47.10. 1 Introduction Coreference resolution is concerned with identifying mentions of entities in text and determining which mentions are referring to the same entity. Previously the focus in the field has been on the latter task. Typically, mentions were considered correct if their span was within the true span of a gold mention, and contained the head word. This task (Pradhan et al., 2011) has set a harder challenge by only considering exact matches to be correct. Our system uses an unsupervised approach based on a generative model. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). This was necessary for the system to be eligible for the closed task. The system detects mentions by finding the maximal projection of every noun and pronoun. For the OntoNotes corpus this approach posed several problems. First, the annotation scheme explicitly rejects noun phrases in certain constructions. And second, it includes coreference for"
W11-1916,J03-4003,0,\N,Missing
W15-1514,W13-3520,0,0.0478908,"Missing"
W15-1514,P14-2133,0,0.14456,"Missing"
W15-1514,P11-1070,1,0.902186,"Missing"
W15-1514,P14-2131,1,0.93903,"tors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recen"
W15-1514,J92-4003,0,0.147691,"Missing"
W15-1514,P05-1022,0,0.212571,"Missing"
W15-1514,D14-1082,0,0.0488528,"re useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014), feature embeddings (Chen et al., 2014), or neural network parsers (Chen and Manning, 2014). Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on ‘head—argument’ pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures, and also fire significantly fewer and simpler feat"
W15-1514,C14-1078,0,0.0459007,"015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014), feature embeddings (Chen et al., 2014), or neural network parsers (Chen and Manning, 2014). Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on ‘head—argument’ pairs (as a single concatenated unit), which allows us to work directly with linguistically-intuitive, higher-order substructures"
W15-1514,D14-1012,0,0.0192162,"vailable) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has address"
W15-1514,D14-1163,0,0.0374344,"Missing"
W15-1514,P12-1092,0,0.0966591,"Missing"
W15-1514,P08-1067,0,0.0856467,"Missing"
W15-1514,P08-1068,0,0.811052,"ilar to state-of-the-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, te"
W15-1514,P14-1130,0,0.0232676,"4; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014), feature embeddings (Chen et al., 2014), or neural network parsers (Chen and Manning, 2014). Secondly, it would also be useful to learn dense representations directly for the higher-order substructures (that structured NLP tasks factor on) so as to explicitly capture the useful, hidden relationships among these substructures, instead of relying on the sparse word-conjoined relationships. In this work, we propose to address both these issues by learning simple dependency link embeddings on ‘head—argument’ pairs (as a single concatenated unit), which allows us to work directly with linguistical"
W15-1514,P14-2050,0,0.0418795,"ally significantly. We make our link embeddings publicly available1 and hope that they will prove useful in various other NLP tasks in future work, e.g., as dense, syntactic features in sentence classification or as linguistically-intuitive, initial units in vectorspace composition. 2 Dependency Link Embeddings To train the link embeddings, we use the speedy, skip-gram neural language model of Mikolov et al. (2013a; 2013b) via their toolkit word2vec.2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to Bansal et al. (2014) and Levy and Goldberg (2014). The goal is to learn similar embeddings for links with similar syntactic contextual properties like label, signed distance, ancestors, etc. To this end, we first parse the BLLIP corpus (minus the PTB portion)3 using the baseline MSTParser (McDonald et al., 2005b). Next, for each predicted link, we create a tuple, consisting of the parent-child pair p—c (concatenated as a single unit, same as p c) and its various properties such as the 1 ttic.edu/bansal 2 https://code.google.com/p/word2vec/ 3 Same dataset as what was used to train the B ROWN clusters in Koo et al. (2008), for comparability. 1"
W15-1514,C14-1017,0,0.292817,"Missing"
W15-1514,P05-1012,0,0.224044,"pace composition. 2 Dependency Link Embeddings To train the link embeddings, we use the speedy, skip-gram neural language model of Mikolov et al. (2013a; 2013b) via their toolkit word2vec.2 We use the original skip-gram model and simply change the context tuple data on which the model is trained, similar to Bansal et al. (2014) and Levy and Goldberg (2014). The goal is to learn similar embeddings for links with similar syntactic contextual properties like label, signed distance, ancestors, etc. To this end, we first parse the BLLIP corpus (minus the PTB portion)3 using the baseline MSTParser (McDonald et al., 2005b). Next, for each predicted link, we create a tuple, consisting of the parent-child pair p—c (concatenated as a single unit, same as p c) and its various properties such as the 1 ttic.edu/bansal 2 https://code.google.com/p/word2vec/ 3 Same dataset as what was used to train the B ROWN clusters in Koo et al. (2008), for comparability. 103 N.Y.–Yonkers, Md.–Columbia, N.Y.–Bronx, Va.–Reston, Ky.–Lexington, Mich.–Kalamazoo, Calif.–Calabasas, ... boost–revenue, tap–markets, take–losses, launch–fight, reduce–holdings, terminate–contract, identify–bidders, ... boosting–bid, meeting–schedules, obtaini"
W15-1514,H05-1066,0,0.20963,"Missing"
W15-1514,D14-1162,0,0.0905778,"ctly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has addressed this issue, via low-ra"
W15-1514,P06-1055,0,0.0402594,"Missing"
W15-1514,P10-1040,0,0.111149,"he-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary f"
W15-1514,N15-1142,0,0.0123996,"NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has addressed this issue, via low-rank tensor mappings (Lei et al., 2014), feature embeddings (Chen"
W15-1514,P14-2089,0,0.0155136,"helf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them. 1 Introduction Word representations and more recently, word embeddings, learned from large amounts of text have been quite successful as features in various NLP tasks (Koo et al., 2008; Turian et al., 2010; Collobert et al., 2011; Dhillon et al., 2012; Al-Rfou’ et al., 2013; Bansal et al., 2014; Guo et al., 2014; Pennington et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wang et al., 2015). While these word representations do capture useful, dense relationships among known and unknown words, one still has to work with sparse conjunctions of features on the multiple words involved in the substructure that a task factors on, e.g., head-argument links in dependency parsing. Therefore, most statistical dependency parsers still suffer from millions of such conjoined, template-based, n-ary features on word clusters or embeddings (Koo et al., 2008; Bansal et al., 2014). Some recent work has addressed this issue, via low-rank tensor mappings (L"
W15-1514,N15-1184,0,\N,Missing
W16-1612,W10-1408,0,0.0608533,"Missing"
W16-1612,D15-1041,0,0.0192778,"the mapper tends to be very quick because training examples are word types rather than word tokens. When we increase τt , the number of training examples reduces further. Hence, since we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014"
W16-1612,P15-1001,0,0.0851014,"as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to"
W16-1612,P11-1070,1,0.750738,"nce we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems"
W16-1612,P14-2131,1,0.778269,"et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al., 2014), there 2 Mapping Unseen Representations Let V = {w1 , . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi computed from this corpus. The initial embeddings are typically learned in an unsupervised 100 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Model Parameters W Annotated Training Sentences Initial Embeddings eoi Parser Training Mapper Function Task-Trained Embeddings eti Mapper Function"
W16-1612,P14-1062,0,0.0219581,"Missing"
W16-1612,W09-3821,0,0.0868285,"Missing"
W16-1612,J03-3005,0,0.0248644,"examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong"
W16-1612,D14-1082,0,0.206154,"s. way, but for our purposes they can be any initial embeddings. Let T ⊆ V be the subset of words that appear in the annotated training data for some supervised task-specific training. We define unseen words as those in the set V  T . While our approach is general, for concreteness, we consider the task of dependency parsing, so the annotated data consists of sentences paired with dependency trees. We assume a dependency parser that learns task-specific word embeddings eti for word wi ∈ T , starting from the original embedding eoi . In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014). The goal of the mapper is as follows. We are given a training set of N pairs of  and task-trained  initial  embeddings D = eo1 , et1 , . . . , eoN , etN , and we want to learn a function G that maps each initial embedding eoi to be as close as possible to its corresponding output embedding eti . We denote the mapped embedding o m em i , i.e., ei = G (ei ). Figure 1a describes the training procedure of the mapper. We use a supervised parser which is trained on an annotated dataset and initialized with pre-trained word embeddings eoi . The parser uses back-propagation to update these embedd"
W16-1612,D14-1181,0,0.0133613,"l., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015). They induce embeddings for unseen words by combining the embeddings of the k nearest neighbors. In Sec. 4, we show that our approach outperforms theirs. Also related is the approach taken by Kiros et al. (2015). They learn a linear mapping of the initial embedding space via unregularize"
W16-1612,D15-1249,0,0.0203196,"and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015)"
W16-1612,D14-1108,0,0.051108,"Missing"
W16-1612,P15-1033,0,0.0139127,"s much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015)."
W16-1612,D15-1176,0,0.129017,"Missing"
W16-1612,W13-3512,0,0.0375429,"including those that learn a single 1 Note that the training of the mapper tends to be very quick because training examples are word types rather than word tokens. When we increase τt , the number of training examples reduces further. Hence, since we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combinin"
W16-1612,D10-1069,0,0.0241754,"e test set, we must use either their initial embeddings or a single unknown embedding, which often leads to errors. We address this by learning a neural network to map from initial embeddings to the task-specific embedding space, via a multi-loss objective function. The technique is general, but here we demonstrate its use for improved dependency parsing (especially for sentences with out-of-vocabulary words), as well as for downstream improvements on sentiment analysis. 1 Karen Livescu† Introduction Performance on NLP tasks drops significantly when moving from training sets to held-out data (Petrov et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al.,"
W16-1612,P15-1002,0,0.0694421,"use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). Closely related to our approach is that"
W16-1612,K15-1009,0,0.0372302,"Missing"
W16-1612,W10-1410,0,0.0515923,"Missing"
W16-1612,J93-2004,0,0.0569575,"Missing"
W16-1612,W10-1402,0,0.0143993,"t , the number of training examples reduces further. Hence, since we do not have many examples, we want the mapping procedure to have as much flexibility as possible, so we use multiple losses and regularization strategies, and then tune their relative strengths. 102 embedding for unseen words (Søgaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011), those that use character-level information (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015; Ballesteros et al., 2015), those using morphological and n-gram information (Candito and Crabb´e, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010; Bansal and Klein, 2011; Keller and Lapata, 2003), and hybrid approaches (Dyer et al., 2015; Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. Other work has also found improvements by combining pre-trained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies"
W16-1612,D13-1170,0,0.0124659,"Missing"
W16-1612,C12-2114,0,0.466507,"stract is a great deal of work on updating embeddings during supervised training to make them more task-specific (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014). These tasktrained embeddings have shown encouraging results but raise some concerns: (1) the updated embeddings of infrequent words are prone to overfitting, and (2) many words in the test data are not contained in the training data at all. In the latter case, at test time, systems either use a single, generic embedding for all unseen words or use their initial embeddings (typically derived from unlabelled data) (Søgaard and Johannsen, 2012; Collobert et al., 2011). Neither choice is ideal: A single unknown embedding conflates many words, while the initial embeddings may be in a space that is not comparable to the trained embedding space. In this paper, we address both concerns by learning to map from the initial embedding space to the task-trained space. We train a neural network mapping function that takes initial word embeddings and maps them to task-specific embeddings that are trained for the given task, via a multi-loss objective function. We tune the mapper’s hyperparameters to optimize performance on each domain of inter"
W16-1612,S15-1021,0,0.028312,"sentiment analysis. 1 Karen Livescu† Introduction Performance on NLP tasks drops significantly when moving from training sets to held-out data (Petrov et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al., 2014), there 2 Mapping Unseen Representations Let V = {w1 , . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi computed from this corpus. The initial embeddings are typically learned in an unsupervised 100 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistic"
W16-1612,P15-1150,0,0.034653,"Missing"
W16-1612,D14-1162,0,0.086068,"he effect of the mapping of unseen words, showing statistically significant improvements on both parsing and a downstream task (sentiment analysis). cance, we use a bootstrap test (Efron and Tibshirani, 1986) with 100K samples. 4 Web Treebank We expect our mapper to be most effective when parsing held-out data with many unseen words. This often happens when the held-out data is drawn from a different distribution than the training data. For example, when training a parser on newswire and testing on web data, 4.2 Pre-Trained Word Embeddings We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.2 4.3 Datasets We consider a number of datasets with varying rates of OOTV words. We define the OOTV rate (or, equivalently, the unseen rate) of a dataset as the percentage of the vocabulary (types) of words occurring in the set that were not seen in training. Wall Street Journal (WSJ) and OntoNotes-WSJ We conduct experiments on the Wall Street Journal portion of the English Penn Treebank dataset (Marcus et al., 1993). We follow the standard splits: sections 2-21 for training,"
W16-1612,P10-1040,0,0.0512462,"ren Livescu† Introduction Performance on NLP tasks drops significantly when moving from training sets to held-out data (Petrov et al., 2010). One cause of this drop is words that do not appear in the training data but appear in test data, whether in the same domain or in a new domain. We refer to such out-of-trainingvocabulary (OOTV) words as unseen words. NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can trigger a cascade of errors in the sentence. Word embeddings can counter the effects of limited training data (Necsulescu et al., 2015; Turian et al., 2010; Collobert et al., 2011). While the effectiveness of pretrained embeddings can be heavily task-dependent (Bansal et al., 2014), there 2 Mapping Unseen Representations Let V = {w1 , . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi computed from this corpus. The initial embeddings are typically learned in an unsupervised 100 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 100–110, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Model Parameters W"
W16-1612,D14-1122,0,0.0540584,"Missing"
W17-4504,S14-1010,0,0.166094,"on inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006). The SNLI corpus Bowman et al. (2015) allows training accurate end-to-end neural networks for this task. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of textual entailment recognition for redundancy detection in summarization. They label relationships between sentences, so as to select the most informative and non-redundant sentences for summarization, via sentence connectivity and graphbased optimization and fusion. Our focus, on the other hand, is entailment generation and not recognition, i.e., to teach summarization models the general natural language inference skill of generating a compressed sentence that logically entails the original longer sentence, so as to produce more effective short summaries. We achieve t"
W17-4504,A00-1043,0,0.032936,"he domain of the summarization datasets (general news), which suggests that the model is learning certain domain-independent inference skills. Our next steps to this workshop paper include incorporating stronger pointer-based models and employing the new multi-domain entailment corpus (Williams et al., 2017). 2 Related Work Earlier summarization work focused more towards extractive (and compression) based summarization, i.e., selecting which sentences to keep vs discard, and also compressing based on choosing grammatically correct sub-sentences having the most important pieces of information (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2008; Filippova et al., 2015). Bigger datasets and neural models have allowed the addressing of the complex reasoning involved in abstractive summarization, i.e., rewriting and compressing the input document into a new summary. Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Recognizing textual entailment (RTE) is the cl"
W17-4504,D15-1075,0,0.10317,"and compressing the input document into a new summary. Several advances have been made in this direction using machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006). The SNLI corpus Bowman et al. (2015) allows training accurate end-to-end neural networks for this task. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of textual entailment recognition for redundancy detection in summarization. They label relationships between sentences, so as to select the most informative and non-redundant sentences for summarization, via sentence connectivity and graphbased optimization and fusion. Our focus, on the other hand, is entailment generation and not recognition, i.e., to teach summarization models the general natural language inference skill of generating a compre"
W17-4504,D15-1166,0,0.124118,"Missing"
W17-4504,W13-2117,0,0.239522,"ing machine translation inspired encoder-aligner-decoder models, convolution-based encoders, switching pointer and copy mechanisms, and hierarchical attention models (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Recognizing textual entailment (RTE) is the classification task of predicting whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (Dagan et al., 2006). The SNLI corpus Bowman et al. (2015) allows training accurate end-to-end neural networks for this task. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has explored the use of textual entailment recognition for redundancy detection in summarization. They label relationships between sentences, so as to select the most informative and non-redundant sentences for summarization, via sentence connectivity and graphbased optimization and fusion. Our focus, on the other hand, is entailment generation and not recognition, i.e., to teach summarization models the general natural language inference skill of generating a compressed sentence that logically entails the original longer sentence, so as to produce more effective short su"
W17-4504,N16-1012,0,0.15465,"informative and non-redundant sentences for summarization, via sentence connectivity and graphbased optimization and fusion. Our focus, on the other hand, is entailment generation and not recognition, i.e., to teach summarization models the general natural language inference skill of generating a compressed sentence that logically entails the original longer sentence, so as to produce more effective short summaries. We achieve this via 3 Models First, we discuss our baseline model which is similar to the machine translation encoder-alignerdecoder model of Luong et al. (2015), and presented by Chopra et al. (2016). Next, we introduce our multi-task learning approach of sharing the parameters between abstractive summarization and entailment generation models. 3.1 Baseline Model Our baseline model is a strong, multi-layered encoder-attention-decoder model with bilinear attention, similar to Luong et al. (2015) and following the details in Chopra et al. (2016). Here, we encode the source document with a two-layered LSTM-RNN and generate the summary using another two-layered LSTM-RNN decoder. The word probability distribution at time step t of the decoder is defined as follows: p(wt |w&lt;t , ct , st ) = sof"
W17-4504,K16-1028,0,0.433171,"ImporIntroduction Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Despite these promising recent improvements, 27 Proceedings of the Workshop on New Frontiers in Summarization, pages 27–32 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics multi-task learning with entailment generation. Multi-task learning involves sharing parameters between related tasks, whereby each task benefits from extra information in the training signals of the related tasks, and also improves its generalization performance. Luong et al. (2016) showed improvements on translation, captioning, and parsing in a shared multi-task"
W17-4504,P02-1040,0,0.0973781,"Missing"
W17-4504,P17-1117,1,0.248925,"Despite these promising recent improvements, 27 Proceedings of the Workshop on New Frontiers in Summarization, pages 27–32 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics multi-task learning with entailment generation. Multi-task learning involves sharing parameters between related tasks, whereby each task benefits from extra information in the training signals of the related tasks, and also improves its generalization performance. Luong et al. (2016) showed improvements on translation, captioning, and parsing in a shared multi-task setting. Recently, Pasunuru and Bansal (2017) extend this idea to video captioning with two related tasks: video completion and entailment generation. We demonstrate that abstractive text summarization models can also be improved by sharing parameters with an entailment generation task. tantly, these improvements are achieved despite the fact that the domain of the entailment dataset (image captions) is substantially different from the domain of the summarization datasets (general news), which suggests that the model is learning certain domain-independent inference skills. Our next steps to this workshop paper include incorporating stron"
W17-4504,D15-1044,0,0.773442,"nly setting of DUC. ImporIntroduction Abstractive summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Despite these promising recent improvements, 27 Proceedings of the Workshop on New Frontiers in Summarization, pages 27–32 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics multi-task learning with entailment generation. Multi-task learning involves sharing parameters between related tasks, whereby each task benefits from extra information in the training signals of the related tasks, and also improves its generalization performance. Luong et al. (2016) showed improvements on translation, captioning, and parsin"
W17-4504,P17-1099,0,0.237317,"active summarization, the task of rewriting a document into a short summary is a significantly more challenging (and natural) task than extractive summarization, which only involves choosing which sentence from the original document to keep or discard in the output summary. Neural sequence-to-sequence models have led to substantial improvements on this task of abstractive summarization, via machine translation inspired encoder-aligner-decoder approaches, further enhanced via convolutional encoders, pointer-copy mechanisms, and hierarchical attention (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). Despite these promising recent improvements, 27 Proceedings of the Workshop on New Frontiers in Summarization, pages 27–32 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics multi-task learning with entailment generation. Multi-task learning involves sharing parameters between related tasks, whereby each task benefits from extra information in the training signals of the related tasks, and also improves its generalization performance. Luong et al. (2016) showed improvements on translation, captioning, and parsing in a shared multi-task setting. Recently,"
W17-4504,P15-1001,0,\N,Missing
W17-4504,W14-3348,0,\N,Missing
W17-4504,W04-1013,0,\N,Missing
W17-4504,D15-1042,0,\N,Missing
W17-5308,W17-5301,0,0.180823,"ence. Our encoder is based on stacked bidirectional LSTM-RNNs with shortcut connections and fine-tuning of word embeddings. The overall supervised model uses the above encoder to encode two input sentences into two vectors, and then uses a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural. Our ShortcutStacked sentence encoders achieve strong improvements over existing encoders on matched and mismatched multi-domain natural language inference (top singlemodel result in the EMNLP RepEval 2017 Shared Task (Nangia et al., 2017)). Moreover, they achieve the new state-of-theart encoding result on the original SNLI dataset (Bowman et al., 2015). 1 Introduction and Background Natural language inference (NLI) or recognizing textual entailment (RTE) is a fundamental semantic task in the field of natural language processing. The problem is to determine whether a given hypothesis sentence can be logically inferred from a given premise sentence. Recently released datasets such as the Stanford Natural Language Inference Corpus (Bowman et al., 2015) (SNLI) and the Multi-Genre Natural Language Inference Corpus (Williams et al.,"
W17-5308,D14-1162,0,0.0900545,"s are based on a smaller model with a 128+256 2-layer biLSTM). Hence, all our models were trained with word embeddings being fine-tuned. The last ablation in Table 4 shows that a classifier with two layers of relu is preferable than other options. Thus, we use that setting for our strongest encoder. mization with 32 batch size. The starting learning rate is 0.0002 with half decay every two epochs. The number of hidden units for MLP in classifier is 1600. Dropout layer is also applied on the output of each layer of MLP, with dropout rate set to 0.1. We used pre-trained 300D Glove 840B vectors (Pennington et al., 2014) to initialize the word embeddings. Tuning decisions for word embedding training strategy, the hyperparameters of dimension and number of layers for biLSTM, and the activation type and number of layers for MLP, are all explained in Section 4. 4 4.1 Results and Analysis Ablation Analysis Results We now investigate the effectiveness of each of the enhancement components in our overall model. These ablation results are shown in Tables 1, 2, 3 and 4, all based on the Multi-NLI development sets. Finally, Table 5 shows results for different encoders on SNLI and Multi-NLI test sets. First, Table 1 sh"
W17-5308,D15-1075,0,0.366923,", such as Tree-based CNN encoders (TBCNN) in Mou et al. (2015) or Stack-augmented Parser-Interpreter Neural Network (SPINN) in Bowman et al. (2016), and (2) joint, pairwise models that use cross-features between the two sentences to encode them, such as the Enhanced Sequential Inference Model (ESIM) in Chen et al. (2017) or the bilateral multiperspective matching (BiMPM) model Wang et al. (2017). Moreover, common sentence encoders can again be classified into tree-based encoders such as SPINN in Bowman et al. (2016) which we mentioned before, or sequential encoders such as the biLSTM model by Bowman et al. (2015). In this paper, we follow the former approach of encoding-based models, and propose a novel yet simple sequential sentence encoder for the MultiNLI problem. Our encoder does not require any syntactic information of the sentence. It also does not contain any attention or memory structure. It is basically a stacked (multi-layered) bidirectional LSTM-RNN with shortcut connections (feeding all previous layers’ outputs and word embeddings to each layer) and word embedding fine-tuning. The overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, a"
W17-5308,P16-1139,0,0.126884,"roposed models can be categorized into two groups: (1) encoding-based models (or sentence encoders), such as Tree-based CNN encoders (TBCNN) in Mou et al. (2015) or Stack-augmented Parser-Interpreter Neural Network (SPINN) in Bowman et al. (2016), and (2) joint, pairwise models that use cross-features between the two sentences to encode them, such as the Enhanced Sequential Inference Model (ESIM) in Chen et al. (2017) or the bilateral multiperspective matching (BiMPM) model Wang et al. (2017). Moreover, common sentence encoders can again be classified into tree-based encoders such as SPINN in Bowman et al. (2016) which we mentioned before, or sequential encoders such as the biLSTM model by Bowman et al. (2015). In this paper, we follow the former approach of encoding-based models, and propose a novel yet simple sequential sentence encoder for the MultiNLI problem. Our encoder does not require any syntactic information of the sentence. It also does not contain any attention or memory structure. It is basically a stacked (multi-layered) bidirectional LSTM-RNN with shortcut connections (feeding all previous layers’ outputs and word embeddings to each layer) and word embedding fine-tuning. The overall sup"
W17-5308,P17-1152,0,0.194423,"ixin1, mbansal}@cs.unc.edu Abstract Depending on whether a model will first encode a sentence into a fixed-length vector without any incorporating information from the other sentence, the several proposed models can be categorized into two groups: (1) encoding-based models (or sentence encoders), such as Tree-based CNN encoders (TBCNN) in Mou et al. (2015) or Stack-augmented Parser-Interpreter Neural Network (SPINN) in Bowman et al. (2016), and (2) joint, pairwise models that use cross-features between the two sentences to encode them, such as the Enhanced Sequential Inference Model (ESIM) in Chen et al. (2017) or the bilateral multiperspective matching (BiMPM) model Wang et al. (2017). Moreover, common sentence encoders can again be classified into tree-based encoders such as SPINN in Bowman et al. (2016) which we mentioned before, or sequential encoders such as the biLSTM model by Bowman et al. (2015). In this paper, we follow the former approach of encoding-based models, and propose a novel yet simple sequential sentence encoder for the MultiNLI problem. Our encoder does not require any syntactic information of the sentence. It also does not contain any attention or memory structure. It is basica"
W17-5308,D17-1070,0,0.394529,"sentence. It also does not contain any attention or memory structure. It is basically a stacked (multi-layered) bidirectional LSTM-RNN with shortcut connections (feeding all previous layers’ outputs and word embeddings to each layer) and word embedding fine-tuning. The overall supervised model uses these shortcutstacked encoders to encode two input sentences into two vectors, and then we use a classifier over the vector combination to label the relationship between these two sentences as that of entailment, contradiction, or neural (similar to the classifier setup of Bowman et al. (2015) and Conneau et al. (2017)). Our simple shortcut-stacked encoders achieve strong improvements over existing encoders due to its multi-layered and shortcutconnected properties, on both matched and misWe present a simple sequential sentence encoder for multi-domain natural language inference. Our encoder is based on stacked bidirectional LSTM-RNNs with shortcut connections and fine-tuning of word embeddings. The overall supervised model uses the above encoder to encode two input sentences into two vectors, and then uses a classifier over the vector combination to label the relationship between these two sentences as that"
