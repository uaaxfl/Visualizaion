1996.amta-1.8,E89-1001,0,0.0605546,"nstraints is performed in a single unification step during the parsing process, allowing simultaneous feedback from syntax and lexical semantics on possible linguistic structures. 3 Translating with Synchronous TAGs The effectiveness of using semantic features in an FB-LTAG is most clearly illustrated by looking at translation examples which heretofore seemed to necessitate interlingua solutions. FB-LTAGs can be applied to MT using the Synchronous Tree-Adjoining Grammar (STAG) formalism, which is uniquely suited to combining the strengths of both the transfer method and the interlingua method [2], [1], [21, 22], [15]. In STAG-based MT, LTAGs are developed independently for source and target languages. Lexicalized trees of the source language grammar are put in bi-directional correspondence with target language trees. The substitution and adjunction sites on the source trees are linked to substitution and adjunction sites on the target trees. By linking the source-language syntactic site for a verb's argument with the target-language site for the same argument, variations between languages in syntactic argument realization can be handled perspicuously. In addition to the simple linking"
1996.amta-1.8,1994.amta-1.7,0,0.0119784,"ng events depending on the characteristics of the objects involved. Da-duan is used for objects shaped like line segments, da-po for objects that break into irregularly shaped pieces, and da-sui for objects that break into many small pieces, [17]. These features can be associated with the correct expressions in the Mandarin FBLTAG, without needing to make the same distinctions in other languages. When attempting translations, it is only necessary to pinpoint the range of possible translation, and the selectional restriction features will ensure that inappropriate combinations are not produced [8], [11]. Semantic features may also be used to represent verb-class membership. For instance, change-of-state verbs such as dry can be marked by a feature in the lexicon.1 Selectional restrictions may be generalized over entire verb classes by specifying feature-structure equations using the verb-class membership features. Also, such features may be used to constrain or facilitate adjunctions in trees anchored by verbs in a particular class. For instance, intransitive mannerof-motion verbs such as walk and float can readily take path prepositional phrases such as to the shore, creating a direct"
1996.amta-1.8,C88-2121,0,0.0879982,"are generally thought of as wholly the domain of interlingua systems - without giving up any of the lexical specificity unique to transfer-based systems. In this way a machine translation system based on STAGs can respond with seamless flexibility to a wide spectrum of phenomena being presented for translation ranging from idioms and idiosyncratic lexical items to well-behaved verbs that follow lexical rules. 1 Introduction This paper describes the use of verb class memberships as a means of capturing generalizations about manner-of-motion verbs in Synchronous Tree Adjoining Grammars, STAGs, [20, 21, 22]. This approach allows STAGs, which are essentially transfer-based, to take advantage of the same types of generalizations which are generally thought of as wholly the domain of interlingua systems - without giving up any of the lexical specificity unique to transfer-based systems. In this way a machine translation system based on STAGs can respond with seamless flexibility to a wide spectrum of phenomena being presented for translation ranging from idioms and idiosyncratic lexical items to well-behaved verbs that follow lexical rules. This paper begins with a review of the role played by lexi"
1996.amta-1.8,C90-3045,0,0.0475737,"are generally thought of as wholly the domain of interlingua systems - without giving up any of the lexical specificity unique to transfer-based systems. In this way a machine translation system based on STAGs can respond with seamless flexibility to a wide spectrum of phenomena being presented for translation ranging from idioms and idiosyncratic lexical items to well-behaved verbs that follow lexical rules. 1 Introduction This paper describes the use of verb class memberships as a means of capturing generalizations about manner-of-motion verbs in Synchronous Tree Adjoining Grammars, STAGs, [20, 21, 22]. This approach allows STAGs, which are essentially transfer-based, to take advantage of the same types of generalizations which are generally thought of as wholly the domain of interlingua systems - without giving up any of the lexical specificity unique to transfer-based systems. In this way a machine translation system based on STAGs can respond with seamless flexibility to a wide spectrum of phenomena being presented for translation ranging from idioms and idiosyncratic lexical items to well-behaved verbs that follow lexical rules. This paper begins with a review of the role played by lexi"
1997.mtsummit-workshop.12,J94-4004,0,0.436237,"ant strides towards understanding why interlingua-based analyses are successful. • There are practical advantages to tying machine translation to surface-oriented representations as much as possible: the availability of large bilingual corpora has made the exploitation of stochastic approaches a crucial element in the practical success of MT, and such approaches are by nature oriented towards the surface form. This paper is structured as follows. In Section 2, we introduce some the standard interlingua-based analysis for a well-known case of “structural divergence”, a difficult MT challenges (Dorr, 1994). In Section 3, we present our MT system and its lexico-structural transfer formalism in particular. In Section 4, we discuss the semantic analysis that underlies our approach. In Section 5, we present a simple transposition of the interlingua approach to lexico-structural transfer. In Section 6, we show how we can eliminate the most unmotivated features of this analysis by using lexical functions, a language-internal device for relating lexemes. We conclude in Section 7. 91 2 Structural Divergences A classic problem in machine translation is the translation of motion verbs from English to Fre"
1997.mtsummit-workshop.12,J87-3006,0,\N,Missing
1997.mtsummit-workshop.2,W96-0306,0,0.026455,"object becomes separated into pieces. If the result is not achieved, there are no attempted breaking actions that can still be recognized. For the cut class of verbs, when there is an at in between the verb and its direct object, it qualifies the assumption of the goal state being achieved. The at has the same effect on the hit, push/pull, swat and poke classes, although it is not commonly found otherwise. 2.1 Ambiguities in Levin classes It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy [4], [6], [3]. However, it would be useful for the WordNet 12 synsets to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components. Identification of these components is critical to the use of classes and their semantic features for translation purposes, whether transfer-based or interlingua based. Although Levin classes group together verbs with similar argument structures, the meanings of the verbs are not necessarily synonymous. So"
1997.mtsummit-workshop.2,1997.mtsummit-workshop.12,1,\N,Missing
2003.mtsummit-papers.13,J00-1004,0,0.0221629,"form competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emerge. However, such approaches have been faced with the problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language, making use of the output of an automatic parser in one of the two parallel languages. This allows the model to make use of the syntactic information provided by treeba"
2003.mtsummit-papers.13,J90-2002,0,0.394016,"Missing"
2003.mtsummit-papers.13,J93-2003,0,0.0235932,"nce University of Pennsylvania Philadelphia, USA {yding, dgildea, mpalmer}@linc.cis.upenn.edu Abstract Structural divergence presents a challenge to the use of syntax in statistical machine translation. We address this problem with a new algorithm for alignment of loosely matched non-isomorphic dependency trees. The algorithm selectively relaxes the constraints of the two tree structures while keeping computational complexity polynomial in the length of the sentences. Experimentation with a large Chinese-English corpus shows an improvement in alignment results over the unstructured models of (Brown et al., 1993). 1 Introduction The statistical approach to machine translation, pioneered by (Brown et al., 1990, 1993), estimates word to word translation probabilities and sentence reordering probabilities directly from a large corpus of parallel sentences. Despite their lack of any internal representation of syntax or semantics, the ability of such systems to leverage large amounts of training data has enabled them to perform competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emer"
2003.mtsummit-papers.13,J94-4004,0,0.0597229,"eordering probabilities directly from a large corpus of parallel sentences. Despite their lack of any internal representation of syntax or semantics, the ability of such systems to leverage large amounts of training data has enabled them to perform competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emerge. However, such approaches have been faced with the problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations"
2003.mtsummit-papers.13,W02-1039,0,0.0523757,"Missing"
2003.mtsummit-papers.13,P00-1056,0,0.0522059,"translation model (TM). During the construction of a machine translation pipeline, the alignment problem is usually handled as part of the TM and P( f |e) = P ( f , a |e) , ∑ a where a is any possible alignment between e and f . This approach requires a generative translation model. However, when the alignment problem is viewed on its own, a generative model is not necessary. In other words, we can simply maximize P ( a |e, f ) using a conditional model. More straightforwardly, the alignment problem can be defined as Definition (1), which is equivalent to the alignment problem definition in (Och and Ney, 2000): Here, f j and ea j are words in the source and target language sentences f and e , respectively. 0 Definition (1) For each f j ∈ f , find a labeling ea j , where ea j ∈ e ∪ {0} stands for the “empty symbol”, which means f j could be aligned to nothing. This definition does not allow multiple English words being aligned to a same foreign language word. 2.2 Algorithm Outline We introduce the framework of the alignment algorithm by first looking at how the IBM models handle alignment. In Model 1, all connections for each foreign position are assumed to be equally likely, which implies that the"
2003.mtsummit-papers.13,J97-3002,0,0.125784,"syntax or semantics, the ability of such systems to leverage large amounts of training data has enabled them to perform competitively with more traditional interlingua based approaches. In recent years, hybrid approaches, which aim at applying statistical models to structural data, have begun to emerge. However, such approaches have been faced with the problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language, making use of the output of an automatic"
2003.mtsummit-papers.13,P01-1067,0,0.161015,"languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. Syntax-based statistical approaches to alignment began with (Wu, 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. Both these approaches learn the tree representations directly from parallel sentences, and do not make allowance for non-isomorphic structures. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language, making use of the output of an automatic parser in one of the two parallel languages. This allows the model to make use of the syntactic information provided by treebanks and the automatic parsers derived from them. While we would like to use syntactic information in both languages, the problem of non-isomorphism grows when trees in both languages are required to match. The use of probabilistic tree substitution grammars for tree-to-tree alignment (Hajic e"
2003.mtsummit-papers.13,P02-1039,0,0.0376038,"Missing"
2003.mtsummit-papers.13,J03-4003,0,\N,Missing
2003.mtsummit-papers.13,P03-1011,1,\N,Missing
2006.amta-papers.5,2003.mtsummit-papers.6,0,0.03402,"to the grammar learner the capability to handle more complicated root-unlexicalized ETs in the future, e.g. ((a) (big) (red) X ). One possible strength of the PSDIG system is that the dependency trees on both languages provide a richer set of features for stronger models to handle more sophisticated language phenomena, e.g. case consistency, number consistency, mechanical translation of numerical values, etc. We believe the system performance can be further improved by introducing other grammar learners and better quality control of the learned treelets. On the other hand, it is reported in (Charniak et al. 2003) that the Bleu evaluation metric tends to reward more local word choices rather than global accuracy. In our system, whether the incorporation of syntax for both source and target languages has provided additional advantages beyond what is measured by Bleu needs further investigation. Human (3 refs average) Bleu 27.7 32.9 37.0 Table 5. Oracles of merging the two systems on NIST 2003 Xinhua portion Comparing with single system scores, we conclude that PSDIG and Pharaoh each excel on different sentences. The oracle of merging the two systems points to the possibility of future work in combining"
2006.amta-papers.5,P05-1067,1,0.587422,"sions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars was proposed by (Shieber and Schabes, 1990). Eisner (2003) proposed viewing the MT as synchronous tree substitution grammar parsing. Melamed (2003) formalized the MT as synchronous parsing based on multi-text grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Lin (2004) proposed to base the MT process on parallel dependency paths. Ding and Palmer (2005) introduced the use of Probabilistic Synchronous Dependency Insertion Grammars (PSDIG) to model machine translation. These approaches model the two languages using tree transduction rules or synchronous grammars, possibly with multi-lemma elementary structures as atomic units. However, large scale implementation and competitive performance of the above mentioned methods are still a challenging task. And to the best of our knowledge, the advantages of syntax based Abstract As an approach to syntax based statistical machine translation (SMT), Probabilistic Synchronous Dependency Insertion Gramma"
2006.amta-papers.5,J94-4004,0,0.0141647,"Missing"
2006.amta-papers.5,P03-2041,0,0.0291985,"ing a syntactic tree into a string of the target language. (Quirck et al., 2005) used syntax structures to guide the phrase reorder process. When researchers try to use syntax trees (here we think of syntax as linguistic syntax, in contrast to formal syntax) in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars was proposed by (Shieber and Schabes, 1990). Eisner (2003) proposed viewing the MT as synchronous tree substitution grammar parsing. Melamed (2003) formalized the MT as synchronous parsing based on multi-text grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Lin (2004) proposed to base the MT process on parallel dependency paths. Ding and Palmer (2005) introduced the use of Probabilistic Synchronous Dependency Insertion Grammars (PSDIG) to model machine translation. These approaches model the two languages using tree transduction rules or synchronous grammars,"
2006.amta-papers.5,W02-1039,0,0.0172799,"current leading SMT system based on phrasal translation, Pharaoh (Koehn, 2004). The rest of this paper describes the system details as follows: Sections 2 sketches the concept of PSDIG. Section 3 describes two new algorithms to induce Synchronous Dependency Insertion Grammars from parallel corpora. Section 4 describes a new decoder which incorporates several translation models. We evaluate our system in section 5 with the Bleu metric (Papineni et al., 2002) and discuss the results in Section 6. 2 A Sketch of PSDIG The framework of using PSDIG for MT was introduced in (Ding and Palmer, 2005). Fox (2002) reports dependency representations have the best inter-lingual phrasal cohesion properties. Furthermore, dependency grammars have the advantage of simple formalism and CFG equivalent formal generative capacity. Being lexicalized, dependency grammars are friendly with probabilistic modeling. A monolingual Dependency Insertion Grammar (DIG) can be viewed as a tree substitution grammar defined on dependency trees (as opposed to phrasal structure trees). The basic units of the grammar are elementary trees (ET), which are subsentential dependency structures containing one or more lexical items. Th"
2006.amta-papers.5,P01-1030,0,0.0313739,"ve approach for a Chinese-English machine translation system. We used an automatic dependency parser (McDonald et al., 2005), trained using the Penn English/Chinese Treebanks. The training set consists of Xinhua newswire data from LDC and the FBIS data, total words: 7.4M English + 6.2M Chinese. The language model is trained using the Xinhua portion of the Gigaword corpus (30.0M words) with modified Kneser-Ney smoothing. The MT systems were evaluated using the n-gram based Bleu (Papineni et al., 2002), configured as case insensitive. We compared to two systems: IBM Model 4 (Brown et al., 1993, Germann et al., 2001) and phrased based SMT system Pharaoh (Koehn, 2004). Following (Och, 2003), we used the development test data from the 2001 NIST MT evaluation workshop to run error minimization training (206 sentences, 4 references each, 5945 words). We used the oracle score for the top 100 translations as the measure for the potential of possible discriminative training. The “oracle” translations are picked by comparing with the references. Model 4 13.1 PSDIG PSDIG Top-100 Oracle Pharaoh Merged As shown above, merging the rules learned by both grammar learners improved system performance. Interestingly, only"
2006.amta-papers.5,P03-1011,0,0.0324281,"entences. However identifying the proper level for the transduction is not an easy task. 2. The induction of a synchronous grammar is usually computationally expensive. The exhaustive search for all possible corresponding sub-sentential structures is NP-complete. 3. The problem is aggravated by the non-perfect training corpora. Loose translations are less of a problem for string based approaches than for approaches that require syntactic analysis. Hajic et al. (2002) limited non-isomorphism by n-to-m matching of nodes in the two trees. However, even by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees overconstrained the alignment problem, and achieved better results with a tree-to-string model. In a different approach, Hwa et al. (2002) aligned the parallel sentences using SMT models and projected the alignments back to the parse trees. The framework of PSDIG, while using trees on both languages, achieves flexible transduction of non-isomorphic trees by (1) relying on dependency structure, which is a deeper form of representation compared to phrasal structure trees; and (2) by allowing multi-lemma elementary trees. The linear time decoding algorithm in (Ding and"
2006.amta-papers.5,N04-1014,0,0.0181091,"x trees (here we think of syntax as linguistic syntax, in contrast to formal syntax) in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars was proposed by (Shieber and Schabes, 1990). Eisner (2003) proposed viewing the MT as synchronous tree substitution grammar parsing. Melamed (2003) formalized the MT as synchronous parsing based on multi-text grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Lin (2004) proposed to base the MT process on parallel dependency paths. Ding and Palmer (2005) introduced the use of Probabilistic Synchronous Dependency Insertion Grammars (PSDIG) to model machine translation. These approaches model the two languages using tree transduction rules or synchronous grammars, possibly with multi-lemma elementary structures as atomic units. However, large scale implementation and competitive performance of the above mentioned methods are still a challenging"
2006.amta-papers.5,C04-1090,0,0.0129684,"sm must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars was proposed by (Shieber and Schabes, 1990). Eisner (2003) proposed viewing the MT as synchronous tree substitution grammar parsing. Melamed (2003) formalized the MT as synchronous parsing based on multi-text grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Lin (2004) proposed to base the MT process on parallel dependency paths. Ding and Palmer (2005) introduced the use of Probabilistic Synchronous Dependency Insertion Grammars (PSDIG) to model machine translation. These approaches model the two languages using tree transduction rules or synchronous grammars, possibly with multi-lemma elementary structures as atomic units. However, large scale implementation and competitive performance of the above mentioned methods are still a challenging task. And to the best of our knowledge, the advantages of syntax based Abstract As an approach to syntax based statist"
2006.amta-papers.5,P02-1050,0,0.0132611,"ensive. The exhaustive search for all possible corresponding sub-sentential structures is NP-complete. 3. The problem is aggravated by the non-perfect training corpora. Loose translations are less of a problem for string based approaches than for approaches that require syntactic analysis. Hajic et al. (2002) limited non-isomorphism by n-to-m matching of nodes in the two trees. However, even by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees overconstrained the alignment problem, and achieved better results with a tree-to-string model. In a different approach, Hwa et al. (2002) aligned the parallel sentences using SMT models and projected the alignments back to the parse trees. The framework of PSDIG, while using trees on both languages, achieves flexible transduction of non-isomorphic trees by (1) relying on dependency structure, which is a deeper form of representation compared to phrasal structure trees; and (2) by allowing multi-lemma elementary trees. The linear time decoding algorithm in (Ding and Palmer, 2005) can be used as a starting point for more sophisticated decoding. In this paper, we examine various aspects of syntax based MT using PSDIG, such as gram"
2006.amta-papers.5,N03-1021,0,0.0216853,"ntax structures to guide the phrase reorder process. When researchers try to use syntax trees (here we think of syntax as linguistic syntax, in contrast to formal syntax) in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars was proposed by (Shieber and Schabes, 1990). Eisner (2003) proposed viewing the MT as synchronous tree substitution grammar parsing. Melamed (2003) formalized the MT as synchronous parsing based on multi-text grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Lin (2004) proposed to base the MT process on parallel dependency paths. Ding and Palmer (2005) introduced the use of Probabilistic Synchronous Dependency Insertion Grammars (PSDIG) to model machine translation. These approaches model the two languages using tree transduction rules or synchronous grammars, possibly with multi-lemma elementary structures as atomic units. However, large scale im"
2006.amta-papers.5,J04-4002,0,0.0266722,"likely to match” POS tag pairs. y Word translation probability: P( f j |ei ) zhu zai) …… y In the above example, the ET pairs corresponding to the string pairs below are learned as PSDIG rules: (I have been in Canada since 1947 -- wo 1947 nian yilai yizhi zhu zai jianada) (have been -- yizhi zhu) (I have been -- wo yizhi zhu) (since 1947 -- 1947 nian yilai) Whether the word pair is in word alignment. The first two heuristics are new in this paper. They are built using a word alignment table generated by bi-directional training of IBM Model 4 (Brown et al. 1993), using the method described in (Och and Ney, 2004). The alignments from models of both directions are intersected and diagonally grown and finalized (a.k.a. grow-diag-final). Suppose we have the word alignment as shown in Figure 4. Please be noted that since the align-grow-final method tends to align adjacent words diagonally, some alignments are not exact or not correct. We define the subtree that is rooted at the word In theory, by permitting n word pairs, 2n combinations are possible. Hence an exponentially large number of ET pairs may be learned. We prune the ET pairs using the following: y Any ET has a max size of sizemax , (currently si"
2006.amta-papers.5,P02-1040,0,0.0735607,"as grammar induction and decoding algorithms. We aim at having the overall MT system perform competitively with the pure statistical MT systems, especially the current leading SMT system based on phrasal translation, Pharaoh (Koehn, 2004). The rest of this paper describes the system details as follows: Sections 2 sketches the concept of PSDIG. Section 3 describes two new algorithms to induce Synchronous Dependency Insertion Grammars from parallel corpora. Section 4 describes a new decoder which incorporates several translation models. We evaluate our system in section 5 with the Bleu metric (Papineni et al., 2002) and discuss the results in Section 6. 2 A Sketch of PSDIG The framework of using PSDIG for MT was introduced in (Ding and Palmer, 2005). Fox (2002) reports dependency representations have the best inter-lingual phrasal cohesion properties. Furthermore, dependency grammars have the advantage of simple formalism and CFG equivalent formal generative capacity. Being lexicalized, dependency grammars are friendly with probabilistic modeling. A monolingual Dependency Insertion Grammar (DIG) can be viewed as a tree substitution grammar defined on dependency trees (as opposed to phrasal structure tree"
2006.amta-papers.5,J97-3002,0,0.0520555,"Missing"
2006.amta-papers.5,P01-1067,0,0.0981285,"nd Decoding for Syntax Based SMT Using PSDIG Yuan Ding Martha Palmer Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA {yding, mpalmer}@cis.upenn.edu tween languages (Dorr, 1994)，due to either systematic differences between languages or loose translations in real corpora, the syntax based MT systems have to transduce between non-isomorphic tree structures, which is a major challenge. (Wu, 1997) and (Alshawi et al., 2000) learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001) used a sequence of tree operations transforming a syntactic tree into a string of the target language. (Quirck et al., 2005) used syntax structures to guide the phrase reorder process. When researchers try to use syntax trees (here we think of syntax as linguistic syntax, in contrast to formal syntax) in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars"
2006.amta-papers.5,koen-2004-pharaoh,0,\N,Missing
2006.amta-papers.5,J93-2003,0,\N,Missing
2006.amta-papers.5,J03-4003,0,\N,Missing
2006.amta-papers.5,P05-1012,0,\N,Missing
2006.amta-papers.5,P05-1034,0,\N,Missing
2006.amta-papers.5,P02-1038,0,\N,Missing
2006.amta-papers.5,J08-3004,0,\N,Missing
2006.amta-papers.5,P03-1021,0,\N,Missing
2010.amta-papers.15,P98-1013,0,0.078272,"ibit Figure 3: Taxonomy of {appear, occur, emerge, exhibit} By merging ‘appear.2’ and ‘appear.5’, ‘appear.2,5’ becomes the least common hypernym of occur and emerge. However, according to WordNet, there is no link between exhibit and ‘appear.2,5’ so that exhibit is not connected to the rest. Thus, we end up having two semantic classes for this set, ‘appear.2,5’ and ‘exhibit’. By using the taxonomy, we can evaluate how well each set of English verbs corresponds to WordNet semantic relations. There already exist English lexical databases such as WordNet, VerbNet (Kipper et al., 2006), FrameNet (Baker et al., 1998), but this approach suggests an automatic way of deriving new semantic classes and validating and extending preexisting classes, which can be applied to other languages. Verifying that English semantic classes can be derived correctly using a parallel corpus gives us an idea about how well this approach may work on other languages that do not already have similar lexical databases. 5 Experiments We used only the SPM method on the Xinhua English-Chinese parallel corpus to derive semantic classes in English and Chinese, since the SPM method performed better than the GIZA++ mapping method on both"
2010.amta-papers.15,W09-3020,1,0.631565,"idea of semantic similarity based on triangulation between parallel corpora outlined in Resnik (2004) and Madnani et al. (2008a; 2008b), but is implemented here quite differently. It is most similar in execution to the work of (Mareˇcek, 2009b), which improves word alignment by aligning tectogrammatical trees in a parallel English/Czech corpus. The Czech corpus is first lemmatized because of the rich morphology, and then the word alignment is “symmetrized”. However, this approach does not explicitly make use of the predicate-argument structure to confirm the alignments or to suggest new ones. Choi et al. (2009) showed how to enhance Chinese-English word alignments by exploring predicate-argument structure alignment using parallel PropBanks. They used the ‘English Chinese Translation Treebank’ (ECTB) for their experiments, the same corpus we use. The resulting system showed improvement over pure GIZA++ alignment, though it only took advantage of Chinese to English GIZA++ output and required careful tuning of a number of threshold parameters to balance between precision and recall. Fung et al. (2007) demonstrated that there is poor semantic parallelism betwen Chinese-English bilingual sentences. Their"
2010.amta-papers.15,D09-1076,0,0.0307221,"For a set of Chinese verbs aligned to an English verb we manually checked semantic similarity between the Chinese verbs within a set. Our results show that the verb sets we generated have a high correlation with semantic classes. This could potentially lead to an automatic technique for generating semantic classes for verbs. 1 Introduction This paper discusses attempts to use alignments between English and Chinese predicate-argument structures in a parallel PropBanked corpus1 as a basis for determining cross-lingual semantic similarity. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment 1 PropBank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate (Palmer et al., 2005). has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent al"
2010.amta-papers.15,2007.tmi-papers.10,0,0.130691,"licitly make use of the predicate-argument structure to confirm the alignments or to suggest new ones. Choi et al. (2009) showed how to enhance Chinese-English word alignments by exploring predicate-argument structure alignment using parallel PropBanks. They used the ‘English Chinese Translation Treebank’ (ECTB) for their experiments, the same corpus we use. The resulting system showed improvement over pure GIZA++ alignment, though it only took advantage of Chinese to English GIZA++ output and required careful tuning of a number of threshold parameters to balance between precision and recall. Fung et al. (2007) demonstrated that there is poor semantic parallelism betwen Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of employing features from automatic syntactic parses of the Chinese and English sentences, word alignment with a bilingual lexicon, and tuning on an unannotated parallel corpus. Later, Wu and Fung (2009) used parallel PropBanks to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs using the predicate-"
2010.amta-papers.15,W10-1810,1,0.855266,"Missing"
2010.amta-papers.15,kipper-etal-2006-extending,1,0.844183,".5 appear.2 occur.3 emerge.1 exhibit Figure 3: Taxonomy of {appear, occur, emerge, exhibit} By merging ‘appear.2’ and ‘appear.5’, ‘appear.2,5’ becomes the least common hypernym of occur and emerge. However, according to WordNet, there is no link between exhibit and ‘appear.2,5’ so that exhibit is not connected to the rest. Thus, we end up having two semantic classes for this set, ‘appear.2,5’ and ‘exhibit’. By using the taxonomy, we can evaluate how well each set of English verbs corresponds to WordNet semantic relations. There already exist English lexical databases such as WordNet, VerbNet (Kipper et al., 2006), FrameNet (Baker et al., 1998), but this approach suggests an automatic way of deriving new semantic classes and validating and extending preexisting classes, which can be applied to other languages. Verifying that English semantic classes can be derived correctly using a parallel corpus gives us an idea about how well this approach may work on other languages that do not already have similar lexical databases. 5 Experiments We used only the SPM method on the Xinhua English-Chinese parallel corpus to derive semantic classes in English and Chinese, since the SPM method performed better than th"
2010.amta-papers.15,2008.amta-papers.13,0,0.485261,"Missing"
2010.amta-papers.15,J03-1002,0,0.00466096,"Missing"
2010.amta-papers.15,J05-1004,1,0.341062,"ed have a high correlation with semantic classes. This could potentially lead to an automatic technique for generating semantic classes for verbs. 1 Introduction This paper discusses attempts to use alignments between English and Chinese predicate-argument structures in a parallel PropBanked corpus1 as a basis for determining cross-lingual semantic similarity. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment 1 PropBank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate (Palmer et al., 2005). has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent alignment across sentences. We begin by running GIZA++ (Och and Ney, 2003), one of the most popular alignment tools, to obtain automatic word alignments between the parallel Engl"
2010.amta-papers.15,N09-2004,0,0.0897596,"+ alignment, though it only took advantage of Chinese to English GIZA++ output and required careful tuning of a number of threshold parameters to balance between precision and recall. Fung et al. (2007) demonstrated that there is poor semantic parallelism betwen Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of employing features from automatic syntactic parses of the Chinese and English sentences, word alignment with a bilingual lexicon, and tuning on an unannotated parallel corpus. Later, Wu and Fung (2009) used parallel PropBanks to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs using the predicate-argument structure annotated in parallel PropBanks. Although their work is not directly related to ours, it shows how parallel PropBanks can be used to improve MT systems in general. 3 3.1 Symmetric predicate mapping Parallel Corpus We apply our predicate-argument mapping method to the ‘Engish Chinese Translation Treebank’ (ECTB), a parallel English-Chinese corpus. In addition to Treebank syntactic structure, the"
2010.amta-papers.15,2007.mtsummit-papers.71,0,0.176751,"Missing"
2010.amta-papers.15,C98-1013,0,\N,Missing
2010.amta-papers.15,P07-2045,0,\N,Missing
2010.amta-papers.15,J06-1003,0,\N,Missing
2019.lilt-17.1,W11-0132,0,0.396965,"t the elementary tree of the nominal specifies the number of arguments for the LVC. This seems to imply that the light verb does not contribute to the event predication. This is untrue because the elementary tree of the nominal is in fact, underspecified and requires the light verb to adjoin. The underspecification analysis separates the subcategorization information from the syntactic composition process, which avoids the additional steps of argument identification and deletion. Moreover, feature structures on both noun and light verb ensure that combinatorial constraints are also specified. Ahmed and Butt (2011) have suggested that the combinatory possibilities of N-V combinations in Hindi and Urdu are in part governed by the lexical semantic compatibility of the noun with the verb. SimiHindi Light Verb Constructions / 15 lar observations have been made for English (Barrett and Davis, 2003, North, 2005). Sulger and Vaidya (2014) found that nouns in Hindi preferentially occur with a particular light verb based on their ontological properties (extracted from Hindi WordNet(Bhattacharyya, 2010)). For example, the noun varsha ‘rain’ has the ontological node description in Hindi WordNet as ‘Natural State,S"
2019.lilt-17.1,ahmed-etal-2012-reference,0,0.0516395,"Missing"
2019.lilt-17.1,W00-2013,0,0.245235,"noun and light verb are combined via the adjunction and substitution operations. The elementary tree of the nominal is an initial tree in our analysis and it also chooses a syntactic structure that will realize all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an adjunct to the nominal’s basic structure and contributes only features. However, as it is a predicate, it is also a special type of auxiliary tree viz., a predicative auxiliary tree (Abeill´e and Rambow, 2000). The initial tree of the nominal is lacking a category specification (Han and Rambow, 2000). We use the label X, which projects to an XP (Figure 7). We also assume that each node is specified with the feature CAT which has values like V or N. The CAT=V feature-value is shown on the initial tree because the [CAT=N] feature-value is not realized unless the light verb composes with the elementary tree of the nominal. Figure 7 shows this underspecification for the nominal chorii, which has the category X. The feature clash at XP with the featurevalue for TENSE ensures that adjunction takes place at this node. The TENSE feature also captures the fact that the tree for the nominal is nei"
2019.lilt-17.1,W14-5501,1,0.837613,"lysis separates the subcategorization information from the syntactic composition process, which avoids the additional steps of argument identification and deletion. Moreover, feature structures on both noun and light verb ensure that combinatorial constraints are also specified. Ahmed and Butt (2011) have suggested that the combinatory possibilities of N-V combinations in Hindi and Urdu are in part governed by the lexical semantic compatibility of the noun with the verb. SimiHindi Light Verb Constructions / 15 lar observations have been made for English (Barrett and Davis, 2003, North, 2005). Sulger and Vaidya (2014) found that nouns in Hindi preferentially occur with a particular light verb based on their ontological properties (extracted from Hindi WordNet(Bhattacharyya, 2010)). For example, the noun varsha ‘rain’ has the ontological node description in Hindi WordNet as ‘Natural State,State,Noun’ and has a high likelihood of occurring with a light verb that is also marked for stativity e.g. the light verb hu- ‘become’. Note that the verb honaa ‘to be’ has two forms, the stative form hE ‘be’ and eventive form hu- ‘become’. Both may occur as light verbs in Hindi. If the ontological properties of noun and"
2019.lilt-17.1,W13-1018,1,0.844828,"operties, alternations and agreement facts for the LVC. The process of syntactic composition as described in this analysis specifies each alternation on both the predicating nominal and the light verb. This is possible using the two operations of adjunction and substitution with complex elementary tree structures and is in keeping with the TAG maxim of “complicate locally, simplify globally” (Bangalore and Joshi, 2010). From the point of view of TAG grammar extraction, the particular design of the elementary tree for the nominal maps directly to the subcategorization frames in Hindi PropBank (Vaidya et al., 2013). The nominal is also specified for all the arguments of the complex predicate in these frames, and this will directly help to determine the number of substitution nodes for the nominal’s elementary tree. Elementary trees extracted from existing noun frames will eliminate an extra rule−writing step to extract predicate heads and their arguHindi Light Verb Constructions / 25 ments from the Hindi Treebank. Instead, it would be possible to utilize hand−corrected frame files for elementary tree generation. These, in conjunction with phrase−structure conversion rules could be used to extract automa"
2019.lilt-17.1,C88-2147,0,0.304541,"that we use in this section. Tree-Adjoining Grammar (TAG) is a formal treerewriting system that is used to describe the syntax of natural languages (Joshi and Schabes, 1997). The primitive of a TAG grammar is an elementary tree, which is a fragment of a phrase structure tree labelled with both terminal and non-terminal nodes. The elementary trees are combined by the operations of substitution (where a terminal node is replaced with a new tree) or adjunction (where an internal node is split to add a new tree, see also Fig 4). The elementary trees in TAG can be enriched with feature structures (Vijay-Shanker and Joshi, 1988). These can capture linguistic descriptions in a more precise manner and also capture adjunction constraints. TAG with feature structures is also known as FTAG (Feature-structure based TAG). A TAG can also be lexicalized i.e. each elementary tree has at least one lexical item as one of its terminal nodes. Lexicalized TAG enhanced with feature structures is known as Lexicalized Featurebased Tree-Adjoining Grammar (LF-TAG). This has been used for developing computational grammars for English (XTAG-Group, 2001), French (Abeill´e and Candito, 2000) and Korean (Han et al., 2000). In our analysis, w"
2020.acl-main.744,P09-1004,0,0.0144486,"Missing"
2020.acl-main.744,D16-1102,0,0.0402497,"Missing"
2020.acl-main.744,D14-1159,1,0.890483,"Missing"
2020.acl-main.744,W05-0620,0,0.401078,"Missing"
2020.acl-main.744,N19-1423,0,0.0543557,"Missing"
2020.acl-main.744,N19-1244,0,0.068078,"e are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections, and helpful comments. We also acknowledge the support of NSF Cyberlearning1822877, SaTC-1801446, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, DARPA Communicating with Computers DARPA 15-18-CwC-FP032, HDTRA1-16-1-0002, and gifts from Google and NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expres"
2020.acl-main.744,D15-1112,0,0.202301,"Missing"
2020.acl-main.744,D09-1002,0,0.0359538,"Missing"
2020.acl-main.744,J12-1005,0,0.0227791,"Missing"
2020.acl-main.744,P15-2036,0,0.0219309,"almer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heuristics (e.g., Ouchi et al.,"
2020.acl-main.744,D19-1405,1,0.937894,"predictions satisfy our constraints. To teach a model to do so, we transform conditional statements into regularizers, such that during training, the model receives a penalty if the rule is not satisfied for an example.2 To soften logic, we use the conversions shown in Table 1 that combine the product and G¨odel tnorms. We use this combination because it offers cleaner derivatives make learning easier. A similar combination of t-norms was also used in prior work (Minervini and Riedel, 2018). Finally, we will transform the derived losses into log space to be consistent with cross-entropy loss. Li et al. (2019) outlines this relationship between the crossentropy loss and constraint-derived regularizers in more detail. Logic V i ai W i ai ¬a a→b G¨odel min (ai ) max (ai ) 1 − a –  Product Πai – 1 − a min 1, ab Table 1: Converting logical operations to differentiable forms. For literals inside of L(s) and R(s), we use the G¨odel t-norm. For the top-level conditional statement, we use the product t-norm. Operations not used this paper are marked as ‘–’. 2.3 Unique Core Roles (U ) Our first constraint captures the idea that, in a frame, there can be at most one core participant of a given type. Operati"
2020.acl-main.744,P19-1028,1,0.870218,"eural networks with contextual embeddings, there is still room for systematically introducing knowledge in the form of constraints, without sacrificing the benefits of end-to-end learning. Structured Losses Chang et al. (2012) and Ganchev et al. (2010) developed models for structured learning with declarative constraints. Our work is in the same spirit of training models that attempts to maintain output consistency. There are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections,"
2020.acl-main.744,2021.ccl-1.108,0,0.101755,"Missing"
2020.acl-main.744,D18-1538,0,0.0177717,"ut consistency. There are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections, and helpful comments. We also acknowledge the support of NSF Cyberlearning1822877, SaTC-1801446, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, DARPA Communicating with Computers DARPA 15-18-CwC-FP032, HDTRA1-16-1-0002, and gifts from Google and NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official polici"
2020.acl-main.744,P14-1111,0,0.0590495,"Missing"
2020.acl-main.744,K18-1007,0,0.101292,"th classification neurons, i.e., the predicted output probabilities are soft versions of these literals. What we want is that model predictions satisfy our constraints. To teach a model to do so, we transform conditional statements into regularizers, such that during training, the model receives a penalty if the rule is not satisfied for an example.2 To soften logic, we use the conversions shown in Table 1 that combine the product and G¨odel tnorms. We use this combination because it offers cleaner derivatives make learning easier. A similar combination of t-norms was also used in prior work (Minervini and Riedel, 2018). Finally, we will transform the derived losses into log space to be consistent with cross-entropy loss. Li et al. (2019) outlines this relationship between the crossentropy loss and constraint-derived regularizers in more detail. Logic V i ai W i ai ¬a a→b G¨odel min (ai ) max (ai ) 1 − a –  Product Πai – 1 − a min 1, ab Table 1: Converting logical operations to differentiable forms. For literals inside of L(s) and R(s), we use the G¨odel t-norm. For the top-level conditional statement, we use the product t-norm. Operations not used this paper are marked as ‘–’. 2.3 Unique Core Roles (U ) Ou"
2020.acl-main.744,P17-1044,0,0.37778,"l., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heuristics (e.g., Ouchi et al., 2018), or simple Viterbi decoding to ensure that token tags are BIO-consistent. By virtue of being constrained by the definition of the task, global inference promises semantically meaningful outputs, and could provide valuable signal when models are being trained. However, beyond Viterbi decoding, it may impose prohibitive computational costs, thus ruling out using inference during training. Indeed, optimal inference may be intractable, and inference-driven training may require ignoring certain constraints that render inference difficult. While global"
2020.acl-main.744,D18-1191,0,0.219448,"r et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heuristics (e.g., Ouchi et al., 2018), or simple Viterbi decoding to ensure that token tags are BIO-consistent. By virtue of being constrained by the definition of the task, global inference promises semantically meaningful outputs, and could provide valuable signal when models are being trained. However, beyond Viterbi decoding, it may impose prohibitive computational costs, thus ruling out using inference during training. Indeed, optimal inference may be intractable, and inference-driven training may require ignoring certain constraints that render inference difficult. While global inference was a mainstay of SRL models until r"
2020.acl-main.744,P18-1013,0,0.0254016,"s to maintain output consistency. There are some recent works on the design of models and loss functions by relaxing Boolean formulas. Kimmig et al. (2012) used the Łukasiewicz t-norm for probabilistic soft logic. Li and Srikumar (2019) augment the neural network architecture itself using such soft logic. Xu et al. (2018) present a general framework for loss design that does not rely on soft logic. Introducing extra regularization terms to a downstream task have been shown to be beneficial in terms of both output structure consistency and prediction accuracy (e.g., Minervini and Riedel, 2018; Hsu et al., 2018; Mehta et al., 2018; Du et al., 2019; Li et al., 2019). We thank members of the NLP group at the University of Utah for their valuable insights and suggestions; and reviewers for pointers to related works, corrections, and helpful comments. We also acknowledge the support of NSF Cyberlearning1822877, SaTC-1801446, U.S. DARPA KAIROS Program No. FA8750-19-2-1004, DARPA Communicating with Computers DARPA 15-18-CwC-FP032, HDTRA1-16-1-0002, and gifts from Google and NVIDIA. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing"
2020.acl-main.744,D08-1008,0,0.0249255,"Missing"
2020.acl-main.744,N06-1025,0,0.0908687,"tart with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints. Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios. 1 Introduction Semantic Role Labeling (SRL, Palmer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can infor"
2020.acl-main.744,W13-3516,0,0.0345854,"Missing"
2020.acl-main.744,W05-0639,1,0.72313,"os. 1 Introduction Semantic Role Labeling (SRL, Palmer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al"
2020.acl-main.744,J08-2005,0,0.247494,"and labels, we get the final regularizer LU (s): LU (s) = X l(u, i, X). (7) (u,i)∈s,X∈Acore Our constraint is universally applied to all words and predicates (i.e., i, u respectively) in the given sentence s. Whenever there is a pair of predicted labels for tokens i, j that violate the rule (6), our loss will yield a positive penalty. Error Measurement ρu To measure the violation rate of this constraint, we will report the percentages of propositions that have duplicate core arguments. We will refer to this error rate as ρu . 2.4 Exclusively Overlapping Roles (O) We adopt this constraint from Punyakanok et al. (2008) and related work. In any sentence, an argument for one predicate can either be contained in or entirely outside another argument for any other predicate. We illustrate the intuition of this constraint in Table 2, assuming core argument spans are unique and tags are BIO-consistent. Based on Table 2, we design a constraint that says: if an argument has boundary [i, j], then no other argument span can cross the boundary at j. 2 Constraint-derived regularizers are dependent on examples, but not necessarily labeled ones. For simplicity, in this paper, we work with sentences from the labeled corpus"
2020.acl-main.744,C04-1197,0,0.136416,"y mapping them to VerbNet, they can be disambiguated. Such mappings naturally define constraints that link semantic ontologies. Final words In this work, we have presented a framework that seeks to predict structurally consistent outputs without extensive model redesign, or any expensive decoding at prediction time. Our experiments on the semantic role labeling task show that such an approach can be especially helpful in scenarios where we do not have the luxury of massive annotated datasets. Constraints have long been a cornerstone in the SRL models. Several early linear models for SRL (e.g. Punyakanok et al., 2004, 2008; Surdeanu et al., 2007) modeled inference for PropBank SRL using integer linear programming. Riedel and MezaRuiz (2008) used Markov Logic Networks to learn and predict semantic roles with declarative constraints. The work of (T¨ackstr¨om et al., 2015) showed that certain SRL constraints admit efficient decoding, leading to a neural model that used this framework (FitzGerald et al., 2015). Learning with constraints has also been widely adopted in semisupervised SRL (e.g., F¨urstenau and Lapata, 2012). Acknowledgements With the increasing influence of neural networks in NLP, however, the"
2020.acl-main.744,W08-2125,0,0.101725,"Missing"
2020.acl-main.744,D18-1548,0,0.107757,"Missing"
2020.acl-main.744,C12-1161,0,0.060428,"Missing"
2020.acl-main.744,P05-1073,0,0.101023,"ic Role Labeling (SRL, Palmer et al., 2010) is the task of labeling semantic arguments of predicates in sentences to identify who does what to whom. Such representations can come in handy in tasks involving text understanding, such as coreference resolution (Ponzetto and Strube, 2006) and reading comprehension (e.g., Berant et al., 2014; Zhang et al., 2020). This paper focuses on the question of how knowledge can influence modern semantic role labeling models. Linguistic knowledge can help SRL models in several ways. For example, syntax can drive feature design (e.g., Punyakanok et al., 2005; Toutanova et al., 2005; Kshirsagar et al., 2015; Johansson and Nugues, 2008, and others), and can also be embedded into neural network architectures (Strubell et al., 2018). In addition to such influences on input representations, knowledge about the nature of semantic roles can inform structured decoding algorithms used to construct the outputs. The SRL literature is witness to a rich array of techniques for structured inference, including integer linear programs (e.g., Punyakanok et al., 2005, 2008), bespoke inference algorithms (e.g., T¨ackstr¨om et al., 2015), A* decoding (e.g., He et al., 2017), greedy heurist"
2020.acl-main.744,Q15-1003,0,\N,Missing
2020.louhi-1.12,araki-etal-2014-detecting,0,0.0186208,"cal, non-identical, or mereologically (part-whole) related. In keeping with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreemen"
2020.louhi-1.12,S17-2093,1,0.860183,"3 School of Information, University of Arizona, Tucson, AZ 4 Department of Computer Science, Loyola University Chicago, Chicago, IL 1 {first.last}@childrens.harvard.edu 2 {first.last}@colorado.edu 3 bethard@arizona.edu 4 ddligach@luc.edu 017 018 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 leading US medical center. This dataset has previously undergone a variety of annotation efforts, most notably temporal annotation (Styler IV et al., 2014). It has been part of several SemEval shared tasks such as Clinical TempEval (Bethard et al., 2017) where state-of-the-art results have been established. Our goal was to utilize this THYME corpus to enable the extraction of more extensive patient timelines by manually creating cross-document links that built off the pre-existing single file annotations. (Wright-Bettner et al., 2019) discuss that a subset of the THYME temporal annotations contributed to incompatible temporal inferences, thus reducing their ability to support meaningful temporal reasoning. Accuracy and informativeness of temporal relation gold annotations are essential for their effectiveness in training a system for temporal"
2020.louhi-1.12,P08-1090,0,0.0789589,"g with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreement While the gold intra-document CON-SUB relations enabled high cross-document"
2020.louhi-1.12,N19-1423,0,0.0715314,"(different narratives have different goals, which in turn influences meaning interpretation). This is discussed in detail in Section 4. We empirically found it essential to take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEP"
2020.louhi-1.12,W13-1203,0,0.191253,"r. ii. cancer CONTAINS tumor iii. cancer CONTAINS adenocarcinoma The merged THYME and coreference annotations2 for (1) and (2) were as follows:      January 17, 2009 CONTAINS CT CT CONTAINS metastases February 20, 2009 CONTAINS resected metastases OVERLAPS resected metastases IDENTICAL metastases Both sets of intra-document links are pragmatically appropriate. Discourse contexts can expand or reduce the level of granularity at which a sense is interpreted (Recasens et al., 2011; also see Hobbs, 1985). In Note A, the text supports a coarse-grained interpretation of adenocarcinoma, or what Hovy et al., 2013 term a “wide” reading; it refers generally to the patient’s cancer. Note B, however, requires a fine-grained (“narrow”) interpretation – adenocarcinoma here refers specifically to the new, inoperable tumor and is contrasted with the original, resected tumor. The quandary for the cross-document task lies in whether to link adenocarcinoma in A as IDENTICAL to adenocarcinoma in B. An IDENTICAL relation entails logical impossibilities: assuming we also link cancerA as IDENT to cancerB, the combined within- and crossdocument relations now say the recurrent adenocarcinoma temporally contains itself"
2020.louhi-1.12,2020.tacl-1.5,0,0.153352,"take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEPART) relations, which were later merged with the temporal annotations (Wright-Bettner et al., 2019). Temporal relations alone are insufficient for timeline extraction; core"
2020.louhi-1.12,P14-1094,0,0.029063,"these pre-existing within-note annotations by manually adding coreference and bridging links across each set of three notes. In the process, we discovered a subset of the original CONTAINS relations contributed to temporally-conflicting information, which led to the addition of two new TLINKs: NOTED-ON and CON-SUB (Wright-Bettner et al., 2019). We discuss below how these updates contribute to more accurate and comprehensive temporal relations which facilitated cross-document linking. As such, this is one of the few studies in clinical NLP for cross-document temporal relation annotations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV"
2020.louhi-1.12,L18-1558,0,0.035104,"Missing"
2020.louhi-1.12,W19-1908,1,0.922329,"e refinements of the THYME+ annotations. Splitting CONTAINS into CONTAINS and CON-SUB relations and OVERLAP into OVERLAP and NOTEDON relations leads to better learnability: CONTAINS goes from 0.664 F1 on THYME to 0.748 F1 on THYME+, and OVERLAP goes from 0.179 on THYME to 0.416 on THYME+. The best results for the new categories of CON-SUB and NOTED-ON are 0.072 F1 and 0.744 F1 respectively – results that establish baselines for these two new temporal relations. The performance on all types of relations for THYME+ is 0.625 F1 compared to 0.548 for THYME (Table 2, Overall column, rows 1 and 2). Lin et al., 2019 report 0.684 F1 for THYME CONTAINS, however the result is achieved when training on and evaluating for only the CONTAINS links, and augmenting the training data with automatically generated CONTAINS relations. Thus, it is not a fair comparison to use for the results reported in Table 2. Of the models beyond BioBERT that we explored, BART-large was the most successful. The result with BART-large was 0.748 F1 (Table 2, CONTAINS column, row 3). In general, certain pre-trained models, like BioBERT and BART, yield better results than the other models. BioBERT is pre-trained on biomedical text and"
2020.louhi-1.12,W16-5706,1,0.878583,"Missing"
2020.louhi-1.12,pustejovsky-etal-2010-iso,0,0.037864,"otations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV et al., 2014)1. To keep annotation manageable and circumvent massively inferential temporal linking, the THYME guidelines constrained TLINK creation to events within the same sentence or adjacent sentences, and specifically prohibited TLINKing across sections (these are clinically-delineated sections separated from each other by numerical section IDs – History of Present Illness is section 20103, Vital Signs is section 20110, etc.) Linguistic evidence for creating these TLINKs included local cues such as temporal 3 Refined Temporal Relation: NOTEDON The THYME guidelines specified t"
2020.louhi-1.12,D19-6201,1,0.888309,"Missing"
2020.lrec-1.601,W13-2322,1,0.910691,"d avatars in the space according to an absolute Cartesian coordinate system. Through a two-step process of sentence-level and document-level annotation designed to capture implicit information, we leverage these coordinates and bearings in the AMRs in combination with spatial framework annotation to ground the spatial language in the dialogues to absolute space. Keywords: AMR, Spatial Relations, PropBank, SRL, Semantic Roles, Minecraft, Frame of Reference, Annotation 1. Introduction This paper presents a spatial addendum to the Abstract Meaning Representation (AMR) semantic annotation schema (Banarescu et al., 2013) with accompanying spatial conceptualization, PropBank rolesets (Palmer et al., 2005), and grounded annotated corpus. AMR in its current form represents sentences as directed, rooted, acyclic graphs that capture core surface-level semantics while abstracting away from syntactic specificity. While AMR has not previously approached spatial semantics with much detail (Bonial et al., 2019), the implicit argument marking capability introduced by O’Gorman et al. (2018b) makes AMR an especially good fit for annotating spatial language. Spatial relations are not only semantically complex, they leverag"
2020.lrec-1.601,W19-0124,0,0.028056,"Missing"
2020.lrec-1.601,bonial-etal-2014-propbank,1,0.880309,"Missing"
2020.lrec-1.601,W16-5401,0,0.0219952,"y approached spatial semantics with much detail (Bonial et al., 2019), the implicit argument marking capability introduced by O’Gorman et al. (2018b) makes AMR an especially good fit for annotating spatial language. Spatial relations are not only semantically complex, they leverage pragmatically essential knowledge about the spatial characteristics of entities and their frameworks to map from linguistic expression onto space and from one spatial framework to another. This addendum takes the fine-grained spatial semantics and object grounding strategies of previous schemata (Pustejovsky, 2017; Gotou et al., 2016; Pustejovsky and Krishnaswamy, 2016; Tellex et al., 2011) and folds them into multi-sentence AMR (MS-AMR). The result is a comprehensive annotation tool that can handle finegrained explicit and implicit nested spatial relationships that are grounded in quantified space and merged fluidly with event dynamics. Because the spatial annotations are incorporated into the domain-general AMR graphs, this approach also captures information about how spatial relations are expressed in the context of whole sentences and overall discourse. The new set of tools and practices we present span single sentenc"
2020.lrec-1.601,kordjamshidi-etal-2010-spatial,0,0.0541264,"Missing"
2020.lrec-1.601,mani-etal-2008-spatialml,0,0.0601922,"c training data of this sort because it automatically tracks locations and orientations of entities in the environment according to an absolute coordinate system. The participants are immersed in the 3D space as they communicate, which means they have access to a wide range of spatial frameworks for their spatial reasoning strategies and FoR selection (Li and Gleitman, 2002). This range exceeds what is available from spatial description tasks and tasks that take place in twodimensions, which are common in current corpora (Kordjamshidi et al., 2010; Suhr et al., 2017; Johnson et al., 2016; and Mani et al., 2008). In each dialogue, participants collaborate in building one of a set of predesigned block structures. Some structures are abstract (i.e. intrinsically non-oriented objects), while other structures are representational models of animals, vehicles, letters, etc. (i.e. oriented objects). The variety of target structures used here elicit a range of FoR strategies. In each task, one participant plays the role of Builder and the other plays Architect. Only the Builder has an avatar in the space and can manipulate blocks. Only the Architect can see plans for the target structure. Conversation betwee"
2020.lrec-1.601,W19-3315,1,0.819732,"l., 2014; Bonial et al., 2015). A single roleset groups together etymologically related aliases represented as verbs, nouns, adjectives, light verb constructions, and other multi-word expressions (MWEs). AMRs are thus ambiguous for part of speech in their predicates, and a single AMR may accurately represent a variety of semantically similar sentences. AMRs are also ambiguous for tense, aspect, and discourse structure and are limited in their representation of quantification and scope, although expansions in these areas are currently underway (Pustejovsky et al., 2019; Donatelli et al., 2019; Myers and Palmer, 2019; Van Gysel et al., 2019; and Vigus et al., 2019). Spatial relations involving a directional difference in LOCATION between two entities se1 and se2 are annotated with the general frame relative-position: Numbered arguments that aren’t represented explicitly in a sentence are left unannotated during the single sentence pass. In the multi-sentence pass, they are brought back into the graphs and marked as potential implicit arguments, available for inclusion in co-reference identity chains and set/member or part/whole bridges. In this way, implicit and explicit information are brought together f"
2020.lrec-1.601,P19-1537,0,0.141798,"e dialogue. This dummy AMR defines specific spatial frameworks for each entity and the environment and describes how these frameworks map together. Accurately representing how frameworks map onto each other and onto language is an essential step in converting spatial language to language-independent spatial representations that are used by NLP systems downstream (see Dan et al., (2019) for discussion of downstream strategies). While Spatial AMR is intended to be adaptable to other environments, we present it here in the specific context of our corpus of Minecraft structure-building dialogues (Narayan-Chen et al., 2019) as an example of its specificity and range. This annotated corpus is being used to train a state-of-the-art semantic parser (Zhang et al., 2019), for which we present preliminary baseline results. 2. 2.1 AMR Single Sentence and Multi-sentence AMR AMR annotation now occurs in two passes, one for single sentence annotation and another for multi-sentence (O’Gorman et al., 2018b). During single sentence annotation, AMR depicts each sentence as a series of nested predicate argument structures (Banarescu et al., 2013). Predicate-specific argument structures come from PropBank (Palmer et al, 2005),"
2020.lrec-1.601,W17-2812,1,0.81483,"like add two 45° to what you just made isn’t annotatable with current tools. 3. The Minecraft Corpus The corpus we draw from consists of hundreds of dialogues and accompanying grounding data elicited during a collaborative human-to-human structure-building task in the 3D virtual environment of Minecraft. These dialogues are part of a larger project undertaken under the Communicating with Computers (CwC) DARPA grant, the goal of which is to create an automatic agent capable of communicating back and forth with a human participant while successfully carrying out real-world spatial instructions (Narayan-Chen et al., 2017; and Narayan-Chen et al., 2019). Minecraft using the Malmo platform (Johnson et al., 2016) provides a particularly rich setting for semantic training data of this sort because it automatically tracks locations and orientations of entities in the environment according to an absolute coordinate system. The participants are immersed in the 3D space as they communicate, which means they have access to a wide range of spatial frameworks for their spatial reasoning strategies and FoR selection (Li and Gleitman, 2002). This range exceeds what is available from spatial description tasks and tasks tha"
2020.lrec-1.601,C18-1313,1,0.901685,"Missing"
2020.lrec-1.601,J05-1004,1,0.468505,"wo-step process of sentence-level and document-level annotation designed to capture implicit information, we leverage these coordinates and bearings in the AMRs in combination with spatial framework annotation to ground the spatial language in the dialogues to absolute space. Keywords: AMR, Spatial Relations, PropBank, SRL, Semantic Roles, Minecraft, Frame of Reference, Annotation 1. Introduction This paper presents a spatial addendum to the Abstract Meaning Representation (AMR) semantic annotation schema (Banarescu et al., 2013) with accompanying spatial conceptualization, PropBank rolesets (Palmer et al., 2005), and grounded annotated corpus. AMR in its current form represents sentences as directed, rooted, acyclic graphs that capture core surface-level semantics while abstracting away from syntactic specificity. While AMR has not previously approached spatial semantics with much detail (Bonial et al., 2019), the implicit argument marking capability introduced by O’Gorman et al. (2018b) makes AMR an especially good fit for annotating spatial language. Spatial relations are not only semantically complex, they leverage pragmatically essential knowledge about the spatial characteristics of entities and"
2020.lrec-1.601,L16-1730,0,0.0259797,"semantics with much detail (Bonial et al., 2019), the implicit argument marking capability introduced by O’Gorman et al. (2018b) makes AMR an especially good fit for annotating spatial language. Spatial relations are not only semantically complex, they leverage pragmatically essential knowledge about the spatial characteristics of entities and their frameworks to map from linguistic expression onto space and from one spatial framework to another. This addendum takes the fine-grained spatial semantics and object grounding strategies of previous schemata (Pustejovsky, 2017; Gotou et al., 2016; Pustejovsky and Krishnaswamy, 2016; Tellex et al., 2011) and folds them into multi-sentence AMR (MS-AMR). The result is a comprehensive annotation tool that can handle finegrained explicit and implicit nested spatial relationships that are grounded in quantified space and merged fluidly with event dynamics. Because the spatial annotations are incorporated into the domain-general AMR graphs, this approach also captures information about how spatial relations are expressed in the context of whole sentences and overall discourse. The new set of tools and practices we present span single sentence and multi-sentence annotation. At"
2020.lrec-1.601,W19-3303,0,0.0107934,"of each role (O’Gorman et al., 2018a; Bonial et al., 2014; Bonial et al., 2015). A single roleset groups together etymologically related aliases represented as verbs, nouns, adjectives, light verb constructions, and other multi-word expressions (MWEs). AMRs are thus ambiguous for part of speech in their predicates, and a single AMR may accurately represent a variety of semantically similar sentences. AMRs are also ambiguous for tense, aspect, and discourse structure and are limited in their representation of quantification and scope, although expansions in these areas are currently underway (Pustejovsky et al., 2019; Donatelli et al., 2019; Myers and Palmer, 2019; Van Gysel et al., 2019; and Vigus et al., 2019). Spatial relations involving a directional difference in LOCATION between two entities se1 and se2 are annotated with the general frame relative-position: Numbered arguments that aren’t represented explicitly in a sentence are left unannotated during the single sentence pass. In the multi-sentence pass, they are brought back into the graphs and marked as potential implicit arguments, available for inclusion in co-reference identity chains and set/member or part/whole bridges. In this way, implicit"
2020.lrec-1.601,P17-2034,0,0.0280102,"vides a particularly rich setting for semantic training data of this sort because it automatically tracks locations and orientations of entities in the environment according to an absolute coordinate system. The participants are immersed in the 3D space as they communicate, which means they have access to a wide range of spatial frameworks for their spatial reasoning strategies and FoR selection (Li and Gleitman, 2002). This range exceeds what is available from spatial description tasks and tasks that take place in twodimensions, which are common in current corpora (Kordjamshidi et al., 2010; Suhr et al., 2017; Johnson et al., 2016; and Mani et al., 2008). In each dialogue, participants collaborate in building one of a set of predesigned block structures. Some structures are abstract (i.e. intrinsically non-oriented objects), while other structures are representational models of animals, vehicles, letters, etc. (i.e. oriented objects). The variety of target structures used here elicit a range of FoR strategies. In each task, one participant plays the role of Builder and the other plays Architect. Only the Builder has an avatar in the space and can manipulate blocks. Only the Architect can see plans"
2020.lrec-1.601,W19-3301,0,0.0328671,"Missing"
2020.lrec-1.601,W19-3321,0,0.0536422,"Missing"
2020.lrec-1.601,P19-1009,0,0.0498546,"Missing"
2020.lrec-1.717,W13-2322,1,0.75012,"retability and generalizability (Hu et al., 2017). Symbolic representations that can support reasoning capabilities are shown to have a critical role in improving both of the above-mentioned aspects (Goldman et al., 2018; Krishnaswamy et al., 2019; Suhr et al., 2017b). A robust intermediate representation can help an off-the-shelf planner to ground a symbolic representation of the spatial concepts to a final executable output. This representation should be as general and domain independent as possible to eliminate the need to retrain an automatic annotation system for a new domain. Using AMR (Banarescu et al., 2013) as a stepping stone to our spatial configurations helps to ensure generalizability and domain independence. Our goal is to identify 5855 Figure 1: An instance of the collaborative building task. The last instruction was : Now add 3 red bricks on the diagonal adjacent to the yellow bricks. Figure 2: An example from the NLVR corpus that demonstrates spatial focus shift from the black item to the yellow item. various spatial semantic aspects that are required by a concrete meaning representation for downstream tasks. 2. Expanded AMRs In this section, we briefly describe the spatial AMR extension"
2020.lrec-1.717,J81-4005,0,0.621277,"Missing"
2020.lrec-1.717,P02-1031,1,0.287852,"ships and spatial object properties from the AMR and organize them into an easily interpretable format that maintains the complex relationships and nesting. The corpus that we describe here 1 includes annotation on over 5000 dialogue sentences (170 full dialogues) that discuss collaborative construction events in the Minecraft dataset (Narayan-Chen et al., 2019). We have an additional 7600 annotated automatically-generated sentences representing builder actions, giving a total of 12,600 spatial AMRs. The AMR expansion involves the addition of 150+ new or updated PropBank (Palmer et al., 2005; Gildea and Palmer, 2002) rolesets as well as a dozen general semantic frames, roles, and concepts that signal spatial relation1 https://github.com/jbonn/CwC-Minecraft-AMR ships not previously covered by AMR. Using rolesets to annotate spatial relations allows us to disambiguate senses and to group together etymologically-related relations from different parts of speech within those senses. For example, the roleset align-02 includes aliases align-v, alignedj, line-up-v, and in-line-p, and down-03 includes aliases down-p, down-r, downward-r, etc.. Prepositional and adverbial aliases are new for PropBank and AMR. The ro"
2020.lrec-1.717,P18-1168,0,0.0144952,"d spatial AMR framework (Bonn et al., 2019). Our spatial configuration scheme takes the spatial roles and relations identified in spatial AMR graphs and converts them into an easy to read general framework, resulting in a maximally expressive spatial meaning representation language. Recent research shows the limitations of training monolithic deep learning models in two important aspects of interpretability and generalizability (Hu et al., 2017). Symbolic representations that can support reasoning capabilities are shown to have a critical role in improving both of the above-mentioned aspects (Goldman et al., 2018; Krishnaswamy et al., 2019; Suhr et al., 2017b). A robust intermediate representation can help an off-the-shelf planner to ground a symbolic representation of the spatial concepts to a final executable output. This representation should be as general and domain independent as possible to eliminate the need to retrain an automatic annotation system for a new domain. Using AMR (Banarescu et al., 2013) as a stepping stone to our spatial configurations helps to ensure generalizability and domain independence. Our goal is to identify 5855 Figure 1: An instance of the collaborative building task. T"
2020.lrec-1.717,S13-2044,1,0.824048,"ration, we present an example from the NVLR dataset (Figure 10), which is a TRUE statement: There is a blue square closely touching the bottom of a box. We also present another example from BLOCKS (Figure 12): Move the Mercedes block to the right of the Nvidia block. The annotation is shown in Table 7 and the graphical representation with aligned AMR in Figure 13. 6. Related Representations The presented scheme is related to previous spatial annotation schemes such as ISO-space (Pustejovsky et al., 2011; Pustejovsky et al., 2015) and SpRL (Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013; Kordjamshidi et al., 2017). SpRL is based on holistic spatial semantics (Zlatev, 2003) and takes both cognitivelinguistic elements and spatial calculi models into account to bridge natural language and formal spatial representations that facilitate reasoning. The result is a light-weight spatial ontology compared to more extensive, linguistically motivated spatial ontologies such as GUM (Bateman et al., 2010; Hois and Kutz, 2008) or type theory motivated (Dobnik and Cooper, 2017) ones. ISO-Space scheme consid5861 id e1 e2 Spatial Entities properties head {name = span} {brand = M ercedes} blo"
2020.lrec-1.717,kordjamshidi-etal-2010-spatial,1,0.89171,"Missing"
2020.lrec-1.717,S12-1048,1,0.86488,"Missing"
2020.lrec-1.717,W18-4704,0,0.205805,"ion or orientation of the current object’s spatial description, e.g., my left and your right. Absolute FoR (geocentered) is a fixed FoR, e.g., North. As an example of a configuration having multiple FoRs with different values, consider : I walked from the center of the park to the left of the statue. In the path for this configuration, the first landmark has an intrinsic FoR while the second landmark has a relative FoR. In contrast to the FoR, the viewer is unique in a single configuration and is represented as a value v ∈ {f irst−person, second−person, third−person} (Tenbrink and Kuhn, 2011; Lee et al., 2018). A configuration 5857 Dataset BLOCKS GQA NLVR SpaceEval SpRL Minecraft Example Sentence Texaco should line up on the top right corner of BMW in the middle of Adidas and HP (1) Are there any cups to the left of the tray that is on top of the table? (2) There is a black square touching the wall with a blue square right on top of it. (3) While driving along the village’s main road the GPS showed us the direction right ahead, and after two minutes, just few hundred meters from the end of the village, we reached the spot. (4) a white bungalow with big windows, stairs to the left and the right, a n"
2020.lrec-1.717,mani-etal-2008-spatialml,0,0.0601948,"2 >, &lt; l3, e1 > &lt;s2,in between > NULL NULL &lt; l2, relative > &lt; l3, relative > first-person &lt;topological, EC> Table 4: Relational representation of spatial entities and configurations in the sentence: Place an orange block to the right of the base red block with two spaces in between. 3.3. Qualitative Types Fine-grained spatial relation semantics can be represented using specialized linguistically motivated ontologies such as General Upper Model (Bateman et al., 2010). Another approach has been mapping to formal spatial knowledge representation models that are independent from natural language (Mani et al., 2008; Kordjamshidi et al., 2010; Pustejovsky et al., 2011). The latter provides a more tractable formalism for spatial reasoning. The spatial formal models are divided into three categories: topological, directional and distal, and for each category specific formalisms have been invented (Wallgr¨un et al., 2007; Liu et al., 2009). We use these qualitative types. Directional systems can be relative or absolute, distal systems can be qualitative or quantitative and topological models can follow 5858 Figure 3: Graphical Representation of Configuration 1 of Table 3 with aligned AMR : Move the large re"
2020.lrec-1.717,P19-1537,0,0.0942124,"ct Meaning Representation (AMR) annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole. Keywords: Semantics, Knowledge Discovery/Representation 1. Introduction Spatial reasoning is necessary for many tasks. For example, consider the collaborative building task (Narayan-Chen et al., 2019) where a human architect (knows the target structure but cannot manipulate blocks) issues commands to a builder (who does not know the target structure but can place and manipulate blocks) with the aim of creating a complex target structure in a grounded environment (Minecraft). This task is challenging from a spatial reasoning perspective because the architect does not issue commands in terms of the unit blocks that the builder uses, but complex shapes and their spatial aspects. A single instruction can convey information about complex spatial configurations of several objects and describe ne"
2020.lrec-1.717,J05-1004,1,0.277483,"the spatial relationships and spatial object properties from the AMR and organize them into an easily interpretable format that maintains the complex relationships and nesting. The corpus that we describe here 1 includes annotation on over 5000 dialogue sentences (170 full dialogues) that discuss collaborative construction events in the Minecraft dataset (Narayan-Chen et al., 2019). We have an additional 7600 annotated automatically-generated sentences representing builder actions, giving a total of 12,600 spatial AMRs. The AMR expansion involves the addition of 150+ new or updated PropBank (Palmer et al., 2005; Gildea and Palmer, 2002) rolesets as well as a dozen general semantic frames, roles, and concepts that signal spatial relation1 https://github.com/jbonn/CwC-Minecraft-AMR ships not previously covered by AMR. Using rolesets to annotate spatial relations allows us to disambiguate senses and to group together etymologically-related relations from different parts of speech within those senses. For example, the roleset align-02 includes aliases align-v, alignedj, line-up-v, and in-line-p, and down-03 includes aliases down-p, down-r, downward-r, etc.. Prepositional and adverbial aliases are new fo"
2020.lrec-1.717,P17-2034,0,0.338907,"sented by existing spatial representations( for a more detailed comparison with previous schemes refer to Section 6). For instance, the spatial representation in (Kordjamshidi et al., 2010) does not capture the fine-grained semantics of the object properties and does not distinguish between the frame of reference and the perspective. The scheme of (Pustejovsky et al., 2011) does not handle complex landmarks in the first example with the sequence of spatial properties. We observe the ubiquity of these hard spatial descriptions across different datasets. Here is another such instance from NLVR (Suhr et al., 2017a) depicting spatial focus shift: There is a box with a black item between 2 items of the same color and no item on top of that. We propose an integrated view called SPATIAL CONFIGURATION that captures extensive spatial semantics that is necessary for reasoning. To develop an automatic builder, in the case of the collaborative construction task, we need an explicit representation of all involved spatial components and their spatial relations in the instruction. One of our key contributions is to represent this information compactly as a part of the configuration. Further, we can represent spat"
2020.lrec-1.717,P19-1009,0,0.0468791,"Missing"
2020.lrec-1.734,P15-1039,0,0.0503252,"Missing"
2020.lrec-1.734,P98-1013,0,0.745789,"Missing"
2020.lrec-1.734,W13-2322,1,0.704565,"Missing"
2020.lrec-1.734,duran-aluisio-2012-propbank,0,0.0620548,"Missing"
2020.lrec-1.734,W19-3315,1,0.884478,"Missing"
2020.lrec-1.734,J05-1004,1,0.580918,", and the process of sense disambiguation. It discusses language-specific issues that complicated the process of building the PropBank and how these challenges were exploited as language-internal guidance for consistency and coherence. Keywords: lexicon, lexical database, corpus (creation, annotation, etc.), semantics 1. Introduction This paper presents a proposition bank for Russian (RuPB) that balances parallelism with the English PropBank against guidance from linguistic properties specific to Russian. A proposition bank, or PropBank, is a lexical resource that follows the PropBank scheme (Palmer et al., 2005) to provide consistent labeling of semantic roles for large corpora. Semantic Role Labeling (SRL) provides consistent semantic information for natural language processing at a level appropriate for statistical machine learning of semantic relations. Data annotated with proposition bank labels supports the training of automatic SRL which improves question answering (Zapirain et al., 2013), information extraction (Moreda et al., 2005), and textual entailments (Sammons et al., 2010), and statistical machine translation (Bazrafshan and Gildea, 2013). Semantic roles communicate “Who does What to Wh"
2020.lrec-1.734,W19-3303,0,0.0209903,"Missing"
2020.lrec-1.734,P10-1122,0,0.021662,"erties specific to Russian. A proposition bank, or PropBank, is a lexical resource that follows the PropBank scheme (Palmer et al., 2005) to provide consistent labeling of semantic roles for large corpora. Semantic Role Labeling (SRL) provides consistent semantic information for natural language processing at a level appropriate for statistical machine learning of semantic relations. Data annotated with proposition bank labels supports the training of automatic SRL which improves question answering (Zapirain et al., 2013), information extraction (Moreda et al., 2005), and textual entailments (Sammons et al., 2010), and statistical machine translation (Bazrafshan and Gildea, 2013). Semantic roles communicate “Who does What to Whom and How and When and Where?” They may appear in various positions in the sentence, as in the examples below where “John” is always the semantic agent, but appears in (1) as the subject noun phrase, in (2) as the direct object in a passive transitive clause, and in (3) as the object of the final prepositional phrase of a passive ditransitive clause. Despite these syntactic alternations, in all three sentences the semantic role of “John” as the Agent of the hitting event never c"
2020.lrec-1.734,sharoff-2002-meaning,0,0.0591909,"Missing"
2020.lrec-1.734,L16-1521,0,0.066826,"Missing"
2020.lrec-1.734,W13-1018,1,0.77715,"Missing"
2020.lrec-1.734,W19-3321,0,0.0213291,"Missing"
2020.lrec-1.734,W10-1836,1,0.834407,"Missing"
2020.lrec-1.734,J13-3006,0,0.020943,"RuPB) that balances parallelism with the English PropBank against guidance from linguistic properties specific to Russian. A proposition bank, or PropBank, is a lexical resource that follows the PropBank scheme (Palmer et al., 2005) to provide consistent labeling of semantic roles for large corpora. Semantic Role Labeling (SRL) provides consistent semantic information for natural language processing at a level appropriate for statistical machine learning of semantic relations. Data annotated with proposition bank labels supports the training of automatic SRL which improves question answering (Zapirain et al., 2013), information extraction (Moreda et al., 2005), and textual entailments (Sammons et al., 2010), and statistical machine translation (Bazrafshan and Gildea, 2013). Semantic roles communicate “Who does What to Whom and How and When and Where?” They may appear in various positions in the sentence, as in the examples below where “John” is always the semantic agent, but appears in (1) as the subject noun phrase, in (2) as the direct object in a passive transitive clause, and in (3) as the object of the final prepositional phrase of a passive ditransitive clause. Despite these syntactic alternations"
2021.acl-demo.19,D13-1178,0,0.0270821,"esire to extend these event representations to include more complex relations led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To our knowledge, no open-source interface for complex schema visualization and editing has previously been developed. One large-scale effort to create a repository of complex event schemas is the DARPA Knowlegedirected Artificial Intelligence Reasoning Over Schemas (KAIROS) program. KAIROS relies on the assumption discussed abov"
2021.acl-demo.19,W13-2322,1,0.613505,"Most longstanding repositories of such frames or scripts were built at the level of single events and their participants, although some have made sparse connections between these simple event descriptions (e.g., FrameNet’s Uses and Precedes relations). The desire to extend these event representations to include more complex relations led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To our knowledge, no open-source interface for complex schema visua"
2021.acl-demo.19,W17-2712,1,0.84485,"gsbury and Palmer, 2002) and VerbNet (Kipper et al., 2008). Most longstanding repositories of such frames or scripts were built at the level of single events and their participants, although some have made sparse connections between these simple event descriptions (e.g., FrameNet’s Uses and Precedes relations). The desire to extend these event representations to include more complex relations led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To o"
2021.acl-demo.19,P16-1025,0,0.019896,"ment of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To our knowledge, no open-source interface for complex schema visualization and editing has previously been developed. One large-scale effort to create a repository of complex event schemas is the DARPA Knowlegedirected Artificial Intelligence Reasoning Over Schemas (KAIROS) program. KAIROS relies on the assumption discussed above, that humans make sense of events by organizing them into frequently occurring narra"
2021.acl-demo.19,kingsbury-palmer-2002-treebank,1,0.607399,"guistics (Fillmore, 1976) and artificial intelligence (Schank and Abelson, 2013). Prototypical sequences of events, such as the medical intervention sequence described above, can be made explicit in scripts that detail the usual subevents, their typical sequence, the people and objects generally involved in those events, and the progression of those participants through the subevents. Proving useful for both linguistic analysis and natural language processing, repositories of defined schemas and event representations were manually developed, such as FrameNet (Fillmore et al., 2002), PropBank (Kingsbury and Palmer, 2002) and VerbNet (Kipper et al., 2008). Most longstanding repositories of such frames or scripts were built at the level of single events and their participants, although some have made sparse connections between these simple event descriptions (e.g., FrameNet’s Uses and Precedes relations). The desire to extend these event representations to include more complex relations led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and"
2021.acl-demo.19,2020.emnlp-main.50,0,0.115811,". Defining such scenarios or templates for use in information extraction, knowledge base construction, and narrative prediction has a long history. As these fields have progressed, the complexity of the events and sequences being represented has increased. Any machine-readable format capable of representing multiple events, tracking their participants across the events, and delineating the temporal and causal relations between the events will be extremely difficult for a person to read and review. Because the extraction and construction of such complex schemas has accelerated in recent years (Li et al., 2020; Zhang et al., 2020), the need for a The complex schemas handled by our tool include multiple levels of intersecting information. For example, imagine a typical emergency medical intervention. We know this includes a Victim who is injured or ill and usually begins with communication by the Victim or a Bystander to an emergency Dispatcher, then progresses to communication from the Dispatcher to Medical Personnel, travel by Medical Personnel to the Victim, immediate medical assessment of the Victim, and, finally, possible transportation of the Victim to a Medical facility. A more complete schem"
2021.acl-demo.19,P15-1019,0,0.0472431,"Missing"
2021.acl-demo.19,D13-1185,0,0.0162458,"complex relations led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To our knowledge, no open-source interface for complex schema visualization and editing has previously been developed. One large-scale effort to create a repository of complex event schemas is the DARPA Knowlegedirected Artificial Intelligence Reasoning Over Schemas (KAIROS) program. KAIROS relies on the assumption discussed above, that humans make sense of events by organi"
2021.acl-demo.19,W16-5706,1,0.896945,"Missing"
2021.acl-demo.19,P09-1068,0,0.0729824,"presentations to include more complex relations led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To our knowledge, no open-source interface for complex schema visualization and editing has previously been developed. One large-scale effort to create a repository of complex event schemas is the DARPA Knowlegedirected Artificial Intelligence Reasoning Over Schemas (KAIROS) program. KAIROS relies on the assumption discussed above, that humans make sense of"
2021.acl-demo.19,N13-1104,0,0.0289785,"ns led to the development of systems of temporal, causal and other semantic relations, such as TimeML (Pustejovsky et al., 2005), Richer Event Descriptions (O’Gorman et al., 2016), Reference Event Ontology (Brown et al., 2017), and Abstract Meaning Representations (Banarescu et al., 2013). Because of the substantial effort involved in manually creating schemas, automatically inducing schemas from textual and visual data has become a priority. Automatically generated schemas have advanced from schemas for single events (Balasubramanian et al., 2013; Chambers and Jurafsky, 2009; Chambers, 2013; Cheung et al., 2013; Huang et al., 2016; Nguyen et al., 2015) to complex, multistep schemas (Li et al., 2020; Zhang et al., 2020) However, for optimal usefulness, these generated schemas still benefit from human revision. To our knowledge, no open-source interface for complex schema visualization and editing has previously been developed. One large-scale effort to create a repository of complex event schemas is the DARPA Knowlegedirected Artificial Intelligence Reasoning Over Schemas (KAIROS) program. KAIROS relies on the assumption discussed above, that humans make sense of events by organizing them into freque"
2021.acl-demo.19,2020.emnlp-main.119,0,0.227035,"cenarios or templates for use in information extraction, knowledge base construction, and narrative prediction has a long history. As these fields have progressed, the complexity of the events and sequences being represented has increased. Any machine-readable format capable of representing multiple events, tracking their participants across the events, and delineating the temporal and causal relations between the events will be extremely difficult for a person to read and review. Because the extraction and construction of such complex schemas has accelerated in recent years (Li et al., 2020; Zhang et al., 2020), the need for a The complex schemas handled by our tool include multiple levels of intersecting information. For example, imagine a typical emergency medical intervention. We know this includes a Victim who is injured or ill and usually begins with communication by the Victim or a Bystander to an emergency Dispatcher, then progresses to communication from the Dispatcher to Medical Personnel, travel by Medical Personnel to the Victim, immediate medical assessment of the Victim, and, finally, possible transportation of the Victim to a Medical facility. A more complete schema would include some"
2021.acl-long.489,D19-1371,0,0.0605024,"Missing"
2021.acl-long.489,C10-1018,0,0.0505882,"ramework to capture dependency structures and entity properties from an external knowledge base. More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with"
2021.acl-long.489,D13-1184,0,0.0294931,"ependency structures and entity properties from an external knowledge base. More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthe"
2021.acl-long.489,D19-1381,0,0.0207709,"mance. probably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns. 4 Related Work Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR semantic parsing and external knowledge to improve the performance. Utilization of External Knowledge In terms of utilization of ext"
2021.acl-long.489,2020.findings-emnlp.89,0,0.0814531,"3) and external knowledge graphs. AMR is a semantic representation language that converts the meaning of each input sentence into a rooted, directed, labeled, acyclic graph structure. AMR semantic representation includes PropBank (Palmer et al., 2005) frames, non-core semantic roles, coreference, entity typing and linking, modality, and negation. The nodes in AMR are concepts instead of words, and the edge types are much more fine-grained compared with traditional semantic languages like dependency parsing and semantic role labeling. We train a transformer-based AMR semantic parser (Fernandez Astudillo et al., 2020) on biomedical scientific texts and use it in our biomedical IE model. To better handle long sentences with distant trigger and entity pairs, we use AMR parsing to compress each sentence and to better capture global interactions between tokens. For example, as shown in Figure 1, the Positive Regulation event trigger “changes” is located far away from its arguments CTF, OTF-1, OTF-2 in the original sentence. However, in the AMR graph, such trigger-entity pairs are linked within two hops. Therefore, it will be much easier for the model to identify such kinds of events with the guidance of AMR pa"
2021.acl-long.489,2020.findings-emnlp.114,0,0.554126,"trigger spans. 6264 where W, We are trainable parameters, and f l and σ(·) are a single layer feed-forward neural network and LeakyReLU activation function respectively. Then we obtain the neighborhood information h∗i by the weighted sum of all neighbor features: X l h∗i = αi,j W∗ hlk , k∈Ni where W∗ is a trainable parameter. The updated node feature is calculated by a combination of the original node feature and its neighborhood information, where γ controls the level of message passing between neighbors. hl+1 = hli + γ · h∗i i (2) Note that our edge-conditioned GAT structure is similar to (Huang et al., 2020). The main difference is that (Huang et al., 2020) only uses edge features l , while we for calculating the attention score αi,j use the concatenation of the feature vectors of each edge and its involved pair of nodes. Such a method can better characterize differing importance levels for neighbor nodes, and thus yield better model performance. We select the last layer hL i as the final representation for each entity or trigger. Message Passing Given the knowledge enriched AMR graph G = (V, E) and representation vectors of extracted trigger and entity spans, we initialize the feature vectors fo"
2021.acl-long.489,W11-1802,0,0.0356039,"activation function in the output layer) for role type classificatt = FFN ([τ : τ ]) or tion, where we have yi,j tt i j te yi,j = FFNte ([τi : εj ]). The overall training objective is defined in a multi-task setting, which includes the cross-entropy loss for trigger and argument classification, as well as the binary classification loss LI . X X tt tt te te L = LI − yi,j log yˆi,j − yi,j log yˆi,j . (3) i,j 3 3.1 i,j Experiments Experimental Setup Data Similarly to the recent work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we also conduct experiments on the BioNLP GENIA 2011 (Kim et al., 2011) dataset consisting of both abstracts and main body texts from biomedical scientific papers. Similarly to previous work (Li et al., 2019; Huang et al., 2020; Ramponi et al., 2020), we only focus on extracting the core events, which involves Protein entities, 9 fine-grained event types, and 2 event argument types. We do not incorporate event ontology or training data from the newer versions of the BioNLP GENIA shared tasks (e.g., GENIA 2013) to ensure fair comparisons with previous models. The statistics of this dataset are shown in Table 2. The original GENIA dataset Data Split Train Set Dev S"
2021.acl-long.489,N19-1145,1,0.866501,"Missing"
2021.acl-long.489,P11-1163,0,0.104179,"Missing"
2021.acl-long.489,2020.nlpcovid19-acl.18,0,0.0987857,"Missing"
2021.acl-long.489,D19-1585,0,0.0277363,"on procedure is shown in Figure 2. 2.4 Node Identification and Message Passing Contextualized Encoder Given an input sentence S, we use the BERT model pretrained on biomedical scientific texts (Lee et al., 2020) to obtain the contextualized word representations {x1 , x2 , · · · , xN }. If one word is split into multiple pieces by the BERT tokenizer, we take the average of the representation vectors for all pieces as the final word representation. Node Identification After encoding the input sentence using BERT, we first identify the entity and trigger spans as the candidate nodes. Similar to (Wadden et al., 2019), given the contextualized word representations, we first enumerate all possible spans up to a fixed length K, and calculate each span representation according to the concatenation of the left and right endpoints and a trainable feature vector characterizing the span length6 . Specifically, given each span si = [start(i), end(i)], the span representation vector is:   si = xstart(i) , xend(i) , z(si ) , (1) where z(si ) denotes a trainable feature vector that is only determined by the span length. We use separate binary classifiers for each specific entity and trigger type to handle the spans"
2021.acl-long.489,J05-1004,1,0.286254,"Missing"
2021.acl-long.489,2020.louhi-1.10,0,0.443205,"g1 , Nikolaus Parulian1 , Heng Ji1 , Ahmed S. Elsayed2 , Skatje Myers2 , Martha Palmer2 1 University of Illinois at Urbana-Champaign 2 University of Colorado Boulder {zixuan11, nnp2, hengji}@illinois.edu {ahmed.s.elsayed, skatje.myers, martha.palmer}@colorado.edu Abstract entities, relations, and key events. It is an essential task for accelerating practical applications of the results and achievements from scientific research. For example, practical progress on combating COVID-19 depends highly on efficient transmission, assessment and extension of cutting-edge scientific research discovery (Wang et al., 2020a; Lybarger et al., 2020; Möller et al., 2020). In this scenario, a powerful biomedical IE system will be able to create a dynamic knowledge base from the surging number of relevant papers, making it more efficient to get access to the latest knowledge and use it for scientific discovery, as well as diagnosis and treatment of patients. IE from biomedical scientific papers presents two unique and non-trivial challenges. First, the authors of scientific papers tend to compose long sentences, where the event triggers and entity mentions are usually located far away from each other within the sent"
2021.acl-long.489,D19-5804,1,0.720275,"base. More recently, GEANet (Huang et al., 2020) introduces a Graph Edge conditioned Attention Network (GEANet) that incorporates domain knowledge from the Unified Medical Language System (UMLS) into the IE framework. The main difference of our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al., 2020). Our COVID-19 dataset will fu"
2021.acl-long.489,N10-1123,0,0.046613,"Protein “RAR beta” Protein “VDR” Table 7: Examples from development set showing how KG enriched AMR graph improves the model performance. probably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns. 4 Related Work Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR seman"
2021.acl-long.489,2020.knlp-1.1,0,0.0346393,"our model is that we use fine-grained AMR parsing to compress the wide context, and manage to use an external KG to enrich the AMR to better incorporate domain knowledge. Incorporating external knowledge is also widely used in other tasks such as relation extraction (Chan and Roth, 2010; Cheng and Roth, 2013), and QA for domainspecific (science) questions (Pan et al., 2019). Biomedical Benchmarks for COVID-19 (Lo et al., 2020) releases a dataset containing openaccess biomedical papers related to COVID-19. A lot of research has been done based on this dataset, including Information Retrieval (Wise et al., 2020), Entity Recognition (Wang et al., 2020b), distant supervision on fine-grained biomedical name entity recognition to support automatic information retrieval indexing or evidence mining (Wang et al., 2020c), and end-to-end Question Answering (QA) system for COVID-19 with domain adaptive synthetic QA training (Reddy et al., 2020). Our COVID-19 dataset will further advance the field in developing effective IE techniques specifically for the COVID-19 domain. 5 Conclusions and Future Work In this paper, we propose a novel biomedical Information Extraction framework to effectively tackle two unique"
2021.acl-long.489,2020.emnlp-main.431,0,0.0604686,"Missing"
2021.acl-long.489,2021.naacl-main.4,1,0.768984,"classifiers for each specific entity and trigger type to handle the spans with multiple labels. Each binary classifier is a feed-forward neural network with ReLU activation in the hidden layer, which is trained with binary cross-entropy loss jointly with the whole model. In the diagnostic setting of using gold-standard entity mentions, we only employ span enumeration for event trigger identification, and use the gold-standard entity set for the following event extraction steps. Edge-conditioned GAT To fully exploit the information of external knowledge and AMR semantic structure, similar to (Zhang and Ji, 2021), we use an L-layer graph attention network to let the model aggregate neighbor information from the fused graph G = (V, E). We use hli to denote the node feature for vi ∈ V in layer l, and ei,j to represent the edge feature vector for ei,j ∈ E. To update the node feature from l to l + 1, we first calculate the attention score for each neighbor j ∈ Ni based on the concatenation of node features hli , hlj and edge features ei,j .    exp σ f l [Whli : We ei,j : Whlj ] l  , αi,j =P l l l k∈Ni exp σ f [Whi : We ei,k : Whk ] 6 We use different maximum span length K for entity and trigger span"
2021.acl-long.489,W17-2315,0,0.0124794,"KG enriched AMR graph improves the model performance. probably because the BERT tokenizer breaks these words into pieces de, phosphorylation, encouraging BERT models to learn misleading patterns. 4 Related Work Biomedical Information Extraction A number of previous studies contribute to biomedical event extraction with various techniques, such as dependency parsing (McClosky et al., 2011; Li et al., 2019), external knowledge base (Li et al., 2019; Huang et al., 2020), joint inference of triggers and arguments (Poon and Vanderwende, 2010; Ramponi et al., 2020), Abstract Meaning Representation (Rao et al., 2017), search based neural models (Espinosa et al., 2019), and multi-turn question answering (Wang et al., 2020b). Recently, to handle the nested biomedical events, BEESL (Ramponi et al., 2020) models biomedical event extraction as a unified sequence labeling problem for end-to-end training. DeepEventMine (Trieu et al., 2020) proposes to use a neural network based classifier to decide the structure of complex nested events. Our model is also in an end-to-end training pipeline, but additionally utilizes fine-grained AMR semantic parsing and external knowledge to improve the performance. Utilization"
2021.dash-1.14,lin-etal-2010-new,0,0.0233881,"dding space. Multilingual Information Retrieval DAPRA KAIROS’ events are similar to the events found in the IARPA BETTER multilingual information retrieval project.11 A future application of TopGuNN could be querying in English and retrieving training examples in another language (or vice versa) by substituting BERT for GigaBERT (Lan et al., 2020) in TopGuNN. With this modification, TopGuNN could help facilitate multilingual retrieval of training examples. 6 Getting started with TopGuNN Related Work Previous work that parallels our work to search and index large corpora includes projects like Lin et al. (2010), which created an index of n-gram counts over a web-scale sized corpus. Similarly, 11 https://www.iarpa.gov/index.php/research-programs/ better 91 Acknowledgements References 2018. Named entity recognition with Bert. Tobias Sterbak Consulting, Akazienstraße 3A, 10823 Berlin, Germany. We would like to thank Erik Bernhardsson for the useful feedback on integrating Annoy indexing. Special thanks to Ashley Nobi for spearheading the annotation effort and Katie Conger at University of Colorado at Boulder for the training sessions on semantic role labeling she gave for the span annotation effort. We"
2021.dash-1.14,2020.acl-main.537,0,0.014535,"pe, sid loc, gpe, fac, per, com, veh, wea, sid com, veh, wea fac, loc, gpe Temporal Start and End Duration (times specific to event) 1 second to multiple years Embedding Model TopGuNN creates contextualized word embeddings for each content word in the corpus and for each query word in the query sentences. We use BERT (Devlin et al., 2019a) to create the embeddings because BERT produces contextually-aware embeddings unlike word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014).3 FastBERT or DistilBERT would also be appropriate choices, but come with an accuracy trade-off for speed (Liu et al., 2020; Sanh et al., 2019). We also investigated running TopGuNN at the sentence-level using sentence embeddings from SBERT and computing averaged sentence embeddings using BERT (Reimers and Gurevych, 2019). Qualitatively, the results from using BERT at the word-level gave us diversity in the results that we desired (see Appendix B). Conflict.Attack a violent physical act causing harm or damage Slot Role Attacker Target Instr./Means Place Corpus 2.3 Retrieving Event Primitives A total of 60 event primitives were annotated using TopGuNN. On average, we were given 2 seed sentences per event and 1-2 vi"
2021.dash-1.14,P10-2041,0,0.0457921,"entire Gigaword corpus can be seen in Table 4. The first query word in the batch of queries takes longer as it must load each Annoy index into memory from disk. For subsequent queries in the batch, the Annoy index is already loaded into memory. (see Section 3.4.1) 10 For example, after searching a query word &quot;identify&quot; on a particular Annoy index all subsequent queried words like “hired” or “launched” on that same Annoy index will leverage the operating system page cache of the Annoy index file and perform faster 90 as an extension to work completed by Lin et al. (1997) and Gao et al. (2002), Moore and Lewis (2010) propose a method for gathering domainspecific training data for languages models for use in tasks such as Machine Translation. By utilizing contextual word embeddings from a modern language model like BERT instead of techniques like n-grams or perplexity analysis as seen in previous approaches, TopGuNN aims to achieve higher quality results. Our work directly builds upon prior research on approximate k-NN algorithms for cosine similarity search. We chose to use the Annoy package for indexing our embeddings in TopGuNN for its particular ability to build on-disk indexes, however, another packag"
2021.dash-1.14,D18-2021,1,0.926766,"ults. We use our look-up dictionaries to return the document, the sentence, and the word of each result. Search results from each of the query words over the Annoy indexes are combined at the end and exported to a .tsv for human annotation and active learning. Indexing All of the embeddings saved in the previous step for each of the 960 partitions are added to an Annoy index, to create 960 Annoy indexes that span our entire corpus. We use Spotify’s Annoy indexing system created by Bernhardsson (2018) for approximate k-NN search, which has been shown to be significantly faster than exact k-NN (Patel et al., 2018). While, there are various competing implementations for approximate k-NN, we ultimately used Annoy to power our similarity search for its 3.4.1 Enhancing Query Performance Sequentially searching each query word against the 960 Annoy indexes before moving on to the next query word is slow. To perform searches more efficiently, we sequentially query each of the 960 Annoy indexes with all query words. This leverages the operating system page cache in such a way that allows for the system to scale better to larger batches of queries. By querying in this manner, we only need to load each of the 96"
2021.dash-1.14,D14-1162,0,0.0890715,"vent’s semantic roles. An example of a KAIROS event primitive is Attack: Label Description 2.2 Slot Argument Constraints per, org, gpe, sid loc, gpe, fac, per, com, veh, wea, sid com, veh, wea fac, loc, gpe Temporal Start and End Duration (times specific to event) 1 second to multiple years Embedding Model TopGuNN creates contextualized word embeddings for each content word in the corpus and for each query word in the query sentences. We use BERT (Devlin et al., 2019a) to create the embeddings because BERT produces contextually-aware embeddings unlike word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014).3 FastBERT or DistilBERT would also be appropriate choices, but come with an accuracy trade-off for speed (Liu et al., 2020; Sanh et al., 2019). We also investigated running TopGuNN at the sentence-level using sentence embeddings from SBERT and computing averaged sentence embeddings using BERT (Reimers and Gurevych, 2019). Qualitatively, the results from using BERT at the word-level gave us diversity in the results that we desired (see Appendix B). Conflict.Attack a violent physical act causing harm or damage Slot Role Attacker Target Instr./Means Place Corpus 2.3 Retrieving Event Primitives"
2021.findings-acl.418,2020.acl-main.697,0,0.0242902,"earning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is"
2021.findings-acl.418,2020.bea-1.0,0,0.168825,"Missing"
2021.findings-acl.418,P98-1032,0,0.366987,"lp of an NLP system and, thus, to improve student learning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 20"
2021.findings-acl.418,W14-3501,0,0.0170187,"lassroom discourse with annotations for talk moves used by the teacher. 3.2 NLP for Educational Applications Our work is a first step towards improving in-class discussions with the help of an NLP system and, thus, to improve student learning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act t"
2021.findings-acl.418,D14-1179,0,0.0394396,"Missing"
2021.findings-acl.418,D11-1010,0,0.0147882,"her. 3.2 NLP for Educational Applications Our work is a first step towards improving in-class discussions with the help of an NLP system and, thus, to improve student learning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the tas"
2021.findings-acl.418,C16-1189,0,0.0120003,"mated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is the task of predicting what the next dialogue act should be, given a conversation history (Tanaka et al., 2019). Pretrained models have been successfully adapted to the task of dialogue generation (Zhang et al., 2020; Wu et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). However, if directly used in the classroom, these models could potentially produce harmful or unsuitable dialogue as they are trained on large datasets comprising conversations from the internet (Bender et al., 2021). Additionally, we want a sys"
2021.findings-acl.418,W02-0109,0,0.664718,"Missing"
2021.findings-acl.418,N19-4009,0,0.0505929,"Missing"
2021.findings-acl.418,P16-1162,0,0.0107246,"sing two special tokens, [USR] and [SYS]. Correspondingly, we use the [USR] token to indicate student utterances and the [SYS] token to indicate teacher utterances. We then concatenate a context of w utterances, marked by speaker tokens when there is a change in speaker, to obtain ctod . Finally, we encode ctod using the pretrained TOD-BERT model and concatenate it with the output of the dialogue encoder and talk move encoder. Equation 5 then becomes: rt = cat(bt , dt , TOD-BERT(ctod )) (7) We call this model 3-E-TOD-BERT. When pretrained sentence encoders are used, we use the respective BPE (Sennrich et al., 2016) tokenizer for each model. 4.2 Computing the Loss We train all models using a cross-entropy loss. However, we observe a strong class imbalance in our training data, cf. Figure 2. Thus, we compute label weights inversely proportional to the frequency of a label’s occurrence in the data and use them to weight the loss for each training example. 4743 5 Experiments 30000 Dataset 25000 5.2 Baselines We compare our model to three baselines. Random Baseline (RB) This baseline randomly selects one of the 8 talk moves for each input. Talk Move Bigram Model (TMBM) For this baseline, we compute the condi"
2021.findings-acl.418,J00-3003,0,0.798124,"Missing"
2021.findings-acl.418,P19-2027,0,0.0198107,"to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is the task of predicting what the next dialogue act should be, given a conversation history (Tanaka et al., 2019). Pretrained models have been successfully adapted to the task of dialogue generation (Zhang et al., 2020; Wu et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). However, if directly used in the classroom, these models could potentially produce harmful or unsuitable dialogue as they are trained on large datasets comprising conversations from the internet (Bender et al., 2021). Additionally, we want a system to facilitate structured conversations, and not cause further diversions – this is in contrast to many task-oriented or open-domain dialogue systems whose purpose is to entertain a"
2021.findings-acl.418,2020.emnlp-main.66,0,0.33312,"the next talk move. We further propose a model for FTMP, which we call 3-E.1 It consists of three recurrent neural network (RNN) encoders: one for individual utterances, one for utterance sequences, and one for talk move sequences. The model is trained on transcripts of classroom discussions where teacher utterances have been annotated for the talk moves they represent. We consider the actions of the teacher to be our gold standard data for FTMP. We show that our model strongly outperforms multiple baselines and that adding sentence representations from RoBERTa (Liu et al., 2019) or TOD-BERT (Wu et al., 2020) – a model trained on task-oriented dialogue – does not increase performance further. Finally, we investigate the performance of human annotators on FTMP. Unlike the teacher, they do not have access to multi-modal signals, subject matter information, or knowledge of student behavior. This setting, which mimics the information available to our model, is significantly different from the teachers who generate the gold standard utterances captured in our data. We present a detailed analysis of their performance on a diagnostic test set, and highlight similarities to our model’s performance. Our fi"
2021.findings-acl.418,2020.acl-demos.30,0,0.0192988,"apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is the task of predicting what the next dialogue act should be, given a conversation history (Tanaka et al., 2019). Pretrained models have been successfully adapted to the task of dialogue generation (Zhang et al., 2020; Wu et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). However, if directly used in the classroom, these models could potentially produce harmful or unsuitable dialogue as they are trained on large datasets comprising conversations from the internet (Bender et al., 2021). Additionally, we want a system to facilitate structured conversations, and not cause further diversions – this is in contrast to many task-oriented or open-domain dialogue systems whose purpose is to entertain and appear personable to the user. Hence, we propose FTMP as a crucial first step towards an NLP system ca"
2021.law-1.13,W13-5503,1,0.698363,"ed, which means that setting 1 yields no output (i.e. does not extract ‘river’ as a created entity). Therefore, we look at the A2 argument in the PropBank parse output of the VNSP, summarized below: [‘text’: ‘The stream’, ‘pb’: ‘A1’, ‘vn’: ‘Patient’, ‘text’: ‘becomes’, ‘pb’: ‘V’, ‘vn’: ‘Verb’, ‘text’: ‘a river’, ‘pb’: ‘A2’, ‘vn’: ‘’] Linguistically, the A2 argument for this verb is described as ‘new state’, which is exactly what we 4.2 Setting 2 need. Since the numbered arguments in PropBank are too coarse-grained, we use SemLink (Palmer In setting 2, we used the PropBank argument roles 2009, Bonial et al. 2013, Stowe et al. 2021) to find included in the VNSP output in addition to the their mapping into VerbNet thematic roles and find VerbNet predicates. This covered cases of verbs those that suit our purposes in this task. that exist in VerbNet, but VNSP fails to instantiate In general, we assumed that A1 (proto-patient, the entity or location. This is most likely due to which is typically the undergoer) is the entity the small size of the VerbNet labeled training data moved or created. This assumption should be accucompared to PropBank labeled training data. For rate in theory at least for verbs i"
2021.law-1.13,P17-1038,0,0.0241009,"2020). However, these models rely on large amounts of annotated data, which is expensive and labor-intensive to provide (Sun et al., 2020). One of the commonly used solutions to the data scarcity problem in NLP tasks is data augmentation. According to Feng et al. (2021), the goal of data augmentation is increasing training data diversity without directly collecting more data. Most strategies for data augmentation consist of creating synthetic data based on the main data. On the same note, automatic training data generation attempts show promise in various NLP tasks, such as event extraction (Chen et al. 2017, Zeng et al. 2018), and named entity recognition (Tchoua et al., 2019). As explained below, Lexis uses a state-of-the-art semantic parser to automatically generate entity states for each sentence. These entity states are the same as the entity state labels provided by human annotators in the dataset on which we evaluate Lexis. The generated inferences can be used, in future work, to augment the existing training data. 3 Data and Methodology The main contribution of this work is bringing to the forefront the advantages of using lexical resource based methods. In particular, in this work, these"
2021.law-1.13,N18-1144,0,0.140191,"tion. In order to reduce the time and expenses associated with annotation, we introduce a new method to automatically extract entity states, including location and existence state of entities, following Dalvi et al. (2018) and Tandon et al. (2020). For this purpose, we rely primarily on the semantic representations generated by the state of the art VerbNet parser (Gung, 2020), and extract the entities (event participants) and their states, based on the semantic predicates of the generated VerbNet semantic representation, which is in propositional logic format. For evaluation, we used ProPara (Dalvi et al., 2018), a reading comprehension dataset which is annotated with entity states in each sentence, and tracks those states in paragraphs of natural human-authored procedural texts. Given the presented limitations of the method, the peculiarities of the ProPara dataset annotations, and that our system, Lexis, makes no use of task-specific training data and relies solely on VerbNet, the results are promising, showcasing the value of lexical resources. 1 Introduction Paragraph: Blood delivers oxygen in the body. Proteins and acids are broken down in the liver. The liver releases waste in the form of urea."
2021.law-1.13,N19-1244,0,0.0118241,"em, Lexis, is able to model using VerbNet. Question answering in reading comprehension This type of reasoning requires event extraction tasks focusing on procedural texts (i.e. texts deas a first step, and event participant state extraction scribing processes) is particularly challenging in as a second step. Our method covers both steps, but natural language processing (NLP), because this is at this point limited to sentence-level inference. type of text describes a changing world state (Clark et al. 2018, Dalvi et al. 2018, Tandon et al. 2018, In section 2, we provide an overview of the work Du et al. 2019, Gupta and Durrett 2019). Tracking related to this research. Section 3 introduces the the state of entities in such texts is an important dataset used to evaluate Lexis, as well as the details task to enable proper question answering in read- of the methods we have used. Section 4 presents ing comprehension tasks. The challenging part in our experimental settings, followed by section 5 such question answering tasks is not where answers which illustrates the results of each setting. Section are explicitly mentioned in a sentence (Clark et al., 6 discusses the advantages of Lexis, and provides"
2021.law-1.13,W19-1502,0,0.0179279,"le to model using VerbNet. Question answering in reading comprehension This type of reasoning requires event extraction tasks focusing on procedural texts (i.e. texts deas a first step, and event participant state extraction scribing processes) is particularly challenging in as a second step. Our method covers both steps, but natural language processing (NLP), because this is at this point limited to sentence-level inference. type of text describes a changing world state (Clark et al. 2018, Dalvi et al. 2018, Tandon et al. 2018, In section 2, we provide an overview of the work Du et al. 2019, Gupta and Durrett 2019). Tracking related to this research. Section 3 introduces the the state of entities in such texts is an important dataset used to evaluate Lexis, as well as the details task to enable proper question answering in read- of the methods we have used. Section 4 presents ing comprehension tasks. The challenging part in our experimental settings, followed by section 5 such question answering tasks is not where answers which illustrates the results of each setting. Section are explicitly mentioned in a sentence (Clark et al., 6 discusses the advantages of Lexis, and provides 2018). For example, in Fi"
2021.law-1.13,D17-1195,0,0.0270182,"n the predicateType ‘Be’ along with a ‘Result’ argument type, we can conclude that the value ‘a stream’ is the created entity. This is then fed into spaCy to extract the head noun ‘stream’ as the entity. Figure 2: VerbNet semantic predicate output of VNSP on the input sentence above, and how it is used to predict a CREATE type change of state for the entity ‘stream’ 2 Related Work tion. In addition, they added new entries for verbs not existing in VerbNet. In contrast, our method is fully automatic (see section 3.2). Most work on tracking entity states uses neural methods (Henaff et al. 2016, Ji et al. 2017, Tandon et al. 2018, Tang et al. 2020). However, these models rely on large amounts of annotated data, which is expensive and labor-intensive to provide (Sun et al., 2020). One of the commonly used solutions to the data scarcity problem in NLP tasks is data augmentation. According to Feng et al. (2021), the goal of data augmentation is increasing training data diversity without directly collecting more data. Most strategies for data augmentation consist of creating synthetic data based on the main data. On the same note, automatic training data generation attempts show promise in various NLP"
2021.law-1.13,kingsbury-palmer-2002-treebank,1,0.373651,"ring those 124 processes. 3.2 Methodology We used the recently developed BERT-based VerbNet semantic parser (Gung 2020, Gung and Palmer 2021), which is located at the GitHub SemParse site 1 , to parse every single sentence in each paragraph. The VerbNet semantic parser (VNSP) returns a json file containing the verb sense disambiguated VerbNet class, the complete logical predicates for that class instantiated with arguments extracted from the sentence, as well as the text spans (phrases) labeled with both VerbNet thematic roles (Schuler, 2005) (if applicable) and PropBank argument role labels (Kingsbury and Palmer 2002, Palmer et al. 2005). The main idea of using VNSP, and in general what gives VNSP an edge over other semantic parsers, is the logical predicates it generates (for a list of some of the VerbNet predicates used in this work, see Table 1). These predicates are utilized here to infer/predict an entity’s change of location and change of existence state (i.e. whether it has been created or destroyed during the course of the sentence). Some of these predicates uncover implicit information about entity states, so our method covers explicit and implicit information, as long as the information is impli"
2021.law-1.13,2020.acl-main.744,1,0.838745,"Missing"
2021.law-1.13,J05-1004,1,0.269989,".2 Methodology We used the recently developed BERT-based VerbNet semantic parser (Gung 2020, Gung and Palmer 2021), which is located at the GitHub SemParse site 1 , to parse every single sentence in each paragraph. The VerbNet semantic parser (VNSP) returns a json file containing the verb sense disambiguated VerbNet class, the complete logical predicates for that class instantiated with arguments extracted from the sentence, as well as the text spans (phrases) labeled with both VerbNet thematic roles (Schuler, 2005) (if applicable) and PropBank argument role labels (Kingsbury and Palmer 2002, Palmer et al. 2005). The main idea of using VNSP, and in general what gives VNSP an edge over other semantic parsers, is the logical predicates it generates (for a list of some of the VerbNet predicates used in this work, see Table 1). These predicates are utilized here to infer/predict an entity’s change of location and change of existence state (i.e. whether it has been created or destroyed during the course of the sentence). Some of these predicates uncover implicit information about entity states, so our method covers explicit and implicit information, as long as the information is implicit in the semantics"
2021.law-1.13,2020.coling-main.305,0,0.0107404,"head noun ‘stream’ as the entity. Figure 2: VerbNet semantic predicate output of VNSP on the input sentence above, and how it is used to predict a CREATE type change of state for the entity ‘stream’ 2 Related Work tion. In addition, they added new entries for verbs not existing in VerbNet. In contrast, our method is fully automatic (see section 3.2). Most work on tracking entity states uses neural methods (Henaff et al. 2016, Ji et al. 2017, Tandon et al. 2018, Tang et al. 2020). However, these models rely on large amounts of annotated data, which is expensive and labor-intensive to provide (Sun et al., 2020). One of the commonly used solutions to the data scarcity problem in NLP tasks is data augmentation. According to Feng et al. (2021), the goal of data augmentation is increasing training data diversity without directly collecting more data. Most strategies for data augmentation consist of creating synthetic data based on the main data. On the same note, automatic training data generation attempts show promise in various NLP tasks, such as event extraction (Chen et al. 2017, Zeng et al. 2018), and named entity recognition (Tchoua et al., 2019). As explained below, Lexis uses a state-of-the-art"
2021.law-1.13,D18-1006,0,0.133536,"icular semantics of this verb. This is the kind of inference our system, Lexis, is able to model using VerbNet. Question answering in reading comprehension This type of reasoning requires event extraction tasks focusing on procedural texts (i.e. texts deas a first step, and event participant state extraction scribing processes) is particularly challenging in as a second step. Our method covers both steps, but natural language processing (NLP), because this is at this point limited to sentence-level inference. type of text describes a changing world state (Clark et al. 2018, Dalvi et al. 2018, Tandon et al. 2018, In section 2, we provide an overview of the work Du et al. 2019, Gupta and Durrett 2019). Tracking related to this research. Section 3 introduces the the state of entities in such texts is an important dataset used to evaluate Lexis, as well as the details task to enable proper question answering in read- of the methods we have used. Section 4 presents ing comprehension tasks. The challenging part in our experimental settings, followed by section 5 such question answering tasks is not where answers which illustrates the results of each setting. Section are explicitly mentioned in a sentence"
2021.law-1.13,2020.emnlp-main.520,0,0.0773674,"Missing"
2021.law-1.13,2020.emnlp-main.591,0,0.0523916,"Missing"
2021.law-1.4,W13-2322,1,0.566337,"mmediately arises of how to efficiently add these new annotation features to pre-existing English AMRs. This paper describes an implementation of an automatic system that relies on VerbNet, a rich lexical resource, as the basis for categorizing event descriptions according to the Aspect guidelines discussed below. Our initial results are quite promising, and there are obvious next steps to take. Introduction 2 As the field of Natural Language Processing advances, there are increasing demands for more sophisticated applications and richer representations. Abstract Meaning Representations (AMR; Banarescu et al. 2013), and their more recent crosslingual incarnation as Uniform Meaning Representations (UMR; Van Gysel et al. 2021), are a response to that demand. AMR/UMRs provide an abstract, directed acyclic graph representation of a complete sentence, focusing on the underlying “who"" did “what"" to “whom"" elements of the events being described. The more information that can be associated with those events, in terms of whether they have been completed, or whether they have achieved their intended results, the better. The increased richness of UMR Tense, Aspect and Modality annotations, as described below, can"
2021.law-1.4,W18-4912,0,0.0158934,"ture (Van Gysel et al., 2019, 2021). Every node classified as an “event” in UMR receives an aspect annotation. UMR defines “event” based on the typological prototype of a verb as described in Croft (2001), and Croft (in press). UMR events exhibit either the prototypical information-packaging of a verb, predication (as opposed to modification and reference), or the prototypical semantic class of a verb, a process (as opposed to a property or an entity). The aspectual distinctions made in UMR are based on Croft (2012)’s two-dimensional analysis of aspect and build on the aspect annotations from Donatelli et al. (2018, 2019). Croft (2012) analyzes aspectual structure as having both a temporal dimension and a qualitative dimension; the temporal dimension measures out the event’s unfolding over time and the qualitative dimension measures out the change that occurs (or does not occur) during the event. The UMR aspectual distinctions do not have a direct correspondence with either specific verbs or specific constructions in a language; instead, they annotate the aspectual structure of an event in its context. In order to ensure the maximum cross-linguistic comparability of annotation values, UMR uses lattices"
2021.law-1.4,D17-1271,0,0.0519595,"Missing"
2021.law-1.4,P16-1166,0,0.0290633,"entation of a complete sentence, focusing on the underlying “who"" did “what"" to “whom"" elements of the events being described. The more information that can be associated with those events, in terms of whether they have been completed, or whether they have achieved their intended results, the better. The increased richness of UMR Tense, Aspect and Modality annotations, as described below, can more clearly identify the completion and achievement of events in a cross-lingual context, providBackground Information Previous automatic annotation models operate on different definitions of aspect. In Friedrich et al. (2016), clauses are annotated for situation entity types, which capture some of the same semantic distinctions as the UMR aspect annotation scheme, including state and habitual. Friedrich et al. (2016) also include modal distinctions in their annotation, such as questions and imperatives. Unlike Friedrich et al. (2016), the UMR aspect annotation distinguishes between different types of dynamic (non-stative) clauses (Activity, Endeavor, and Performance); these are all annotated as Event in Friedrich et al. (2016). Friedrich and Gateva (2017) annotate a binary telicity distinction: telic vs. atelic. T"
2021.law-1.4,W19-3315,1,0.847739,"ing any tokens that the spaCy English Web Core Large NLP parser (Honnibal et al., 2021) marks as having a part-of-speech (POS) corresponding to any of the Penn Treebank VERB tags2 to continue on to the sequence of rule-based decisions (Steps 2a-8). 1 All code can be found at https://github.com/ dchensta/AutoAspect. 2 VB (base form), VBD (past tense), VBG (gerund or present participle), VBN (past participle), VBP (non-3rd person singular present, and VBZ (3rd person singular present). We experimented with various parsers for extracting verbal events, which included running the ClearTAC parser (Myers and Palmer, 2019) and iterating through the list of events SemParse (Gung, 2020; Gung and Palmer, 2021) generates for the input sentence. At this stage, we are able to extract more verbal events using the spaCy NLP parser. 3.2 Step 1b: Event Nominals Branch While SemParse misses some verbal events, it is the sole parser that can identify nominal tokens that correspond to VerbNet frames. With this functionality, a derived nominal like explosion shares the same VerbNet ID as its parent verb explode. It can thus be recognized by SemParse as an event nominal. Thus, to follow the event nominals annotation branch, w"
2021.law-1.4,P18-1018,0,0.0442773,"Missing"
2021.law-1.4,L16-1521,0,0.0270452,"Missing"
2021.law-1.4,W19-3301,1,0.890671,"Missing"
2021.naacl-demos.16,D14-1148,0,0.0290568,". We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very fe"
2021.naacl-demos.16,C16-1201,0,0.0607333,"Missing"
2021.naacl-demos.16,N13-1073,0,0.043681,"luster, we apply these patterns as highprecision patterns before two statistical temporal ordering models separately. The schema matching algorithm will select the best matching from two graphs as the final instantiated schema results. Because the annotation for non-English data can be expensive and time-consuming, the temporal event tracking component has only been trained on English input. To extend the temporal event tracking capability to cross-lingual setting, we apply Google Cloud neural machine translation 6 to translate Spanish documents into English and apply the FastAlign algorithm (Dyer et al., 2013) to obtain word alignment. 2.6 Cross-media Information Grounding and Fusion Visual event and argument role extraction: Our goal is to extract visual events along with their argument roles from visual data, i.e., images and videos. In order to train event extractor from visual data, we have collected a new dataset called Video M2E2 which contains 1,500 video-article pairs by searching over YouTube news channels using 18 event primitives related to visual concepts as search keywords. We have extensively annotated the the videos and sampled key frames for annotating bounding boxes of argument rol"
2021.naacl-demos.16,2020.acl-main.718,0,0.068856,"Missing"
2021.naacl-demos.16,D19-5102,0,0.0208286,"arch purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Pr"
2021.naacl-demos.16,glavas-etal-2014-hieve,0,0.0312373,"l cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: ht"
2021.naacl-demos.16,R13-2011,0,0.025761,"ross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeli"
2021.naacl-demos.16,D19-1041,0,0.0480812,"Missing"
2021.naacl-demos.16,N19-1085,0,0.0191037,"oend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE sy"
2021.naacl-demos.16,W18-3101,0,0.0195422,"ly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who di"
2021.naacl-demos.16,N16-1056,0,0.030098,"nologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of even"
2021.naacl-demos.16,W18-5620,0,0.0245812,"ts described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but the current, first-generati"
2021.naacl-demos.16,L16-1545,0,0.0202604,"rstand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a complete answer to “Who died from COVID-19?”. Progress in natural language understanding and computer vision has helped automate some parts of event understanding but th"
2021.naacl-demos.16,2021.naacl-main.274,1,0.542222,"e postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents, we create N (N2−1) pairs of documents and treat each pair as a single “mega-document”. We apply our model to each mega-document and, at the end, aggregate the predictions across all megadocuments to extract the coreference clusters. Finally, we also apply a simple heuristic rule that prevents two entity mentions from being merged together if they are linked to different entities with high confidence. Our event coreference resolution method (Lai et al., 2021) is similar to entity coreference resolution, while incorporating additional symbolic features such as the event type information. If the input documents are all about one specific complex event, we apply some schema-guided heuristic rules to further refine the predictions of the neural event coreference resolution model. For example, in a bombing schema, there is typically only one bombing event. Therefore, in a document cluster, if there are two event mentions of type bombing and they have several arguments in common, these two mentions will be considered as coreferential. 2.5 Cross-document"
2021.naacl-demos.16,D17-1018,0,0.0120578,"5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that app"
2021.naacl-demos.16,2020.acl-main.703,0,0.0204692,"racted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments. Our model is based on BART (Lewis et al., 2020), which is an encoder-decoder language model. To utilize the encoder-decoder LM for argument extraction, we construct an input sequence of hsi template hsih/sidocument h/si. All argument names (arg1, arg2 etc.) in the template are replaced by a special placeholder token hargi. This model is trained in an end-to-end fashion by directly optimizing the generation probability. To align the extracted arguments back to the document, we adopt a simple postprocessing procedure and find the matching text span closest to the corresponding event trigger. Given N hybrid English and Spanish input documents"
2021.naacl-demos.16,N19-4019,1,0.796479,"ence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide ra"
2021.naacl-demos.16,2020.acl-demos.11,1,0.922974,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2020.emnlp-main.50,1,0.880674,"document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time prediction (Wen We use the multilingual XLM-RoBERTa (XLMet al., 2021; Li et al., 2020a). We only consider R) Transformer model (Conneau et al., 2020) so event mention pairs which are within neighboring that our coreference resolution model can handle sentences, or can be connected by shared argunon-English data. Second, we port the e2e-coref ments. model to the cross-lingual cross-document setting. Besides model prediction, we also learn high 5 https://www.wikidata.org/ confident patterns from the schema repository. We 135 consider temporal relations that appear very frequently as our prior knowledge. For each given document cluster, we apply these patterns as highprecision pa"
2021.naacl-demos.16,2021.naacl-main.69,1,0.764758,"chemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. The condition consists of the original document and a blank event template. For example, the template for Transportation event type is arg1 transported arg2 in arg3 from arg4 place to arg5 place. The desired output is a filled template with the arguments."
2021.naacl-demos.16,2020.acl-main.713,1,0.86722,"und the extracted knowledge elements onto our extracted graph via cross-media event coreference resolution (Section 2.6). Finally, our system selects the schema from a schema repository that best matches the extracted IE graph and merges these two graphs (Section 2.7). Our system can extract 24 types of entities, 46 types of relations and 67 types of events as defined in the DARPA KAIROS3 ontology. times for each detected words, as well as potential alternative transcriptions. Then from the speech recognition results and text input, we extract entity, relation, and event mentions using OneIE (Lin et al., 2020), a stateof-the-art joint neural model for sentence-level information extraction. Given a sentence, the goal of this module is to extract an information graph G = (V, E), where V is the node set containing entity mentions and event triggers and E is the edge set containing entity relations and event-argument links. We use a pre-trained BERT encoder (Devlin et al., 2018) to obtain contextualized word representations for the input sentence. Next, we adopt separate conditional random field-based taggers to identify entity mention and event trigger spans from the sentence. We represent each span,"
2021.naacl-demos.16,2021.ccl-1.108,0,0.0312676,"Missing"
2021.naacl-demos.16,L16-1631,0,0.0207744,"ences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolutio"
2021.naacl-demos.16,P17-1009,0,0.0621125,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.253,0,0.0765544,"Missing"
2021.naacl-demos.16,2020.findings-emnlp.344,0,0.0324985,"match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columb"
2021.naacl-demos.16,D17-1108,1,0.83481,"on. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et al., 2020; Yu et al., 2020). We incorporate a wide range of symbolic features (Chen and Ji, 2009; Chen et al., 2009; Sammons et al., 2015; Lu and Ng, 2016, 2017; Duncan et al., 2017), such as event attributes and types, into our event coreference resolution module using a contextdependent gate mechanism. Temporal Event Ordering. Temporal relations between events are extracted for neighbor events in one sentence (Ning et al., 2017, 2018a, 2019; Multimedia Information Extraction. Previous Han et al., 2019), ignoring the temporal dependenmultimedia IE systems (Li et al., 2020b; Yazici cies between events across sentences. We perform et al., 2018) only include cross-media entity coref- document-level event ordering and propagate temerence resolution by grounding the extracted visual poral attributes through shared arguments. Furtherentities to text. We are the first to perform cross- more, we take advantage of the schema repository media joint event extraction and coreference reso- knowledge by using the frequent temporal"
2021.naacl-demos.16,P18-1212,1,0.895436,"Missing"
2021.naacl-demos.16,D19-1642,1,0.844444,"Missing"
2021.naacl-demos.16,P18-1122,1,0.846572,"oss-document Temporal Event Ordering Based on the event coreference resolution component described above, we group all mentions into clusters. Next we aim to order events along a timeline. We follow Zhou et al. (2020) to design a component for temporal event ordering. Specifically, we further pre-train a T5 model (Raffel et al., 2020) with distant temporal ordering supervision signals. These signals are acquired through two set of syntactic patterns: 1) before/after keywords in text and 2) explicit date and time mentions. We take such a pre-trained temporal T5 model and finetune it on MATRES (Ning et al., 2018b) and use it as the system for temporal event ordering. We 2.4 Cross-document Cross-lingual Entity and perform pair-wise temporal relation classification Event Coreference Resolution for all event mention pairs in a documents. After extracting all mentions of entities and events, We further train an alternative model from finewe apply our cross-document cross-lingual entity tuning RoBERTa (Liu et al., 2019) on MATRES coreference resolution model, which is an exten(Ning et al., 2018b). This model has also been sucsion of the e2e-coref model (Lee et al., 2017). cessfully applied for event time"
2021.naacl-demos.16,P17-1178,1,0.849941,"he audio signal. It cific global feature. We compute the global feature returns the transcription with starting and ending score as uf , where u is a learnable weight vec3 https://www.darpa.mil/program/knowledge-di tor. Finally, we use a beam search-based decoder rected-artificial-intelligence-reasoning-overto generate the information graph with the highest schemas 4 https://aws.amazon.com/transcribe/ global score. After we extract these mentions, we 134 apply a syntactic parser (Honnibal et al., 2020) to extend mention head words to their extents. Then we apply a cross-lingual entity linker (Pan et al., 2017) to link entity mentions to WikiData (Vrandeˇci´c and Krötzsch, 2014)5 . 2.3 Document-level Event Argument Extraction The previous module can only operate on the sentence level. In particular, event arguments can often be found in neighboring sentences. To make up for this, we further develop a document-level event argument extraction model (Li et al., 2021) and use the union of the extracted arguments from both models as the final output. We formulate the argument extraction problem as conditional text generation. Our model can easily handle the case of missing arguments and multiple argument"
2021.naacl-demos.16,W15-0812,0,0.0200332,"11 139 1,213 Videos 31 Table 4: Data statistics for schema matching corpus (LDC2020E39). Schema-guided Information Extraction. The Category Extracted Schema Instantiated Events Steps Steps performance of each component is shown in Table 3. We evaluate the end-to-end perfor# 3,180 1,738 958 mance of our system on a complex event corTable 5: Results of schema matching. pus (LDC2020E39), which contains multi-lingual multi-media document clusters. The data statistics are shown in Table 4. We train our mention ex3.3 Qualitative Analysis traction component on ACE 2005 (Walker et al., 2006) and ERE (Song et al., 2015); document- Figure 2 illustrates a subset of examples for the best matched results from our end-to-end system. We level argument exraction on ACE 2005 (Walker 7 et al., 2006) and RAMS (Ebner et al., 2020); corefLDC2017E03 8 erence component on ACE 2005 (Walker et al., LDC2017E52 137 Extracted Graph Old Bailey A court in British legal history Max Hill Manchester Communicator Place JudgeCourt JudgeCourt Place JudgeCourt Broadcast ReleaseParole Resident ... ... ChargeIndict TrialHearing Defendant Sentence ArrestJailDetain Defendant Defendant Detainee Defendant ReleaseParole Defendant Salman Abedi"
2021.naacl-demos.16,W02-2024,0,0.567788,"Missing"
2021.naacl-demos.16,D19-1585,0,0.0252013,".. ... ChargeIndict TrialHearing Defendant Defendant Sentence Detainee ArrestJailDetain ReleaseParole Defendant Convict ReleaseParole Figure 2: The visualization of schema matching results from extracted graph and schema. The unmatched portions for both extracted graph and schema are blurred. can see that our system can extract events, entities and relations and align them well with the selected schema. The final instantiated schema is the hybrid of two graphs from merging the matched elements. 4 Related Work Text Information Extraction. Existing end-toend Information Extraction (IE) systems (Wadden et al., 2019; Li et al., 2020b; Lin et al., 2020; Li et al., 2019) mainly focus on extracting entities, events and entity relations from individual sentences. In contrast, we extract and infer arguments over the global document context. Furthermore, our IE system is guided by a schema repository. The extracted graph will be used to instantiate a schema graph, which can be applied to predict future events. Coreference Resolution. Previous neural models for event coreference resolution use noncontextual (Nguyen et al., 2016; Choubey et al., 2020; Huang et al., 2019) or contextual word representations (Lu et"
2021.naacl-demos.16,W12-4501,0,0.0999034,"Missing"
2021.naacl-demos.16,2021.naacl-main.6,1,0.690677,"Missing"
2021.naacl-demos.16,P18-4009,0,0.0278418,"lized system publicly available for research purpose at GitHub1 , with a demo video2 . 1 Introduction Event extraction and tracking technologies can help us understand real-world events described in the overwhelming amount of news data, and how they are inter-connected. These techniques have already been proven helpful in various application domains, including news analysis (Glavaš and Štajner, 2013; Glavaš et al., 2014; Choubey et al., 2020), aiding natural disaster relief efforts (Panem et al., 2014; Zhang et al., 2018; Medina Maza et al., 2020), financial analysis (Ding et al., 2014, 2016; Yang et al., 2018; Jacobs et al., 2018; Ein-Dor et al., 2019; Özbayoglu et al., 2020) and healthcare monitoring (Raghavan et al., 2012; Jagannatha and Yu, 2016; Klassen et al., 2016; Jeblee and Hirst, 2018). However, it’s much more difficult to remember event-related information compared to entityrelated information. For example, most people in 1 Github: https://github.com/RESIN-KAIROS/RESI N-pipeline-public 2 Video: http://blender.cs.illinois.edu/softwa re/resin/resin.mp4 the United States will be able to answer the question “Which city is Columbia University located in?”, but very few people can give a compl"
2021.naacl-demos.16,N18-5009,1,0.831121,"Missing"
2021.naacl-demos.8,W19-1909,0,0.0217165,"Missing"
2021.naacl-demos.8,2020.emnlp-demos.18,0,0.241329,"ung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated"
2021.naacl-demos.8,D19-1371,0,0.0246983,"89178)] To address the role of angiotensin in lung injury, there is an ongoing clinical trial to examine whether losartan treatment affects outcomes in COVID-19 associated ARDS (NCT04312009). [PMID:32439915 (PMC7242178)] Losartan was also the molecule chosen in two trials recently started in the United States by the University of Minnesota to treat patients with COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from te"
2021.naacl-demos.8,W17-2307,0,0.0224213,"posing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated with Evidence amlodipine and benazepril, and chronic back pain. Sentences [PMID:32081428 (PMC7092824)] On the other hand, many ACE inhibitors are cu"
2021.naacl-demos.8,W13-2001,0,0.0375856,"sults, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Disease"
2021.naacl-demos.8,N19-1145,1,0.895006,"Missing"
2021.naacl-demos.8,Q17-1008,0,0.0252127,"s Factor, and Interleukin-10. We see all of these connections in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many l"
2021.naacl-demos.8,2020.bionlp-1.22,0,0.0299439,"Missing"
2021.naacl-demos.8,D19-6204,1,0.833702,"ctions in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and"
2021.naacl-demos.8,2020.acl-demos.11,1,0.877148,"2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., biomedical scientists) to identify specific drug candidates and molecular targets that are relevant in their biomedical and clinical research aims. The use of our know"
2021.naacl-demos.8,W19-5006,0,0.0162201,"ults, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19"
2021.naacl-demos.8,2020.acl-main.230,1,0.885325,"Missing"
2021.naacl-demos.8,D19-1410,0,0.0136688,"nowledge elements in each path in the KG. Each edge is assigned a salience score by aggregating the scores of paths passing through it. In addition to knowledge elements, we also present related sentences and source information as evidence. We use BioBert (Lee et al., 2020), a pre-trained language model to represent each sentence along with its left and right neighboring sentences as local contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evid"
2021.naacl-demos.8,2020.bionlp-1.21,0,0.0201708,"igation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease"
2021.naacl-demos.8,D18-1230,1,0.819154,"Missing"
2021.naacl-demos.8,2020.bionlp-1.3,0,0.0196518,"h COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., bio"
2021.naacl-demos.8,W16-3104,0,0.0689349,"Missing"
2021.naacl-demos.8,P19-1191,1,0.889692,"Missing"
2021.naacl-demos.8,C14-1149,1,0.831567,"Missing"
2021.naacl-demos.8,2020.acl-demos.8,1,0.900567,"cal contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evidence from a background corpora of COVID-19. 4 4.1 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 CATB-10270 CATB-1418 CATB-1674 CATB-16A CATB-16D2 CATB-1852 CATB1874 CATB-2744 CATB-3098 CATB-348 CATB-3483 CATB-5880 CATB-84 CATB912 CATD CATHY CATK"
A83-1010,C69-0101,0,0.121408,"Missing"
A83-1010,T75-2001,0,\N,Missing
atkins-etal-2002-resources,2001.mtsummit-papers.39,1,\N,Missing
atkins-etal-2002-resources,bel-etal-2000-simple,1,\N,Missing
atkins-etal-2002-resources,calzolari-etal-2002-towards,1,\N,Missing
bhatia-etal-2010-empty,J05-1004,1,\N,Missing
bhatia-etal-2010-empty,I08-2099,1,\N,Missing
bonial-etal-2014-propbank,W04-2705,0,\N,Missing
bonial-etal-2014-propbank,N07-1069,1,\N,Missing
bonial-etal-2014-propbank,J93-2004,0,\N,Missing
bonial-etal-2014-propbank,W10-1810,1,\N,Missing
bonial-etal-2014-propbank,W04-0401,0,\N,Missing
bonial-etal-2014-propbank,P09-1033,0,\N,Missing
bonial-etal-2014-propbank,P06-1117,0,\N,Missing
bonial-etal-2014-propbank,J05-1004,1,\N,Missing
bonial-etal-2014-propbank,W13-2322,1,\N,Missing
brown-etal-2010-number,W04-0811,1,\N,Missing
brown-etal-2010-number,W99-0502,0,\N,Missing
brown-etal-2010-number,S07-1006,0,\N,Missing
brown-etal-2010-number,S07-1016,1,\N,Missing
brown-etal-2010-number,W02-0817,0,\N,Missing
brown-etal-2010-number,N06-2015,1,\N,Missing
brown-etal-2010-number,P06-1014,0,\N,Missing
brown-etal-2010-number,P07-1005,0,\N,Missing
brown-etal-2010-number,J08-4004,0,\N,Missing
brown-etal-2010-number,W07-1508,1,\N,Missing
brown-etal-2010-number,S01-1001,0,\N,Missing
C00-2148,W98-0106,0,0.0134125,"ency representation. 3.2 Semantics for TAGs There is a range of previous work in incorporating semantics into TAG trees. Stone and Doran (1997) describe a system used for generation that simultaneously constructs the semantics and syntax of a sentence using LTAGs. Joshi and Vijay-Shanker (1999), and Kallmeyer and Joshi (1999), describe the semantics of a derivation tree as a set of attachments of trees. The semantics of these attachments is given as a conjunction of formulae in a flat semantic representation. They provide a specific methodology for composing semantic representations much like Candito and Kahane (1998), where the directionality of dominance in the derivation tree should be interpreted according to the operations used to build it. Kallmeyer and Joshi also use a flat semantic representation to handle scope phenomena involving quantifiers. 4 Description of the verb lexicon VerbNet can be viewed in both a static and a dynamic way. The static aspect refers to the verb and class entries and how they are organized, providing the characteristic descriptions of a verb sense or a verb class (Kipper et al., 2000). The dynamic aspect of the lexicon constrains the entries to allow a compositional interp"
C00-2148,C96-1034,0,0.0190468,"etail is necessary for applications such as animation of natural language instructions (Bindiganavale et al., 2000). Another important contribution of this work is that by dividing each event into a tripartite structure, we permit a more precise definition of the associated semantics. Finally, the operation of adjunction in TAGs provides a principled approach to representing the type of regular polysemy that has been a major obstacle in building verb lexicons. Researching whether a TAG grammar for VerbNet can be automatically constructed by using development tools such as Xia et al. (1999) or Candito (1996) is part of our next step. We also expect to be able to factor out some class-specific auxiliary trees to be used across several verb classes. 6 Conclusion 7 Acknowledgments We have presented a class-based approach to building a verb lexicon that makes explicit and implements the close association between syntax and semantics, as postulated by Levin. The power of the lexicon comes from its dynamic aspect that is based The authors would like to thank the anonymous reviewers for their valuable comments. This research was partially supported by NSF grants IIS-9800658 and IIS-9900297 and CAPES gra"
C00-2148,P98-1046,1,0.865835,"ion of adjunction in LTAGs provides a mechanism for extending verb senses. 2 Levin classes Levin verb classes are based on the ability of a verb to occur in diathesis alternations, which are pairs of syntactic frames that are in some sense meaning preserving. The fundamental assumption is that the syntactic frames are a direct reflection of the underlying semantics. However, Levin classes exhibit inconsistencies that have hampered researchers’ ability to reference them directly in applications. Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. Dang et al. (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes. We represent these verb classes and their regular sense extensions in the LTAG formalism. 3 Lexicalized Tree Adjoining Grammars 3.1 Overview of formalism Lexicalized Tree Adjoining Grammars consist of a finite set of initial and auxiliary elementary trees, and two operations to combine them. The minimal, non-recursive linguistic structures of a language, su"
C00-2148,W97-0204,0,0.080107,"s. We have used Lexicalized Tree Adjoining Grammars to capture the syntax associated with each verb class and have augmented the trees to include selectional restrictions. In addition, semantic predicates are associated with each tree, which allow for a compositional interpretation. 1 Introduction The difficulty of achieving adequate hand-crafted semantic representations has limited the field of natural language processing to applications that can be contained within well-defined sub-domains. Despite many different lexicon development approaches (Mel’cuk, 1988; Copestake and Sanfilippo, 1993; Lowe et al., 1997), the field has yet to develop a clear consensus on guidelines for a computational lexicon. One of the most controversial areas in building such a lexicon is polysemy: how senses can be computationally distinguished and characterized. We address this problem by employing compositional semantics and the adjunction of syntactic phrases to support regular verb sense extensions. This differs from the Lexical Conceptual Structure (LCS) approach exemplified by Voss (1996), which requires a separate LCS representation for each possible sense extension. In this paper we describe the construction of Ve"
C00-2148,J88-2003,0,0.609367,"in classes, which is facilitated by using intersective Levin classes. 5 Compositional Semantics We use TAG elementary trees for the description of allowable frames and associate semantic predicates with each tree, as was done by Stone and Doran. The semantic predicates are primitive enough so that many may be reused in different trees. By using TAGs we get the additional benefit of an existing parser that yields derivations and derived trees from which we can construct the compositional semantics of a given sentence. We decompose each event E into a tripartite structure in a manner similar to Moens and Steedman (1988), introducing a time function for each predicate to specify whether the predicate is true in the preparatory (during (E )), culmination (end(E )), or consequent (result(E )) stage of an event. Initial trees capture the semantics of the basic senses of verbs in each class. For example, many 1 These restrictions are more like preferences that generate a preferred reading of a sentence. They may be relaxed depending on the domain of a particular application. [ S event=E NParg1 # [ +animate ] ] [ S event=E2 [ VP event=E ] V run NParg 0 # [ +animate ] ] [ VP event=E1 V run ] NParg 1 # [ +animate ]"
C00-2148,P97-1026,0,0.0286981,"ituted or adjoined in at appropriate places in the skeleton tree in the course of the derivation. The composition of trees during parsing is recorded in a derivation tree. The derivation tree nodes correspond to lexically anchored elementary trees, and the arcs are labeled with information about how these trees were combined to produce the parse. Since each lexically anchored initial tree corresponds to a semantic unit, the derivation tree closely resembles a semantic-dependency representation. 3.2 Semantics for TAGs There is a range of previous work in incorporating semantics into TAG trees. Stone and Doran (1997) describe a system used for generation that simultaneously constructs the semantics and syntax of a sentence using LTAGs. Joshi and Vijay-Shanker (1999), and Kallmeyer and Joshi (1999), describe the semantics of a derivation tree as a set of attachments of trees. The semantics of these attachments is given as a conjunction of formulae in a flat semantic representation. They provide a specific methodology for composing semantic representations much like Candito and Kahane (1998), where the directionality of dominance in the derivation tree should be interpreted according to the operations used"
C00-2148,C98-1046,1,\N,Missing
C02-1143,S01-1005,1,\N,Missing
C02-1143,J96-3004,0,\N,Missing
C02-1143,J98-1001,0,\N,Missing
C02-1143,P97-1003,0,\N,Missing
C02-1143,W00-1201,0,\N,Missing
C02-1143,J96-1002,0,\N,Missing
C02-1143,W02-0813,1,\N,Missing
C02-1143,S01-1001,0,\N,Missing
C02-1143,A97-1029,0,\N,Missing
C02-1145,P00-1058,0,0.0194968,"Missing"
C02-1145,W00-1207,0,0.0132515,"Missing"
C02-1145,xia-etal-2000-developing,1,0.653451,"is the corpus applicable, and finally (iv) what future work we anticipate. Introduction The Penn Chinese Treebank (CTB) is an ongoing project, with its objective being to create a segmented Chinese corpus annotated with POS tags and syntactic brackets. The first installment of the project (CTB-I) consists of Xinhua newswire between the years 1994 and 1998, totaling 100,000 words, fully segmented, POS-tagged and syntactically bracketed and it has been released to the public via the Penn Linguistic Data Consortium (LDC). The preliminary results of this phase of the project have been reported in Xia et al (2000). Currently the second installment of the project, the 400,000-word CTB-II is being developed and is expected to be completed early in the year 2003. CTB-II will follow the standards set up in the segmentation (Xia 2000b), POS tagging (Xia 2000a) and bracketing guidelines (Xue and Xia 2000) and it will use articles from Peoples&apos; Daily, Hong Kong newswire and material translated into Chinese from other languages in addition to the Xinhua newswire used in CTB-I in an effort to diversify the sources. The availability of CTB-I changed our approach to CTB-II considerably. Due to the existence of CT"
C02-1145,W96-0213,0,\N,Missing
C02-1145,H01-1026,1,\N,Missing
C16-1125,C12-1026,0,0.0435096,"Missing"
C16-1125,J13-1009,0,0.0656823,"Missing"
C16-1125,E95-1014,0,0.220266,"ata consists of nouns that alternate with more than one light verb. These alternations show that a collocational measure that only looks at the bigram occurrences may not be able to capture a noun-light verb alternation that is relatively infrequent. Therefore, linguistic information would be required to identify a predicating nominal that appears in a number of contexts. In the following section, we propose new features that could help capture this information. 3 Linguistic features used for LVCs English LVC identification focuses on extracting linguistic features for LVCs (Tan et al., 2006; Grefenstette and Teufel, 1995; Stevenson et al., 2004). For example, the morpho-syntactic similarity between nominal predicates and their verbal counterparts (e.g. walk and take a walk) is often exploited. Other cues include the presence of indefinite determiners (such as a) before the nominal predicate. Tu and Roth (2011) look at both statistical and linguistic contexts to detect English complex predicates. Among their local linguistic features, they utilize bigram information about the nominal head and light verb, the nouns themselves and the Levin verb class members of deverbal nouns. In a more recent study, Chen et al"
C16-1125,2011.mtsummit-papers.23,0,0.0475751,"apers, pages 1320–1329, Osaka, Japan, December 11-17 2016. Butt (2010) notes that light verbs in Hindi LVCs act as a verbalizers in order to create new predicates and incorporate borrowed items into the language e.g. email kar ‘email do; email’. Therefore, LVCs are sometimes described as “a preferred way of augmenting the creative potential of the language” (Kachru, 2006, pp 93). The identification of LVCs in Hindi (as well as other South Asian languages) is an important NLP task, which has been shown to improve parsing accuracy (Begum et al., 2011) as well as machine translation performance (Pal et al., 2011). The detection of multi-words such as LVCs has been widely studied and association measures, linguistic knowledge and parallel corpora have been used. As LVCs are a type of multiword, a commonly used method is ‘N-gram classification’ (Green et al., 2013). This strategy extracts n-grams from the corpus, filters them and assigns some values based on a bigram measures such as log-likelihood or mutual information. A classifier is then used to make a LVC/non-LVC decision. However, previous work has shown that LVCs benefit from the use of linguistic features for identification. Vincze et al. (2011)"
C16-1125,W15-5943,0,0.0312559,"Y Y Y Y Y Begum et al’11 (Hin) Y Y Y Y Table 1: Commonly used linguistic features for English and Hindi LVC detection. 1323 The work for Hindi LVC detection makes use of a similar set of linguistic features. Begum et al. (2011) look for the presence of postpositions and demonstratives, which preferentially do not occur with a noun that is a part of an LVC. Like Tu and Roth (2011), they use the verb-object bigram and the noun class information from Hindi WordNet (Narayan et al., 2002). They have achieved an accuracy of around 0.85 for identification of Hindi complex predicates. More recently, Singh et al. (2015) have compared word embeddings and WordNet-based measures to detect Hindi noun compounds and LVCs. While word embeddings are effective for compounds, they perform poorly for LVCs, suggesting the importance of more precise linguistic features. 3.1 Linguistic features for Hindi Our linguistic analysis of LVCs indicates that the properties of both noun and light verb are crucial as features for identifying LVCs. In the previous section, we described some of the commonly used features for LVC identification. Table 1 shows some of these: the presence of post-positions after the predicating noun, co"
C16-1125,W04-0401,0,0.0511707,"rnate with more than one light verb. These alternations show that a collocational measure that only looks at the bigram occurrences may not be able to capture a noun-light verb alternation that is relatively infrequent. Therefore, linguistic information would be required to identify a predicating nominal that appears in a number of contexts. In the following section, we propose new features that could help capture this information. 3 Linguistic features used for LVCs English LVC identification focuses on extracting linguistic features for LVCs (Tan et al., 2006; Grefenstette and Teufel, 1995; Stevenson et al., 2004). For example, the morpho-syntactic similarity between nominal predicates and their verbal counterparts (e.g. walk and take a walk) is often exploited. Other cues include the presence of indefinite determiners (such as a) before the nominal predicate. Tu and Roth (2011) look at both statistical and linguistic contexts to detect English complex predicates. Among their local linguistic features, they utilize bigram information about the nominal head and light verb, the nouns themselves and the Levin verb class members of deverbal nouns. In a more recent study, Chen et al. (2015) have described a"
C16-1125,W14-5501,1,0.853407,"c features for Hindi Our linguistic analysis of LVCs indicates that the properties of both noun and light verb are crucial as features for identifying LVCs. In the previous section, we described some of the commonly used features for LVC identification. Table 1 shows some of these: the presence of post-positions after the predicating noun, collocational features and lexical features. In this section, we introduce two new features that are based on our study of Hindi LVCs. The first is based on the idea that there are semantic constraints on the combination of a particular noun and light verb. Sulger and Vaidya (2014) examined the combinatorial properties of noun and light verb based on relative frequency of occurrence. They found that a light verb such as de ‘give’ is likelier to combine with nouns that have a ‘transfer’ property, whereas nouns that occur with kar ‘do’ will occur with nouns that describe actions with animate agents. Light verb ho ‘be’ often appears with stative nouns or those that indicate mental states. In order to capture these properties, we used a feature that associated a light verb with the ontological property of the noun that is likely to occur with it. For example kar was associa"
C16-1125,W06-2407,0,0.0594999,"out 1/4th of the data consists of nouns that alternate with more than one light verb. These alternations show that a collocational measure that only looks at the bigram occurrences may not be able to capture a noun-light verb alternation that is relatively infrequent. Therefore, linguistic information would be required to identify a predicating nominal that appears in a number of contexts. In the following section, we propose new features that could help capture this information. 3 Linguistic features used for LVCs English LVC identification focuses on extracting linguistic features for LVCs (Tan et al., 2006; Grefenstette and Teufel, 1995; Stevenson et al., 2004). For example, the morpho-syntactic similarity between nominal predicates and their verbal counterparts (e.g. walk and take a walk) is often exploited. Other cues include the presence of indefinite determiners (such as a) before the nominal predicate. Tu and Roth (2011) look at both statistical and linguistic contexts to detect English complex predicates. Among their local linguistic features, they utilize bigram information about the nominal head and light verb, the nouns themselves and the Levin verb class members of deverbal nouns. In"
C16-1125,W11-0807,0,0.0931683,", a commonly used method is ‘N-gram classification’ (Green et al., 2013). This strategy extracts n-grams from the corpus, filters them and assigns some values based on a bigram measures such as log-likelihood or mutual information. A classifier is then used to make a LVC/non-LVC decision. However, previous work has shown that LVCs benefit from the use of linguistic features for identification. Vincze et al. (2011) used bigram association measures for English nounnoun compounds and LVCs. They found that LVC detection improves when linguistic features are used in addition to n-gram information. Tu and Roth (2011) showed that linguistic and statistical features perform at par for English LVC detection. For Hindi, we expect that linguistic features will be useful for automatic detection. At the same time, the productivity and range of LVC constructions in Hindi result in some specific challenges. In the next section, we carry out a linguistic analysis of LVCs based on the annotations in the Hindi Treebank. We use these insights to propose two new features to identify LVCs. Following this, we describe our experimental setup and then discuss the results. 2 Linguistic challenges for Hindi The linguistic no"
C16-1125,W11-0817,0,0.025624,"e (Pal et al., 2011). The detection of multi-words such as LVCs has been widely studied and association measures, linguistic knowledge and parallel corpora have been used. As LVCs are a type of multiword, a commonly used method is ‘N-gram classification’ (Green et al., 2013). This strategy extracts n-grams from the corpus, filters them and assigns some values based on a bigram measures such as log-likelihood or mutual information. A classifier is then used to make a LVC/non-LVC decision. However, previous work has shown that LVCs benefit from the use of linguistic features for identification. Vincze et al. (2011) used bigram association measures for English nounnoun compounds and LVCs. They found that LVC detection improves when linguistic features are used in addition to n-gram information. Tu and Roth (2011) showed that linguistic and statistical features perform at par for English LVC detection. For Hindi, we expect that linguistic features will be useful for automatic detection. At the same time, the productivity and range of LVC constructions in Hindi result in some specific challenges. In the next section, we carry out a linguistic analysis of LVCs based on the annotations in the Hindi Treebank."
C18-1224,W17-2712,1,0.897238,"ular annotated data resource are spelled out in various “linking models” that import both the reference ontology and a resource ontology. In the example shown, REO Discharge events map to the Releasing frame in FrameNet and the ReleaseParole event type in ERE. However, other mappings between ERE and FN are necessarily more indirect. With respect to the Communication node in REO, ERE/ACE only maps to instrument communication and statement while FN has mappings to nearly all daughters, as does VN, and VN maps to the mother Communication node as well. Additional indirect mappings are detailed in Brown et al. (2017). Individual words within the resource classes can be detected in text to find a wide variety of each event type, or one can query to view its participants and its relations to other events that is independent of the various lexical resource schemas. 4 Related Work In considering how to best capture the type of information desired for relating E NDURANTS to P ER DURANTS , we have examined the approaches of other benchmark ontologies. We find that the Basic Formal Ontology (BFO) (Smith et al., 2014) and the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) (Masolo et al., 20"
C18-1224,doddington-etal-2004-automatic,0,0.100163,"ent for Heating, through which the PreparedFood gets heated. 3.2 REO (Rich Event Ontology) The Rich Event Ontology (REO) unifies existing SRL schemas by providing an independent conceptual backbone through which they can be associated, and it augments the schemas with event-to-event causal and temporal relations. The ontology was developed in response to unsuccessful efforts to map directly between FrameNet (Fillmore et al., 2002) and the small set of disparate event types in Rich Entities, Relations and Events (ERE) (Song et al., 2015) (originally based on Automatic Content Extraction (ACE) (Doddington et al., 2004)). The difficulty was mainly due to differences in the granularity of events described by the FrameNet frames and ERE event types and inconsistencies in how the resources divided the semantic space. FrameNet, ERE, and VerbNet (Schuler, 2005) have wide-coverage lexicons having to do with events, and they contribute annotated corpora and additional semantic and syntactic information that can be crucial to identifying events and their participants. REO serves as a shared hub for the disparate annotation schemas and therefore enables the combination of SRL training data into a larger, more diverse"
C18-1224,fillmore-etal-2002-framenet,0,0.132634,"lates the purpose of a Microwave using the hasPurpose relation, with a formula that can translate to: there is an entity PreparedFood and a process Heating, and the Microwave is an instrument for Heating, through which the PreparedFood gets heated. 3.2 REO (Rich Event Ontology) The Rich Event Ontology (REO) unifies existing SRL schemas by providing an independent conceptual backbone through which they can be associated, and it augments the schemas with event-to-event causal and temporal relations. The ontology was developed in response to unsuccessful efforts to map directly between FrameNet (Fillmore et al., 2002) and the small set of disparate event types in Rich Entities, Relations and Events (ERE) (Song et al., 2015) (originally based on Automatic Content Extraction (ACE) (Doddington et al., 2004)). The difficulty was mainly due to differences in the granularity of events described by the FrameNet frames and ERE event types and inconsistencies in how the resources divided the semantic space. FrameNet, ERE, and VerbNet (Schuler, 2005) have wide-coverage lexicons having to do with events, and they contribute annotated corpora and additional semantic and syntactic information that can be crucial to ide"
C18-1224,J91-4003,0,0.760001,"a is used to keep the rain off of you, that bread is baked, or that dandelions are made up of leaves, stems, and flowers. Commonsense, real-world knowledge about the events that entities or “things in the world” are typically involved in, as well as part-whole relationships, is valuable for allowing computational systems to draw everyday inferences about the world. Here, we describe our approach for automatically extracting this information from the Suggested Upper Merged Ontology (SUMO) (Niles and Pease, 2001a; Pease, 2011). We assume the theoretical framework of the Generative Lexicon (GL) (Pustejovsky, 1991), as we find GL qualia relations useful for considering the typical involvement of a particular entity in events. Thus, we focus on relationships between entities and events captured in a set of qualia associated with an entity. We find that human annotators asked to evaluate the quality and accuracy of the information extracted overwhelmingly find the origins, functions and part-whole relationships proposed to be reasonable. One of our motivations is to add the extracted qualia relations to an ontology hub of Natural Language Processing (NLP) semantic role labeling (SRL) resources, the Rich E"
C18-1224,W15-0812,0,0.0157424,"n entity PreparedFood and a process Heating, and the Microwave is an instrument for Heating, through which the PreparedFood gets heated. 3.2 REO (Rich Event Ontology) The Rich Event Ontology (REO) unifies existing SRL schemas by providing an independent conceptual backbone through which they can be associated, and it augments the schemas with event-to-event causal and temporal relations. The ontology was developed in response to unsuccessful efforts to map directly between FrameNet (Fillmore et al., 2002) and the small set of disparate event types in Rich Entities, Relations and Events (ERE) (Song et al., 2015) (originally based on Automatic Content Extraction (ACE) (Doddington et al., 2004)). The difficulty was mainly due to differences in the granularity of events described by the FrameNet frames and ERE event types and inconsistencies in how the resources divided the semantic space. FrameNet, ERE, and VerbNet (Schuler, 2005) have wide-coverage lexicons having to do with events, and they contribute annotated corpora and additional semantic and syntactic information that can be crucial to identifying events and their participants. REO serves as a shared hub for the disparate annotation schemas and"
C18-1313,W13-2322,1,0.958139,"s one way of capturing document level semantics, by annotating coreference, implicit roles and bridging relations on top of gold Abstract Meaning Representations of sentence-level semantics. We present the methodology of developing this corpus, alongside analysis of its quality and a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) illustrates a feasible approach to developing rich representations of document meaning, useful for tasks such as information extraction and question answering. 1 Introduction Although Abstract Meaning Representation (AMR) (Banarescu et al., 2013) shows promise for a range of tasks such as summarization (Liu et al., 2015; Viet et al., 2017) and information extraction (Garg et al., 2016), it is restricted to capturing the semantics of individual sentences. For many purposes, when examining the semantics of a document, one also needs access to cross-sentence information such as coreference. We suggest that the AMR approach to semantic representation has useful characteristics for an extension to discourse-level representations. AMR represents sentence meaning in a simple, readable semantic graph, such that annotators may directly mark co"
C18-1313,W16-1004,1,0.894831,"Missing"
C18-1313,L18-1266,1,0.839029,"checked against speaker metadata to confirm that annotators were properly keeping track of discourse 3695 Files AMRs Tokens Coreference Chains Implicit Roles Bridging Relations Train 284 7826 122000 3810 2386 1792 Test 9 201 3700 87 67 54 Double Annotation 43 588 8200 381 371 160 Table 1: Basic statistics about size of the MS-AMR corpus participants, and certain highly anaphoric elements (such as “he” and “she”) flagged whenever not annotated as anaphoric. As the AMR corpus was also being corrected and revised during this annotation to improve predicate coverage and treatment of comparatives (Bonial et al., 2018), annotations were stored with their concept labels and double-checked for changes in the underlying AMR. Final data (included in an upcoming AMR public release) includes a description of each AMR document (as defined by LDC segmentations of multi-thread discourse into tractable discussions), defined as an ordered list of AMRs identified by their IDs. Each coreference cluster identifies explicit mentions by the ID within the normal AMR release and the variable within that AMR; implicit roles are identified by the identity of the predicate they are an argument of, with a label for the numbered"
C18-1313,P13-2131,1,0.941548,"tence, we separately annotated two multi-sentence AMR annotations for that same document, each annotated on top of a different set of AMRs. The challenges in evaluating this kind of agreement presage the challenges of measuring the quality of system-predicted MS-AMR. In traditional annotations of coreference over surface forms, one can assume that two mentions are identical when they refer to the same span of text. However, with two separate AMRs, one cannot directly infer whether two mentions reference the same span, but must determine a mapping between the variables of each AMR. The SMATCH (Cai and Knight, 2013) algorithm, used for evaluating AMRs, calculates such a mapping. Using SMATCH to align those AMRs and coreference scores from the reference implementation of the scoring metrics (Pradhan et al., 2014b), we find the CoNLL-2012 F1 to be 66.72. Such a score is limited in that it was measured over a very small exploration set (40 AMRs), but it provides some preliminary suggestion that the identical underlying AMRs used in the IAA numbers above are not dramatically inflating the inter-annotator agreement. 3.3 Implicit Role Agreement Implicit roles have been annotated in prior corpora, but those ann"
C18-1313,N13-3004,0,0.0802189,"an be added to coreference clusters just like any other variable. This is similar to prior works in implicit role annotation (Gerber and Chai, 2010) in that we are using semantic role inventories to prompt annotators with possible implicit roles, while adding the innovation of fitting this within a coreference task. However, while previous annotations prompted annotators with an implicit role and asked them to look through prior text for its referents, this annotation fits implicit roles into the task of coreference labeling. An example of the actual act of annotation with the Anafora toolkit(Chen and Styler, 2013) for these additional implicit role options can be seen in Figure 2. This annotation also labels some examples of “bridging” coreference relations (Clark, 1977; Poesio et al., 1997). We annotate two more common bridging relations, part/whole relations (as in example 1) and set/member relations (as in example 2), with a focus upon those between named entities and common nouns. 1. I think this shows that pretty much every President can do any design thing they want with both the 3694 Figure 2: Annotation interface, illustrating implicit role links. Annotators click on boxes within the AMR (left)"
C18-1313,D16-1245,0,0.018099,"r mental state verb – such as the person being interested in “that’s interesting”, or the cause of “I laughed out loud”. A similar issue involved the recipient or listener role for verbs like “say-01” or “ask-01”, which can sometimes be inferable from context but are low in prominence. 4 4.1 Measuring MS-AMR similarity — scoring system performance A baseline implementation We present a simple baseline that hints at a lower bound for the task. We use the publicly available version of the Brandeis transition-based AMR parser (Wang et al., 2016) combined with an off-the-shelf coreference system (Clark and Manning, 2016) using an AMR-to-surface-form aligner (Flanigan et al., 2014) to convert the surface coreference to links between AMR nodes. 4.2 Simple evaluation of system prediction As mentioned in section 3.2, one hurdle in evaluating system predictions comes from the absence of clearly alignable “mentions” for use in coreference metrics. We can use the SMATCH metric on each individual sentence within an AMR document to resolve this, as calculating a SMATCH score involves determining the highest scoring alignment between variables within a system prediction and a gold AMR. We can then score against that ma"
C18-1313,cmejrek-etal-2004-prague,0,0.084218,"Missing"
C18-1313,P14-1134,0,0.090278,"hat’s interesting”, or the cause of “I laughed out loud”. A similar issue involved the recipient or listener role for verbs like “say-01” or “ask-01”, which can sometimes be inferable from context but are low in prominence. 4 4.1 Measuring MS-AMR similarity — scoring system performance A baseline implementation We present a simple baseline that hints at a lower bound for the task. We use the publicly available version of the Brandeis transition-based AMR parser (Wang et al., 2016) combined with an off-the-shelf coreference system (Clark and Manning, 2016) using an AMR-to-surface-form aligner (Flanigan et al., 2014) to convert the surface coreference to links between AMR nodes. 4.2 Simple evaluation of system prediction As mentioned in section 3.2, one hurdle in evaluating system predictions comes from the absence of clearly alignable “mentions” for use in coreference metrics. We can use the SMATCH metric on each individual sentence within an AMR document to resolve this, as calculating a SMATCH score involves determining the highest scoring alignment between variables within a system prediction and a gold AMR. We can then score against that mapping. The simple baseline outlined above was evaluated accor"
C18-1313,P10-1160,0,0.418977,"e within-sentence AMR has all predicates annotated with PropBank senses, we have access to the lexicon with a list of all numbered arguments we might expect for that predicate. We can therefore produce a list of the numbered arguments that are not explicitly filled in the text and show these unfilled roles to annotators. These unfilled roles are temporarily added during annotation – as is illustrated with the arg3 and arg4 of the predicate “arrive01” in Figure 1 – and can be added to coreference clusters just like any other variable. This is similar to prior works in implicit role annotation (Gerber and Chai, 2010) in that we are using semantic role inventories to prompt annotators with possible implicit roles, while adding the innovation of fitting this within a coreference task. However, while previous annotations prompted annotators with an implicit role and asked them to look through prior text for its referents, this annotation fits implicit roles into the task of coreference labeling. An example of the actual act of annotation with the Anafora toolkit(Chen and Styler, 2013) for these additional implicit role options can be seen in Figure 2. This annotation also labels some examples of “bridging” c"
C18-1313,J12-4003,0,0.138305,"erence, but only cover a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003),"
C18-1313,N15-1114,0,0.360852,"it roles and bridging relations on top of gold Abstract Meaning Representations of sentence-level semantics. We present the methodology of developing this corpus, alongside analysis of its quality and a plausible baseline for comparison. It is hoped that this Multi-Sentence AMR corpus (MS-AMR) illustrates a feasible approach to developing rich representations of document meaning, useful for tasks such as information extraction and question answering. 1 Introduction Although Abstract Meaning Representation (AMR) (Banarescu et al., 2013) shows promise for a range of tasks such as summarization (Liu et al., 2015; Viet et al., 2017) and information extraction (Garg et al., 2016), it is restricted to capturing the semantics of individual sentences. For many purposes, when examining the semantics of a document, one also needs access to cross-sentence information such as coreference. We suggest that the AMR approach to semantic representation has useful characteristics for an extension to discourse-level representations. AMR represents sentence meaning in a simple, readable semantic graph, such that annotators may directly mark coreference relations upon the AMR graph itself. AMR also annotates implicit"
C18-1313,H05-1004,0,0.0566913,"ease. 3.1 Coreference Chain Quality When dealing with two MS-AMR annotations done over the same set of gold AMRs – as with interannotator agreement data here – we can treat a given variable in each AMR as a possible “mention”, just as one might treat a span of text in a document. With that assumption, we may therefore measure MS-AMR IAA coreference scores using standard means of measuring coreference quality. One current approach – used because it is the standard for scoring coreference systems – is to use an average of the BCUB (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF-E (Luo, 2005) metrics, referred to as the “CoNLL-2012 F1 score” (calculated using the reference implementation of Pradhan et al (2014a)). Under those assumptions, the annotations get a CoNLL-2012 F1 of 69.86, using the reference implementation for these scores. For comparison, more traditional span-based coreference annotations (O’Gorman et al., 2016) found inter-annotator agreement of 65.5 for event F1, and 70.4 for entity F1 (CoNLL F1 score). This is therefore in the rough range of where one might expect human performance to be. However, this does not have actual comparability to other annotation schemes"
C18-1313,meyers-etal-2004-annotating,0,0.0250154,"of connected graphs, rather than many different layers of annotation. Annotations such as ACE and ERE also capture roles and entity annotations alongside coreference, but only cover a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional co"
C18-1313,miltsakaki-etal-2004-penn,0,0.19873,"Missing"
C18-1313,W13-0211,0,0.0153168,"a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003), SDRT (Baldridge et"
C18-1313,W09-3017,0,0.0456552,"Missing"
C18-1313,W16-5706,1,0.924886,"Missing"
C18-1313,J05-1004,1,0.754767,"name :op1 “Paris”) He arrived at noon (a / arrive-01 : ARG 1 (h / he) : ARG 3 (i / implicit role: start point) : ARG 4 (i2 / implicit role: end point; destination) : TIME (d / date-entity :dayperiod (n3 / noon))) Figure 1: Example of MS-AMR annotation; annotators link coreferent variables (such as marking a relation between between p and h (in red)) and implicit roles, here linking the destination (arg4) in the second sentence to the previous variable c (in blue) of how that meaning was expressed. AMRs capture basic representations of semantic roles (using the numbered arguments of PropBank (Palmer et al., 2005)), as well as within-sentence coreference, named entity and entity linking information. Thus, a simple sentence such as “Bill left for Paris” can be represented as in the first AMR in Figure 1, with “leave-11” denoting the physical departure sense of “leave”, numbered arguments such as “arg2” denoting semantic roles that are unique to that sense (e.g. for leave11, arg2 is the destination), and “Bill” and “Paris” both having named entity labels (person and city) as well as links to Wikipedia when possible. One can therefore think of an AMR as a graph of the meaning of a sentence, with “variable"
C18-1313,W97-1301,0,0.0174545,"inventories to prompt annotators with possible implicit roles, while adding the innovation of fitting this within a coreference task. However, while previous annotations prompted annotators with an implicit role and asked them to look through prior text for its referents, this annotation fits implicit roles into the task of coreference labeling. An example of the actual act of annotation with the Anafora toolkit(Chen and Styler, 2013) for these additional implicit role options can be seen in Figure 2. This annotation also labels some examples of “bridging” coreference relations (Clark, 1977; Poesio et al., 1997). We annotate two more common bridging relations, part/whole relations (as in example 1) and set/member relations (as in example 2), with a focus upon those between named entities and common nouns. 1. I think this shows that pretty much every President can do any design thing they want with both the 3694 Figure 2: Annotation interface, illustrating implicit role links. Annotators click on boxes within the AMR (left) to add them to coreference chains (full chains shown on the right), as with the link between the implicit topic (“i2”) and the earlier “l / lie” mention. Residence and the Office W"
C18-1313,P04-1019,0,0.0156849,"uns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003), SDRT (Baldridge et al., 2007), GraphBank (Wolf et al., 2004) or PDTB (Miltsakaki et al., ) therefore capture dimensions of meaning that differ from the propositional content captured in this corpus. 3 Measuring MS-AMR Corpus Q"
C18-1313,poesio-artstein-2008-anaphoric,0,0.146342,"Missing"
C18-1313,W11-1901,1,0.913919,"o “the White House”) 2. I also liked “Deception point”. So I have read all four of his books and enjoyed them. (set/member relation) Those examples illustrate the emphasis upon capturing part/whole and set/member relations that require contextual understanding; annotators were instructed not to link part/whole relations that are only knowable through world knowledge, specifically those between named, wikified entities (such as knowing that Damascus is part of Syria). This annotation also captures more event coreference phenomena than what is captured in OntoNotesstyle coreference annotations (Pradhan et al., 2011). While those prior annotations focused upon nominal coreference, capturing verbal mentions only occasionally (when they were coreferent with a nominal mention), multi-sentence AMR annotators were instructed to link together coreferent variables regardless of their part of speech. Furthermore, because of the AMR normalization of surface-form variation, complex details regarding how to represent an event (such as the span to use for light verbs) is already normalized into single PropBank rolesets during AMR annotation. Annotation guidelines are publicly available at https://github.com/timjogorm"
C18-1313,P14-2006,0,0.241317,"s – as with interannotator agreement data here – we can treat a given variable in each AMR as a possible “mention”, just as one might treat a span of text in a document. With that assumption, we may therefore measure MS-AMR IAA coreference scores using standard means of measuring coreference quality. One current approach – used because it is the standard for scoring coreference systems – is to use an average of the BCUB (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF-E (Luo, 2005) metrics, referred to as the “CoNLL-2012 F1 score” (calculated using the reference implementation of Pradhan et al (2014a)). Under those assumptions, the annotations get a CoNLL-2012 F1 of 69.86, using the reference implementation for these scores. For comparison, more traditional span-based coreference annotations (O’Gorman et al., 2016) found inter-annotator agreement of 65.5 for event F1, and 70.4 for entity F1 (CoNLL F1 score). This is therefore in the rough range of where one might expect human performance to be. However, this does not have actual comparability to other annotation schemes, as this data uses both event and entity coreference, and does not encompass within-sentence coreference (which is alre"
C18-1313,L18-1058,0,0.0215915,"ng and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 2008; O’Gorman et al., 2016; Roesiger, 2018) and resulted in systems (Poesio et al., 2004) capable of predicting such relations. Some bridging relations were also captured by within-sentence AMR annotations, encoded by the “include-91” relation. While MS-AMR focuses upon capturing the propositional content of a document, it does not capture other dimensions of discourse annotation, such as rhetorical structure. Corpora such as RST (Carlson 3696 et al., 2003), SDRT (Baldridge et al., 2007), GraphBank (Wolf et al., 2004) or PDTB (Miltsakaki et al., ) therefore capture dimensions of meaning that differ from the propositional content captur"
C18-1313,S10-1008,1,0.841856,"Missing"
C18-1313,W15-0812,0,0.026372,"emantic data and coreference have been represented in the multiple layers of the OntoNotes corpus (Pradhan et al., 2011) and the Prague Czech-English Dependency ˇ Treebank (PCEDT) (Cmejrek et al., 2004). This MS-AMR data differs primarily in that the data is more directly joined into a single set of connected graphs, rather than many different layers of annotation. Annotations such as ACE and ERE also capture roles and entity annotations alongside coreference, but only cover a small portion of the total semantics of a document, filtering only the elements relevant to a task-specific ontology (Song et al., 2015; Bies et al., 2016). Li Song and Xue (2018) annotates dropped pronouns over Chinese AMR annotators, but only deals with implicit roles in specific constructions. Annotations independent of AMR have provided implicit role annotations with PropBank or NomBank(Palmer et al., 2005; Meyers et al., 2004) semantic roles, but both resources were limited to a small inventory of 5-10 predicate types, rather than all implicit arguments in a text(Gerber and Chai, 2012; Moor et al., 2013). Bridging information has also been annotated in a range of recent corpora (Nedoluzhko et al., 2009; Poesio et al., 20"
C18-1313,M95-1005,0,0.120834,"” column, and provided in the release. 3.1 Coreference Chain Quality When dealing with two MS-AMR annotations done over the same set of gold AMRs – as with interannotator agreement data here – we can treat a given variable in each AMR as a possible “mention”, just as one might treat a span of text in a document. With that assumption, we may therefore measure MS-AMR IAA coreference scores using standard means of measuring coreference quality. One current approach – used because it is the standard for scoring coreference systems – is to use an average of the BCUB (Bagga and Baldwin, 1998), MUC (Vilain et al., 1995) and CEAF-E (Luo, 2005) metrics, referred to as the “CoNLL-2012 F1 score” (calculated using the reference implementation of Pradhan et al (2014a)). Under those assumptions, the annotations get a CoNLL-2012 F1 of 69.86, using the reference implementation for these scores. For comparison, more traditional span-based coreference annotations (O’Gorman et al., 2016) found inter-annotator agreement of 65.5 for event F1, and 70.4 for entity F1 (CoNLL F1 score). This is therefore in the rough range of where one might expect human performance to be. However, this does not have actual comparability to o"
C18-1313,S16-1181,0,0.0207727,"rence, most commonly with the non-focused element in a communication or mental state verb – such as the person being interested in “that’s interesting”, or the cause of “I laughed out loud”. A similar issue involved the recipient or listener role for verbs like “say-01” or “ask-01”, which can sometimes be inferable from context but are low in prominence. 4 4.1 Measuring MS-AMR similarity — scoring system performance A baseline implementation We present a simple baseline that hints at a lower bound for the task. We use the publicly available version of the Brandeis transition-based AMR parser (Wang et al., 2016) combined with an off-the-shelf coreference system (Clark and Manning, 2016) using an AMR-to-surface-form aligner (Flanigan et al., 2014) to convert the surface coreference to links between AMR nodes. 4.2 Simple evaluation of system prediction As mentioned in section 3.2, one hurdle in evaluating system predictions comes from the absence of clearly alignable “mentions” for use in coreference metrics. We can use the SMATCH metric on each individual sentence within an AMR document to resolve this, as calculating a SMATCH score involves determining the highest scoring alignment between variables"
C98-1046,W98-0104,1,0.730806,"Missing"
C98-1046,W96-0306,0,0.029103,"these actions to be performed without the end result being achieved, b u t where the cutting manner can still be recognized, i.e., John cut at the loaf. Where break is concerned, the only thing speeifled is the resulting change of state where the object becomes separated into pieces. If the result is not achieved, there are no attempted breaking actions that can still be recognized. 2.1 A m b i g u i t i e s in L e v i n c l a s s e s It is not clear how much WordNet synsets should be expected to overlap with Levin 294 classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Doff, 1997). However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap between Levin and WordNet. The association of sets of syn"
C98-1046,W97-0204,0,0.0515111,"and semantic properties. 1 Introduction The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within well-defined subdomains. The only escape from this limitation will be through the use of automated or semi-automated methods of lexical acquisition. However, the field has yet to develop a clear consensus on guidelines for a computational lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and Sanfilippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the support of DARPA graat N66001-94C-6043, ARO grant DAAH04-94G-0426, and CAPES grant 0914/95-2. 293 One of the most controversial areas has to do with polysemy. What constitutes a clear separation into senses for any one verb, and how can these senses be computationally characterized and distinguished? The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single greatest limitation on the general application of natural language processing techniques. In this paper we specificall"
C98-1046,C94-1038,0,0.116936,"Missing"
C98-1046,J91-4003,0,0.0286896,"at these verbs demonstrate similarly coherent syntactic and semantic properties. 1 Introduction The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within well-defined subdomains. The only escape from this limitation will be through the use of automated or semi-automated methods of lexical acquisition. However, the field has yet to develop a clear consensus on guidelines for a computational lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and Sanfilippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the support of DARPA graat N66001-94C-6043, ARO grant DAAH04-94G-0426, and CAPES grant 0914/95-2. 293 One of the most controversial areas has to do with polysemy. What constitutes a clear separation into senses for any one verb, and how can these senses be computationally characterized and distinguished? The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single greatest limitation on the general application of natural langu"
choi-etal-2010-propbank,han-etal-2002-development,1,\N,Missing
choi-etal-2010-propbank,P98-1013,0,\N,Missing
choi-etal-2010-propbank,C98-1013,0,\N,Missing
choi-etal-2010-propbank,J05-1004,1,\N,Missing
choi-etal-2010-propbank,kipper-etal-2006-extending,1,\N,Missing
choi-etal-2010-propbank,palmer-etal-2008-pilot,1,\N,Missing
choi-etal-2010-propbank-instance,J93-2004,0,\N,Missing
choi-etal-2010-propbank-instance,han-etal-2002-development,1,\N,Missing
choi-etal-2010-propbank-instance,J05-1004,1,\N,Missing
choi-etal-2010-propbank-instance,palmer-etal-2008-pilot,1,\N,Missing
corvey-etal-2012-foundations,N06-4006,0,\N,Missing
corvey-etal-2012-foundations,P11-2121,1,\N,Missing
D13-1149,W04-3205,0,0.0561749,"sks, posed to subjects in the form of games. 1 Introduction One key application of Natural Language Processing (NLP) is meaning extraction. Of particular importance is propositional meaning: To understand “Jessica sprayed paint on the wall,” it is not enough to know who Jessica is, what paint is, and where the wall is, but that, by the end of the event, some quantity of paint that was not previously on the wall now is. One must extract not only meanings for individual words but also the relations between them. One option is to learn these relations in a largely bottom-up, data-driven fashion (Chklovski and Pantel, 2004; Poon and Domingos, 2009). For instance, Poon and Domingos (2009) first extracts dependency trees, converts those into quasi-logical form, recursively induces lambda expressions from them, and uses clustering to derive progressively abstract knowledge. An alternative is to take a human-inspired approach, mapping the linguistic input onto the kinds of representations that linguistic and psychological research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences ("
D13-1149,W13-0215,0,0.0656549,"Missing"
D13-1149,D09-1001,0,\N,Missing
D13-1149,D09-1000,0,\N,Missing
D13-1149,W13-2323,0,\N,Missing
D19-6201,cybulska-vossen-2014-using,0,0.0634294,"Missing"
D19-6201,W16-1701,1,0.897502,"Missing"
D19-6201,W13-1203,0,0.394035,"Missing"
D19-6201,W16-5706,1,0.928784,"Missing"
D19-6201,recasens-etal-2012-annotating,0,0.397119,"Missing"
D19-6201,Q14-1012,1,0.893257,"Missing"
E14-1007,P10-1024,0,0.0149542,"hey, we, ...} observe dobj:{effect} nsubj:{he, ...} observe dobj:{result} prep at:{time} nsubj:{you, child, ...} observe dobj:{bird} .. . 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation to get a set of initial frames, and Figure 1: Examples of predicate-argument structures and initial frames for the verb “observe.” 3. apply clustering to the initial frames based on the Chinese Restaurant Process to produce the final semantic frames. frequencies in the applications of semantic frames or the method proposed by Abend and Rappoport (2010). We apply the following processes to extracted predicate-argument structures: Each of these steps is described in the following sections in detail. 3.1 Extracting Predicate-argument Structures from a Raw Corpus • A verb and an argument are lemmatized, and only the head of an argument is preserved for compound nouns. We first apply dependency parsing to a large raw corpus. We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases. Then, we extract predicate-argument structures from the depende"
E14-1007,kawahara-kurohashi-2006-case,1,0.679562,"Missing"
E14-1007,P98-1013,0,0.304305,"ga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to"
E14-1007,N06-1023,1,0.835493,"Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed For other languages than English, Kawahara and Kurohashi (2006a) proposed a method for automatically compiling Japanese semantic frames from a large web corpus. They applied conventional agglomerative clustering to predicateargument structures using word/frame similarity based on a manually-crafted thesaurus. Since Japanese is head-final and has case-marking postpositions, it seems easier to build semantic frames with it than with other languages such as English. They also achieved an improvement in dependency parsing and predicate-argument structure 59 Sentences: They observed the effects of ... This statistical ability to observe an effect ... We did n"
E14-1007,boas-2002-bilingual,0,0.0222164,"our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006"
E14-1007,korhonen-etal-2006-large,0,0.0632328,"nslation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *”"
E14-1007,A97-1052,0,0.111055,"arsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal"
E14-1007,N10-1137,0,0.0714345,"., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying thi"
E14-1007,P10-1046,0,0.106572,"the previous approaches, especially considering that no filtering procedures were applied to the induced frames. We anticipate that the results based on the prototypical induced frames with SUMO attributes would be competitive. Our post-analysis revealed that the entropy can be lowered further if an automatic filtering based on frequencies is applied. 4.5 Evaluation of the Quality of Selectional Preferences We also investigated the quality of selectional preferences within the induced semantic frames. The only publicly available test data for selectional preferences, to our knowledge, is from Chambers and Jurafsky (2010). This data consists of quadruples (verb, relation, word, confounder) and does not contain their context.7 A typical way for using our semantic frames is to select an appropriate frame for an input sentence and judge the eligibility of the word uses against the selected frame. However, due to the lack of context for the above data, it is difficult to select a corresponding semantic frame for a test quadruple and thus the induced semantic frames cannot be naturally applied to this data. To investigate the potential for selectional preferences of the semantic frames, we approximately match a qua"
E14-1007,P11-1112,0,0.121541,"orhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor"
E14-1007,de-marneffe-etal-2006-generating,0,0.00430938,"Missing"
E14-1007,D11-1122,0,0.144094,"orhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor"
E14-1007,W07-1424,0,0.0310914,"imental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, stude"
E14-1007,P12-1044,0,0.0467125,"information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *” means a preposition. 58"
E14-1007,W13-0117,1,0.919031,"the current number of initial frames assigned to the semantic frame fj . α is a hyper-parameter that determines how likely it is for a new semantic frame to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of vi . P (vi |fj ) is defined based on the DirichletMultinomial distribution as follows: ∏ P (vi |fj ) = P (w|fj )count(vi ,w) , (2) dobj, ccomp, nsubj, prep ∗, iobj. w∈V This selection of a predominant argument order above is justified by relative comparisons of the discriminative power of the different slots for CPA frames (Popescu, 2013). If a predicate-argument structure does not have any of the above slots, it is discarded. Then, the predicate-argument structures that have the same verb and argument pair (slot and where V is the vocabulary in all case slots cooccurring with the verb. It is distinguished by the case slot, and thus consists of pairs of slots and words, e.g., “nsubj:child” and “dobj:bird.” count(vi , w) is the number of w in the initial frame vi . P (w|fj ) is defined as follows: 3 If a predicate-argument structure has multiple prepositional phrases, one of them is randomly selected. P (w|fj ) = ∑ 61 count(fj"
E14-1007,P13-1085,0,0.309287,"urdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *” means a preposition. 58 Proceedings of the 14th Confer"
E14-1007,D13-1121,1,0.84905,"hen, we extract predicate-argument structures from the dependency parses. Dependents that have the following dependency relations to a verb are extracted as arguments: • Phrasal verbs are also distinguished from non-phrasal verbs. For example, “look up” has independent frames from “look.” • The passive voice of a verb is distinguished from the active voice, and thus these have independent frames. Passive voice is detected using the part-of-speech tag “VBN” (past participle). The alignment between frames of active and passive voices will be done after the induction of frames using the model of Sasano et al. (2013) in the future. nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗ Here, we do not distinguish adjuncts from arguments. All extracted dependents of a verb are handled as arguments. This distinction is left for future work, but this will be performed using slot 2 • “xcomp” (open clausal complement) is renamed to “ccomp” (clausal complement) and “xsubj” (controlling subject) is renamed to “nsubj” (nominal subject). This is because http://nlp.stanford.edu/software/lex-parser.shtml 60 word, e.g., “dobj:effect”) are merged into an initial frame (Figure 1). After this process, we discard minor initial f"
E14-1007,D09-1067,0,0.0615122,"sien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domi"
E14-1007,P03-1002,0,0.0720399,"verage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen"
E14-1007,N13-1051,0,0.278596,"nto, Italy dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu Abstract argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Materna, 2013). Materna induced semantic frames (called LDA-Frames) from triples of (subject, verb, object) in the British National Corpus (BNC) based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. LDAFrames capture limited linguistic phenomena of these triples, and are defined across verbs based on probabilistic topic distributions. This paper presents a method for automatically building verb-specific semantic frames from a large raw corpus. Our semantic frames are verbspecific like PropBank and semantically distinguished. A frame has several syntactic case slots, each of which consists of"
E14-1007,P11-1145,0,0.0615372,"rs in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalabilit"
E14-1007,W12-1901,0,0.252813,"rowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Mate"
E14-1007,E12-1003,0,0.10559,"articipate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. The"
E14-1007,C04-1100,0,0.040668,"rpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent o"
E14-1007,W09-0210,0,0.0675922,"ct, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role"
E14-1007,J05-1004,1,0.186197,"es are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing"
E14-1007,H93-1052,0,0.534394,"model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. ("
E14-1007,D09-1001,0,0.0389919,"orhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotat"
E14-1007,W06-1601,0,\N,Missing
E14-1007,P07-1028,0,\N,Missing
E14-1007,C98-1013,0,\N,Missing
E14-1007,P93-1032,0,\N,Missing
E14-1007,P12-1090,0,\N,Missing
E17-1053,W13-2322,1,0.923054,"Missing"
E17-1053,bonial-etal-2014-propbank,1,0.844889,"P (cj |di = a(cj ), dl = a(cpj )) Experimental Data The LDC DEFT Phase 2 AMR Annotation Release 2.01 consists of AMRs with English sentence 1 LDC DEFT Phase 2 AMR Annotation Release 2.0, Release date: March 10th, 2016. https://catalog.ldc.upenn.edu /LDC2016E25 This decoding problem finds the alignment a that maximizes the likelihood, which we define in 563 Data pairs. Annotated selections from various genres (including newswire, discussion forum, other web logs, and television transcripts) are available, for a total of 39,260 sentences. This release uses the PropBank Unification frame files (Bonial et al., 2014; Bonial et al., 2016). To generate automatic dependency parses for all DEFT AMR Release data, we use ClearNLP (Choi and Mccallum, 2013) to produce dependency parses. ClearNLP also labels semantic roles and named entity tags automatically on the generated dependency parses. This data set is named “All”. To compare the effect of applying automatic dependency parses to our aligner with gold dependency parses, we select the sentences which appear in the OntoNotes 5.02 release as well. The OntoNotes data contains TreeBanking, PropBanking, and Named Entity annotations. OntoNotes 5.0 also uses PropB"
E17-1053,J93-2003,0,0.0751647,"Missing"
E17-1053,H05-1091,0,0.10932,"robability of the AMR relation label of c, given the parse tree path between dc and dcp , where dc and dcp represent the dependency nodes that are aligned by c and cp , respectively. Parse tree path is the concatenation of all dependency tree and direction labels through the tree path between dc and dcp . For example, the relation probability of c = 61, dc = 61, and dcp = old in Figure 3b is P (quant|advmod ↓ num ↓). A parse tree path is a useful feature for extracting relations between any two tree nodes, e.g., Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002) and relation extraction (Bunescu and Mooney, 2005; Kambhatla, 2004; Xu et al., 2015), so we add relation probability to our model. External Features To capture alignments for concepts which do not match any of the above basic rules, we design the following four external feature probabilities: PN E (c, dc ) = P (c|NamedEntity(dc )) Named Entity Probability is the probability of the concept c conditioned on different named entity types (e.g., PERSON, DATE, ORGANIZAPLemma (c, dc ) = P (c|Word(dc )) Lemma Probability represents the likelihood that a concept c aligns to a dependency word di . For 560 temporalquantity temporalquantity quant quant"
E17-1053,P15-3007,1,0.867719,"Missing"
E17-1053,J05-1004,1,0.290115,"with the experimental data, and enhances AMR parsing. 1 Figure 1: The AMR annotation of sentence “Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.” in PENMAN format Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation that expresses the logical meaning of English sentences with rooted, directed, acylic graphs. AMR associates semantic concepts with the nodes on a graph, while the relations are the label edges between concept nodes. Meanwhile, AMR relies heavily on predicate-argument relations from PropBank (Palmer et al., 2005), which share several edge labels. The representation also encodes rich information, like semantic roles (all the “ARGN” tags from PropBank), named entities (NE) (“person”, “location”, etc., concepts), wiki-links (“:wiki” tags), and co-reference (reuse of variables, e.g., p). An example AMR in PENMAN format (Matthiessen and Bateman, 1991) is shown in Figure 1. The design of an AMR to English sentence aligner is the first step for implementation of an AMR parser, since AMR annotation does not contain links between each AMR concept and the original span of words. The basic alignment strategy is"
E17-1053,D14-1048,0,0.538404,"Missing"
E17-1053,P13-1104,0,0.0220327,"sentence 1 LDC DEFT Phase 2 AMR Annotation Release 2.0, Release date: March 10th, 2016. https://catalog.ldc.upenn.edu /LDC2016E25 This decoding problem finds the alignment a that maximizes the likelihood, which we define in 563 Data pairs. Annotated selections from various genres (including newswire, discussion forum, other web logs, and television transcripts) are available, for a total of 39,260 sentences. This release uses the PropBank Unification frame files (Bonial et al., 2014; Bonial et al., 2016). To generate automatic dependency parses for all DEFT AMR Release data, we use ClearNLP (Choi and Mccallum, 2013) to produce dependency parses. ClearNLP also labels semantic roles and named entity tags automatically on the generated dependency parses. This data set is named “All”. To compare the effect of applying automatic dependency parses to our aligner with gold dependency parses, we select the sentences which appear in the OntoNotes 5.02 release as well. The OntoNotes data contains TreeBanking, PropBanking, and Named Entity annotations. OntoNotes 5.0 also uses PropBank Unification frame files for PropBanking. This data set, containing a total of 8,276 of selected AMRs and their dependency parses fro"
E17-1053,N15-1040,0,0.160358,"Missing"
E17-1053,P14-1134,0,0.52501,"Missing"
E17-1053,S16-1181,0,0.0627232,"Missing"
E17-1053,J02-3001,0,0.0195422,"c , dcp )) Relation Probability is the conditional probability of the AMR relation label of c, given the parse tree path between dc and dcp , where dc and dcp represent the dependency nodes that are aligned by c and cp , respectively. Parse tree path is the concatenation of all dependency tree and direction labels through the tree path between dc and dcp . For example, the relation probability of c = 61, dc = 61, and dcp = old in Figure 3b is P (quant|advmod ↓ num ↓). A parse tree path is a useful feature for extracting relations between any two tree nodes, e.g., Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002) and relation extraction (Bunescu and Mooney, 2005; Kambhatla, 2004; Xu et al., 2015), so we add relation probability to our model. External Features To capture alignments for concepts which do not match any of the above basic rules, we design the following four external feature probabilities: PN E (c, dc ) = P (c|NamedEntity(dc )) Named Entity Probability is the probability of the concept c conditioned on different named entity types (e.g., PERSON, DATE, ORGANIZAPLemma (c, dc ) = P (c|Word(dc )) Lemma Probability represents the likelihood that a concept c aligns to a dependency word di . For"
E17-1053,P15-1095,0,0.0255573,"Missing"
E17-1053,D15-1206,0,0.0248839,"given the parse tree path between dc and dcp , where dc and dcp represent the dependency nodes that are aligned by c and cp , respectively. Parse tree path is the concatenation of all dependency tree and direction labels through the tree path between dc and dcp . For example, the relation probability of c = 61, dc = 61, and dcp = old in Figure 3b is P (quant|advmod ↓ num ↓). A parse tree path is a useful feature for extracting relations between any two tree nodes, e.g., Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002) and relation extraction (Bunescu and Mooney, 2005; Kambhatla, 2004; Xu et al., 2015), so we add relation probability to our model. External Features To capture alignments for concepts which do not match any of the above basic rules, we design the following four external feature probabilities: PN E (c, dc ) = P (c|NamedEntity(dc )) Named Entity Probability is the probability of the concept c conditioned on different named entity types (e.g., PERSON, DATE, ORGANIZAPLemma (c, dc ) = P (c|Word(dc )) Lemma Probability represents the likelihood that a concept c aligns to a dependency word di . For 560 temporalquantity temporalquantity quant quant unit 61 years num year 61 year 61 u"
E17-1053,P03-1054,0,0.0123385,"Missing"
E17-1053,J93-2004,0,0.0594888,"Missing"
E17-1053,S16-1166,0,0.46574,"Missing"
E17-1053,P13-2131,0,\N,Missing
E17-1053,S16-1176,0,\N,Missing
H01-1010,C00-2148,1,0.815449,"Missing"
H01-1010,P98-1046,1,0.906641,"Missing"
H01-1010,P00-1065,0,0.0595871,"Missing"
H01-1010,W97-0204,0,0.111615,"Missing"
H01-1010,H94-1020,0,\N,Missing
H01-1010,C98-1046,1,\N,Missing
H01-1014,P97-1003,0,0.0305428,"NCY STRUCTURES The notion of head is important in both phrase structures and dependency structures. In many linguistic theories such as X-bar theory and GB theory, each phrase structure has a head that determines the main properties of the phrase and a head has several levels of projections; whereas in a dependency structure the head is linked to its dependents. In practice, the head information is explicitly marked in a dependency Treebank, but not always so in a phrase-structure Treebank. A common way to find the head in a phrase structure is to use a head percolation table, as discussed in [7, 1] among others. For example, the entry (S right S/VP) in the head percolation table says that the head child1 of an S node is the first child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1 The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. (b) In the dependency structure, make the"
H01-1014,P99-1065,0,0.342088,"phrase structure in Figure 3(c). If a head has multiple modifiers, the algorithm could use either a single X’ or stacked X’ [3]. Figure 4 shows the phrase structure for the d-tree in Figure 2, where the algorithm uses a single X’ for multiple modifiers of the same head.2 VP V’ NP N’ VP V’ NNP V’ join Vinken will NP VB MD PP NP P’ DTP N’ DT’ NN DT board N’ IN NP as the N’ DTP DT’ DT a ADJP N’ ADJ’ JJ NP N’ N’ CD NNP 29 NN Nov director nonexecutive Figure 4: The phrase structure built by algorithm 1 for the dtree in Figure 2 3.2 Algorithm 2 Algorithm 2, as adopted by Collins and his colleagues [2] when they converted the Czech dependency Treebank [6] into a phrasestructure Treebank, produces phrase structures that are as flat as possible. It uses the following heuristic rules to build phrase structures: One level of projection for any category: X has only one level of projection: XP. 2 To make the phrase structure more readable, we use N’ and NP as the X’ and XP for all kinds of POS tags for nouns (e.g., NNP, NN, and CD). Verbs and adjectives are treated similarly. Minimal projections for dependents: A dependent Y does not project to Y P unless it has its own dependents. Fixed position"
H01-1014,P95-1037,0,0.0122292,"NCY STRUCTURES The notion of head is important in both phrase structures and dependency structures. In many linguistic theories such as X-bar theory and GB theory, each phrase structure has a head that determines the main properties of the phrase and a head has several levels of projections; whereas in a dependency structure the head is linked to its dependents. In practice, the head information is explicitly marked in a dependency Treebank, but not always so in a phrase-structure Treebank. A common way to find the head in a phrase structure is to use a head percolation table, as discussed in [7, 1] among others. For example, the entry (S right S/VP) in the head percolation table says that the head child1 of an S node is the first child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1 The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. (b) In the dependency structure, make the"
H01-1014,J93-2004,0,0.0366946,"first child of the node from the right with the label S or VP. Once the heads in phrase structures are found, the conversion from phrase structures to dependency structures is straightforward, as shown below: (a) Mark the head child of each node in a phrase structure, using the head percolation table. 1 The head-child of a node XP is the child of the node XP that is the ancestor of the head of the XP in the phrase structure. (b) In the dependency structure, make the head of each nonhead-child depend on the head of the head-child. Figure 1 shows a phrase structure in the English Penn Treebank [8]. In addition to the syntactic labels (such as NP for a noun phrase), the Treebank also uses function tags (such as SBJ for the subject) for grammatical functions. In this phrase structure, the root node has two children: the NP and the VP. The algorithm would choose the VP as the head-child and the NP as a non-head-child, and make the head Vinkin of the NP depend on the head join of the VP in the dependency structure. The dependency structure of the sentence is shown in Figure 2. A more sophisticated version of the algorithm (as discussed in [10]) takes two additional tables (namely, the argu"
H01-1014,W00-1307,1,0.687762,"a phrase structure in the English Penn Treebank [8]. In addition to the syntactic labels (such as NP for a noun phrase), the Treebank also uses function tags (such as SBJ for the subject) for grammatical functions. In this phrase structure, the root node has two children: the NP and the VP. The algorithm would choose the VP as the head-child and the NP as a non-head-child, and make the head Vinkin of the NP depend on the head join of the VP in the dependency structure. The dependency structure of the sentence is shown in Figure 2. A more sophisticated version of the algorithm (as discussed in [10]) takes two additional tables (namely, the argument table and the tagset table) as input and produces dependency structures with the argument/adjunct distinction (i.e., each dependent is marked in the dependency structure as either an argument or an adjunct of the head). S NP-SBJ NNP Vinken VP MD will VP VB join NP DT NN IN the NP-TMP PP-CLR NP board as DT NNP CD Nov 29 JJ NN director a nonexecutive Figure 1: A phrase structure in the Penn Treebank join Vinken as board will director the a 29 Nov nonexecutive Figure 2: A dependency tree for the sentence in Figure 1. Heads are marked as parents"
H01-1026,W00-1201,1,0.81787,"Missing"
H01-1026,P00-1058,1,0.892378,"Missing"
H01-1026,P97-1003,0,0.119704,"Missing"
H01-1026,P89-1015,0,0.0706941,"Missing"
H01-1026,W00-1306,0,0.0320163,"Missing"
H01-1026,P95-1037,0,0.189475,"Missing"
H01-1026,J93-2004,0,0.0308379,"Missing"
H01-1026,P95-1021,0,0.0479746,"Missing"
H01-1026,C92-2065,0,0.066503,"Missing"
H01-1026,C92-2066,0,0.124868,"Missing"
H01-1026,xia-etal-2000-developing,1,0.796706,"Missing"
H86-1011,H86-1012,1,\N,Missing
H86-1011,P81-1029,1,\N,Missing
han-etal-2000-handling,palmer-etal-1998-rapid,1,\N,Missing
han-etal-2000-handling,A00-1009,1,\N,Missing
han-etal-2000-handling,P97-1003,0,\N,Missing
han-etal-2000-handling,C90-3001,0,\N,Missing
han-etal-2000-handling,1997.mtsummit-workshop.12,1,\N,Missing
han-etal-2000-handling,J94-4004,0,\N,Missing
han-etal-2000-handling,A97-1039,1,\N,Missing
han-etal-2002-development,han-etal-2000-handling,1,\N,Missing
han-etal-2002-development,A97-1014,0,\N,Missing
han-etal-2002-development,C92-2066,0,\N,Missing
han-etal-2002-development,P97-1003,0,\N,Missing
han-etal-2002-development,W00-1307,1,\N,Missing
han-etal-2002-development,C92-2065,0,\N,Missing
han-etal-2002-development,P99-1065,0,\N,Missing
hwang-etal-2014-criteria,W10-0801,1,\N,Missing
hwang-etal-2014-criteria,zaenen-etal-2008-encoding,1,\N,Missing
hwang-etal-2014-criteria,H94-1020,0,\N,Missing
hwang-etal-2014-criteria,P02-1031,1,\N,Missing
I05-1081,S01-1001,0,0.144414,"Missing"
I05-1081,W02-0813,1,0.725743,"e in Section 2. In Section 3, we show the evaluation results on SENSEVAL2 English verbs and show how much the three enhancements improve our system&apos;s performance. We then discuss the potential improvements of our system in the future. In Section 4, we investigate the robustness of our system and propose our strategy for alleviating the negative effects of poor preprocessing. We conclude our discussion in Section 5. Towards Robust High Performance Word Sense Disambiguation of English Verbs 935 2 System Description Our original WSD system was inspired by the successful MaxEnt WSD system of Dang [5,10]. We used the same machine learning model, Mallet, that implements a smoothing maximum entropy (ME) model with a Gaussian prior [12]. An attractive property of ME models is that there is no assumption of feature independence [13]. Empirical studies have shown that a ME model with a Gaussian prior generally outperforms ME models with other smoothing methods [14]. In addition to topical and collocation features, we also used similar rich syntactic and semantic features, although we implemented them in different ways. Furthermore, we enhanced the treatment of certain rich linguistic features, whi"
I05-1081,C02-1112,0,0.0608891,"Missing"
I05-1081,P97-1009,0,0.186865,"Missing"
I05-1081,W02-1006,0,0.0327398,"Missing"
I05-1081,W04-0838,0,0.0431237,"Missing"
I05-1081,J96-1002,0,0.00299597,"re. In Section 4, we investigate the robustness of our system and propose our strategy for alleviating the negative effects of poor preprocessing. We conclude our discussion in Section 5. Towards Robust High Performance Word Sense Disambiguation of English Verbs 935 2 System Description Our original WSD system was inspired by the successful MaxEnt WSD system of Dang [5,10]. We used the same machine learning model, Mallet, that implements a smoothing maximum entropy (ME) model with a Gaussian prior [12]. An attractive property of ME models is that there is no assumption of feature independence [13]. Empirical studies have shown that a ME model with a Gaussian prior generally outperforms ME models with other smoothing methods [14]. In addition to topical and collocation features, we also used similar rich syntactic and semantic features, although we implemented them in different ways. Furthermore, we enhanced the treatment of certain rich linguistic features, which we believed would boost the system&apos;s performance. Before discussing these enhancements, we first briefly describe the basic syntactic and semantic features used by our system: Syntactic features: 1. Is the sentence passive, se"
I05-1081,W00-0103,0,0.0604917,"Missing"
I05-1081,W04-2807,1,0.868381,"Missing"
I05-1081,H94-1020,0,0.101128,": How much will a relatively poor quality of preprocessing negatively affect the system&apos;s performance? Which strategies can we adopt to alleviate these negative effects? 4.1 Experiment I Since the parser is the most critical component of our preprocessing and is more likely to have lower performance when it is used in an unfamiliar data set, we investigate how the performance of the parser on different test data sets affects our system. We divided the SENSEVAL2 test data into two sets: an easy set and a hard set. The test data from the Wall Street Journal (wsj) sections of Penn Treebank (PTB) [22] are put into the easy set because they are similar to the parser&apos;s training data: 02-21 wsj sections. The hard set contains test data from the Brown sections of PTB and BNC data. It is expected that the parser and therefore the system will perform better on the easy set. We trained our system on the whole SENSEVAL2 training data set and evaluated its performance on the easy and hard test sets separately. The results are shown in Table 3. Table 3. Performance on different test data sets Test data set Num. of test inst. Average Acc. Hard 895 62.2 Easy 911 66.9 Whole Set 1806 64.6 As we expected"
I05-1081,S01-1040,0,\N,Missing
I05-1081,P05-3014,0,\N,Missing
I05-1081,J94-4002,0,\N,Missing
J05-1004,P98-1013,0,0.547446,"Missing"
J05-1004,J99-2004,0,0.0793722,"iscuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence and its subject in the second but does not ind"
J05-1004,J93-2002,0,0.0764722,"Missing"
J05-1004,A97-1052,0,0.0690349,"Missing"
J05-1004,W04-2412,0,0.0762594,"Missing"
J05-1004,A00-2018,0,0.0611182,"o define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence and its subject in the second but does not indicate that it pl"
J05-1004,W03-1006,0,0.0731698,"Missing"
J05-1004,P98-1046,1,0.0921239,"Missing"
J05-1004,W03-1005,0,0.0602322,"Missing"
J05-1004,W03-1008,1,0.0985942,"Missing"
J05-1004,J02-3001,1,0.544914,"ver those involving complex syntactic structure not immediately relevant to the lexical predicate itself. Only sentences in which the lexical predicate was used ‘‘in frame’’ were annotated. A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al. (2002), and the ramifications for automatic classification are discussed more thoroughly in Gildea and Jurafsky (2002). In contrast with FrameNet, PropBank is aimed at providing data for training statistical systems and has to provide an annotation for every clause in the Penn Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank also attempts to label semantically related verbs consistently, relying primarily on VerbNet classes for determining sema"
J05-1004,P02-1031,1,0.09693,"bs et al. (1997) built finite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while Ray and Craven (2001) used low-level ‘‘chunks’’ from a general-purpose syntactic analyzer as observations in a trained hidden Markov model. Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided. It is also possible that this approach may be more robust to error than parsers. Our experiments working with a flat, ‘‘chunked’’ representation of the input sentence, described in more detail in Gildea and Palmer (2002), test this finite-state hypothesis. In the chunked representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (48) [NP Big investment banks] [VP refused to step] [ADVP up] [PP to] [NP the plate] [VP to support] [NP the beleaguered floor traders] [PP by] [VP buying] [NP bigblocks] [PP of] [NP stock], [NP traders] [VP say]. (wsj_2300) Our chunks were derived from the treebank trees using the conversion described by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried o"
J05-1004,hajicova-kucerova-2002-argument,0,0.24424,"cate was used ‘‘in frame’’ were annotated. A word with multiple distinct senses would generally be analyzed as belonging to different frames in each sense but may only be found in the FrameNet corpus in the sense for which a frame has been defined. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). A more complete description of the FrameNet project can be found in Baker, Fillmore, and Lowe (1998) and Johnson et al. (2002), and the ramifications for automatic classification are discussed more thoroughly in Gildea and Jurafsky (2002). In contrast with FrameNet, PropBank is aimed at providing data for training statistical systems and has to provide an annotation for every clause in the Penn Treebank, no matter how complex or unexpected. Similarly to FrameNet, PropBank also attempts to label semantically related verbs consistently, relying primarily on VerbNet classes for determining semantic relatedness. However, there is much less emphasis on the definition of the semantics of the class that the verbs are associ"
J05-1004,P02-1043,0,0.064046,"Missing"
J05-1004,P02-1018,0,0.0648451,"Missing"
J05-1004,W04-2606,0,0.149793,"Missing"
J05-1004,P03-1009,0,0.0622622,"Missing"
J05-1004,J93-2004,0,0.0909159,"Missing"
J05-1004,A00-2034,0,0.0767001,"Missing"
J05-1004,J01-3003,0,0.140055,"frame element’s syntactic relation to the predicate. 6. A Quantitative Analysis of the Semantic-Role Labels The stated aim of PropBank is the training of statistical systems. It also provides a rich resource for a distributional analysis of semantic features of language that have hitherto been somewhat inaccessible. We begin this section with an overview of general characteristics of the syntactic realization of the different semantic-role labels and then attempt to measure the frequency of syntactic alternations with respect to verb class membership. We base this analysis on previous work by Merlo and Stevenson (2001). In the following section we discuss the performance of a system trained to automatically assign the semantic-role labels. 6.1 Associating Role Labels with Specific Syntactic Constructions We begin by simply counting the frequency of occurrence of roles in specific syntactic positions. In all the statistics given in this section, we do not consider past- or presentparticiple uses of the predicates, thus excluding any passive-voice sentences. The syntactic positions used are based on a few heuristic rules: Any NP under an S node in the treebank is considered a syntactic subject, and any NP und"
J05-1004,W04-2807,1,0.0979165,"Missing"
J05-1004,H01-1010,1,0.12072,"Missing"
J05-1004,W97-0301,0,0.0793095,"tive statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ‘‘trace’’ categories of the treebank. 1. Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille´ 2003), have had a major impact on the field of natural language processing in recent years. However, the syntactic analyses produced by these parsers are a long way from representing the full meaning of the sentences that are parsed. As a simple example, in the sentences (1) John broke the window. (2) The window broke. a syntactic analysis will represent the window as the verb’s direct object in the first sentence"
J05-1004,C00-2108,0,0.0697808,"Missing"
J05-1004,P02-1029,0,0.072459,"Missing"
J05-1004,P03-1002,0,0.127172,"r, still have a long way to go. Our results using hand-annotated parse trees including traces show that improvements in parsing should translate directly into more accurate semantic representations. There has already been a demonstration that a preliminary version of these data can be used to simplify the effort involved in developing information extraction (IE) systems. Researchers were able to construct a reasonable IE system by simply mapping specific Arg labels for a set of verbs to template slots, completely avoiding the necessity of building explicit regular expression pattern matchers (Surdeanu et al. 2003). There is equal hope for advantages for machine translation, and proposition banks in Chinese (Xue and Palmer 2003) and Korean are already being built, focusing where possible on parallel data. The general approach ports well to new languages, with the major effort continuing to go into the creation of frames files for verbs. There are many directions for future work. Our preliminary linguistic analyses have merely scratched the surface of what is possible with the current annotation, and yet it is only a first approximation at capturing the richness of semantic representation. Annotation of"
J05-1004,W00-0726,0,0.128223,"Missing"
J05-1004,W04-3212,1,0.16398,"nguistics Volume 31, Number 1 The broad-coverage annotation has proven to be suitable for training automatic taggers, and in addition to ourselves there is a growing body of researchers engaged in this task. Chen and Rambow (2003) make use of extracted tree-adjoining grammars. Most recently, the Gildea and Palmer (2002) scores presented here have been improved markedly through the use of support-vector machines as well as additional features for named entity tags, headword POS tags, and verb clusters for back-off (Pradhan et al. 2003) and using maximum-entropy classifiers (He and Gildea 2004, Xue and Palmer 2004). This group also used Charniak’s parser instead of Collins’s and tested the system on TDT data. The performance on a new genre is lower, as would be expected. Despite the complex relationship between syntactic and semantic structures, we find that statistical parsers, although computationally expensive, do a good job of providing information relevant for this level of semantic interpretation. In addition to the constituent structure, the headword information, produced as a side product, is an important feature. Automatic parsers, however, still have a long way to go. Our results using hand-an"
J05-1004,W04-2704,1,\N,Missing
J05-1004,X98-1014,0,\N,Missing
J05-1004,J06-2001,0,\N,Missing
J05-1004,W03-1707,1,\N,Missing
J05-1004,J03-4003,0,\N,Missing
J05-1004,J08-2004,0,\N,Missing
J05-1004,C98-1046,1,\N,Missing
J05-1004,C98-1013,0,\N,Missing
J05-1004,P93-1032,0,\N,Missing
J05-1004,W05-0620,0,\N,Missing
J05-1004,J06-3002,0,\N,Missing
J90-3005,H89-1049,0,0.0159054,"urate output until a system that uses particular algorithms to handle particular phenomena has been implemented. In addition to methods for measuring performance of entire systems, we also need ways of measuri!ng progress in characterizing phenomena and developing algorithms that will contribute to system development. Once a system is up and running, the accuracy of its output can then be measured. The different types of the output ,:an be associated with the phenomena that have to be handled in order to produce each type. For example, consider a phrase from the trouble failure report domain (Ball 1989): Replaced interlock switch with new one. In order accurately to fill in the slot in the database field associa~ed with the new-part-installed(-) relation, the ""one"" anaphora has to be correctly resolved, requiring a complex interaction between semantic and pragmatic analysis. It is possible to have two systems that produce the same output, but do it very differently. This is where such issues as effic!iency, extensibility, maintainability, and robustness come in. A more efficient implementation, for example, may be able to support a larger, more complex domain. With a more general implementat"
J90-3005,P87-1022,0,0.035346,"Missing"
J90-3005,H89-1032,0,0.19292,"also had higher scores on the problem-solving tasks (Napier 1989). 2.1 BLACK-BOX EVALUATION Black-box evaluation is primarily focused on ""what a system does."" Ideally, it should be possible to measure performance based on well-defined I/0 pairs. If accurate output is produced with respect to particular input, then the system is performing correctly. In practice, this is more difficult than it appears. There is no consensus on how to evaluate the correctness of semantic representations, so output has to be in terms of some specific application task such as databzse answering or template fill (Sundheim 1989). This allows for an astonishing amount of variation between systems, and makes it difficult to separate out issues of coverage of linguistic phenomena from robustness and error&apos; recovery (see Figure 2). In addition to the accuracy of the output, systems could also be evaluated in terms of their user-friendliness, modularity, portability, and maintainability. How easy are they to us.e, how well do they plug into other components, can they be ported and maintained by someone who is not a system expert? In general, it should be possible to perform Computational Linguistics Volume 16, Number 3, S"
J90-3005,P83-1007,0,\N,Missing
J96-4004,O91-1004,0,0.175439,"emantic network of linguistic concepts, which ensures that these activities do not operate independently of the system's representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation m"
J96-4004,C92-1019,0,0.103694,"'mark'. As a result, this method fails to correctly identify the word boundaries in sentence (6). Within statistical approaches, considering, for example, the mutual information method (Lua and Gan 1994), the same fragment is identified as a bisyllabic word in both sentences (3a) and (6) 7. By checking the structural relationships among words in a sentence, rule-based approaches aim to overcome limitations faced by pattern-matching and statistical approaches. However, many of the rules in existing rule-based systems (Huang 1989; Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu 1992) are either arbitrary and word-specific, or overly general. For example, Rule Given an ambiguous fragment xyz where x, z, xy, and yz are all possible words, if x can be analyzed as a so-called direction word, segment the fragment as x yz, else segment it as xy z (Liang 1990). This syntactic rule works in sentence (7). (7) ~ t~ he 'He ~ -[v ff~ xi?~ bend down bends down his ~--~ shenzi body body.' The fragment T : ~ xi?~ shen zi in sentence (7) is ambiguous. As -F xi?~ 'down' is a direction word, the fragment is segmented as -~ ~:j~ xi?l sh@nzi 'down body', which is as desired. Similarly, this"
J96-4004,O92-1003,0,0.460666,"work of linguistic concepts, which ensures that these activities do not operate independently of the system's representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system"
J96-4004,P93-1046,0,0.0284477,"ous parallel mode. • Computational activities are a combination of top-down and bottom-up activities. • Computational activities are indirectly guided by a semantic network of linguistic concepts, which ensures that these activities do not operate independently of the system's representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its inte"
J96-4004,P94-1010,0,0.0202735,"of the system's representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction of words and of relations between words. Later, the sys"
J96-4004,J96-3004,0,0.33246,"resentation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction of words and of relations between words. Later, the system progresses to ide"
J96-4004,O93-1009,0,0.159755,"that these activities do not operate independently of the system's representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction"
K19-1034,P98-1013,0,0.053002,"Missing"
K19-1034,D12-1091,0,0.0226009,"Missing"
K19-1034,E06-1042,0,0.637049,"oric words and identifying verbs), four of the top five systems use some form of long shortterm memory network (LSTM). The system of Wu et al. (2018) performed best on both tasks (F1 of .651 on all parts of speech, and .672 for verbs) using a combination of a convolutional neural network (CNN) and bidirectional LSTM. Since the shared task, a variety of other approaches have been developed using similar deep learning techniques. Most recently, the work of Gao et al. (2018) achieved state-of-the-art performance on the shared task data as well as a variety of other datasets, including the TroFi (Birke and Sarkar, 2006) and Mohammad et al. (2016) datasets, using Bi-LSTM models coupled with GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. For the VUAMC shared task, they report F1 scores of .726 for all parts of speech and .697 for verbs. For the Mohammad et al. dataset, they report an average F1 score of .791, and for the Trofi they report an F1 score of .72, slightly lower than the current state of the art (.75 of K¨oper et al. (2017)). Despite recent advances in evaluation and algorithm performance, the task still remains difficult, with the highest F1 scores nearing only .73 on th"
K19-1034,D18-1060,0,0.638789,"lit that has been used to regularize the evaluation of metaphor identification systems. For both sections of the task (identifying all metaphoric words and identifying verbs), four of the top five systems use some form of long shortterm memory network (LSTM). The system of Wu et al. (2018) performed best on both tasks (F1 of .651 on all parts of speech, and .672 for verbs) using a combination of a convolutional neural network (CNN) and bidirectional LSTM. Since the shared task, a variety of other approaches have been developed using similar deep learning techniques. Most recently, the work of Gao et al. (2018) achieved state-of-the-art performance on the shared task data as well as a variety of other datasets, including the TroFi (Birke and Sarkar, 2006) and Mohammad et al. (2016) datasets, using Bi-LSTM models coupled with GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. For the VUAMC shared task, they report F1 scores of .726 for all parts of speech and .697 for verbs. For the Mohammad et al. dataset, they report an average F1 score of .791, and for the Trofi they report an F1 score of .72, slightly lower than the current state of the art (.75 of K¨oper et al. (2017)). D"
K19-1034,W06-3506,0,0.138953,"Missing"
K19-1034,J13-2003,0,0.0257707,"ing systems. Deep learning models require sufficient quality data, which is lacking for many metaphorical expressions. We automatically fill gaps in metaphor training data by exploiting syntax in two ways: first, we use the syntactically-motivated lexical resource VerbNet to identify additional data through metaphoric and literal sense identification, and second, we use syntactic properties of certain lexemes, which allow us to identify relevant sentences via dependency parses. These methods yield training data that improves performance for metaphor classification across a variety of tasks. 2 Shutova et al. (2013) also employs selectional preferences based on argument structure, identifying verb-subject and verb-direct object pairs in corpora. They begin with a seed set of metaphoric pairs, similar to our methods of collecting instances based on syntactic information. They use these seed pairs to identify new metaphors, similar to our usage of syntactic patterns to identify training data. Their methods are based on the selectional preferences of verbs, and thus are less concerned with the variety of syntactic patterns metaphors can participate in. We will identify much more complex syntactic patterns,"
K19-1034,W17-1903,0,0.269567,"Missing"
K19-1034,W18-0907,0,0.364962,"Missing"
K19-1034,W18-0913,0,0.0901241,"ic or literal, and extract training data from existing VerbNet annotation. Second, we analyze syntactic patterns from Wikipedia data. We identify patterns that indicate metaphoric or literal senses of verbs, and then extract additional data based on these patterns. et al., 2018). They use the VUAMC, providing a train/test split that has been used to regularize the evaluation of metaphor identification systems. For both sections of the task (identifying all metaphoric words and identifying verbs), four of the top five systems use some form of long shortterm memory network (LSTM). The system of Wu et al. (2018) performed best on both tasks (F1 of .651 on all parts of speech, and .672 for verbs) using a combination of a convolutional neural network (CNN) and bidirectional LSTM. Since the shared task, a variety of other approaches have been developed using similar deep learning techniques. Most recently, the work of Gao et al. (2018) achieved state-of-the-art performance on the shared task data as well as a variety of other datasets, including the TroFi (Birke and Sarkar, 2006) and Mohammad et al. (2016) datasets, using Bi-LSTM models coupled with GloVe (Pennington et al., 2014) and ELMo (Peters et al"
K19-1034,J04-1002,0,0.078594,"hen use the data for training metaphor systems rather than identifying selectional preferences. Related Work While most computational metaphor processing methods rely heavily on lexical semantics, many previous approaches also employ syntactic structures to varying degrees. Most prior work involving argument structure is based on the idea of selectional preferences: certain verbs prefer certain arguments when used literally, and others when used metaphorically. This idea is captured by determining what kinds of arguments fill syntactic and semantic roles for specific verbs. The CorMet system (Mason, 2004) employs this paradigm, and is similar to ours in their collection of key verbs and analysis of syntactic arguments and semantic roles. They automatically collect documents for particular domains based on key words, and identify selectional preferences based on the WordNet hierarchy for verbs in these particular domains. For example, they find that assault typically takes direct objects of the type fortification in the MILITARY domain. This allows them to make inferences about when selectional preferences are adhered to, and they can then identify mappings between different domains. While thei"
K19-1034,S16-2003,0,0.184971,"erbs), four of the top five systems use some form of long shortterm memory network (LSTM). The system of Wu et al. (2018) performed best on both tasks (F1 of .651 on all parts of speech, and .672 for verbs) using a combination of a convolutional neural network (CNN) and bidirectional LSTM. Since the shared task, a variety of other approaches have been developed using similar deep learning techniques. Most recently, the work of Gao et al. (2018) achieved state-of-the-art performance on the shared task data as well as a variety of other datasets, including the TroFi (Birke and Sarkar, 2006) and Mohammad et al. (2016) datasets, using Bi-LSTM models coupled with GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. For the VUAMC shared task, they report F1 scores of .726 for all parts of speech and .697 for verbs. For the Mohammad et al. dataset, they report an average F1 score of .791, and for the Trofi they report an F1 score of .72, slightly lower than the current state of the art (.75 of K¨oper et al. (2017)). Despite recent advances in evaluation and algorithm performance, the task still remains difficult, with the highest F1 scores nearing only .73 on the VUAMC data. This is likel"
K19-1034,J05-1004,1,0.177895,"oth datasets was effective for classification tasks, we believe the difficulty in combining both datasets in the sequence models is due 6 Conclusions We show that using external data found through syntactic structures and lexical resources can be used to improve deep learning methods for metaphoric classification. This is due to regular syntactic patterns of metaphoric usage, and the idea that the semantics of verbs can be dependent on the syntactic patterns that it participates in. For future improvements, there are other resources available that could be leveraged in the same way. PropBank (Palmer et al., 2005), FrameNet (Baker 369 conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any government agency. et al., 1998), and WordNet (Fellbaum, 2010) all offer some syntactic and/or semantic information, and data annotated with these resources could prove another valuable source of additional samples. We also only examine some basic syntactic patterns for a small number of verbs, and this was done manually. Improved methods for automatically detecting relevant syntactic patterns as well as further effort in manual identification"
K19-1034,D14-1162,0,0.0830066,"network (LSTM). The system of Wu et al. (2018) performed best on both tasks (F1 of .651 on all parts of speech, and .672 for verbs) using a combination of a convolutional neural network (CNN) and bidirectional LSTM. Since the shared task, a variety of other approaches have been developed using similar deep learning techniques. Most recently, the work of Gao et al. (2018) achieved state-of-the-art performance on the shared task data as well as a variety of other datasets, including the TroFi (Birke and Sarkar, 2006) and Mohammad et al. (2016) datasets, using Bi-LSTM models coupled with GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. For the VUAMC shared task, they report F1 scores of .726 for all parts of speech and .697 for verbs. For the Mohammad et al. dataset, they report an average F1 score of .791, and for the Trofi they report an F1 score of .72, slightly lower than the current state of the art (.75 of K¨oper et al. (2017)). Despite recent advances in evaluation and algorithm performance, the task still remains difficult, with the highest F1 scores nearing only .73 on the VUAMC data. This is likely due to the relatively small dataset size (app. 200,000 words), which is in"
K19-1034,N18-1202,0,0.02674,"t al. (2018) performed best on both tasks (F1 of .651 on all parts of speech, and .672 for verbs) using a combination of a convolutional neural network (CNN) and bidirectional LSTM. Since the shared task, a variety of other approaches have been developed using similar deep learning techniques. Most recently, the work of Gao et al. (2018) achieved state-of-the-art performance on the shared task data as well as a variety of other datasets, including the TroFi (Birke and Sarkar, 2006) and Mohammad et al. (2016) datasets, using Bi-LSTM models coupled with GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. For the VUAMC shared task, they report F1 scores of .726 for all parts of speech and .697 for verbs. For the Mohammad et al. dataset, they report an average F1 score of .791, and for the Trofi they report an F1 score of .72, slightly lower than the current state of the art (.75 of K¨oper et al. (2017)). Despite recent advances in evaluation and algorithm performance, the task still remains difficult, with the highest F1 scores nearing only .73 on the VUAMC data. This is likely due to the relatively small dataset size (app. 200,000 words), which is in part caused by difficulties in"
kawahara-palmer-2014-single,de-marneffe-etal-2006-generating,0,\N,Missing
kawahara-palmer-2014-single,W11-0110,1,\N,Missing
kawahara-palmer-2014-single,H05-1111,0,\N,Missing
kawahara-palmer-2014-single,N09-1064,0,\N,Missing
kawahara-palmer-2014-single,C08-1002,0,\N,Missing
kawahara-palmer-2014-single,P98-1013,0,\N,Missing
kawahara-palmer-2014-single,C98-1013,0,\N,Missing
kawahara-palmer-2014-single,J92-4003,0,\N,Missing
kawahara-palmer-2014-single,P08-2008,1,\N,Missing
kawahara-palmer-2014-single,N13-1090,0,\N,Missing
kawahara-palmer-2014-single,P12-1028,1,\N,Missing
kawahara-palmer-2014-single,P10-1040,0,\N,Missing
kawahara-palmer-2014-single,J05-1004,1,\N,Missing
kawahara-palmer-2014-single,C10-1140,0,\N,Missing
kingsbury-palmer-2002-treebank,han-etal-2000-handling,1,\N,Missing
kingsbury-palmer-2002-treebank,J93-2004,0,\N,Missing
kingsbury-palmer-2002-treebank,P97-1003,0,\N,Missing
kingsbury-palmer-2002-treebank,P98-1046,1,\N,Missing
kingsbury-palmer-2002-treebank,C98-1046,1,\N,Missing
kingsbury-palmer-2002-treebank,P98-1013,0,\N,Missing
kingsbury-palmer-2002-treebank,C98-1013,0,\N,Missing
kipper-etal-2004-extending,kingsbury-palmer-2002-treebank,1,\N,Missing
kipper-etal-2004-extending,P98-1013,0,\N,Missing
kipper-etal-2004-extending,C98-1013,0,\N,Missing
kipper-etal-2004-extending,P99-1001,0,\N,Missing
kipper-etal-2004-extending,C00-2148,1,\N,Missing
kipper-etal-2006-extending,kingsbury-palmer-2002-treebank,1,\N,Missing
kipper-etal-2006-extending,J88-2003,0,\N,Missing
kipper-etal-2006-extending,C94-1042,0,\N,Missing
kipper-etal-2006-extending,C00-2094,0,\N,Missing
kipper-etal-2006-extending,W02-0907,1,\N,Missing
kipper-etal-2006-extending,W04-2606,1,\N,Missing
kipper-etal-2006-extending,W02-1016,0,\N,Missing
kipper-etal-2006-extending,P98-1013,0,\N,Missing
kipper-etal-2006-extending,C98-1013,0,\N,Missing
kipper-etal-2006-extending,P87-1027,0,\N,Missing
L16-1145,W13-2322,1,0.860349,"Missing"
L16-1145,bies-etal-2014-incorporating,1,0.844507,"ish Treebank within BOLT improved annotation quality by adding several rounds of Quality Control (QC) to the annotation process. The first QC process consists of a series of specific searches for approximately 200 types of potential inconsistencies and parser or annotation errors. Any errors found in these searches were hand corrected. An additional QC process then identifies repeated text and structures, and flags non-matching annotations. Identified annotation errors are also manually corrected. A special effort for English translation Treebank is the annotation of alternative translations (Bies et al. 2014). Both literal and fluent translation alternates are annotated for word-level tokenization and part-of-speech, whereas only the fluent translation alternates are annotated as part of the syntactic structure of the tree. 3.1.2 Arabic Treebank Arabic Treebank started with Penn Arabic Treebank guidelines, enhanced first with the GALE (Global Autonomous Language Exploitation) project and now with the BOLT project (Maamouri et al. 2011). The two efforts of Arabic Treebank within the GALE project are enhancement of Penn style Treebank annotation (Maamouri & Bies, 2010) and the creation of CATiB (Col"
L16-1145,bonial-etal-2014-propbank,1,0.859958,"or challenge for Arabic was with special features of the Egyptian dialect. It was sometimes very difficult to decide if an MSA word form in the dialect had an equivalent meaning or a slightly different meaning. Additionally, as a pro-drop language, Arabic poses special issues for coreference. Co-reference chains creation problems were initially solved by manual annotation but are now able to be created during post-processing. The English PropBank has focused on expanding predicate annotation beyond the verb and is now annotating verbs, eventive nouns, adjectives, and light verb constructions (Bonial et al. 2014). In English, light verbs are semantically bleached verbs. The argument structure for light verb and non-light verb instances are different. In light verb constructions, the real predicate is generally the nominalized predicate that the light verb supports. A major focus for English PropBank has been to unify FrameFiles across these different parts of speech. This means that the frame used for 'bathe' is always identical to that used for 'bath'. The goal of this expansion is to provide event semantic representations for the entire sentence, specifically pieces most often missed when looking so"
L16-1145,cotterell-callison-burch-2014-multi,0,0.0685218,"Missing"
L16-1145,W10-0702,0,0.0308468,"nd facilitating linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2010-enriching,1,0.823206,"ting linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2012-parallel,1,0.828457,"-9 (-NONE- *T*))))))) (. .)) ) POS annotation is stored in .pos files, where POS tags are attached to tokenized/segmented word units, each sentence per line, as shown in the following example. 好_VA 的_SP 呗_SP ，_PU 来_VV 的话_SP 打_VV 我 _PN 电话_NN 就_AD 可以_VV ，_PU 或者_CC 报_VV 我_PN 名字_NN ，_PU 我_PN 定_VV 了_AS 包厢_NN 3.2 Word Alignment 3.2.1 Alignment Approach Word alignment guidelines are developed based on guidelines for the Blinker and ARCADE projects, enriched during GALE by adding tagging guidelines (Li et al. 2010), and further enhanced to tackle new genres and language features for the BOLT project (Li et al. 2012). The alignment annotation involves a process of 2-pass annotation plus one round of cross-file checking. The initial alignment by junior annotators goes through quality control by senior annotators, and is then followed by a cross-file check by lead annotators for consistency. The word alignment tool is developed by LDC (Figure 3), allowing annotators to align source and translation words as well as to label both alignment links and individual words. Alignment is performed on two pairs of languages: Chinese-English and Egyptian-English. Chinese alignment is performed at two levels: character-"
L16-1145,maamouri-etal-2014-developing,1,0.87281,"Missing"
L16-1145,N13-1039,0,0.101377,"Missing"
L16-1377,I08-2099,1,0.837098,"king the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top of Urdu Dependency Treebank (Bhat and Sharma, 2012). 3. CPG Formalism The CPG formalism or Computational Paninian Grammar (Begum et al., 2008) is influenced by the grammatical framework of Panini, the fifth century B.C. grammarian of Sanskrit. It is a dependency grammar in which the syntactic structures consist of a set of paired, asymmetric relations between words of a sentence. A dependency relation is defined between a dependent, a syntactically subordinate word and a head word on which it depends. In CPG, the verb is treated as the primary modified or as the root of the dependency tree and the elements modifying the verb event are defined by it. Karaka relations describe the manner in which arguments occur in the activity descri"
L16-1377,W12-3623,1,0.911754,"alternation patterns for forming sense distinctions for Chinese verbs as a crucial step in marking the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top of Urdu Dependency Treebank (Bhat and Sharma, 2012). 3. CPG Formalism The CPG formalism or Computational Paninian Grammar (Begum et al., 2008) is influenced by the grammatical framework of Panini, the fifth century B.C. grammarian of Sanskrit. It is a dependency grammar in which the syntactic structures consist of a set of paired, asymmetric relations between words of a sentence. A dependency relation is defined between a dependent, a syntactically subordinate word and a head word on which it depends. In CPG, the verb is treated as the primary modified or as the root of the dependency tree and the elements modifying the verb event are defined"
L16-1377,W14-4206,1,0.923651,"round 200K words) have been annotated with dependency structure. Each sentence contains an average of 29 words and an average of 13.7 chunks of average length 2.0. 4. Urdu PropBank Development Process In this section, we describe the development of Urdu PropBank. In general, PropBank annotation is a 2-step process. The initial step involves the creation of frame files. The subsequent step in this process is the actual annotation of predicate-argument structure using the frame files. In case of Urdu, instead of creating frame files afresh, the frame files from Hindi and Arabic were ported (see Bhat et al. (2014) for more details). 4.1. Porting and Adapting Predicate frames from Hindi and Arabic PropBanks The first step in building a PropBank is to make the predicate frames for simple and complex predicates (described later) available. Instead of creating predicate frames for each and every verb present in Urdu, which would indeed be very demanding and time-consuming task, (Bhat et al., 2014) explored the feasibility of porting predicate frames from already built Arabic and Hindi PropBank for use in Urdu PropBanking. This instigation was prompted from the fact that Hindi and Urdu are linguistically si"
L16-1377,W09-3036,1,0.800155,"ces are represented in the Shakti Standard Format (Bharati et al., 2007). The Dependency tagset consists of about 43 labels. PropBank is mainly concerned with those labels depicting dependencies in the context of verb predicates. The Dependency Treebanks for Hindi and Urdu are developed following a generic pipeline. It involves building multi-layered and multi-representational Treebanks for Hindi and Urdu. The steps in the process of building the Urdu Treebank under this pipeline consists of (i) Tokenization, (ii) MorphAnalysis, (iii) POS-tagging, (iv) Chunking, and (v) Dependency annotation (Bhatt et al., 2009). Annotation process commences with the tokenization of raw text. The tokens thus obtained are annotated with morphological and POS information. After morph-analysis and POS-tagging, words are grouped into chunks. All the above processing steps have been automated by high accuracy tools (rule-based or statistical) thus speeding up the manual process. The last process in this pipeline so far is the manual dependency annotation. The inter-chunk dependencies are marked leaving the dependencies between words in a chunk unspecified for the intra-chunk dependencies. PropBanking is the next step in t"
L16-1377,J96-2004,0,0.558258,"between the two annotators using a data set of 44,000 words double-annotated (double PropBanked) by two annotators, without either annotator knowing other’s judgment of marking semantic role labels. A healthy agreement on the data set will ensure that the decisions taken by the annotators during building UPB and thereafter the annotations on arguments of verb predicates are consistent and reliable. We measured the Inter-annotator agreement using Fleiss’ kappa (Randolph, 2005) which is the frequently used agreement coefficient for annotation tasks on categorical data. Kappa was introduced by (Carletta, 1996) and since then many linguistics resources have been evaluated by the coefficient. The kappa statistics in this section show the agreement between the annotators on a given data-set and the compatibility and consistency with which the annotations have been performed on predicate-argument structures by the two annotators. These measures also demonstrate the conformity in their understanding of the PropBank annotation guidelines. The Fleiss’ kappa is calculated as: k= P r(a) − P r(e) 1 − P r(e) (9) The factor 1−P r(e) estimates the degree of agreement that is attainable above chance, and P r(a)−"
L16-1377,choi-etal-2010-propbank-instance,1,0.801884,"served between the annotators. In this sentence, snaan karta is a complex predicate because there is a nominal element snaan present along with the verb kar. Simple predicates are those which only have a light verb present in them. For example (2) (Shoeb ne) (Saba ko) (kitaab) (di). Shoeb ERG Saba DAT kitaab give. Shoeb gave book to Saba. 5. 5.1. Annotating the Urdu PropBank Following the porting and adaptation of predicate frames from Hindi and Arabic PropBanks, the annotation process for marking the predicate argument structures for each verb instance commenced. The Jubilee annotation tool (Choi et al., 2010) was used for the annotation of the Urdu PropBank. Some changes were made in the tool to take into account the dependency trees of Urdu sentences and to make them available for annotators as a reference. The UPB currently consists of 20 labels including both numbered arguments and modifiers (Table 1). Label ARG0 ARG1 ARG2 ARG3 ARG2-ATR ARG2-LOC ARG2-GOL ARG2-SOU ARGM-PRX ARGM-ADV ARGM-DIR ARGM-EXT ARGM-MNR ARGM-PRP ARGM-DIS ARGM-LOC ARGM-MNS ARGM-NEG ARGM-TMP ARGM-CAU Challenges and redundancies The challenges which the two annotators encountered are described below. We also discuss the approa"
L16-1377,duran-aluisio-2012-propbank,0,0.322353,"Missing"
L16-1377,P09-1033,0,0.0692065,"Missing"
L16-1377,W07-1513,0,0.0259067,"for several European and Asian languages. The first PropBank, English PropBank was built on top of the phrase structure trees in Penn Treebank and adds a semantic layer to its syntactic structures (Palmer et al., 2005). Among European languages, work on PropBank has been reported in French, German, Dutch and Portuguese languages. Merlo and Van Der Plas (2009) showed the cross-lingual validity of PropBank by applying the annotation scheme developed for English on a large portion of French sentences. Van Der Plas et al. (2010) used English FrameNet to build a corpus of German PropBank manually. Monachesi et al. (2007) used PropBank semantic labels for semi-automatic annotation of a corpus of Dutch. Duran and Alu´ısio (2012) talk about the annotation of a Brazilian Portuguese Treebank with semantic role labels based on Propbank guidelines. As far as Asian languages are concerned, PropBanks exist for Chinese, Korean and Arabic. A PropBank for Korean 2379 language is being built which treats each verb and adjective in the Korean Treebank as a semantic predicate and annotation is done to mark the arguments and adjuncts of the predicate (Palmer et al., 2006). For Arabic, a Semitic language, Palmer et al. (2010)"
L16-1377,J05-1004,1,0.348001,"on 5 illustrates the challenges pertaining to Urdu Propbank annotation. In section 6, we report the Inter-annotator Agreement between the annotators. In section 7, we present several experiments which we did on the Urdu Propbank and their results. We conclude the paper in Section 8 and give future directions in section 9. 2. Related Work and comparison with other PropBanks PropBanks are being built for several European and Asian languages. The first PropBank, English PropBank was built on top of the phrase structure trees in Penn Treebank and adds a semantic layer to its syntactic structures (Palmer et al., 2005). Among European languages, work on PropBank has been reported in French, German, Dutch and Portuguese languages. Merlo and Van Der Plas (2009) showed the cross-lingual validity of PropBank by applying the annotation scheme developed for English on a large portion of French sentences. Van Der Plas et al. (2010) used English FrameNet to build a corpus of German PropBank manually. Monachesi et al. (2007) used PropBank semantic labels for semi-automatic annotation of a corpus of Dutch. Duran and Alu´ısio (2012) talk about the annotation of a Brazilian Portuguese Treebank with semantic role labels"
L16-1377,singh-ambati-2010-integrated,0,0.0150235,"ependency annotation. The inter-chunk dependencies are marked leaving the dependencies between words in a chunk unspecified for the intra-chunk dependencies. PropBanking is the next step in this generic pipeline which is aimed at establishing another layer of semantics on the Urdu Treebank. As part of the overall effort, a PropBank for Hindi is also built thus adding a semantic layer to the Hindi Treebank. The Urdu Dependency Treebank is developed following this Treebanking pipeline for the newspaper articles using a team of linguistics annotators. The tool used for the annotation is Sanchay (Singh and Ambati, 2010). All the annotations are represented in Shakti Standard Format (SSF). So far, ∼7,000 sentences (around 200K words) have been annotated with dependency structure. Each sentence contains an average of 29 words and an average of 13.7 chunks of average length 2.0. 4. Urdu PropBank Development Process In this section, we describe the development of Urdu PropBank. In general, PropBank annotation is a 2-step process. The initial step involves the creation of frame files. The subsequent step in this process is the actual annotation of predicate-argument structure using the frame files. In case of Urd"
L16-1377,W11-0403,1,0.884415,"d Arabic proposition bank (APB) in which predicates are identified with their relevant arguments and adjuncts in Arabic texts and an automatic process was put in place to map existing annotation to the new trees. Xue (2008) shows the use of diathesis alternation patterns for forming sense distinctions for Chinese verbs as a crucial step in marking the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top of Urdu Dependency Treebank (Bhat and Sharma, 2012). 3. CPG Formalism The CPG formalism or Computational Paninian Grammar (Begum et al., 2008) is influenced by the grammatical framework of Panini, the fifth century B.C. grammarian of Sanskrit. It is a dependency grammar in which the syntactic structures consist of a set of paired, asymmetric relations between words of a sentence. A dependency re"
L16-1377,W10-1814,0,0.563171,"Missing"
L16-1377,J08-2004,0,0.0371245,"r as Asian languages are concerned, PropBanks exist for Chinese, Korean and Arabic. A PropBank for Korean 2379 language is being built which treats each verb and adjective in the Korean Treebank as a semantic predicate and annotation is done to mark the arguments and adjuncts of the predicate (Palmer et al., 2006). For Arabic, a Semitic language, Palmer et al. (2010) presents a revised Arabic proposition bank (APB) in which predicates are identified with their relevant arguments and adjuncts in Arabic texts and an automatic process was put in place to map existing annotation to the new trees. Xue (2008) shows the use of diathesis alternation patterns for forming sense distinctions for Chinese verbs as a crucial step in marking the predicatestructure of Chinese verbs. For Indian languages, a PropBank for Hindi is being built. Unlike PropBanks in other European or Asian languages, the Hindi PropBank is annotated on top of Dependency structure, the Hindi Dependency Treebank (Vaidya et al., 2011). Dependency structure is particularly suited for flexible word order languages such as Hindi. Urdu is another free word order language for which a Propbanking effort is going on and it is built on top o"
L16-1377,W10-1836,1,0.92849,"Missing"
L16-1628,D13-1160,0,0.103478,"Missing"
L16-1628,bonial-etal-2014-propbank,1,0.921266,"Missing"
L16-1628,W10-1810,1,0.918322,"ehensively all mentions of events of states, which can be realized alternately as verbs (fear), nouns, (fear), adjectives (afraid) or LVCs (have fear). Although each new relation type has presented unique annotation challenges, ensuring consistent and comprehensive annotation of LVCs has proved particularly challenging because LVCs are semi-productive, difficult to define objectively, and there are often borderline cases. This research describes the iterative process of developing PropBank annotation guidelines for LVCs (for a description of the early development stages of LVC annotation, see Hwang et al., 2010), the current guidelines,1 and a comparison to the annotation practices of two related resources: the TectoGrammatical Treebank (Cinková et al., 2004) and Tu and Roth’s (2011) LVC dataset that was developed specifically for enabling automatic LVC detection. 1 2 http://propbank.github.io/ https://github.com/propbank/propbank-frames PropBank Background PropBank annotation consists of two tasks: sense annotation and role annotation. The PropBank lexicon provides a listing of the coarse-grained senses of a verb, noun, or adjective relation, and the set of roles associated with each sense (thus, ca"
L16-1628,meyers-etal-2004-annotating,0,0.024814,"e labeling, NLP resources 2. 1. Introduction The goal of PropBank (Palmer et al., 2005) is to supply consistent, general-purpose labeling of semantic roles across different syntactic realizations. With over two million words from diverse genres, the benchmark annotated corpus supports the training of automatic semantic role labelers, which in turn support other Natural Language Processing (NLP) areas, such as machine translation. PropBank annotation consists of two tasks: sense and role annotation. Previously, PropBank annotation was focused on verbs (a separate, but related project, NomBank (Meyers et al., 2004), focused on noun relations), but recent efforts have shifted to expanding the annotation coverage of PropBank from verb relations to adjective and noun relations, as well as light verb constructions (LVCs; e.g., have a nap, do an investigation, give a kiss) (Bonial et al., 2014). This shift has allowed PropBank to capture more comprehensively all mentions of events of states, which can be realized alternately as verbs (fear), nouns, (fear), adjectives (afraid) or LVCs (have fear). Although each new relation type has presented unique annotation challenges, ensuring consistent and comprehensive"
L16-1628,J05-1004,1,0.30189,"ile each new relation type has presented unique annotation challenges, ensuring consistent and comprehensive annotation of light verb constructions has proved particularly challenging, given that light verb constructions are semi-productive, difficult to define, and there are often borderline cases. This research describes the iterative process of developing PropBank annotation guidelines for light verb constructions, the current guidelines, and a comparison to related resources. Keywords: light verb constructions, semantic role labeling, NLP resources 2. 1. Introduction The goal of PropBank (Palmer et al., 2005) is to supply consistent, general-purpose labeling of semantic roles across different syntactic realizations. With over two million words from diverse genres, the benchmark annotated corpus supports the training of automatic semantic role labelers, which in turn support other Natural Language Processing (NLP) areas, such as machine translation. PropBank annotation consists of two tasks: sense and role annotation. Previously, PropBank annotation was focused on verbs (a separate, but related project, NomBank (Meyers et al., 2004), focused on noun relations), but recent efforts have shifted to ex"
L16-1628,W04-0401,0,0.0573487,"istically and within a language remain nebulous, with multiple labels applied to similar phenomena and arguably distinct phenomena labeled as ‘light’ verbs (Jespersen, 1942). Nonetheless, LVCs in English are largely thought to consist of a semantically general, highly polysemous verb and a noun denoting an event or state (e.g., Butt, 2003). More detailed aspects of LVCs remain debatable; thus, different LVC resources encompass different scopes of constructions (see Section 5). LVCs are semi-productive, meaning that some novel combinations of light verbs (LVs) and eventive/stative nouns arise (Stevenson et al., 2004), but other combinations are unacceptable. The seemingly idiosyncratic nature of LVC productivity contributes to the degree of difficulty in automatically detecting LVCs. Furthermore, surface-identical forms can be LVCs or heavy usages of the same verbs: 3. 4. He took a drink of the soda. (LVC) He took a drink off the bar. (non-LVC) Such usages are syntactically indistinguishable (Butt & Geuder, 2001); thus, neither automatic systems nor annotators can rely on syntactic criteria for detection. Manual annotation must use semantic criteria instead, outlined in the guidelines and described in the"
L16-1628,W11-0807,0,0.101392,"o the degree of difficulty in automatically detecting LVCs. Furthermore, surface-identical forms can be LVCs or heavy usages of the same verbs: 3. 4. He took a drink of the soda. (LVC) He took a drink off the bar. (non-LVC) Such usages are syntactically indistinguishable (Butt & Geuder, 2001); thus, neither automatic systems nor annotators can rely on syntactic criteria for detection. Manual annotation must use semantic criteria instead, outlined in the guidelines and described in the next section. 4. PropBank LVC Annotation Guidelines Like other resources that seek to distinguish LVCs (e.g., Tu & Roth, 2011), initial guidelines relied on the basic heuristic that the LVC candidate can be rephrased using a related lexical verb without any significant loss in meaning, as seen in the swap of offer and make an offer in examples (1) and (2). Although intuitively appealing, reliance on this heuristic provides an overly narrowly view of LVCs, since there are two types of LVCs that are difficult to rephrase. First, some LVCs in English have no etymologically related verbal counterpart (e.g., commit suicide). Secondly, some LVC counterparts of verbs affect valency in a manner similar to the passive voice."
L18-1009,W17-2812,1,0.277459,"bNet defines a set of members, thematic roles for the predicate-argument structure of these members, selectional restrictions on the arguments, and frames consisting of a syntactic description and a corresponding semantic representation. It has long been used for semantic role labeling and other inferenceenabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is to revise VerbNet to take advantage of GL’s progress in representing subevent structure while preserving VerbNet’s strengths in linking predicate argument structure, thematic roles and semantic representations. The remainder of this paper will describe"
L18-1009,W13-5401,1,0.813638,"s (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is to revise VerbNet to take advantage of GL’s progress in representing subevent structure while preserving VerbNet’s strengths in linking predicate argument structure, thematic roles and semantic representations. The remainder of this paper will describe the changes being made to VerbNet’s semantic representations and the reasons behind those changes. Section 2 briefly describes the role semantic representations play in VerbNet and breaks down 2. VerbNet and Its Representation of Events Each VerbNet class contains semantic representations that are compatible w"
L18-1009,W13-3004,0,0.0309998,"g a semantic expression evaluated with respect to its opposition: give, open; build: Binary transition (achievement): ¬φ ∈ S1 , and φ ∈ S2 T S1 S2 Complex transition (accomplishment): ¬φ ∈ P , and φ ∈ S T P S The basic event types are the states and processes, which can represent independent events or be combined to derive complex events (transitions). Subevents within an event are ordered by temporal relations and relative prominence or 2 The resulting structure is equivalent to a Labeled Transition System (van Benthem, 1991), and is consistent with the approach developed in (Fernando, 2009; Fernando, 2013). 58 cause(e2 , e3 ) has location(e4 , Theme, ?Destination) relations, i.e., relations from states to states, and hence interpreted over an input/output state-state pairing (cf. (Naumann, 2001)). The model encodes three kinds of representations: (i) predicative content of a frame; (ii) programs that move from frame to frame; and tests that must be satisfied for a program to apply. These include: pre-tests, while-tests, and result-tests. 5. (See sections 4.1-4.3 for further examples.) A more minor adjustment concerns the path rel predicate, which was introduced earlier in the revision process t"
L18-1009,P06-1117,0,0.542098,"erbNet (Kipper et al., 2006) is a promising source of such information. It is a hierarchical, domain-independent verb lexicon that groups verbs into classes based on similarities in their syntactic and semantic behavior (Schuler, 2005). Each class in VerbNet defines a set of members, thematic roles for the predicate-argument structure of these members, selectional restrictions on the arguments, and frames consisting of a syntactic description and a corresponding semantic representation. It has long been used for semantic role labeling and other inferenceenabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is"
L18-1009,kawahara-palmer-2014-single,1,0.220308,"ed on similarities in their syntactic and semantic behavior (Schuler, 2005). Each class in VerbNet defines a set of members, thematic roles for the predicate-argument structure of these members, selectional restrictions on the arguments, and frames consisting of a syntactic description and a corresponding semantic representation. It has long been used for semantic role labeling and other inferenceenabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007). Automatic disambiguation of a verb’s VerbNet class has also improved (Abend et al., 2008; Brown et al., 2014; Kawahara and Palmer, 2014). Efforts to use its semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017) have revealed a need to revise them for consistency and greater expressiveness, in particular, a clearer representation of subevents. Recent work in Generative Lexicon (GL) has focused on further articulating the semantics of subevent structure in language (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013). Hence a reasonable undertaking is to revise VerbNet to take advantage of GL’s progress in representing subevent structure while preserving VerbNet’s strengths in linking predicate argument stru"
L18-1231,W13-2322,1,0.869887,"esent analysis of some appealing characteristics of this final dataset, and present preliminary results of training and evaluating SRL systems on this combined set, to spur usage of this challenging new dataset. Keywords: Propbank,SRL, Semantic Roles, Corpora 1. Introduction We introduce the conversion of all existing Propbank data — constituting more than half a million predicate instances in English — into a format in which etymologically related senses from different parts are speech are merged, making that data compatible with the predicate senses used for Abstract Meaning Representation (Banarescu et al., 2013). This constitutes a large set of data made consistent to use the same frames and conventions, both increasing the amount of training data available for Propbank SRL and also providing a large corpus of semantic role labeling whose rolesets and numbered arguments match those of the Abstract Meaning Representation data (while containing over twice as many predicate instances). We describe the combination of automatic and manual methods used in converting these corpora, provide some analysis to characterize the resulting corpora, and present preliminary SRL results against the test sets presente"
L18-1231,bonial-etal-2014-propbank,1,0.854515,"esets used in AMR; while there is a large drop in verbal senses (notably due to the omission of semantically light predicates), it still remains a very large corpus. Work is ongoing adding more of the Propbank nominal and adjectival rolesets to AMR. 2.5. Expansion of multi-word predicate coverage More recent work has expanded coverage of the Propbank lexicon to encompass multi-word predicates as well, such as take with a grain of salt, cut slack, or jump on bandwagon. Propbank has long annotated certain classes of 1458 multi-word predicates such as verb-particle constructions and light verbs (Bonial et al., 2014), and is now expanded to arbitrarily structured semi-fixed expressions. We added coverage to many of the most high-frequency multi-word predicates in the corpus, and created lexical entries for each multi-word predicate. The important contribution is not simply the detection of these MWP elements (which can also be found in larger resources such as PARSEME (Savary et al., 2017)), but the annotation of semantic roles for each MWP. For example, something like “jump on the bandwagon” would be as follows: • jump-on-bandwagon.09: join an activity or group because of its popularity – Arg0: person ju"
L18-1231,W05-0620,0,0.0208076,"Missing"
L18-1231,W15-3904,0,0.033934,"Missing"
L18-1231,D15-1112,0,0.0253945,"Missing"
L18-1231,W10-1810,1,0.807463,"d Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of that graph to phrases within a sentence. While Propbank 1.0 (Kingsbury and Palmer, 2002) annotated only verbal predicates, it was later expanded to nouns (Hwang et al., 2010) and predicative adjectives (Bonial et al., 2017), creating new Propbank senses (called “rolesets”) for those nouns and adjectives. Parallel work in the Abstract Meaning Representation project also handled nouns and adjectives, but did so by representing them with etymologically related verbal rolesets – so that a noun such as “insertion” would not have its own rolesets, but would instead be labeled with a verbal sense of “insert”. This approach has a range of useful properties in reducing the number of senses with small amounts of training data, and also better conforms to the approach of Fra"
L18-1231,N06-2001,0,0.02169,"Missing"
L18-1231,N16-1030,0,0.0134305,"t to the first, concatenating outputs for each element (Graves et al., 2013). In our approach, and that of Zhou and Xu (2015), the result of the forward pass is used as input to the backward pass, enabling repeated stacking of these layers to form a deep topology. Instead of scoring each label locally, the addition of a CRF loss function allows for globally normalized scoring of all possible sequences of labels, maximizing the sequence-level loglikelihood (Collobert et al., 2011). This approach has been shown to improve performance on a variety of tasks (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). Following standard practices for applying neural architectures to NLP tasks, we initialize our network with word embeddings trained on orders of magnitude more data than is available for our task (SRL). Specifically, we use publicly available GloVe 100-dimensional vectors trained on 6 billion words from Wikipedia and Gigaword (Pennington et al., 2014). These embeddings are updated during training as network parameters along with a single outof-vocabulary (OOV) vector, which is randomly initial1460 ized. We simplify the features used in the original model of (Zhou and Xu, 2015), using only th"
L18-1231,P16-1101,0,0.0941005,"rom the last element to the first, concatenating outputs for each element (Graves et al., 2013). In our approach, and that of Zhou and Xu (2015), the result of the forward pass is used as input to the backward pass, enabling repeated stacking of these layers to form a deep topology. Instead of scoring each label locally, the addition of a CRF loss function allows for globally normalized scoring of all possible sequences of labels, maximizing the sequence-level loglikelihood (Collobert et al., 2011). This approach has been shown to improve performance on a variety of tasks (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). Following standard practices for applying neural architectures to NLP tasks, we initialize our network with word embeddings trained on orders of magnitude more data than is available for our task (SRL). Specifically, we use publicly available GloVe 100-dimensional vectors trained on 6 billion words from Wikipedia and Gigaword (Pennington et al., 2014). These embeddings are updated during training as network parameters along with a single outof-vocabulary (OOV) vector, which is randomly initial1460 ized. We simplify the features used in the original model of (Zhou and Xu"
L18-1231,D14-1162,0,0.082664,"Missing"
L18-1231,W11-1901,1,0.223768,"predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hindi/Urdu (Bhatt et al., 2009), Finnish (Haverinen et al., 2013), Turkish (Sahin, 2016) and Brazilian Portuguese (Duran and Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of that graph to phrases within a sentence. While Propbank 1.0 (Kingsbury and Palmer, 2002) annotated only verbal predicates, it was later expanded to nouns (Hwang et al., 2010) and predicative adjectives (Bonial et al., 2017), creating new Propbank senses (called “rolesets”) for those nouns and adjectives. Parallel work in the Abstract Meaning Representation project also handled nouns and a"
L18-1231,W17-1704,0,0.015137,"word predicates as well, such as take with a grain of salt, cut slack, or jump on bandwagon. Propbank has long annotated certain classes of 1458 multi-word predicates such as verb-particle constructions and light verbs (Bonial et al., 2014), and is now expanded to arbitrarily structured semi-fixed expressions. We added coverage to many of the most high-frequency multi-word predicates in the corpus, and created lexical entries for each multi-word predicate. The important contribution is not simply the detection of these MWP elements (which can also be found in larger resources such as PARSEME (Savary et al., 2017)), but the annotation of semantic roles for each MWP. For example, something like “jump on the bandwagon” would be as follows: • jump-on-bandwagon.09: join an activity or group because of its popularity – Arg0: person jumping on the bandwagon – Arg1: popular thing joined – Arg2: action done which gets one on the bandwagon This is a step forward in not simply detecting these MWPs, but being able to represent them in structured semantic representations such as AMR. 3. OntoNotes (ON) EWT BOLT ON+EWT+BOLT in AMR Subset Conll-2012 nouns (light v.) 40,163 (2,215) 9,453 (732) 18,839 (1973) 68,455 (49"
L18-1231,W08-2121,0,0.0687151,"Missing"
L18-1231,Q15-1003,0,0.022375,"Missing"
L18-1231,P15-2141,1,0.860405,"for specific semantic functions and the reification of semantic roles, such as INCLUDE -91 for set operations or HAVE - ORG - ROLE -91 for organizational membership. Other than those AMR rolesets ending in 91, the rolesets used within AMR are a subset of the rolesets used in Propbank, adopting nearly every verbal form in Propbank and most nominal and adjectival senses. The most common reason why a verbal roleset is not in AMR is because AMR deletes “semantically light” predicates, such as copular “be” or auxiliary “have”. As the use of existing SRL systems has been shown to help AMR parsing (Wang et al., 2015), we expect that the introduction of a larger SRL dataset with closer alignment to the AMR lexicon should increase that utility. The AMR SUBSET line in Table 1 illustrates the size of these corpora when limited to only the rolesets used in AMR; while there is a large drop in verbal senses (notably due to the omission of semantically light predicates), it still remains a very large corpus. Work is ongoing adding more of the Propbank nominal and adjectival rolesets to AMR. 2.5. Expansion of multi-word predicate coverage More recent work has expanded coverage of the Propbank lexicon to encompass"
L18-1231,P15-1109,0,0.0613342,"ow the coverage differs between each resource, by illustrating the kinds of nominal predicates unique to each annotation project. Propbank rolesets which overlap with FrameNet and NomBank (the left column) illustrate prototypical nominalizations. In contrast, nouns not represented in Propbank but captured in FrameNet or NomBank show their coverage over more traditional entities, objects and relational nouns. Preliminary Results Semantic Role Labeler Our SRL system (Gung and Pradhan, 2018) uses a deep neural network model which does not include explicit syntactic information. We closely follow Zhou and Xu (2015), treating SRL as an IOB tagging problem and using deep bidirectional LSTMs with a linear chain conditional random field (Lafferty et al., 2001) loss function. Long-short term memory networks (LSTMs) are a form of recurrent neural network (RNN) that has been successfully applied to many NLP tasks. Sequential inputs are often processed using pairs of RNNs, with one RNN processing from the first to last element and the other RNN processing from the last element to the first, concatenating outputs for each element (Graves et al., 2013). In our approach, and that of Zhou and Xu (2015), the result"
L18-1231,W09-3036,1,0.853669,"adigm for the development of semantic role labeling corpora, designed for large-scale annotation. It focuses upon annotation of coarse-grained senses (“rolesets”) which provide predicate-specific definitions of numbered arguments (ARG 0, ARG 1, etc.) to represent semantic roles. By using coarse-grained sense labels and these predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hindi/Urdu (Bhatt et al., 2009), Finnish (Haverinen et al., 2013), Turkish (Sahin, 2016) and Brazilian Portuguese (Duran and Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of that graph to phrases within a sentence. While Propbank 1.0 (Ki"
L18-1231,W11-4519,0,0.562612,"Missing"
L18-1231,W13-5609,0,0.0231877,"Missing"
L18-1231,N06-2015,1,0.556117,"nglish. Light verbs are annotated using nominal frames (Hwang et al. 2010) and therefore included in those counts core data (Strassel and Tracey, 2016), image captions of the Flickr 8k corpus (Hodosh et al., 2013), MASC data (Ide et al., 2008), and Earth Science data affiliated with the ClearEarth project (Duerr et al., 2016). These additional corpora – as noted in Table 2 below – illustrate the range of domain-specific annotations of Propbank data which might be utilized for semantic role labeling in specific domains. Larger Landscape of Propbank Resources L ORELEI MASC The OntoNotes corpus (Hovy et al., 2006), most recently released as part of the Conll-2012 (Pradhan et al., 2011), was developed during the DARPA-GALE annotation project covering a wide range of domains, and has been the largest resource for training and evaluating semantic role labeling systems. However, the range of other corpora that have been annotated with Propbank roles since OntoNotes – most notably, the English Web Treebank and the BOLT corpora – collectively constitute an amount of additional predicate annotations the same size as OntoNotes itself. The English Web Treebank encompasses a range of genres of the web, such as r"
L18-1231,ide-etal-2008-masc,0,0.0285513,"tecting these MWPs, but being able to represent them in structured semantic representations such as AMR. 3. OntoNotes (ON) EWT BOLT ON+EWT+BOLT in AMR Subset Conll-2012 nouns (light v.) 40,163 (2,215) 9,453 (732) 18,839 (1973) 68,455 (4920) 63,585 (4714) 20,305 adjectives 750 3,305 10,957 15,012 10,121 0 Table 1: Core Corpora Annotated with Propbank rolesets for general English. Light verbs are annotated using nominal frames (Hwang et al. 2010) and therefore included in those counts core data (Strassel and Tracey, 2016), image captions of the Flickr 8k corpus (Hodosh et al., 2013), MASC data (Ide et al., 2008), and Earth Science data affiliated with the ClearEarth project (Duerr et al., 2016). These additional corpora – as noted in Table 2 below – illustrate the range of domain-specific annotations of Propbank data which might be utilized for semantic role labeling in specific domains. Larger Landscape of Propbank Resources L ORELEI MASC The OntoNotes corpus (Hovy et al., 2006), most recently released as part of the Conll-2012 (Pradhan et al., 2011), was developed during the DARPA-GALE annotation project covering a wide range of domains, and has been the largest resource for training and evaluating"
L18-1231,kingsbury-palmer-2002-treebank,1,0.783934,"d manual methods used in converting these corpora, provide some analysis to characterize the resulting corpora, and present preliminary SRL results against the test sets presented here, as a baseline for future evaluation of SRL. We suggest that evaluating against the combined test sets (OntoNotes, English Web Treebank and BOLT) can provide a challenging test set that could encourage the community to build SRL systems with a greater coverage over nominal, adjectival and light verb data, and with robustness to a range of difficult genres. 1.1. Motivations for Unifying Parts of Speech Propbank (Kingsbury and Palmer, 2002) is a paradigm for the development of semantic role labeling corpora, designed for large-scale annotation. It focuses upon annotation of coarse-grained senses (“rolesets”) which provide predicate-specific definitions of numbered arguments (ARG 0, ARG 1, etc.) to represent semantic roles. By using coarse-grained sense labels and these predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hin"
L18-1231,W04-2705,0,0.383619,"Missing"
L18-1231,W05-0309,1,0.442672,"g Parts of Speech Propbank (Kingsbury and Palmer, 2002) is a paradigm for the development of semantic role labeling corpora, designed for large-scale annotation. It focuses upon annotation of coarse-grained senses (“rolesets”) which provide predicate-specific definitions of numbered arguments (ARG 0, ARG 1, etc.) to represent semantic roles. By using coarse-grained sense labels and these predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hindi/Urdu (Bhatt et al., 2009), Finnish (Haverinen et al., 2013), Turkish (Sahin, 2016) and Brazilian Portuguese (Duran and Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of"
L18-1231,song-etal-2014-collecting,0,0.0170062,"ce for training and evaluating semantic role labeling systems. However, the range of other corpora that have been annotated with Propbank roles since OntoNotes – most notably, the English Web Treebank and the BOLT corpora – collectively constitute an amount of additional predicate annotations the same size as OntoNotes itself. The English Web Treebank encompasses a range of genres of the web, such as reviews and emails (Bies et al., 2012), and the BOLT datasets encompass informal corpora of English discussion forum data, SMS text, and translations of conversational data (Garland et al., 2012; Song et al., 2014). All of these resources are either currently released or in the process of being released, with stand-off SRL annotations available at propbank.github.io . We suggest that the combination of the test sets of the three major corpora provides a more interesting and challenging dataset against which to evaluate a semantic role labeling system. This is due to both the challenging informal domains (such as SMS messages and discussion forum posts) as well as to the increase in the coverage of nouns and adjectives in the data. Table 1 illustrates the size of these corpora, broken down by the parts o"
L18-1231,L16-1521,0,0.0139295,"ng joined – Arg2: action done which gets one on the bandwagon This is a step forward in not simply detecting these MWPs, but being able to represent them in structured semantic representations such as AMR. 3. OntoNotes (ON) EWT BOLT ON+EWT+BOLT in AMR Subset Conll-2012 nouns (light v.) 40,163 (2,215) 9,453 (732) 18,839 (1973) 68,455 (4920) 63,585 (4714) 20,305 adjectives 750 3,305 10,957 15,012 10,121 0 Table 1: Core Corpora Annotated with Propbank rolesets for general English. Light verbs are annotated using nominal frames (Hwang et al. 2010) and therefore included in those counts core data (Strassel and Tracey, 2016), image captions of the Flickr 8k corpus (Hodosh et al., 2013), MASC data (Ide et al., 2008), and Earth Science data affiliated with the ClearEarth project (Duerr et al., 2016). These additional corpora – as noted in Table 2 below – illustrate the range of domain-specific annotations of Propbank data which might be utilized for semantic role labeling in specific domains. Larger Landscape of Propbank Resources L ORELEI MASC The OntoNotes corpus (Hovy et al., 2006), most recently released as part of the Conll-2012 (Pradhan et al., 2011), was developed during the DARPA-GALE annotation project cov"
L18-1266,K16-1007,0,0.0145068,"termed the Standard (the compared-to entity) in the Comparison frame is replaced by a Comparison Set. Our treatment generalizes somewhat over the FrameNet treatment by exploiting a single roleset 3 For ARG0 and ARG1 only, an effort is made to map to Dowty’s prototypical agent and patient (Dowty, 1991), respectively. 1678 for all of these constructions (see Section 5.1). Our general roleset is very similar (in definition, albeit distinct in labeling) to that of Bakhshandeh and Allen (2015), who aim to predict the predicate-argument structure of comparison sentences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer,"
L18-1266,W13-2322,1,0.895126,"Missing"
L18-1266,L16-1628,1,0.836428,"andeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the PropBank rolesets for blink list appropriate semant"
L18-1266,bonial-etal-2014-propbank,1,0.924991,"antic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the PropBank rolesets for bli"
L18-1266,P13-2131,1,0.826219,"Missing"
L18-1266,W17-0812,0,0.0128119,"a Comparison Set. Our treatment generalizes somewhat over the FrameNet treatment by exploiting a single roleset 3 For ARG0 and ARG1 only, an effort is made to map to Dowty’s prototypical agent and patient (Dowty, 1991), respectively. 1678 for all of these constructions (see Section 5.1). Our general roleset is very similar (in definition, albeit distinct in labeling) to that of Bakhshandeh and Allen (2015), who aim to predict the predicate-argument structure of comparison sentences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank"
L18-1266,W10-1810,1,0.796285,"ences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the Prop"
L18-1266,N15-1114,0,0.0551831,"meaning. Keywords: Semantics, Constructions, Meaning Representation 1. Introduction The Abstract Meaning Representation (AMR) project (Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of"
L18-1266,W16-5706,1,0.830623,"Missing"
L18-1266,J05-1004,1,0.31696,"grammar approach, the construction itself can license the arguments. This distinction also affects what is thought to be stored in the lexicon and, perhaps most relevant to AMR, what would need to be represented in a computational lexicon: additional senses tied to a lexical predicate, or constructional entries. 2.2. Abstract Meaning Representation Annotation The AMR project annotations are completed on a sentenceby-sentence basis, where each sentence is represented by a rooted directed acyclic graph (DAG). See Figure 1. PENMAN notation. AMR concepts are either English words (boy), PropBank (Palmer et al., 2005) rolesets (want-01), or special keywords indicating generic entity types: date-entity, world-region, distance-quantity, etc. In addition to the PropBank lexicon of rolesets, which associate argument numbers (ARG 0–6) with predicate-specific3 semantic roles (e.g., ARG0=wanter in ex. 1), AMR uses approximately 100 relations of its own (e.g., :time, :age, :quantity, :destination, etc.). These AMR-specific relations can be thought of as a fine-grained inventory of modifier role labels. AMR abstracts away from language-specific, idiosyncratic facts, such that distinct syntactic realizations of the"
L18-1266,N15-1119,1,0.802771,"(Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). However, as the project matures, we are aiming to expand the representation to go beyond capturing the semantics"
L18-1266,W16-6603,1,0.775868,": Semantics, Constructions, Meaning Representation 1. Introduction The Abstract Meaning Representation (AMR) project (Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). Howe"
L18-1266,W17-2315,1,0.813627,"cs bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). However, as the project matures, we are aiming to expand the representation to go beyond capturing the semantics of purely compositional language to better capture the semantics"
N06-1016,W02-0817,0,0.0354071,"ka, 1998). In other words, if c and c &apos; are the two most likely categories for example xn , the margin is measured as follows: (2) M n = Pr(c |x n ) − Pr(c&apos; |x n ) In this case Algorithm 1 would rank examples by increasing values of margin, with the smallest value at the top of the ranking. Using either method of uncertainty sampling, the computational cost of picking an example from T candidates is: O(TD) where D is the number of model parameters. 2.3 Related Work To our best knowledge, there have been very few attempts to apply active learning to WSD in the literature (Fujii and Inui, 1999; Chklovski and Mihalcea, 2002; Dang, 2004). Fujii and Inui (1999) developed an example sampling method for their example-based WSD system in the active learning of verb senses in a pool-based setting. Unlike the uncertainty sampling methods (such as the two methods we used), their method did not select examples for which the system had the minimal certainty. Rather, it selected the examples such that after training using those examples the system would be most certain about its predictions on the rest of the unlabeled examples in the next iteration. This sample selection criterion was enforced by calculating a training ut"
N06-1016,P04-1075,0,0.391246,"ive learning Data analysis on the learning process, where there is an abundant supply of unlabeled based on both instance and feature levels, data, but where the labeling process is expensive. suggests that a careful treatment of feature In NLP problems such as text classification (Lewis extraction is important for the active and Gale, 1994; McCallum and Nigam, 1998), learning to be useful for WSD. The statistical parsing (Tang et al., 2002), information overfitting phenomena that occurred extraction (Thompson et al., 1999), and named during the active learning process are entity recognition (Shen et al., 2004), pool-based identified as classic overfitting in machine active learning has produced promising results. learning based on the data analysis. This paper presents our experiments in applying two active learning methods, a min-margin based 1 Introduction method and a Shannon-entropy based one, to the Corpus-based methods for word sense task of the disambiguation of English verb senses. disambiguation (WSD) have gained popularity in The contribution of our work is not only in recent years. As evidenced by the SENSEVAL demonstrating that these methods work well for the exercises (http://www.sense"
N06-1016,P02-1016,0,0.226056,"Missing"
N06-1016,J98-4002,0,\N,Missing
N06-1016,N06-2015,1,\N,Missing
N06-1016,I05-1081,1,\N,Missing
N06-2015,I05-1081,1,0.260652,"Missing"
N06-2015,N06-1024,1,0.181596,"ed, allowing the entity mentions that are propositional arguments to be resolved in context. Annotation will cover multiple languages (English, Chinese, and Arabic) and multiple genres (newswire, broadcast news, news groups, weblogs, etc.), to create a resource that is broadly applicable. 2 Treebanking The Penn Treebank (Marcus et al., 1993) is annotated with information to make predicate-argument structure easy to decode, including function tags and markers of “empty” categories that represent displaced constituents. To expedite later stages of annotation, we have developed a parsing system (Gabbard et al., 2006) that recovers both of these latter annotations, the first we know of. A firststage parser matches the Collins (2003) parser on which it is based on the Parseval metric, while simultaneously achieving near state-of-the-art performance on recovering function tags (F-measure 89.0). A second stage, a seven stage pipeline of maximum entropy learners and voted perceptrons, achieves state-of-the-art performance (F-measure 74.7) on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods. * This work"
N06-2015,J93-2004,1,0.0841798,"with parse (TreeBank) and propositional (PropBank) structures, which provide normalization over predicates and their arguments. Word sense ambiguities are then resolved, with each word sense also linked to the appropriate node in the Omega ontology. Coreference is also annotated, allowing the entity mentions that are propositional arguments to be resolved in context. Annotation will cover multiple languages (English, Chinese, and Arabic) and multiple genres (newswire, broadcast news, news groups, weblogs, etc.), to create a resource that is broadly applicable. 2 Treebanking The Penn Treebank (Marcus et al., 1993) is annotated with information to make predicate-argument structure easy to decode, including function tags and markers of “empty” categories that represent displaced constituents. To expedite later stages of annotation, we have developed a parsing system (Gabbard et al., 2006) that recovers both of these latter annotations, the first we know of. A firststage parser matches the Collins (2003) parser on which it is based on the Parseval metric, while simultaneously achieving near state-of-the-art performance on recovering function tags (F-measure 89.0). A second stage, a seven stage pipeline of"
N06-2015,J05-1004,1,0.144555,"Missing"
N06-2015,I05-7009,1,0.218789,"e information extraction, summarization and machine translation. The subtle finegrained sense distinctions in WordNet have not lent themselves to high agreement between human annotators or high automatic tagging performance. Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 2004; Palmer et al., 2006), we have developed a process for rapid sense inventory creation and annotation that includes critical links between the grouped word senses and the Omega ontology (Philpot et al., 2005; see Section 5 below). This process is based on recognizing that sense distinctions can be represented by linguists in an hierarchical structure, similar to a decision tree, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntacAnnotate test (2 people) not OK Results: agreement and confusion matrix OK Figure 1. Annotation Procedure As part of OntoNotes we are annotating the most frequent noun and verb se"
N06-2015,P98-1013,0,0.0785645,"her phenomena, including temporal and spatial relations, numerical expressions, deixis, etc. One of the principal aims of OntoNotes is to enable automated semantic analysis. The best current algorithm for semantic role labeling for PropBank style annotation (Pradhan et al., 2005) achieves an F-measure of 81.0 using an SVM. OntoNotes will provide a large amount of new training data for similar efforts. Existing work in the same realm falls into two classes: the development of resources for specific phenomena or the annotation of corpora. An example of the former is Berkeley’s FrameNet project (Baker et al., 1998), which produces rich semantic frames, annotating a set of examples for each predicator (including verbs, nouns and adjectives), and describing the network of relations among the semantic frames. An example of the latter type is the Salsa project (Burchardt et al., 2004), which produced a German lexicon based on the FrameNet semantic frames and annotated a large German newswire corpus. A second example, the Prague Dependency Treebank (Hajic et al., 2001), has annotated a large Czech corpus with several levels of (tectogrammatical) representation, including parts of speech, syntax, and topic/fo"
N06-2015,reeder-etal-2004-interlingual,1,0.282926,"Missing"
N06-2015,W04-2705,0,\N,Missing
N06-2015,W04-2704,1,\N,Missing
N06-2015,P05-1072,0,\N,Missing
N06-2015,C98-1013,0,\N,Missing
N07-1069,H94-1020,0,0.0192645,"chniques for generalizing the semantic role labeling task are still needed. In this paper, we demonstrate an increase in the generality of our semantic role labeling based on a mapping that has been developed between PropBank and another lexical resource, VerbNet. By taking advantage of VerbNet’s more consistent set of labels, we can generate more useful role label annotations with a resulting improvement in SRL performance on novel genres. 2 2.1 Background PropBank PropBank (Palmer et al., 2005) is an annotation of one million words of the Wall Street Journal portion of the Penn Treebank II (Marcus et al., 1994) with predicate-argument structures for verbs, using semantic role labels for each verb argument. In order to remain theory neutral, and to increase annotation speed, role labels were defined on a per-verbsense basis. Although the same tags were used for all verbs, (namely Arg0, Arg1, ..., Arg5), these tags are meant to have a verb-specific meaning. Thus, the use of a given argument label should be consistent across different uses of that verb, including syntactic alternations. For example, the Arg1 (underlined) in “John broke the window” is the same window that is annotated as the Arg1 in “Th"
N07-1069,J05-1004,1,0.483313,"any researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics about how any one verb’s arguments relate to other verbs’ arguments, or about general distinctions between verb arguments and adjuncts. However, there are several limitations to this approach. The first is that it can be difficult to make inferences and generalizations based on"
N07-1069,W05-0634,0,0.0784196,"ssary step for successful natural language processing applications, such as text summarization, question answerAn important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, c Roche"
N07-1069,P05-1072,0,0.0259873,"ssary step for successful natural language processing applications, such as text summarization, question answerAn important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, c Roche"
N07-1069,W05-0639,1,0.935385,"ul natural language processing applications, such as text summarization, question answerAn important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, c Rochester, NY, April 2007. 2007"
N07-1069,W05-0620,0,0.125226,"Missing"
N07-1069,W03-1006,0,0.0608757,"entities and successfully disambiguating the relations between them and their predicates is an important and necessary step for successful natural language processing applications, such as text summarization, question answerAn important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings."
N07-1069,W03-1008,0,0.0523654,"lly disambiguating the relations between them and their predicates is an important and necessary step for successful natural language processing applications, such as text summarization, question answerAn important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank ca"
N07-1069,P05-1073,0,0.102358,"ssing applications, such as text summarization, question answerAn important part of this task is Semantic Role Labeling (SRL), where the goal is to locate the constituents which are arguments of a given verb, and to assign them appropriate semantic roles that describe how they relate to the verb. Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al., 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al., 2005b; Punyakanok et al., 2005; Toutanova et al., 2005). For two years, the CoNLL workshop has made this problem the shared task (Carreras and M´arquez, 2005). However, there is still little consensus in the linguistic and NLP communities about what set of role labels are most appropriate. The Proposition Bank (PropBank) corpus (Palmer et al., 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, . . . , Arg5), and by defining those labels to have verb-specific meanings. Under this scheme, PropBank can avoid making any claims 548 Proceedings of NAACL HLT 2007, pages 548–555, c Rochester, NY, April 2007. 2007 Association for Computat"
N07-1069,N04-1030,0,\N,Missing
N09-3005,N06-1016,1,0.849983,"e instances from the corpus in a way that would ensure that all the senses are as well represented as possible. Because some senses of these verbs are very rare, the pool of instances pre-selected for the annotation should include as many as possible instances of the rare senses. Random sampling – the simplest approach – will clearly not work: the pre-selected data will contain roughly the same proportion of the rare sense instances as the original set. If random sampling is not the answer, the data must be selected in some non-uniform way, i.e. using selective sampling. Active learning (e.g. Chen et al., 2006) is one approach to this problem. Some evidence is available (Zhu and Hovy, 2007) that active learning outperforms random sampling in finding the instances of rare senses. However, active learning has several shortcomings: (1) it requires some annotated data to start the process; (2) it is problematic when the initial training set only contains the data for a single class (e.g. the pseudomonosemous verbs); (3) it is not always efficient in practice: In the OntoNotes project, the data is annotated by two human taggers and the disagreements are adjudicated by the third. In classic active learnin"
N09-3005,P08-2008,1,0.832949,"corpus that contains T candidate instances from which S instances are to be selected for annotation Basic Steps 1. Compute the language model for the corpus 2. Compute the probability distribution over the T candidate instances of the target verb 3. Rank the T candidate instances by their probabilities 4. Form a cluster by selecting S instances with the lowest probability Figure 1. Basic steps of LMS 27 Let us now clarify a few practical points. Although an instance of the target verb can be represented as the entire sentence containing the verb, from the experiments with automatic WSD (e.g. Dligach and Palmer, 2008), it is known that having access to just a few words in the neighborhood of the target verb is sufficient in many cases to predict the sense. For the purpose of LMS we represent an instance as the chunk of text centered upon the target verb plus the surrounding words on both sides within a three-word window. Although the size of the window around the target verb is fixed, the actual number of words in each chunk may vary when the target verb is close to the beginning or the end of sentence. Therefore, we need some form of length normalization. We normalize the log probability of each chunk by"
N09-3005,2005.iwslt-1.7,0,0.0348849,"Missing"
N09-3005,N06-1017,0,0.0129403,". Several studies recently appeared that attempted to apply active learning principles to rare category detection (Pelleg and Moore, 2004; He and Carbonell, 2007). In addition to the issues with active learning outlined in the introduction, the algorithm described in (He and Carbonell, 2007) requires the knowledge of the priors, which is hard to obtain for word senses. WSD has a long history of experiments with unsupervised learning (e.g. Schutze, 1998; Purandare and Peterson, 2004). McCarthy et al. (2004) propose a method for automatically identifying the predominant sense in a given domain. Erk (2006) describes an application of an outlier detection algorithm to the task of identifying the instances of unknown senses. Our task differs from the latter two works in that it is aimed at finding the instances of the rare senses. Finally, the idea of LMS is similar to the techniques for sentence selection based on rare n-gram co-occurrences used in machine translation (Eck et al., 2005) and syntactic parsing (Hwa, 2004). 3 Language Modeling for Data Selection Our method is outlined in Figure 1: Input A large corpus that contains T candidate instances from which S instances are to be selected for"
N09-3005,N06-2015,1,0.699547,"Missing"
N09-3005,D07-1082,0,0.0259394,"Missing"
N09-3005,P04-1036,0,0.0185593,". Weiss, 2004). However, the costs of misclassifying the senses are highly domain specific and hard to estimate. Several studies recently appeared that attempted to apply active learning principles to rare category detection (Pelleg and Moore, 2004; He and Carbonell, 2007). In addition to the issues with active learning outlined in the introduction, the algorithm described in (He and Carbonell, 2007) requires the knowledge of the priors, which is hard to obtain for word senses. WSD has a long history of experiments with unsupervised learning (e.g. Schutze, 1998; Purandare and Peterson, 2004). McCarthy et al. (2004) propose a method for automatically identifying the predominant sense in a given domain. Erk (2006) describes an application of an outlier detection algorithm to the task of identifying the instances of unknown senses. Our task differs from the latter two works in that it is aimed at finding the instances of the rare senses. Finally, the idea of LMS is similar to the techniques for sentence selection based on rare n-gram co-occurrences used in machine translation (Eck et al., 2005) and syntactic parsing (Hwa, 2004). 3 Language Modeling for Data Selection Our method is outlined in Figure 1: Inp"
N09-3005,W04-2406,0,0.0415222,"Missing"
N09-3005,J04-3001,0,\N,Missing
N09-3005,J98-1004,0,\N,Missing
N10-2004,J93-2004,0,0.0372778,"the same open-dialog as you saw in Figure. 3. The [New Tasks] shows a list of tasks that have not been adjudicated, and the [My Tasks] shows a list of tasks that have been adjudicated. Gold mode does not allow adjudicators to open tasks that have not been at least single-annotated. 5 5.1 Figure 3: Open-dialog Once you choose a task and click the [Enter] button, Jubilee’s main window will be prompted (Figure 4). There are three views available in the main window: the treebank view, frameset view and argument view. By default, the treebank view shows the first tree (in the Penn Treebank format (Marcus et al., 1993)) in the selected task. The frameset view displays rolesets and allows the annotator to choose the sense of the predicate with respect to the current tree. The argument view contains buttons representing each of the Propbank argument labels. 15 Gold mode Demonstrations Cornerstone We will begin by demonstrating how to view frameset files in both multi-lemma and unilemma mode. In each mode, we will open an existing frameset file, compare its interface with the actual xml file, and show how intuitive it is to interact with the tool. Next, we will demonstrate how to create and edit a new frameset"
N10-2004,palmer-etal-2008-pilot,1,0.824823,"using one tool that simultaneously provides rich syntactic information as well as comprehensive semantic information. Both Cornerstone and Jubilee are developed in Java (Jdk 6.0), so they run on any platform where the Java virtual machine is installed. They are light enough to run as X11 applications. This aspect is important because Propbank data are usually stored in a server, so annotators need to update them remotely (via ssh). One of the biggest advantages of using these tools is that they accommodate several languages; in fact, the tools have been used for Propbank projects in Arabic (M.Diab et al., 2008), Chinese (Xue and Palmer, 2009), English (Palmer et al., 2005) and Hindi, and have been tested in Korean (Han et al., 2002). This demo paper details how to create Propbank framesets in Cornerstone, and how to annotate Propbank instances using Jubilee. There are two modes in which to run Cornerstone: multi-lemma and uni-lemma mode. In multilemma mode, a predicate can have multiple lem13 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 13–16, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics mas, whereas a predicate can have only one lemma in un"
N10-2004,J05-1004,1,0.555357,"absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation. Cornerstone is a user-friendly xml editor, customized to allow frame authors to create and edit frameset files. Both tools have been successfully adapted to many Propbank projects; they run platform independently, are light enough to run as X11 applications and support multiple languages such as Arabic, Chinese, English, Hindi and Korean. 1 Introduction Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles (Palmer et al., 2005). Propbank annotation also requires the choice of a sense id for each predicate. Thus, for each predicate in the Propbank, there exists a corresponding frameset file encompassing one or more senses of the predicate. All frameset files are written in xml, which is somewhat difficult to read and edit. Although there already exist many xml editors, most of them require some degree of knowledge of xml, and none of them are specifically customized for frameset files. This motivated the development of our own frameset editor, Cornerstone. Jubilee is a Propbank instance editor. For each verb predicat"
N10-2004,han-etal-2002-development,1,\N,Missing
nielsen-etal-2008-annotating,W06-2933,0,\N,Missing
nielsen-etal-2008-annotating,W03-0210,0,\N,Missing
nielsen-etal-2008-annotating,W07-1405,1,\N,Missing
nielsen-etal-2008-annotating,C04-1010,0,\N,Missing
nielsen-etal-2008-annotating,J02-3001,0,\N,Missing
nielsen-etal-2008-annotating,J05-1004,1,\N,Missing
nielsen-etal-2008-annotating,W05-0202,0,\N,Missing
P02-1031,P98-1013,0,0.174814,"Missing"
P02-1031,P97-1003,0,0.54256,"icate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e ect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system rst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentenc"
P02-1031,W00-0726,0,0.0435352,"Missing"
P02-1031,J02-3001,1,0.903105,"ing the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e ect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system rst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this e ect. Of the systems evalu"
P02-1031,kingsbury-palmer-2002-treebank,1,0.505,"semantic phenomena including quanti cation, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler nite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for speci c semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles. Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of in"
P02-1031,C98-1013,0,\N,Missing
P02-1031,A97-1029,0,\N,Missing
P04-1038,C02-1143,1,0.794756,"of each verb in the test data. 5.1 Evaluation Methods We use two external quality measures, purity and normalized mutual information (NMI) (Strehl. 2002) to evaluate the clustering performance. Assuming a verb has l senses, the clustering model assigns n instances of the verb into k clusters, ni is the size of the ith cluster, n j is the number of instances hand-tagged with the jth sense, and nij is the number of instances with the jth sense in the ith cluster, purity is defined in equation (4): purity = 1 k max nij ∑ n i =1 j (4) 10 The sense-tagged PDN data we used here are the same as in (Dang et al., 2002). 11 It is calculated as the entropy of the sense distribution of a verb in the test data divided by the largest possible entropy, i.e., log2 (the number of senses of the verb in the test data). It can be interpreted as classification accuracy when for each cluster we treat the majority of instances that have the same sense as correctly classified. The baseline purity is calculated by treating all instances for a target verb in a single cluster. The purity measure is very intuitive. In our case, since the number of clusters is preset to the number of senses, purity for verbs with two senses is"
P04-1038,J02-3001,0,0.041618,"on large unannotated corpora and evaluated on a relatively small sense-tagged corpus, it can be used to find indicative features for sense distinctions through exploring huge amount of available unannotated text data. The EM clustering algorithm (Hofmann and Puzicha, 1998) used here is an unsupervised machine learning algorithm that has been applied in many NLP tasks, such as inducing a semantically labeled lexicon and determining lexical choice in machine translation (Rooth et al., 1998), automatic acquisition of verb semantic classes (Schulte im Walde, 2000) and automatic semantic labeling (Gildea and Jurafsky, 2002). In our task, we equipped the EM clustering model with rich linguistic features that capture the predicate-argument structure information of verbs and restricted the feature set for each verb using knowledge from dictionaries. We also semiautomatically built a semantic taxonomy for Chinese nouns based on two Chinese electronic semantic dictionaries, the Hownet dictionary1 and the Rocling dictionary.2 The 7 top-level categories of this taxonomy were used as semantic features for the model. Since external knowledge is used to obtain the semantic features and guide feature selection, the model i"
P04-1038,C00-2108,0,0.0202669,"on, since the clustering model can be trained on large unannotated corpora and evaluated on a relatively small sense-tagged corpus, it can be used to find indicative features for sense distinctions through exploring huge amount of available unannotated text data. The EM clustering algorithm (Hofmann and Puzicha, 1998) used here is an unsupervised machine learning algorithm that has been applied in many NLP tasks, such as inducing a semantically labeled lexicon and determining lexical choice in machine translation (Rooth et al., 1998), automatic acquisition of verb semantic classes (Schulte im Walde, 2000) and automatic semantic labeling (Gildea and Jurafsky, 2002). In our task, we equipped the EM clustering model with rich linguistic features that capture the predicate-argument structure information of verbs and restricted the feature set for each verb using knowledge from dictionaries. We also semiautomatically built a semantic taxonomy for Chinese nouns based on two Chinese electronic semantic dictionaries, the Hownet dictionary1 and the Rocling dictionary.2 The 7 top-level categories of this taxonomy were used as semantic features for the model. Since external knowledge is used to obtain th"
P04-1038,J98-1004,0,0.0202707,"odel’s performance for the three most challenging verbs chosen from the first set of experiments. 1 Introduction Highly ambiguous words may lead to irrelevant document retrieval and inaccurate lexical choice in machine translation (Palmer et al., 2000), which suggests that word sense disambiguation (WSD) is beneficial and sometimes even necessary in such NLP tasks. This paper addresses WSD in Chinese through developing an Expectation-Maximization (EM) clustering model to learn Chinese verb sense distinctions. The major goal is to do sense discrimination rather than sense labeling, similar to (Schütze, 1998). The basic idea is to divide instances of a word into several clusters that have no sense labels. The instances in the same cluster are regarded as having the same meaning. Word sense discrimination can be applied to document retrieval and similar tasks in information access, and to facilitating the building of large annotated corpora. In addition, since the clustering model can be trained on large unannotated corpora and evaluated on a relatively small sense-tagged corpus, it can be used to find indicative features for sense distinctions through exploring huge amount of available unannotated"
P05-1006,W02-0811,0,0.0179103,"Missing"
P05-1006,N01-1012,0,0.0663794,"Missing"
P05-1006,kingsbury-palmer-2002-treebank,1,0.791715,"ents and their semantic classes. We describe our approach to automatic WSD of verbs using maximum entropy models to combine information from lexical collocations, syntax, and semantic class constraints on verb arguments. The system performs at the best published accuracy on the English verbs of the Senseval-2 (Palmer et al., 2001) exercise on evaluating automatic WSD systems. The Senseval-2 verb instances have been manually tagged with their WordNet sense and come primarily from the Penn Treebank WSJ. The WSJ corpus has also been manually annotated for predicate arguments as part of PropBank (Kingsbury and Palmer, 2002), and the intersection of PropBank and Senseval-2 forms a corpus containing gold-standard annotations of WordNet senses and PropBank semantic role labels. This provides a unique opportunity to investigate the role of predicate arguments in verb sense disambiguation. We show that our system’s accuracy improves significantly by adding features from PropBank, which explicitly encodes the predicate-argument informa42 Proceedings of the 43rd Annual Meeting of the ACL, pages 42–49, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics tion that our original set of syntactic and sema"
P05-1006,W02-1006,0,0.0933666,"Missing"
P05-1006,S01-1005,1,0.919686,"er extraction of semantic roles. 1 Introduction A word can have different meanings depending on the context in which it is used. Word Sense Disambiguation (WSD) is the task of determining the correct meaning (“sense”) of a word in context, and several efforts have been made to develop automatic WSD systems. Early work on WSD (Yarowsky, 1995) was successful for easily distinguishable homonyms like bank, which have multiple unrelated meanings. While homonyms are fairly tractable, highly polysemous verbs, which have related but subtly distinct senses, pose the greatest challenge for WSD systems (Palmer et al., 2001). Verbs are syntactically complex, and their syntax is thought to be determined by their underlying semantics (Grimshaw, 1990; Levin, 1993). Levin verb In this paper we show that the performance of automatic WSD systems can be improved by using richer linguistic features that capture information about predicate arguments and their semantic classes. We describe our approach to automatic WSD of verbs using maximum entropy models to combine information from lexical collocations, syntax, and semantic class constraints on verb arguments. The system performs at the best published accuracy on the Eng"
P05-1006,N04-1030,0,0.0493969,"Missing"
P05-1006,A97-1029,0,0.015289,"as to see the extent to which sense-tagging of verbs could be improved by adding features that capture information about predicate-arguments and selectional restrictions. We used the Mallet toolkit (McCallum, 2002) for learning maximum entropy models with Gaussian priors for all our experiments. In order to extract the linguistic features necessary for the models, all sentences containing the target word were automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes.1 2.1 Topical features We categorized the possible model features into topical features and several types of local contextual features. Topical features for a verb in a sentence look for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). These features are supposed to show the domain in which the verb is being used, since some verb senses are used in only certain domains. The set of keywords is specific to each verb lemma to"
P05-1006,P97-1003,0,\N,Missing
P05-1067,J00-1004,0,0.163407,"ally, by combining the natural language syntax and machine learning methods, a broad-coverage and linguistically wellmotivated statistical MT system can be constructed. However, structural divergences between languages (Dorr, 1994)，which are due to either systematic differences between languages or loose translations in real corpora，pose a major challenge to syntax-based statistical MT. As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) represents each production in parallel dependency trees as a finite-state transducer. Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-i"
P05-1067,J93-2003,0,0.0118088,"Missing"
P05-1067,W04-1513,1,0.697681,"ee transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing 541 Proceedings of the 43rd Annual Meeting of the ACL, pages 541–548, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches, though different in formalism, model the two languages using tree-based transduction rules or a synchronous grammar, possibly probabilistic, and using multi-lemma elementary structures as atomic units. The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process. However, few of the above mentioned formalisms hav"
P05-1067,J94-4004,0,0.0588896,"recent years, syntax-based statistical machine translation, which aims at applying statistical models to structural data, has begun to emerge. With the research advances in natural language parsing, especially the broad-coverage parsers trained from treebanks, for example (Collins, 1999), the utilization of structural analysis of different languages has been made possible. Ideally, by combining the natural language syntax and machine learning methods, a broad-coverage and linguistically wellmotivated statistical MT system can be constructed. However, structural divergences between languages (Dorr, 1994)，which are due to either systematic differences between languages or loose translations in real corpora，pose a major challenge to syntax-based statistical MT. As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) represents each production in parallel dependency trees as a finite-state transducer. Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorph"
P05-1067,P03-2041,0,0.687861,", 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing 541 Proceedings of the 43rd Annual Meeting of the ACL, pages 541–548, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches, though different in formalism, model the two languages using tree-based transduction rules or a syn"
P05-1067,W02-1039,0,0.521626,"Missing"
P05-1067,P01-1030,0,0.0345352,"words and the shared forest representation has m nodes. Suppose for each word, there are maximally k different ETs containing it, we have m ≤ kn . Let b be the max breadth factor in the packed forest, it can be shown that the decoder visits at most mb nodes during execution. Hence, we have: T (decoding ) ≤ O(kbn) (11) which is linear to the input size. Combined with a polynomial time parsing algorithm, the whole decoding process is polynomial time. 5 Onaizan et al., 1999; Och and Ney, 2003). The IBM models were trained on the same training data as our system. We used the ISI Rewrite decoder (Germann et al. 2001) to decode the IBM models. The results are shown in Figure 9. The score types “I” and “C” stand for individual and cumulative n-gram scores. The final NIST and Bleu scores are marked with bold fonts. Systems Score Type NIST I Bleu IBM Model 4 NIST C Bleu NIST I Bleu SDIG NIST C Bleu 1-gram 2-gram 3-gram 4-gram 2.562 0.714 2.562 0.470 5.130 0.688 5.130 0.674 0.412 0.267 2.974 0.287 0.763 0.224 5.892 0.384 0.051 0.099 3.025 0.175 0.082 0.075 5.978 0.221 0.008 0.040 3.034 0.109 0.013 0.029 5.987 0.132 Figure 9. Evaluation Results. Evaluation We implemented the above approach for a Chinese-English"
P05-1067,P03-1011,0,0.133367,"uction rules are usually computationally expensive if all the possible operations and elementary structures are allowed. The exhaustive search for all the possible sub-sentential structures in a syntax tree of a sentence is NP-complete. 3. The problem is aggravated by the non-perfect training corpora. Loose translations are less of a problem for string based approaches than for approaches that require syntactic analysis. Hajic et al. (2002) limited non-isomorphism by n-to-m matching of nodes in the two trees. However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. In a different approach, Hwa et al. (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees. This motivated us to look for a more efficient and effective way to induce a synchronous grammar from parallel corpora and to build an MT system that performs competitively with the pure 542 statistical MT systems. We chose to build the synchronous gramma"
P05-1067,N04-1014,0,0.0768647,"orphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing 541 Proceedings of the 43rd Annual Meeting of the ACL, pages 541–548, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches, though different in formalism, model the two languages using tree-based transduction rules or a synchronous grammar, possibly probabilistic, and using multi-lemma elementary structures as atomic units. The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process. However, few of the above mentioned formalisms have large scale implementations. And to the best of our knowledge, the advantages of syntax based statist"
P05-1067,P02-1050,0,0.0683235,"a sentence is NP-complete. 3. The problem is aggravated by the non-perfect training corpora. Loose translations are less of a problem for string based approaches than for approaches that require syntactic analysis. Hajic et al. (2002) limited non-isomorphism by n-to-m matching of nodes in the two trees. However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. In a different approach, Hwa et al. (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees. This motivated us to look for a more efficient and effective way to induce a synchronous grammar from parallel corpora and to build an MT system that performs competitively with the pure 542 statistical MT systems. We chose to build the synchronous grammar on the parallel dependency structures of the sentences. The synchronous grammar is induced by hierarchical tree partitioning operations. The rest of this paper describes the system details as follows: Sections 2"
P05-1067,N03-1021,0,0.0700316,"astic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing 541 Proceedings of the 43rd Annual Meeting of the ACL, pages 541–548, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches, though different in formalism, model the two languages using tree-based transduction rules or a synchronous grammar, possibly probabilistic, and using multi-lemma elementary structures as atomic units. The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process. However, few of the above mentioned formali"
P05-1067,P02-1040,0,0.108588,"Missing"
P05-1067,J03-1002,0,0.00911693,"structure, utilizing the dynamic programming property of the tree structures explicitly. Suppose the input sentence has n words and the shared forest representation has m nodes. Suppose for each word, there are maximally k different ETs containing it, we have m ≤ kn . Let b be the max breadth factor in the packed forest, it can be shown that the decoder visits at most mb nodes during execution. Hence, we have: T (decoding ) ≤ O(kbn) (11) which is linear to the input size. Combined with a polynomial time parsing algorithm, the whole decoding process is polynomial time. 5 Onaizan et al., 1999; Och and Ney, 2003). The IBM models were trained on the same training data as our system. We used the ISI Rewrite decoder (Germann et al. 2001) to decode the IBM models. The results are shown in Figure 9. The score types “I” and “C” stand for individual and cumulative n-gram scores. The final NIST and Bleu scores are marked with bold fonts. Systems Score Type NIST I Bleu IBM Model 4 NIST C Bleu NIST I Bleu SDIG NIST C Bleu 1-gram 2-gram 3-gram 4-gram 2.562 0.714 2.562 0.470 5.130 0.688 5.130 0.674 0.412 0.267 2.974 0.287 0.763 0.224 5.892 0.384 0.051 0.099 3.025 0.175 0.082 0.075 5.978 0.221 0.008 0.040 3.034 0."
P05-1067,C90-3045,0,0.391805,"ions directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner (2003) proposed viewing the MT problem as a probabilistic synchronous tree substitution grammar parsing 541 Proceedings of the 43rd Annual Meeting of the ACL, pages 541–548, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics problem. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. All these approaches"
P05-1067,J97-3002,0,0.0399813,"ple (Collins, 1999), the utilization of structural analysis of different languages has been made possible. Ideally, by combining the natural language syntax and machine learning methods, a broad-coverage and linguistically wellmotivated statistical MT system can be constructed. However, structural divergences between languages (Dorr, 1994)，which are due to either systematic differences between languages or loose translations in real corpora，pose a major challenge to syntax-based statistical MT. As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) represents each production in parallel dependency trees as a finite-state transducer. Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be a"
P05-1067,P01-1067,0,0.75833,"o either systematic differences between languages or loose translations in real corpora，pose a major challenge to syntax-based statistical MT. As a result, the syntax based MT systems have to transduce between non-isomorphic tree structures. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) represents each production in parallel dependency trees as a finite-state transducer. Both approaches learn the tree representations directly from parallel sentences, and do not make allowances for nonisomorphic structures. (Yamada and Knight, 2001, 2002) modeled translation as a sequence of tree operations transforming a syntactic tree into a string of the target language. When researchers try to use syntax trees in both languages, the problem of non-isomorphism must be addressed. In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. Eisner"
P05-1067,P02-1039,0,0.0686248,"Missing"
P05-1067,W03-1608,0,\N,Missing
P05-1067,J03-4003,0,\N,Missing
P05-1067,P04-1083,0,\N,Missing
P05-1067,P05-1012,0,\N,Missing
P05-1067,J08-3004,0,\N,Missing
P05-1067,W90-0102,0,\N,Missing
P06-2118,W04-0807,0,0.0819859,"Missing"
P06-2118,P04-1038,1,0.849462,"segmented, POS-tagged and parsed version of the same text to facilitate the extraction of the different types of features. The extraction of the semantic role labels as features requires the use of a semantic role tagger, which we describe in greater detail in Section 2.2. In addition to using the semantic role labeling information, we also extract another type of semantic features from the verb’s NP arguments. These features are top-level semantic categories from a three-level general taxonomy for Chinese nouns, which was created semi-automatically based on two Chinese semantic dictionaries (Chen and Palmer, 2004). 2.1 A Comparison with Our English WSD System Similar to our English WSD system, which achieved the best published results on SENSEVAL2 English verbs for both finegrained and coarse-grained senses (Chen and Palmer, 2005), our Chinese WSD system uses the same smoothed MaxEnt machine learning model and linguistically motivated features for Chinese verb sense disambiguation. However, the features used in the two systems differ 922 somewhat due to the different properties of the two languages . For example, our English system uses the inflected form and the part-of-speech tag of the target verb a"
P06-2118,S01-1005,1,0.880072,"Missing"
P06-2118,J05-1004,1,0.415007,"勒 腰腰 集 tighten waistband collect 公公 (direct object)。 highway speech tags as collocation features. A further investigation on the different sizes of the context window (3,5,7,9,11) showed that increasing the window size decreased our system’s accuracy. 2.2 Features Based on Automatic Semantic Role Tagging In a recent paper on the WSD of English verbs, Dang and Palmer (2005) showed that semantic role information significantly improves the WSD accuracy of English verbs for both fine-grained and coarse-grained senses. However, this result assumes the human annotation of the Penn English Propbank (Palmer et al, 2005). It seems worthwhile to investigate whether the semantic role information produced by a fully automatic Semantic Role tagger can improve the WSD accuracy on verbs, and test the hypothesis that the senses of a verb have a high correlation to the arguments it takes. To that end, we assigned semantic role labels to the arguments of the target verb with a fully automatic semantic role tagger (Xue and Palmer, 2005) trained on the Chinese Propbank (CPB) (Xue and Palmer, 2003), a corpus annotated with semantic role labels that are similiar in style to the Penn English Propbank. In this annotation, c"
P06-2118,C02-1143,1,0.842499,"Missing"
P06-2118,P05-1006,1,0.846016,"ommon nouns that often require determiners such as the, a or an, Chinese common nouns can stand alone. Therefore, the direct object of a verb often occurs right after the verb in Chinese, as shown in (2). （2）动动 mobilize 资 funds 群群 people 开 build 勒勒 腰腰 集 tighten waistband collect 公公 (direct object)。 highway speech tags as collocation features. A further investigation on the different sizes of the context window (3,5,7,9,11) showed that increasing the window size decreased our system’s accuracy. 2.2 Features Based on Automatic Semantic Role Tagging In a recent paper on the WSD of English verbs, Dang and Palmer (2005) showed that semantic role information significantly improves the WSD accuracy of English verbs for both fine-grained and coarse-grained senses. However, this result assumes the human annotation of the Penn English Propbank (Palmer et al, 2005). It seems worthwhile to investigate whether the semantic role information produced by a fully automatic Semantic Role tagger can improve the WSD accuracy on verbs, and test the hypothesis that the senses of a verb have a high correlation to the arguments it takes. To that end, we assigned semantic role labels to the arguments of the target verb with a f"
P06-2118,W03-1707,1,0.835673,"r both fine-grained and coarse-grained senses. However, this result assumes the human annotation of the Penn English Propbank (Palmer et al, 2005). It seems worthwhile to investigate whether the semantic role information produced by a fully automatic Semantic Role tagger can improve the WSD accuracy on verbs, and test the hypothesis that the senses of a verb have a high correlation to the arguments it takes. To that end, we assigned semantic role labels to the arguments of the target verb with a fully automatic semantic role tagger (Xue and Palmer, 2005) trained on the Chinese Propbank (CPB) (Xue and Palmer, 2003), a corpus annotated with semantic role labels that are similiar in style to the Penn English Propbank. In this annotation, core arguments such as agent or theme are labeled with numbered arguments such as Arg0 and Arg1, up to Arg5 while adjunct-like elements are assigned functional tags such as TMP (for temporal), MNR, prefixed by ArgM. The Semantic Role tagger takes as input syntactic parses produced by the parser described above as input and produces a list of arguments for each of the sense-tagged target verbs and assigns argument labels to them. Features are extracted from both the core a"
P06-2118,N06-2015,1,0.814745,"Missing"
P06-2118,I05-3012,0,0.036656,"Missing"
P06-2118,I05-1081,1,\N,Missing
P06-2118,W04-0847,0,\N,Missing
P08-2008,I05-1081,1,0.852506,"Missing"
P08-2008,P90-1034,0,0.352793,"rward: (1) find the noun object of the ambiguous verb (2) extract the DDNs for that noun (3) sort the DDNs by frequency and keep the top 50 (4) include these DDNs in the feature vector so that each of the extracted verbs becomes a separate feature. 4 At the core of our work lies the notion of distrib utional similarity (Harris, 1968), which states that similar words occur in similar contexts. In various sources, the notion of context ranges from bag-ofwords-like approaches to more structured ones in which syntax plays a role. Schutze (1998) used bag-of-words contexts for sense discrimination. Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts. Our approach is similar with the difference that we do not group noun arguments into finite categories, but instead leave the category boundaries blurry and allow overlaps. The DDNs are essentially a form of world knowledge which we extract automatically and apply to VSD. Other researches attacked the problem of unsupervised extraction of world know ledge: Schubert (2003) reports a method for extracting general facts about the world from treebanked Brown corpus. Lin and Pantel in (2001) describe their"
P08-2008,W03-0902,0,0.0145008,"Missing"
P08-2008,J98-1004,0,0.00836134,"action of DDNs, the algorithm for applying them to VSD is straightforward: (1) find the noun object of the ambiguous verb (2) extract the DDNs for that noun (3) sort the DDNs by frequency and keep the top 50 (4) include these DDNs in the feature vector so that each of the extracted verbs becomes a separate feature. 4 At the core of our work lies the notion of distrib utional similarity (Harris, 1968), which states that similar words occur in similar contexts. In various sources, the notion of context ranges from bag-ofwords-like approaches to more structured ones in which syntax plays a role. Schutze (1998) used bag-of-words contexts for sense discrimination. Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts. Our approach is similar with the difference that we do not group noun arguments into finite categories, but instead leave the category boundaries blurry and allow overlaps. The DDNs are essentially a form of world knowledge which we extract automatically and apply to VSD. Other researches attacked the problem of unsupervised extraction of world know ledge: Schubert (2003) reports a method for extracting general facts about the world fr"
P08-2061,J02-3001,0,0.047315,"ings of ACL-08: HLT, Short Papers (Companion Volume), pages 241–244, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These reference answers were manually decomposed into fine-grained facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by determining the dependency parse following the style of MaltParser (Nivre et al., 2006). This dependency parse was then modified in several ways. The rationale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to generate features for the assessment classification task. These types of modifications to the parser output address known limitati"
P08-2061,nielsen-etal-2008-annotating,1,0.880816,"Missing"
P08-2061,W06-2933,0,0.0351744,"Papers (Companion Volume), pages 241–244, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These reference answers were manually decomposed into fine-grained facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by determining the dependency parse following the style of MaltParser (Nivre et al., 2006). This dependency parse was then modified in several ways. The rationale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to generate features for the assessment classification task. These types of modifications to the parser output address known limitations of current statis"
P08-2061,J05-1004,1,0.355102,"Missing"
P11-2002,N06-1016,1,0.937712,"Missing"
P11-2002,P08-2008,1,0.865802,"Missing"
P11-2002,N09-3005,1,0.858111,"uistics sampled and predominant class enriched seeds. In this paper, we propose a simple automatic approach for selecting the seeds that are rich in the examples of the rare class. We then demonstrate that this approach to seed selection accelerates AL. Finally, we analyze the mechanism of this acceleration. 2 Approach Language Model (LM) Sampling is a simple unsupervised technique for selecting unlabeled data that is enriched with rare class examples. LM sampling involves training a LM on a corpus of unlabeled candidate examples and selecting the examples with low LM probability. Dligach and Palmer (2009) used this technique in the context of word sense disambiguation and showed that rare sense examples tend to concentrate among the examples with low probability. Unfortunately these authors provided a limited evaluation of this technique: they looked at its effectiveness only at a single selection size. We provide a more convincing evaluation in which the effectiveness of this approach is examined for all sizes of the selected data. Seed Selection for AL is typically done randomly. However, for datasets with a skewed distribution of classes, rare class examples may end up being underrepresente"
P11-2002,N06-2015,1,0.76726,"Missing"
P11-2002,W09-1902,0,0.061909,"Missing"
P11-2121,W09-1210,0,0.00760678,"nglish L AS UAS 88.54 90.57 88.62 90.66 89.15∗ 91.18∗ 88.79 (3) 89.88 (1) - Czech L AS UAS 78.12 83.29 78.30 83.47 80.24∗ 85.24∗ 80.38 (1) 80.11 (2) - Table 3: Accuracy comparisons between different parsing approaches (L AS/UAS: labeled/unlabeled attachment score). ∗ indicates a statistically significant improvement. (#) indicates an overall rank of the system in CoNLL’09. Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Parsing speed (in ms) Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open source project, called ClearParser (code. google.com/p/clearparser). Note that features used in MaltParser have not"
P11-2121,C10-1011,0,0.0114405,"earning models are excluded because they are independent from the parsing algorithms. The average parsing speeds are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre, CN, and Our+, respectively. Our approach shows linear growth all along, even for the sentence groups where some approaches start showing curves. Nivre 22 18 14 CN 10 6 Our+ 2 0 10 20 30 40 50 60 70 Sentence length Figure 1: Average parsing speeds with respect to sentence groups in Table 4. 3 Later, ‘Merlo’ and ‘Bohnet” introduced more advanced systems, showing some improvements over their previous approaches (Titov et al., 2009; Bohnet, 2010). 691 < 10 1,415 < 20 2,289 < 30 1,714 < 40 815 < 50 285 < 60 72 < 70 18 Table 4: # of sentences in each group, extracted from both English/Czech evaluation sets. ‘< n’ implies a group containing sentences whose lengths are less than n. We also measured average parsing speeds for ‘Our’, which showed a very similar growth to ‘Our+’. The average parsing speed of ‘Our’ was 2.20 ms; it performed slightly faster than ‘Our+’ because it skipped more nodes by performing more non-deterministic SHIFT’s, which may or may not have been correct decisions for the corresponding parsing states. It is worth me"
P11-2121,D07-1101,0,0.007977,"non-projective parsing by selecting LEFT-POP or LEFT-ARC , respectively. Our experiments show that this additional transition improves both parsing accuracy and speed. The advantage derives from improving the efficiency of the choice mechanism; it is now simply a transition choice and requires no additional processing. 3 Bootstrapping automatic parses tially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using goldTransition-based parsing has"
P11-2121,cer-etal-2010-parsing,0,0.0356787,"Missing"
P11-2121,W10-3110,0,0.00549945,"g as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because"
P11-2121,W09-1205,0,0.00769796,"is even more significant in a language like Czech for which parsers generally perform more poorly. CN Our Our+ Merlo Bohnet English L AS UAS 88.54 90.57 88.62 90.66 89.15∗ 91.18∗ 88.79 (3) 89.88 (1) - Czech L AS UAS 78.12 83.29 78.30 83.47 80.24∗ 85.24∗ 80.38 (1) 80.11 (2) - Table 3: Accuracy comparisons between different parsing approaches (L AS/UAS: labeled/unlabeled attachment score). ∗ indicates a statistically significant improvement. (#) indicates an overall rank of the system in CoNLL’09. Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Parsing speed (in ms) Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open"
P11-2121,P10-1001,0,0.016745,"arsing by selecting LEFT-POP or LEFT-ARC , respectively. Our experiments show that this additional transition improves both parsing accuracy and speed. The advantage derives from improving the efficiency of the choice mechanism; it is now simply a transition choice and requires no additional processing. 3 Bootstrapping automatic parses tially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using goldTransition-based parsing has the advantage of using p"
P11-2121,H05-1066,0,0.152438,"Missing"
P11-2121,P08-1108,0,0.0813248,"n discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 Martha Palme"
P11-2121,P05-1013,0,0.0808087,"that this implementation reduces the parsing complexity from O(n2 ) to linear time in practice (a worst-case complexity is O(n2 )). We suggest another transition-based parsing approach that reduces the search space even more. The idea is to merge transitions in Choi-Nicolov’s non-projective algorithm with transitions in Nivre’s projective algorithm (Nivre, 2003). Nivre’s projective algorithm has a worst-case complexity of O(n), which is faster than any non-projective parsing algorithm. Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. Ideally, it is better to perform projective parsing for most cases and perform non-projective parsing only when it is needed. In this algorithm, we add another transition to Choi-Nicolov’s approach, LEFT-POP , similar to the L EFT-A RC transition in Nivre’s projective algorithm. By adding this transition, an oracle can now choose either projective or non-projective parsing depending on parsing states.1 1 We also tried adding the R IGHT-A RC transition from Nivre’s projective algorithm, which did not improve parsing performa"
P11-2121,W03-3017,0,0.604553,"t improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 Martha Palmer Department of Linguistics University of Colorado at Boulder mpalmer@colorado.edu approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n)"
P11-2121,J08-4003,0,0.587147,"a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 Martha Palmer Department of Linguistics University of Colorado at Boulder mpalmer@colorado.edu approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2 ) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use p"
P11-2121,P09-1040,0,0.47951,"ally performs noticeably faster than the graph-based 687 Martha Palmer Department of Linguistics University of Colorado at Boulder mpalmer@colorado.edu approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2 ) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use parse history as features to make the next prediction. This parse information helps to improve parsing accuracy without hurting parsing complexity (Nivre, 2006). Most current transition-based approaches use gold-standard parses as features during training; however, this is not necessarily w"
P11-2121,P08-1066,0,0.0108105,"n-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn"
P11-2121,D08-1059,0,0.134199,"nsitive. Furthermore, S EARN interpolates the current policy with the previous policy whereas we do not perform such interpolation. During decoding, S EARN generates a sequence of decisions and makes a final prediction. In our case, the decisions are predicted dependency relations and the final prediction is a dependency tree. S EARN has been successfully adapted to several NLP tasks such as named entity recognition, syntactic chunking, and POS tagging. To the best of our knowledge, this is the first time that this idea has been applied to transition-based parsing and shown promising results. Zhang and Clark (2008) suggested a transitionbased projective parsing algorithm that keeps B different sequences of parsing states and chooses the 690 one with the best score. They use beam search and show a worst-case parsing complexity of O(n) given a fixed beam size. Similarly to ours, their learning mechanism using the structured perceptron algorithm involves training on automatically derived parsing states that closely resemble potential states encountered during decoding. 5 Experiments 5.1 Corpora and learning algorithm All models are trained and tested on English and Czech data using automatic lemmas, POS ta"
P11-2121,W09-1201,0,\N,Missing
P12-1028,P98-1013,0,0.214058,"t something like in: ... Michelle blabs about it to a sandwich man while ordering lunch over the phone . Introduction Verb classification is a fundamental topic of computational linguistics research given its importance for understanding the role of verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: .. ordered the lunch to be delivered .. in Ve"
P12-1028,W11-0110,1,0.906954,"in a specific argument position and the above verb order is a clear example. In the direct object position of the example sentence for the first sense 60.1 of order, we found 264 commission in the role PATIENT of the predicate. It clearly satisfies the +A NIMATE/+O RGANIZATION restriction on the PATIENT role. This is not true for the direct object dependency of the alternative sense 13.5.1, which usually expresses the T HEME role, with unrestricted type selection. When properly generalized, the direct object information has thus been shown highly predictive about verb sense distinctions. In (Brown et al., 2011), the so called dynamic dependency neighborhoods (DDN), i.e., the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e.g., WordNet). The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the a"
P12-1028,H05-1091,0,0.0169362,"Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 20"
P12-1028,A00-2018,0,0.147273,"Missing"
P12-1028,P02-1034,0,0.405859,"006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Mosch"
P12-1028,P10-1025,1,0.852968,"ity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIEN"
P12-1028,D11-1096,1,0.930454,"tov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexical nodes 265 are just"
P12-1028,W04-3233,0,0.0650439,"Missing"
P12-1028,P08-2008,1,0.873656,"e about verb sense distinctions. In (Brown et al., 2011), the so called dynamic dependency neighborhoods (DDN), i.e., the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e.g., WordNet). The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenste"
P12-1028,J02-3001,0,0.0553081,"verbs in conveying semantics of natural language (NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: .. ordered the lunch to be delivered .. in Verb class 13.5.1. For such a case, selectional restrictions are needed. These have also been shown to be useful for semantic role classification (Zapirain et al., 2010). Note that their coding in learning algorithms is rather complex: we need to take into a"
P12-1028,P02-1031,1,0.652564,"(NL). Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), which has also fostered production of many automatic approaches to predicate argument extraction. Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al., 2005; Gildea and Palmer, 2002). However, the definition of models for optimally combining lexical and syntactic constraints is Clearly, the syntactic realization can be used to discern the cases above but it would not be enough to correctly classify the following verb occurrence: .. ordered the lunch to be delivered .. in Verb class 13.5.1. For such a case, selectional restrictions are needed. These have also been shown to be useful for semantic role classification (Zapirain et al., 2010). Note that their coding in learning algorithms is rather complex: we need to take into account syntactic structures, which may require a"
P12-1028,P06-1117,1,0.924782,"Missing"
P12-1028,P05-1050,0,0.0134613,"to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structu"
P12-1028,W08-2123,0,0.0663246,"Missing"
P12-1028,P03-1004,0,0.046988,"matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classif"
P12-1028,P05-1024,0,0.023694,"ntered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 20"
P12-1028,N10-1146,1,0.878723,"Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new innovative structures and similarity functions based on LSA. The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexica"
P12-1028,N07-1071,0,0.0210915,"eometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case of sense 60.1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisations. In supervised language learning, when few examples are available, DMs support cost-effective lexical generalizations, often outperforming knowledge based resources (such as WordNet, as in (Pantel et al., 2007)). Obviously, the choice of the context type determines the type of targeted semantic properties. Wider contexts (e.g., entire documents) are shown to suggest topical relations. Smaller contexts tend to capture more specific semantic aspects, e.g. the syntactic behavior, and better capture paradigmatic relations, such as synonymy. In particular, word space models, as described in (Sahlgren, 2006), define contexts as the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of t"
P12-1028,W09-1106,1,0.879694,"Missing"
P12-1028,D09-1012,1,0.868753,"Missing"
P12-1028,W10-2926,1,0.864629,"Missing"
P12-1028,J98-1004,0,0.201143,"a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). In order to generalize the above DDN feature, distributional models are ideal, as they are designed to model all the collocations of a given noun, according to large scale corpus analysis. Their ability to capture lexical similarity is well established in WSD tasks (e.g. (Schutze, 1998)), thesauri harvesting (Lin, 1998), semantic role labeling (Croce et al., 2010)) as well as information retrieval (e.g. (Furnas et al., 1988)). Distributional Models (DMs). These models follow the distributional hypothesis (Firth, 1957) and characterize lexical meanings in terms of context of use, (Wittgenstein, 1953). By inducing geometrical notions of vectors and norms through corpus analysis, they provide a topological definition of semantic similarity, i.e., distance in a space. DMs can capture the similarity between words such as delegation, deputation or company and commission. In case o"
P12-1028,W03-1012,0,0.0171574,"the words appearing in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehd"
P12-1028,W06-2902,0,0.0182858,"get word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 S"
P12-1028,W04-3222,0,0.0290889,"in a n-sized window, centered around a target word. Cooccurrence counts are thus collected in a words-bywords matrix, where each element records the number of times two words co-occur within a single window of word tokens. Moreover, robust weighting schemas are used to smooth counts against too frequent co-occurrence pairs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b"
P12-1028,C00-2137,0,0.0355617,"Missing"
P12-1028,N10-1058,0,0.0368592,"Missing"
P12-1028,W02-1010,0,0.0339909,"irs: Pointwise Mutual Information (PMI) scores (Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels"
P12-1028,N06-1037,0,0.0976457,"Turney and Pantel, 2010) are commonly adopted. Structural Kernels. Tree and sequence kernels have been successfully used in many NLP applications, e.g., parse reranking and adaptation, (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing, e.g., (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition, (Cumby and Roth, 2003), text categorization, e.g., (Cancedda et al., 2003; Gliozzo et al., 2005), and relation extraction, e.g., (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al., 2010; Croce et al., 2011). 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al., 2011). We define new in"
P12-1028,C98-1013,0,\N,Missing
P12-1028,P98-2127,0,\N,Missing
P12-1028,C98-2122,0,\N,Missing
P12-2071,P07-1033,0,0.185376,"Missing"
P12-2071,gimenez-marquez-2004-svmtool,0,0.533414,"Missing"
P12-2071,P07-1096,0,0.0307625,"Missing"
P12-2071,P11-2009,0,0.235986,"Missing"
P12-2071,E09-1087,0,0.0578867,"Missing"
P12-2071,N03-1033,0,0.436616,"p: POS, a: ambiguity class, c∗ : character sequence in wi (e.g., c:2 : the 1st and 2nd characters of wi , cn−1: : the n-1’th and n’th characters of wi ). See Gim´enez and M`arquez (2004) for more details. 2.4 Machine learning Liblinear L2-regularization, L1-loss support vector classification is used for our experiments (Hsieh et al., 2008). From several rounds of cross-validation, learning parameters of (c = 0.2, e = 0.1, B = 0.4) and (c = 0.1, e = 0.1, B = 0.9) are found for the generalized and domain-specific models, respectively (c: cost, e: termination criterion, B: bias). 3 Related work Toutanova et al. (2003) introduced a POS tagging algorithm using bidirectional dependency networks, and showed the best contemporary results. Gim´enez and M`arquez (2004) used one-pass, left-to-right and right-to-left combined tagging algorithm and achieved near state-of-the-art results. Shen et al. 365 (2007) presented a tagging approach using guided learning for bidirectional sequence classification and showed current state-of-the-art results.3 Our individual models (generalized and domainspecific) are similar to Gim´enez and M`arquez (2004) in that we use a subset of their features and take onepass, left-to-right"
P14-1097,D07-1018,0,0.0264447,"jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. Aft"
P14-1097,P03-1009,0,0.103917,"Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarize"
P14-1097,korhonen-etal-2006-large,0,0.0813188,"5.70 64.18 94.66 78.64 99.30 77.38 96.25 F1 52.44 53.10 54.40 55.04 52.24 52.59 53.58 54.24 51.76 57.91 60.14 61.97 65.49 47.38 63.84 52.40 68.90 52.62 71.39 53.13 Table 2: Type-level multi-class evaluations. K represents the (average) number of induced classes. “S” denotes the use of slot-only features and “SW” denotes the use of slot-word pair features. For example, “SW-S” means that slot-word pair features are used for semantic frame induction and slotonly features are used for verb class induction. the evaluations of soft clusterings for their future work. For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies. To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities. That is,"
P14-1097,J04-1003,0,0.236831,"im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce"
P14-1097,de-marneffe-etal-2006-generating,0,0.0182596,"Missing"
P14-1097,P12-1090,0,0.221374,"ng (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consi"
P14-1097,D11-1025,0,0.0208016,"Missing"
P14-1097,N06-1023,1,0.708505,"Missing"
P14-1097,E14-1007,1,0.921209,"wo clustering steps. Our procedure to automatically induce verb classes from verb uses is summarized as follows: 1. induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and 2. induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1. Each of these two steps is described in the following sections in detail. 3.2 Inducing Verb-specific Semantic Frames We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and 1 In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2"
P14-1097,P08-1050,0,0.133537,"een used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im W"
P14-1097,P12-1044,0,0.0164185,"nson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering ste"
P14-1097,N13-1051,0,0.23495,"eir models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure. Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilistic semantic frames automatically induced from a raw corpus. He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012). He did not evaluate the resulting frames as verb classes. In sum, there have been no studies that quantitatively evaluate polysemou"
P14-1097,D09-1138,0,0.0772066,"ed to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives. The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods"
P14-1097,W12-1901,0,0.0523786,"ess mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a v"
P14-1097,W03-0410,0,0.043177,"basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et a"
P14-1097,N09-1064,0,0.0478714,"Missing"
P14-1097,P13-1085,0,0.0170314,"cea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic fr"
P14-1097,W10-2911,0,0.0208275,"eans that semantic similarity based on word overlap is naturally considered by looking at lexical information. We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps. 4 Experiments and Evaluations We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes. Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations. These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation. Finally, we discuss the results of our full experiments. 4.1 Experimental Settings We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus. To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information. Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences. From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words. We focused on verbs whose frequency in the web corpus"
P14-1097,N09-1059,1,0.845519,"tevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce semantic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models. In this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that, we induce verb-specific semantic frames by clustering verb uses. Then, we induce verb classes by clustering these verbspecific semantic frames across verbs. An interesting point here is that we can use exactly the"
P14-1097,P08-1057,0,0.159318,"8; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy. Our novel contributions are summarized as follows: • induce both semantic frames and verb classes from a massiv"
P14-1097,J06-2001,0,0.0724373,"behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Bre"
P14-1097,C10-1113,0,0.0549359,"Missing"
P14-1097,I13-1072,0,0.0156675,"e precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)). However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4 We propose a normalized version of modified purity and inverse purity. This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013). To measure the precision of a clustering, a normalized version of modified purity is defined as follows. Suppose K is the set of automatically induced clusters and G is the set of gold classes. Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class. Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb. Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s class"
P14-1097,D09-1067,0,0.493143,"t need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we prop"
P14-1097,I08-2107,0,0.0199328,"P applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008)"
P14-1097,P13-2129,0,0.0829034,"ing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first indu"
P14-1097,H05-1111,0,0.0376783,"uring the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP. Verb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott"
P14-1097,E12-1003,0,0.0683136,"cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy. Our approach also uses Bayesian methods, but is designed to capture verb polysemy. We summarize a few studies that consider polysemy of verbs in the rest of this section. Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns"
P14-1097,W09-0210,0,0.294698,"tics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009). There have also been many attempts to automatically acquire verb classes with the goal of either adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008). In this paper, we propose an unsupervised me"
P14-1097,H93-1052,0,0.423358,"wing sections in detail. 3.2 Inducing Verb-specific Semantic Frames We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and 1 In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU. To reach 1,000 iterations, which are reported to be optimum, it would take three months. 1032 3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames. These three steps are briefly described below. 3.2.1 Extracting Predicate-argument Structures from a Raw Corpus We apply dependency parsing to a large raw corpus. We use the Stanford parser with S"
P14-2065,W12-1701,0,0.0275992,"E(AGENT, E) Importantly, the semantics listed here is not just for the verb spray but applies to all verbs from the Spray Class whenever they appear in that syntactic frame – that is, VerbNet assumes the Semantic Consistency Hypothesis. VerbNet and its semantic features have been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard et al., 2009). It has also been employed in models of language acquisition (Parisien and Stevenson, 2011; Barak et al., 2012). In general, there has been interest in the NLP literature in using these syntactially-relevant semantic features for shallow semantic parsing (e.g., Giuglea and Moschitti, 2006). 2 Empirical Status of the Semantic Consistency Hypothesis 3 VerbCorner The VerbCorner Project3 is devoted to collecting semantic judgments for a comprehensive set of verbs along a comprehensive set of theoreticallyrelevant semantic dimension. These data can be used to test the Semantic Consistency Hypothesis. Given the prominence of the Semantic Consistency Hypothesis in both theory and practice, one might expect th"
P14-2065,zaenen-etal-2008-encoding,0,0.0120714,"xample Jessica sprayed the wall. Syntax AGENT V T HEME {+LOC|+DEST CONF} D ESTINATION Semantics MOTION(DURING(E), T HEME) N OT(P REP(START(E), T HEME, D ESTINATION)) P REP(END(E), T HEME, D ESTINATION) C AUSE(AGENT, E) Importantly, the semantics listed here is not just for the verb spray but applies to all verbs from the Spray Class whenever they appear in that syntactic frame – that is, VerbNet assumes the Semantic Consistency Hypothesis. VerbNet and its semantic features have been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard et al., 2009). It has also been employed in models of language acquisition (Parisien and Stevenson, 2011; Barak et al., 2012). In general, there has been interest in the NLP literature in using these syntactially-relevant semantic features for shallow semantic parsing (e.g., Giuglea and Moschitti, 2006). 2 Empirical Status of the Semantic Consistency Hypothesis 3 VerbCorner The VerbCorner Project3 is devoted to collecting semantic judgments for a comprehensive set of verbs along a comprehensive set of theoreticall"
P14-2065,W13-2323,0,0.0217691,"Missing"
P81-1029,J76-3005,0,\N,Missing
P86-1004,P81-1029,1,0.907271,"Missing"
P86-1004,H86-1012,1,\N,Missing
P87-1019,P81-1029,1,0.80468,"Missing"
P87-1019,T87-1032,1,\N,Missing
P87-1019,P86-1004,1,\N,Missing
P87-1019,H86-1011,1,\N,Missing
P94-1019,C92-1029,0,0.102596,"Missing"
P94-1019,H86-1003,0,\N,Missing
P98-1046,W98-0104,1,0.78297,"Missing"
P98-1046,W96-0306,0,0.024385,"for these actions to be performed without the end result being achieved, b u t where the cutting manner can still be recognized, i.e., John cut at the loaf. Where break is concerned, the only thing specified is the resulting change of state where the object becomes separated into pieces. If the result is not achieved, there are no attempted breaking actions that can still be recognized. 2.1 A m b i g u i t i e s in Levin c l a s s e s It is not clear how much WordNet synsets should be expected to overlap with Levin 294 classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Doff, 1997). However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semantic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap between Levin and WordNet. The association of sets of syn"
P98-1046,W97-0204,0,0.0350583,"and semantic properties. 1 Introduction The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within well-defined subdomains. The only escape from this limitation will be through the use of automated or semi-automated methods of lexical acquisition. However, the field has yet to develop a clear consensus on guidelines for a computational lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and Sanfilippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the support of DARPA grant N66001-94C-6043, ARO grant DAAH04-94G-0426, and CAPES grant 0914/95-2. 293 One of the most controversial areas has to do with polysemy. What constitutes a clear separation into senses for any one verb, and how can these senses be computationally characterized and distinguished? The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single greatest limitation on the general application of natural language processing techniques. In this paper we specificall"
P98-1046,C94-1038,0,0.125479,"Missing"
P98-1046,J91-4003,0,0.0269656,"at these verbs demonstrate similarly coherent syntactic and semantic properties. 1 Introduction The difficulty of achieving adequate handcrafted semantic representations has limited the field of natural language processing to applications that can be contained within well-defined subdomains. The only escape from this limitation will be through the use of automated or semi-automated methods of lexical acquisition. However, the field has yet to develop a clear consensus on guidelines for a computational lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and Sanfilippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the support of DARPA grant N66001-94C-6043, ARO grant DAAH04-94G-0426, and CAPES grant 0914/95-2. 293 One of the most controversial areas has to do with polysemy. What constitutes a clear separation into senses for any one verb, and how can these senses be computationally characterized and distinguished? The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single greatest limitation on the general application of natural langu"
palmer-etal-1998-rapid,W97-0311,0,\N,Missing
palmer-etal-1998-rapid,A97-1050,0,\N,Missing
palmer-etal-1998-rapid,C94-1024,0,\N,Missing
palmer-etal-1998-rapid,1997.mtsummit-workshop.12,1,\N,Missing
palmer-etal-1998-rapid,P96-1025,0,\N,Missing
palmer-etal-1998-rapid,A97-1039,0,\N,Missing
palmer-etal-2000-semantic,P97-1003,0,\N,Missing
palmer-etal-2000-semantic,P98-1046,1,\N,Missing
palmer-etal-2000-semantic,C98-1046,1,\N,Missing
palmer-etal-2008-pilot,W04-3212,1,\N,Missing
palmer-etal-2008-pilot,W05-0630,0,\N,Missing
palmer-etal-2008-pilot,W03-1006,0,\N,Missing
palmer-etal-2008-pilot,N07-2014,0,\N,Missing
palmer-etal-2008-pilot,P98-1013,0,\N,Missing
palmer-etal-2008-pilot,C98-1013,0,\N,Missing
palmer-etal-2008-pilot,P04-1043,0,\N,Missing
palmer-etal-2008-pilot,S07-1026,1,\N,Missing
palmer-etal-2008-pilot,W05-0620,0,\N,Missing
palmer-etal-2008-pilot,J02-3001,0,\N,Missing
palmer-etal-2008-pilot,P02-1031,1,\N,Missing
palmer-etal-2008-pilot,N04-1030,0,\N,Missing
palmer-etal-2008-pilot,N04-1032,0,\N,Missing
peterson-etal-2014-focusing,N09-3005,1,\N,Missing
peterson-etal-2014-focusing,W05-0634,0,\N,Missing
peterson-etal-2014-focusing,I11-1021,0,\N,Missing
peterson-etal-2014-focusing,bonial-etal-2014-propbank,1,\N,Missing
peterson-etal-2014-focusing,P11-2002,1,\N,Missing
popescu-etal-2014-mapping,D09-1067,0,\N,Missing
popescu-etal-2014-mapping,N06-2015,1,\N,Missing
popescu-etal-2014-mapping,J03-4003,0,\N,Missing
popescu-etal-2014-mapping,P11-1145,0,\N,Missing
popescu-etal-2014-mapping,P98-1046,1,\N,Missing
popescu-etal-2014-mapping,C98-1046,1,\N,Missing
popescu-etal-2014-mapping,P13-1085,0,\N,Missing
popescu-etal-2014-mapping,C04-1109,0,\N,Missing
popescu-etal-2014-mapping,J04-1003,0,\N,Missing
popescu-etal-2014-mapping,D09-1001,0,\N,Missing
popescu-etal-2014-mapping,W09-3021,0,\N,Missing
popescu-etal-2014-mapping,W11-1003,1,\N,Missing
popescu-etal-2014-mapping,P11-2002,1,\N,Missing
popescu-etal-2014-mapping,N13-1051,0,\N,Missing
popescu-etal-2014-mapping,E14-1007,1,\N,Missing
popescu-etal-2014-mapping,P12-1090,0,\N,Missing
popescu-etal-2014-mapping,W04-0834,0,\N,Missing
popescu-etal-2014-mapping,S01-1029,0,\N,Missing
popescu-etal-2014-mapping,D11-1122,0,\N,Missing
Q14-1012,J86-2003,0,0.0356834,"l subdomain. 3 Interpreting ‘Event’ and Temporal Expressions in the Clinical Domain Much prior work has been done on standardizing the annotation of events and temporal expressions in text. The most widely used approach is the ISOTimeML specification (Pustejovsky et al., 2010), an ISO standard that provides a common framework for annotating and analyzing time, events, and event relations. As defined by ISO-TimeML, an E VENT refers to anything that can be said “to obtain or hold true, to happen or to occur”. This is a broad notion of event, consistent with Bach’s use of the term “eventuality” (Bach, 1986) as well as the notion of fluents in AI (McCarthy, 2002). Because the goals of the THYME project involve automatically identifying the clinical timeline for a patient from clincal records, the scope of what should be admitted into the domain of events is interpreted more broadly than in ISO-TimeML3 . Within the THYME-TimeML guideline, an E VENT is anything relevant to the clinical timeline, i.e., anything that would show up on a detailed timeline of the patient’s care or life. The best single-word syntactic head for the E VENT is then used as its span. For example, a diagnosis would certainly"
Q14-1012,S13-2002,1,0.783065,"rk A BEFORE C, then an equivalent inferred TLINK would be used to match it. E VENT and T IMEX 3 IAA was generated based on exact and overlapping spans, respectively. These results are reported in Table 3. The THYME corpus also differs from ISOTimeML in terms of E VENT properties, with the addition of DocTimeRel, ContextualModality and ContextualAspect. IAA for these properties is in Table 4. 7.3 Baseline Systems To get an idea of how much work will be necessary to adapt existing temporal information extraction systems to the clinical domain, we took the freely available ClearTK-TimeML system (Bethard, 2013), 151 which was among the top performing systems in TempEval 2013 (UzZaman et al., 2013), and evaluated its performance on the THYME corpus. ClearTK-TimeML uses support vector machine classifiers trained on the TempEval 2013 training data, employing a small set of features including character patterns, tokens, stems, part-of-speech tags, nearby nodes in the constituency tree, and a small time word gazetteer. For E VENTs and T IMEX 3s, the ClearTK-TimeML system could be applied directly to the THYME corpus. For DocTimeRels, the relation for an E VENT was taken from the TLINK between that E VENT"
Q14-1012,W13-1903,1,0.813475,"ation for these events, (3) the interaction of general and domain-specific events and their importance in the final timeline, and, more generally, (4) the importance of rough temporality and narrative containers as a step towards finer-grained timelines. We have several avenues of ongoing and future work. First, we are working to demonstrate the utility of the THYME corpus for training machine learning models. We have designed support vector machine models with constituency tree kernels that were able to reach an F1-score of 0.737 on an E VENT-T IMEX 3 narrative container identification task (Miller et al., 2013), and we are working on training models to identify events, times and the remaining types of temporal relations. Second, as per our motivating use cases, we are working to integrate this annotation data with timeline visualization tools and to use these annotations in quality-of-care research. For example, we are using temporal reasoning built on this work to investigate the liver toxicity of methotrexate across a large corpus of EHRs (Lin et al., under review)]. Finally, we plan to explore the application of our notion of an event (anything that should be visible on a domain-appropriate timel"
Q14-1012,miltsakaki-etal-2004-penn,0,0.0158666,"h the number of events and times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force"
Q14-1012,W04-0210,0,0.0401525,"d times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force annotations to"
Q14-1012,W11-0419,1,0.93622,"s, as well as to develop state-of-the-art algointerventions and diagnostics which have been thus rithms to train and test on this dataset. far attempted. In other sections, the doctor may outDeriving timelines from news text requires the conline her current plan for the patient’s treatment, then crete realization of context-dependent assumptions later describe the patient’s specific medical history, about temporal intervals, orderings and organization, allergies, care directives, and so forth. underlying the explicit signals marked in the text Most critically for temporal reasoning, each clin(Pustejovsky and Stubbs, 2011). Deriving patient ical note reflects a single time in the patient’s treathistory timelines from clinical notes also involves ment history at which all of the doctor’s statements these types of assumptions, but there are special deare accurate (the D OCTIME), and each section tends mands imposed by the characteristics of the clinical to describe events of a particular timeframe. For narrative. Due to both medical shorthand practices example, ‘History of Present illness’ predominantly and general domain knowledge, many event-event describes events occuring before D OCTIME, whereas relations are"
Q14-1012,pustejovsky-etal-2010-iso,1,0.785939,"e can 2 The Nature of Clinical Documents be both domain-specific and complex, and are often we have been examining left implicit, requiring significant domain knowledge In the THYME corpus, 1 notes from a large healthcare 1,254 de-identified to accurately detect and interpret. In this paper, we discuss the demands on accurately practice (the Mayo Clinic), representing two distinct annotating such temporal information in clinical fields within oncology: brain cancer, and colon cannotes. We describe an implementation and extension cer. To date, we have principally examined two difof ISO-TimeML (Pustejovsky et al., 2010), devel- ferent general types of clinical narrative in our EHRs: oped specifically for the clinical domain, which we clinical notes and pathology reports. Clinical notes are records of physician interactions refer to as the “THYME Guidelines to ISO-TimeML” (“THYME-TimeML”), where THYME stands for with a patient, and often include multiple, clearly “Temporal Histories of Your Medical Events”. A sim- delineated sections detailing different aspects of the plified version of these guidelines formed the basis patient’s care and present illness. These notes are for the 2012 i2b2 medical-domain tempo"
Q14-1012,S13-2001,1,0.830816,"Missing"
Q14-1012,W08-0606,0,0.0123269,"for situations where doctors proffer a diagnosis, but do so cautiously, to avoid legal liability for an incorrect diagnosis or for overlooking a correct one. For example: (3) a. The signal in the MRI is not inconsistent with a tumor in the spleen. b. The rash appears to be measles, awaiting antibody test to confirm. These H EDGED E VENTs are more real than a hypothetical diagnosis, and likely merit inclusion on a timeline as part of the diagnostic history, but must not be conflated with confirmed fact. These (and other forms of uncertainty in the medical domain) are discussed extensively in (Vincze et al., 2008). In contrast, G ENERIC E VENTs do not refer to the patient’s illness or treatment, but instead discuss illness or treatment in general (often in the patient’s specific demographic). For example: (4) In other patients without significant comorbidity that can tolerate adjuvant chemotherapy, there is a benefit to systemic adjuvant chemotherapy. These sections would be true if pasted into any patient’s note, and are often identical chunks of text repeatedly used to justify a course of action or treatment as well as to defend against liability. Contextual Aspect (to distinguish from grammatical as"
S07-1016,W04-2412,0,0.0234898,"Missing"
S07-1016,W05-0620,0,0.029889,"Missing"
S07-1016,N06-1016,1,0.634629,"Missing"
S07-1016,W02-0817,0,0.234086,"Missing"
S07-1016,W04-0827,0,0.0139734,"Missing"
S07-1016,N06-2015,1,0.63929,"between the verb and noun performance seems to indicate that in general the verbs were more difficult than the nouns. However, this might just be owing to this particular test sample having more verbs with higher perplexities, and maybe even ones that are indeed difficult to disambiguate – in spite of high human agreement. The hope is that better knowledge sources can overcome the gap still existing between the system performance and human agreement. Overall, however, this data indicates that the approach suggested by (Palmer, 2000) and that is being adopted in the ongoing OntoNotes project (Hovy et al., 2006) does result in higher system performance. Whether or not the more coarse-grained senses are effective in improving natural language processing applications remains to be seen. Lemma turn.v go.v come.v set.v hold.v raise.v work.v keep.v start.v lead.v see.v ask.v find.v fix.v buy.v begin.v kill.v join.v end.v do.v examine.v report.v regard.v recall.v prove.v claim.v build.v feel.v care.v contribute.v maintain.v complain.v propose.v promise.v produce.v prepare.v explain.v believe.v occur.v grant.v enjoy.v need.v disclose.v point.n position.n defense.n carrier.n order.n exchange.n system.n sourc"
S07-1016,kipper-etal-2006-extending,1,0.60897,"ciated with each tag, we can not make generalizations across verb classes. In contrast, the use of a shared set of role labels, such 91 System UBC-UPC UBC-UPC RTV Without “say” UBC-UPC UBC-UPC RTV Type Open Closed Closed Precision 84.51 85.04 81.82 Recall 82.24 82.07 70.37 F 83.36±0.5 83.52±0.5 75.66±0.6 Open Closed Closed 78.57 78.67 74.15 74.70 73.94 57.85 76.60±0.8 76.23±0.8 65.00±0.9 Table 5: System performance on PropBank arguments. as VerbNet roles, would facilitate both inferencing and generalization. VerbNet has more traditional labels such as Agent, Patient, Theme, Beneficiary, etc. (Kipper et al., 2006). Therefore, we chose to annotate the corpus using two different role label sets: the PropBank role set and the VerbNet role set. VerbNet roles were generated using the SemLink mapping (Loper et al., 2007), which provides a mapping between PropBank and VerbNet role labels. In a small number of cases, no VerbNet role was available (e.g., because VerbNet did not contain the appropriate sense of the verb). In those cases, the PropBank role label was used instead. We proposed two levels of participation in this task: i) Closed – the systems could use only the annotated data provided and nothing el"
S07-1016,W04-0803,0,0.0234443,"guments. (1) a. What do lobsters like to eat? b. Recent studies have shown that lobsters primarily feed on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. Traditionally, SRL systems have been trained on either the PropBank corpus (Palmer et al., 2005) – for two years, the CoNLL workshop (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005) has made this their shared task, or the FrameNet corpus – Senseval-3 used this for their shared task (Litkowski, 2004). However, there is still little consensus in the linguistics and NLP communities about what set of role labels are most appropriate. The PropBank corpus avoids this issue by using theoryagnostic labels (A RG 0, A RG 1, . . . , A RG 5), and by defining those labels to have only verb-specific meanings. Under this scheme, PropBank can avoid making any claims about how any one verb’s arguments relate to other verbs’ arguments, or about general distinctions between verb arguments and adjuncts. However, there are several limitations to this approach. The first is that it can be difficult to make in"
S07-1016,J05-1004,1,0.118792,"semantic systems. For example, in order to determine that question (1a) is answered by sentence (1b), but not by sentence (1c), we must determine the relationships between the relevant verbs (eat and feed) and their arguments. (1) a. What do lobsters like to eat? b. Recent studies have shown that lobsters primarily feed on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. Traditionally, SRL systems have been trained on either the PropBank corpus (Palmer et al., 2005) – for two years, the CoNLL workshop (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005) has made this their shared task, or the FrameNet corpus – Senseval-3 used this for their shared task (Litkowski, 2004). However, there is still little consensus in the linguistics and NLP communities about what set of role labels are most appropriate. The PropBank corpus avoids this issue by using theoryagnostic labels (A RG 0, A RG 1, . . . , A RG 5), and by defining those labels to have only verb-specific meanings. Under this scheme, PropBank can avoid making any claims about how any one verb’s ar"
S07-1017,P98-1013,0,0.00879976,"over 8,000 synsets with over 15,000 words; about 1,400 synsets refer to Named Entities. Shallow approaches to text processing have been garnering a lot of attention recently. Specifically, shallow approaches to semantic processing are making large strides in the direction of efficiently and effectively deriving tacit semantic information from text. Semantic Role Labeling (SRL) is one such approach. With the advent of faster and powerful computers, more effective machine learning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al., 1998) and ProbBank corpora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Ro"
S07-1017,W05-0620,0,0.320515,"Missing"
S07-1017,J02-3001,0,0.231356,"arning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al., 1998) and ProbBank corpora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Role Labeling (SRL) Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL systems exist for Arabic. Challenges of Arabic for SRL Given the deep difference between such languages, this method may not be straightforward. To clarify this point, let us consider Figure 1. It illustrates a sample Arabic syntactic tree with the relevant part of speech tags and arguments defined. The sentence is                     .      ("
S07-1017,J05-1004,1,0.380781,"; about 1,400 synsets refer to Named Entities. Shallow approaches to text processing have been garnering a lot of attention recently. Specifically, shallow approaches to semantic processing are making large strides in the direction of efficiently and effectively deriving tacit semantic information from text. Semantic Role Labeling (SRL) is one such approach. With the advent of faster and powerful computers, more effective machine learning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al., 1998) and ProbBank corpora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Role Labeling (SRL) Xue and Palmer, 2004; Pra"
S07-1017,W04-3212,1,0.868328,"ora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Role Labeling (SRL) Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL systems exist for Arabic. Challenges of Arabic for SRL Given the deep difference between such languages, this method may not be straightforward. To clarify this point, let us consider Figure 1. It illustrates a sample Arabic syntactic tree with the relevant part of speech tags and arguments defined. The sentence is                     .      (                  m$rwE AlAmm AlmtHdp frD mhlp nhAyp l AtAHp AlfrSp AmAm qbrS. meaning ‘The United Nations’ project imposed a final grace period as an o"
S07-1017,J98-1001,0,\N,Missing
S07-1017,C98-1013,0,\N,Missing
S10-1008,S07-1018,1,0.754355,"rd word senses (i.e., frames) for the target words and the participants had to perform role recognition/labeling and null instantiation linking, and a NI only task, in which the test set was already annotated with gold standard semantic argument structures and the participants only had to recognize definite null instantiations and find links to antecedents in the wider context (NI linking). However, it turned out that the basic semantic role labeling task was already quite challenging for our data set. Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al., 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available. In our case the difficulty was increased because our data came from a new genre and domain (i.e., crime fiction, see Section 3.2). Hence, we decided to add standard SRL, i.e., role recognition and labeling, as a third task (SRL only). This task did not involve NI linking. The theory of null complementation used here is the one adopted by FrameNet, which derives from the work of Fillmore (1986).3 Briefly, omissions of core arguments of predicates are categoriz"
S10-1008,erk-pado-2004-powerful,0,0.102782,"Missing"
S10-1008,J02-3001,0,0.0764846,"scourse entities or events. In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization. 1 Introduction Semantic role labeling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. However, semantic role labeling as it is currently defined misses a lot of information due to the fact that it is viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded. This view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation 1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalm"
S10-1008,P86-1004,1,0.741997,"fer and Caroline Sporleder Roser Morante Computational Linguistics CNTS Saarland University University of Antwerp {josefr,csporled}@coli.uni-sb.de Roser.Morante@ua.ac.be Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu Collin Baker ICSI Berkeley, CA 94704 collin@icsi.berkeley.edu projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and tendencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved. Two notable exceptions are Fillmore and Baker (2001) and Burchardt et al. (2005). Fillmore and Baker (2001) analyse a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions"
S10-1008,W09-2417,1,0.499792,"Missing"
S10-1008,H86-1011,1,\N,Missing
S15-1006,P02-1031,1,0.391832,"here the verb does not conform to the expected semantics of the verb (e.g. The crowd laughed the clown off the stage). We expand on a previous study on the classification CMCs (Hwang et al., 2010) to show that CMCs can be successfully identified in the corpus data. In this paper, we present the classifier and the series of experiments carried out to improve its performance. 1 Introduction While natural language processing performance has been improved through the recognition that there is a relationship between the semantics of the verb and the syntactic context in which the verb is realized (Guildea and Palmer, 2002), sentences where the verb does not conform to the expected syntaxsemantic patterning behavior remain problematic. 1. The goalie kicked the ball into the field. 2. The crowd laughed the clown off the stage. These sentences are semantically related – an entity causes a second entity to go along the path described by the prepositional phrase: in 1, the goalie causes the ball to go into the field, and in 2, the crowd causes the clown to go off the stage. 51 Martha Palmer University of Colorado at Boulder Boulder, CO 80309 martha.palmer@colorado.edu While only the verb in the first sentence is gen"
S15-1006,W10-0801,1,0.810544,"Missing"
S15-1006,hwang-etal-2014-criteria,1,0.667498,"f-score values for our experiments. Here we show results for three feature combinations: the Baseline set encoded from the verb’s lemma, the Baseline plus the preposition feature set (Baseline+P), and the Full Set that includes all of the features listed in Section 3.5. The best performance values are bold-faced. The significance of a feature set’s performance was evaluated via a chi-squared test (McNemar, p < 0.05). Statistically significant change from the Baseline feature set is marked with a †. Additionally, for the CMC classification we show the inter-annotator agreement (Gold) f-score (Hwang et al., 2014). Our best performances in CMC classification as measured by the f-score are comparable or higher than the inter annotator agreement f-score. 4.1 Syntactic vs. Semantic Features With the exception of the DISPLACE classifier on the WEB corpus, both the Baseline+P and the Full Set of features perform significantly better than the Baseline in both sets of experiments. It is interesting that the Baseline+P set performs just as well and sometimes better than the full set of feature consistently across the corpora, though the differences in the values are not statistically significant. In order to g"
S15-1006,H94-1020,0,0.354835,". Though the PP in 7 expresses a path, it is not a path over which Jen causes “the highway” to move. 3 3.1 Experimental Setup Corpora Our data comes from the latest version of OntoNotes, version 5.0, (Weischedel et al., 2012). 52 Gold annotations for Penn Treebank, PropBank, and Verb Sense Annotation are available for all of OntoNotes corpora. As we did for the pilot study, we use the Wall Street Journal (WSJ) corpus. This corpus contains over 846K words selected from the non “strictly” financial (e.g., daily market reports) portion of the Wall Street Journal included in the Penn Treebank II (Marcus et al., 1994). We also pull from the smaller of the two WebText (WEB) data sets published in OntoNotes. This corpus contains 85K words selected from English weblogs. This portion of the data is not to be confused with the the larger 200K word web data, which is a separate corpus in OntoNotes. The third corpus used in our experiments is the 200K word Broadcast News (BN) data. OntoNotes’ BN data contains news texts from broadcasting sources such as CNN, ABC, and PRI (Public Radio International). 3.2 Data Selection In order to narrow the data down to a more manageable size for annotation, we exclude instances"
S15-1027,D09-1003,0,0.0492118,"Missing"
S15-1027,C10-3014,0,0.00981914,"h SRL comparison (CoNLL-2005 WSJ) CoNLL-2005 development set. Compared to Zapirain et al. (2013) (table 5), our SP approach had a smaller (but still statistically significant) absolute F1 gain, with most of the gain coming from core argument type improvements. But with a much higher performing baseline system (one of the highest reported results using a single input parse per sentence), the error reduction rate is comparable. 6 Conclusion for future improvement. Notably, these include techniques used for English SP such as computing similarity based on lexical resources (for Chinese - HowNet (Dong et al., 2010)), distributional similarity, latent word language model (Deschacht and Moens, 2009), different variants of LDA topic models, as well as taking advantages of argument constraints in parallel corpora to extract higher quality SP. Acknowledgement We gratefully acknowledge the support of the National Science Foundation CISE-IISRI-0910992, Richer Representations for Machine Translation, DARPA FA8750-09-C-0179 (via BBN) Machine Reading: Ontology Induction: Semlink+, and DARPA HR0011-11-C-0145 (via LDC) BOLT. This work utilized the Janus supercomputer, which is supported by the National Science Foun"
S15-1027,J02-3001,0,0.16139,"ents of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver,"
S15-1027,P11-1112,0,0.0262044,"ts of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, Colorado, June 4–5, 2015. mance of an English PropBank SRL system by 0.4 F1 points using a variety of word similarity measures, from WordNet hierarchy distance to distributional similarity measures. Ritter and Etzioni (2010) reasoned that the set of hidden variables mod"
S15-1027,P98-2127,0,0.678372,"Missing"
S15-1027,W97-0209,0,0.337665,"is paper, we apply SP to Chinese SRL (which has few morphological clues that impacts parsing accuracy) for arguments of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end"
S15-1027,P10-1044,0,0.023193,"ully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, Colorado, June 4–5, 2015. mance of an English PropBank SRL system by 0.4 F1 points using a variety of word similarity measures, from WordNet hierarchy distance to distributional similarity measures. Ritter and Etzioni (2010) reasoned that the set of hidden variables modeled by latent Dirichlet allocation (LDA) naturally represents the semantic structure of a document collection, and the topics generated can be viewed as the latent set of classes that store preferences. The work utilizes LinkLDA, a variant of the standard LDA that models two sets of distributions for each topic simultaneously, with the resulting topics encoding the mutual constraints of a pair of arguments for the same predicate. S´eaghdha and Korhonen (2014) also proposed SP w/ the LDA variants ROOTH-LDA and LEX-LDA. There has also been work on C"
S15-1027,J14-3005,0,0.0531591,"Missing"
S15-1027,P10-2019,0,0.0207172,"enre results Bank 3.0. Only Sinorama has nominal SRL annotations. As table 3 shows, even though the absolute performance is much lower, SP improved the precision and recall in all cases, the nominal SRL score on Sinorama by 2.30 F1 points, and verb SRL score by 0.31-0.46 F1 point. Again, these F1 differences were statistically significant. 225 5.2.1 Comparison Direct performance comparison with previous Chinese SRL systems is a bit difficult: Xue (2008), Zhuang and Zong (2010) trained the syntactic parsers with an additional 250K word broadcast news corpus found in Chinese TreeBank 6.0, while Sun (2010) only reported results using gold POS tags but no additional gold parses. However, as table 4 shows, for verb predicates, our system bests Xue’s (2008) system by 4-7 F1 points with less parser training data and when tested with (but was not retrained to take full advantage of) gold POS tags besting Sun’s (2010) system by 0.53 F1 point. For nominal predicates, our system bests Xue’s (2008) system, by 1.9 F1 points on arguments of nominal predicates (since we have an integrated SRL system, the results are obtained by training both verb and nominal predicates, then using only the nominal classifi"
S15-1027,E12-1003,0,0.0201755,"s, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, Colorado, June 4–5, 2015. mance of an English PropBank SRL system by 0.4 F1 points using a variety of word similarity measures, from WordNet hierarchy distance to distributional similarity measures. Ritter and Etzioni (2010) reasoned that the set of hidden variables modeled by latent Dirichlet allocation (LDA) naturally represents the semantic"
S15-1027,J08-2004,0,0.0877786,"p 37.58 39.72 67.13 67.56 62.01 62.70 r 25.10 27.36 50.37 50.59 50.74 51.03 f1 30.10 32.40 57.55 57.86 55.81 56.27 Table 3: Chinese PropBank 3.0 out-of-genre results Bank 3.0. Only Sinorama has nominal SRL annotations. As table 3 shows, even though the absolute performance is much lower, SP improved the precision and recall in all cases, the nominal SRL score on Sinorama by 2.30 F1 points, and verb SRL score by 0.31-0.46 F1 point. Again, these F1 differences were statistically significant. 225 5.2.1 Comparison Direct performance comparison with previous Chinese SRL systems is a bit difficult: Xue (2008), Zhuang and Zong (2010) trained the syntactic parsers with an additional 250K word broadcast news corpus found in Chinese TreeBank 6.0, while Sun (2010) only reported results using gold POS tags but no additional gold parses. However, as table 4 shows, for verb predicates, our system bests Xue’s (2008) system by 4-7 F1 points with less parser training data and when tested with (but was not retrained to take full advantage of) gold POS tags besting Sun’s (2010) system by 0.53 F1 point. For nominal predicates, our system bests Xue’s (2008) system, by 1.9 F1 points on arguments of nominal predic"
S15-1027,C00-2137,0,0.403628,"Missing"
S15-1027,D10-1030,0,0.178412,"2 67.13 67.56 62.01 62.70 r 25.10 27.36 50.37 50.59 50.74 51.03 f1 30.10 32.40 57.55 57.86 55.81 56.27 Table 3: Chinese PropBank 3.0 out-of-genre results Bank 3.0. Only Sinorama has nominal SRL annotations. As table 3 shows, even though the absolute performance is much lower, SP improved the precision and recall in all cases, the nominal SRL score on Sinorama by 2.30 F1 points, and verb SRL score by 0.31-0.46 F1 point. Again, these F1 differences were statistically significant. 225 5.2.1 Comparison Direct performance comparison with previous Chinese SRL systems is a bit difficult: Xue (2008), Zhuang and Zong (2010) trained the syntactic parsers with an additional 250K word broadcast news corpus found in Chinese TreeBank 6.0, while Sun (2010) only reported results using gold POS tags but no additional gold parses. However, as table 4 shows, for verb predicates, our system bests Xue’s (2008) system by 4-7 F1 points with less parser training data and when tested with (but was not retrained to take full advantage of) gold POS tags besting Sun’s (2010) system by 0.53 F1 point. For nominal predicates, our system bests Xue’s (2008) system, by 1.9 F1 points on arguments of nominal predicates (since we have an i"
S15-1027,J13-3006,0,\N,Missing
S15-1027,W05-0620,0,\N,Missing
S15-1027,C98-2122,0,\N,Missing
S15-1027,H91-1060,0,\N,Missing
S16-2012,P06-1117,0,0.380816,"Missing"
S16-2012,P14-1097,1,0.452225,"nguistic intuition used in its construction. Its great strength is also its downfall: adding new verbs, new senses, and new classes requires trained linguists - at least, to preserve the integrity of the resource. According to Levin’s hypothesis, knowing the set of allowable syntactic patterns for a verb sense is sufficient to make meaningful semantic classifications. Large-scale corpora provide an extremely comprehensive picture of the possible syntactic realizations for any particular verb. With enough data in the training set, even infrequent verbs have sufficient data to support learning. Kawahara et al. (2014) showed that, using a Dirichlet Process Mixture Model (DPMM), a VerbNet-like clustering of verb senses can be built from counts of syntactic features. We develop a model to extend VerbNet, using a large corpus with machine-annotated dependencies. We build on prior work by adding partial supervision from VerbNet, treating VerbNet classes as additional latent variables. The resulting clusters are more similar to the evaluation set, and each cluster in the DPMM predicts its VerbNet class distribution naturally. Because the technique is data-driven, it is easily adaptable to domainspecific corpora"
S16-2012,P03-1009,0,0.14761,"Missing"
S16-2012,P09-1033,0,0.0585909,"Missing"
S16-2012,D09-1026,0,0.0381471,"the verb sense induction and the clustering of verb senses, the features can be optimized for the distinct tasks. According to (Kawahara et al., 2014), the best features for inducing verb classes are joint slot:token pairs. For the verb clustering task, slot features which ignore the lexical items were the most effective. This aligns with Levin’s hypothesis of diathesis alternations - the syntactic contexts are sufficient for the clustering. In this paper, we re-create the second stage clustering with the same features, but add supervision. Supervised Topic Modeling (Mimno and McCallum, 2008; Ramage et al., 2009) builds on the Bayesian framework by adding, for each item, a Dirichlet Process Mixture Models The DPMM used in Kawahara et al. (2014) is shown in Figure 1. The clusters are drawn from a Dirichlet Process with hyperparameter α and base distribution G. The Dirichlet process prior creates a clustering effect described by the Chinese Restaurant Process. Each cluster is chosen proportionally to the number of elements it already 103 prediction about a variable of interest, which is observed at least some of the time. This encourages the topics to be useful at predicting a supervised signal, as well"
S16-2012,N07-1069,1,0.770948,"Missing"
S16-2012,N06-5006,0,\N,Missing
S17-2093,S15-2136,1,0.731438,"Missing"
S17-2093,N13-3004,0,0.0211527,"Missing"
S17-2093,S17-2177,0,0.026199,"tor machines and structured perceptrons with features including words and part-of-speech tags. For domain adaptation, KULeuven-LIIR tried assigning higher weight to the brain cancer training data, and representing unknown words in the input vocabulary. LIMSI-COT (Tourille et al., 2017) combined recurrent neural networks with character and word embeddings, and support vector machines with features including words and part-of-speech tags. For domain adaptation, LIMSI-COT tried disallowing modification of pre-trained word embeddings, and representing unknown words in the input vocabulary. NTU-1 (Huang et al., 2017) combined support vector machines and conditional random fields with features including word n-grams, part-of-speech tags, word shapes, named entities, dependency trees, and UMLS concept types. ULISBOA (Lamurias et al., 2017) combined conditional random fields and rules with features including character n-grams, words, part-ofspeech tags, and UMLS concept types. XJNLP (Long et al., 2017) combined rules, support vector machines, and recurrent and convolutional neural networks, with features including words, word embeddings, and verb tense. Several other teams (WuHanNLP, UNICA, UTD, and IIIT) al"
S17-2093,S17-2179,0,0.0278089,"words in the input vocabulary. LIMSI-COT (Tourille et al., 2017) combined recurrent neural networks with character and word embeddings, and support vector machines with features including words and part-of-speech tags. For domain adaptation, LIMSI-COT tried disallowing modification of pre-trained word embeddings, and representing unknown words in the input vocabulary. NTU-1 (Huang et al., 2017) combined support vector machines and conditional random fields with features including word n-grams, part-of-speech tags, word shapes, named entities, dependency trees, and UMLS concept types. ULISBOA (Lamurias et al., 2017) combined conditional random fields and rules with features including character n-grams, words, part-ofspeech tags, and UMLS concept types. XJNLP (Long et al., 2017) combined rules, support vector machines, and recurrent and convolutional neural networks, with features including words, word embeddings, and verb tense. Several other teams (WuHanNLP, UNICA, UTD, and IIIT) also competed, but did not submit a system description. 8 Evaluation Results Tables 2 to 4 show the results of the evaluation. In all tables, the best system score from each column is in bold. Systems marked with † were submitt"
S17-2093,S17-2181,0,0.0428032,"Missing"
S17-2093,S17-2178,0,0.0383126,"Missing"
S17-2093,S17-2180,0,0.0292967,"on to it in the text. 7 Participating Systems 11 teams submitted a total of 28 runs, 10 for the unsupervised domain adaptation phase, and 18 for the supervised domain adaptation phase. All participating systems trained some form of supervised classifiers, with common features including character n-grams, words, part-of-speech tags, and Unified Medical Language System (UMLS) concept types. Below is a brief description of each participating team, and a note if they performed any more elaborate domain adaptation than simply adding the extra 30 brain cancer notes to their training data. 568 GUIR (MacAvaney et al., 2017) combined conditional random fields, rules, and decision tree ensembles, with features including character n-grams, words, word shapes, word clusters, word embeddings, part-of-speech tags, syntactic and dependency tree paths, semantic roles, and UMLS concept types. Team GUIR KULeuven-LIIR LIMSI-COT ULISBOA Hitachi baseline WuHanNLP Hitachi (P R et al., 2017) combined conditional random fields, rules, neural networks, and decision tree ensembles, with features including character n-grams, word n-grams, word shapes, word embeddings, verb tense, section headers, and sentence embeddings. GUIR LIMS"
S17-2093,S17-2176,0,0.0310189,"Missing"
S17-2093,W11-0419,1,0.216445,"data. All colon cancer data was released as part of Clinical TempEval 2015 and 2016. The Train-10 column is the data from the first 10 patients of the brain cancer Train data, which was the only additional training data released in Clinical TempEval 2017. tested on the annotations of the brain cancer Test set. Systems were again free to use all the raw brain cancer text if they had a way to do so. Note that across all phases, the only brain cancer data released was the Train-10 set. The remainder of the brain cancer data was reserved for future evaluations. 3 – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : Tasks Nine tasks were included (the same as those of Clinical TempEval 2015 and 2016), grouped into three categories: P = • Identifying time expressions (TIMEX3 annotations in the THYME corpus) consisting of the following components: |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is the set of items predi"
S17-2093,S17-2098,0,0.0529925,"Missing"
S17-2093,P11-2061,0,0.101468,"ns were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ON TAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| To predict with the model, the raw text of the test data was searched for all exact character matches of any of the memorized phrases, preferring longer phrases when multiple matches overlapped. Wherever a phrase match was found, an event or time with the memorized (most frequent) attribute values was predicted. Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of systempredicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Human Agreement We also provide two types of human agreement on the tasks, measured with the same evaluation"
S17-2093,S13-2001,1,0.94542,"s were trained on colon cancer patients, but tested on brain cancer patients. The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients. As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016. 2 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007, 2010; UzZaman et al., 2013). In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures. In the Clinical TempEval shared tasks (Bethard et al., 2015, 2016), participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations. For example, Figure 1 shows the annotations that a system is expected to produce when given the text: Data The C"
S17-2093,S07-1014,1,0.86365,"cer patients, in 2017, systems were trained on colon cancer patients, but tested on brain cancer patients. The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients. As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016. 2 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007, 2010; UzZaman et al., 2013). In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures. In the Clinical TempEval shared tasks (Bethard et al., 2015, 2016), participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations. For example, Figure 1 shows the annotations that a system is expected to produce wh"
S18-1011,S17-2093,1,0.912197,"Missing"
S18-1011,N13-3004,0,0.134366,"VAL of the time entity other, and so on. Finally, all the time entities must be completed with some additional properties needed for their interpretation. For example, the time entity other should have a VALUE of 2, the E ND -I NTERVAL of since is the Document Creation Time, etc. Once again, the properties required by each time entity type are defined by the SCATE schema.1 Every resulting graph, composed of a set of linked time entities, represents a time expression that can be semantically interpreted. For this purpose, we provide a Scala library2 that reads the graphs in Anafora XML format (Chen and Styler, 2013) and converts them into intervals on the timeline. An example of interpreting the time entities corresponding to the expression every Saturday since March 6 relative to an anchor time of April 21, 2017 is given in Figure 2. In this example, the values today and result store the entities that represent the time expressions April 21, 2017 and every Saturday since March 6 respectively. The Scala command on the right side interprets the latter and produces the corresponding time intervals. The task includes two evaluation methods, one for the parsing step, i.e. time entity identification Tasks The"
S18-1011,Q18-1025,1,0.912451,"nts SCATE entities SCATE time exp. SCATE bounded Newswire Clinical Train Dev Test Train Dev Test 64 14 20 232 35 141 1,628 402 398 14,936 2,896 9,530 636 146 186 4,469 879 2,815 391 80 93 2,303 430 1,471 Table 1: Number of documents and SCATE annotations for both sections of the corpus following the SCATE schema. 5 divided by the total length of the intervals in the second set. Formally:  IS IH = {i ∩ j : i ∈ IS ∧ j ∈ IH } P |i| T Pint (IS , IH ) = i∈COMPACT(IS P i∈IS Rint (IS , IH ) = P P Two systems were used as baselines to compare the participating systems against. Character-based model (Laparra et al., 2018) is a novel supervised approach for time normalization that follows the SCATE schema. This model decomposes the normalization of time expressions into two modules: IH ) |i| i∈COMPACT(IS i∈∪IH T IH ) Baseline systems |i| time entity identification detects the spans of characters that belong to each time expression and labels them with their corresponding time entity type. This step is performed by character-based recurrent neural network with two stacked bidirectional Gated Recurrent Units. |i| where IS and IH are sets of intervals, i ∩ j is the possibly empty interval in common between the int"
S18-1011,D13-1078,1,0.843655,"ng Time Normalizations Egoitz Laparra University of Arizona Tucson, AZ 85721, USA Dongfang Xu University of Arizona Tucson, AZ 85721, USA Steven Bethard University of Arizona Tucson, AZ 85721, USA laparra@email.arizona.edu dongfangxu9@email.arizona.edu bethard@email.arizona.edu Ahmed S. Elsayed University of Colorado Boulder Boulder, CO 80309 Martha Palmer University of Colorado Boulder Boulder, CO 80309 ahmed.s.elsayed@colorado.edu martha.palmer@colorado.edu Abstract normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules (Bethard, 2013; Lee et al., 2014; Str¨otgen and Gertz, 2015). The Parsing Time Normalizations shared task is a new approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task. The top of Figure 1 shows an example. Each annotation in the example corresponds to a formally defined time entity"
S18-1011,P14-1135,0,0.0223854,"zations Egoitz Laparra University of Arizona Tucson, AZ 85721, USA Dongfang Xu University of Arizona Tucson, AZ 85721, USA Steven Bethard University of Arizona Tucson, AZ 85721, USA laparra@email.arizona.edu dongfangxu9@email.arizona.edu bethard@email.arizona.edu Ahmed S. Elsayed University of Colorado Boulder Boulder, CO 80309 Martha Palmer University of Colorado Boulder Boulder, CO 80309 ahmed.s.elsayed@colorado.edu martha.palmer@colorado.edu Abstract normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules (Bethard, 2013; Lee et al., 2014; Str¨otgen and Gertz, 2015). The Parsing Time Normalizations shared task is a new approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task. The top of Figure 1 shows an example. Each annotation in the example corresponds to a formally defined time entity. For instance, th"
S18-1011,S15-2136,1,0.901333,"Missing"
S18-1011,S18-1012,0,0.0182708,"d to be executed in a specific order. However, some of them were swapped and not analyzed in the expected order. Second, the system was supposed to assume there is only one year, one month, and one day mentioned per temporal phrase. This worked for the month and day, however, it was failing with 4-digit years. Finally, for most parsing methods the system loops through each token in the temporal phrase but it skipped the loop when identifying full numeric expressions, like ”1953” or ”08091998”. Thus, phrases like ”Last 1953” were not being counted as having any numeric values in them. C HRONO (Olex et al., 2018) is a primarily rulebased system that performs time normalization by running the following three steps: 1) Temporal tokens are identified and flagged using regex expressions to identify formatted dates/times, and by parsing out specific temporal words and numeric tokens. 2) Temporal phrases are identified by searching for consecutive numeric/temporal tokens according to certain constraints. 3) Temporal phrases are parsed and normalized into the SCATE schema via detailed rulebased parsing, including the utilization of partof-speech tags, to identify each component of an expression and link sub-"
S18-1011,L16-1599,1,0.936786,"u9@email.arizona.edu bethard@email.arizona.edu Ahmed S. Elsayed University of Colorado Boulder Boulder, CO 80309 Martha Palmer University of Colorado Boulder Boulder, CO 80309 ahmed.s.elsayed@colorado.edu martha.palmer@colorado.edu Abstract normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules (Bethard, 2013; Lee et al., 2014; Str¨otgen and Gertz, 2015). The Parsing Time Normalizations shared task is a new approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task. The top of Figure 1 shows an example. Each annotation in the example corresponds to a formally defined time entity. For instance, the annotation on top of since corresponds to a B ETWEEN entity that identifies an interval starting at the most recent March 6 and ending at the document creation time. The bottom of Figure 1 shows how those time entities can b"
S18-1011,D15-1063,0,0.0267168,"Missing"
S18-1011,S16-1165,1,0.926046,"Missing"
S18-1011,S13-2001,0,0.346177,"n a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals). 1 Introduction The task of extracting and normalizing time expressions (e.g., finding phrases like two days ago and converting them to a standardized form like 2017-07-17) is a fundamental component of any time-aware language processing system. TempEval 2010 and 2013 (Verhagen et al., 2010; UzZaman et al., 2013) included a restricted version of a time normalization task as part of their shared tasks. However, the annotation scheme used in these tasks (TimeML; (ISO, 2012)) has some significant limitations: it assumes times can be described as a prefix of YYYY-MM-DDTHH:MM:SS (so it can’t represent, e.g., the past three summers), it is unable to represent times that are are relative to events (e.g., three weeks postoperative), and it fails to reflect the compositional nature of time expressions (e.g., that following represents a similar temporal operation in the following day and the following year). Th"
vaidya-etal-2012-empty,bhatia-etal-2010-empty,1,\N,Missing
vaidya-etal-2012-empty,W11-0403,1,\N,Missing
vaidya-etal-2012-empty,W10-1836,1,\N,Missing
vaidya-etal-2012-empty,W09-3036,1,\N,Missing
vaidya-etal-2012-empty,J08-2004,0,\N,Missing
vaidya-etal-2012-empty,J05-1004,1,\N,Missing
vaidya-etal-2012-empty,I08-2099,0,\N,Missing
vaidya-etal-2012-empty,choi-etal-2010-propbank-instance,1,\N,Missing
W00-0202,P98-1046,1,0.842289,"raditional statespace properties of actions, such as applicability conditions and preparatory actions that have to be satis ed before the action can be executed, and termination conditions and post assertions which determine when an action is concluded and what changes it makes to the environment state. We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associated with each other under a common parent that captures the properties these verbs all share (Dang et al., 1998). The highest nodes in the hierarchy are occupied by generalized PAR schemas which represent the basic predicate-argument structures for entire groups of subordinate actions. The lower nodes are occupied by progressively more speci c schemas that inherit information from the generalized PARs, and can be instantiated with arguments from natural language to represent a speci c action such as John hit the ball with his bat. The example in Figure 1 is a generalized PAR schema for contact ac1 Introduction In this paper, we describe a Parameterized Action Representation (PAR) (Badler et al., 1999) t"
W00-0202,P99-1012,0,0.0118275,"th a resounding crack would all map to the same schema.3 3  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 3 Generating Animations The main components of our animation system are: a natural language interface, a planner and a graphical animation (see Figure 3). The PARs are used as intermediate representations of the actions between components. An instruction in natural language starts the process. We use a Synchronous Tree Adjoining Grammar (Shieber and Schabes, 1990; Shieber, 1994) for parsing natural language instructions into derivations containing predicate-argument dependencies (Schuler, 1999). The synchronous parser extracts these predicate-argument structures by rst associating each word in an input sentence with one or more elementary trees, which are combined into a single derivation tree for the entire input sentence using the constrained operations of substitution and adjunction in the Tree Adjoining Grammar formalism (Joshi, 1985; Joshi, 1987). As the parser assembles these elementary tree predicates into a predicate-argument structure, it simultaneously selects and assembles the corresponding schemas. It lls in the participants and modi ers, and outputs the PAR schema for t"
W00-0202,C90-3045,0,0.109139,"all map to the same PAR schema. For example, John hit the ball, John hit the ball with a bat and John swung mightily and his bat hit the ball with a resounding crack would all map to the same schema.3 3  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 3 Generating Animations The main components of our animation system are: a natural language interface, a planner and a graphical animation (see Figure 3). The PARs are used as intermediate representations of the actions between components. An instruction in natural language starts the process. We use a Synchronous Tree Adjoining Grammar (Shieber and Schabes, 1990; Shieber, 1994) for parsing natural language instructions into derivations containing predicate-argument dependencies (Schuler, 1999). The synchronous parser extracts these predicate-argument structures by rst associating each word in an input sentence with one or more elementary trees, which are combined into a single derivation tree for the entire input sentence using the constrained operations of substitution and adjunction in the Tree Adjoining Grammar formalism (Joshi, 1985; Joshi, 1987). As the parser assembles these elementary tree predicates into a predicate-argument structure, it sim"
W00-0202,C98-1046,1,\N,Missing
W00-0202,W90-0102,0,\N,Missing
W00-0505,M92-1038,0,0.0242643,"ticipant slots plus optional date and location slots.2 We then gathered a small corpus of thirty articles by searching for articles containing ""North Korea"" and one or more of about 15 keywords. The first two sentences (with a few exceptions) were then annotated with the slots to be extracted, leading to a total of 51 sentences containing 47 scenario templates and 89 total 4.2 2 In the end, we did not use the &apos;issue&apos; slot shown in Figure 1, as it contained more complex Idlers than those that typically have been handled in IE systems. For our feasibility study, we chose to follow the AutoSlog (Lehnert et al., 1992; Riloff, 1993) approach to extraction pattern acquisition. In this approach, extraction patterns are acquired 34 Extraction Pattern Learning i. E: &lt;target-np>=&lt;subject> &lt;active voice verb> &lt;participant> MET K: &lt;target-np>=&lt;subject> &lt;active voice verb> &lt;John-i> MANNASSTA &lt;John-nom>&apos;MET 2. E: &lt;target-np>=&lt;subject> &lt;verb> &lt;infinitive> &lt;participant> agreed to MEET K: &lt;target-np>=&lt;subject> &lt;verbl-ki-lo> &lt;verb2> &lt;John-un> MANNA-ki-lo hapuyhayssta &lt;John-nom> MEET-ki-lo agreed (-ki: nominalization ending, -io: an adverbial postposition) Figure 3 via a one-shot general-to-specific learning algorithm d"
W00-0505,1981.tc-1.3,0,0.0738175,"Missing"
W00-0505,W98-1428,1,0.744077,"an Transfer Lexicon is used to map the English keywords to corresponding Korean ones. When the match falls below a user• configurable threshold, the extracted information is filtered out. • The MT component. The MT component (cf. Lavoie et al., 2000) translates the extracted Korean phrases or sentences into corresponding English ones. • The Presentation Generator component. This component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information. It uses existing NLG components, including the Exemplars text planning framework (White and Caldwell, 1998) and the RealPro syntactic realizer (Lavoie and Rainbow, 1997). In our feasibility study, the majority of the effort went towards developing the PIE component, described in the next section. This component was implemented in a general way, i.e. in a way that we would expect to work beyond the specific training/test corpus described below. In contrast, we only implemented initial versions of the User Interface, Ranker and Presentation Generator components, in order to demonstrate the system concept; that is, these initial versions were only intended.to work with our training/test corpus, and wi"
W00-0505,1997.iwpt-1.25,1,0.761584,"Missing"
W00-0505,palmer-etal-1998-rapid,1,\N,Missing
W00-0505,M91-1033,1,\N,Missing
W00-0505,A00-1009,1,\N,Missing
W00-0505,1997.mtsummit-workshop.12,1,\N,Missing
W00-0505,A97-1039,1,\N,Missing
W00-1208,J94-4004,0,0.0516326,"Missing"
W00-1208,W00-1307,1,0.931318,"Chinese in the sense that argument NPs are freely permutable (subject to certain discourse constraints). Third, Korean and Chinese freely allow subject and object deletion, but English does not. Fourth, Korean has richer inflectional morphology t h a n English, whereas Chinese has little, if any, inflectional morphology. 2.2 (NP (NP (iNNfountain)(NNS pens)) (CO and) (NP (VBGblotting)(NN papers)))))))) Figure 1: An example from Penn English Treebank 3.1 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II (Marcus et al., 1993), the Chinese P e n n Treebank (Xia et al., 2000b), and the Korean Penn Treebank (Chung-hye Han, 2000). The main parameters of these Treebanks are summarized in Table 1.1 The tags in each tagset can be classified into one of four types: (1) syntactic tags for phrase-level annotation, (2) PartOf-Speech (POS) tags for head-level annotation, (3) function tags for grammatical function annotation, and (4) empty category tags for dropped arguments, traces, and so on. We chose these Treebanks because they all use phrase structure annotation and their annotation schemata are similar, which facilitates the comparison between the extracted Treebank g"
W00-1208,xia-etal-2000-developing,1,0.818684,"Chinese in the sense that argument NPs are freely permutable (subject to certain discourse constraints). Third, Korean and Chinese freely allow subject and object deletion, but English does not. Fourth, Korean has richer inflectional morphology t h a n English, whereas Chinese has little, if any, inflectional morphology. 2.2 (NP (NP (iNNfountain)(NNS pens)) (CO and) (NP (VBGblotting)(NN papers)))))))) Figure 1: An example from Penn English Treebank 3.1 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II (Marcus et al., 1993), the Chinese P e n n Treebank (Xia et al., 2000b), and the Korean Penn Treebank (Chung-hye Han, 2000). The main parameters of these Treebanks are summarized in Table 1.1 The tags in each tagset can be classified into one of four types: (1) syntactic tags for phrase-level annotation, (2) PartOf-Speech (POS) tags for head-level annotation, (3) function tags for grammatical function annotation, and (4) empty category tags for dropped arguments, traces, and so on. We chose these Treebanks because they all use phrase structure annotation and their annotation schemata are similar, which facilitates the comparison between the extracted Treebank g"
W00-1208,palmer-etal-1998-rapid,1,\N,Missing
W00-1208,J93-2004,0,\N,Missing
W00-1307,2000.iwpt-1.9,0,0.179946,"Missing"
W00-1307,E99-1025,0,0.033721,"Missing"
W00-1307,P97-1003,0,0.241549,"ls. Figure 7 shows the fully bracketed ttree. The nodes inserted by LexTract are in bold face. Treebank-specific information The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997). An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table specifies what function tags always mark arguments (adjuncts, heads, respectively). LexTract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5, the head of the root S is the verb draft, and the verb has two siblings: the noun phrase"
W00-1307,C94-2149,0,0.0207537,"types of annotation errors. We have used LexTract for the final cleanup of the Penn Chinese Treebank. Due to space limitation, in this paper we will only discuss the first two tasks. 4.1 Evaluating the coverage of hand-crafted grammars The XTAG grammar (XTAG-Group, 1998) is a hand-crafted large-scale grammar for English, which has been developed at University of Pennsylvania in the last decade. It has been used in many NLP applications such as generation (Stone and Doran, 1997). Evaluating the coverage of such a grammar is important for both its developers and its users. Previous evaluations (Doran et al., 1994; Srinivas et al., 1998) of the XTAG grammar use raw data (i.e., a set of sentences without syntactic bracketing). The data are first parsed by an LTAG parser and the coverage of the g r a m m a r is measured as the percentage of sentences in the data that get at least one parse, which is not necessarily the correct parse. For more discussion on this approach, see (Prasad and Sarkar, 2000). We propose a new evaluation m e t h o d that takes advantage of Treebanks and LexTract. The idea is as follows: given a Treebank T and a hand-crafted grammar Gh, the coverage of Gh on T can be measured by t"
W00-1307,C94-1024,1,0.738348,"Missing"
W00-1307,P98-1115,0,0.111927,"ects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG formalism The primitive elements of an LTAG are elementary trees (etrees). Each etree is associated with a lexical item (called the anchor of the tree) on its frontier. We choose LTAGs as our target grammars (i.e., the grammars to be extracted) be"
W00-1307,P95-1037,0,0.0945816,"o different levels. Figure 7 shows the fully bracketed ttree. The nodes inserted by LexTract are in bold face. Treebank-specific information The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997). An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table specifies what function tags always mark arguments (adjuncts, heads, respectively). LexTract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5, the head of the root S is the verb draft, and the verb has two siblings:"
W00-1307,P92-1007,0,0.0295146,"lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG formalism The primitiv"
W00-1307,C96-2103,1,0.838615,"Missing"
W00-1307,W00-2027,0,0.0272379,"It has been used in many NLP applications such as generation (Stone and Doran, 1997). Evaluating the coverage of such a grammar is important for both its developers and its users. Previous evaluations (Doran et al., 1994; Srinivas et al., 1998) of the XTAG grammar use raw data (i.e., a set of sentences without syntactic bracketing). The data are first parsed by an LTAG parser and the coverage of the g r a m m a r is measured as the percentage of sentences in the data that get at least one parse, which is not necessarily the correct parse. For more discussion on this approach, see (Prasad and Sarkar, 2000). We propose a new evaluation m e t h o d that takes advantage of Treebanks and LexTract. The idea is as follows: given a Treebank T and a hand-crafted grammar Gh, the coverage of Gh on T can be measured by the overlap of Gh and a Treebank grammar Gt that is produced by LexTract from T. In this case, we will estimate the coverage of the XTAG grammar on the English Penn Treebank (PTB) using the Treebank grammar G2. There are obvious differences between these two grammars. For example, feature structures and multi-anchor etrees are present only in the XTAG grammar, whereas frequency information"
W00-1307,P92-1022,0,0.0701148,"Missing"
W00-1307,P97-1026,0,0.120564,"entative of a class of lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG f"
W00-1307,W98-0315,1,0.566444,"e our approaches with related work. 1 Introduction There are various grammar frameworks proposed for natural languages. We take Lexicalized Tree-adjoining Grammars (LTAGs) as representative of a class of lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalize"
W00-1307,W00-2030,1,0.863403,"Missing"
W00-1307,W00-1208,1,0.84805,"et al., 1993) and got two Treebank grammars. The first one, G1, uses the Treebank&apos;s tagset. The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag. For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced tagset is basically the same as the tagset used in the XTAG g r a m m a r ( X T A G - G r o u p , 1998). G2 is built so that we can compare it with the XTAG grammar, as will be discussed in the next section. We also ran the system on the 100-thousand-word Chinese Penn Treebank (Xia et al., 2000b) and on a 30-thousand-word Korean Penn Treebank. The sizes of extracted grammars are shown in Table 1. (For more discussion on the Chinese and the Korean Treebanks and the comparison between these Treebank grammars, see (Xia et al., 2000a)). The second column of the table lists the numbers of unique templates in each grammar, where templates are etrees with the lexical items removed, s The third column shows the numbers of unique S NP VP I &lt;1 N V I t John left Ja,. (E l) (T*) [ &~m t (E) lea (E,) I I John left (E2) lc~hn [ (Es) The Experiments Icft (E6) Figure 11: Tree sets for a fully brack"
W00-1307,xia-etal-2000-developing,1,0.69136,"et al., 1993) and got two Treebank grammars. The first one, G1, uses the Treebank&apos;s tagset. The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag. For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced tagset is basically the same as the tagset used in the XTAG g r a m m a r ( X T A G - G r o u p , 1998). G2 is built so that we can compare it with the XTAG grammar, as will be discussed in the next section. We also ran the system on the 100-thousand-word Chinese Penn Treebank (Xia et al., 2000b) and on a 30-thousand-word Korean Penn Treebank. The sizes of extracted grammars are shown in Table 1. (For more discussion on the Chinese and the Korean Treebanks and the comparison between these Treebank grammars, see (Xia et al., 2000a)). The second column of the table lists the numbers of unique templates in each grammar, where templates are etrees with the lexical items removed, s The third column shows the numbers of unique S NP VP I &lt;1 N V I t John left Ja,. (E l) (T*) [ &~m t (E) lea (E,) I I John left (E2) lc~hn [ (Es) The Experiments Icft (E6) Figure 11: Tree sets for a fully brack"
W00-1307,palmer-etal-1998-rapid,1,\N,Missing
W00-1307,J93-2004,0,\N,Missing
W00-1307,C98-1111,0,\N,Missing
W00-2021,W98-0104,1,0.872805,"Missing"
W00-2021,W98-0106,0,0.0508368,"Missing"
W00-2021,P98-1046,1,0.88436,"Missing"
W00-2021,J88-2003,0,0.280656,"Missing"
W00-2021,C98-1046,1,\N,Missing
W00-2028,C94-2149,0,0.0453454,"Missing"
W00-2028,P85-1008,0,0.170189,"Missing"
W00-2028,J93-4004,0,0.085703,"Missing"
W00-2028,P97-1026,1,0.928694,"Missing"
W00-2028,W98-1419,1,0.868778,"Missing"
W00-2028,P99-1006,1,0.880658,"Missing"
W00-2028,W98-0143,1,0.876531,"Missing"
W00-2028,W98-0106,0,\N,Missing
W00-2041,C96-1034,0,0.0267563,"Missing"
W00-2041,2000.iwpt-1.9,0,0.02631,"Missing"
W00-2041,H94-1020,0,0.0339469,"Missing"
W00-2041,C96-2103,0,0.0560381,"Missing"
W00-2041,W98-0143,1,0.877555,"Missing"
W02-0813,J96-1002,0,0.00625426,"er bounds on the accuracy of feature sets that do not impose substantial pre-processing requirements. In contrast, we wish to demonstrate that such pre-processing significantly improves accuracy for sense-tagging English verbs, because we believe that they allow us to extract a set of features that more closely parallels the information humans use for sense disambiguation. 3 System Description We developed an automatic WSD system that uses a maximum entropy framework to combine linguistic contextual features from corpus instances of each verb to be tagged. Under the maximum entropy framework (Berger et al., 1996), evidence from different features can be combined with no assumptions of feature independence. The automatic tagger estimates the conditional probability that a word has sense x given that it occurs in context y, where y is a conjunction of features. The estimated probability is derived from feature weights which are determined automatically from training data so as to produce a probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence. In order to extract the linguistic features necessary for the model, all sentences were first autom"
W02-0813,A97-1029,0,0.01118,"en that it occurs in context y, where y is a conjunction of features. The estimated probability is derived from feature weights which are determined automatically from training data so as to produce a probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence. In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes. Following work by Chodorow, Leacock and Miller, we divided the possible model features into topical and local contextual features. Topical features looked for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). The set of 200-300 keywords is specific to each lemma to be disambiguated, and is determined automatically from training data so as to minimize the entropy of the probability of the senses conditioned on the keyword."
W02-0813,P97-1009,0,0.115614,"Missing"
W02-0813,P96-1006,0,0.0707303,"using naive bayesian models for WSD, combining sets of linguistically impoverished features that were classified as either topical or local. Topical features consisted of a bag of open-class words in a wide window covering the entire context provided; local features were words and parts of speech within a small window or at particular offsets from the target word. The system was configured to use only local, only topical, or both local and topical features for each word, depending on which configuration produced the best result on a held-out portion of the training data. Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. Additionally, Pedersen (Pedersen, 2001; Pedersen, 2000) has pursued the approach of using simple word bigrams and other linguistically impoverished feature sets for sense tagging, to establish upper bounds on the accuracy of feature sets that do not impose substantial pre-processing requirements. In contrast, we wish to demonstrate that such pre-processing significantly improves accuracy for sense-tagging English verbs, b"
W02-0813,S01-1005,1,0.885967,"Missing"
W02-0813,A00-2009,0,0.0220216,"e context provided; local features were words and parts of speech within a small window or at particular offsets from the target word. The system was configured to use only local, only topical, or both local and topical features for each word, depending on which configuration produced the best result on a held-out portion of the training data. Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. Additionally, Pedersen (Pedersen, 2001; Pedersen, 2000) has pursued the approach of using simple word bigrams and other linguistically impoverished feature sets for sense tagging, to establish upper bounds on the accuracy of feature sets that do not impose substantial pre-processing requirements. In contrast, we wish to demonstrate that such pre-processing significantly improves accuracy for sense-tagging English verbs, because we believe that they allow us to extract a set of features that more closely parallels the information humans use for sense disambiguation. 3 System Description We developed an automatic WSD system that uses a maximum entro"
W02-0813,N01-1011,0,0.0164913,"vering the entire context provided; local features were words and parts of speech within a small window or at particular offsets from the target word. The system was configured to use only local, only topical, or both local and topical features for each word, depending on which configuration produced the best result on a held-out portion of the training data. Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. Additionally, Pedersen (Pedersen, 2001; Pedersen, 2000) has pursued the approach of using simple word bigrams and other linguistically impoverished feature sets for sense tagging, to establish upper bounds on the accuracy of feature sets that do not impose substantial pre-processing requirements. In contrast, we wish to demonstrate that such pre-processing significantly improves accuracy for sense-tagging English verbs, because we believe that they allow us to extract a set of features that more closely parallels the information humans use for sense disambiguation. 3 System Description We developed an automatic WSD system that use"
W02-0813,P97-1003,0,0.0344822,"timates the conditional probability that a word has sense x given that it occurs in context y, where y is a conjunction of features. The estimated probability is derived from feature weights which are determined automatically from training data so as to produce a probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence. In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). In addition, an automatic named entity tagger (Bikel et al., 1997) was run on the sentences to map proper nouns to a small set of semantic classes. Following work by Chodorow, Leacock and Miller, we divided the possible model features into topical and local contextual features. Topical features looked for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). The set of 200-300 keywords is specific to each lemma to be disambiguated, and is determined automatically from training data so as to minimize the e"
W02-0813,C02-1143,1,0.819929,"reated sense groups, on the other hand, can capture broader, more underspecified senses which are not explicitly listed and which do not participate in any of the WordNet semantic relations. 5 Conclusion We have demonstrated that our approach to disambiguating verb senses using maximum entropy models to combine as many linguistic knowledge sources as possible, yields state-of-the-art performance for English. This may be a language-dependent feature, as other experiments indicate that additional linguistic pre-processing does not necessarily improve tagging accuracy for languages like Chinese (Dang et al., 2002). In examining the instances that proved troublesome to both the human taggers and the automatic system, we found errors that were tied to subtle sense distinctions which were reconciled by backing off to the more coarse-grained sense groups. Achieving higher inter-annotator agreement is necessary in order to provide consistent training data for supervised WSD systems. Lexicographers have long recognized that many natural occurrences of polysemous words are embedded in underspecified contexts and could correspond to more than one specific sense. Annotators need the option of selecting, as an a"
W03-1707,W00-1201,0,0.0258615,"al to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predicate-argument structure of lexical items are not captured. Recent effort in semantic annotation, the creation of the Penn Proposition Bank (Kingsbury and Palmer, 2002) on top of the Penn English Treebank is beginning to address this issue for English. In this new layer of annotation, the regularities of the predicates, mo"
W03-1707,A00-2018,0,0.00397732,"ions for this resource. 1 Introduction Linguistically interpreted corpora are instrumental in supervised machine learning paradigms of natural language processing. The information encoded in the corpora to a large extent determines what can be learned by supervised machine learning systems. Therefore, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predic"
W03-1707,P00-1058,0,0.012057,"e, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predicate-argument structure of lexical items are not captured. Recent effort in semantic annotation, the creation of the Penn Proposition Bank (Kingsbury and Palmer, 2002) on top of the Penn English Treebank is beginning to address this issue for English. In this new layer of annotation, the regularit"
W03-1707,P97-1003,0,0.0103606,", we discuss possible applications for this resource. 1 Introduction Linguistically interpreted corpora are instrumental in supervised machine learning paradigms of natural language processing. The information encoded in the corpora to a large extent determines what can be learned by supervised machine learning systems. Therefore, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, signif"
W03-1707,kingsbury-palmer-2002-treebank,1,0.867257,"USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the information encoded there is ”shallow”. Important information useful for natural language applications is missing. Most notably, significant regularities in the predicate-argument structure of lexical items are not captured. Recent effort in semantic annotation, the creation of the Penn Proposition Bank (Kingsbury and Palmer, 2002) on top of the Penn English Treebank is beginning to address this issue for English. In this new layer of annotation, the regularities of the predicates, mostly verbs, are captured in the predicate-argument structure. For example, in the sentences “The Congress passed the bill” and “The bill passed”, it is intuitively clear that “the bill” plays the same role in the two occurrences of the verb “pass”. Similar regularities also exist in Chinese. For example, in “ /this /CL /bill /pass /AS” and “ /Congress /pass /AS /this /CL /bill”, “ /bill” also plays the same role for the verb “ /pass” even t"
W03-1707,J93-2004,0,0.0317252,"e then discuss how a lexical database with predicate-argument structure information can be used to ensure consistent annotation. Finally, we discuss possible applications for this resource. 1 Introduction Linguistically interpreted corpora are instrumental in supervised machine learning paradigms of natural language processing. The information encoded in the corpora to a large extent determines what can be learned by supervised machine learning systems. Therefore, it is crucial to encode the desired level of information for its automatic acquisition. The creation of the Penn English Treebank (Marcus et al., 1993), a syntactically interpreted corpus, played a crucial role in the advances in natural language parsing technology (Collins, 1997; Collins, 2000; Charniak, 2000) for English. The creation of the Penn Martha Palmer Dept. of Computer and Info. Science University of Pennsylvania Philadelphia, PA 19104, USA mpalmer@linc.cis.upenn.edu Chinese Treebank (Xia et al., 2000) is also beginning to help advance technologies in Chinese syntactic analysis (Chiang, 2000; Bikel and Chiang, 2000). Since the treebanks are generally syntactically oriented (cf. Sinica Treebank (Chen et al., to appear)), the inform"
W03-1707,J98-1001,0,\N,Missing
W04-0811,S01-1005,1,\N,Missing
W04-1513,J00-1004,0,0.0655737,"dvantage of leveraging large parallel corpora, the statistical MT approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available (Och, 2003). However, a major criticism of this approach is that it is void of any internal representation for syntax or semantics. In recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge. Syntax based statistical MT approaches began with (Wu 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language. The syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. While we would like to use syntactic information in both"
W04-1513,J93-2003,0,0.0120901,"Missing"
W04-1513,J94-4004,0,0.138014,"ho introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language. The syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. While we would like to use syntactic information in both languages, the problem of nonisomorphism grows when trees in both languages are required to match. To allow the syntax based machine translation approaches to work as a generative process, certain isomorphism assumptions have to be made. Hence a reasonable question to ask is: to what extent should the grammar formalism, which we choose to represent syntactic language transfer, assume isomorphism between the structures of the two languages? (Hajic et al., 2002) allows for limited no"
W04-1513,W02-1039,0,0.0941703,"Missing"
W04-1513,C92-2066,0,0.110363,"on subtrees, (Gildea, 2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model using one input tree than with a tree-to-tree model using two. At the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for MT, such as Synchronous Context Free Grammars (S-CFG) (Wu, 1997) or Synchronous Tree Adjoining Grammars (S-TAG) (Shieber and Schabes, 1990). Mathematically, generative synchronous grammars share many good properties similar to their monolingual counterparts such as CFG or TAG (Joshi and Schabes, 1992). If such a synchronous grammar could be learnt from parallel corpora, the MT task would become a mathematically clean generative process. However, the problem of inducing a synchronous grammar from empirical data was never solved. For example, Synchronous TAGs, proposed by (Shieber and Schabes, 1990), which were introduced primarily for semantics but were later also proposed for translation. From a formal perspective, Syn-TAGs characterize the correspondences between languages by a set of synchronous elementary tree pairs. While examples show that this formalism does capture certain cross lan"
W04-1513,P03-1021,0,0.00533856,"ing transduction rule set or a transfer lexicon, which largely limits the performance and application domain of the resultant machine translation system. In the early 1990s, (Brown et. al. 1993) introduced the idea of statistical machine translation, where the word to word translation probabilities and sentence reordering probabilities are estimated from a large set of parallel sentence pairs. By having the advantage of leveraging large parallel corpora, the statistical MT approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available (Och, 2003). However, a major criticism of this approach is that it is void of any internal representation for syntax or semantics. In recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge. Syntax based statistical MT approaches began with (Wu 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. (Yamada and Knight, 2001, 2002) model translation as"
W04-1513,C90-3045,0,0.161971,"ws for limited nonisomorphism in that n-to-m matching of nodes in the two trees is permitted. However, even after extending this model by allowing cloning operations on subtrees, (Gildea, 2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model using one input tree than with a tree-to-tree model using two. At the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for MT, such as Synchronous Context Free Grammars (S-CFG) (Wu, 1997) or Synchronous Tree Adjoining Grammars (S-TAG) (Shieber and Schabes, 1990). Mathematically, generative synchronous grammars share many good properties similar to their monolingual counterparts such as CFG or TAG (Joshi and Schabes, 1992). If such a synchronous grammar could be learnt from parallel corpora, the MT task would become a mathematically clean generative process. However, the problem of inducing a synchronous grammar from empirical data was never solved. For example, Synchronous TAGs, proposed by (Shieber and Schabes, 1990), which were introduced primarily for semantics but were later also proposed for translation. From a formal perspective, Syn-TAGs chara"
W04-1513,J97-3002,0,0.798058,"s and sentence reordering probabilities are estimated from a large set of parallel sentence pairs. By having the advantage of leveraging large parallel corpora, the statistical MT approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available (Och, 2003). However, a major criticism of this approach is that it is void of any internal representation for syntax or semantics. In recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge. Syntax based statistical MT approaches began with (Wu 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language. The syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between language"
W04-1513,P01-1067,0,0.165945,"adequate parallel corpora is available (Och, 2003). However, a major criticism of this approach is that it is void of any internal representation for syntax or semantics. In recent years, hybrid approaches, which aim at applying statistical learning to structured data, began to emerge. Syntax based statistical MT approaches began with (Wu 1997), who introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. (Alshawi et al., 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. (Yamada and Knight, 2001, 2002) model translation as a sequence of operations transforming a syntactic tree in one language into the string of the second language. The syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. While we would like to use syntactic information in both languages, the problem of nonisomorphism grows when trees in both languages are required to match. To allow the syntax based machine translation app"
W04-1513,J03-4003,0,\N,Missing
W04-1513,W90-0102,0,\N,Missing
W04-1513,P02-1039,0,\N,Missing
W04-1513,P03-1011,0,\N,Missing
W04-2424,P02-1031,1,0.877886,"Missing"
W04-2424,P98-1046,1,0.880397,"Missing"
W04-2424,W02-0813,1,0.880977,"Missing"
W04-2424,C98-1046,1,\N,Missing
W04-2424,J05-1004,1,\N,Missing
W04-2604,P98-1013,0,0.107471,"Missing"
W04-2604,C00-2148,1,0.868478,"Missing"
W04-2604,J87-1007,0,0.299558,"me consists of the thematic roles, the verb, and other lexical items which may be required for a particular construction or alternation. Additional restrictions may be further imposed on the thematic roles (quotation, plural, in nitival, etc.). Illustrations of syntactic frames are shown in examples 1, 2, and 3. (1) Agent V Patient (John hit the ball) (2) Agent V at Patient (John hit at the window) (3) Agent V Patient[+plural] together (John hit the sticks together) VerbNet also includes a hierarchy of prepositions, with 57 entries, derived from an extended version of work described in Sparck-Jones and Boguraev (1987). This restriction is necessary in order to specify which prepositions are possible in a particular frame since many of Levin's alternations require speci c prepositions such as `as' or `with/against'. A partial and somewhat simpli ed hierarchy is shown in Figure 1. This gure shows the spatial prepositions hierarchy divided into path and locative prepositions. Path prepositions are further subdivided into source, direction, and destination prepositions. A syntactic frame with Prep[+src] as a constraint will allow only those speci c prepositions (from, out, out of, etc) that are part of the spa"
W04-2604,kingsbury-palmer-2002-treebank,1,0.863535,"Missing"
W04-2604,kipper-etal-2004-extending,1,0.676017,"Missing"
W04-2604,J88-2003,0,\N,Missing
W04-2604,H94-1020,0,\N,Missing
W04-2604,C98-1013,0,\N,Missing
W04-2704,S01-1001,0,0.0687376,"Missing"
W04-2704,W98-0315,1,0.884365,"Missing"
W04-2704,kingsbury-palmer-2002-treebank,1,0.773736,"ng or annotation. The Proposition Bank is designed as a broad-coverage resource to facilitate the development of more general systems. It focuses on the argument structure of verbs, and provides a complete corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming a component of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. 1 PropBank (Kingsbury & Palmer, 2002) is an annotation of the Wall Street Journal portion of the Penn Treebank II (Marcus, 1994) with `predicate-argument&apos; structures, using sense tags for highly polysemous words and semantic role labels for each argument. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG1 the window] and [ARG1 The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in th"
W04-2704,miltsakaki-etal-2004-penn,1,0.886331,"Missing"
W04-2704,W04-2807,1,\N,Missing
W04-2704,J00-4005,0,\N,Missing
W04-2704,W01-1605,0,\N,Missing
W04-2704,W03-1707,1,\N,Missing
W04-2704,kipper-etal-2004-extending,1,\N,Missing
W04-2807,W00-0103,0,0.166572,"Missing"
W04-2807,W02-0813,1,0.891569,"Missing"
W04-2807,S01-1001,0,0.0253012,"Missing"
W04-2807,kingsbury-palmer-2002-treebank,1,0.632876,"Missing"
W04-2807,P03-1058,0,0.0519956,"Missing"
W04-2807,J91-4003,0,0.0720167,"Missing"
W04-3111,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3111,P03-1002,0,\N,Missing
W04-3111,N03-1028,0,\N,Missing
W04-3111,P96-1008,0,\N,Missing
W04-3111,tateisi-tsujii-2004-part,0,\N,Missing
W04-3212,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3212,J93-2004,0,\N,Missing
W04-3212,W03-1008,0,\N,Missing
W04-3212,W03-1707,1,\N,Missing
W04-3212,W03-1006,0,\N,Missing
W04-3212,J03-4003,0,\N,Missing
W04-3212,H94-1020,0,\N,Missing
W04-3212,P98-1013,0,\N,Missing
W04-3212,C98-1013,0,\N,Missing
W04-3212,J02-3001,0,\N,Missing
W04-3212,P02-1031,1,\N,Missing
W04-3212,P01-1017,0,\N,Missing
W04-3212,J05-1004,1,\N,Missing
W04-3212,N04-1030,0,\N,Missing
W04-3212,W04-2412,0,\N,Missing
W04-3212,N04-1032,0,\N,Missing
W05-0302,P98-1013,0,0.0768075,"Missing"
W05-0302,W99-0302,0,0.0733028,"Missing"
W05-0302,W01-1514,0,0.0273335,"k, NomBank and Coreference, the core predicate argument structures and referents for the arguments. One possible representation format would be to convert each annotation into features and values to be added to a larger feature structure. 1 The resulting feature structure would combine stand alone and offset annotation – it would include actual words and features from the text as well as special features that point to the actual text (character offsets) and, perhaps, syntactic trees (offsets along the lines of PropBank/NomBank). Alternative global annotation schemes include annotation graphs (Cieri & Bird, 2001), and MATE (Carletta, et. al., 1999). There are many areas in which the boundaries between these annotations have not been clearly defined, such as the treatment of support constructions and light verbs, as discussed below. Determining the most suitable format for the merged representation should be a top priority. NomBank would add arguments for report, trial, launch and beginning as follows: According to [Rel_report.01 reports], [Arg1 [ArgM-LOC sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a patrol boat] developed by Kazakhstan] are being conducted and the [ArgM-MNR formal] [Rel_lau"
W05-0302,hajicova-kucerova-2002-argument,0,0.0418506,"Missing"
W05-0302,W04-2705,1,0.895192,"Missing"
W05-0302,W04-0413,1,0.854761,"Missing"
W05-0302,miltsakaki-etal-2004-penn,0,0.0711906,"Missing"
W05-0302,J05-1004,1,0.327965,"fforts. This level could provide the foundation for a major advance in our ability to automatically extract salient relationships from text. This will in turn facilitate breakthroughs in message understanding, machine translation, fact retrieval, and information retrieval. Many recent annotation efforts for English have focused on pieces of the larger problem of semantic annotation, rather than producing a single unified representation like Head-driven Phrase Structure Grammar (Pollard and Sag 1994) or the Prague Dependency Tectogramatical Representation (Hajicova & Kucerova, 2002). PropBank (Palmer et al, 2005) annotates predicate argument structure anchored by verbs. NomBank (Meyers, et. al., 2004a) annotates predicate argument structure anchored by nouns. TimeBank (Pustejovsky et al, 2003) 2. The Component Annotation Schemata We describe below existing independent annotation efforts, each one of which is focused on a specific aspect of the semantic representation task: semantic role labeling, 5 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5–12, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics tuned to a hiring scenario (MUC-6, 1995),"
W05-0302,W99-0309,0,0.0605805,"esolved in order to bring these different layers together seamlessly. Most of these approaches have annotated the same type of data, Wall Street Journal text, so it is also important to demonstrate that the annotation can be extended to other genres such as spoken language. The demonstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community conse"
W05-0302,J98-2001,1,0.810969,"monstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community consensus on coreference. PropBank: The Penn Proposition Bank focuses on the argument structure of verbs, and provides a corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. An important goal is to provide consistent semantic role label"
W05-0302,W04-2709,0,\N,Missing
W05-0302,W04-2704,1,\N,Missing
W05-0302,J93-2004,0,\N,Missing
W05-0302,W04-0210,1,\N,Missing
W05-0302,C98-1013,0,\N,Missing
W05-0302,poesio-kabadjov-2004-general,1,\N,Missing
W05-0302,W04-2327,1,\N,Missing
W05-0309,P98-1013,0,0.0755344,"Frameset. For instance, leave has both a DEPART Frameset ([ARG0 John] left [ARG1 the room]) and a GIVE Frameset, ([ARG0 I] left [ARG1 my pearls] [ARG2 to my daughter-inlaw] [ARGM-LOC in my will].) While most Framesets have three or four numbered roles, as many as six can appear, in particular for certain verbs of motion. Verbs can take any of a set of general, adjunct-like arguments (ARGMs), such as LOC (location), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked. There are several other annotation projects, FrameNet (Baker et al., 1998), Salsa (Ellsworth et 62 al., 2004), and the Prague Tectogrammatics (Hajicova and Kucerova, 2002), that share similar goals. Berkeley.s FrameNet project, (Baker et al., 1998; Fillmore and Atkins, 1998; Johnson et al., 2002) is committed to producing rich semantic frames on which the annotation is based, but it is less concerned with annotating complete texts, concentrating instead on annotating a set of examples for each predicator (including verbs, nouns and adjectives), and attempting to describe the network of relations among the semantic frames. For instance, the buyer of a buy event and t"
W05-0309,P01-1017,0,0.0387506,"Missing"
W05-0309,hajicova-kucerova-2002-argument,0,0.0178125,"om]) and a GIVE Frameset, ([ARG0 I] left [ARG1 my pearls] [ARG2 to my daughter-inlaw] [ARGM-LOC in my will].) While most Framesets have three or four numbered roles, as many as six can appear, in particular for certain verbs of motion. Verbs can take any of a set of general, adjunct-like arguments (ARGMs), such as LOC (location), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked. There are several other annotation projects, FrameNet (Baker et al., 1998), Salsa (Ellsworth et 62 al., 2004), and the Prague Tectogrammatics (Hajicova and Kucerova, 2002), that share similar goals. Berkeley.s FrameNet project, (Baker et al., 1998; Fillmore and Atkins, 1998; Johnson et al., 2002) is committed to producing rich semantic frames on which the annotation is based, but it is less concerned with annotating complete texts, concentrating instead on annotating a set of examples for each predicator (including verbs, nouns and adjectives), and attempting to describe the network of relations among the semantic frames. For instance, the buyer of a buy event and the seller of a sell event would both be Arg0.s (Agents) in PropBank, while in FrameNet one is the"
W05-0309,J93-2004,0,0.0248355,"Missing"
W05-0309,H94-1020,0,0.0374539,"nguistics icates has already begun at NYU. This paper describes the results of PropBank II, a project to provide richer semantic annotation to structures that have already been propbanked, specifically, eventuality ID.s, coreference, coarse-grained sense tags, and discourse connectives. Of special interest to the machine translation community is our finding, presented in this paper, that PropBank II annotation reconciles many of the surface differences of the two languages. 2 PropBank I PropBank (Palmer et al., 2005) is an annotation of the Wall Street Journal portion of the Penn Treebank II (Marcus et al., 1994) with ‘predicate-argument’ structures, using sense tags for highly polysemous words and semantic role labels for each argument. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG1 the window] and [ARG1 The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in that the annotation is close to the syntactic structure and each verb is its own predicate. I"
W05-0309,miltsakaki-etal-2004-penn,0,0.0153934,"e appropriateness of the grouped sense tags, and indicates potential for providing a useful level of granularity for MT. 3.3 Discourse connectives Another component of the Chinese / English Parallel Propbank II is the annotation of dis-course connectives for both Chinese corpus and its English translation. Like the other two components, the annotation is performed on the first 100K words of the Parallel Chinese English Treebank. The annotation of Chinese discourse connectives follows in large part the theoretic assumptions and annotation practices of the English Penn Discourse Project (PDTB) (Miltsakaki et al., 2004). Adaptations are made only when they are warranted by the linguistic facts of Chinese. While the English PTDB annotates both explicit and implicit discourse connectives, our iniNoun organization party investment development resolution English senses individuals working together event: putting things together state: the quality of being well-organization event: an occasion on which people can assemble for social interaction and entertainment political organization a band of people associated temporarily in some activity person or side in legal context time or money risked in hopes of profit th"
W05-0309,W04-2807,1,0.844354,"Missing"
W05-0309,J05-1004,1,0.556359,"ia Grant EIA02-05448 . event identifiers, and discourse and temporal relations, could provide the foundation for a major advance in our ability to automatically extract salient relationships from text. This will in turn facilitate breakthroughs in message understanding, machine translation, fact retrieval, and information retrieval. The Proposition Bank project is a major step towards providing this type of annotation. It takes a practical approach to semantic representation, adding a layer of predicate argument information, or semantic roles, to the syntactic structures of the Penn Treebank (Palmer et al., 2005). The Frame Files that provide guidance to the annotators constitute a rich English lexicon with explicit ties between syntactic realizations and coarse-grained senses, Framesets. PropBank Framesets are distinguished primarily by syntactic criteria such as differences in subcategorization frames, and can be seen as the toplevel of an hierarchy of sense distinctions. Groupings of fine-grained WordNet senses, such as those developed for Senseval2 (Palmer et al., to appear) provide an intermediate level, where groups are distinguished by either syntactic or semantic criteria. WordNet senses const"
W05-0309,W04-2704,1,\N,Missing
W05-0309,W05-0312,1,\N,Missing
W05-0309,W03-1707,1,\N,Missing
W05-0309,J03-4003,0,\N,Missing
W05-0309,C98-1013,0,\N,Missing
W05-0639,A00-2018,0,0.372094,"aper is organized as follows: Section 2 demonstrates the components of our SRL system. We elaborate the importance of training a new parser and outline our approach in Section 3 and Section 4. Finally, Section 5 reports and discusses the results. 2 Semantic Role Labeling: the Architecture Our SRL system has 5 phases: Parsing, Pruning, Argument Identification, Argument Classification, and Post Processing. The Argument Identification and Classification components are trained with Sec 0221 of the Penn Treebank corpus. 2.1 Parsing Previous SRL systems usually use a pure syntactic parser, such as (Charniak, 2000; Collins, 1999), to retrieve possible constituents. Once the boundary of a constituent is defined, there is no way to change it in later phases. Therefore the quality of the syntactic parser has a major impact on the final per237 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 237–240, Ann Arbor, June 2005. 2005 Association for Computational Linguistics formance of an SRL system, and the percentage of correct constituents that is generated by the syntactic parser also defines the recall upper bound of an SRL system. In order to attack this problem"
W05-0639,P02-1031,1,0.823793,"downstream applications, such as text summarization, question answering, and machine translation. As a result, semantic parsing could be an important intermediate step for natural language comprehension. In this paper, we investigate the task of Semantic Role Labeling (SRL): Given a verb in a sentence, the goal is to locate the constituents which are arguments of the verb, and assign them appropriate semantic roles, such as, Agent, Patient, and Theme. Previous SRL systems have explored the effects of using different lexical features, and experimented on different machine learning algorithms. (Gildea and Palmer, 2002; Pradhan et al., 2005; Punyakanok et al., 2004) However, these SRL systems generally extract features from sentences processed by a syntactic parser or other shallow parsing components, Martha Palmer University of Pennsylvania 3330 Walnut Street Philadelphia, PA 19104 USA mpalmer@linc.cis.upenn.edu such as a chunker and a clause identifier. As a result, the performance of the SRL systems relies heavily on those syntax-analysis tools. In order to improve the fundamental performance of an SRL system, we trained parsers with training data containing not only syntactic constituent information but"
W05-0639,H94-1020,0,0.018484,"usly selected arguments based on the constraints described above. 3 Training a Parser with Semantic Argument Information A good start is always important, especially for a successful SRL system. Instead of passively accepting candidate constituents from the upstream syntactic parser, an SRL system needs to interact with the parser in order to obtain improved performance. This motivated our first attempt which is to integrate syntactic parsing and semantic parsing as a single step, and hopefully as a result we would be able to discard the SRL pipeline. The idea is to augment the Penn Treebank (Marcus et al., 1994) constituent labels with the semantic role labels from the PropBank (Palmer et al., 2005), and generate a rich training corpus. For example, if an NP is also an argument ARG0 of a verb in the given sentence, we change the constituent label NP into NP-ARG0. A parser therefore is trained on this new corpus and should be able to serve as an SRL system at the same time as predicting a parse. However, this ideal approach is not feasible. Given the fact that there are many different semantic role labels and the same constituent can be different arguments of different verbs in the same sentence, the"
W05-0639,J05-1004,1,0.48786,"h Semantic Argument Information A good start is always important, especially for a successful SRL system. Instead of passively accepting candidate constituents from the upstream syntactic parser, an SRL system needs to interact with the parser in order to obtain improved performance. This motivated our first attempt which is to integrate syntactic parsing and semantic parsing as a single step, and hopefully as a result we would be able to discard the SRL pipeline. The idea is to augment the Penn Treebank (Marcus et al., 1994) constituent labels with the semantic role labels from the PropBank (Palmer et al., 2005), and generate a rich training corpus. For example, if an NP is also an argument ARG0 of a verb in the given sentence, we change the constituent label NP into NP-ARG0. A parser therefore is trained on this new corpus and should be able to serve as an SRL system at the same time as predicting a parse. However, this ideal approach is not feasible. Given the fact that there are many different semantic role labels and the same constituent can be different arguments of different verbs in the same sentence, the number of constituent labels will soon grow out of control and make the parser training c"
W05-0639,C04-1197,0,0.0639156,"tion, question answering, and machine translation. As a result, semantic parsing could be an important intermediate step for natural language comprehension. In this paper, we investigate the task of Semantic Role Labeling (SRL): Given a verb in a sentence, the goal is to locate the constituents which are arguments of the verb, and assign them appropriate semantic roles, such as, Agent, Patient, and Theme. Previous SRL systems have explored the effects of using different lexical features, and experimented on different machine learning algorithms. (Gildea and Palmer, 2002; Pradhan et al., 2005; Punyakanok et al., 2004) However, these SRL systems generally extract features from sentences processed by a syntactic parser or other shallow parsing components, Martha Palmer University of Pennsylvania 3330 Walnut Street Philadelphia, PA 19104 USA mpalmer@linc.cis.upenn.edu such as a chunker and a clause identifier. As a result, the performance of the SRL systems relies heavily on those syntax-analysis tools. In order to improve the fundamental performance of an SRL system, we trained parsers with training data containing not only syntactic constituent information but also semantic argument information. The new par"
W05-0639,W04-3212,1,0.918332,"Missing"
W05-0639,J03-4003,0,\N,Missing
W06-0609,I05-1081,1,0.820744,"p (NP (NP musical acts) (PP for (NP a new record label))))) .) the original PropBank annotation, a further link is provided, which specifies the relative pronoun as being of “semantic type” answers. (9) Original PropBank annotation: Arg1: [NP *T*-6] -&gt; which -&gt; answers rel: have Arg0: [NP-SBJ *-3] -&gt; we In PropBank, the different pieces of the utterance, including the trace under the verb said, were concatenated This additional link between which and answers is important for many applications that make use of preferences for semantic types of verb arguments, such as Word Sense Disambiguation (Chen & Palmer 2005). In the new annotation scheme, annotators will first label traces as arguments: (12) Original PropBank annotation: ARG1: [ Among other things] [ Mr. Azoff] [ would develop musical acts for a new record label] [ [*T*-1]] ARG0: they rel: said (10) Revised PropBank annotation (stage 1): Rel: have Arg1: [*T*-6] Arg0: [NP-SBJ *-3] Under the new approach, in stage one, Treebank annotation will introduce not a trace of the S clause, but rather *?*, an empty category indicating ellipsis. In stage three, PropBank annotators will link this null element to the S node, but the resulting chain will not be"
W06-0609,H94-1020,1,0.457429,"o tasks: part-of-speech tagging and syntactic annotation. The first task is to provide a part-of-speech tag for every token. Particularly relevant for PropBank work, verbs in any form (active, passive, gerund, infinitive, etc.) are marked with a verbal part of speech (VBP, VBN, VBG, VB, etc.). (Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles. (Marcus, et al. 1994; Bies, et al. 1995) Constituent boundaries are shown through syntactic node labels in the trees. In the simplest case, a node will contain an entire constituent, complete with any associated arguments or modifiers. However, in structures involving syntactic movement, sub-constituents may be displaced. In these cases, Treebank annotation represents the original position with a trace and shows the relationship as co-indexing. In (1) below, for example, the direct object of entail is shown with the trace *T*, which is coindexed to the WHNP node of the question word what. Introduction The PropBan"
W06-0609,J93-2004,0,0.0290163,"ismatches between the syntactic bracketing and the semantic role labeling that can be found, and our plans for reconciling them. 1 1.1 Treebank The Penn Treebank annotates text for syntactic structure, including syntactic argument structure and rough semantic information. Treebank annotation involves two tasks: part-of-speech tagging and syntactic annotation. The first task is to provide a part-of-speech tag for every token. Particularly relevant for PropBank work, verbs in any form (active, passive, gerund, infinitive, etc.) are marked with a verbal part of speech (VBP, VBN, VBG, VB, etc.). (Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles. (Marcus, et al. 1994; Bies, et al. 1995) Constituent boundaries are shown through syntactic node labels in the trees. In the simplest case, a node will contain an entire constituent, complete with any associated arguments or modifiers. However, in structures involving syntactic movement, sub-constituents may be dis"
W06-0609,J05-1004,1,0.470057,"Missing"
W07-1508,N06-1016,1,0.891856,"Missing"
W07-1508,W04-2807,1,0.851894,"Missing"
W07-1508,J05-1004,1,0.397769,"Missing"
W07-1508,I05-7009,0,0.0302756,"tropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sampleannotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al., 2005). Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actualannotation and adjudication. The nex"
W07-1508,N06-2015,1,\N,Missing
W09-2417,S07-1018,1,0.851873,"nce resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank,"
W09-2417,W04-2412,0,0.0812467,"Missing"
W09-2417,W05-0620,0,0.313707,"Missing"
W09-2417,J02-3001,0,0.0966796,"s sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their coreferents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inf"
W09-2417,W04-0803,0,0.0306698,"so from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (B"
W09-2417,S07-1008,0,0.0681195,"Missing"
W09-2417,C08-1084,1,0.879566,"Missing"
W09-2417,P86-1004,1,0.770233,"he fact that large-scale manual annotation projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. Furthermore, in the case of FrameNet, the annotation effort did not start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from a very large corpus, which did not allow for a view of the data as ‘full-text annotation’. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and ten1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalmer/ projects/ace.html 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106–111, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics dencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semanti"
W09-2417,D07-1002,0,0.0290019,"in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic information. This view of SRL as a sentence-"
W09-2417,P03-1002,0,0.0620561,"years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic"
W09-2417,W08-2121,0,0.0907887,"Missing"
W09-2417,S07-1017,1,\N,Missing
W09-2417,H86-1011,1,\N,Missing
W09-2417,erk-pado-2004-powerful,0,\N,Missing
W09-3020,J07-3002,0,0.027874,"d into two parts: the Xinhua Chinese newswire with literal English translations (4,363 parallel sentences) and the Sinorama Chinese news magazine with non-literal English translations (12,600 parallel sentences). We experimented with the two parts separately to see how literal and non-literal translations affect word-alignments. Introduction Since verbs tend to be the roots of dependency relations in a sentence (Palmer et al., 2005), when it comes down to translations, finding correct mappings between verbs in a source and a target language is very important. Many machine translation systems (Fraser and Marcu, 2007) use wordalignment tools such as GIZA++ (Och and Ney, 2003) to retrieve word mappings between a source and a target language. Although GIZA++ gives well-structured alignments, it has limitations in several ways. First, it is hard to verify if alignments generated by GIZA++ are correct. Second, GIZA++ may not find alignments for low-frequent words. Third, GIZA++ does not account for any semantic information. In this paper, we suggest a couple of ways to enhance word-alignments for predicating expressions such as verbs1 . We restricted the source and the target language to Chinese and English, r"
W09-3020,J03-1002,0,0.00432598,"sh translations (4,363 parallel sentences) and the Sinorama Chinese news magazine with non-literal English translations (12,600 parallel sentences). We experimented with the two parts separately to see how literal and non-literal translations affect word-alignments. Introduction Since verbs tend to be the roots of dependency relations in a sentence (Palmer et al., 2005), when it comes down to translations, finding correct mappings between verbs in a source and a target language is very important. Many machine translation systems (Fraser and Marcu, 2007) use wordalignment tools such as GIZA++ (Och and Ney, 2003) to retrieve word mappings between a source and a target language. Although GIZA++ gives well-structured alignments, it has limitations in several ways. First, it is hard to verify if alignments generated by GIZA++ are correct. Second, GIZA++ may not find alignments for low-frequent words. Third, GIZA++ does not account for any semantic information. In this paper, we suggest a couple of ways to enhance word-alignments for predicating expressions such as verbs1 . We restricted the source and the target language to Chinese and English, respectively. The goal is to use the linguistic annotation a"
W09-3020,J05-1004,1,0.35791,"Missing"
W09-3020,W04-2705,0,\N,Missing
W09-3036,H05-1066,0,0.0147844,"ation of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conversion approach. 2 Two Kinds of Syntactic Structure Two different approaches to describing syntactic structure, dependency structure (DS) (Mel’čuk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides. Formally, in a PS tree,"
W09-3036,P99-1065,0,0.0911636,"ed; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dep"
W09-3036,P03-1054,0,0.00169422,"how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conve"
W09-3036,C02-2025,0,0.0152376,"d theories. The next section highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure version. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al. 2005). A multi-layered approach is also found in the Prague Dependency Treebank (Hajič et al. 2001), or in treebanks based on LFG (King et al. 2003) or HPSG (Oepen et al. 2002). A lesson learned here is that the addition of deeper, more semantic levels may be complicated if the syntactic annotation was not designed with the possibility of multiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of propbanking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not nat"
W09-3036,J05-1004,1,0.175107,"s are carefully coordinated. 1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natural language processing components. Today, treebanks have been constructed for many languages, including Arabic, Chinese, Czech, English, French, German, Korean, Spanish, and Turkish. This paper describes the creation of a Hindi/Urdu multi-representational and multilayered treebank. Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English PropBank (Palmer et al. 2005). Multirepresentational means that we distinguish conceptually what is being represented from how it is represented; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is"
W09-3036,P07-1021,0,\N,Missing
W10-0512,doddington-etal-2004-automatic,0,0.0155622,"Missing"
W10-0512,N06-4006,0,0.0212006,"ties that were included in the Automatic Content Extraction (ACE) guidelines (LDC 2004) are: Person, Location, Organization, and Facility, the four maximal entity classes. Our preliminary annotation task consists of identifying the syntactic span and entity class for these four types of entities in a pilot set of Twitter data (200 tweets from a data set generated during the 2009 Oklahoma grassfires). In future annotation, the ontology will be expanded to include event and relation annotations, as well as additional subclasses of the entities now examined. Annotations are done using Knowtator (Ogren 2006), a tool built within the Protégé framework (http://protege.stanford.edu/). The ontology development is data-driven; as such it is likely that certain ACE annotations will never emerge and other annotations (such as disaster-relevant materials) will be necessary additions. Three annotators undertook pilot annotation as part of the construction of preliminary annotation guidelines; the top pairwise ITA score is reported below. Twitter data makes reference to numerous entity spans that are of specific interest to this annotation task, such as road intersections and multiword named entities. The"
W10-0801,N06-2015,1,0.808089,"Missing"
W10-0801,P96-1006,0,0.0111307,"Missing"
W10-0801,J05-1004,1,0.222428,"r each predicate, if a particular predicate does not exist in the sparse training data, a system cannot create an accurate semantic interpretation. Even if the predicate is present, the appropriate sense might not be. In such a case, the WSD will again be unable to contribute to a correct overall semantic interpretation. This is the case in example (1), where even the extremely fine-grained sense distinctions provided by WordNet do not include a sense of hiss that is consistent with the caused motion interpretation rendered in the example. Available for SRL tasks are efforts such as PropBank (Palmer et al., 2005) and FrameNet (Fillmore et al., 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but ra"
W10-0801,J93-2004,0,\N,Missing
W10-0801,W05-0620,0,\N,Missing
W10-0801,J02-3001,0,\N,Missing
W10-1808,J08-4004,0,0.188827,"ators, and invest the savings into annotating more data. Perhaps more appropriately, it may be feasible to simply cut back on the amount of training provided per annotator and instead annotate more data. On the other hand, when the unlabeled data is not freely obtainable, double annotation may be more suitable as a route to improving system performance. There may also be factors other than cost-effectiveness which make double annotation desirable. Many projects point to their ITA rates and corresponding kappa values as a measure of annotation quality, and of the reliability of the annotators (Artstein and Poesio, 2008). The OntoNotes project used ITA rates as a way of evaluating the clarity of the sense inventory that was being developed in parallel with the annotation. Lexical entries that resulted in low ITA rates were revised, usually improving the ITA rate. Calculating these rates requires double-blind annotation. Annotators who consistently produced ITA rates lower than average were also removed from the project. Therefore, caution is advised in determining when to dispense with double annotation in favor of more cost effective single annotation. Double annotation can also be used to shed light on othe"
W10-1808,P09-1032,0,0.0580489,"Boulder Rodney.Nielsen@colorado.edu Martha.Palmer@colorado.edu Abstract Currently, the commonly accepted wisdom sides with the view that says that blind double annotation followed by adjudication of disagreements is necessary to create annotated corpora that leads to the best possible performance. We provide empirical evidence that this is unlikely to be the case. Rather, the greatest value for your annotation dollar lies in single annotating more data. There may, however, be other considerations that still argue in favor of double annotation. In this paper, we also consider the arguments of Beigman and Klebanov (2009), who suggest that data should be multiply annotated and then filtered to discard all of the examples where the annotators do not have perfect agreement. We provide evidence that single annotating more data for the same cost is likely to result in better system performance. This paper proceeds as follows: first, we outline our evaluation framework in Section 2. Next, we compare the single annotation and adjudication scenarios in Section 3. Then, we compare the annotation scenario of Beigman and Klebanov (2009) with the single annotation scenario in Section 4. After that, we discuss the results"
W10-1808,J09-4005,0,0.119391,"Missing"
W10-1808,P08-2008,1,0.928253,"ples of each of the 15 most frequent verbs. For the remaining 200 verbs, we utilized all the annotated examples. The resulting dataset contained 66,228 instances of the 215 most frequent verbs. Table 1 shows various important characteristics of this dataset averaged across the 215 verbs. Inter-tagger agreement Annotator1-gold standard agreement Share of the most frequent sense Number of classes (senses) per verb 2.3 For the experiments we conduct in this study, we needed a word sense disambiguation (WSD) system. Our WSD system is modeled after the stateof-the-art verb WSD system described in (Dligach and Palmer, 2008). We will briefly outline it here. We view WSD as a supervised learning problem. Each instance of the target verb is represented as a vector of binary features that indicate the presence (or absence) of the corresponding features in the neighborhood of the target verb. We utilize all of the linguistic features that were shown to be useful for disambiguating verb senses in (Chen et al., 2007). To extract the lexical features we POS-tag the sentence containing the target verb and the two surrounding sentences using MXPost software (Ratnaparkhi, 1998). All open class words (nouns, verbs, adjectiv"
W10-1808,N06-2015,1,0.752435,"e. Rather, the greatest value for your annotation dollar lies in single annotating more data. 1 Introduction In recent years, supervised learning has become the dominant paradigm in Natural Language Processing (NLP), thus making the creation of handannotated corpora a critically important task. A corpus where each instance is annotated by a single tagger unavoidably contains errors. To improve the quality of the data, an annotation project may choose to annotate each instance twice and adjudicate the disagreements, thus producing the (largely) error-free gold standard. For example, OntoNotes (Hovy et al., 2006), a large-scale annotation project, chose this option. However, given a virtually unlimited supply of unlabeled data and limited funding – a typical set of constraints in NLP – an annotation project must always face the realization that for the cost of double annotation, more than twice as much data can be single annotated. The philosophy behind this alternative says that modern machine learning algorithms can still generalize well in the presence of noise, especially when given larger amounts of training data. 2 2.1 Evaluation Data For evaluation we utilize the word sense data annotated by th"
W10-1808,I05-1081,1,\N,Missing
W10-1810,J05-1004,1,0.108587,"Missing"
W10-1810,P98-1013,0,0.078782,"Missing"
W10-1810,N04-1030,0,0.0417353,"Missing"
W10-1810,J02-3001,0,0.0135769,"Missing"
W10-1810,W04-0401,0,0.0591779,"Missing"
W10-1810,W04-2705,0,0.119612,"Missing"
W10-1810,C98-1013,0,\N,Missing
W10-1811,W04-3228,0,0.0233037,"nformation got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. Figure 1 shows a dependency tree that has been converted from the corresponding phrase structure tree. S VP NP1 Introduction Dependency structure has recently gained wide interest because it is simple yet provides useful information for many NLP tasks such as sentiment analysis (Kessler and Nicolov, 2009) or machine translation (Gildea, 2004). Although dependency structure is a kind of syntactic structure, it is quite different from phrase structure: phrase structure gives phrase information by grouping constituents whereas dependency structure gives dependency relations between pairs of words. Many dependency relations (e.g., subject, object) have high correlations with semantic roles (e.g., agent, patient), which makes dependency structure suitDT NNS VBP The results appear PP1 IN NP in NP NN POS today ’s ROOT NMOD root The NN news PMOD NMOD SBJ results LOC appear NMOD in today &apos;s news Figure 1: Phrase vs. dependency structure 91"
W10-1811,W07-2416,0,0.046988,"nn Treebank, but used a different dependency conversion tool, Penn2Malt.1 Our work is distinguished from theirs because we keep the tree structure but use heuristics to find the boundaries. Johansson (2008) also tried to find semantic boundaries for evaluation of his semantic role labeling system using dependency structure. He used heuristics that apply to general cases whereas we add more detailed heuristics for specific cases. 3 We used the same tool as the one used for the CoNLL’09 shared task to automatically convert the phrase structure trees in the Penn Treebank to the dependency trees (Johansson and Nugues, 2007). The script gives several options for the conversion; we mostly used the default values except for the following options:2 • splitSlash=false: do not split slashes. This option is taken so the dependency trees preserve the same number of word-tokens as the original phrase structure trees. • noSecEdges=true: ignore secondary edges if present. This option is taken so all siblings of verb predicates in phrase structure become children of the verbs in dependency structure regardless of empty categories. Figure 3 shows the converted dependency tree, which is produced when the secondary edge (*ICH*"
W10-1811,J93-2004,0,0.0378088,"Missing"
W10-1811,J05-1004,1,0.392094,"hermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation. 1 In 2009, the Conference on Computational Natural Language Learning (CoNLL) opened a shared task: the participants were supposed to take dependency trees as input and produce semantic role labels as output (Hajiˇc et al., 2009). The dependency trees were automatically converted from the Penn Treebank (Marcus et al., 1993), which consists of phrase structure trees, using some heuristics (cf. Section 3). The semantic roles were extracted from the Propbank (Palmer et al., 2005). Since Propbank arguments were originally annotated at the phrase level using the Penn Treebank and the phrase information got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. Figure 1 shows a dependency tree that has been converted from the corresponding phrase structure tree. S VP NP1 Introduction Dependency structure has recently gained wide interest because it is simple yet provides usef"
W10-1811,N04-1030,0,0.0400337,"ns. First, it is often not clear how punctuation 96 NMOD P NMOD a APPO fellow OPRD DEP TMP PMOD Accuracy = named John , who stayed for years 1 X · c(gold(arg), sys(arg)) T ∀arg Figure 20: Past-participle example 2 P recision = ∀arg needs to be annotated in either Treebank or Propbank; because of that, annotation for punctuation is not entirely consistent, which makes it hard to evaluate. Second, although punctuation gives useful information for obtaining semantic boundaries, it is not crucial for semantic roles. In fact, some of the state-of-art semantic role labeling systems, such as ASSERT (Pradhan et al., 2004), give an option for omitting punctuation from the output. For these reasons, our final model ignores punctuation for semantic boundaries. 6 Recall = 1 X |gold(arg) ∩ sys(arg)| · T |gold(arg)| ∀arg F1 = 2 · P recision · Recall P recision + Recall Table 1 shows the results from the models using the measurements. As expected, each model shows improvement over the previous one in terms of accuracy and F1-score. The F1-score of Model VI shows improvement that is statistically significant compared to Model I using t-test (t = 149.00, p < 0.0001). The result from the final model is encouraging becau"
W10-1811,W09-1201,0,\N,Missing
W10-1811,W09-3020,1,\N,Missing
W10-1836,W03-1006,0,0.060505,". 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument,"
W10-1836,choi-etal-2010-propbank-instance,1,0.897928,"integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, which has improved the annotation process by displaying several types of relevant syntactic and semantic information at the same time. Having everything displayed helps the annotator quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation (Choi et al., 20010b). Both tools are available as Op"
W10-1836,P08-1091,1,0.8821,"e Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ w"
W10-1836,J02-3001,0,0.0350371,"the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the"
W10-1836,P02-1031,1,0.76542,"d multi-word expressions. New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the"
W10-1836,N07-2014,0,0.0683748,"Missing"
W10-1836,N06-2015,1,0.851769,"Missing"
W10-1836,W10-1810,1,0.834857,"ing of cohesion in the group. The APB has decided to thoroughly tackle light verb constructions and multi-word expressions as part of an effort to facilitate mapping between the different languages that are being PropBanked. In the process of setting this up a number of challenges have surfaced which include: how can we cross-linguistically approach these phenomena in a (semi) integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, w"
W10-1836,P04-1043,0,0.0648245,"e in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be lab"
W10-1836,W05-0630,0,0.0204675,"tomated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experi"
W10-1836,J05-1004,1,0.599345,"aper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ would be the theme/content. According to PropBank, ‘John’ is labeled Arg0 (or enjoyer) and ‘movies’ is labeled Arg1 (or thing enjoyed). Crucially, that independent of the l"
W10-1836,N04-1030,1,0.800833,"New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’"
W10-1836,W04-3212,1,0.799084,"implify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, a"
W10-1836,P80-1024,0,0.623039,"Missing"
W10-1836,P09-5003,0,\N,Missing
W10-1836,W05-0620,0,\N,Missing
W10-1836,palmer-etal-2008-pilot,1,\N,Missing
W11-0110,W04-3213,0,\N,Missing
W11-0110,zaenen-etal-2008-encoding,0,\N,Missing
W11-0110,C08-1002,0,\N,Missing
W11-0110,P10-1160,0,\N,Missing
W11-0110,P08-2008,1,\N,Missing
W11-0110,D09-1120,0,\N,Missing
W11-0110,P08-1063,0,\N,Missing
W11-0110,W02-0813,1,\N,Missing
W11-0110,I05-1081,1,\N,Missing
W11-0133,C08-1010,0,0.138348,"serve as a stepping stone towards these goals. Such a scheme should account for differences in tutoring style and question scaffolding techniques and should capture the subtle distinctions between different question types. To do this, requires a representation that connects a turn’s communicative and rhetorical functions to its underlying semantic content. While efforts such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2009) have helped to make dialogue act annotation more uniform and applicable to a wider audience, and while tutoring-specific initiatives (Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008) have helped to bring dialogue acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences in tutorial questioning styles exhibited in our corpus of Socratic-style tutorial dialogues. Conversely, question type categories (Graesser and Person, 1994; Nielsen et al., 2008) have been designed with education in mind, but they largely ignore how the student and tutor may work together to construct meaning. The DISCOUNT scheme’s (Pilkington, 1999) combination of dialogue acts and rhetorical functions enabled it to better capture tutoring moves, but its om"
W11-0133,W04-2307,0,0.162091,"l dialogue annotation scheme can serve as a stepping stone towards these goals. Such a scheme should account for differences in tutoring style and question scaffolding techniques and should capture the subtle distinctions between different question types. To do this, requires a representation that connects a turn’s communicative and rhetorical functions to its underlying semantic content. While efforts such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2009) have helped to make dialogue act annotation more uniform and applicable to a wider audience, and while tutoring-specific initiatives (Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008) have helped to bring dialogue acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences in tutorial questioning styles exhibited in our corpus of Socratic-style tutorial dialogues. Conversely, question type categories (Graesser and Person, 1994; Nielsen et al., 2008) have been designed with education in mind, but they largely ignore how the student and tutor may work together to construct meaning. The DISCOUNT scheme’s (Pilkington, 1999) combination of dialogue acts and rhetorical functions enabled it to better capture"
W11-0403,I08-2099,0,0.0893979,"ation as well as lexical semantic information in the form of PropBank. The corpus also produces phrase structure representations in addition to de1 The term ’semantic argument’ is used to indicate all numbered arguments as well as modifiers in PropBank. 21 Proceedings of the Fifth Law Workshop (LAW V), pages 21–29, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics pendency structure. The Hindi Dependency Treebank has created an annotation scheme for Hindi by adapting labels from Panini’s Sanskrit grammar (also known as CPG: Computational Paninian Grammar; see Begum et al. (2008)). Previous work has demonstrated that the English PropBank tagset is quite similar to English dependency trees annotated with the Paninian labels (Vaidya et al., 2009). PropBank has also been mapped to other dependency schemes such as Functional Generative Description (Cinkova, 2006). 2.2 Hindi Dependency Treebank The Hindi Dependency Treebank (HDT) includes morphological, part-of-speech and chunking information as well as dependency relations. These are represented in the Shakti Standard Format (SSF; see Bharati et al. (2007)). The dependency labels depict relations between chunks, which are"
W11-0403,bhatia-etal-2010-empty,1,0.819722,"Missing"
W11-0403,W09-3036,1,0.903849,"(from now on, PropBank) is a corpus in which the arguments of each verb predicate are annotated with their semantic roles (Palmer et al., 2005). PropBank annotation has been carried out in several languages; most of them are annotated on top of Penn Treebank style phrase structure (Xue and Palmer, 2003; Palmer et al., 2008). However, a different grammatical analysis has been used for the Hindi PropBank annotation, dependency structure, which may be particularly suited for the analysis of flexible word order languages such as Hindi. As a syntactic corpus, we use the Hindi Dependency Treebank (Bhatt et al., 2009). Using dependency structure has some advantages. First, semantic arguments1 can be marked explicitly on the syntactic trees, so annotations of the predicate argument structure can be more consistent with the dependency structure. Second, the Hindi Dependency Treebank provides a rich set of dependency relations that capture the syntactic-semantic information. This facilitates mappings between syntactic dependents and semantic arguments. A successful mapping would reduce the annotation effort, improve the inter-annotator agreement, and guide a full fledged semantic role labeling task. In this p"
W11-0403,choi-etal-2010-propbank,1,0.941772,"indi PropBank (HPB) contains the labeling of semantic roles, which are defined on a verb-by-verb basis. The description at the verb-specific level is fine-grained; e.g., ‘hitter’ and ‘hittee’. These verbspecific roles are then grouped into broader categories using numbered arguments (ARG#). Each verb can also have modifiers not specific to the verb (ARGM*). The annotation process takes place in two stages: the creation of frameset files for individual verb types, and the annotation of predicate argu22 ment structures for each verb instance. As annotation tools, we use Cornerstone and Jubilee (Choi et al., 2010a; Choi et al., 2010b). The annotation is done on the HDT; following the dependency annotation, PropBank annotates each verb’s syntactic dependents as their semantic arguments at the chunk level. Chunked trees are conveniently displayed for annotators in Jubilee. PropBank annotations generated in Jubilee can also be easily projected onto the SSF format of the original dependency trees. The HPB currently consists of 24 labels including both numbered arguments and modifiers (Table 1). In certain respects, the HPB labels make some distinctions that are not made in some other language such as Engl"
W11-0403,choi-etal-2010-propbank-instance,1,0.840294,"Missing"
W11-0403,cinkova-2006-propbank,0,0.0177999,"shop (LAW V), pages 21–29, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics pendency structure. The Hindi Dependency Treebank has created an annotation scheme for Hindi by adapting labels from Panini’s Sanskrit grammar (also known as CPG: Computational Paninian Grammar; see Begum et al. (2008)). Previous work has demonstrated that the English PropBank tagset is quite similar to English dependency trees annotated with the Paninian labels (Vaidya et al., 2009). PropBank has also been mapped to other dependency schemes such as Functional Generative Description (Cinkova, 2006). 2.2 Hindi Dependency Treebank The Hindi Dependency Treebank (HDT) includes morphological, part-of-speech and chunking information as well as dependency relations. These are represented in the Shakti Standard Format (SSF; see Bharati et al. (2007)). The dependency labels depict relations between chunks, which are “minimal phrases consisting of correlated, inseparable entities” (Bharati et al., 2006), so they are not necessarily individual words. The annotation of chunks also assumes that intra-chunk dependencies can be extracted automatically (Husain et al., 2010). The dependency tagset consi"
W11-0403,hajicova-kucerova-2002-argument,0,0.0884332,"Missing"
W11-0403,W10-1810,1,0.788338,"is that neither the nominal nor the light verb seem to project arguments of their own. Important progress has been made in this work k7p pof Table 3: Mappings to the HPB modifiers. 3.3 इस_काम_& Simple and complex predicates this_work_LOC HPB distinguishes annotations between simple and complex predicates. Simple predicates consist of only a single verb whereas complex predicates consist of a light verb and a pre-verbal element. The complex predicates are identified with a special label ARGM-PRX (ARGument-PRedicating eXpresstion), which is being used for all light verb annotations in PropBank (Hwang et al., 2010). Figure 2 shows an example of the predicating noun mention annotated as ARGM-PRX, used with come. The predicating noun also has its own argument, matter of, indicated with the HDT label r6-k1. The HDT has two labels, r6-k1 and r6-k2, for the arguments of the predicating noun. Hence, the argument span for complex predicates includes not only direct dependents of the verb but also dependents of the noun. During the hearing on Wednesday, the matter was mentioned k7t k7t r6-k1 स""नवाई_&apos;_दौरान hearing_of_during ब""धवार_को Wed._of माम0_का matter_of pof िज3_भी mention_to आया come ARGM-PRX ARG1 ARGM-TM"
W11-0403,H94-1020,0,0.282794,"_का matter_of pof िज3_भी mention_to आया come ARGM-PRX ARG1 ARGM-TMP ARGM-TMP Figure 2: Complex predicate example. The ARGM-PRX label usually overlaps with the HDT label pof, indicating a ‘part of units’ as pre24 मह+वप.ण_1ग / 4त important_progress &apos;ई_) be_PRES ARG1 ARGM-LOC Figure 3: HDT vs. HPB on complex predicates. 4 Automatic mapping of HDT to HPB Mapping between syntactic and semantic structures has been attempted in other languages. The Penn English and Chinese Treebanks consist of several semantic roles (e.g., locative, temporal) annotated on top of Penn Treebank style phrase structure (Marcus et al., 1994; Xue and Palmer, 2009). The Chinese PropBank specifies mappings between syntactic and semantic arguments in frameset files (e.g., SBJ → ARG0) that can be used for automatic mapping (Xue and Palmer, 2003). However, these Chinese mappings are limited to certain types of syntactic arguments (mostly subjects and objects). Moreover, semantic annotations on the Treebanks are done independently from PropBank annotations, which causes disagreement between the two structures. Dependency structure transparently encodes relations between predicates and their arguments, which facilitates mappings between"
W11-0403,J05-1004,1,0.513651,"rically-derived rules: predicate ID, predicate’s voice type, and argument’s dependency label. The predicate ID is either the lemma or the roleset ID of the predicate. Predicate lemmas are already provided in HDT. When we use predicate lemmas, we assume no manual annotation of PropBank. Thus, rules generated from predicate lemmas can be applied to any future data without modification. When we use roleset ID’s, we assume that sense annotations are already done. PropBank includes annotations of coarse verb senses, called roleset ID’s, that differentiate each verb predicate with different senses (Palmer et al., 2005). A verb predicate can form several argument structures with respect to different senses. Using roleset ID’s, we generate more fine-grained rules that are specific to those senses. The predicate’s voice type is either ‘active’ or ‘passive’, also provided in HDT. There are not many instances of passive construction in our current data, which makes it difficult to generate rules general enough for future data. However, even with the lack of training instances, we find some advantage of using the voice feature in our experiments. Finally, the argument’s dependency label is the dependency label of"
W11-0403,palmer-etal-2008-pilot,1,0.730572,"Missing"
W11-0403,W03-1707,1,0.733316,"as pre24 मह+वप.ण_1ग / 4त important_progress &apos;ई_) be_PRES ARG1 ARGM-LOC Figure 3: HDT vs. HPB on complex predicates. 4 Automatic mapping of HDT to HPB Mapping between syntactic and semantic structures has been attempted in other languages. The Penn English and Chinese Treebanks consist of several semantic roles (e.g., locative, temporal) annotated on top of Penn Treebank style phrase structure (Marcus et al., 1994; Xue and Palmer, 2009). The Chinese PropBank specifies mappings between syntactic and semantic arguments in frameset files (e.g., SBJ → ARG0) that can be used for automatic mapping (Xue and Palmer, 2003). However, these Chinese mappings are limited to certain types of syntactic arguments (mostly subjects and objects). Moreover, semantic annotations on the Treebanks are done independently from PropBank annotations, which causes disagreement between the two structures. Dependency structure transparently encodes relations between predicates and their arguments, which facilitates mappings between syntactic and semantic arguments. Hajiˇcov´a and Kuˇcerov´a (2002) tried to project PropBank semantic roles onto the Prague Dependency Treebank, and showed that the projection is not trivial. The same ma"
W11-0403,N06-5006,0,\N,Missing
W11-0408,W99-0606,0,0.439558,"fe annotation project. Our work also borrows from the error detection literature. Researchers have explored error detection for manually tagged corpora in the context of pos-tagging (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), dependency parsing (Dickinson, 2009), and text-classification (Fukumoto and Suzuki, 2004). The approaches to error detection include anomaly detection (Eskin, 2000), finding inconsistent annotations (van Halteren, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), and using the weights assigned by learning algorithms such as boosting (Abney et al., 1999; Luo et al., 2005) and SVM (Nakagawa and Matsumoto, 2002; Fukumoto and Suzuki, 2004) by exploiting the fact that errors tend to concentrate among the examples with large weights. Some of 66 these works eliminate the errors (Luo et al., 2005). Others correct them automatically (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Fukumoto and Suzuki, 2004; Dickinson, 2009) or manually (Kvˇetoˇn and Oliva, 2002). Several authors also demonstrate ensuing performance improvements (Fukumoto and Suzuki, 2004; Luo et al., 2005; Dickinson, 2009). All of these researchers experimented with single annotated data suc"
W11-0408,N06-1016,1,0.845212,"3, we evaluate our approach in section 4, we discuss the results and draw a conclusion in section 5, and finally, we talk about our plans for future work in section 6. 2 Related Work Active Learning (Settles, 2009; Olsson, 2009) has been the traditional avenue for reducing the amount of annotation. However, in practice, serial active learning is difficult in a multi-tagger environment (Settles, 2009) when many annotators are working in parallel (e.g. OntoNotes employs tens of taggers). At the same time, several papers recently appeared that used OntoNotes data for active learning experiments (Chen et al., 2006; Zhu, 2007; Zhong et al., 2008). These works all utilized OntoNotes gold standard labels, which were obtained via double annotation and adjudication. The implicit assumption, therefore, was that the same process of double anno65 Proceedings of the Fifth Law Workshop (LAW V), pages 65–73, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics tation and adjudication could be reproduced in the process of active learning. However, this assumption is not very realistic and in practice, these approaches may not bring about the kind of annotation cost reduction that the"
W11-0408,E09-1023,0,0.0283108,"at, our algorithms can be applied to select a subset of the single annotated data for the second round of annotation and adjudication. Our algorithms select the data for repeated labeling in a single batch, which means the selection can be done off-line. This should greatly simplify the application of our approach in a real life annotation project. Our work also borrows from the error detection literature. Researchers have explored error detection for manually tagged corpora in the context of pos-tagging (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), dependency parsing (Dickinson, 2009), and text-classification (Fukumoto and Suzuki, 2004). The approaches to error detection include anomaly detection (Eskin, 2000), finding inconsistent annotations (van Halteren, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), and using the weights assigned by learning algorithms such as boosting (Abney et al., 1999; Luo et al., 2005) and SVM (Nakagawa and Matsumoto, 2002; Fukumoto and Suzuki, 2004) by exploiting the fact that errors tend to concentrate among the examples with large weights. Some of 66 these works eliminate the errors (Luo et al., 2005). Others correct them automa"
W11-0408,P08-2008,1,0.837172,"labeled and the goal is to select data for repeated labeling. 3.2 Machine Tagger Algorithm The main goal of the machine tagger algorithm is finding inconsistent labeling in the data. This algorithm operates by training a discriminative classifier and making a prediction for each instance in the pool. Whenever this prediction disagrees with the human-assigned label, the instance is selected for repeated labeling. For classification we choose a support vector machine (SVM) classifier because we need a highaccuracy classifier. The state-of-the art system we use for our experiments is SVM-based (Dligach and Palmer, 2008). The specific classification software we utilize is LibSVM (Chang and Lin, 2001). We accept the default settings (C = 1 and linear kernel). 3.3 3.4 The ambiguity detector algorithm trains a probabilistic classifier and makes a prediction for each instance in the pool. However, unlike the previous algorithm, the objective in this case is to find the instances that are potentially hard to annotate due to their ambiguity. The instances that lie close to the decision boundary are intrinsically ambiguous and therefore harder to annotate. We hypothesize that a human tagger is more likely to make a"
W11-0408,A00-2020,0,0.917922,"nstances selected via active learning, random sampling or some other technique). After that, our algorithms can be applied to select a subset of the single annotated data for the second round of annotation and adjudication. Our algorithms select the data for repeated labeling in a single batch, which means the selection can be done off-line. This should greatly simplify the application of our approach in a real life annotation project. Our work also borrows from the error detection literature. Researchers have explored error detection for manually tagged corpora in the context of pos-tagging (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), dependency parsing (Dickinson, 2009), and text-classification (Fukumoto and Suzuki, 2004). The approaches to error detection include anomaly detection (Eskin, 2000), finding inconsistent annotations (van Halteren, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), and using the weights assigned by learning algorithms such as boosting (Abney et al., 1999; Luo et al., 2005) and SVM (Nakagawa and Matsumoto, 2002; Fukumoto and Suzuki, 2004) by exploiting the fact that errors tend to concentrate among the examples with large weigh"
W11-0408,C04-1125,0,0.54304,"a subset of the single annotated data for the second round of annotation and adjudication. Our algorithms select the data for repeated labeling in a single batch, which means the selection can be done off-line. This should greatly simplify the application of our approach in a real life annotation project. Our work also borrows from the error detection literature. Researchers have explored error detection for manually tagged corpora in the context of pos-tagging (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), dependency parsing (Dickinson, 2009), and text-classification (Fukumoto and Suzuki, 2004). The approaches to error detection include anomaly detection (Eskin, 2000), finding inconsistent annotations (van Halteren, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), and using the weights assigned by learning algorithms such as boosting (Abney et al., 1999; Luo et al., 2005) and SVM (Nakagawa and Matsumoto, 2002; Fukumoto and Suzuki, 2004) by exploiting the fact that errors tend to concentrate among the examples with large weights. Some of 66 these works eliminate the errors (Luo et al., 2005). Others correct them automatically (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Fukum"
W11-0408,N06-2015,1,0.875158,"Missing"
W11-0408,W09-1904,0,0.0471945,"rrors because of the large size of their data sets. Instead, to demonstrate the effectiveness of their approaches, they examined a selected subset of the detected examples (e.g. (Abney et al., 1999; Eskin, 2000; Nakagawa and Matsumoto, 2002; Nov´ak and Raz´ımov´a, 2009)). In this paper, we experiment with fully double annotated and adjudicated data, which allows us to evaluate the effectiveness of our approach more precisely. A sizable body of work exists on using noisy labeling obtained from low-cost annotation services such as Amazon’s Mechanical Turk (Snow et al., 2008; Sheng et al., 2008; Hsueh et al., 2009). Hsueh et al. (2009) identify several criteria for selecting high-quality annotations such as noise level, sentiment ambiguity, and lexical uncertainty. (Sheng et al., 2008) address the relationships between various repeated labeling strategies and the quality of the resulting models. They also propose a set of techniques for selective repeated labeling which are based on the principles of active learning and an estimate of uncertainty derived from each example’s label multiset. These authors focus on the scenario where multiple (greater than two) labels can be obtained cheaply. This is not t"
W11-0408,C02-1021,0,0.388956,"Missing"
W11-0408,J93-2004,0,0.0415135,"and SVM (Nakagawa and Matsumoto, 2002; Fukumoto and Suzuki, 2004) by exploiting the fact that errors tend to concentrate among the examples with large weights. Some of 66 these works eliminate the errors (Luo et al., 2005). Others correct them automatically (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Fukumoto and Suzuki, 2004; Dickinson, 2009) or manually (Kvˇetoˇn and Oliva, 2002). Several authors also demonstrate ensuing performance improvements (Fukumoto and Suzuki, 2004; Luo et al., 2005; Dickinson, 2009). All of these researchers experimented with single annotated data such as Penn Treebank (Marcus et al., 1993) and they were often unable to hand-examine all the data their algorithms marked as errors because of the large size of their data sets. Instead, to demonstrate the effectiveness of their approaches, they examined a selected subset of the detected examples (e.g. (Abney et al., 1999; Eskin, 2000; Nakagawa and Matsumoto, 2002; Nov´ak and Raz´ımov´a, 2009)). In this paper, we experiment with fully double annotated and adjudicated data, which allows us to evaluate the effectiveness of our approach more precisely. A sizable body of work exists on using noisy labeling obtained from low-cost annotati"
W11-0408,C02-1101,0,0.813641,"om the error detection literature. Researchers have explored error detection for manually tagged corpora in the context of pos-tagging (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), dependency parsing (Dickinson, 2009), and text-classification (Fukumoto and Suzuki, 2004). The approaches to error detection include anomaly detection (Eskin, 2000), finding inconsistent annotations (van Halteren, 2000; Kvˇetoˇn and Oliva, 2002; Nov´ak and Raz´ımov´a, 2009), and using the weights assigned by learning algorithms such as boosting (Abney et al., 1999; Luo et al., 2005) and SVM (Nakagawa and Matsumoto, 2002; Fukumoto and Suzuki, 2004) by exploiting the fact that errors tend to concentrate among the examples with large weights. Some of 66 these works eliminate the errors (Luo et al., 2005). Others correct them automatically (Eskin, 2000; Kvˇetoˇn and Oliva, 2002; Fukumoto and Suzuki, 2004; Dickinson, 2009) or manually (Kvˇetoˇn and Oliva, 2002). Several authors also demonstrate ensuing performance improvements (Fukumoto and Suzuki, 2004; Luo et al., 2005; Dickinson, 2009). All of these researchers experimented with single annotated data such as Penn Treebank (Marcus et al., 1993) and they were of"
W11-0408,W09-3024,0,0.041651,"Missing"
W11-0408,D08-1027,0,0.240931,"Missing"
W11-0408,W00-1907,0,0.806536,"Missing"
W11-0408,D08-1105,0,0.017074,"section 4, we discuss the results and draw a conclusion in section 5, and finally, we talk about our plans for future work in section 6. 2 Related Work Active Learning (Settles, 2009; Olsson, 2009) has been the traditional avenue for reducing the amount of annotation. However, in practice, serial active learning is difficult in a multi-tagger environment (Settles, 2009) when many annotators are working in parallel (e.g. OntoNotes employs tens of taggers). At the same time, several papers recently appeared that used OntoNotes data for active learning experiments (Chen et al., 2006; Zhu, 2007; Zhong et al., 2008). These works all utilized OntoNotes gold standard labels, which were obtained via double annotation and adjudication. The implicit assumption, therefore, was that the same process of double anno65 Proceedings of the Fifth Law Workshop (LAW V), pages 65–73, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics tation and adjudication could be reproduced in the process of active learning. However, this assumption is not very realistic and in practice, these approaches may not bring about the kind of annotation cost reduction that they report. For example, an instan"
W11-0408,D07-1082,0,0.0191336,"approach in section 4, we discuss the results and draw a conclusion in section 5, and finally, we talk about our plans for future work in section 6. 2 Related Work Active Learning (Settles, 2009; Olsson, 2009) has been the traditional avenue for reducing the amount of annotation. However, in practice, serial active learning is difficult in a multi-tagger environment (Settles, 2009) when many annotators are working in parallel (e.g. OntoNotes employs tens of taggers). At the same time, several papers recently appeared that used OntoNotes data for active learning experiments (Chen et al., 2006; Zhu, 2007; Zhong et al., 2008). These works all utilized OntoNotes gold standard labels, which were obtained via double annotation and adjudication. The implicit assumption, therefore, was that the same process of double anno65 Proceedings of the Fifth Law Workshop (LAW V), pages 65–73, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics tation and adjudication could be reproduced in the process of active learning. However, this assumption is not very realistic and in practice, these approaches may not bring about the kind of annotation cost reduction that they report. F"
W11-0410,W06-0602,0,0.0363629,"Missing"
W11-0410,W05-1306,1,0.867645,"Missing"
W11-0410,kingsbury-palmer-2002-treebank,1,0.68288,"class of semantic representation known as a predicate argument structure (PAS). Predicate argument structures are important in the context of resource development in part because they are the fundamental annotation target of the class of corpus known as a proposition bank. Much of the significance claim for this work comes from the significance of proposition banks themselves in recent research on natural language processing and computational lexical semantics. The impact of proposition banks on work in these fields is suggested by the large number of citations of just the three publications (Kingsbury and Palmer, 2002; Kingsbury et al., 2002; Palmer et al., 2005)—at the time of writing, 290, 220, and 567, respectively. Additional indications of the impact of PropBank on the field of natural language processing include its use as the data source for two shared tasks ((Carreras and M`arquez, 2005)). The methodology consists of looking for arguments that never co¨occur with each other. In structural linguistics, this property of non-co¨occurrence is known as complementary distribution. Complementary distribution occurs when two linguistic elements never occur in the same environment. In this case, the environ"
W11-0410,J82-2006,0,0.618585,"bject noun phrases both in French and in English, as has the sublanguage of technical manuals in English. (Neither of these sublanguages have been noted to occur in the PropBank corpus. The sublanguage of stock reports, however, presumably does occur in the corpus; this sublanguage has been noted to exhibit distributional subtleties of predicates and their arguments that might be relevant to the accuracy of the semantic representations in PropBank, but the distributional facts do not seem to include variability in argument co¨occurrence so much as patterns of argument/predicate co¨occurrence (Kittredge, 1982).) Finally, incidence of the predicate in the corpus could affect the observed distribution, and in particular, the range of argument co¨occurrences that are attested: the lower the number of observations of a predicate, the lower the chance of observing any two arguments together, and as the number of arguments in a roleset increases, the higher the chance of failing to see any pair together. That is, for a roleset with an arity of three and an incidence of n occurrences in a corpus, the likelihood of never seeing any two of the three arguments together is much lower than for a roleset with a"
W11-0410,J93-2004,0,0.0359813,"005b) that address architectural, sampling, and procedural issues, as well as publications such as (Hripcsak and Rothschild, 2005; Artstein and Poesio, 2008) that address issues in inter-annotator agreement. However, there is not yet a significant body of work on the subject of quality assurance for corpora, or for that matter, for many other types of linguistic resources. (Meyers et al., 2004) describe three error-checking measures used in the construction of NomBank, and the use of inter-annotator agreement as a quality control measure for corpus construction is discussed at some length in (Marcus et al., 1993; Palmer et al., 2005). However, discussion of quality control for corpora is otherwise limited or nonexistent. With the exception of the inter-annotatoragreement-oriented work mentioned above, none of this work is quantitative. This is a problem if our goal is the development of a true science of annotation. Work on quality assurance for computational lexical resources other than ontologies is especially lacking. However, the body of work on quality assurance for ontologies (Kohler et al., 2006; Ceusters et al., 2004; Cimino et al., 2003; Cimino, 1998; Cimino, 2001; Ogren et al., 2004) is wor"
W11-0410,meyers-etal-2004-annotating,0,0.0139248,"linguistics community has arguably been developing at least a nascent science of annotation for years, represented by publications such as (Leech, 1993; Ide and Brew, 2000; Wynne, 2005; Cohen et al., 2005a; Cohen et al., 2005b) that address architectural, sampling, and procedural issues, as well as publications such as (Hripcsak and Rothschild, 2005; Artstein and Poesio, 2008) that address issues in inter-annotator agreement. However, there is not yet a significant body of work on the subject of quality assurance for corpora, or for that matter, for many other types of linguistic resources. (Meyers et al., 2004) describe three error-checking measures used in the construction of NomBank, and the use of inter-annotator agreement as a quality control measure for corpus construction is discussed at some length in (Marcus et al., 1993; Palmer et al., 2005). However, discussion of quality control for corpora is otherwise limited or nonexistent. With the exception of the inter-annotatoragreement-oriented work mentioned above, none of this work is quantitative. This is a problem if our goal is the development of a true science of annotation. Work on quality assurance for computational lexical resources other"
W11-0410,J05-1004,1,0.503174,"chitectural, sampling, and procedural issues, as well as publications such as (Hripcsak and Rothschild, 2005; Artstein and Poesio, 2008) that address issues in inter-annotator agreement. However, there is not yet a significant body of work on the subject of quality assurance for corpora, or for that matter, for many other types of linguistic resources. (Meyers et al., 2004) describe three error-checking measures used in the construction of NomBank, and the use of inter-annotator agreement as a quality control measure for corpus construction is discussed at some length in (Marcus et al., 1993; Palmer et al., 2005). However, discussion of quality control for corpora is otherwise limited or nonexistent. With the exception of the inter-annotatoragreement-oriented work mentioned above, none of this work is quantitative. This is a problem if our goal is the development of a true science of annotation. Work on quality assurance for computational lexical resources other than ontologies is especially lacking. However, the body of work on quality assurance for ontologies (Kohler et al., 2006; Ceusters et al., 2004; Cimino et al., 2003; Cimino, 1998; Cimino, 2001; Ogren et al., 2004) is worth considering in the"
W11-0410,W05-0620,0,\N,Missing
W11-0410,J08-4004,0,\N,Missing
W11-0906,W10-1811,1,0.895778,"Missing"
W11-0906,P11-2121,1,0.849858,"do not include predicate sense classification as a part of our task, which is rather a task of word sense disambiguation than semantic role labeling. For in-domain and out-of-domain evaluations, W SJ section 23 and the Brown corpus are used, also distributed by CoNLL’09. To retrieve automatically generated dependency trees as input to our semantic role labeler, we train our open source dependency parser, called ClearParser3 , on the training set and run the parser on the evaluation sets. ClearParser uses a transition-based dependency parsing algorithm that gives near state-of-the-art results (Choi and Palmer, 2011), and mirrors our SRL algorithm. 5.3 Accuracy comparisons Baseline +Dynamic +Cluster 5.2 Statistical models We use Liblinear L2-L1 S VM for learning; a linear classification algorithm using L2 regularization and L1 loss function. This algorithm is designed to handle large scale data: it assumes the data to be linearly separable so does not use any kind of kernel space (Hsieh et al., 2008). As a result, it significantly reduces training time compared to typical S VM, yet performs accurately. For our experiments, we use the following learning parameters: c = 0.1 (cost), e = 0.2 (termination crit"
W11-0906,J02-3001,0,0.648942,"hansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002). This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (Hsieh et al., 2008). Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing). This motivates the development of"
W11-0906,W09-1201,0,0.0783207,"Missing"
W11-0906,W08-2122,0,0.0519197,"of relation between any word pair, semantic role labeling restricts its search only to top-down relations between predicate and argument pairs. Second, dependency parsing requires one head for each word, so the final output is a tree, whereas semantic role labeling allows multiple predicates for each argument. Thus, not all dependency parsing algorithms, such as a maximum spanning tree algorithm (Mcdonald and Pereira, 2006), can be naively applied to semantic role labeling. Some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results (Henderson et al., 2008; Titov et al., 2009). However, these algorithms are originally designed for dependency parsing, so are not necessarily customized for semantic role label38 ing. Here, we present a novel transition-based algorithm dedicated to semantic role labeling. The key difference between this algorithm and most other transition-based algorithms is in its directionality. Given an identified predicate, this algorithm tries to find top-down relations between the predicate and the words on both left and right-hand sides, whereas other transition-based algorithms would consider words on either the left or the"
W11-0906,P10-1110,0,0.066037,"Missing"
W11-0906,D08-1008,0,0.238536,"atistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 1 Introduction Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). Lately, dependency-based SRL has shown advantages over constituent-based SRL (Johansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002"
W11-0906,C10-1081,0,0.0127869,"e that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 1 Introduction Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). Lately, dependency-based SRL has shown advantages over constituent-based SRL (Johansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semanti"
W11-0906,E06-1011,0,0.0314723,"of dependency parsing in the sense that both try to find relations between word pairs. However, they are distinguished in two major ways. First, unlike dependency parsing that tries to find some kind of relation between any word pair, semantic role labeling restricts its search only to top-down relations between predicate and argument pairs. Second, dependency parsing requires one head for each word, so the final output is a tree, whereas semantic role labeling allows multiple predicates for each argument. Thus, not all dependency parsing algorithms, such as a maximum spanning tree algorithm (Mcdonald and Pereira, 2006), can be naively applied to semantic role labeling. Some transition-based dependency parsing algorithms have been adapted to semantic role labeling and shown good results (Henderson et al., 2008; Titov et al., 2009). However, these algorithms are originally designed for dependency parsing, so are not necessarily customized for semantic role label38 ing. Here, we present a novel transition-based algorithm dedicated to semantic role labeling. The key difference between this algorithm and most other transition-based algorithms is in its directionality. Given an identified predicate, this algorith"
W11-0906,J08-4003,0,0.0300517,"e takes less time than training them as a joint-inference task. However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (Hsieh et al., 2008). Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing). This motivates the development of a new semantic role labeling algorithm that treats these two steps as a joint inference task. Our algorithm is inspired by shift-reduce parsing (Nivre, 2008). The algorithm uses several transitions to identify predicates and their arguments with semantic roles. One big advantage of the transitionbased approach is that it can use previously identified arguments as features to predict the next argument. We apply this technique to our approach and achieve comparable results to another state-of-theart system evaluated on the same data sets. 37 Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 37–45, c Portland, Oregon, USA, June 23, 2011. 2011 Association for Computational Linguistics NO-PRED SHIFT NO-ARC← NO-A"
W11-0906,J05-1004,1,0.336784,"Missing"
W11-0906,J08-2006,0,0.0229608,"rate classifiers for identification and classification, which did not lead to better performance in our case. Table 2 shows parsing states generated by our algorithm. Our experiments show that this algorithm gives comparable results against another state-ofthe-art system. 3 want buy A0 A1 1 1 1 1 ... john:A0 to:A1 car:A1 ... 0s 0s 1 1 1 0 0 1 0s 0s Figure 2: Projecting the predicate argument structure of each verb into vector space. Predicate argument clustering Some studies showed that verb clustering information could improve performance in semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2008). This is because semantic role labelers usually perform worse on verbs not seen during training, for which the clustering information can provide useful features. Most previous studies used either bag-ofwords or syntactic structure to cluster verbs; however, this may or may not capture the nature of predicate argument structure, which is more semantically oriented. Thus, it is preferable to cluster verbs by their predicate argument structures to get optimized features for semantic role labeling. In this section, we present a self-learning clustering technique that effectively improves labelin"
W11-0906,scheible-2010-evaluation,0,0.0620201,"Missing"
W11-0906,D07-1002,0,0.076197,"ing clustering technique that effectively improves labeling accuracy in the test domain. For better generalization of the statistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL’09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks. 1 Introduction Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). Lately, dependency-based SRL has shown advantages over constituent-based SRL (Johansson and Nugues, 2008). Two main benefits can be found. First, dependency parsing is much faster than constituent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or d"
W11-0906,W04-3212,1,0.934216,"tuent parsing, whereas constituent parsing is usually considered to be a bottleneck to SRL in terms of execution time. Second, dependency structure is more similar to predicate argument structure than phrase structure because it specifically defines relations between a predicate and its arguments with labeled arcs. Unlike constituent-based SRL Traditionally, either constituent or dependencybased, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002). This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. However, recent machine learning algorithms can deal with large scale vector spaces without taking too much training time (Hsieh et al., 2008). Furthermore, from our experience in dependency parsing, handling these steps together improves accuracy in identification as well as classification (unlabeled and labeled attachment scores in dependency parsing). This motivates the development of a new semantic role labeling algorithm that treats these two steps as a joint inference task. Our algorith"
W11-0906,D08-1059,0,0.0715103,"Missing"
W11-0906,D09-1004,0,0.0145657,"ied as an argument of wj with a label L. These transitions can be performed in any order as long as their preconditions are satisfied. For our experiments, we use the following generalized sequence: ← [ (NO-PRED)∗ ⇒ (LEFT-ARC← L |N O -A RC )∗ ⇒ → (RIGHT-ARC→ L |N O -A RC )∗ ⇒ S HIFT ]∗ By adding the NO-ARC transitions, we successfully merge these two steps together without decrease in labeling accuracy.1 Since each word can be a predicate candidate and each predicate considers all other words as argument candidates, a worst-case complexity of the algorithm is O(n2 ). To reduce the complexity, Zhao et al. (2009) reformulated a pruning algorithm introduced by Xue and Palmer (2004) for dependency structure by considering only direct dependents of a predicate and its ancestors as argument candidates. This pruning algorithm can be easily applied to our algorithm: the oracle can prefilter such dependents and uses the information to perform NO-ARC transitions without consulting statistical models. 1 Notice that this algorithm does not take separate steps for argument identification and classification. 39 We also experimented with the traditional approach of building separate classifiers for identification"
W11-0906,N07-1070,0,\N,Missing
W11-0906,W05-0620,0,\N,Missing
W11-0910,P98-1013,0,0.229848,"of the construction across verb classes in the lexicon and evaluate against VerbNet. We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages. 1 Introduction Automatic semantic analysis has been very successful when taking a supervised learning approach on data labeled with sense tags and semantic roles (e.g., see Màrquez et al., 2008). Underlying these recent successes are lexical resources, such as PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998; Fillmore et al., 2002), which encode the relational semantics of numerous lexical items, especially verbs. However, because authors and speakers use verbs productively in previously unseen ways, semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources (cf. Pustejovsky & Jezek, 2008). To achieve more accurate semantic analyses, we must augment such resources with knowledge of the extensibility of verbs. Central to verb extensibility is the process of semantic and syntactic coercion. Coercion allows a verb to be used"
W11-0910,P98-1046,1,0.549854,"that coerce the meaning of blink to fit with a CM event would currently be misanalysed. One option might be to augment the Hiccup class with the CM frame from the Pour class, which would ensure that such sentences would be analyzed more accurately. However, given the productive nature of constructional coercion and its widespread applicability, the approach of adding any possible pattern to each class is not appropriate: this would undermine the definitional distinctions between classes and greatly lessen their usefulness. Complicating the issue is the phenomenon of regular sense extensions (Dang et al., 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb. For example, the verbs in the Push class express the general meaning of exerting force on an object, such as She pushed on the wall. Often, the exertion of force moves the object, which can be expressed in a CM construction such as She pushed the box across the room. VerbNet accounts for this regular sense extension by including most of the Push verbs in the Carry class as well, which has the CM construction as one of its frames. Deciding when to include a verb in another class base"
W11-0910,P10-1160,0,0.0310982,"project the syntactic structures that encode that information. Verbs are also highly variable, displaying a rich range of semantic and syntactic behavior. Verb classifications help NLP systems to deal with this complexity by organizing verbs into groups that share core semantic and syntactic properties. For example, VerbNet (derived from Levin‟s [1993] work, Kipper et al., 2008) is widely used for a number of semantic processing tasks, including semantic role labeling (Swier and Stevenson, 2004), the creation of semantic parse trees (Shi and Mihalcea, 2005), and implicit argument resolution (Gerber and Chai, 2010). The detailed semantic predicates listed with each VerbNet class also have the potential to contribute to textspecific semantic representations and, thereby, to tasks requiring inferencing (Zaenen et al., 2008; Palmer et al., 2009). VerbNet identifies semantic roles and syntactic patterns characteristic of the verbs in each class makes explicit the connections between the syntactic patterns and the underlying semantic relations that can be inferred for all members of the class. Each syntactic frame in a class has a corresponding semantic representation that details the semantic relations betw"
W11-0910,J08-2001,1,0.827612,"bNet fails to accurately capture all usages of the construction. We use unsupervised methods to estimate probabilistic measures from corpus data for predicting usage of the construction across verb classes in the lexicon and evaluate against VerbNet. We discuss how these methods will form the basis for enhancements for VerbNet supporting more accurate analysis of the relational semantics of a verb across productive usages. 1 Introduction Automatic semantic analysis has been very successful when taking a supervised learning approach on data labeled with sense tags and semantic roles (e.g., see Màrquez et al., 2008). Underlying these recent successes are lexical resources, such as PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998; Fillmore et al., 2002), which encode the relational semantics of numerous lexical items, especially verbs. However, because authors and speakers use verbs productively in previously unseen ways, semantic analysis systems must not be limited to direct extrapolation from previously seen usages licensed by static lexical resources (cf. Pustejovsky & Jezek, 2008). To achieve more accurate semantic analyses, we must augment such resource"
W11-0910,J05-1004,1,0.546474,"Missing"
W11-0910,W10-0801,1,0.87385,"Missing"
W11-0910,E03-1073,0,0.0325187,"nt knowledge about verbs in a lexical resource. Importantly, constructional coercion is not an all-ornothing process – a word must be semantically and syntactically compatible in some respects with a context in order for its use to be extended to that context, but the restrictions on compatibility are not hard-and-fast rules (Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 2006; Goldberg, to appear). Gradience of compatibility plays an important role in coercion, suggesting that a probabilistic approach may be necessary for encoding knowledge of constructional coercion in a verb lexicon (cf. Lapata & Lascarides, 2003). Our hypothesis here is that, due to this gradient process of productivity, existing verb lexicons do not adequately capture the actual patterns of use of extensible constructions. In this paper, we focus on the CAUSEDMOTION (CM) construction as an initial test case. We first annotate the classes of an extensive verb lexicon, VerbNet, as to whether the CM construction is allowed for all, some, or none of the verbs in the class, noting additionally whether it is a typical or coerced usage. We find that many of the classes that allow the construction for at least some verbs do not include the C"
W11-0910,W04-3213,1,\N,Missing
W11-0910,zaenen-etal-2008-encoding,0,\N,Missing
W11-0910,C98-1046,1,\N,Missing
W11-0910,C98-1013,0,\N,Missing
W11-1003,D08-1092,0,0.0236327,"n semantic consistencies (Wu and Fung, 2009b) (Carpuat et al., 2010). While a comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic SRL, we can improve coverage (and possibly accuracy) of Chinese semantic class generation (Wu et al., 2010) by running the system on a large, unannotated parallel corpus. Using predicate-argument mappings as constraints, it may be possibly to improve SRL output by performing joint inference of SRL in source and target languages simultaneously, much like what Burkett and Klein (2008) was able to achieve with syntactic parsing. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent alignment across sentences. We begin"
W11-1003,D07-1007,0,0.0677155,"lish predicates, suggesting its potential as a valuable resource for improving word alignment and reranking MT output. 1 Introduction As the demand for semantically consistent machine translation rises (Wu and Fung, 2009a), the need for a comprehensive semantic mapping tool has become more apparent. With the current architecture of machine translation decoders, few ways of incorporating semantics in MT output include using 21 Martha Palmer Department of Linguistics Univerisity of Colorado at Boulder martha.palmer@colorado.edu word sense disambiguation to select the correct target translation (Carpuat and Wu, 2007) and reordering/reranking MT output based on semantic consistencies (Wu and Fung, 2009b) (Carpuat et al., 2010). While a comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic SRL, we can improve coverage (and possibly accuracy) of Chinese semantic class generation (Wu et al., 2010) by running the system on a large, unannotated parallel corpus. Using predicate-argument mappings as constraints, it may be possibly to improve SRL output by performing joint inference of SRL in source and targe"
W11-1003,P10-2033,0,0.0474455,"Missing"
W11-1003,W09-3020,1,0.840743,"differs from ours in that it only provided one-to-one mapping of numbered arguments and may not be able to detect predicate mapping with no lexical relations that are nevertheless semantically related. Later, Wu and Fung (2009b) used parallel semantic roles to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs based on the best predicate-argument mapping. The resulting system showed a 0.5 point BLEU score improvement even though the BLEU metric often discounts improvement in semantic consistency of MT output. Choi et al. (2009) (and later Wu et al. (2010)) showed how to enhance Chinese-English verb alignments by exploring predicate-argument structure alignment using parallel PropBanks. The resulting system showed improvement over pure GIZA++ alignment. Those two systems differs from ours in that they operated on gold standard parses and semantic roles. The systems also did not provide explicit argument mapping between the aligned predicate-argument structures. 3 Resources To perform automatic semantic mapping, we need an annotated corpus to evaluate the results. In addition, we also need a word aligner, a syntactic"
W11-1003,D09-1076,0,0.0143389,"ent or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic SRL, we can improve coverage (and possibly accuracy) of Chinese semantic class generation (Wu et al., 2010) by running the system on a large, unannotated parallel corpus. Using predicate-argument mappings as constraints, it may be possibly to improve SRL output by performing joint inference of SRL in source and target languages simultaneously, much like what Burkett and Klein (2008) was able to achieve with syntactic parsing. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent alignment across sentences. We begin by running GIZA++ (Och and Ney, 2003), one of the most popular alignment tools, to obtain automatic word alignments between pa"
W11-1003,2007.tmi-papers.10,0,0.421078,"arallel English/Czech corpus. The Czech corpus is first lemmatized because of the rich morphology, and then the word alignment is “symmetrized”. However, this approach does not explicitly make use of the predicate-argument structure to confirm the alignments or to suggest new ones. Pad´o and Lapata (2005; 2006) used word alignment and syntax based argument similarity to project English FrameNet semantic roles to German. The approach relied on annotated semantic roles on the source side only, precluding joint inferenece of the projection using reference or automatic target side semantic roles. Fung et al. (2007) demonstrated that there is poor semantic parallelism between Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of matching predicates with a bilingual lexicon, computing cosine-similarity (based on lexical translation) of arguments and tuning on an unannotated 22 parallel corpus. The system differs from ours in that it only provided one-to-one mapping of numbered arguments and may not be able to detect predicate mapping with no lexical relations that are nevertheless semantically related. Late"
W11-1003,N06-1015,0,0.0417665,"Missing"
W11-1003,2008.amta-papers.13,0,0.108742,"at, we pose predicate-argument mapping as a linear assignment problem (optimizing the total similarity of the mapping) and solve it with the Kuhn-Munkres method (Kuhn, 1955). With this approach, we were able to incur only a small predicate-argument F-score degradation over using manual PropBank annotation. The output also provides much more fine-grained argument mapping that can be used for downstream MT applications. 2 Related work Our basic approach to semantic mapping is similar to the idea of semantic similarity based on triangulation between parallel corpora outlined in Resnik (2004) and Madnani et al. (2008a; 2008b), but is implemented here quite differently. It is most similar in execution to the work of (Mareˇcek, 2009b), which improves word alignment by aligning tectogrammatical trees in a parallel English/Czech corpus. The Czech corpus is first lemmatized because of the rich morphology, and then the word alignment is “symmetrized”. However, this approach does not explicitly make use of the predicate-argument structure to confirm the alignments or to suggest new ones. Pad´o and Lapata (2005; 2006) used word alignment and syntax based argument similarity to project English FrameNet semantic ro"
W11-1003,J03-1002,0,0.00250423,"e with syntactic parsing. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent alignment across sentences. We begin by running GIZA++ (Och and Ney, 2003), one of the most popular alignment tools, to obtain automatic word alignments between parallel English/Chinese corpora. To achieve a broader coverage of semantic mappings than just those annoProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 21–30, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics tated in parallel PropBank-ed corpora, we attempt to map automatically generated predicate-argument structures. For each Chinese and English verb predicate pairs within a parallel sentence, we exam"
W11-1003,H05-1108,0,0.159915,"Missing"
W11-1003,P06-1146,0,0.135464,"Missing"
W11-1003,N07-1051,0,0.0121662,"the TreeBank and PropBank annotation of Ontonotes 4.0 (Hovy et al., 2006), which includes a wide array of data sources like broadcast news, news wire, magazine, web text, etc. A small percentage of the 2000 sentences were discarded because of tokenization differences. We dubbed the resulting 1939 parallel sentences as the triple-gold Xinhua corpus. 3.2 Word Alignment We chose GIZA++ (Och and Ney, 2003) as our word alignment tool primarily because of its popularity, though there are other alternatives like LacosteJulien et al. (2006). 3.3 Phrase Structure Parsing We chose the Berkeley Parser (Petrov and Klein, 2007) for phrase structure parsing since it has been tested on both English and Chinese corpora and can be easily retrained. 3.4 Semantic Role Labeling For semantic role labeling (SRL), we built our own system using a fairly standard approach: SRL is posed as a multi-class classification problem requiring the identification of argument candidates for each predicate and their argument types. Typically, argument identification and argument labeling are performed in two separate stages because of time/resource constraints during training/labeling. For our system, we chose LIBLINEAR (Fan et al., 2008),"
W11-1003,W05-0635,0,0.0669202,"Missing"
W11-1003,2009.eamt-1.30,0,0.416887,"monstrated that there is poor semantic parallelism between Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of matching predicates with a bilingual lexicon, computing cosine-similarity (based on lexical translation) of arguments and tuning on an unannotated 22 parallel corpus. The system differs from ours in that it only provided one-to-one mapping of numbered arguments and may not be able to detect predicate mapping with no lexical relations that are nevertheless semantically related. Later, Wu and Fung (2009b) used parallel semantic roles to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs based on the best predicate-argument mapping. The resulting system showed a 0.5 point BLEU score improvement even though the BLEU metric often discounts improvement in semantic consistency of MT output. Choi et al. (2009) (and later Wu et al. (2010)) showed how to enhance Chinese-English verb alignments by exploring predicate-argument structure alignment using parallel PropBanks. The resulting system showed improvement over p"
W11-1003,N09-2004,0,0.515233,"monstrated that there is poor semantic parallelism between Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) consists of matching predicates with a bilingual lexicon, computing cosine-similarity (based on lexical translation) of arguments and tuning on an unannotated 22 parallel corpus. The system differs from ours in that it only provided one-to-one mapping of numbered arguments and may not be able to detect predicate mapping with no lexical relations that are nevertheless semantically related. Later, Wu and Fung (2009b) used parallel semantic roles to improve MT system outputs. Given the outputs from Moses (Koehn et al., 2007), a machine translation decoder, they reordered the outputs based on the best predicate-argument mapping. The resulting system showed a 0.5 point BLEU score improvement even though the BLEU metric often discounts improvement in semantic consistency of MT output. Choi et al. (2009) (and later Wu et al. (2010)) showed how to enhance Chinese-English verb alignments by exploring predicate-argument structure alignment using parallel PropBanks. The resulting system showed improvement over p"
W11-1003,2010.amta-papers.15,1,0.914739,"ntics in MT output include using 21 Martha Palmer Department of Linguistics Univerisity of Colorado at Boulder martha.palmer@colorado.edu word sense disambiguation to select the correct target translation (Carpuat and Wu, 2007) and reordering/reranking MT output based on semantic consistencies (Wu and Fung, 2009b) (Carpuat et al., 2010). While a comprehensive semantic mapping tool can supplement or improve the results of such techniques, there are many other exciting ideas we can explore: with automatic SRL, we can improve coverage (and possibly accuracy) of Chinese semantic class generation (Wu et al., 2010) by running the system on a large, unannotated parallel corpus. Using predicate-argument mappings as constraints, it may be possibly to improve SRL output by performing joint inference of SRL in source and target languages simultaneously, much like what Burkett and Klein (2008) was able to achieve with syntactic parsing. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments"
W11-1003,W04-3212,1,0.815456,"Missing"
W11-1003,J08-2004,0,0.147961,"Missing"
W11-1003,C10-2158,0,0.0653896,"Missing"
W11-1003,2007.mtsummit-papers.71,0,0.0316629,"a large, unannotated parallel corpus. Using predicate-argument mappings as constraints, it may be possibly to improve SRL output by performing joint inference of SRL in source and target languages simultaneously, much like what Burkett and Klein (2008) was able to achieve with syntactic parsing. As the foundation of many machine translation decoders (DeNeefe and Knight, 2009), word alignment has continuously played an important role in machine translation. There have been several attempts to improve word alignment, most of which have focused on tree-to-tree alignments of syntactic structures (Zhang et al., 2007; Mareˇcek, 2009a). Our hypothesis is that the predicate-argument structure alignments can abstract away from language specific syntactic variation and provide a more robust, semantically coherent alignment across sentences. We begin by running GIZA++ (Och and Ney, 2003), one of the most popular alignment tools, to obtain automatic word alignments between parallel English/Chinese corpora. To achieve a broader coverage of semantic mappings than just those annoProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 21–30, c ACL HLT 2011, Portlan"
W11-1003,N06-2015,1,\N,Missing
W11-1003,P07-2045,0,\N,Missing
W11-1901,W06-0609,1,0.725549,"he Switchboard Treebank. Given the frequency of disfluencies and the performance with which one can identify them automatically,8 a probable processing pipeline would filter them out before parsing. Since we did not have a readily available tagger for tagging disfluencies, we decided to remove them using oracle information available in the Treebank. Propositions The propositions in OntoNotes constitute PropBank semantic roles. Most of the verb predicates in the corpus have been annotated with their arguments. Recent enhancements to the PropBank to make it synchronize better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify"
W11-1901,P06-1005,0,0.929236,"ers including the parses, semantic roles, word senses, and named entities. As is customary for CoNLL tasks, there were two tracks, closed and open. For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data. 4.2.1 Closed Track In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006). For the training and test data, in addition to the underlying text, predicted versions of all the supplementary layers of annotation were provided, where those predictions were derived using off-the-shelf tools (parsers, semantic role labelers, named entity taggers, etc.) as described in Section 4.4.2. For the training data, however, in addition to predicted values for the other layers, we also provided manual gold-standard annotations for all the layers. Participants were allowed to use either the gold-standard or predicted annotation for training their systems. They were also free to use t"
W11-1901,W10-4305,0,0.156408,"Missing"
W11-1901,N01-1016,0,0.0275817,"ze better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. 9 doc/propbank/english-propbank.pdf lease. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the Release 1.0 of the Proposition Bank. This functionality is part of the OntoNotes DB Tool.10 Word Sense Gold word sense annotation was supplied using sense numbers as specified in the O"
W11-1901,P05-1022,0,0.0274644,"coreference but that have been annotated for other layers. For training 10 11 http://cemantix.org/ontonotes.html It should be noted that word sense annotation in OntoNotes is note complete, so only some of the verbs and nouns have word sense tags specified. 10 Senses Lemmas 1 2 &gt;2 1,506 1,046 1,016 Table 6: Word sense polysemy over verb and noun lemmas in OntoNotes models for each of the layers, where feasible, we used all the data that we could for that layer from the training portion of the entire OntoNotes release. Parse Trees Predicted parse trees were produced using the Charniak parser (Charniak and Johnson, 2005).12 Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were appropriately extended. The parser was then re-trained on the training portion of the release 4.0 data using 10-fold crossvalidation. Table 5 shows the performance of the re-trained Charniak parser on the CoNLL-2011 test set. We did not get a chance to re-train the re-ranker, and since the stock re-ranker crashes when run on nbest parses containing NMLs, because it has no"
W11-1901,N07-1011,0,0.525918,"fined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In"
W11-1901,N07-1030,0,0.0811053,"to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress,"
W11-1901,N10-1061,0,0.393386,"y work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surfa"
W11-1901,N01-1008,0,0.21035,"Missing"
W11-1901,N06-2015,1,0.528696,"2009) devoted to joint learning of syntactic and semantic dependencies. A principle ingredient for joint learning is the presence of multiple layers of semantic information. One fundamental question still remains, and that is – what would it take to improve the state of the art in coreference resolution that has not been attempted so far? Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) is to explore whether it can fill this void and help push the progress further – not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that cov1 2 http://projects.ldc.upenn.edu/ace/data/ http://www.bbn.com/nlp/ontonotes 2 ers entities and events not limited to noun phrases or a limited set of entity types. A small portion of this corpus from the newswire and broadcast news genres (∼120k) was recently used for a S EM E VAL task (Recasens et al., 2010)."
W11-1901,H05-1004,0,0.943059,"Missing"
W11-1901,J93-2004,1,0.0615149,"Missing"
W11-1901,P00-1023,0,0.0803844,"partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collabor"
W11-1901,P10-1142,0,0.220993,"and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC). These corpora were tagged with coreferring entities identified by noun phrases in the text. The de facto standard datasets for current coreference studies are the MUC (Hirschman and Chin1 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1–27, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics chor, 1997; Chinchor, 2001; Chinchor and Sun"
W11-1901,J05-1004,1,0.333011,"Missing"
W11-1901,passonneau-2004-computing,0,0.00517084,"ing and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving"
W11-1901,W05-0311,0,0.0114861,", but represent small training and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step"
W11-1901,P09-5006,0,0.0282076,"Missing"
W11-1901,N06-1025,0,0.730558,"d-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gende"
W11-1901,D09-1101,0,0.522965,"annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques st"
W11-1901,W09-2411,0,0.155304,"Missing"
W11-1901,J01-4004,0,0.993465,"stering them into equivalence classes, has been well recognized in the natural language processing community. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of"
W11-1901,P09-1074,0,0.237943,"ry of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention. There are many different parameters involved in defining a coreference task. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is somewhat easier (Culotta et al., 2007). Given the space constraints, we refer the reader to Stoyanov et al. (2009) for a detailed treatment of the issue. Limitations in the size and scope of the available datasets have also constrained research progress. The MUC and ACE corpora are the two that have been used most for reporting comparative results, but they differ in the types of entities and coreference annotated. The ACE corpus is also one that evolved over a period of almost five years, with different incarnations of the task definition and different corpus cross-sections on which performance numbers have been reported, making it hard to untangle and interpret the results. The availability of the OntoN"
W11-1901,D07-1052,0,0.0160767,"ic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entiti"
W11-1901,M95-1005,0,0.967872,"2011 coreference task are likely to be lower than for coref evaluations based on MUC, where the mention spans are specified in the input,17 or those based on ACE data, where an approximate match is often allowed based on the specified head of the NP mention. 4.5.1 Metrics As noted above, the choice of an evaluation metric for coreference has been a tricky issue and there does not appear to be any silver bullet approach that addresses all the concerns. Three metrics have been proposed for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al., 1995), ii) The mention based B - CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005). Very recently BLANC (BiLateral Assessment of NounPhrase Coreference) measure (Recasens and Hovy, 17 2011) has been proposed as well. Each of the metric tries to address the shortcomings or biases of the earlier metrics. Given a set of key entities K, and a set of response entities R, with each entity comprising one or more mentions, each metric generates its variation of a precision and recall measure. The MUC measure if the oldest and mos"
W11-1901,W08-2121,0,\N,Missing
W11-1901,E06-2015,0,\N,Missing
W11-1901,D08-1067,0,\N,Missing
W11-1901,S10-1001,0,\N,Missing
W11-1901,doddington-etal-2004-automatic,1,\N,Missing
W11-1901,W04-2327,0,\N,Missing
W11-3801,P11-2121,1,0.853282,"형태소 분석기: http://www.sejong.or.kr/ 6 The reason we use outputs from two different systems is to compare the impact of fine vs. coarsegrained morphologies on dependency parsing in Korean. I MA gives not only richer POS tags but also more fine-grained (segmented) morphemes than Mach. We hypothesize that a richer morphology does not necessarily provide better features for dependency parsing. We evaluate our hypothesis by comparing parsing models trained on morphemes and POS tags generated by these two systems. 5 Dependency parsing 5.1 Parsing algorithm To build statistical parsing models, we use Choi and Palmer (2011)’s transition-based dependency parsing approach, which has shown state-of-the-art performance in English and Czech. The key idea of this approach is to combine transitions from projective and non-projective dependency parsing algorithms so it can perform projective and non-projective parsing accordingly. As a result, it shows an expected linear time parsing speed for generating both projective and non-projective dependency trees. Our algorithm uses three lists: λ1 , λ2 , and β. λ1,2 contain tokens that have been processed and β contains tokens that have not been processed by the algorithm. For"
W11-3801,W10-1406,0,0.105991,"nd their heads from the rightmost children, which aligns with the general concept of Korean being a head-final language. Note that these headrules do not involve the POS tags in Table 1; those POS tags are used only for morphemes within tokens (and each token is annotated with a phrase-level tag). It is possible to extend the headrules to token-level and find the head morpheme of each token; however, finding dependencies between different morphemes within a token is not especially interesting although 4 there are some approaches that have treated each morpheme as an individual token to parse (Chung et al., 2010).5 S Q NP VP VNP AP DP IP X|L|R r l r r r r r r r VP;VNP;S;NP|AP;Q;* S|VP|VNP|NP;Q;* NP;S;VP;VNP;AP;* VP;VNP;NP;S;IP;* VNP;NP;S;* AP;VP;NP;S;* DP;VP;* IP;VNP;* * Table 3: Head-percolation rules for the Sejong Treebank. l/r implies looking for the leftmost/rightmost constituent. * implies any phrase-level tag. |implies a logical OR and ; is a delimiter between tags. Each rule gives higher precedence to the left (e.g., S takes the highest precedence in VP). Once we have the headrules, it is pretty easy to generate dependency trees from constituent trees. For each phrase (or clause) in a constitu"
W11-3801,han-etal-2000-handling,1,0.661818,"ded chunking for parsing and conditional random fields for learning. Our work is distinguished from theirs in mainly two ways. First, we add labels to dependency edges during the conversion, so parsing performance can be evaluated on both labels and edges. Second, we selectively choose morphemes useful for dependency parsing, which prevents generating very sparse features. The morpheme selection is done automatically by applying our linguistically motivated rules (cf. Section 5.3). 3 The system is not publicly available but can be requested from the Sejong project (http://www.sejong.or.kr). 3 Han et al. (2000) presented an approach for handling structural divergence and recovering dropped arguments in a Korean-to-English machine translation system. In their approach, they used a Korean dependency parser for lexico-structural processing. 3 Constituent-to-dependency conversion 3.1 Constituent trees in the Sejong Treebank The Sejong Treebank contains constituent trees similar to ones in the Penn Treebank.4 Figure 2 shows a constituent tree and morphological analysis for a sentence, She still loved him, in Korean. S NP-SBJ VP AP 그녀는 She 여전히 still VP NP-OBJ VP 그를 him 사랑했다 loved 그녀는 → 그녀(she)/NP+는/JX 여전히"
W11-3801,W07-2416,0,0.211438,"However, there is a Treebank, called the Sejong Treebank1 , containing a large number of constituent trees in Korean (about 60K sentences), 1 http://www.sejong.or.kr/eindex.php 1 Proceedings of the 2nd Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 1–11, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical features for dependency parsing under an assumption that each token consists of a single POS tag. This assumption is only partially true in Korean; a token can consist of a sequence of morph"
W11-3801,N10-1095,0,0.0500215,"Missing"
W11-3801,J93-2004,0,0.037206,"ependency parsing. To perform statistical dependency parsing, we need sufficiently large training data. There is not much training data available for dependency structure in Korean. However, there is a Treebank, called the Sejong Treebank1 , containing a large number of constituent trees in Korean (about 60K sentences), 1 http://www.sejong.or.kr/eindex.php 1 Proceedings of the 2nd Workshop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2011), pages 1–11, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical fe"
W11-3801,de-marneffe-etal-2006-generating,0,0.0911649,"Missing"
W11-3801,P05-1012,0,0.031882,"MRL 2011), pages 1–11, c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical features for dependency parsing under an assumption that each token consists of a single POS tag. This assumption is only partially true in Korean; a token can consist of a sequence of morphemes with different POS tags.2 말한다 talk (verb) ➔ 말/NNG talk (noun) 한/XSV 다/EF do ending_marker Figure 1: Morphological analysis of a verb talk in Korean. P OS tags are described in Table 1. grained morphology does not necessarily mean a better morphology for parsing"
W11-3801,P08-1108,0,0.0644132,"Missing"
W11-3801,P05-1013,0,0.0469165,"010)). A dependency tree generated by this procedure is guaranteed to be well-formed (unique root, single head, connected, and acyclic); however, it does not include labels yet. Section 3.3 shows how to add dependency labels to these trees. In addition, Section 3.4 describes heuristics to resolve some of the special cases (e.g., coordinations, nested function tags). It is worth mentioning that constituent trees in the Sejong Treebank do not include any empty categories. This implies that dependency trees generated by these headrules consist of only projective dependencies (non-crossing edges; Nivre and Nilsson (2005)). On the other hand, the Penn Korean Treebank contains empty categories representing longdistance dependencies. It will be interesting to see if we can train empty category insertion and resolution models on the Penn Korean Treebank, run the mod5 Chung et al. (2010) also showed that recovering certain kinds of null elements improves PCFG parsing, which can be applied to dependency parsing as well. els on the Sejong Treebank, and use the automatically inserted and linked empty categories to generate non-projective dependencies. 3.3 Dependency labels Two types of dependency labels are derived f"
W11-3801,J08-4003,0,0.019912,"c Dublin, Ireland, October 6, 2011. 2011 Association for Computational Linguistics formated similarly to the Penn Treebank (Marcus et al., 1993). The Penn Treebank style constituent trees have been reliably converted to dependency trees using head-percolation rules and heuristics (Marneffe et al., 2006; Johansson and Nugues, 2007). By applying a similar conversion strategy to the Sejong Treebank, we can achieve a large set of training data for Korean dependency parsing. Once we generate the dependency Treebank, any statistical dependency parsing approach can be applied (McDonald et al., 2005; Nivre, 2008). The challenging part is how to extract features from tokens consisting of multiple morphemes. For example, POS tags are typical features for dependency parsing under an assumption that each token consists of a single POS tag. This assumption is only partially true in Korean; a token can consist of a sequence of morphemes with different POS tags.2 말한다 talk (verb) ➔ 말/NNG talk (noun) 한/XSV 다/EF do ending_marker Figure 1: Morphological analysis of a verb talk in Korean. P OS tags are described in Table 1. grained morphology does not necessarily mean a better morphology for parsing. For instance"
W11-3801,C02-1092,0,0.0791834,"dividual morphemes where each morpheme has its own POS tag. It is not clear which combination of these morphemes yields the best representation of the token for dependency parsing. Moreover, deriving joined features from multiple tokens (e.g., a joined feature of POS tags between two tokens) can be problematic; considering all combinations of morphemes within multiple tokens can be cumbersome and generate very sparse features. Obviously, having a good morphological analysis is very important for parsing. There are many automatic morphological analyzers available in Korean (Kang and Woo, 2001; Shim and Yang, 2002). Some of them use different kinds of morphologies better suited for their purposes. It is useful to have a fine-grained morphology; however, a more fine2 English words can consist of multiple morphemes as well (e.g., buying → buy/verb + ing/progressive suffix), but such morphology is usually not used in parsing. 2 Related work Marneffe et al. (2006) introduced a system for extracting typed dependencies from the Penn Treebank style constituent trees, known as the Stanford dependencies. Johansson and Nugues (2007) presented the LTH constituent-to-dependency converter that had been used to prepa"
W11-3801,W08-2121,0,0.0300793,"Missing"
W11-3801,W09-1201,0,\N,Missing
W12-2001,W11-1407,0,0.0287846,"approaches are typically optimized to maximize learning gains, and are not necessarily focused on replicating human tutor behavior. Other work has explored specific factors in questioning such as when to ask “why” questions (Rose et al., 2003), provide hints (Tsovaltzi and Matheson, 2001), or insert discourse markers (Kim et al., 2000). There is also an expanding body of work that applies ranking algorithms toward the task of question generation (QG) using approaches such as overgeneration-and-ranking (Heilman and Smith, 2010), language model ranking (Yao, 2010), and heuristicsbased ranking (Agarwal and Mannem, 2011). While the focus of these efforts centers on issues of grammaticality, fluency, and content selection for automatic creation of standalone questions, we move to the higher level task of choosing context appropriate questions. Our work merges aspects of these QG approaches with the sentence planning tradition from natural language generation (Walker et al., 2001; Rambow et al., 2001). In sentence planning the goal is to select lexico-structural resources that encode communicative action. Rather than selecting representations, we use them directly as part of the feature space for learning funct"
W12-2001,W11-0133,1,0.872597,"Missing"
W12-2001,C08-1010,0,0.0136052,"Unifying Speech and Semantics (DISCUSS) (Becker et al., 1 http://www.fossweb.com 2011), a multidimensional dialogue move taxonomy that captures both the pragmatic and semantic interpretation of an utterance. Instead of using one label, a DISCUSS move is a tuple composed of three dimensions: Dialogue Act, Rhetorical Form, Predicate Type. Together these labels account for the action, function, and content of an utterance. This scheme draws from past work in task-oriented dialogue acts (Bunt, 2009; Core and Allen, 1997), tutorial act taxonomies (Pilkington, 1999; Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008; Boyer et al., 2009b) discourse relations (Mann and Thompson, 1986) and question taxonomies (Graesser and Person, 1994; Nielsen et al., 2008). Dialogue Act (22 tags): The dialogue act dimension is the top-level dimension in DISCUSS, and its values govern the possible values for the other dimensions. Though the DISCUSS dialogue act layer seeks to replicate the learnings from other wellestablished taxonomies like DIT++ (Bunt, 2009) or DAMSL (Core and Allen, 1997) wherever possible, the QtA style of pedagogy driving our tutoring sessions dictated the addition of two tutorial specific acts: marki"
W12-2001,J96-2004,0,0.130082,"Missing"
W12-2001,N10-1086,0,0.0263743,", 2010; Chi et al., 2009; Chi et al., 2008) to discover tutoring strategies. However, these approaches are typically optimized to maximize learning gains, and are not necessarily focused on replicating human tutor behavior. Other work has explored specific factors in questioning such as when to ask “why” questions (Rose et al., 2003), provide hints (Tsovaltzi and Matheson, 2001), or insert discourse markers (Kim et al., 2000). There is also an expanding body of work that applies ranking algorithms toward the task of question generation (QG) using approaches such as overgeneration-and-ranking (Heilman and Smith, 2010), language model ranking (Yao, 2010), and heuristicsbased ranking (Agarwal and Mannem, 2011). While the focus of these efforts centers on issues of grammaticality, fluency, and content selection for automatic creation of standalone questions, we move to the higher level task of choosing context appropriate questions. Our work merges aspects of these QG approaches with the sentence planning tradition from natural language generation (Walker et al., 2001; Rambow et al., 2001). In sentence planning the goal is to select lexico-structural resources that encode communicative action. Rather than sel"
W12-2001,N04-3002,0,0.0418435,"iting information from the learner to help them build their own connections to the material. The role of a tutor in a Socratic dialogue is to scaffold the material and present questions that ultimately lead the student to an “A-ha!” moment. Numerous studies have illustrated the effectiveness of Socratic-style tutoring (VanLehn et al., 2007; Rose et al., 2001; Collins and Stevens, 1982); consequently recreating the behavior on a computer has long been a goal of research in Intelligent Tutoring Systems (ITS). Recent successes have shown the efficacy of conversational ITS (Graesser et al., 2005; Litman and Silliman, 2004; Ward et al., 2011b), however these systems are still not as effective as human tutors, and much improvement is needed before they can truly claim to be Socratic. Furthermore, development and tuning of tutorial dialogue behavior requires significant human effort. While our overarching goal is to improve ITS by automatically learning tutorial dialogue strategies directly from expert tutor behavior, we focus on the crucial subtask of selecting follow-up questions. Although asking questions is only a subset of the overall tutoring process, it is still a complex process that requires understandin"
W12-2001,P01-1056,0,0.0344799,"s ranking algorithms toward the task of question generation (QG) using approaches such as overgeneration-and-ranking (Heilman and Smith, 2010), language model ranking (Yao, 2010), and heuristicsbased ranking (Agarwal and Mannem, 2011). While the focus of these efforts centers on issues of grammaticality, fluency, and content selection for automatic creation of standalone questions, we move to the higher level task of choosing context appropriate questions. Our work merges aspects of these QG approaches with the sentence planning tradition from natural language generation (Walker et al., 2001; Rambow et al., 2001). In sentence planning the goal is to select lexico-structural resources that encode communicative action. Rather than selecting representations, we use them directly as part of the feature space for learning functions to rank the questions’ actual surface form realization. To our knowledge there has been no research in ranking the quality and suitability of questions within a tutorial dialogue context. Because questioning tactics depend heavily on the curriculum and choice of pedagogy, we ground our investigations within the context of the My Science Tutor (MyST) intelligent tutoring system ("
W12-2001,W04-2307,0,0.0263435,"need we use the Dialogue Schema Unifying Speech and Semantics (DISCUSS) (Becker et al., 1 http://www.fossweb.com 2011), a multidimensional dialogue move taxonomy that captures both the pragmatic and semantic interpretation of an utterance. Instead of using one label, a DISCUSS move is a tuple composed of three dimensions: Dialogue Act, Rhetorical Form, Predicate Type. Together these labels account for the action, function, and content of an utterance. This scheme draws from past work in task-oriented dialogue acts (Bunt, 2009; Core and Allen, 1997), tutorial act taxonomies (Pilkington, 1999; Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008; Boyer et al., 2009b) discourse relations (Mann and Thompson, 1986) and question taxonomies (Graesser and Person, 1994; Nielsen et al., 2008). Dialogue Act (22 tags): The dialogue act dimension is the top-level dimension in DISCUSS, and its values govern the possible values for the other dimensions. Though the DISCUSS dialogue act layer seeks to replicate the learnings from other wellestablished taxonomies like DIT++ (Bunt, 2009) or DAMSL (Core and Allen, 1997) wherever possible, the QtA style of pedagogy driving our tutoring sessions dictated the addition of two tut"
W12-2001,N01-1003,0,0.041653,"y of work that applies ranking algorithms toward the task of question generation (QG) using approaches such as overgeneration-and-ranking (Heilman and Smith, 2010), language model ranking (Yao, 2010), and heuristicsbased ranking (Agarwal and Mannem, 2011). While the focus of these efforts centers on issues of grammaticality, fluency, and content selection for automatic creation of standalone questions, we move to the higher level task of choosing context appropriate questions. Our work merges aspects of these QG approaches with the sentence planning tradition from natural language generation (Walker et al., 2001; Rambow et al., 2001). In sentence planning the goal is to select lexico-structural resources that encode communicative action. Rather than selecting representations, we use them directly as part of the feature space for learning functions to rank the questions’ actual surface form realization. To our knowledge there has been no research in ranking the quality and suitability of questions within a tutorial dialogue context. Because questioning tactics depend heavily on the curriculum and choice of pedagogy, we ground our investigations within the context of the My Science Tutor (MyST) intelli"
W12-2001,W11-1417,0,0.0773233,"learner to help them build their own connections to the material. The role of a tutor in a Socratic dialogue is to scaffold the material and present questions that ultimately lead the student to an “A-ha!” moment. Numerous studies have illustrated the effectiveness of Socratic-style tutoring (VanLehn et al., 2007; Rose et al., 2001; Collins and Stevens, 1982); consequently recreating the behavior on a computer has long been a goal of research in Intelligent Tutoring Systems (ITS). Recent successes have shown the efficacy of conversational ITS (Graesser et al., 2005; Litman and Silliman, 2004; Ward et al., 2011b), however these systems are still not as effective as human tutors, and much improvement is needed before they can truly claim to be Socratic. Furthermore, development and tuning of tutorial dialogue behavior requires significant human effort. While our overarching goal is to improve ITS by automatically learning tutorial dialogue strategies directly from expert tutor behavior, we focus on the crucial subtask of selecting follow-up questions. Although asking questions is only a subset of the overall tutoring process, it is still a complex process that requires understanding of the dialogue s"
W13-1018,ahmed-etal-2012-reference,0,0.160719,"Missing"
W13-1018,I08-2099,0,0.0267423,"d a corresponding list of light verbs that occur with them. In Section 2, we showed that the noun as well as the light verb in a sentence influence the type of semantic roles that will occur. Our method builds on this idea and uses two resources in order to derive linguistic knowledge about the NVC: PropBank frame files for simple verbs in Hindi and the Hindi Treebank, annotated with dependency labels. The next two sections describe the use of these resources in some detail. http://verbs.colorado.edu/propbank/framesets-noun/ 128 The annotated Hindi Treebank is based on a dependency framework (Begum et al., 2008) and has a very rich set of dependency labels. These labels (also known as karaka labels) represent the relations between a head (e.g. a verb) and its dependents (e.g. arguments). Using the Treebank we extract all the dependency karaka label combinations that occur with a unique instance of an NVC. We filter them to include argument labels and discard those labels that are usually used for adjuncts. We then calculate the most frequently occurring combination of labels that will occur with that NVC. Finally, we get a tuple consisting of an NVC, a set of karaka argument labels that occur with it"
W13-1018,bhatia-etal-2010-empty,1,0.930597,"Missing"
W13-1018,W10-1810,1,0.888798,"Missing"
W13-1018,J05-1004,1,0.494314,"verb karnaa giving us the meaning ‘steal’. Complex predicates 1 may be found in English e.g. take a walk and many other languages such as Japanese, Persian, Arabic and Chinese (Butt, 1993; Fazly and Stevenson, 2007). 1 They are also otherwise known as light verb, support verb or conjunct verb constructions. Background The goal of this paper is to produce a lexical resource for Hindi NVCs. This resource is in the form of ‘frame files’, which are directly utilized for PropBank annotation. PropBank is an annotated corpus of semantic roles that has been developed for English, Arabic and Chinese (Palmer et al., 2005; Palmer et al., 2008; Xue and Palmer, 2003). In Hindi, the task of PropBank annotation is part of a larger effort to create a multi-layered treebank for Hindi as well as Urdu (Palmer et al., 2009). PropBank annotation assumes that syntactic parses are already available for a given corpus. Therefore, Hindi PropBanking is carried out on top of the syntactically annotated Hindi Dependency Treebank. As the name suggests, the syntactic representation is dependency based, which has several advantages for the PropBank annotation process (see Section 3). The PropBank annotation process for Hindi foll"
W13-1018,palmer-etal-2008-pilot,1,0.867283,"s the meaning ‘steal’. Complex predicates 1 may be found in English e.g. take a walk and many other languages such as Japanese, Persian, Arabic and Chinese (Butt, 1993; Fazly and Stevenson, 2007). 1 They are also otherwise known as light verb, support verb or conjunct verb constructions. Background The goal of this paper is to produce a lexical resource for Hindi NVCs. This resource is in the form of ‘frame files’, which are directly utilized for PropBank annotation. PropBank is an annotated corpus of semantic roles that has been developed for English, Arabic and Chinese (Palmer et al., 2005; Palmer et al., 2008; Xue and Palmer, 2003). In Hindi, the task of PropBank annotation is part of a larger effort to create a multi-layered treebank for Hindi as well as Urdu (Palmer et al., 2009). PropBank annotation assumes that syntactic parses are already available for a given corpus. Therefore, Hindi PropBanking is carried out on top of the syntactically annotated Hindi Dependency Treebank. As the name suggests, the syntactic representation is dependency based, which has several advantages for the PropBank annotation process (see Section 3). The PropBank annotation process for Hindi follows the same two-step"
W13-1018,J06-2001,0,0.0274054,"Missing"
W13-1018,W11-0403,1,0.860076,"all the dependency karaka label combinations that occur with a unique instance of an NVC. We filter them to include argument labels and discard those labels that are usually used for adjuncts. We then calculate the most frequently occurring combination of labels that will occur with that NVC. Finally, we get a tuple consisting of an NVC, a set of karaka argument labels that occur with it and a count of the number of times that NVC has occurred in the corpus. The karaka labels are then mapped onto PropBank labels. We reproduce in Table 3 the numbered arguments to karaka label mapping found in Vaidya et al., (2011). PropBank label Arg0 (agent) Arg1 (theme, patient) Arg2 (beneficiary) Arg2-ATR(attribute) Arg2-SOU(source) Arg2-GOL(goal) Arg3 (instrument) Treebank label k1 (karta); k4a (experiencer) k2 (karma) k4 (beneficiary) k1s (attribute) k5 (source) k2p (goal) k3 (instrument) Table 3: Mapping from Karaka labels to PropBank 3.2 Verb Frames Our second resource consists of PropBank frames for full Hindi verbs. Every light verb that occurs in Hindi is also used as a full verb, e.g. de ‘give’ in Table 1 may be used both as a ‘full’ verb as well as a ‘light’ verb. As a full verb, it has a frame file in Hind"
W13-1018,vaidya-etal-2012-empty,1,0.787898,"cedure integrates the two resources described above. First, the tuple consisting of karaka labels for a particular NVC is mapped to PropBank labels. But many NVC cases occur just once in the corpus and the karaka label tuple may not be very reliable. Hence, the likelihood that the mapped tuple accurately depicts the correct semantic frame is not very high. Secondly, Hindi can drop mandatory subjects or objects in a sentence e.g., (vo) kitaab paRegaa; ‘(He) will read the book’. These are not inserted by the dependency annotation (Bhatia et al., 2010) and are not easy to discover automatically (Vaidya et al., 2012). We cannot afford to ignore any of the low frequency cases as each NVC in the corpus must be annotated with semantic roles. In order to get reasonable predictions for each NVC, we use a simple rule. We carry out a mapping from karaka to PropBank labels only if the NVC occurs at least 30 times in the corpus. If the NVC occurs fewer than 30 times, then we use the “canonical” verb list. 4 Evaluation The automatic method described in the previous section generated 1942 nominal frame files. In order to evaluate the frame files, we opted for manual checking of the automatically generated frames. Th"
W13-1018,W03-1707,1,0.546334,". Complex predicates 1 may be found in English e.g. take a walk and many other languages such as Japanese, Persian, Arabic and Chinese (Butt, 1993; Fazly and Stevenson, 2007). 1 They are also otherwise known as light verb, support verb or conjunct verb constructions. Background The goal of this paper is to produce a lexical resource for Hindi NVCs. This resource is in the form of ‘frame files’, which are directly utilized for PropBank annotation. PropBank is an annotated corpus of semantic roles that has been developed for English, Arabic and Chinese (Palmer et al., 2005; Palmer et al., 2008; Xue and Palmer, 2003). In Hindi, the task of PropBank annotation is part of a larger effort to create a multi-layered treebank for Hindi as well as Urdu (Palmer et al., 2009). PropBank annotation assumes that syntactic parses are already available for a given corpus. Therefore, Hindi PropBanking is carried out on top of the syntactically annotated Hindi Dependency Treebank. As the name suggests, the syntactic representation is dependency based, which has several advantages for the PropBank annotation process (see Section 3). The PropBank annotation process for Hindi follows the same two-step process used for other"
W13-1018,J01-3003,0,\N,Missing
W13-1018,Y05-1003,0,\N,Missing
W13-2322,P13-1091,1,0.350566,"uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. (2012). Disjunctive AMR. AMR aims to canonicalize mul"
W13-2322,N12-1017,0,0.0148132,"Missing"
W13-2322,kingsbury-palmer-2002-treebank,1,0.410787,"ive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are wri"
W13-2322,martins-2012-le,0,0.0102149,"re able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tas"
W13-2322,W04-2705,0,0.0181197,"Missing"
W13-2322,W13-0101,0,0.0287813,"nces from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future"
W13-2322,P98-1013,0,0.597537,"Missing"
W13-2322,basile-etal-2012-developing,0,0.035603,". AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking tools (e.g., Basile et al. (2012b)), annotation guidelines,5 and sembanks that cover a wide range of genres, from news to fiction. UNL and AMR have both annotated many of the same sentences, providing the potential for direct comparison. We currently have a manually-constructed AMR bank of several thousand sentences, a subset of which can be freely downloaded,4 the rest being distributed via the LDC catalog. In initially developing AMR, the authors built consensus AMRs for: • 225 short sentences for tutorial purposes • 142 sentences of newswire (*) • 100 sentences of web data (*) Trained annotators at LDC then produced AMRs"
W13-2322,J05-1004,1,0.185231,"design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings"
W13-2322,P02-1040,0,0.108678,"ted script.3 Smatch reports the semantic overlap between two AMRs by viewing each AMR as a conjunction of logical triples (see Figure 1). Smatch computes precision, recall, and F-score of one AMR’s triples against the other’s. To match up variables from two input AMRs, smatch needs to execute a brief search, looking for the variable mapping that yields the highest F-score. Smatch makes no reference to English strings or word indices, as we do not enforce any particular string-to-meaning derivation. Instead, we compare semantic representations directly, in the same way that the MT metric Bleu (Papineni et al., 2002) compares target strings without making reference to the source. For an initial IAA study, and prior to adjusting the AMR Editor to encourage consistency, 4 expert AMR annotators annotated 100 newswire sentences and 80 web text sentences. They then created consensus AMRs through discussion. The average annotator vs. consensus IAA (smatch) was 0.83 for newswire and 0.79 for web text. When newly trained annotators doubly annotated 382 web text sentences, their annotator vs. annotator IAA was 0.71. (m / marble :location (j / jar)) the marble in the jar ... (b / be-located-at-91 :arg1 (m / marble)"
W13-2322,W12-6207,1,0.412113,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W12-4209,1,0.36463,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W13-0215,0,0.00777424,"ry))”, because “profess-01” 2 3 183 AMR Editor: amr.isi.edu/editor.html Smatch: amr.isi.edu/evaluation.html 6 Current AMR Bank order logic. GMB and ST both include universal quantification. Granularity. GMB and UCCA annotate short texts, so that the same entity can participate in events described in different sentences; other systems annotate individual sentences. Entities. AMR uses 80 entity types, while GMB uses 7. Manual versus automatic. AMR, UNL, and UCCA annotation is fully manual. GMB and ST produce meaning representations automatically, and these can be corrected by experts or crowds (Venhuizen et al., 2013). Derivations. AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking"
W13-2322,N06-1056,0,0.0133733,"AMRs for: • 1546 sentences from the novel “The Little Prince” • 1328 sentences of web data • 1110 sentences of web data (*) • 926 sentences from Xinhua news (*) • 214 sentences from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific Pr"
W13-2322,E12-2019,0,\N,Missing
W13-2322,C12-1083,1,\N,Missing
W13-2322,C98-1013,0,\N,Missing
W13-5407,W11-0910,1,0.918971,"oth humans and computers to detect and delimit. Such semi-productive constructions are generally very problematic for lexical resources such as VN, but also FrameNet and WordNet (Fellbaum, 1998), because all of these resources are somewhat static in nature, such that they are currently unable to reflect the possibility for speakers to use verbs in novel contexts that shift and extend their meanings. LVCs, like caused-motion constructions (e.g. She blinked the snow off of her eyelashes), are productive enough to be extremely problematic for coverage by a lexical resource (Hwang et al., 2010-b; Bonial et al., 2011). Fixed idiomatic expressions, which are not productive and undergo only morphosyntactic variation, can be stored as a single entry or lexical item, following a words with spaces approach (more flexible idiomatic constructions require a more general treatment). In contrast, the productivity and flexibility of LVCs (both syntactic flexibility and flexibility of adding elements such as determiners and modifiers) make this somewhat impractical. There are promising approaches for the automatic identification of non-frozen, variable idiomatic expressions (e.g. blow one’s own trumpet and toot one’s"
W13-5407,J09-1005,0,0.0768842,"Missing"
W13-5407,W10-0801,1,0.931859,"s but no VN annotations because the verb is simply not present in VN. PropBank is the most comprehensive resource because, unlike FrameNet and VN, the primary goal in developing PropBank was not lexical resource creation, but the development of an annotated corpus to be used as training data for supervised machine learning systems. PropBank, like FrameNet, also includes relations other than verb relations, with annotations for noun, adjective, and complex light verb construction predicates (see http://verbs.colorado.edu/propbank/EPBAnnotation-Guidelines.pdf for full annotation guidelines, see Hwang et al., 2010-a for a description of the annotation of light verbs). As mentioned previously, verbs that are present in PropBank, and therefore SemLink, but not present in VN are prime candidates for addition. 3 Challenges of Adding VerbNet Members The motivation for the expansion of VN is to make it a more robust tool for use in NLP by increasing its coverage. Pursuant to that, we work from a list of verbs that are relatively frequent in SemLink. In some cases, intuitive or lexicographic examination of a verb is sufficient for locating its destination in VN. When a verb has the same syntactic behavior as"
W13-5407,J05-1004,1,0.210151,"h for a given word that are assigned a score above a certain threshold. The score is based on the number of triples that two words share across a corpus. The higher the score, the more similar the behavior of the two words, and thus the more likely they are to be synonyms for computational purposes. This function is also useful when considering VN membership, because similar words will often share classes. 2.3 SemLink Background SemLink (Palmer, 2009; Loper et al., 2007) is both a mapping resource and an annotated corpus. It provides mappings between complementary lexical resources: PropBank (Palmer et al., 2005), VN, FrameNet (Fillmore et al., 2002), and the recently added OntoNotes sense groupings (Pradhan et al., 2007). Each of these lexical resources varies in the level and nature of semantic detail represented, since each was created independently with somewhat differing goals. Nonetheless, all of these resources can be used to associate semantic information with the propositions of natural language. SemLink serves as a platform to unify these resources and therefore combine the finegranularity and rich semantics of FrameNet, the syntactically-based generalizations of VN, and the relatively coars"
W13-5407,W04-2606,0,\N,Missing
W13-5503,W13-2322,1,0.787376,"Missing"
W13-5503,P11-2002,1,0.818345,"Missing"
W13-5503,W07-1508,1,0.81057,". The third part is the PB corpus with mappings from PB roleset ID’s to FN frames and mappings from the PB arguments to FN frame elements. This has recently been manually updated and corrected due to changes in each resource; this process is discussed in more detail in 3.1. 2.4 OntoNotes Sense Groupings The ON Sense Groupings can be thought of as a more coarse-grained view of WordNet senses. This is because these sense groupings were based on WordNet senses that were successively merged into more coarse-grained senses based on the results of inter-annotator agreement in tagging of the senses (Duffield et al., 2007; Pradhan et al., 2007). Essentially, where two annotators were consistently able to distinguish between two senses, the distinction was kept. Where annotators were not able to consistently distinguish between two senses, the senses were conflated into one sense. For example, the sense groupings for the verb leave include the following 6 senses, whereas the WordNet entry includes 14 senses: Sense 1 name=‘depart, go forth, exit’ Sense 2 name=‘leave something behind...’ Sense 3 name=‘cause an effect that remains’ Sense 4 name=‘stop, terminate, end’ Sense 5 name=‘exclude, neglect to include’ Sens"
W13-5503,eckle-kohler-etal-2012-uby,0,0.0237227,"ut the available data is annotated only with a coarsegrained resource like PB, SemLink provides a bridge to make that data useable. As the coverage of SemLink expands to more data, more lexical units, and more resources, this functionality becomes more and more useful in traversing the gap between different annotations and different resource-oriented goals. Efforts to expand and improve SemLink and some of the individual resources therein are discussed in the sections to follow. The utility of integrating resources generally, and of SemLink in particular, is also reflected in the work on UBY (Eckle-Kohler et al., 2012; Gurevychy et al., 2012), a large scale lexical semantic resource using lexical markup framework (an ISO-standard for modeling lexical resources) to uniformly represent and combine a wide range of lexical-semantic resources, like WordNet, FN and VN, but also Wiktionary and Wikipedia in both English and German. This project made use of SemLink’s mappings between VN classes and FN frames to supplement its integration of resources. The UBY project brings to light the need to expand such mappings to resources between many languages, instead of being limited to English. Ideally, SemLink could in t"
W13-5503,E12-1059,0,0.0566159,"notated only with a coarsegrained resource like PB, SemLink provides a bridge to make that data useable. As the coverage of SemLink expands to more data, more lexical units, and more resources, this functionality becomes more and more useful in traversing the gap between different annotations and different resource-oriented goals. Efforts to expand and improve SemLink and some of the individual resources therein are discussed in the sections to follow. The utility of integrating resources generally, and of SemLink in particular, is also reflected in the work on UBY (Eckle-Kohler et al., 2012; Gurevychy et al., 2012), a large scale lexical semantic resource using lexical markup framework (an ISO-standard for modeling lexical resources) to uniformly represent and combine a wide range of lexical-semantic resources, like WordNet, FN and VN, but also Wiktionary and Wikipedia in both English and German. This project made use of SemLink’s mappings between VN classes and FN frames to supplement its integration of resources. The UBY project brings to light the need to expand such mappings to resources between many languages, instead of being limited to English. Ideally, SemLink could in the future integrate with"
W13-5503,W04-2606,0,0.0899745,"Missing"
W13-5503,J93-2004,0,0.0432413,"Missing"
W13-5503,P09-1033,0,0.261647,"Missing"
W13-5503,W04-2705,0,0.0996918,"largely of verbs. FN, in comparison, also includes nouns and adjectives. To address this gap, PB annotations have increasingly focused on noun and adjective predicate annotations. Guidelines for noun annotation have been developed over the past two years (guidelines available at http://verbs.colorado.edu/propbank/EPBAnnotation-Guidelines.pdf), and there are now approximately 48,000 noun annotations (although some of these simply note that the noun is not relational in the instance), and framesets for 2,549 nouns. The framesets borrow heavily from many of the frameset choices made by NomBank (Meyers et al., 2004), although the guidelines have some significant differences. Guidelines for adjective annotation are also being developed based on pilot annotations of about 5400 adjective predicates. Framesets for these adjectives are also currently being created, with 111 existing framesets. These new rolesets include mappings to FN frames and etymologically related VN classes, which will allow for future versions of SemLink to be efficiently updated. Although separate framesets are created for each part of speech, each roleset also contains mappings to related rolesets of other parts of speech. Thus, for e"
W13-5503,J05-1004,1,0.503833,"e strengths of each resource and provides the groundwork for incorporating these lexical resources effectively into linked data resources. SemLink and the resources included therein are discussed with a focus on the value of using lexical resources in a complementary fashion. Recent improvements to SemLink, including the addition of a new resource, the OntoNotes sense groupings, are described. Work to address future goals, including further expansion of SemLink, is also discussed. 1 Introduction SemLink (Palmer, 2009) is an ongoing effort to map complementary lexical resources: PropBank (PB) (Palmer et al., 2005), VerbNet (VN) (Kipper et al., 2008), FrameNet (FN) (Fillmore et al., 2002), and the recently added OntoNotes (ON) sense groupings (Pradhan et al., 2007). Each of these lexical resources varies in the level and nature of semantic detail represented, since each was created independently with somewhat differing goals. Nonetheless, all of these resources can be used to associate semantic information with the propositions of natural language. SemLink serves as a platform to unify these resources and therefore combine the fine-granularity and rich semantics of FN, the syntactically-based generaliza"
W13-5503,N07-1069,1,0.876497,"Missing"
W14-0816,W13-2322,1,0.679559,"Missing"
W14-0816,2013.iwslt-evaluation.5,0,0.0217643,"Missing"
W14-0816,J05-1004,1,0.384406,"Missing"
W14-2903,D12-1045,0,0.126383,"Missing"
W14-2903,Q14-1012,1,\N,Missing
W14-3004,W13-5503,1,0.873988,"ic role labeling performance (Bauer & Rambow, 2011; Dipanjan, et al., 2010; Giuglea & Moschitti, 2006; Merlo & der Plas, 2009; Yi, et al., 2007). That is one of the primary goals of SemLink. The first release of SemLink (1.1) contained mappings between these three lexical resources as well as a set of PropBank instances from the Wall Street Journal data with mappings to VerbNet classes and thematic roles (Palmer, 2009). Our most recent release, SemLink 1.2,3 now includes mappings to FrameNet frames and Frame Elements wherever they are available (FN version 1.5), as well as ON sense groupings (Bonial, et al., 2013). The mapping files between PropBank and VerbNet (version 3.2), and FrameNet have also been checked for consistency and updated to more accurately reflect the current relations between these resources. This annotated corpus can now be used to train and evaluate VerbNet Class and FrameNet Frame classifiers, to explore clusters of Frame Elements that map to the same VerbNet and PropBank semantic roles, and to evaluate approaches to semantic role labeling that use the type-to-type mappings to bootstrap VerbNet and FrameNet role labels from automatic PropBank semantic role labels. 4 the kitchen ev"
W14-3004,N10-1138,0,0.0836314,"Missing"
W14-3004,P09-1033,0,0.0602151,"Missing"
W14-3004,P06-1117,0,0.0798849,"Missing"
W14-3004,J05-1004,1,0.286109,"l give us myriads of alternatives for “coining a phrase.” This causes immense difficulty for NLP systems. No one has made greater contributions to advancing the state of the art of lexical semantics, and its applications to NLP, than Chuck Fillmore. In this paper we focus on the central role that FrameNet has played in our development of SemLink+ and in our current explorations into event ontologies that can play a practical role in accurate automatic event extraction. 2 3 SemLink+ and Semantic Roles SemLink (Palmer, 2009) is an ongoing effort to map complementary lexical resources: PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), FrameNet (Fillmore et al., 2004), and the recently added OntoNotes (ON) sense groupings (Weischedel, et al., 2011). They all associate semantic information with the propositions in a sentence. Each was created independently with somewhat differing goals, and they vary in the level and nature of semantic detail represented. FrameNet is the Detecting events An elusive goal of current NLP systems is the accurate detection of events – recognizing the meaningful relations among the topics, people, 13 Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuc"
W14-3004,lopez-de-lacalle-etal-2014-predicate,0,\N,Missing
W14-3004,bonial-etal-2014-propbank,1,\N,Missing
W14-3004,W14-2903,1,\N,Missing
W14-3004,fillmore-etal-2004-framenet,0,\N,Missing
W14-4206,C12-2029,0,0.0269227,"Choi et al., 2010a) for PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the problem (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In order to predict the source of an Urdu word, we frame two classification tasks: (1) binary classification into Indic and Persio-Arabic and, (2) triclass classification into Arabic, Indic and Persian. Both the problems are modeled using smoothed ngram based language models. Identifying the source of Urdu Vocabulary Predicting the source of a word is similar to language identification where the task is to identify the language a given document is written in. However, language identification at word level is more challenging"
W14-4206,W10-1810,1,0.786556,"identified as noun-verb complex predicates (NVC) at the dependency level. Typically, a noun-verb complex predicate chorii ‘theft’ karnaa ‘to do’ has two components: a noun chorii and a light verb karnaa giving us the meaning ‘steal’. The verbal component in NVCs has reduced predicating power (although it is inflected for person, number, and gender agreement as well as tense, aspect and mood) and its nominal complement is considered the true predicate. In our annotation of NVCs, we follow a procedure common to all PropBanks, where we create frame files for the nominal or the ‘true’ predicate (Hwang et al., 2010). An example of a frame file for a noun such as chorii is described in Table (9). The creation of a frame file for the set of true predicates that occur in an NVC is important from the point of view of linguistic annotation. Given the large number of NVCs, a semiautomatic method has been proposed for creating Hindi nominal frame files, which saves the manual effort required for creating frames for nearly 6 light verb: kar‘do; to steal’ person who steals thing stolen light verb: ho ‘be/become; to get stolen’ thing stolen 5 Conclusion In this paper we have exploited the overlap between the lexic"
W14-4206,N13-1131,0,0.0210816,"PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the problem (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In order to predict the source of an Urdu word, we frame two classification tasks: (1) binary classification into Indic and Persio-Arabic and, (2) triclass classification into Arabic, Indic and Persian. Both the problems are modeled using smoothed ngram based language models. Identifying the source of Urdu Vocabulary Predicting the source of a word is similar to language identification where the task is to identify the language a given document is written in. However, language identification at word level is more challenging than a typical documen"
W14-4206,Q14-1003,0,0.0134097,"file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the problem (Dunning, 1994; Elfardy and Diab, 2012; King and Abney, 2013; Nguyen and Dogruoz, 2014; Lui et al., 2014). In order to predict the source of an Urdu word, we frame two classification tasks: (1) binary classification into Indic and Persio-Arabic and, (2) triclass classification into Arabic, Indic and Persian. Both the problems are modeled using smoothed ngram based language models. Identifying the source of Urdu Vocabulary Predicting the source of a word is similar to language identification where the task is to identify the language a given document is written in. However, language identification at word level is more challenging than a typical document level language identification problem. The"
W14-4206,malik-etal-2010-transliterating,0,0.0726245,"Missing"
W14-4206,W12-3623,1,0.45342,"u vocabulary just by using letter-based heuristics. For example neither Arabic nor Persian has aspirated consonants like bH , ph Aspirated Bilabial Plosives; tSh , dZH Aspirated Alveolar Fricatives; ãH Aspirated Retroflex Plosive; gH , kh Aspirated Velar Plosives etc. while Hindi does. Similarly, the following sounds occur only in Arabic and Persian: Z Fricative Postalveolar; T, D Fricative Dental; è Fricative Pharyngeal; X Fricative Uvular etc. Using these heuristics we could identify 2,682 types as Indic, and 3,968 as either Persian or Arabic out of 12,223 unique types in the Urdu treebank (Bhat and Sharma, 2012). = arg max p(w|ci ) ∗ p(ci ) (1) ci The prior distribution p(c) of a class is estimated from the respective training sets shown in Table (3). Each training set is used to train a separate letter-based language model to estimate the probability of word w. The language model p(w) is implemented as an n-gram model using the IRSTLM-Toolkit (Federico et al., 2008) with Kneser-Ney smoothing. The language model is defined as: p(w) = n Y i=1 i−1 p(li |li−k ) (2) where, l is a letter and k is a parameter indicating the amount of context used (e.g., k = 4 means 5-gram model). 3.2 Etymological Data In o"
W14-4206,W09-3036,1,0.783639,"ni}@colorado.edu, taf seer@dsu.edu.pk Abstract drawing its higher lexicon from Sanskrit and Urdu from Persian and Arabic) to the point where the two styles/languages become mutually unintelligible. In written form, not only the vocabulary but the way Urdu and Hindi are written makes one believe that they are two separate languages. They are written in separate orthographies, Hindi being written in Devanagari, and Urdu in a modified Persio-Arabic script. Given such (apparent) divergences between the two varieties, two parallel treebanks are being built under The Hindi-Urdu treebanking Project (Bhatt et al., 2009; Xia et al., 2009). Both the treebanks follow a multi-layered and multi-representational framework which features Dependency, PropBank and Phrase Structure annotations. Among the two treebanks the Hindi treebank is ahead of the Urdu treebank across all layers. In the case of PropBanking, the Hindi treebank has made considerable progress while Urdu PropBanking has just started. The creation of predicate frames is the first step in PropBanking, which is followed by the actual annotation of verb instances in corpora. In this paper, we look at the possibility of porting related frames from Arabic"
W14-4206,palmer-etal-2008-pilot,1,0.813864,"for Urdu simple verbs. There were no significant differences found between the Urdu and Hindi rolesets, which describe either semantic variants of the same verb or its causative forms. Further, in order to name the frame files with their corresponding Urdu lemmas, we used Konstanz’s Urdu transliteration scheme As discussed in Section 2.1.1, the creation of predicate frames precedes the actual annotation of verb instances in a given corpus. In this section, we describe our approach towards the first stage of Urdu PropBanking by adapting related predicate frames from Arabic and Hindi PropBanks (Palmer et al., 2008; Vaidya et al., 2011). Since a PropBank is not available for Persian, we could only adapt those predicate frames which are shared with Arabic and Hindi. Although, Urdu shares or borrows most of its literary vocabulary from Arabic and Persian, it retains its simple verb (as opposed to compound or complex verbs) inventory from Indo-Aryan ancestry. Verbs from Arabic and Persian are borrowed less frequently, although there are examples such 5 Borrowed verbs often do not function as simple verbs rather they are used like nominals in complex predicate constructions such as mehsoos in ‘mehsoos karna"
W14-4206,choi-etal-2010-propbank,1,0.860681,"as the object in the other. This is the primary difference between PropBank’s approach to semantic role labels and the Paninian approach to karaka labels, de.01 to give Arg0 Arg1 Arg2 the giver thing given recipient Table 1: A Frame File The annotation process for the PropBank takes place in two stages: the creation of frame files for individual verb types, and the annotation of predicate argument structures for each verb instance. The annotation for each predicate in the corpus is carried out based on its frame file definitions. 48 The PropBank makes use of two annotation tools viz. Jubilee (Choi et al., 2010b) and Cornerstone (Choi et al., 2010a) for PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the prob"
W14-4206,choi-etal-2010-propbank-instance,1,0.818522,"as the object in the other. This is the primary difference between PropBank’s approach to semantic role labels and the Paninian approach to karaka labels, de.01 to give Arg0 Arg1 Arg2 the giver thing given recipient Table 1: A Frame File The annotation process for the PropBank takes place in two stages: the creation of frame files for individual verb types, and the annotation of predicate argument structures for each verb instance. The annotation for each predicate in the corpus is carried out based on its frame file definitions. 48 The PropBank makes use of two annotation tools viz. Jubilee (Choi et al., 2010b) and Cornerstone (Choi et al., 2010a) for PropBank instance annotation and PropBank frame file creation respectively. For annotation of the Hindi and Urdu PropBank, the Jubilee annotation tool had to be modified to display dependency trees and also to provide additional labels for the annotation of empty arguments. 3 Singular Plural khabar khabar khabarain khabaron Direct Oblique Table 2: Morphological Paradigm of khabar This explains the efficiency of n-gram based approaches to either document level or word level language identification tasks as reported in the recent literature on the prob"
W14-4206,N13-1031,0,0.0330134,"Missing"
W14-4206,W11-0403,1,0.880982,". There were no significant differences found between the Urdu and Hindi rolesets, which describe either semantic variants of the same verb or its causative forms. Further, in order to name the frame files with their corresponding Urdu lemmas, we used Konstanz’s Urdu transliteration scheme As discussed in Section 2.1.1, the creation of predicate frames precedes the actual annotation of verb instances in a given corpus. In this section, we describe our approach towards the first stage of Urdu PropBanking by adapting related predicate frames from Arabic and Hindi PropBanks (Palmer et al., 2008; Vaidya et al., 2011). Since a PropBank is not available for Persian, we could only adapt those predicate frames which are shared with Arabic and Hindi. Although, Urdu shares or borrows most of its literary vocabulary from Arabic and Persian, it retains its simple verb (as opposed to compound or complex verbs) inventory from Indo-Aryan ancestry. Verbs from Arabic and Persian are borrowed less frequently, although there are examples such 5 Borrowed verbs often do not function as simple verbs rather they are used like nominals in complex predicate constructions such as mehsoos in ‘mehsoos karnaa’ to feel. 52 (Malik"
W14-4206,W13-1018,1,0.895298,"Missing"
W14-4206,kingsbury-palmer-2002-treebank,1,\N,Missing
W14-4206,D13-1084,0,\N,Missing
W14-5816,W11-0132,0,0.174798,"ll is Figure 2: Derivation tree for ‘Jill is running’. The dashed node indicates adjunction and the solid node indicates substitution 129 same lexical item realized as the anchor of varying syntactic realizations. For example a verb such as run will anchor a different elementary tree for its passive or interrogative variant. 3 Data In this section, we introduce the nominal predicates that will be the focus of our LTAG analysis. Such nominals allow an agentive (ergative-marked2 ) subject with the light verb kar ‘do’. In contrast, the same nominal does not have an agentive subject with ho ‘be’ (Ahmed and Butt, 2011). The alternation with ho ‘be’ has an intransitivizing effect. In (1) and (2), a change in the light verb results in the presence or absence of the agent argument. The nominal chorii is the same, but the LVC in (1) requires only a Theme argument, whereas (2) needs an Agent and a Theme. (1) gehene chorii hue. jewels.M theft.F be.Perf.MPl ‘The jewels got stolen’ (2) Ram-ne gehene chorii kiye. Ram-Erg jewels.M.Pl theft.F do.Perf.M.Pl ‘Ram stole the jewels ’ In English, a similar alternation structure may be found with light verbs in bring to light vs. come to light (Claridge, 2000). Here, two lig"
W14-5816,ahmed-etal-2012-reference,0,0.0669763,"ents for the light verb construction is instead represented in the nominal’s tree. The light verb can only choose 3 Based on the comments of the reviewers we are now considering a revision of the noun-centric analysis in this paper. It may seem that a verb-centric analysis may be more appropriate for Hindi LVCs. However, due to lack of space, we do not explore the second option fully in this paper and leave it to future work. 131 the semantic property of the nominal it may combine with (e.g., the light verb ho may combine only with nominals that have no agentive arguments). Other analyses e.g Ahmed et al. (2012) represent the light verb kar ‘do’ with arguments of its own. We discuss this in Section 5. Our work follows Han and Rambow (2000)’s representation of Sino-Korean LVCs. This work has also proposed separate trees for the nominal and light verb. The elementary tree of the nominal is an an initial tree, and as it is considered the true predicate, it also chooses a syntactic structure that will realize all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an adjunct to the nominal’s basic structure. However, as it is a predicate, it is also a spec"
W14-5816,W12-4619,1,0.84393,"two approaches need to be explored more thoroughly within the TAG framework and we leave this to future work. While this work has examined one class of nominals that occur as part of light verb constructions, it does not complete the analysis of light verb constructions in Hindi. The behaviour of other nominal classes remains to be explored. There are also nominals that occur with light verbs other than kar ‘do’ and ho ‘be’. Finally, while the work presented here is mainly theoretical, it is in keeping with recent proposals for extracting a Hindi TAG grammar from a phrase structure treebank (Bhatt et al., 2012; Mannem et al., 2009). The algorithm in Bhatt et al. (2012) relies on the annotated Hindi Dependency Treebank and proposes a rule extraction system for elementary trees. Therefore, the description of Hindi LVCs in TAG would be a useful addition to the implementation of a grammar extraction task. Acknowledgements The first author was supported by DAAD (Deutscher Akademischer Austausch Dienst) for a research stay at the University of Konstanz in 2013-14s. We are also thankful to Prof Miriam Butt for hosting the first author at the University of Konstanz. We would like to thank Bhuvana Narasimha"
W14-5816,W00-2013,1,0.639076,"comments of the reviewers we are now considering a revision of the noun-centric analysis in this paper. It may seem that a verb-centric analysis may be more appropriate for Hindi LVCs. However, due to lack of space, we do not explore the second option fully in this paper and leave it to future work. 131 the semantic property of the nominal it may combine with (e.g., the light verb ho may combine only with nominals that have no agentive arguments). Other analyses e.g Ahmed et al. (2012) represent the light verb kar ‘do’ with arguments of its own. We discuss this in Section 5. Our work follows Han and Rambow (2000)’s representation of Sino-Korean LVCs. This work has also proposed separate trees for the nominal and light verb. The elementary tree of the nominal is an an initial tree, and as it is considered the true predicate, it also chooses a syntactic structure that will realize all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an adjunct to the nominal’s basic structure. However, as it is a predicate, it is also a special type of auxiliary tree viz., a predicative auxiliary tree (Abeill´e and Rambow, 2000). The second feature of this analysis, al"
W14-5816,C88-2147,0,0.857551,"Feature based Tree Adjoining Grammar Tree-Adjoining Grammar (TAG) is a formal tree-rewriting system that is used to describe the syntax of natural languages (Joshi and Schabes, 1997). The basic structure of a TAG grammar is an elementary tree, which is a fragment of a phrase structure tree labelled with both terminal and non-terminal nodes. The elementary trees are combined by the operations of substitution (where a terminal node is replaced with a new tree) or adjunction (where an internal node is split to add a new tree). The elementary trees in TAG can be enriched with feature structures (Vijay-Shanker and Joshi, 1988). These can capture linguistic descriptions in a more precise manner and also capture adjunction constraints. TAG with feature structures is also known as FTAG (Feature-structure based TAG). A TAG can also be lexicalized i.e., an elementary tree has a lexical item as one of its terminal nodes. Lexicalized TAG enhanced with feature structures is known as Lexicalized Feature-based Tree-Adjoining Grammar (LF-TAG). This has been used for developing computational grammars for English (XTAG-Group, 2001), French (Abeill´e and Candito, 2000) and Korean (Han et al., 2000). In our analysis, we will also"
W15-1012,P98-1013,0,0.0241645,"en using all automatic SRL and word alignment inputs. 1 Table 1: Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self-contained and independent of the predicate or language, as Fung et al. (2007), Choi et al. (2009), and our previous work (Wu and Palmer, 2011) have demonstrated, assuming alignment between argumen"
W15-1012,W13-2322,1,0.905063,": Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self-contained and independent of the predicate or language, as Fung et al. (2007), Choi et al. (2009), and our previous work (Wu and Palmer, 2011) have demonstrated, assuming alignment between arguments of the same type is insufficient. Table 1 shows the alignment di"
W15-1012,bonial-etal-2014-propbank,1,0.900585,"Missing"
W15-1012,D07-1007,0,0.0278038,"inese-English predicate-topredicate and argument type-to-argument type alignment probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs. 1 Table 1: Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling syst"
W15-1012,P96-1041,0,0.0771841,"e probability of an English predicate p(al,e |ak,c , predi,c , predj,e ) : given an aligned Chinese & English predicate pair and the Chinese argument type, the probability of an English argument type In addition to producing a better alignment output, these 2 probabilities (along with probabilities in the English-to-Chinese alignment direction) may also be used to compute the semantic similarity of a pair of parallel sentences. Figure 1: Chinese predicate-arguments mapping example Figure 2: Bad predicate-argument alignment (solid lines) caused by word alignment (dashed lines) error 3.2.1 ing (Chen and Goodman, 1996) to smooth Predicate-to-predicate mapping probability There are over 20,000 Chinese predicates and over 10,000 English predicates (in OntoNotes 5.0 PropBank frame files). Even on a large corpora, f reqmap (predi,c , predj,e ) will be low or zero for many predicate pairs when producing a probability estimate. We chose the Simple Good-Turing smoothing method (Gale, 1995) to smooth the seen mapping frequency counts and P estimate the total unseen mapping probability j∈f reqmap (predi,c ,predj,e )=0 p(predj,e |predi,c ). 3.2.2 p(al,e |ak,c , predi,c , predj,e ) = max(f req(al,e |ak,c , predi,e , p"
W15-1012,W09-3020,1,0.896125,"system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self-contained and independent of the predicate or language, as Fung et al. (2007), Choi et al. (2009), and our previous work (Wu and Palmer, 2011) have demonstrated, assuming alignment between arguments of the same type is insufficient. Table 1 shows the alignment distribution of the core argument types between Chinese and English. While ARG0 and ARG1 alignments are relatively deterministic, alignment involving ARG2-5 and adjunct argument types (not shown) are much more varied. Part of this alignment variety is caused by differences in argument annotation guidelines between English and Chinese, but another part is caused by verb predicates being nominalized in the translation. Our previous wo"
W15-1012,2007.tmi-papers.10,0,0.152798,"g semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self-contained and independent of the predicate or language, as Fung et al. (2007), Choi et al. (2009), and our previous work (Wu and Palmer, 2011) have demonstrated, assuming alignment between arguments of the same type is insufficient. Table 1 shows the alignment distribution of the core argument types between Chinese and English. While ARG0 and ARG1 alignments are relatively deterministic, alignment involving ARG2-5 and adjunct argument types (not shown) are much more varied. Part of this alignment variety is caused by differences in argument annotation guidelines between English and Chinese, but another part is caused by verb predicates being nominalized in the translat"
W15-1012,P11-1023,0,0.0568724,"ent type alignment probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs. 1 Table 1: Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were in"
W15-1012,P13-2067,0,0.108689,"t probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs. 1 Table 1: Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self"
W15-1012,2008.amta-papers.13,0,0.0220029,"EM) techniques that iteratively refines these probabilities, we achieved an 1 F1 point predicate alignment performance improvement using all automatic (SRL and word alignment) inputs. More over, even though the alignment probabilities were generated from automatic system inputs, in some instances, we were able to improve alignment performances using gold SRL inputs. 2 Related Work Resnik (2004) was one of the earlier works proposing semantic similarity (with a looser definition of semantically similar/equivalent phrases) using triangulation between parallel corpora. This was extended later by Madnani et al. (2008a; 2008b)). Mareˇcek (2009) proposed aligning tectogrammatical trees, where only content (autosemantic) words are nodes, in a parallel English/Czech corpus to improve overall word alignment and thereby improve machine translation. Pad´o and Lapata (2005; 2006) used word alignment and syntax based argument similarity to project English FrameNet seman75 tic roles to German. Fung et al. (2007) demonstrated that there is poor semantic parallelism between Chinese-English bilingual sentences. Their technique for improving Chinese-English predicate-argument mapping (ARGChinese,i 7→ ARGEnglish,j ) con"
W15-1012,H05-1108,0,0.0934065,"Missing"
W15-1012,P06-1146,0,0.0347507,"Missing"
W15-1012,J05-1004,1,0.235687,"English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self-contained and independent of the predicate or language, as Fung et al. (2007), Choi et al. (2009), and our previous work (Wu and Palmer, 2011) have demonstrated, assuming alignment between arguments of the same type is insufficient. Table 1 shows the alignment distribution of the core argument"
W15-1012,2009.eamt-1.30,0,0.115994,"e-topredicate and argument type-to-argument type alignment probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs. 1 Table 1: Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argu"
W15-1012,N09-2004,0,0.162445,"e-topredicate and argument type-to-argument type alignment probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs. 1 Table 1: Chinese argument type (column) to English argument type (row) alignment counts using gold SRL and word alignment annotated Xinhua News data Introduction With the growing interest in building semanticallydriven machine translation (MT) systems/evaluation metrics (Carpuat and Wu, 2007; Wu and Fung, 2009b; Wu and Fung, 2009a; Lo and Wu, 2011; Lo et al., 2013; Ma, 2014), the need for a comprehensive and high performing semantic alignment system has become more pressing. While there are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argu"
W15-1012,W11-1003,1,0.878005,"re are finer grained representations such as FrameNet (Baker et al., 1998) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013), PropBank (Palmer et al., 2005) semantic representation has been popular in the MT community partly because of the availability of large quantity of annotated data in multiple languages, enabling the development of accurate automatic semantic role labeling systems. While the argument types defined in PropBank were intended to be self-contained and independent of the predicate or language, as Fung et al. (2007), Choi et al. (2009), and our previous work (Wu and Palmer, 2011) have demonstrated, assuming alignment between arguments of the same type is insufficient. Table 1 shows the alignment distribution of the core argument types between Chinese and English. While ARG0 and ARG1 alignments are relatively deterministic, alignment involving ARG2-5 and adjunct argument types (not shown) are much more varied. Part of this alignment variety is caused by differences in argument annotation guidelines between English and Chinese, but another part is caused by verb predicates being nominalized in the translation. Our previous work tried to address the first issue by using"
W15-1012,S15-1027,1,0.81324,"To generate reference predicate-argument alignments, we ran the alignment system with a cutoff threshold of Fc,e &lt; 0.4 (i.e., alignments with Fscore below 0.4 are discarded) using all gold annotations. We selected a small random sample of the Xinhua output and found the output to have both high precision and recall, with only occasional discrepancies caused by possible word alignment errors (and was no worse than inter-annotator disagreements). For predicate-argument alignments using automatic word alignment input, we chose a cutoff threshold of Fc,e &lt; 0.2. We trained our Chinese SRL system (Wu and Palmer, 2015) with Berkeley Parser output on Chinese PropBank 1.0 (all Xinhua News, excluding files in the triple-gold corpus). We trained our English SRL system (same architecture as the Chinese SRL system) with Berkley parser output on OntoNotes Release 5.0 (excluding files in the triple-gold corpus) and BOLT phase 1 data (which also includes nominal annotation). We use the Berkeley aligner trained on a 1.6M sentence parallel corpora collected from a variety of sources3 . These same corpora were also used to build our probabilistic alignment model. LDC2013T19 78 LDC2003E07, LDC2005T06, LDC2006E26, LDC200"
W15-1012,2010.amta-papers.15,1,0.838767,"Missing"
W15-1012,C00-2137,0,0.0730505,"Missing"
W15-1012,D10-1030,0,0.0320604,"Missing"
W15-1012,C98-1013,0,\N,Missing
W15-1012,P07-2045,0,\N,Missing
W15-1612,J09-2001,0,0.0616612,"s were clustered automatically, then the clusters were manually refined and given names). Detailed in Srikumar and Roth (2013a), those categories cut across preposition types to combine related TPP senses for better data-driven generalization. Cohen’s κ for inter-annotator agreement was 0.75, which is encouraging, though it is unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’H"
W15-1612,W13-2322,1,0.777114,"Missing"
W15-1612,bhatia-etal-2014-unified,0,0.0287958,"cial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. Th"
W15-1612,W07-1604,0,0.0343143,"unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comp"
W15-1612,W06-1670,0,0.146858,"and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numer"
W15-1612,W06-1207,0,0.0343708,"nses for better data-driven generalization. Cohen’s κ for inter-annotator agreement was 0.75, which is encouraging, though it is unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Nei"
W15-1612,D09-1047,0,0.617638,"matic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of"
W15-1612,J14-1002,1,0.642481,"more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspective—i.e. with the goal of describing and documenting the uses of individual prepositions in a lexical resource rather than labeling a corpus with free-text preposition annotations. We hope that the latter, token-driven approach will be taken for annotating text with preposition supersenses so that those annotations will be suitable for training statistical NLP systems. 2 Our Approach With the end of free-text semantic annotation in mind, we develop and document a preposition"
W15-1612,C04-1198,0,0.0252432,"es. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framewor"
W15-1612,C10-2052,0,0.261708,"cs in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspective—i.e. with the go"
W15-1612,P11-2056,0,0.150935,"Missing"
W15-1612,hwang-etal-2014-criteria,1,0.867098,"Missing"
W15-1612,W07-1601,0,0.0452238,"Missing"
W15-1612,P14-1120,0,0.245532,"Missing"
W15-1612,S07-1005,0,0.348619,"Missing"
W15-1612,H94-1020,0,0.121481,"Missing"
W15-1612,W10-1827,0,0.02899,"The descriptive challenges raised by prepositions have not gone unnoticed in the literature; see, e.g., Saint-Dizier (2006a) for an assortment of syntactic and semantic issues. Here we touch on some of the lines of inquiry, resources, and NLP approaches to preposition semantics found in previous work. tention from theorists (Bowerman and Choi, 2001; Hagège, 2009; Regier, 1996; Xu and Kemp, 2010; Zelinsky-Wibbelt, 1993) but is of practical interest as well, especially when it comes to machine translation and second language acquisition. A corpus creation project for German preposition senses (Müller et al., 2010, 2011) is similar in spirit to the supersense approach taken below. Finally, the PrepNet resource (Saint-Dizier, 2006b) aimed to describe the semantics of prepositions across several languages; however, it seems not to have progressed beyond the preliminary stages. Thus far, our approach has focused on English, but aims to define supersense categories semantically rather than by language-specific criteria (e.g., syntactic tests) so as to encourage its adaptation to other languages in the future. 1.1 1.2 1 Background Linguistic Approaches Most studies of preposition semantics are limited to so"
W15-1612,W03-0411,0,0.252975,"Missing"
W15-1612,J09-2002,0,0.154496,"Missing"
W15-1612,D10-1032,0,0.0281672,"ns are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairl"
W15-1612,saint-dizier-2006-prepnet,0,0.462283,"3, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. The descriptive challenges raised by prepositions have not gone unnoticed in the literature; see, e.g., Saint-Dizier (2006a) for an assortment of syntactic and semantic issues. Here we touch on some of the lines of inquiry, resources, and NLP approaches to preposition semantics found in previous work. tention from theorists (Bowerman and Choi, 2001; Hagège, 2009; Regier, 1996; Xu and Kemp, 2010; Zelinsky-Wibbelt, 1993) but is of practical interest as well, especially when it comes to machine translation and second language acquisition. A corpus creation project for German preposition senses (Müller et al., 2010, 2011) is similar in spirit to the supersense approach taken below. Finally, the PrepNet resource (Sain"
W15-1612,P12-2050,1,0.404449,"; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki i"
W15-1612,N15-1177,1,0.172806,"wang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and"
W15-1612,Q13-1019,1,0.876864,"e descriptive annotation scheme for prepositions must deal with these messy facts. Following a brief discussion of existing approaches to preposition semantics (§1), this paper offers a new approach to characterizing their functions at a coarsegrained level. Our scheme is intended to apply to almost all preposition tokens, though some are excluded on the grounds that they belong to a larger multiword expression or are purely syntactic (§2). The rest of the paper is devoted to our coarse semantic categories, supersenses (§3).2 Many of these categories are based on previous proposals—primarily, Srikumar and Roth (2013a) (so-called preposition relations) and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also"
W15-1612,N09-3017,0,0.194973,"of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspecti"
W15-1612,tsvetkov-etal-2014-augmenting-english,1,0.604089,"m into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples"
W15-1612,S07-1051,0,0.17518,"apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotat"
W15-1612,P98-1013,0,\N,Missing
W15-1612,C98-1013,0,\N,Missing
W16-1003,P10-1143,0,0.0673268,"Missing"
W16-1003,W14-2903,1,0.900929,"Missing"
W16-1003,liu-etal-2014-supervised,0,0.0206275,"the generation of virtual action videos rather than analysis, but it provides encouraging evidence that the same level of abstraction could benefit analysis as well. In the sections to follow, we introduce the ways in which our ontology and its connections to established lexical resources could be uniquely valuable for both a text and video processing application. 3.1 Text Use Case For supervised machine learning systems, instances unseen in training data are problematic. Access to 22 related terms found in lexical resources can allow them to generalize training data to additional instances. Liu et al. (2014) identify problems even with human annotation of events, noting that in the EventCorefBank corpus (Bejan and Harabajiu, 2010), “seizing 12 Somali and 11 Yemeni nationals” and “capturing 23 of the raiders” had not been identified as the same event. Our ontology, in which the lexical items seize and capture are linked to the same class, could be helpful in automatically connecting these two mentions as the same event. In the discussion of their system of event coreference, Liu et al. also noted that “[event coreference] can possibly be improved by other types of event relations, such as subevent"
W16-1003,pustejovsky-etal-2010-iso,0,0.0308632,", but the primary focus of RED is to represent the temporal and causal relationships between those eventualities. The final goal is to produce annotations rich enough that a computer, using complex inferencing, co-reference, and domain-specific algorithms, would be able to construct an accurate timeline of when the events in a given document occur relative to any fixed dates present and relative to one another (e.g., automatically constructed timelines of medical histories). RED builds on THYME (Styler et al., 2014), a temporal relationship annotation of clinical data that is based on TimeML (Pustejovsky et al., 2010). The temporal relations are quite fine-grained, including Before, Before+overlap, Overlap, and CONTAIN. These labels are further distinguished with causal labels where appropriate: Before/Causes, Before/Preconditions. To anchor the events into a 21 timeline, RED links the event to a document time or section time where applicable, and marks up explicit references to time in the document. 2.3 Snapshot of the Ontology The construction of the ontology is still underway, and has involved a combination of bottom-up and top-down approaches. As ERE event types provide useful constraints on which even"
W16-1003,W15-0812,0,0.0176405,"lications, specifically to recognize and reason about events in both text and video. We find that the ontology facilitates the generalization of potentially noisy or sparse individual realizations of events into larger categories of events and enables reasoning about event relations and participants, both of which are useful in event recognition and interpretation regardless of modality. 1 Introduction & Background The valuable computational lexical resources, VerbNet (Kipper et al., 2008), FrameNet (Fillmore et al., 2002), and the Rich Entities, Relations and Events (ERE) annotation project (Song et al., 2015), each provide somewhat distinct information about which eventualities are related syntactically, semantically, or both, and which types of participants are involved in classes of eventualities. VerbNet and FrameNet also involve long-standing and comprehensive annotation efforts, using the class and participant type labels set out in each resource. The resulting annotated corpora have proved to be useful sources of training data for a variety of Natural Language Processing (NLP) systems, including automatic semantic role labeling, word sense disambiguation, and question-answering systems. Rece"
W16-1003,Q14-1012,1,0.882001,"Missing"
W16-1003,doddington-etal-2004-automatic,0,\N,Missing
W16-1004,W13-2322,1,0.735417,"me. RED also labels a causal and temporal relation between the two events, &quot;BEFORE/PRECONDITIONS&quot;, showing that the quitting event leads to, but does not directly cause, the replacement, and a temporal CONTAINS relation linking quit to Wednesday.     Event 1: quit - BEFORE DOCTIME, Actual Modality Event 2: replace - AFTER DOCTIME, Actual Modality Relation 1: quit BEFORE/ PRECONDITIONS replace Relation 2: Wednesday CONTAINS quit Although RED does not annotate the arguments of events, it is intended to be combined with semantic role annotations such as PropBank (Bonial et al., 2014) or AMR (Banarescu et al., 2013), which would provide the argument information. For this example, the quit and replace events would also be given the predicate argument structures below: quit.01 Arg0: Media Tycoon Barry Diller Arg1: as chief of Vivendi Universal Entertainment ArgM-TMP: on Wednesday replace.01 Arg2: Parent company chairman JeanRene Fourtou Arg1: Diller ArgM-MOD: will ArgM-PRD: as chief executive of US unit. EER: The following events are connected by Condition and Temporality relations:   Event 1 (Personnel.EndPosition): quit Event 2 (Personnel.StartPosition): replace 34 A preliminary analysis of the Rich ER"
W16-1004,W14-2903,1,0.927744,"Missing"
W16-1004,D12-1045,0,0.048737,"y – along with 21 sense-based subtypes (or relation senses), as shown in Table 1. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency_Causality will play Cause and Result roles respectively. Figure 1 shows more information about types and roles. Figure 1: Roles and examples specific to fine-grained event-event relation subtypes. 2.5 Richer Event Descriptions (RED) 3 RED annotation (Ikuta et al., 2014) marks all events in a document, as well as certain relations between those events. RED combines coreference (Pradhan et al., 2007; Lee et al., 2012) and THYME Temporal Relations annotation (Styler et al., 2014) to provide a thorough representation of entities, events and their relations. The RED schema also goes beyond prior annotations of coreference or temporal relations by also annotating subevent structure, cause-effect relations and reporting relations. Guidelines for RED annotation can be found at https://github.com/timjogorman/RicherEventDescr iption/blob/master/guidelines.md. 31 Annotation Data Annotated Features and The representation of events and the scope of annotation vary across the different annotation approaches. Table 2 c"
W16-1004,W15-0809,1,0.776814,"verlapping data set could be used to explore how the differences in annotation procedure lead to differences in decisions about event granularity. 2.3 Event Nugget (EN) An Event Nugget is a tuple of an event trigger, classification of event type and subtype, and realis attribute. It is similar to an event mention in ERE, but arguments are not labelled. EN annotation in 2014 focused on event nuggets (expanded triggers) only, and followed the same taxonomy of 33 event types and subtypes as Light ERE. However, instead of tagging minimal extent as the trigger, EN allowed multi-word event nuggets (Mitamura et al., 2015). Multi-word event nuggets can be either continuous or discontinuous, and are based on the goal of marking the maximal extent of a semantically meaningful unit to express the event in a sentence. EN also added a realis attribute for each event mention. The realis attribute labels each event as Actual, Generic, or Other. TAC KBP 2014 conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute. 30 Table 1: EER event relation types. In 2015,"
W16-1004,W15-0812,1,0.92996,"e taggable, and entity subtypes are not labeled). The event ontology of Light ERE is similar to ACE, with slight modification and reduction, and there is strict coreference of events within documents (Aguilar et al., 2014). As in ACE, the annotation of each event mention includes the identification of a trigger, the labeling of the event type, subtype, and participating event argument entities and time expressions. Simplifying from ACE, only attested actual events are annotated (no irrealis events or arguments). Rich ERE annotation expands on both the inventories and taggability of Light ERE (Song et al., 2015). Rich ERE Entity annotation adds nonspecific entities and nominal head marking, in addition to adding a distinction between Location and Facility entity types. Rich ERE Relation annotation doubles the Light ERE ontology to twenty relation subtypes, and also adds future, hypothetical, and 28 conditional relations. A new category of argument fillers was added for Rich ERE, to allow arguments that are not taggable as entities to be used as fillers for specific relation and event subtypes. For each event mention, Rich ERE labels the event type and subtype, its realis attribute, any of its argumen"
W16-1004,W16-1005,1,0.820435,"event relation types. In 2015, TAC KBP ran an open evaluation on EN that was expanded to three evaluation tasks: Event Nugget Detection, Event Nugget Detection and Coreference, and Event Nugget Coreference. Full Event Nugget Coreference is identified when two or more Event Nuggets refer to the same event. EN annotation in 2015 followed the Rich ERE event taxonomy, which added 5 event types and subtypes to make a total of 38 event types and subtypes, and also followed the Rich ERE guidelines on trigger extents, which adopted the minimal extent rule and disallowed discontinuous event triggers (Song et al., 2016). Annotation of Event Nugget Coreference adopted the concept of Event Hopper as in Rich ERE. 2.4 Event-Event Relations (EER) EER annotation focuses on relations between events in the ERE/ACE taxonomy, both within document and cross-document (Hong et al., 2016). Our general goal is to construct event-centric knowledge networks, where each node is an event and the edges effectively capture the relations between any two events. EER includes five main types of event relations – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 sense-based subtypes (or relation senses)"
W16-1701,P08-1030,1,0.776377,"d texts. Most previous IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 201"
W16-1701,P03-2030,0,0.0231323,"aki et al., 2004; Radev, 2000), which focus on the relatedness between two sentences, by tackling a full document or multiple documents. We adopted some terminology (e.g., Causality and Expansion) from the taxonomy of discourse relations (Miltsakaki et al., 2004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al.,"
W16-1701,D12-1045,0,0.110855,"Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco"
W16-1701,P10-1143,0,0.0939859,"ilar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgeme"
W16-1701,S13-2002,0,0.0743148,"004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity"
W16-1701,P13-1008,1,0.805039,"ic Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Event 1 Event 2 T"
W16-1701,W09-3208,1,0.81815,"ture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.r"
W16-1701,D14-1198,1,0.856623,"event. Event mention: a phrase or sentence within which an event is described, including a trigger and a set of arguments. Event: a set of coreferential event mentions within one document. We define the event-event relation task as the annotation of all applicable logical relations between two events. For example, as illustrated in Figure 1, the following events are connected by Condition and Temporality relations: Event 1: Media tycoon Barry Diller on Wednesday quit as chief of Vivendi Universal EntertainThe ultimate goal of Information Extraction (IE) is to construct “Information Networks” (Li et al., 2014) from unstructured texts. Most previous IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extrac"
W16-1701,W09-4303,1,0.782356,"etworks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_r"
W16-1701,P10-1081,0,0.0395486,"IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 2016 Association for Computa"
W16-1701,W13-1903,0,0.0219813,"ope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the"
W16-1701,D11-1027,0,0.0372204,"alysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, and Bernardo Magnin"
W16-1701,miltsakaki-etal-2004-penn,0,0.0955379,"kes room for the successor. Conjunction Sub-event Sub-event Sub-event / e2 / e5 o / e6 Conjunction Conjunction Figure 2: A hierarchical event network 3.2 Contingency and Comparison A Contingency relation indicates either an event leading to the emergence (Causality) or serving as a triggering condition (Conditional) of another event. Comparison relations indicate deeper logical contrasts between relations. Opposition indicates a relation in which two events are mutually contradictory, and unlikely to be both true. This has some similarity to Contrast.Opposition in the Penn Discourse Treebank (Miltsakaki et al., 2004) or specific annotations of opposition (Feltracco et al., 2015; Takabatake et al., 2015). Negation indicates that while two events could both be true, one shows that the other is no longer true. Competition shows that two events are contrasting versions of the same underlying “event” (e.g., retreat versus escape in disorder). Event-Event Relation Schema Our event-event relation schema includes 5 main Types – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 Subtypes as shown in Table 1. Table 1 also demonstrates Roles. Events involved in a relation play certain ro"
W16-1701,D12-1062,0,0.042122,"formation Extraction (IE) is to construct “Information Networks” (Li et al., 2014) from unstructured texts. Most previous IE work focused on constructing entity-centric Information Networks where each node represents an entity and each edge represents a relation. We propose a novel task to construct a new layer of eventcentric Information Networks across multiple documents, where each node is an event and the edges capture the relations between two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceeding"
W16-1701,C14-1198,0,0.0192457,"et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, and Bernardo Magnini. 2015. Opposition relations among verb frames."
W16-1701,ovchinnikova-etal-2010-data,0,0.0262188,"entences, by tackling a full document or multiple documents. We adopted some terminology (e.g., Causality and Expansion) from the taxonomy of discourse relations (Miltsakaki et al., 2004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tone"
W16-1701,W11-0419,0,0.0362447,"events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand"
W16-1701,W00-1009,0,0.0938791,"gn the inventory here with other ongoing efforts to annotate within-document event-event relations. Table 5 shows a mapping between a subset of the relations proposed here and those used in the Richer Event Descriptions (RED) (Ikuta et al., 2014). Other similar resources – such as Penn Discourse Treebank (Miltsakaki et al., 2004) – could also be used. 5 RED RED (Ikuta et al., 2014) is in general more coarsegrained and has fewer types and subtypes. Event-event relations differ from textual entailment (Dagan et al., 2013) or discourse relations (Soricut and Marcu, 2003; Miltsakaki et al., 2004; Radev, 2000), which focus on the relatedness between two sentences, by tackling a full document or multiple documents. We adopted some terminology (e.g., Causality and Expansion) from the taxonomy of discourse relations (Miltsakaki et al., 2004). We focus on a wider scope of cross-document events with richer and more finegrained structured event representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus"
W16-1701,W13-4004,0,0.0210329,"aints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, and Bernardo Magnini. 2015. Opposition re"
W16-1701,N03-1030,0,0.386271,"en two events. This task can provide building blocks for many important applications such as event knowledge base population and temporal event tracking (Do et al., 2012). The nodes can be extracted by existing fine-grained event extraction approaches (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2014). However, little previous work can be directly exploited to construct the edges. In this paper we define a comprehensive schema that includes multiple fine-grained event-event relation types. Some types are similar to those in discourse parsing (Soricut and Marcu, 2003). 1 http://projects.ldc.upenn.edu/ace https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/ files/english-events-guidelines-v5.4.3.pdf 2 1 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 1–6, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Event 1 Event 2 T ype End-Position Start Position T rigger quit replace P erson Barry Diller Jean-Rene P osition chief chief executive Organization Vivendi U.E. U.S. unit t Event2 o Contingency.Condition T emporality.Before-After Boston M arathon Bombings Sub-event o bombings O e4 o Event1 ment. Event 2:"
W16-1701,W15-0813,0,0.0250267,"onjunction Conjunction Figure 2: A hierarchical event network 3.2 Contingency and Comparison A Contingency relation indicates either an event leading to the emergence (Causality) or serving as a triggering condition (Conditional) of another event. Comparison relations indicate deeper logical contrasts between relations. Opposition indicates a relation in which two events are mutually contradictory, and unlikely to be both true. This has some similarity to Contrast.Opposition in the Penn Discourse Treebank (Miltsakaki et al., 2004) or specific annotations of opposition (Feltracco et al., 2015; Takabatake et al., 2015). Negation indicates that while two events could both be true, one shows that the other is no longer true. Competition shows that two events are contrasting versions of the same underlying “event” (e.g., retreat versus escape in disorder). Event-Event Relation Schema Our event-event relation schema includes 5 main Types – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 Subtypes as shown in Table 1. Table 1 also demonstrates Roles. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency.Causality will pl"
W16-1701,S13-2001,0,0.0549046,"t representations. If we consider each event-event relation instance as a frame (e.g., a contingency/causality event-event relation is similar to the frame causation), the architecture of the Event Networks is also similar to FrameNet (Baker and Sato, 2003) and thus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with othe"
W16-1701,D15-1020,1,0.85988,"hus the ontological analysis and constraints in (Ovchinnikova et al., 2010) are also applicable to our task. Related Work The proposed schema covers event-event relation types that have been widely studied: (Styler IV et al., 2014; Bethard, 2013; Allen, 1983; Miller et al., 2013; Pustejovsky and Stubbs, 2011; Pustejovsky et al., 2005; UzZaman et al., 2013) also focused on the relation types which are related to Temporality. Methods about extracting Coreference relations have also been discussed and proposed in (Chen and Ji, 2009; Chen et al., 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Zhang et al., 2015). (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014) work on Causality relation. Similar event-event relation schema such as 6 Conclusions and Future Work Our work will expand the research venue of IE from entity-centric to event-centric. In the future we will further expand the corpus3 , and compare and integrate with other within-document eventevent relation schemas such as RED. We also plan to develop a pilot system using these resources. 3 The annotated corpus is available at http://nlp. cs.rpi.edu/data/event_relation.zip 4 Acknowledgements Anna Feltracco, Elisabetta Jezek, a"
W16-1701,P11-1113,1,\N,Missing
W16-1701,W15-0803,0,\N,Missing
W16-1701,Q14-1012,1,\N,Missing
W16-1701,W14-2903,1,\N,Missing
W16-1712,P08-1037,0,0.0242269,"of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clu"
W16-1712,bonial-etal-2014-propbank,1,0.91389,"Missing"
W16-1712,W07-1604,0,0.0246655,"his paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and ev"
W16-1712,W06-1670,0,0.0856608,"asses that label a large number of word types (i.e., they are unlexicalized). The best-known supersense scheme draws on two inventories—one for nouns and one for verbs—which originated as a high-level partitioning of senses in WordNet (Miller et al., 1990). A scheme for adjectives has been proposed as well (Tsvetkov et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘ind"
W16-1712,D09-1047,0,0.77986,"ities, prepositions serve as essential linkers of meaning, and the few extremely frequent ones are exploited for many different functions (figure 1). For all their importance, however, prepositions have received relatively little attention in computational semantics, and the community has not yet arrived at a comprehensive and reliable scheme for annotating the semantics of prepositions in context (§2). We believe that such annotation of preposition functions is needed if preposition sense disambiguation systems are to be useful for downstream tasks—e.g., translation3 or semantic parsing (cf. Dahlmeier et al., 2009; Srikumar and Roth, 2011). This paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and"
W16-1712,hashemi-hwa-2014-comparison,0,0.105801,"tated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotate"
W16-1712,L16-1629,1,0.821862,"opBank expertise) checked the gold PropBank annotations, agreeing that 5 of the tokens were clearly incorrect. This analysis tells us that obvious errors with both types of annotation are indeed present in the corpus (11 tokens in the sample), adding some noise to the supersense–function tag correspondences. However, the outright errors are probably dwarfed by difficult/borderline cases for which the annotations are not entirely consistent throughout the corpus. For example, on time (i.e., ‘not late’) is variously annotated as S TATE, M ANNER, and T IME. Inconsistency detection methods (e.g., Hollenstein et al., 2016) may help identify these— though it remains to be seen whether methods developed for nouns and verbs would succeed on function words so polysemous as prepositions. Summary. The (mostly) clean correspondences of the supersenses to the independently annotated PropBank modifier labels speak to the linguistic validity of our supersense hierarchy. On the other hand, the confusion evident for the supersense labels corresponding to PropBank’s numbered arguments suggests further analysis and refinement is necessary for both annotation schemes. Some of these issues—especially correspondences between la"
W16-1712,C10-2052,0,0.267964,"arners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previ"
W16-1712,S14-1001,0,0.0160165,"al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English preposition"
W16-1712,P14-1120,0,0.236706,"s-based lexicography centered around individual preposition types. Most previous datasets of English preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).4 We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus, covering the full range of usages possible for all English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, since sentences were sampled separately for each preposition, there is only one annotated preposition token per sentence. By contrast, we will fully annotate documents for all preposition tokens. No interannotator agreement figures have been reported for the PDEP data to indicate its quality, or the overall difficulty of token annotation with TPP senses across a broad range of prepositions. 2.2 Supersenses From the literature on other kinds of supersenses, there is reason to believe that t"
W16-1712,S07-1005,0,0.110407,"Missing"
W16-1712,W03-0411,0,0.0816644,"Missing"
W16-1712,J05-1004,1,0.526768,"Missing"
W16-1712,picca-etal-2008-supersense,0,0.131684,"rdNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they gro"
W16-1712,N13-1076,1,0.852834,"for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they group senses from a lexicon rather than directly annotati"
W16-1712,P12-2050,1,0.867655,"v et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen"
W16-1712,schneider-etal-2014-comprehensive,1,0.808201,"with sparse training data. The Supersense Hierarchy Unlike the noun, verb, and adjective supersense schemes mentioned in §2.2, the preposition supersense inventory is hierarchical (as are Litkowski’s (2015) and Müller et al.’s (2012) inventories). The hierarchy, depicted in figure 2, encodes inheritance: 6 http://tiny.cc/prepwiki 101 3 3.1 Corpus Annotation Annotating Preposition Supersenses Source data. We fully annotated the R EVIEWS section of the English Web Treebank (Bies et al., 2012), chosen because it had previously been annotated for multiword expressions, noun and verb supersenses (Schneider et al., 2014; Schneider and Smith, 2015), and PropBank predicate-argument structures (§4). The corpus comprises 55,579 tokens organized into 3,812 sentences and 723 documents with gold tokenization and PTB-style POS tags. Identifying preposition tokens. TPP, and therefore PrepWiki, contains senses for canonical prepositions, i.e., those used transitively in the [PP P NP] construction. Taking inspiration from Pullum and Huddleston (2002), PrepWiki further assigns supersenses to spatiotemporal particle uses of out, up, away, together, etc., and subordinating uses of as, after, in, with, etc. (including infi"
W16-1712,N15-1177,1,0.85174,"ta. The Supersense Hierarchy Unlike the noun, verb, and adjective supersense schemes mentioned in §2.2, the preposition supersense inventory is hierarchical (as are Litkowski’s (2015) and Müller et al.’s (2012) inventories). The hierarchy, depicted in figure 2, encodes inheritance: 6 http://tiny.cc/prepwiki 101 3 3.1 Corpus Annotation Annotating Preposition Supersenses Source data. We fully annotated the R EVIEWS section of the English Web Treebank (Bies et al., 2012), chosen because it had previously been annotated for multiword expressions, noun and verb supersenses (Schneider et al., 2014; Schneider and Smith, 2015), and PropBank predicate-argument structures (§4). The corpus comprises 55,579 tokens organized into 3,812 sentences and 723 documents with gold tokenization and PTB-style POS tags. Identifying preposition tokens. TPP, and therefore PrepWiki, contains senses for canonical prepositions, i.e., those used transitively in the [PP P NP] construction. Taking inspiration from Pullum and Huddleston (2002), PrepWiki further assigns supersenses to spatiotemporal particle uses of out, up, away, together, etc., and subordinating uses of as, after, in, with, etc. (including infinitival to and infinitival-s"
W16-1712,W15-1612,1,0.871481,"own University Jena D. Hwang IHMC Vivek Srikumar University of Utah jhwang@ihmc.us svivek@cs.utah.edu nschneid@inf.ed.ac.uk Meredith Green Abhijit Suresh Kathryn Conger Tim O’Gorman University of Colorado at Boulder Martha Palmer {laura.green,abhijit.suresh,kathryn.conger,timothy.ogorman,martha.palmer}@colorado.edu Abstract (1) I have been going to/D ESTINATION the Wildwood_,_NJ for/D URATION over 30 years for/P URPOSE summer~vacations We present the first corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). The preposition supersenses are organized hierarchically and designed to facilitate comprehensive manual annotation. Our dataset is publicly released on the web.1 1 (2) It is close to/L OCATION bus_lines for/D ESTINATION Opera_Plaza (3) I was looking~to/`i bring a customer to/D ESTINATION their lot to/P URPOSE buy a car Figure 1: Preposition supersenses illustrating the polysemy of to and for. Both can mark a D ESTINATION or P URPOSE, while there are other functions that do not overlap. The syntactic complement use of infinitival to is tagged as `i. The over token in (1) receives the label A"
W16-1712,W97-0811,0,0.211554,"itions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic function"
W16-1712,W12-0514,0,0.0797358,"ew corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requir"
W16-1712,D11-1012,1,0.950338,"e as essential linkers of meaning, and the few extremely frequent ones are exploited for many different functions (figure 1). For all their importance, however, prepositions have received relatively little attention in computational semantics, and the community has not yet arrived at a comprehensive and reliable scheme for annotating the semantics of prepositions in context (§2). We believe that such annotation of preposition functions is needed if preposition sense disambiguation systems are to be useful for downstream tasks—e.g., translation3 or semantic parsing (cf. Dahlmeier et al., 2009; Srikumar and Roth, 2011). This paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation system"
W16-1712,Q13-1019,1,0.913045,"translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of English p"
W16-1712,N09-3017,0,0.440547,"ly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of English preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).4 We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus, covering the full range of usages possible for all English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, since sentences were sampled separately for each preposition, there is only one annotated preposition t"
W16-1712,D11-1116,0,0.0234141,"et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they group senses from a lexicon rather than directly annotating tokens, and restrict each sense 5 php 100 http://www.clres.com/db/classes/ClassAnalysis. Superset Possessor Co-Agent Creator Whole Elements Instance Agent Species Causer Quantity Configuration Patient Accompanier Co-Patient Undergoer Reciprocation Purpose Theme Participant Experiencer Stimulus Via Place Value Path Manner Time Frequency Duration Temporal Circumstance Extent Location Beneficiary Ins"
W16-1712,D15-1243,0,0.0468006,"Missing"
W16-1712,W13-0906,0,0.0262783,"nemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hov"
W16-1712,tsvetkov-etal-2014-augmenting-english,1,0.844664,", a disambiguation system trained on this dataset will therefore be biased and perform poorly on an ecologically valid sample of tokens. preposition supersenses (Schneider et al., 2015) will be more scalable and useful than senses. The term supersense has been applied to lexical semantic classes that label a large number of word types (i.e., they are unlexicalized). The best-known supersense scheme draws on two inventories—one for nouns and one for verbs—which originated as a high-level partitioning of senses in WordNet (Miller et al., 1990). A scheme for adjectives has been proposed as well (Tsvetkov et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al"
W16-1712,S07-1051,0,0.3404,"ing second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual prepositio"
W16-1712,W15-1806,0,\N,Missing
W16-1712,S16-1084,1,\N,Missing
W16-5706,W13-2322,1,0.935194,"ormation in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion of corpora, only a few of the above resources attempt to provide an integrated annotation of many different event-event relations. Minard et al. (2016) annotated event and entity coreference and temporal relations (as well as semantic roles and cross-document coreference), but omitted both subevent structure and causal relations. Glavas and Snajder (2014) annotated event coreference and subevent relations, but did not capture tem48 poral or causal st"
W16-5706,bethard-etal-2008-building,0,0.0619472,"fazadeh et al., 2016), and bears practical similarity to the decisions in Hong et al. (2016) to allow multiple labels between two events, or the layered annotation of Mirza et al. (2014) on top of temporal structure. This annotation aims towards logical definitions for cause and preconditions outlined in Ikuta et al (2014). This defines CAUSES as being true “if, according to the writer, the particular EVENT Y was inevitable given the particular EVENT X.”, and PRECONDITION as being true when, “had the particular EVENT X not happened, the particular EVENT Y would not have happened.”. Following (Bethard et al., 2008; Bethard, 2007; Prasad et al., 2008), those logical definitions were supplemented by guidelines for particular contexts, and for paraphrasing with particular implicit connectives, and case-by-case guideline for specific problematic frames, to handle edge cases which where challenging for classification by logical definition alone. Table 1 illustrates an example in which all four relations are illustrated: The ouster of Morsi and the subsequent suppression of the Brotherhood has enraged the groups members and led to a spate of scapegoating attacks by Muslim extremists ouster BEFORE / CAUSES en"
W16-5706,cybulska-vossen-2014-using,0,0.0299548,"tion and coreference exist in a number of forms. The original MUC tasks dealt with events and scenarios that fit within a particular ontology, and such ontology-driven event annotations have been extended through the ACE and ERE corpora and through the TAC-KBP evaluations (Humphreys et al. 1997, Bagga and Baldwin 1999, Song et al 2015). Unrestricted event coreference annotations were later developed in OntoNotes (Weischedel et al., 2011) – which annotated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate ca"
W16-5706,N13-1112,0,0.153195,"Missing"
W16-5706,W15-1622,0,0.0468473,"Missing"
W16-5706,W14-3705,0,0.047911,"otated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion of corpora, only a few of the above resources attempt to provide an integrated annotation of many different event-event relations. Minard e"
W16-5706,glavas-etal-2014-hieve,0,0.11591,"Missing"
W16-5706,W16-1701,1,0.911594,"forms. The original MUC tasks dealt with events and scenarios that fit within a particular ontology, and such ontology-driven event annotations have been extended through the ACE and ERE corpora and through the TAC-KBP evaluations (Humphreys et al. 1997, Bagga and Baldwin 1999, Song et al 2015). Unrestricted event coreference annotations were later developed in OntoNotes (Weischedel et al., 2011) – which annotated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event"
W16-5706,W14-2903,1,0.862418,"be viewed by annotators as inferrable, and therefore does not need to be annotated. RED guidelines furthermore limit B E FORE and OVERLAP relations to contexts in which the relation is perceived by an annotator to be explicitly expressed in the context. Section details more nuanced agreement results of temporal annotation. 3.4 Causal Annotation Causation has often been divided into CAUSE, ENABLE and PREVENT, as outlined in Hobbs (2005) and Wolff (2007), and implemented in Mirza et al. (2014) and Mostafazadeh et al. (2016). RED annotation, based on preliminary studies of causal annotation in (Ikuta et al., 2014), adopted a twoway distinction between C AUSES and P RECONDI TION similar to the distinction often made between “Cause” and “enable”. RED represents “prevent” relations simply through polarity (being the cause or precondition for a negated event), which does re51 quire that all prevented events have a negated polarity. These C AUSES and P RECONDITION labels have been noted to generally combine with temporal information, and therefore annotators annotate causality with one of four fused labels: B EFORE /C AUSES, OVERLAP /C AUSES, B EFORE /P RECONDITION, and OVERLAP /P RECONDITION. This distinct"
W16-5706,D13-1027,0,0.0270382,"rding to whether or not they represent an actual discourse referent in the discourse. Such an annotation could easily be adapted to OntoNotesstyle annotation (by stripping out the singletons), but adds information that could be very useful for detection of the anaphoricity of mentions, a factor considered to be very useful in coreference resolution (Harabagiu et al., 2001, Ng and Cardie 2002). In RED annotation, these entities and events are also labeled using a minimal-span approach in which only the headwords are labeled. This annotation style may reduce the “span match” errors observed by (Kummerfeld and Klein, 2013) in recent systems, and some researchers working on coreference have observed the utility of focusing upon headwords, with (Peng et al., 2015) claiming that “identifying and co-referring mention heads is not only sufficient but is more robust than working with complete mentions” (Peng et al. 2015:1). Richer Event Description also annotates events and entities with a representation of the polarity and modality of the events and entities in context, making a four-way distinction between AC TUAL , G ENERIC , H EDGED /U NCERTAIN , OR H YPOTHETICAL , and temporal expressions are distinguished into"
W16-5706,D12-1045,0,0.0272156,"ora for event detection and coreference exist in a number of forms. The original MUC tasks dealt with events and scenarios that fit within a particular ontology, and such ontology-driven event annotations have been extended through the ACE and ERE corpora and through the TAC-KBP evaluations (Humphreys et al. 1997, Bagga and Baldwin 1999, Song et al 2015). Unrestricted event coreference annotations were later developed in OntoNotes (Weischedel et al., 2011) – which annotated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events direct"
W16-5706,P10-1081,0,0.00893782,"thin event annotation are an The fundamental contribution of the corpus is one in which a wide range of event-event and event coreference relations are annotated in a consistent and integrated manner. By capturing coreference, bridging, temporal, causal and subevent relations in the same annotation, the annotations may provide a more integrated sense of how the events in a particular document relate to each other, and encourage the development of systems that learn rich interactions between systems. Rich interactions between events in a text, moreover, may be useful for a wide range of goals; Liao and Grishman (2010) found that looking at related events within a document could aid ACE-style event detection, and Vossen et al. (2015) discussed the value of combining timelines with bridging and causal relations in the construction of storylines. This paper covers the details of RED annotation, and illustrates a number of annotation methods used to overcome the challenges of annotating such a rich 47 Proceedings of 2nd Workshop on Computing News Storylines, pages 47–56, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics inventory. We suggest that the advantages of annotating many d"
W16-5706,L16-1699,0,0.0812201,"Missing"
W16-5706,W14-0702,0,0.100304,"Unrestricted event coreference annotations were later developed in OntoNotes (Weischedel et al., 2011) – which annotated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion of corpora, only a f"
W16-5706,W16-1007,0,0.432118,"win 1999, Song et al 2015). Unrestricted event coreference annotations were later developed in OntoNotes (Weischedel et al., 2011) – which annotated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion"
W16-5706,J05-1004,1,0.224328,"l structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion of corpora, only a few of the above resources attempt to provide an integrated annotation of many different event-event relations. Minard et al. (2016) annotated event and entity coreference and temporal relations (as well as semantic roles and cross-document coreference), but omitted both subevent structure and causal relations. Glavas and Snajder (2014) annotated event coreference and subevent relations, but did not capture tem48 poral or causal structure. Hong et al, (2016) annotated a wide inventory of event-event relations, but covered only events within t"
W16-5706,K15-1002,0,0.0410212,"tation (by stripping out the singletons), but adds information that could be very useful for detection of the anaphoricity of mentions, a factor considered to be very useful in coreference resolution (Harabagiu et al., 2001, Ng and Cardie 2002). In RED annotation, these entities and events are also labeled using a minimal-span approach in which only the headwords are labeled. This annotation style may reduce the “span match” errors observed by (Kummerfeld and Klein, 2013) in recent systems, and some researchers working on coreference have observed the utility of focusing upon headwords, with (Peng et al., 2015) claiming that “identifying and co-referring mention heads is not only sufficient but is more robust than working with complete mentions” (Peng et al. 2015:1). Richer Event Description also annotates events and entities with a representation of the polarity and modality of the events and entities in context, making a four-way distinction between AC TUAL , G ENERIC , H EDGED /U NCERTAIN , OR H YPOTHETICAL , and temporal expressions are distinguished into DATE , TIME , DURATION , QUANTI FIER , PREPOSTEXP and SET , following the ThymeTimeML annotation of clinical temporal expressions (Styler IV e"
W16-5706,W97-1301,0,0.344278,"016 96.5 96.5 98.3 91.5 92.0 77.2 85.1 88.7 85.5 75.6 90.3 79.5 Table 1: Agreement F1 for Eventy, Event and TIMEX 3 detection. Scores for features only measured when annotators agree that an event exists. Highest reported scores on Tempeval-2016 (a corpus annotated with similar event guidelines) are reported to give an approximation of system performance all entities in the document, alongside annotation of apposition relations and three bridging relations. The bridging relations are important for capturing a range of anaphora phenomena that are not strict identity relationships (Clark, 1975; Poesio et al., 1997). S ET /M EMBER was a label used both for setsubset and set-member relationships, PART /W HOLE captured relationships between entities that physically composed a larger whole, and a general B RIDGING relation was used for any class of bridging that did not fit into other categories, such as events of differing modality, allegations of identity (such as links between “the murderer” and a particular suspect). The fact that this annotation explicitly labels modality and polarity features can have important consequences for coreference and bridging annotation. Even annotations which do not annotat"
W16-5706,P14-2006,0,0.0167923,"ed to pronouns referring to them, or in specific headline constructions). This means that annotator behavior is dependent upon a separate decision (whether or not a markable is generic) that is never explicitly annotated. RED explicitly annotates modality, and constrains IDEN TITY relations to only apply to be between elements with the same modality and polarity, and providing bridging relations to capture relations that do not pass this strict definition of identity. We evaluate entity coreference scores using the reference implementation of a variety of scoring metrics that was provided in (Pradhan et al., 2014), which are shown in Table 2. All agreement numbers are scored on a 55-document subset of the corpus sampled from discussion fora and newswire documents. entity IAA vs gold muc 75.3 80.06 bcub 68.5 85.0 ceafe 67.6 79.5 conll f1 70.4 81.8 Table 2: Agreement scores between annotators and agreement with gold, for Entity coreference Table 3 shows the scores of entity-entity and event-event bridging relations. Entity set/member whole/part bridging apposition ITA 21.5 25.8 7.1 51.2 gold 46.5 56.3 25.6 67.6 3.2 Event Coreference and Event Bridging After the adjudication of event and entity markables,"
W16-5706,prasad-etal-2008-penn,0,0.0613438,"et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion of corpora, only a few of the above resources attempt to provide an integrated annotation of many different event-event relations. Minard et al. (2016) annotated event and entity coreference and temporal relations (as well as semantic roles and cross-document coreference), but omitted both subevent structure and causal relations. Glavas and Snajder (2014) annotated event coreference and subevent relations, but did not capture tem48 poral or causal structure. Hong et al, (2016) annotated a wid"
W16-5706,W11-0419,0,0.0666999,"2014). Figure 1 shows both the accuracy of annotations on these phenomena, and the best performance of systems on a the Tempeval2016 task, which was on the similarly annotated Thyme data. Modality guidelines were also added to allow annotation of entity modality, primarily to capture reference to generic entities. A number of additional characteristics of events are annotated (such as intermittence (CONTEXTUAL ASPECT ) and whether the event was explicit or implied), but the important additional feature is that of the DocTimeRel, or relationship to document time. Following the methodology of (Pustejovsky and Stubbs, 2011; Styler IV et al., 2014), annotators assume four implicit narrative containers within each document – B EFORE , OVERLAP, B E FORE /OVERLAP or A FTER document time – and each event is labeled with the best such container. This obviates the necessity of labeling many of the more obvious temporal relations (such as knowing that events in the past happen before events in the future). As can be seen in Table 1, agreement of annotators with the adjudicated gold is very high for such DocTimeRel annotations, and system performance in the clinical domain for this kind of annotation is promising. Coref"
W16-5706,W13-4004,0,0.0135338,"IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016; Mirza et al., 2014; Hong et al., 2016; Dunietz et al., 2015). Subevent relations corpora have also been annotated (Glava and najder, 2014; Hong et al., 2016). In addition to those resources, there are many corpora which do not focus on annotating events directly, but which do annotate causal, temporal, event-structural or coreference relations within limited scopes, such as within the same clause or across adjacent sentences (Banarescu et al., 2013; Carlson et al., 2003; Prasad et al., 2008; Riaz and Girju, 2013; Fillmore and Baker, 2000; Palmer et al., 2005). However, despite the profusion of corpora, only a few of the above resources attempt to provide an integrated annotation of many different event-event relations. Minard et al. (2016) annotated event and entity coreference and temporal relations (as well as semantic roles and cross-document coreference), but omitted both subevent structure and causal relations. Glavas and Snajder (2014) annotated event coreference and subevent relations, but did not capture tem48 poral or causal structure. Hong et al, (2016) annotated a wide inventory of event-e"
W16-5706,W15-1612,1,0.345875,"Missing"
W16-5706,W15-0812,0,0.0781081,"ciation for Computational Linguistics inventory. We suggest that the advantages of annotating many different event-event phenomena at once can outweigh those challenges. Our corpus and guidelines will be made publicly available. 2 Related Work Large-scale corpora for event detection and coreference exist in a number of forms. The original MUC tasks dealt with events and scenarios that fit within a particular ontology, and such ontology-driven event annotations have been extended through the ACE and ERE corpora and through the TAC-KBP evaluations (Humphreys et al. 1997, Bagga and Baldwin 1999, Song et al 2015). Unrestricted event coreference annotations were later developed in OntoNotes (Weischedel et al., 2011) – which annotated event coreference but did not explicitly differentiate events and entities – and in cross-document event corpora such as Lee et al. (2012), Cybulska and Vossen (2014), Minard et al (2016) and Hong et al. (2016). Corpora for event-event and event-time relations have also been developed, both for temporal information in the TimeML tradition (Pustejovsky et al., 2003; Styler IV et al., 2014; Minard et al., 2016), and causal structure (Bethard, 2007; Mostafazadeh et al., 2016;"
W16-5706,Q13-1019,0,0.0226733,"Missing"
W16-5706,P11-2061,0,0.088625,"Missing"
W16-5706,W15-4507,0,0.0143496,"vent coreference relations are annotated in a consistent and integrated manner. By capturing coreference, bridging, temporal, causal and subevent relations in the same annotation, the annotations may provide a more integrated sense of how the events in a particular document relate to each other, and encourage the development of systems that learn rich interactions between systems. Rich interactions between events in a text, moreover, may be useful for a wide range of goals; Liao and Grishman (2010) found that looking at related events within a document could aid ACE-style event detection, and Vossen et al. (2015) discussed the value of combining timelines with bridging and causal relations in the construction of storylines. This paper covers the details of RED annotation, and illustrates a number of annotation methods used to overcome the challenges of annotating such a rich 47 Proceedings of 2nd Workshop on Computing News Storylines, pages 47–56, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics inventory. We suggest that the advantages of annotating many different event-event phenomena at once can outweigh those challenges. Our corpus and guidelines will be made publicly"
W16-6201,D11-1145,0,0.0252218,"ituational awareness. 2.2 Tweet Classification Identifying relevant information in social media is challenging due to the low signal-to-noise ratio. A number of researchers have used NLP to address this challenge. There is significant work in the medi1 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 1–6, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics cal domain related to identifying health crises and events in social media data. Multiple studies have been done to analyze flu-related tweets (Culotta, 2010; Aramaki et al., 2011). Most closely related to our work (but in a different domain) is the flu classification system of Lamb et al. (2013), which first classifies tweets for relevance and then applies finergrained classifiers. Similar systems have been developed to categorize tweets in more general domains, for example by identifying tweets related to news, events, and opinions (Sankaranarayanan et al., 2009; Sriram et al., 2010). Similar classifiers have been developed for sentiment analysis (Pang and Lee, 2008) to identify and categorize sentiment-expressing tweets (Go et al., 2009; Kouloumpis et al., 2011). 3 D"
W16-6201,N13-1097,1,0.670448,"e low signal-to-noise ratio. A number of researchers have used NLP to address this challenge. There is significant work in the medi1 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 1–6, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics cal domain related to identifying health crises and events in social media data. Multiple studies have been done to analyze flu-related tweets (Culotta, 2010; Aramaki et al., 2011). Most closely related to our work (but in a different domain) is the flu classification system of Lamb et al. (2013), which first classifies tweets for relevance and then applies finergrained classifiers. Similar systems have been developed to categorize tweets in more general domains, for example by identifying tweets related to news, events, and opinions (Sankaranarayanan et al., 2009; Sriram et al., 2010). Similar classifiers have been developed for sentiment analysis (Pang and Lee, 2008) to identify and categorize sentiment-expressing tweets (Go et al., 2009; Kouloumpis et al., 2011). 3 Data 3.1 Collection In late October 2012, Hurricane Sandy generated a massive, disperse reaction in social media chann"
W16-6201,D11-1141,0,0.0113252,"seline P R .80 .56 .44 .19 .57 .24 .04 .04 .44 .23 .76 .40 .64 .26 All Features F1 P R .71 .81 .64 .39 .46 .35 .48 .57 .41 .07 .10 .07 .36 .41 .32 .73 .71 .75 .53 .58 .49 Best Features F1 P R .72 .79 .66 .41 .42 .40 .49 .50 .49 .08 .10 .07 .36 .38 .35 .75 .71 .80 .52 .52 .52 Table 2: Results for relevance and fine-grained classification. uational awareness vs not. We used these four Verma classifiers to tag our Hurricane Sandy dataset and included these tags as features. • We included n-grams augmented with their partof-speech tags, as well as named entities, using the Twitter-based tagger of Ritter et al. (2011). 4.3 • Word embeddings have been used extensively in recent NLP work, with promising results (Goldberg, 2015). A Word2Vec model (Mikolov et al., 2013) was trained on the 22.2M tweets collected from the Hurricane Sandy dataset, using ˇ uˇrek and Sojka, 2010), the Gensim package (Reh˚ using the C-BOW algorithm with negative sampling (n=5), a window of 5, and with 200 dimensions per word. For each tweet, the mean embedding of all words was used to create 200 features. Classification performance was measured using fivefold cross-validation. We conducted an ablation study (Figure 1), removing indi"
W17-2712,W14-2903,1,0.894927,"Missing"
W17-2712,doddington-etal-2004-automatic,0,0.456867,"Missing"
W17-2712,J88-2003,0,0.860806,"Missing"
W17-2712,W16-5706,1,0.867929,"Missing"
W17-2712,W15-0812,0,0.133177,"Missing"
W17-2812,W10-2903,1,0.74867,"e language in the context of that task (i.e. to map between utterances and meaning representations the problem solving components of the agent can act on in a particular situation). The agent may also need to initiate clarification requests when communication fails, and to learn new domain (or conversation) specific vocabulary and its meaning. This kind of symmetric, grounded communication with a problem-solving agent goes significantly beyond the one-step, single direction understanding tasks considered in standard semantic parsing (e.g. Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al., 2010) or even short, simple instructions to robots (e.g. Tellex et al., 2011). In order to focus on these concept learning and communication issues, we deliberately limit ourselves here to a simple, simulated environment. 95 Proceedings of the First Workshop on Language Grounding for Robotics, pages 95–103, c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics Figure 1: A complex shape, which can be viewed as conjunction of simpler known shapes: a row (dark, width = 4) and a square (light, size = 3). 2 Domain and Problem Setup We consider a two-dimensional (2"
W17-2812,P16-1004,0,0.0294175,"= h{hsi , idi , ∧k di i}i∈S , ∧j∈[S×S] fj i, where In this section, we describe the current implementations of the different modules (language comprehension, memory, problem solving, language production, and dialogue mediation) in COG, noting that the architecture is flexible and allows for us to plug-in other implementations as needed. 4.1 Language Comprehension The LC module consists of semantic parsing and language grounding components. 4.1.1 Language Grounding Semantic Parsing Our semantic parser is implemented as a neural sequence-to-sequence model with attention (Bahdanau et al., 2014; Dong and Lapata, 2016; Jia and Liang, 2016). The model consists of two LSTMs. The first LSTM (the encoder) processes the input sentence x = (x1 , . . . , xm ) token-by-token, producing a sequence of hidden states hs = (hs1 , . . . , hsm ) as output. The second LSTM (the decoder) models a distribution P (yi |y1 , . . . , yi−1 ; hs ) at each time step over output tokens as a function of the encoder hidden states and the previous outputs. The final parse y = (y1 , . . . , yn ) is obtained by selecting the token at each time step that maximizes this probability and feeding a learned embedding for it into (k) S is the"
W17-2812,W03-2316,0,0.0595628,"expect to see a large number of examples. We will consider the use of probabilistic logic models which can handle both issues by explicitly including the trade-off in the optimization function (Odom et al., 2015). A final challenge is the application of our agent to new domains. Currently, the memory module contains all the knowledge required to plan and produce comprehensible responses. This declarative approach should generalize well to some simple enough domains, but will need to be extended to deal with more involved tasks and domains. veloped a grammar-based realizer inspired by OpenCCG (White and Baldridge, 2003; White, 2006) that operates over the first-order semantic representations used by our agent. We plan to augment the realizer’s semantic lexicon with the learned definitions of predicates for new shapes, allowing our system to generate natural language instructions describing the new configurations. One of the key challenges of the scenario we envision (and a fundamental problem in language acquisition) is the necessity to generalize across situations. This is required in order to learn general concepts from a few specific instances. At this point, our agent is able to generalize from a single"
W17-2812,P16-1154,0,0.0142285,"ere the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquired predicates that are used by LC and LP."
W17-2812,P16-1014,0,0.0162333,"entence-by-sentence, where the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquired predicates that are u"
W17-2812,P16-1002,0,0.050247,"nputs are processed sentence-by-sentence, where the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquire"
W18-0903,W15-1406,0,0.0179275,"ide an accurate semantic interpretation for an utterance (here called metaphor interpretation). The first has largely been approached as a supervised machine learning problem, typically using lexical semantic features and their interaction with context to learn the kinds of situations where lexical metaphors appear. The problem of metaphor interpretation is more complex, with approaches including the implementation of full metaphoric interpretation systems (Martin, 1990), (Ovchinnikova et al., 2014), identification of source and target domains (Dodge et al., 2015), developing knowledge bases (Gordon et al., 2015), and providing literal paraphrases to metaphoric phrases (Shutova, 2010), (Shutova, 2013). In both identification and interpretation systems, syntax tends to play a limited role. Many systems rely only on lexical semantics of target words, or use only minimal context or dependency relations to help disambiguate in context (Gargett and Barnden, 2015), (Rai et al., 2016). Others rely on topic modeling and other document and sentence level features to provide general semantics, and compare the lexical semantics to that, ignoring the more ”middle”-level syntactic interactions (Heintz et al., 2013"
W18-0903,P16-1018,0,0.0217543,"ating 17 Proceedings of the Workshop on Figurative Language Processing, pages 17–26 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics been a variety of approaches that incorporate syntactic information. Many computational approaches focus on specific constructions, perhaps indicating the need to classify different metaphoric constructions through different means. The dataset of (Tsvetkov et al., 2014) provides adjective-noun annotation which has been extensively studied (Rei et al., 2017), (Bulat et al., 2017). A particularly promising approach is that of (Gutierrez et al., 2016), who use compositional distributional semantic models (CDSMs) to represent metaphors as transformations in vector space, specifically for adjective-noun constructions. Another relevant approach is that of (Haagsma and Bjerva, 2016) who use clustering and selectional preference information to detect metaphors in predicate argument constructions, including verbs with objects, subjects, and both. Their highest F1 is 57.8 for verbs with both arguments. Many systems that rely heavily on lexical resources also include some dependency information. (Rai et al., 2016) and (Gargett and Barnden, 2015) u"
W18-0903,W16-1102,0,0.0245167,"ormation. Many computational approaches focus on specific constructions, perhaps indicating the need to classify different metaphoric constructions through different means. The dataset of (Tsvetkov et al., 2014) provides adjective-noun annotation which has been extensively studied (Rei et al., 2017), (Bulat et al., 2017). A particularly promising approach is that of (Gutierrez et al., 2016), who use compositional distributional semantic models (CDSMs) to represent metaphors as transformations in vector space, specifically for adjective-noun constructions. Another relevant approach is that of (Haagsma and Bjerva, 2016) who use clustering and selectional preference information to detect metaphors in predicate argument constructions, including verbs with objects, subjects, and both. Their highest F1 is 57.8 for verbs with both arguments. Many systems that rely heavily on lexical resources also include some dependency information. (Rai et al., 2016) and (Gargett and Barnden, 2015) use a variety of syntactic features including lemma, part of speech, and dependency relations. However, both systems are feature-rich and these syntactic elements’ contribution is unclear. (?) use lexical features along with contrast"
W18-0903,W13-0908,0,0.0320659,"ordon et al., 2015), and providing literal paraphrases to metaphoric phrases (Shutova, 2010), (Shutova, 2013). In both identification and interpretation systems, syntax tends to play a limited role. Many systems rely only on lexical semantics of target words, or use only minimal context or dependency relations to help disambiguate in context (Gargett and Barnden, 2015), (Rai et al., 2016). Others rely on topic modeling and other document and sentence level features to provide general semantics, and compare the lexical semantics to that, ignoring the more ”middle”-level syntactic interactions (Heintz et al., 2013). While these approaches have been effective in many areas, there is evidence that figurative language is significantly influenced by syntactic constructions, and thus if they can be represented more effectively, metaphor processing Identification of metaphoric language in text is critical for generating effective semantic representations for natural language understanding. Computational approaches to metaphor identification have largely relied on heuristic based models or feature-based machine learning, using hand-crafted lexical resources coupled with basic syntactic information. However, re"
W18-0903,W13-0907,0,0.195273,"systems are feature-rich and these syntactic elements’ contribution is unclear. (?) use lexical features along with contrasting those features between the target word and its head. (Dodge et al., 2015) employ a variety of constructions in identifying metaphoric source and target domains. They identify a broad range of constructions and use these as templates that metaphoric expressions can fill. Our work expands on this idea by formalizing the constructions into features for statistical metaphor identification. Perhaps the most syntactically oriented metaphor identification system is that of (Hovy et al., 2013), who uses syntactic tree kernels to identify metaphor. They use combinations of syntactic features via tree kernels and semantics via WordNet supersenses and target word embeddings. Our approach expands on this by exploring different syntactic representations and incorporating semantics through word embeddings into the syntactic structures. capabilities can be improved. We will examine five kinds of predicateargument constructions in corpus data to assess their metaphoric distributions and usefulness as features for classification. Our contribution is twofold. First, we examine the LCC metaph"
W18-0903,P16-2017,0,0.104112,", target (TRG), and non-metaphoric (-MET) instances are counted, as well as those for all of each construction’s defining arguments. 5.2.4 from relevant contexts. 5.2.1 VerbNet is a lexical semantic resource that groups verbs into classes based on their syntactic behavior (Kipper-Schuler, 2005). It categorizes over 6,000 verbs into classes, each of which contains syntactic frames that the verbs in the class can appear in. It also contains distinct senses, allowing it to distinguish between different verb uses in context. Previous approaches have employed VerbNet as a lexical resource (Beigman Klebanov et al., 2016), but aggregated the senses of each verb, removing the syntactic distinctions that VerbNet makes for different word senses. We ran word-sense disambiguation to determine the VerbNet class for each verb token (Palmer et al., 2017). We included one-hot vectors representing verb senses for each token, and combining this with knowledge of the particular constructions and the lexical semantics provided by embeddings for each token gives syntactically motivated information about the semantics of the utterance. For noun identification, we include the VerbNet class of the head of that noun. Predicate"
W18-0903,P14-5010,0,0.00550346,". To better understand how these structures determine metaphor, we explored metaphor-annotated corpus data for predicate-argument constructions. 3 4 Corpus Analysis Sullivan identifies a large number of constructions and the possible configurations of their arguments with regard to source and target domains. While some corpus examples are provided that show the Computational Approaches While metaphor processing has largely been focused on capturing lexical semantics, there have 18 gold-standard dependency parses. For the LCC dataset, we used the dependency parser from Stanford Core NLP tools (Manning et al., 2014). These parses are sufficient to identify intransitives, transitives, and ditransitive constructions. Verb instances that have an indirect object are ditransitive, those that lack an indirect object but have a direct object are transitive, and those that lack either but have a subject are intransitive. Copulas are marked in the dependency parses, so we can easily identify equative constructions. While similes can take many forms, Sullivan’s work focuses on simile constructions that consist of a copular verb and the word ’like’. This oversimplifies to some degree, as many similes don’t need a c"
W18-0903,E17-2084,0,0.120115,"as identifying these utterances as metaphoric is critical for generating 17 Proceedings of the Workshop on Figurative Language Processing, pages 17–26 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics been a variety of approaches that incorporate syntactic information. Many computational approaches focus on specific constructions, perhaps indicating the need to classify different metaphoric constructions through different means. The dataset of (Tsvetkov et al., 2014) provides adjective-noun annotation which has been extensively studied (Rei et al., 2017), (Bulat et al., 2017). A particularly promising approach is that of (Gutierrez et al., 2016), who use compositional distributional semantic models (CDSMs) to represent metaphors as transformations in vector space, specifically for adjective-noun constructions. Another relevant approach is that of (Haagsma and Bjerva, 2016) who use clustering and selectional preference information to detect metaphors in predicate argument constructions, including verbs with objects, subjects, and both. Their highest F1 is 57.8 for verbs with both arguments. Many systems that rely heavily on lexical resources also include some depen"
W18-0903,W15-1405,0,0.147557,"metaphor identification), and attempting to provide an accurate semantic interpretation for an utterance (here called metaphor interpretation). The first has largely been approached as a supervised machine learning problem, typically using lexical semantic features and their interaction with context to learn the kinds of situations where lexical metaphors appear. The problem of metaphor interpretation is more complex, with approaches including the implementation of full metaphoric interpretation systems (Martin, 1990), (Ovchinnikova et al., 2014), identification of source and target domains (Dodge et al., 2015), developing knowledge bases (Gordon et al., 2015), and providing literal paraphrases to metaphoric phrases (Shutova, 2010), (Shutova, 2013). In both identification and interpretation systems, syntax tends to play a limited role. Many systems rely only on lexical semantics of target words, or use only minimal context or dependency relations to help disambiguate in context (Gargett and Barnden, 2015), (Rai et al., 2016). Others rely on topic modeling and other document and sentence level features to provide general semantics, and compare the lexical semantics to that, ignoring the more ”middle”"
W18-0903,P14-1024,0,0.0590918,"”linguistic metaphor” is used to indicate these types of words and phrases. We will focus on linguistic metaphor, as identifying these utterances as metaphoric is critical for generating 17 Proceedings of the Workshop on Figurative Language Processing, pages 17–26 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics been a variety of approaches that incorporate syntactic information. Many computational approaches focus on specific constructions, perhaps indicating the need to classify different metaphoric constructions through different means. The dataset of (Tsvetkov et al., 2014) provides adjective-noun annotation which has been extensively studied (Rei et al., 2017), (Bulat et al., 2017). A particularly promising approach is that of (Gutierrez et al., 2016), who use compositional distributional semantic models (CDSMs) to represent metaphors as transformations in vector space, specifically for adjective-noun constructions. Another relevant approach is that of (Haagsma and Bjerva, 2016) who use clustering and selectional preference information to detect metaphors in predicate argument constructions, including verbs with objects, subjects, and both. Their highest F1 is"
W18-0903,L16-1668,0,0.0133641,"and semantics via WordNet supersenses and target word embeddings. Our approach expands on this by exploring different syntactic representations and incorporating semantics through word embeddings into the syntactic structures. capabilities can be improved. We will examine five kinds of predicateargument constructions in corpus data to assess their metaphoric distributions and usefulness as features for classification. Our contribution is twofold. First, we examine the LCC metaphor corpus, which includes source and target annotations, to determine their use in predicateargument constructions (Mohler et al., 2016), and employ syntactic representations as features to improve source/target classification. Second, we investigate predicate-argument constructions in the VUAMC corpus of metaphor annotation (Pragglejaz Group, 2007), and employ syntactic features to predict metaphoric vs non-metaphoric words. 2 Metaphor and Constructions Recent metaphor research has indicated that construction grammar can be employed to determine the source and target domains of linguistic metaphors (Sullivan, 2013). In many cases, certain constructions can determine what syntactic components are allowable as source and target"
W18-0903,W14-2305,0,0.0250449,"tasks: identifying which words are being used metaphorically (here called metaphor identification), and attempting to provide an accurate semantic interpretation for an utterance (here called metaphor interpretation). The first has largely been approached as a supervised machine learning problem, typically using lexical semantic features and their interaction with context to learn the kinds of situations where lexical metaphors appear. The problem of metaphor interpretation is more complex, with approaches including the implementation of full metaphoric interpretation systems (Martin, 1990), (Ovchinnikova et al., 2014), identification of source and target domains (Dodge et al., 2015), developing knowledge bases (Gordon et al., 2015), and providing literal paraphrases to metaphoric phrases (Shutova, 2010), (Shutova, 2013). In both identification and interpretation systems, syntax tends to play a limited role. Many systems rely only on lexical semantics of target words, or use only minimal context or dependency relations to help disambiguate in context (Gargett and Barnden, 2015), (Rai et al., 2016). Others rely on topic modeling and other document and sentence level features to provide general semantics, and"
W18-0903,D14-1162,0,0.0800786,"Missing"
W18-0903,W16-1103,0,0.0374137,"Missing"
W18-0903,D17-1162,0,0.142908,"inguistic metaphor, as identifying these utterances as metaphoric is critical for generating 17 Proceedings of the Workshop on Figurative Language Processing, pages 17–26 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics been a variety of approaches that incorporate syntactic information. Many computational approaches focus on specific constructions, perhaps indicating the need to classify different metaphoric constructions through different means. The dataset of (Tsvetkov et al., 2014) provides adjective-noun annotation which has been extensively studied (Rei et al., 2017), (Bulat et al., 2017). A particularly promising approach is that of (Gutierrez et al., 2016), who use compositional distributional semantic models (CDSMs) to represent metaphors as transformations in vector space, specifically for adjective-noun constructions. Another relevant approach is that of (Haagsma and Bjerva, 2016) who use clustering and selectional preference information to detect metaphors in predicate argument constructions, including verbs with objects, subjects, and both. Their highest F1 is 57.8 for verbs with both arguments. Many systems that rely heavily on lexical resources a"
W18-0903,S13-1040,0,0.0230726,"The first has largely been approached as a supervised machine learning problem, typically using lexical semantic features and their interaction with context to learn the kinds of situations where lexical metaphors appear. The problem of metaphor interpretation is more complex, with approaches including the implementation of full metaphoric interpretation systems (Martin, 1990), (Ovchinnikova et al., 2014), identification of source and target domains (Dodge et al., 2015), developing knowledge bases (Gordon et al., 2015), and providing literal paraphrases to metaphoric phrases (Shutova, 2010), (Shutova, 2013). In both identification and interpretation systems, syntax tends to play a limited role. Many systems rely only on lexical semantics of target words, or use only minimal context or dependency relations to help disambiguate in context (Gargett and Barnden, 2015), (Rai et al., 2016). Others rely on topic modeling and other document and sentence level features to provide general semantics, and compare the lexical semantics to that, ignoring the more ”middle”-level syntactic interactions (Heintz et al., 2013). While these approaches have been effective in many areas, there is evidence that figura"
W18-3512,P17-1068,0,0.0593985,"Missing"
W18-3512,D11-1145,0,0.116129,"Missing"
W18-3512,P17-2101,0,0.0960996,"oblem for many domains. One common task is identifying political affiliation. Both linguistic and non-linguistic features have proven effective in classifying political leanings of users in Twitter data (Tatman et al., 2017). PreoiucPietro et al. (2017) provide a method for identifying whether users are liberal or conservative, and point to a variety of user level classifications that can be predictive of political ideology. These userlevel attributes apply generally; we intend to classify users based on a particular behavior they engage in (evacuation or sheltering in place). More similar is Sanagavarapu et al. (2017), who predict whether users participate in events that they are tweeting about. They use linguistics features coupled with support vector machines to predict users’ participation in specific events, which parallels our task of predicting a user’s eventrelated behavior. In the domain of crisis informatics, recent work by Mart´ın et al. (2017) identifies evacuation patterns, using aggregates of geo-located tweets as well as particular user behaviors. However, they don’t empirically validate their observations, and thus don’t attempt statistic learning for classification. Another study from Yang"
W18-3512,D14-1181,0,0.00257335,"of dimension 300 using the pretrained Google News vectors, and fed this through a 50 node dense layer using a rectified linear unit (relu) activation with a dropout rate of .5. This was then fed into the output layer, using sigmoid activation to predict either relevant or irrelevant. The model was trained using categorical hinge loss, running 50 epochs. Feature-based 2.3 Convolutional Neural Network (CNN) Convolutional neural networks incorporate local word context using convolutions of words within a contextual window, and have proven effective in a variety of sentence classification tasks (Kim, 2014; Li et al., 2017). As tweets can be considered a sentence, we experiment with using CNNs for relevance classification. We follow the approach of Kim (2014), using an embedding layer (from the Google News vectors), which is then fed into a convolutional layer. We use kernel sizes of 2, 3, and 4, with 16 filters per kernel size. We use max pooling to combine the outputs, with a pool size of 4. Finally, we use a fully connected layer to the binary output nodes, using sigmoid activation to predict relevance. Both deep learning models improve over the reimplemented SVM baseline. However, the CNN A"
W18-3512,W16-6201,1,0.842925,"g CNNs for relevance classification. We follow the approach of Kim (2014), using an embedding layer (from the Google News vectors), which is then fed into a convolutional layer. We use kernel sizes of 2, 3, and 4, with 16 filters per kernel size. We use max pooling to combine the outputs, with a pool size of 4. Finally, we use a fully connected layer to the binary output nodes, using sigmoid activation to predict relevance. Both deep learning models improve over the reimplemented SVM baseline. However, the CNN As a baseline for feature-based classification, we follow the setup and features of Stowe et al. (2016), who employ support vector machines and linguistic features to classify hurricane related tweets. As a baseline, we re-implement this approach, leaving out features that appeared to have negligible contribution. We used the following features from their set: • Bag of words based on Pointwise Mutual Information (PMI) for unigrams, bigrams, and trigrams. We chose the n terms with highest PMI for positive and negative classes, with n set to 200 as was determined in validation. Selecting the bag of words lexicon based on PMI significantly improves results over using the full set of words. 68 new"
W18-3512,W17-2909,0,0.0268046,"ew problems, we would ideally like to be able to generate 69 Figure 1: Classification as Training Data is Added ysis of user attitudes during crises. language and their actions. In this section we describe our annotation of Twitter users’ evacuation behavior, and show that linguistic and geospatial data can used for classification. User level classification for Twitter users is a well known problem for many domains. One common task is identifying political affiliation. Both linguistic and non-linguistic features have proven effective in classifying political leanings of users in Twitter data (Tatman et al., 2017). PreoiucPietro et al. (2017) provide a method for identifying whether users are liberal or conservative, and point to a variety of user level classifications that can be predictive of political ideology. These userlevel attributes apply generally; we intend to classify users based on a particular behavior they engage in (evacuation or sheltering in place). More similar is Sanagavarapu et al. (2017), who predict whether users participate in events that they are tweeting about. They use linguistics features coupled with support vector machines to predict users’ participation in specific events,"
W18-3512,N13-1097,0,0.0274113,"Twitter Behavior During Hurricane Events Kevin Stowe, Jennings Anderson, Martha Palmer, Leysia Palen, Ken Anderson University of Colorado, Boulder, CO 80309 [kest1439, jennings.anderson, mpalmer, palen, kena]@colorado.edu 2 Abstract Twitter data is often difficult to understand due to limited length of tweets and the noise inherent in the medium. As a result, there is a variety of research in attempting to effectively identify and classify tweets. There are multiple studies in classification of flu-related tweets (Culotta, 2010; Aramaki et al., 2011). One relevance classification approach is Lamb et al. (2013), which initially classifies tweets for relevance and then applies finer-grained classifiers. They build classifiers using syntactic and Twitter-specific features to detect awareness versus infection, self versus others, and whether tweets are relevant to the flu or not. Sriram et al. (2010) propose a somewhat more specific system, classifying tweets into general categories like news, events, and opinions, achieving accuracies between .85 and .95 depending on category. Sankaranarayanan et al. (2009) perform a similar task, classifying tweets into either news or non-news. Recently, the work of"
W18-3512,D17-1201,0,0.0236674,"on 300 using the pretrained Google News vectors, and fed this through a 50 node dense layer using a rectified linear unit (relu) activation with a dropout rate of .5. This was then fed into the output layer, using sigmoid activation to predict either relevant or irrelevant. The model was trained using categorical hinge loss, running 50 epochs. Feature-based 2.3 Convolutional Neural Network (CNN) Convolutional neural networks incorporate local word context using convolutions of words within a contextual window, and have proven effective in a variety of sentence classification tasks (Kim, 2014; Li et al., 2017). As tweets can be considered a sentence, we experiment with using CNNs for relevance classification. We follow the approach of Kim (2014), using an embedding layer (from the Google News vectors), which is then fed into a convolutional layer. We use kernel sizes of 2, 3, and 4, with 16 filters per kernel size. We use max pooling to combine the outputs, with a pool size of 4. Finally, we use a fully connected layer to the binary output nodes, using sigmoid activation to predict relevance. Both deep learning models improve over the reimplemented SVM baseline. However, the CNN As a baseline for f"
W18-3512,P17-2102,0,0.115207,", which initially classifies tweets for relevance and then applies finer-grained classifiers. They build classifiers using syntactic and Twitter-specific features to detect awareness versus infection, self versus others, and whether tweets are relevant to the flu or not. Sriram et al. (2010) propose a somewhat more specific system, classifying tweets into general categories like news, events, and opinions, achieving accuracies between .85 and .95 depending on category. Sankaranarayanan et al. (2009) perform a similar task, classifying tweets into either news or non-news. Recently, the work of Volkova et al. (2017) attempts to classify suspicious and trusted tweets. They find that deep learning models outperform feature-based models, but linguistics features can be helpful. They report F1 scores of between .88 and .92 depending on the category classified. For our first task of relevant tweet classification, we employ supervised machine learning to predict whether individual tweets are relevant to a hurricane. This study focuses on the Hurricane Sandy event in October of 2012. This hurricane made landfall on the eastern seaboard of the United States on October 29, causing massive damage to many areas inc"
W18-3512,D17-1055,0,0.0245276,"2017), who predict whether users participate in events that they are tweeting about. They use linguistics features coupled with support vector machines to predict users’ participation in specific events, which parallels our task of predicting a user’s eventrelated behavior. In the domain of crisis informatics, recent work by Mart´ın et al. (2017) identifies evacuation patterns, using aggregates of geo-located tweets as well as particular user behaviors. However, they don’t empirically validate their observations, and thus don’t attempt statistic learning for classification. Another study from Yang et al. (2017) studies user behavior during crisis events, using linguistic and spatial features to analyze shifting sentiment during Hurricane Sandy. While they focus on keyword tweets clustered geographically, they show that geospatial features are helpful for anal3.1 Data Our analysis is focused on users that are potentially at risk, but these users are difficult to identify due to the noisiness of Twitter data. To alleviate this problem, we attempt to identify vulnerable users using geospatial information. For our data, location-enabled tweets include any tweet returned by the Twitter API with a precise"
W18-3512,D14-1162,0,0.0816301,"Missing"
W18-4915,W17-5202,0,0.0199791,".), and damages to mental and physical health (physical or emotional suffering, loss of life, injury, illness, etc.). Introduced as a subcategory of INFORMATION in Scheme 2, it was pulled out into its own category in Schemes 3 and 4, along with substantial clarifications to the guidelines. This category is hard to consistently annotate, as it is broad and not very frequent, but we’ve maintained it due to its importance to the understanding of hazard events. 4.7 Sentiment SENTIMENT is the most traditional category of annotation, having received a large amount of treatment for social media (see Barnes et al (2017) for a recent review of sentiment models and tasks). Our goal was ambitious - we aimed to capture a large set of diverse sentiments, rather than simply positive/negative. For Schemes 1 and 2, we included 8 different subcategories of sentiment. These are awe, boredom, excitement, humor, frustration, positive coping, worry, and being settled. For Scheme 3, we removed categories that were rare (awe, boredom, settled, excitement), as well as those with very low agreement rates (positive coping, humor). We kept three different sentiment categories as top level categories : WORRY, FRUSTRATION, and a"
W19-3315,J05-1004,1,0.571014,"essing applications. Our long term goal is to augment Abstract Meaning Representations (Banarescu et al., 2013) with tense and aspect information. With the assumption that an automatic pre-processing step could greatly reduce the annotation effort involved, we have been exploring different options for English tense and aspect annotation. In this paper we compare two approaches to automatically classifying tense (present, past, etc.), aspect (progressive, perfect, etc.), and the form of verb (finite, participle, etc.). Our own work trains a BiLSTM-CRF NN, ClearTAC, on the PropBank annotations (Palmer et al., 2005) for the form, tense, and aspect of verbs. We compare the results to TMV-annotator, a rule-based system developed by (Ramm et al., 2017). Not surprisingly, we find our NN system significantly outperforms the rulebased system on the Propbank test data. In Section 2 we discuss related work and provide background information on TMV-annotator. Section 3 reviews the PropBank annotation and our modifications to 136 Proceedings of the First International Workshop on Designing Meaning Representations, pages 136–140 c Florence, Italy, August 1st, 2019 2019 Association for Computational Linguistics teli"
W19-3315,D14-1162,0,0.094779,"ee Section 3.1 for a legend. ’#’ is the label for a non-verb token. ClearTAC System Architecture Bidirectional LSTM-CRF models have been shown to be useful for numerous sequence labeling tasks, such as part of speech tagging, named entity recognition, and chunking (Huang et al., 2015). Based on these results, we expected good performance on classification of tense and aspect. Our neural network consists of a Bi-LSTM layer with 150 hidden units followed by a CRF layer. The inputs to the NN were sentence-length sequences, with each token represented by pretrained 300-dimension GloVe embeddings (Pennington et al., 2014). No part-of-speech or syntactic pre-processing was used. Classifying form, tense, and aspect was treated as a joint task. 5 Performance across the board for the various subtasks on both datasets was consistently in the mid-90’s. The more challenging task of tagging all forms, tenses, and aspects in Propbank I saw a performance decrease of only 2 points compared to the reduced dataset. 5.1 Error Analysis Overall, the model had the most challenges with gerunds and verbs with modals, often predicting them not to be a verb. With these forms also being tenseless, the effect can also be seen in the"
W19-3315,P17-4001,0,0.459508,"Missing"
W19-3315,D10-1032,0,0.0175571,"eaning Representations, pages 136–140 c Florence, Italy, August 1st, 2019 2019 Association for Computational Linguistics telicity, verb punctuality, and temporal ordering between adjacent events. The NLPWin pipeline (Vanderwende, 2015) consists of components spanning from lexical analysis to construction of logical form representations to collecting these representations into a knowledge database. Tense is included as one of the attributes of the declension of a verb. This system is a rule-based approach, as is TMV-annotator described below. Other recent work on tense classification includes (Reichart and Rappoport, 2010) attempting to distinguish between the different word senses within a tense/aspect. (Ferreira and Pereira, 2018) performed tense classification with the end goal of transposing verb tenses in a sentence for language study. 2.1 Sentence Verbal complex Main Finite? Tense Progressive? Table 1: Partial output of TMV-annotator for an example verbal complex, showing the fields relevant to this work. The information in the inflection field consists of form, tense, aspect, person, and voice. We trained our model to predict form, tense, and aspect, which were labeled in the dataset with the following p"
W19-3315,W13-2322,1,0.713909,"and outline our plans for further development. This paper proposes using a Bidirectional LSTM-CRF model in order to identify the tense and aspect of verbs. The information that this classifier outputs can be useful for ordering events and can provide a pre-processing step to improve efficiency of annotating this type of information. This neural network architecture has been successfully employed for other sequential labeling tasks, and we show that it significantly outperforms the rule-based tool TMV-annotator on the Propbank I dataset. 1 2 Background Abstract Meaning Representations (AMRs) (Banarescu et al., 2013) are a graph-based representation of the semantics of sentences. They aim to strip away syntactic idiosyncrasies of text into a standardized representation of the meaning. The initial work on AMRs left out tense and aspect as being more syntactic features than semantic, but the absence of this feature makes generation from AMRs and temporal reasoning much more difficult. Very recently there have been efforts underway to extend AMRs to incorporate this type of temporal information (Donatelli et al., 2018). Since existing AMR corpora will need to be revised with annotations of this type of infor"
W19-3315,W06-0107,0,0.0583606,"vised with annotations of this type of information, automatically classifying the tense and aspect of verbs could provide a shortcut. Annotators can work much more efficiently by only checking the accuracy of the automatic labels instead of annotating from scratch. Availability of automatic tense and aspect tagging could also prove useful for any system interested in extracting temporal sequences of events, and has been a long-standing research goal. Much of the previous work on tense classification has been for the purpose of improving machine translation, including (Ye and Zhang, 2005) and (Ye et al., 2006), which explored tense classification of Chinese as a sequential classification task, using conditional random fields and a combination of surface and latent features, such as verb Introduction Identifying the tense and aspect of predicates can provide important clues to the sequencing and structure of events, which is a vital part of numerous down-stream natural language processing applications. Our long term goal is to augment Abstract Meaning Representations (Banarescu et al., 2013) with tense and aspect information. With the assumption that an automatic pre-processing step could greatly re"
W19-3315,D12-1133,0,0.243626,"this work. The information in the inflection field consists of form, tense, aspect, person, and voice. We trained our model to predict form, tense, and aspect, which were labeled in the dataset with the following possible values: TMV-annotator • Form: TMV-annotator (Ramm et al., 2017) is a rulebased tool for annotating verbs with tense, mood, and voice in English, German, and French. In the case of English, it also identifies whether the verb is progressive. Although the rules were hand-crafted for each language, they operate on dependency parses. The authors specifically use the Mate parser (Bohnet and Nivre, 2012) for their reported results, although the tool could be used on any dependency parses that use the same part of speech and dependency labels as Mate. The first step of their tool is to identify verbal complexes (VCs), which consist of a main verb and verbal particles and negating words. Subsequent rules based on the words in the VC and their dependencies make binary decisions about whether the VC is finite, progressive, active or passive voice, subjunctive or indicative, as well as assign a tense. A subset of output for an example sentence is shown in Table 1. For tense tagging, the authors re"
W19-3315,I05-1077,0,0.0698973,"orpora will need to be revised with annotations of this type of information, automatically classifying the tense and aspect of verbs could provide a shortcut. Annotators can work much more efficiently by only checking the accuracy of the automatic labels instead of annotating from scratch. Availability of automatic tense and aspect tagging could also prove useful for any system interested in extracting temporal sequences of events, and has been a long-standing research goal. Much of the previous work on tense classification has been for the purpose of improving machine translation, including (Ye and Zhang, 2005) and (Ye et al., 2006), which explored tense classification of Chinese as a sequential classification task, using conditional random fields and a combination of surface and latent features, such as verb Introduction Identifying the tense and aspect of predicates can provide important clues to the sequencing and structure of events, which is a vital part of numerous down-stream natural language processing applications. Our long term goal is to augment Abstract Meaning Representations (Banarescu et al., 2013) with tense and aspect information. With the assumption that an automatic pre-processing"
W19-3315,W18-4912,0,0.0606245,"notator on the Propbank I dataset. 1 2 Background Abstract Meaning Representations (AMRs) (Banarescu et al., 2013) are a graph-based representation of the semantics of sentences. They aim to strip away syntactic idiosyncrasies of text into a standardized representation of the meaning. The initial work on AMRs left out tense and aspect as being more syntactic features than semantic, but the absence of this feature makes generation from AMRs and temporal reasoning much more difficult. Very recently there have been efforts underway to extend AMRs to incorporate this type of temporal information (Donatelli et al., 2018). Since existing AMR corpora will need to be revised with annotations of this type of information, automatically classifying the tense and aspect of verbs could provide a shortcut. Annotators can work much more efficiently by only checking the accuracy of the automatic labels instead of annotating from scratch. Availability of automatic tense and aspect tagging could also prove useful for any system interested in extracting temporal sequences of events, and has been a long-standing research goal. Much of the previous work on tense classification has been for the purpose of improving machine tr"
W19-3318,P17-1044,0,0.0313199,"and Jurafsky, 2002; Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018) using solely ELMo embeddings (without any pre-trained or fine-tuned word-specific vectors) trained on a combination of three PropBank annotated corpora described in (O’Gorman et al., 2019): OntoNotes (Hovy et al., 2006), the English Web TreeBank (Bies et al., 2012), and the BOLT corpus (Garland et al., 2012). For alignment, we begin by applying updated SemLink mappings (Palmer, 2009) to map PropBank roles to linked VN thematic roles for the identified VN class. Remaining arguments are then mapped using heuristics based on the syntactic and selectional restrictions def"
W19-3318,N06-2015,1,0.264637,"system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018) using solely ELMo embeddings (without any pre-trained or fine-tuned word-specific vectors) trained on a combination of three PropBank annotated corpora described in (O’Gorman et al., 2019): OntoNotes (Hovy et al., 2006), the English Web TreeBank (Bies et al., 2012), and the BOLT corpus (Garland et al., 2012). For alignment, we begin by applying updated SemLink mappings (Palmer, 2009) to map PropBank roles to linked VN thematic roles for the identified VN class. Remaining arguments are then mapped using heuristics based on the syntactic and selectional restrictions defined in the VN class. To seAcknowledgments We gratefully acknowledge the support of DTRAl -16-1-0002/Project 1553695, eTASC - Empirical Evidence for a Theoretical Approach to Semantic Components and DARPA 15-18-CwC-FP-032 Communicating with Comp"
W19-3318,W08-2222,0,0.0110709,"00 class: has possession(E, Pivot, Theme), or During(E), as for the contiguous location-47.8 class (Italy borders France): contact(During(E), Theme, CoTheme). Most classes having to do with change, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument role constructions of Goldberg ("
W19-3318,kawahara-palmer-2014-single,1,0.85287,"e, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument role constructions of Goldberg (2006). A previous revision of the VerbNet semantic representations made the correspondence of these patterns to constructions more explicit by using a common predicate (i.e., path rel) for all caused-motion cons"
W19-3318,kipper-etal-2006-extending,1,0.611019,"er analysis and planning systems. For applications like robotics or interactions with avatars, commonsense inferences needed to understand human language directions or interactions are often not derivable directly from the utterance. Tracking intrinsic and extrinsic states of entities, such as their existence, location or functionality, currently requires explicit statements with precise temporal sequencing. In this paper, we describe new semantic representations for the lexical resource VerbNet that provide this sort of information for thousands of 2 Background The language resource VerbNet (Kipper et al., 2006) is a hierarchical, wide-coverage verb lexicon that groups verbs into classes based on similarities in their syntactic and semantic behavior (Schuler, 2005). Each class in VerbNet includes a set of member verbs, the thematic roles used in the predicate-argument structure of these members (Bonial et al., 2011), and the class-specific selectional preferences for those roles. The class also provides a set of typical syntactic patterns and corresponding semantic representations. A verb can be a member of multiple classes; for example, run is a member of 8 VerbNet classes, including the run-51.3.2"
W19-3318,L18-1009,1,0.783127,"s or intervals. This Dynamic Event Model (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013) explicitly labels the transitions that move an event from frame to frame. Although people infer that an entity is no longer at its initial location once motion has begun, computers need explicit mention of this fact to accurately track the location of an entity. Similarly, some states hold throughout an event, while others do not. Our new representations make these distinctions clear, where pre-event, while-event, and post-event conditions are distinguished formally in the representation. Elsewhere (Brown et al., 2018), we discuss in more detail the Dynamic Event Model, show the effect of the new subevent structure on the interpretation of the role of the Agent, and give further examples of the new change of location and change of state representations. Applying the Dynamic Event Model to VerbNet semantic representations allowed us refine the event sequences by expanding the previous tripartite division of Start(E), During(E), and End(E) to an indefinite number of subevents. These numbered subevents allow very precise tracking of participants across time and a nuanced representation of causation and action"
W19-3318,P12-1028,1,0.823006,"ing to do with change, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument role constructions of Goldberg (2006). A previous revision of the VerbNet semantic representations made the correspondence of these patterns to constructions more explicit by using a common predicate (i.e., path rel"
W19-3318,W17-2812,1,0.836296,"onstruction frames(Hwang, 2014). At the request of some users, we are substituting more specific predicates for the general path rel predicate, such as has location, has state and has possession, although the subevent patterns continue to show the commonality across (1) The rabbit hopped across the lawn. Theme V Trajectory motion(during(E), Theme) path rel(start(E), Theme, ?Initial location1 , CH OF LOC , prep) path rel(during(E), Theme, Trajectory, CH OF LOC , prep) path rel(end(E), Theme, ?Destination, CH OF LOC , prep) Efforts to use VerbNet’s semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017), however, indicated a need for greater consistency and expressiveness. We have addressed consistency on several fronts. First, all necessary participants are accounted for in the representations, whether they are instantiated in the syntax, incorporated in the verb itself (e.g., to drill), or simply logically necessary (e.g., all entities that change location begin in an initial location, whether it is commonly mentioned or not). 1 A question mark in front of a thematic role indicates a role that appears in the syntax in some frames for the class but not in this particular frame. 155 troduced"
W19-3318,J02-3001,0,0.100875,"rsing To facilitate immediate use of the new VerbNet semantic representations, we are releasing a semantic parser that predicts the updated semantic representations from events in natural language input sentences. For a given predicative verb in a sentence, we define VerbNet semantic parsing as the task of identifying the VN class, associated thematic roles, and corresponding semantic representations linked to a frame within the class. We approach VerbNet semantic parsing in three distinct steps: 1. Sense disambiguation to identify the appropriate VN class, 2. PropBank semantic role labeling (Gildea and Jurafsky, 2002; Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) an"
W19-3318,P06-1117,0,0.0400665,"indicated with either a bare E, as for the own-100 class: has possession(E, Pivot, Theme), or During(E), as for the contiguous location-47.8 class (Italy borders France): contact(During(E), Theme, CoTheme). Most classes having to do with change, such as changes in location, changes in state and changes in possession, used a path rel predicate in combination with Start(E), During(E), and End/Result(E) to show the transition from one location or state to another (1). VerbNet VerbNet has long been used in NLP for semantic role labeling and other inference-enabling tasks (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Loper et al., 2007; Bos, 2008). In addition, automatic disambiguation of a verb’s VerbNet class has been used as a stand-in for verb sense disambiguation (Abend et al., 2008; Brown et al., 2014; Croce et al., 2012; Kawahara and Palmer, 2014). VerbNet’s semantic representations use a Davidsonian first-order-logic formulation that incorporates the thematic roles of the class. Each frame in a class is labeled with a flat syntactic pattern (e.g., NP V NP). The ”syntax” that follows shows how the thematic roles for that class appear in that pattern (e.g., Agent V Patient), much like the argument"
W19-3318,J05-1004,1,0.198198,"te use of the new VerbNet semantic representations, we are releasing a semantic parser that predicts the updated semantic representations from events in natural language input sentences. For a given predicative verb in a sentence, we define VerbNet semantic parsing as the task of identifying the VN class, associated thematic roles, and corresponding semantic representations linked to a frame within the class. We approach VerbNet semantic parsing in three distinct steps: 1. Sense disambiguation to identify the appropriate VN class, 2. PropBank semantic role labeling (Gildea and Jurafsky, 2002; Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018)"
W19-3318,N18-1202,0,0.0462361,"Palmer et al., 2005) to identify and classify arguments, and 3. Alignment of PropBank semantic roles with VN thematic roles within a frame belonging to the predicted VN class. After aligning arguments from the PropBank SRL system’s output with the thematic roles in a particular VN frame, the frame’s associated semantic predicates can be instantiated using the aligned arguments. For sense disambiguation, we use a supervised verb sense classifier trained on updated VN class tags (Palmer et al., 2017). For semantic role labeling, we use a variation of the system described in He et al. (2017) and Peters et al. (2018) using solely ELMo embeddings (without any pre-trained or fine-tuned word-specific vectors) trained on a combination of three PropBank annotated corpora described in (O’Gorman et al., 2019): OntoNotes (Hovy et al., 2006), the English Web TreeBank (Bies et al., 2012), and the BOLT corpus (Garland et al., 2012). For alignment, we begin by applying updated SemLink mappings (Palmer, 2009) to map PropBank roles to linked VN thematic roles for the identified VN class. Remaining arguments are then mapped using heuristics based on the syntactic and selectional restrictions defined in the VN class. To"
W19-3318,W13-5401,1,0.740109,"tates represented with a simple e, processes as a sequence of states characterizing values of some attribute, e1 ...en , and transitions describing the opposition inherent in achievements and accomplishments. In subsequent work within GL, event structure has been integrated with dynamic semantic models in order to more explicitly represent the attribute modified in the course of the event (the location of the moving entity, the extent of a created or destroyed entity, etc.) as a sequence of states related to time points or intervals. This Dynamic Event Model (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013) explicitly labels the transitions that move an event from frame to frame. Although people infer that an entity is no longer at its initial location once motion has begun, computers need explicit mention of this fact to accurately track the location of an entity. Similarly, some states hold throughout an event, while others do not. Our new representations make these distinctions clear, where pre-event, while-event, and post-event conditions are distinguished formally in the representation. Elsewhere (Brown et al., 2018), we discuss in more detail the Dynamic Event Model, show the effect of the"
W19-3318,zaenen-etal-2008-encoding,1,0.719911,"r all caused-motion construction frames(Hwang, 2014). At the request of some users, we are substituting more specific predicates for the general path rel predicate, such as has location, has state and has possession, although the subevent patterns continue to show the commonality across (1) The rabbit hopped across the lawn. Theme V Trajectory motion(during(E), Theme) path rel(start(E), Theme, ?Initial location1 , CH OF LOC , prep) path rel(during(E), Theme, Trajectory, CH OF LOC , prep) path rel(end(E), Theme, ?Destination, CH OF LOC , prep) Efforts to use VerbNet’s semantic representations (Zaenen et al., 2008; Narayan-Chen et al., 2017), however, indicated a need for greater consistency and expressiveness. We have addressed consistency on several fronts. First, all necessary participants are accounted for in the representations, whether they are instantiated in the syntax, incorporated in the verb itself (e.g., to drill), or simply logically necessary (e.g., all entities that change location begin in an initial location, whether it is commonly mentioned or not). 1 A question mark in front of a thematic role indicates a role that appears in the syntax in some frames for the class but not in this pa"
W19-4016,aker-etal-2012-assessing,0,0.0615037,"Missing"
W19-4016,W10-1808,1,0.841093,"Missing"
W19-4016,J11-2010,0,0.0338381,"dence are used to update the existing model in the form of updated or new rules and train the algorithm further (e.g. cf. Sevastjanova et al., 2018). Similarly, the produced justifications in such annotation tasks could be integrated in a “static” learning system in the form of additional rules, patterns or weights and thus lead to a more explainable model. Such justifications can be beneficial in annotations where there is a specific label or score to be chosen among other labels/scores, e.g. in NLI, in semantic similarity tasks, in sentiment analysis, in argument annotation, etc. practices (Fort et al., 2011). Considerably less research has been done in task-specific annotations. For NLI there is work discussing annotation challenges (de Marneffe et al., 2008; Kalouli et al., 2017b) and other focusing on improving crowdsourced corpora (Kalouli et al., 2017a, 2018). 7 We would like to thank the Colorado students for their annotations, Bettina Braun and Katharina Zahner for useful feedback on the statistical analysis, as well as the anonymous reviewers for their constructive comments. We gratefully acknowledge the support of DTRA 1-IDTRAl 16-1-0002/Project 1553695, eTASC - Empirical Evidence for a T"
W19-4016,W07-1401,0,0.0764643,"ble evaluation measure for real natural language understanding, as discussed by Condoravdi et al. (2003) and others. It is also a necessary step towards reasoning as more recently discussed by Goldberg and Hirst (2017) and Nangia et al. (2017) who say that solving NLI perfectly means achieving human level understanding of language. Thus, there is an increasing effort to design high-performing NLI systems, which in turn leads to the creation of massive learning corpora. Early datasets, like FraCas (Consortium et al., 1996) or the seven RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Dagan et al., 2010; Bentivogli et al., 2009b,a, 2011), contained a few hundred handannotated pairs. More recent sets have exploded from some thousand pairs (e.g., SICK, Marelli et al., 2014b) to some hundred thousand examples: SciTail (Khot et al., 2018), SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018). The latter two have been vastly used to train learning algorithms and achieve high performance. However, it was recently shown that this high performance can drop significantly by slightly modifying the training process (Poliak et al., 2017; Glockner et al., 2018). It was also s"
W19-4016,P18-2103,0,0.0364502,"t al., 2006; Giampiccolo et al., 2007; Dagan et al., 2010; Bentivogli et al., 2009b,a, 2011), contained a few hundred handannotated pairs. More recent sets have exploded from some thousand pairs (e.g., SICK, Marelli et al., 2014b) to some hundred thousand examples: SciTail (Khot et al., 2018), SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018). The latter two have been vastly used to train learning algorithms and achieve high performance. However, it was recently shown that this high performance can drop significantly by slightly modifying the training process (Poliak et al., 2017; Glockner et al., 2018). It was also shown that such training sets contain annotation artifacts that bias the learning (Gururangan et al., 2018; Naik et al., 2018). Other recent work (Kalouli et al., 2017b,a, 2018) discussed problematic annotations of the SICK corpus (Marelli et al., 2014b) and attempted to improve the annotations. All this work leads to the conclusion that corpus construction, including the annotation process, is much more important than what is often assumed and that bad corpora can falsely deliver promising results. In this paper we take a closer look at the work The vast amount of research intro"
W19-4016,D15-1075,0,0.41346,"ieving human level understanding of language. Thus, there is an increasing effort to design high-performing NLI systems, which in turn leads to the creation of massive learning corpora. Early datasets, like FraCas (Consortium et al., 1996) or the seven RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Dagan et al., 2010; Bentivogli et al., 2009b,a, 2011), contained a few hundred handannotated pairs. More recent sets have exploded from some thousand pairs (e.g., SICK, Marelli et al., 2014b) to some hundred thousand examples: SciTail (Khot et al., 2018), SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018). The latter two have been vastly used to train learning algorithms and achieve high performance. However, it was recently shown that this high performance can drop significantly by slightly modifying the training process (Poliak et al., 2017; Glockner et al., 2018). It was also shown that such training sets contain annotation artifacts that bias the learning (Gururangan et al., 2018; Naik et al., 2018). Other recent work (Kalouli et al., 2017b,a, 2018) discussed problematic annotations of the SICK corpus (Marelli et al., 2014b) and attempted to improve the a"
W19-4016,N18-2017,0,0.0600057,"Missing"
W19-4016,J96-2004,0,0.781769,"Missing"
W19-4016,W03-0906,1,0.526226,"Missing"
W19-4016,N13-1132,0,0.104066,"Missing"
W19-4016,W17-7205,1,0.933895,"Missing"
W19-4016,W10-0701,0,0.0313422,"Missing"
W19-4016,W13-3819,0,0.0631758,"Missing"
W19-4016,W17-6915,1,0.850735,"Missing"
W19-4016,E17-2081,0,0.171995,"Missing"
W19-4016,S14-2055,0,0.0164343,"se due to its nature in this dataset; it is expected that in more complex data, negation will play a different role. No significant interactions could be established for this model. Note that the small differences in the average CF scores shown in Table 1 result from the actual average scores used by the annotators for each pair ranging from a minimum of 3.54 to a maximum of 8.65. In a small side experiment we also tested how the CF scores correlate with what is really hard for automatic systems. We chose the best performing system from the SemEval 2014 task (Marelli et al., 2014a) on SICK by Lai and Hockenmaier (2014) and extracted from their test data those pairs that were also included in our subcorpus. These 92 pairs were split into two groups: those where the label given by the automatic system was the same as the label given by our annotators and those where it was different, i.e. the system got it wrong. For each of those groups we calculated the average CF score. Both groups have an average CF between 6.2 and 6.8, which means that for our subcorpus and this NLI system there is no strong correlation between what our annotators considered hard for machines and what is indeed hard. but contain hard cor"
W19-4016,sabou-etal-2014-corpus,0,0.049229,"Missing"
W19-4016,S14-2001,0,0.617259,"ldberg and Hirst (2017) and Nangia et al. (2017) who say that solving NLI perfectly means achieving human level understanding of language. Thus, there is an increasing effort to design high-performing NLI systems, which in turn leads to the creation of massive learning corpora. Early datasets, like FraCas (Consortium et al., 1996) or the seven RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Dagan et al., 2010; Bentivogli et al., 2009b,a, 2011), contained a few hundred handannotated pairs. More recent sets have exploded from some thousand pairs (e.g., SICK, Marelli et al., 2014b) to some hundred thousand examples: SciTail (Khot et al., 2018), SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018). The latter two have been vastly used to train learning algorithms and achieve high performance. However, it was recently shown that this high performance can drop significantly by slightly modifying the training process (Poliak et al., 2017; Glockner et al., 2018). It was also shown that such training sets contain annotation artifacts that bias the learning (Gururangan et al., 2018; Naik et al., 2018). Other recent work (Kalouli et al., 2017b,a, 2018) discussed prob"
W19-4016,marelli-etal-2014-sick,0,0.696662,"ldberg and Hirst (2017) and Nangia et al. (2017) who say that solving NLI perfectly means achieving human level understanding of language. Thus, there is an increasing effort to design high-performing NLI systems, which in turn leads to the creation of massive learning corpora. Early datasets, like FraCas (Consortium et al., 1996) or the seven RTE challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Dagan et al., 2010; Bentivogli et al., 2009b,a, 2011), contained a few hundred handannotated pairs. More recent sets have exploded from some thousand pairs (e.g., SICK, Marelli et al., 2014b) to some hundred thousand examples: SciTail (Khot et al., 2018), SNLI (Bowman et al., 2015), Multi-NLI (Williams et al., 2018). The latter two have been vastly used to train learning algorithms and achieve high performance. However, it was recently shown that this high performance can drop significantly by slightly modifying the training process (Poliak et al., 2017; Glockner et al., 2018). It was also shown that such training sets contain annotation artifacts that bias the learning (Gururangan et al., 2018; Naik et al., 2018). Other recent work (Kalouli et al., 2017b,a, 2018) discussed prob"
W19-4016,D08-1027,0,0.243216,"Missing"
W19-4016,P08-1118,0,0.419747,"Missing"
W19-4016,P14-2021,0,0.073825,"Missing"
W19-4016,W05-1206,0,0.300232,"ng nor unambiguous guidelines can prevent that. However, accepting the fact that the task, though very useful, cannot be well-defined should not scare us but instead motivate us to deal with it in a more efficient way. We need to start devising corpora based on the notion of human inference which includes some inherent variability, and find appropriate methods to train our systems on such data and measure their performance on them. For example, NLI pairs could be labelled with the information about the specific kind of inference they are dealing with, similarly to what was already proposed by Zaenen et al. (2005). On the other hand, the systems could be adapted to consider these different labels: in the case of directionality, for example, we could post-hoc measure the IAAs of each pair in both directions and find the harder one. This feature can then be exploited by automatic systems to evaluate their performance on “harder” vs. “easier” cases. It can also be considered for the training process itself: pairs in the “easier” direction have a higher IAA, are more reliable and should have a stronger learning effect, e.g. have higher training weights, than pairs in the “harder”, less-reliable direction."
W19-5014,W17-2339,1,0.89318,"tors and behaviours that enable cancer to thrive in the body. Introduced by Weinberg and Hanahan (2000), it has been widely used in biomedical NLP, including as part of HOC Document Train Dev Test Total 1,303 183 366 1,852 EXP Sentence Document Sentence 12,279 1,775 3,410 17,464 2,555 384 722 3,661 25,307 3,770 7,100 36,177 Table 2: Summary statistics of the Hallmarks of Cancer (HOC) and the Chemical Exposure Assessment (EXP) datasets. The model follows the convolutional neural network (CNN) model proposed by Kim (2014). An implementation of this algorithm on HOC and EXP has been published by Baker and Korhonen (2017); we use this implementation in our experiment. The input to the model is an initial word embedding layer that maps input texts into matrices, which is then followed by convolutions of different filter sizes, 1-max pooling, and finally a fully-connected layer leading to an output Softmax layer predicting labels for text. Model hyperparameters and the training setup are summarized in Table 3. Parameters Values Vector dimension Filter sizes Number of filters Dropout probability Minibatch size Input size (in tokens) 200 3,4 and 5 300 0.5 50 500 (documents), 100 (sentences) Table 3: Hyper-paramete"
W19-5014,W18-2311,0,0.037339,"Missing"
W19-5014,P05-1022,0,0.0396845,"ROT) (Krallinger et al., 2017). The corpus provides mention and relation annotations for complex events related to chemical– protein interaction in molecular biology. The goal of this task is to predict whether a given chemical– protein pair is related or not, and to then verify its corresponding relation type. There are five types of relations: Up-regulator, Down-regulator, Agonist, Antagonist, and Substrate. The corpus is provided in the Turku Event Extraction System (TEES) XML format and are installed with the Turku Extraction System (Bj¨orne, 2014). It is parsed with the the BLLIP parser (Charniak and Johnson, 2005) with the McClosky bio-model (Mcclosky, 2010), followed by conversion of the constituency parses into dependency parses using the Stanford Tools (MacCartney et al., 2006). Table 4 summarizes key statistics for the dataset. Train Dev Test Total initial word embedding layer that maps input texts into matrices, followed by convolutions of different filter sizes and 1-max pooling, and finally a fully connected layer, leading to an output Softmax layer for predicting labels. Performance is evaluated using the standard precision, recall, and F1 -score metrics of the labels in the model. Classificati"
W19-5014,W16-2922,1,0.932118,"can be applied to any pretrained word embedding vectors. The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word 125 Proceedings of the BioNLP 2019 workshop, pages 125–134 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics types and similar to their original distributional representations. Using a standard “off-the-shelf” retrofitting algorithm, we apply the idea of retrofitting to verb clusters to two sets of widely-used pretrained embedding vectors in BioNLP (those by Pyysalo et al. (2013a) and by Chiu et al. (2016)) to obtain improved embeddings. We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task. We make our retrofitted embeddings freely available to the BioNLP community along with our code.1 The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain. Retrofitting has thus far only been applied for aligning vectors to Medical Subject Headings (MeSH) (Yu e"
W19-5014,N15-1184,0,0.496344,"esis is therefore that by retrofitting embedded word representations to semantic verb classes, semanticallysimilar verbs (i.e. member verbs within the same lexical class) like “suppress” and “inhibit” will be pulled together in vector space, whereas verbs like “collect” and “examine” will not. Consequently, this allows NLP systems to generalize away from individual verbs, alleviating the data sparseness problem of representing each verb in the corpus individually. Retrofitting is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors (Faruqui et al., 2015). It is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors. It can be applied to any pretrained word embedding vectors. The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word 125 Proceedings of the BioNLP 2019 workshop, pages 125–134 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics types and similar to their original distributional representations. Using a standard “off-the-shelf” retrofitting alg"
W19-5014,N13-1092,0,0.102769,"Missing"
W19-5014,D14-1181,0,0.00425196,"classifications. The Hallmarks of Cancer depicts a set of interrelated biological factors and behaviours that enable cancer to thrive in the body. Introduced by Weinberg and Hanahan (2000), it has been widely used in biomedical NLP, including as part of HOC Document Train Dev Test Total 1,303 183 366 1,852 EXP Sentence Document Sentence 12,279 1,775 3,410 17,464 2,555 384 722 3,661 25,307 3,770 7,100 36,177 Table 2: Summary statistics of the Hallmarks of Cancer (HOC) and the Chemical Exposure Assessment (EXP) datasets. The model follows the convolutional neural network (CNN) model proposed by Kim (2014). An implementation of this algorithm on HOC and EXP has been published by Baker and Korhonen (2017); we use this implementation in our experiment. The input to the model is an initial word embedding layer that maps input texts into matrices, which is then followed by convolutions of different filter sizes, 1-max pooling, and finally a fully-connected layer leading to an output Softmax layer predicting labels for text. Model hyperparameters and the training setup are summarized in Table 3. Parameters Values Vector dimension Filter sizes Number of filters Dropout probability Minibatch size Inpu"
W19-5014,P06-1044,1,0.754334,"Missing"
W19-5014,C18-1205,0,0.0306855,"ing large corpora for (re-)training as the joint-learning models do. Among these methods, retrofitting (Faruqui et al., 2015) is widely used. Given any (pretrained) vector-space representations, the goal of retrofitting is to bring closer words which are connected via a relation (e.g. synonyms) in a given semantic network or lexical resource (i.e. linguistic constraints). For example, Yu et al. (2016, 2017) retrofit word vector spaces of MeSH terms by using additional linkage information from the UMNSRS hierarchy to improve the representations of biomedical concepts. Building on retrofitting, Lengerich et al. (2018) generalize retrofitting methods by explicitly modelling individual linguistic constraints that are commonly found in health and clinical-related lexicons (e.g. causal-relations between diseases and drugs). In theory, the joint-learning models could be as effective (or better) as those produced by finetuning distributional vectors. However, the performance of joint-learning models has not surpassed that of fine-tuning methods.2 Furthermore, the joint-learning objectives are usually model-specific and are tailored to a particular model, making it difficult to use them with other methods. In thi"
W19-5014,de-marneffe-etal-2006-generating,0,0.0710737,"Missing"
W19-5014,N10-1004,0,0.0201926,"on and relation annotations for complex events related to chemical– protein interaction in molecular biology. The goal of this task is to predict whether a given chemical– protein pair is related or not, and to then verify its corresponding relation type. There are five types of relations: Up-regulator, Down-regulator, Agonist, Antagonist, and Substrate. The corpus is provided in the Turku Event Extraction System (TEES) XML format and are installed with the Turku Extraction System (Bj¨orne, 2014). It is parsed with the the BLLIP parser (Charniak and Johnson, 2005) with the McClosky bio-model (Mcclosky, 2010), followed by conversion of the constituency parses into dependency parses using the Stanford Tools (MacCartney et al., 2006). Table 4 summarizes key statistics for the dataset. Train Dev Test Total initial word embedding layer that maps input texts into matrices, followed by convolutions of different filter sizes and 1-max pooling, and finally a fully connected layer, leading to an output Softmax layer for predicting labels. Performance is evaluated using the standard precision, recall, and F1 -score metrics of the labels in the model. Classification is performed as multilabel classification"
W19-5014,W13-2008,0,0.272906,"ion to update word vectors. It can be applied to any pretrained word embedding vectors. The intuition behind retrofitting is to encourage the retrofitted vectors to be similar to the vectors of related word 125 Proceedings of the BioNLP 2019 workshop, pages 125–134 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics types and similar to their original distributional representations. Using a standard “off-the-shelf” retrofitting algorithm, we apply the idea of retrofitting to verb clusters to two sets of widely-used pretrained embedding vectors in BioNLP (those by Pyysalo et al. (2013a) and by Chiu et al. (2016)) to obtain improved embeddings. We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task. We make our retrofitted embeddings freely available to the BioNLP community along with our code.1 The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain. Retrofitting has thus far only been applied for aligning vectors to Medical S"
W19-5014,P14-2089,0,0.0292684,"ed evaluation. We end with a discussion of the evaluation results. 2 Related work Lexical resources can be used to enrich representation models by providing them other sources of linguistics information beyond the distributional statistics obtained from corpora. In recent literature, various methods to leverage knowledge available in human- and automatically-constructed lexical resources have been proposed. One such method involves modifying the objectives in the original representation learning procedures so that they can jointly learn both distributional and lexical information—for example, Yu and Dredze (2014) modify the CBOW objective function by introducing semantic constraints as obtained from the paraphrase database (Ganitkevitch et al., 2013) to train word representations which focus on word similarity over word relatedness. 1 Our retrofitted embeddings and code are released under an open license and can be found here: https://github.com/cambridgeltl/ retrofitted-bio-embeddings Another class of methods incorporates lexical information into the vector representations as a post-processing procedure. The method fine-tunes the pretrained word vectors to satisfy linguistic constraints from the exte"
W19-5014,W16-6106,0,0.120032,"016)) to obtain improved embeddings. We show that by doing nothing more than using this simple approach, we achieve state-of-the-art results on two text classification tasks (both tasks evaluated on document and sentence level classification), and a relation extraction task. We make our retrofitted embeddings freely available to the BioNLP community along with our code.1 The main contribution of this work is to be the first of its kind to apply verb-based retrofitting in the biomedical domain. Retrofitting has thus far only been applied for aligning vectors to Medical Subject Headings (MeSH) (Yu et al., 2016), and been validated only in an extrinsic setting. We show that with very little effort, we can achieve state-of-the art results on various downstream tasks in a range of biomedical subdomains. This paper will first describe relevant work on retrofitting to lexical resources in BioNLP; we then briefly give an overview of two verb cluster and lexicons that we use in our methodology, and then our task-based evaluation. We end with a discussion of the evaluation results. 2 Related work Lexical resources can be used to enrich representation models by providing them other sources of linguistics inf"
W91-0218,P87-1019,1,0.750828,"Missing"
W91-0218,P86-1004,1,\N,Missing
W91-0218,H86-1011,1,\N,Missing
W98-0104,P97-1026,0,0.0570599,"Missing"
W98-0104,W98-1419,0,0.016817,"e all sources and goals to be present in the elementary tree. Only PPs whose meaning is implicit in the meaning of the verb itself are present in the elementary tree, whereas all other PPs are adjoined. This is in contrast with the analysis provided by Levin and Rappaport Hovav (1995) in which all sources and goals are treated as arguments as a result of a lexical rule that applies to verbs of motion. The goal of our work is to capture lexical semantic properties that we hope will be helpful in reducing the search space in parsing, as well as aid in generation (SPUD; see Stone and Doran 1997; Stone and Webber 1998) and machine translation (in the transfer of lexical semantic properties) (see Palmer, et al. (to appear)). We have examined several subclasses of motion verbs, and posited features to capture their semantic properties. These features not only allow us to place restrictions on the verbs to constrain possible derivations, but also allow us to account for regular sense extensions through the underspecification of certain features and by having modifiers introduce these features in the course of the derivation. The Conative Construction and Elementary Trees The other case to consider is the conat"
W98-0143,C96-1034,0,0.526327,"wh-movement, in many different trees creates redundancy, which poses a problem for grammar development and maintenance {VijayShanker and Schabes, 1992). To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it generates 180 the Penn English"
W98-0143,E89-1009,0,0.0214411,"-subject extraction block. 3 We have not yet attempted to extend our coverage to include punctuation, it-clefts, and a few idiosyncratic analyses that are included in the sixty trees we are not generating. . 182 tures, so that the head features will propagate from modifiee to modified node, while non-head features from the predicate as the head of the modifier will be passed to the modified node. 4 Comparison to Other Work Evans, Gazdar and Weir (Evans et al., 1995) also discuss a method for organizing the trees in a TAG hierarchically, using an existing lexical representational system, DATR (Evans and Gazdar, 1989). Since DATR can not capture directly dominance relation in the trees, these must be simulated by using feature equations. There are substantial similarities and significant differences in our approach and Candito&apos;s approach, which she applied primarily to French and Italian. Both systems have built upon the basic ideas expressed in (Vijay-Shanker and Schabes, 1992) for organizing trees hierarchically and the use of tree descriptions that encode substructures found in several trees. The main difference is how Candito uses her dimensions in generating the trees. Her system imposes explicit cond"
W98-0143,P95-1011,0,0.65657,"e of tree substructures, such as wh-movement, in many different trees creates redundancy, which poses a problem for grammar development and maintenance {VijayShanker and Schabes, 1992). To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it gen"
W98-0143,C92-1034,1,0.845874,"aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it generates 180 the Penn English grammar as well as a Chinese TAG. We describe the significant properties of both grammars, pointing out the major differences between them, and the methods by which our system is informed about these language-specific prop"
xue-etal-2014-interlingua,N07-1051,0,\N,Missing
xue-etal-2014-interlingua,W04-2705,0,\N,Missing
xue-etal-2014-interlingua,A00-2018,0,\N,Missing
xue-etal-2014-interlingua,J93-2004,0,\N,Missing
xue-etal-2014-interlingua,C12-1083,0,\N,Missing
xue-etal-2014-interlingua,W04-3212,1,\N,Missing
xue-etal-2014-interlingua,J03-4003,0,\N,Missing
xue-etal-2014-interlingua,Q13-1034,1,\N,Missing
xue-etal-2014-interlingua,W09-1201,1,\N,Missing
xue-etal-2014-interlingua,J02-3001,0,\N,Missing
xue-etal-2014-interlingua,J05-1004,1,\N,Missing
xue-etal-2014-interlingua,Q13-1019,0,\N,Missing
xue-etal-2014-interlingua,prasad-etal-2008-penn,0,\N,Missing
xue-etal-2014-interlingua,W13-2322,1,\N,Missing
xue-etal-2014-interlingua,N04-1030,0,\N,Missing
Y02-1007,P97-1003,0,0.0201622,"1995) and Penn Chinese Treebank, (Xia et al., 2000b). This annotation is preferable to a pure dependency annotation because it can encode richer structural information. For instance, some of the structural information that a phrase structure annotation can encode, while dependency annotation cannot, are (i) phrasal level node labels such as VP and NP; (ii) explicit representation of empty arguments; (iii) distinction between complementation and adjunction; and (iv) use of traces for displaced constituents. Although having traces and empty arguments may be controversial, it has been shown in (Collins, 1997; Collins et al., 1999) that such rich structural annotation is crucial in improving the efficiency of stochastic parsers that are trained on Treebanks. Moreover, it has been shown in (Rambow and Joshi, 1997) that a complete mapping from dependency structure to phrase structure cannot be done, although the other direction is possible. This means that a phrase structure Treebank can always be converted to a dependency Treebank if necessary, but not the other way around. The bracketing tagset of our Treebank can be divided into four types: (i) POS tags for headlevel annotation (e.g., NNC, VV, AD"
Y02-1007,P99-1065,0,0.0191019,"Chinese Treebank, (Xia et al., 2000b). This annotation is preferable to a pure dependency annotation because it can encode richer structural information. For instance, some of the structural information that a phrase structure annotation can encode, while dependency annotation cannot, are (i) phrasal level node labels such as VP and NP; (ii) explicit representation of empty arguments; (iii) distinction between complementation and adjunction; and (iv) use of traces for displaced constituents. Although having traces and empty arguments may be controversial, it has been shown in (Collins, 1997; Collins et al., 1999) that such rich structural annotation is crucial in improving the efficiency of stochastic parsers that are trained on Treebanks. Moreover, it has been shown in (Rambow and Joshi, 1997) that a complete mapping from dependency structure to phrase structure cannot be done, although the other direction is possible. This means that a phrase structure Treebank can always be converted to a dependency Treebank if necessary, but not the other way around. The bracketing tagset of our Treebank can be divided into four types: (i) POS tags for headlevel annotation (e.g., NNC, VV, ADV); (ii) syntactic tags"
Y02-1007,han-etal-2000-handling,1,0.869784,"Missing"
Y02-1007,J93-2004,0,0.0237339,"tically more sound, from a descriptive point of view, they are more like bound morphemes, in that they are rarely separated from stems in written form, and native speakers of Korean share the intuition that they can never stand alone meaningfully in both written and spoken form. To reflect this intuition, we have chosen to annotate the inflections as bound morphemes assigning them each with a function tag. 2.2 Syntactic bracketing Penn Korean Treebank uses phrase structure annotation for syntactic bracketing. Similar phrase structure annotation schemes were also used by Penn English Treebank (Marcus et al., 1993; Bies et al., 1995), Penn Middle English Treebank (Kroch and Taylor, 1995) and Penn Chinese Treebank, (Xia et al., 2000b). This annotation is preferable to a pure dependency annotation because it can encode richer structural information. For instance, some of the structural information that a phrase structure annotation can encode, while dependency annotation cannot, are (i) phrasal level node labels such as VP and NP; (ii) explicit representation of empty arguments; (iii) distinction between complementation and adjunction; and (iv) use of traces for displaced constituents. Although having tr"
Y02-1007,A97-1014,0,0.0845418,"Missing"
Y02-1007,xia-etal-2000-developing,1,0.886167,"from stems in written form, and native speakers of Korean share the intuition that they can never stand alone meaningfully in both written and spoken form. To reflect this intuition, we have chosen to annotate the inflections as bound morphemes assigning them each with a function tag. 2.2 Syntactic bracketing Penn Korean Treebank uses phrase structure annotation for syntactic bracketing. Similar phrase structure annotation schemes were also used by Penn English Treebank (Marcus et al., 1993; Bies et al., 1995), Penn Middle English Treebank (Kroch and Taylor, 1995) and Penn Chinese Treebank, (Xia et al., 2000b). This annotation is preferable to a pure dependency annotation because it can encode richer structural information. For instance, some of the structural information that a phrase structure annotation can encode, while dependency annotation cannot, are (i) phrasal level node labels such as VP and NP; (ii) explicit representation of empty arguments; (iii) distinction between complementation and adjunction; and (iv) use of traces for displaced constituents. Although having traces and empty arguments may be controversial, it has been shown in (Collins, 1997; Collins et al., 1999) that such rich"
zhao-etal-2000-machine,C90-3001,0,\N,Missing
zhao-etal-2000-machine,C90-3045,0,\N,Missing
zhao-etal-2000-machine,W90-0102,0,\N,Missing
