2020.coling-main.425,D18-2029,0,0.0116276,"obabilistic Language Model (Bengio et al., 2003), word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LexVec (Salle et al., 2016), dependency-based embeddings (DepVec) (Levy and Goldberg, 2014) and more recently ELMo (Peters et al., 2018). ELMo can generate different word embeddings for a word that captures the context of a word – that is its position in a sentence. ELMo achieves this by using two deep unidirectional LSTMs (forward and backward) and then computing embedding for a word as a weighted combination of hidden layer outputs at that position. Universal Sentence Encoder (Cer et al., 2018) is based on the Transformer encoder (Vaswani et al., 2017) and a deep averaging network. It is trained using unsupervised data from Wikipedia, web news, web question-answer pages and discussion forums, and supervised data from the Stanford Natural Language Inference (SNLI) corpus. We experiment with two models – ELMo (Peters et al., 2018) and Google’s Universal sentence encoder (Cer et al., 2018) representations for transforming the clickbait title into a dense numerical vector representation. 5.2 Transformer Representations After the original Transformer work by Vaswani et al. (2017), severa"
2020.coling-main.425,D14-1181,0,0.00276962,"ral Network (RNN) (Schuster and Paliwal, 1997) and fastText (Joulin et al., 2016) on word distributed representations, respectively for clickbait detection. Most of the initial efforts on clickbait detection focused only on news headlines. Recently, there have been efforts at identifying clickbaits from social media like Twitter. Potthast et al. (2016) trained a random forest classifier by extracting various features from the post texts, linked webpages and associated meta information of tweets, to decide if a tweet was a clickbait. Agrawal (2016) trained a Convolutional Neural Network (CNN) (Kim, 2014), using the post texts only, to detect clickbait posts in Reddit, Facebook and Twitter. In (Chakraborty et al., 2017), researchers analysed the differences in content, sentiment, consumers, etc., between the clickbait and non-clickbait tweets. 2.2 Clickbait Regression Binary clickbait classification is not sufficient. Rather, it is useful to predict the finegrained intensity of the clickbait which can enable ranking of clickbaits, thereby providing a knob for elimination of clickbaits rather than a blanket binary elimination. The Clickbait Challenge (Potthast et al., 2017) has 4837 Team Name c"
2020.coling-main.425,P14-2050,0,0.0174782,"pplications as they provide semantic vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (CamachoCollados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LexVec (Salle et al., 2016), dependency-based embeddings (DepVec) (Levy and Goldberg, 2014) and more recently ELMo (Peters et al., 2018). ELMo can generate different word embeddings for a word that captures the context of a word – that is its position in a sentence. ELMo achieves this by using two deep unidirectional LSTMs (forward and backward) and then computing embedding for a word as a weighted combination of hidden layer outputs at that position. Universal Sentence Encoder (Cer et al., 2018) is based on the Transformer encoder (Vaswani et al., 2017) and a deep averaging network. It is trained using unsupervised data from Wikipedia, web news, web question-answer pages and discus"
2020.coling-main.425,D14-1162,0,0.0850988,"s useful to predict the finegrained intensity of the clickbait which can enable ranking of clickbaits, thereby providing a knob for elimination of clickbaits rather than a blanket binary elimination. The Clickbait Challenge (Potthast et al., 2017) has 4837 Team Name carpetshark whitebait pike tuna torpedo salmon snapper albacore pineapplefish zingel Method Ensemble of Linear SVMs LSTMs, word2vec (Mikolov et al., 2013) Hand-crafted 331 features, Linear, Logistic, Random Forest regression Character level embeddings using CNNs, word2vec (Mikolov et al., 2013), LSTMs Hand-crafted features, GloVe (Pennington et al., 2014), Linear Regression Hand-crafted features, XGBoost 65 Hand-crafted features, Stacking Bidirectional GRUs (Cho et al., 2014), GloVe (Pennington et al., 2014) CNN and Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) Bidirectional GRUs (Cho et al., 2014) with attention (Bahdanau et al., 2014), GloVe (Pennington et al., 2014) Paper (Grigorev, 2017) (Thomas, 2017) (Cao et al., 2017) (Gairola et al., 2017) (Indurthi and Oota, 2017) (Elyashar et al., 2017) (Papadopoulou et al., 2017) (Omidvar et al., 2018) (Glenski et al., 2017) (Zhou, 2017) Table 2: Some of the best teams who partici"
2020.coling-main.425,N18-1202,0,0.0160266,"sentation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (CamachoCollados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LexVec (Salle et al., 2016), dependency-based embeddings (DepVec) (Levy and Goldberg, 2014) and more recently ELMo (Peters et al., 2018). ELMo can generate different word embeddings for a word that captures the context of a word – that is its position in a sentence. ELMo achieves this by using two deep unidirectional LSTMs (forward and backward) and then computing embedding for a word as a weighted combination of hidden layer outputs at that position. Universal Sentence Encoder (Cer et al., 2018) is based on the Transformer encoder (Vaswani et al., 2017) and a deep averaging network. It is trained using unsupervised data from Wikipedia, web news, web question-answer pages and discussion forums, and supervised data from the Sta"
2020.coling-main.425,C18-1127,0,0.0248363,"ons for the clickbait strength prediction problem. In this challenge, the goal is to predict the intensity of clickbaits rather than just predicting if a particular item is a clickbait or not. Table 2 shows various approaches that have been proposed for the clickbait intensity prediction task. Some approaches use traditional machine learning regression methods using a large set of hand crafted features, while others look at neural architectures (like RNNs, LSTMs, GRUs and CNNs) supported by word embeddings like word2vec and GloVe. 3 Dataset The Webis Clickbait Corpus consists of 38517 tweets (Potthast et al., 2018). Restricting to Englishlanguage publishers, Potthast et al. (2018) obtain a ranking of the top-most retweeted news publishers from the NewsWhip social media analytics service5 . Taking the top 27 publishers, they used Twitter”s API to record every tweet they published in the period from December 1, 2016, through April 30, 2017. They filtered and sampled from this collection of 459541 tweets to obtain a clean dataset of 38517 tweets. Each of the tweets was annotated for clickbait intensity label by five different workers from Amazon Mechanical Turk (AMT). A 4-point Likert scale was followed wi"
2020.coling-main.425,P16-2068,0,0.0132138,"ve been widely used in modern Natural Language Processing applications as they provide semantic vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (CamachoCollados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), LexVec (Salle et al., 2016), dependency-based embeddings (DepVec) (Levy and Goldberg, 2014) and more recently ELMo (Peters et al., 2018). ELMo can generate different word embeddings for a word that captures the context of a word – that is its position in a sentence. ELMo achieves this by using two deep unidirectional LSTMs (forward and backward) and then computing embedding for a word as a weighted combination of hidden layer outputs at that position. Universal Sentence Encoder (Cer et al., 2018) is based on the Transformer encoder (Vaswani et al., 2017) and a deep averaging network. It is trained using unsupervised dat"
2020.coling-main.511,D18-2029,0,0.01182,"biLSTM-Attention: The word embeddings for a post are passed through a bidirectional LSTM with and without the attention scheme from Yang et al. (2016). Hierarchical-biLSTM-Attention: In an architecture similar to Yang et al. (2016) with GRU replaced with LSTM, the word embeddings are first fed to biLSTM with attention to create a representation for each sentence. These sentence embeddings are then passed through another instance of biLSTM with attention. BERT-biLSTM-Attention and USE-biLSTM-Attention: Sentence representations are generated using BERT via bert-as-service (Xiao, 2018) and USE (Cer et al., 2018) each and fed to a biLSTM with attention. CNN-Kim: Convolutional and max-over-time pooling layers are applied to the word vectors for a post in this method similar to Kim (2014). Table 2: Results for the proposed methods as well as the traditional machine learning (TML), deep learning (DL), and semi-supervised baselines TML baselines DL baselines Semi-supervised baselines Proposed multi-task methods Proposed objective functions with (Parikh et al., 2019) Our best method Approach Random Word-ngrams-SVM Word-ngrams-LR Word-ngrams-RF ELMO-SVM ELMO-LR ELMO-RF biLSTM biLSTM-Attention Hierarchical-b"
2020.coling-main.511,P19-1241,0,0.0116513,"e detection of sexism differs from and can complement the classification of sexism. In a forum where instances of sexism are mixed with other posts unrelated to sexism, sexism detection can be used to identify the posts on which to perform sexism classification. Moreover, we observe the distinction between sexist statements (e.g., posts whereby one perpetrates sexism) and the accounts of sexism suffered or witnessed (e.g., personal recollections shared as part of the #metoo movement). We also note the prior work on detecting or classifying personal stories of sexual harassment and/or assault (Chowdhury et al., 2019; Karlekar and Bansal, 2018). In this paper, we focus on classifying an account (report) of sexism involving any set of categories of sexism. Most of the existing research on sexism classification (Anzovino et al., 2018; Jafarpour et al., 2018; Jha and Mamidi, 2017) considers at most five categories of sexism. Further, the majority of prior approaches associate only one category of sexism with an instance of sexism. Having mutually exclusive categories of sexism is unreasonable and limiting, as substantiated by Table 1. To the best of our knowledge, Parikh et al. (2019) is the only work that e"
2020.coling-main.511,P18-1064,0,0.0128863,"pe(s) of sexism that goes further than using unlabeled instances only for fine-tuning pre-trained models. Multi-Task Learning (MTL) is inspired by human learning activities wherein people often apply the knowledge learned from previous tasks to help learn a new task (Zhang and Yang, 2017). It is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. There has been considerable interest in applying MTL to a variety of problems including text classification using deep neural networks (DNNs) (Liu et al., 2019; Xu et al., 2019; Guo et al., 2018; Ruder et al., 2019). MTL provides an effective way of leveraging labeled data from auxiliary tasks for the core task, especially when labeled data available for single-task learning is not large. In this work, we adopt MTL for fine-grained multi-label sexism classification using several auxiliary tasks. 3 Proposed Semi-supervised Multi-task Approach for Sexism Classification Our problem statement is to classify an account of sexism (also referred to as a post henceforth) into one or more of 23 categories of sexism. This section introduces a semi-supervised multi-task approach for it. We begi"
2020.coling-main.511,W18-5114,0,0.0201335,"Missing"
2020.coling-main.511,W17-2902,0,0.157633,"over, we observe the distinction between sexist statements (e.g., posts whereby one perpetrates sexism) and the accounts of sexism suffered or witnessed (e.g., personal recollections shared as part of the #metoo movement). We also note the prior work on detecting or classifying personal stories of sexual harassment and/or assault (Chowdhury et al., 2019; Karlekar and Bansal, 2018). In this paper, we focus on classifying an account (report) of sexism involving any set of categories of sexism. Most of the existing research on sexism classification (Anzovino et al., 2018; Jafarpour et al., 2018; Jha and Mamidi, 2017) considers at most five categories of sexism. Further, the majority of prior approaches associate only one category of sexism with an instance of sexism. Having mutually exclusive categories of sexism is unreasonable and limiting, as substantiated by Table 1. To the best of our knowledge, Parikh et al. (2019) is the only work that explores the multi-label categorization of accounts involving any type(s) of sexism. It provides the largest dataset containing ∗ *Both authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons."
2020.coling-main.511,D18-1303,0,0.360051,"ffers from and can complement the classification of sexism. In a forum where instances of sexism are mixed with other posts unrelated to sexism, sexism detection can be used to identify the posts on which to perform sexism classification. Moreover, we observe the distinction between sexist statements (e.g., posts whereby one perpetrates sexism) and the accounts of sexism suffered or witnessed (e.g., personal recollections shared as part of the #metoo movement). We also note the prior work on detecting or classifying personal stories of sexual harassment and/or assault (Chowdhury et al., 2019; Karlekar and Bansal, 2018). In this paper, we focus on classifying an account (report) of sexism involving any set of categories of sexism. Most of the existing research on sexism classification (Anzovino et al., 2018; Jafarpour et al., 2018; Jha and Mamidi, 2017) considers at most five categories of sexism. Further, the majority of prior approaches associate only one category of sexism with an instance of sexism. Having mutually exclusive categories of sexism is unreasonable and limiting, as substantiated by Table 1. To the best of our knowledge, Parikh et al. (2019) is the only work that explores the multi-label cate"
2020.coling-main.511,D14-1181,0,0.00238631,"In an architecture similar to Yang et al. (2016) with GRU replaced with LSTM, the word embeddings are first fed to biLSTM with attention to create a representation for each sentence. These sentence embeddings are then passed through another instance of biLSTM with attention. BERT-biLSTM-Attention and USE-biLSTM-Attention: Sentence representations are generated using BERT via bert-as-service (Xiao, 2018) and USE (Cer et al., 2018) each and fed to a biLSTM with attention. CNN-Kim: Convolutional and max-over-time pooling layers are applied to the word vectors for a post in this method similar to Kim (2014). Table 2: Results for the proposed methods as well as the traditional machine learning (TML), deep learning (DL), and semi-supervised baselines TML baselines DL baselines Semi-supervised baselines Proposed multi-task methods Proposed objective functions with (Parikh et al., 2019) Our best method Approach Random Word-ngrams-SVM Word-ngrams-LR Word-ngrams-RF ELMO-SVM ELMO-LR ELMO-RF biLSTM biLSTM-Attention Hierarchical-biLSTM-Attention BERT-biLSTM-Attention USE-biLSTM-Attention CNN-Kim CNN-biLSTM-Attention C-biLSTM BERT-t-biLSTM-Attention Self-training (Parikh et al., 2019) Auxiliary tasks Topi"
2020.coling-main.511,P19-1441,0,0.0192106,"ation of accounts describing any type(s) of sexism that goes further than using unlabeled instances only for fine-tuning pre-trained models. Multi-Task Learning (MTL) is inspired by human learning activities wherein people often apply the knowledge learned from previous tasks to help learn a new task (Zhang and Yang, 2017). It is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. There has been considerable interest in applying MTL to a variety of problems including text classification using deep neural networks (DNNs) (Liu et al., 2019; Xu et al., 2019; Guo et al., 2018; Ruder et al., 2019). MTL provides an effective way of leveraging labeled data from auxiliary tasks for the core task, especially when labeled data available for single-task learning is not large. In this work, we adopt MTL for fine-grained multi-label sexism classification using several auxiliary tasks. 3 Proposed Semi-supervised Multi-task Approach for Sexism Classification Our problem statement is to classify an account of sexism (also referred to as a post henceforth) into one or more of 23 categories of sexism. This section introduces a semi-supervised"
2020.coling-main.511,D19-1174,1,0.0798231,"sment and/or assault (Chowdhury et al., 2019; Karlekar and Bansal, 2018). In this paper, we focus on classifying an account (report) of sexism involving any set of categories of sexism. Most of the existing research on sexism classification (Anzovino et al., 2018; Jafarpour et al., 2018; Jha and Mamidi, 2017) considers at most five categories of sexism. Further, the majority of prior approaches associate only one category of sexism with an instance of sexism. Having mutually exclusive categories of sexism is unreasonable and limiting, as substantiated by Table 1. To the best of our knowledge, Parikh et al. (2019) is the only work that explores the multi-label categorization of accounts involving any type(s) of sexism. It provides the largest dataset containing ∗ *Both authors contributed equally. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5810 Proceedings of the 28th International Conference on Computational Linguistics, pages 5810–5820 Barcelona, Spain (Online), December 8-13, 2020 Table 1: An Instance of sexism associated with multiple categories Account “A colleague once saw me washing my coffe"
2020.coling-main.511,D14-1162,0,0.0835629,"ification Fig. 2 presents our multi-task neural architecture. If the list of chosen tasks consists of sexism detection, the training input is a batch of tuples (samples), each of which consists of a post from L and |L| posts randomly picked from the sexism detection training data each. Each of the two posts in a sample is processed hierarchically for the creation of its post representation similar to Parikh et al. (2019) using the same set of layers/weights (block A in Fig. 2). First, the words of each sentence (in each post) are embedded separately using ELMo (Peters et al., 2018) and GloVe (Pennington et al., 2014). The two word vector matrices are passed through biLSTM linked with an attention scheme (Yang et al., 2016), yielding two sentence representations. This is complemented by another sentence embedding generated using BERT-t. Next, the concatenation of the three sentence vectors is fed to biLSTM + attention to 5813 create the post representation. The vector representation for the post from L is passed to a fully connected layer custom-made for each of sexism classification, Topic-p, and Cl-pred. The other post representation is fed to a dense layer designed for S-det. For sexism classification a"
2020.coling-main.511,N18-1202,0,0.023698,"Proposed multi-task neural classification Fig. 2 presents our multi-task neural architecture. If the list of chosen tasks consists of sexism detection, the training input is a batch of tuples (samples), each of which consists of a post from L and |L| posts randomly picked from the sexism detection training data each. Each of the two posts in a sample is processed hierarchically for the creation of its post representation similar to Parikh et al. (2019) using the same set of layers/weights (block A in Fig. 2). First, the words of each sentence (in each post) are embedded separately using ELMo (Peters et al., 2018) and GloVe (Pennington et al., 2014). The two word vector matrices are passed through biLSTM linked with an attention scheme (Yang et al., 2016), yielding two sentence representations. This is complemented by another sentence embedding generated using BERT-t. Next, the concatenation of the three sentence vectors is fed to biLSTM + attention to 5813 create the post representation. The vector representation for the post from L is passed to a fully connected layer custom-made for each of sexism classification, Topic-p, and Cl-pred. The other post representation is fed to a dense layer designed fo"
2020.coling-main.511,R15-1086,0,0.0705881,"Missing"
2020.coling-main.511,P16-2037,0,0.0136638,"0.475 0.307 0.472 0.499 0.516 0.431 0.402 0.513 0.278 0.328 0.472 0.557 0.572 SA 0.003 0.107 0.287 0.272 0.206 0.279 0.185 0.147 0.176 0.191 0.089 0.061 0.195 0.035 0.038 0.127 0.220 0.242 0.716 0.716 0.720 0.726 0.724 0.720 0.728 0.716 0.711 0.723 0.718 0.731 0.560 0.558 0.552 0.559 0.558 0.550 0.565 0.546 0.550 0.550 0.559 0.573 0.669 0.668 0.673 0.679 0.679 0.673 0.677 0.668 0.663 0.672 0.670 0.681 0.574 0.577 0.581 0.589 0.589 0.583 0.590 0.575 0.570 0.584 0.577 0.595 0.241 0.251 0.256 0.273 0.275 0.266 0.276 0.242 0.247 0.264 0.245 0.281 CNN-biLSTM-Attention: In this baseline similar to Wang et al. (2016), each sentence’s word embeddings are passed through convolutional and max-over-time pooling layers. The resultant representations are then passed through a biLSTM with attention. C-biLSTM: This is a variant of the C-LSTM architecture (Zhou et al., 2015) somewhat related to a method used in Karlekar and Bansal (2018). After applying convolution on a post’s word vectors, the feature maps are stacked along the filter dimension to generate a series of window vectors, which are subsequently fed to biLSTM. • Semi-supervised BERT-t-biLSTM-Attention: This is the same as BERT-biLSTM-Attention except t"
2020.coling-main.511,N16-2013,0,0.0343088,"re also included in this review. We end this section with a brief review of multi-task learning. Melville et al. (2018) apply topic modeling to data obtained from The Everyday Sexism Project and maps the semantic relations between topics. ElSherief et al. (2017) study user engagement with posts related to gender based violence and their language nuances. Since sexism classification can be preceded by sexism detection to remove posts unrelated to sexism, we note that sexism is detected by some hate speech classification methods that include sexism as a category of hate (Badjatiya et al., 2017; Waseem and Hovy, 2016; Zhang and Luo, 2018; Davidson et al., 2017). Frenda et al. (2019) present an approach for detecting sexism and misogyny from tweets. Given our focus on sexism classification, we do not delve into prior work related to hate speech or cyber-bullying (Van Hee et al., 2015; Agrawal and Awekar, 2018). Karlekar and Bansal (2018) explore CNN, RNN, and a combination of them for categorizing personal experiences of sexual harassment into one or more of three classes. In Yan et al. (2019), a density matrix encoder inspired by quantum mechanics is used for the classification of personal stories of sexu"
2020.coling-main.511,N19-1271,0,0.0158318,"describing any type(s) of sexism that goes further than using unlabeled instances only for fine-tuning pre-trained models. Multi-Task Learning (MTL) is inspired by human learning activities wherein people often apply the knowledge learned from previous tasks to help learn a new task (Zhang and Yang, 2017). It is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. There has been considerable interest in applying MTL to a variety of problems including text classification using deep neural networks (DNNs) (Liu et al., 2019; Xu et al., 2019; Guo et al., 2018; Ruder et al., 2019). MTL provides an effective way of leveraging labeled data from auxiliary tasks for the core task, especially when labeled data available for single-task learning is not large. In this work, we adopt MTL for fine-grained multi-label sexism classification using several auxiliary tasks. 3 Proposed Semi-supervised Multi-task Approach for Sexism Classification Our problem statement is to classify an account of sexism (also referred to as a post henceforth) into one or more of 23 categories of sexism. This section introduces a semi-supervised multi-task approa"
2020.coling-main.511,N16-1174,0,0.0685904,"tion, the training input is a batch of tuples (samples), each of which consists of a post from L and |L| posts randomly picked from the sexism detection training data each. Each of the two posts in a sample is processed hierarchically for the creation of its post representation similar to Parikh et al. (2019) using the same set of layers/weights (block A in Fig. 2). First, the words of each sentence (in each post) are embedded separately using ELMo (Peters et al., 2018) and GloVe (Pennington et al., 2014). The two word vector matrices are passed through biLSTM linked with an attention scheme (Yang et al., 2016), yielding two sentence representations. This is complemented by another sentence embedding generated using BERT-t. Next, the concatenation of the three sentence vectors is fed to biLSTM + attention to 5813 create the post representation. The vector representation for the post from L is passed to a fully connected layer custom-made for each of sexism classification, Topic-p, and Cl-pred. The other post representation is fed to a dense layer designed for S-det. For sexism classification and S-det, the sigmoid activation is used; for the other two tasks, we use softmax. For sexism classification"
2020.coling-main.511,P95-1026,0,0.780765,"Missing"
2020.finnlp-1.17,D14-1181,0,0.00802914,"Missing"
2020.finnlp-1.17,D14-1162,0,0.0822269,"Missing"
2020.finnlp-1.17,L16-1056,0,0.032851,"Missing"
2020.finnlp-1.17,S16-1168,0,0.0367453,"Missing"
2020.finnlp-1.17,S18-1115,0,0.0352407,"Missing"
2020.finnlp-1.17,D18-2029,0,0.0281678,"Missing"
2020.finnlp-1.17,C92-2082,0,0.570247,"Missing"
2020.nuse-1.11,W13-3520,0,0.011428,"rimental Setup We used the Google SyntaxNet2 for dependency parsing the Hindi sentences. SyntaxNet is a TensorFlow toolkit for deep learning powered natural language understanding developed at Google. The Parsey Universal component of SyntaxNet supports NLP preprocessing tasks such as POS tagging, morphological analysis and dependency parsing for 40 different languages including Hindi. We employed two different NER approaches proposed for Hindi and consider a word as part of a named entity if either or both of them identify it as a named entity. One of the approaches is Polyglot, proposed in (Al-Rfou et al., 2013). It is based on using language agnostic techniques involving Wikipedia and Freebase and no human annotated NER training data. The second approach is proposed by (Murthy et al., 2019) and is based on a supervised deep learning architecture for NER in Hindi. To access the Hindi WordNet (Narayan et al., 2002), we use the pyiwn toolkit (Panjwani et al., 2018) which is a python API to access Indian language WordNets. We use the Sardar dataset as a training set to iteratively revise and improve the extraction algorithms while keeping the other datasets unseen. In order to filter out an irrealis or"
2020.nuse-1.11,W17-5912,1,0.737164,"in the sentence rAm n  EktAb dF (’Ram ne Kitab di’; Ram gave the book), dF (di;give) is the event, however in the sentence rAm n  EktAb Ko dF (’Ram ne Kitab kho di’; Ram lost the book.), Ko dF(’kho di’; lost) is the event. Similarly, in the sentence rAm s  EktAb gm ho gI (’Ram se kitab gum ho gayee’; Ram lost the book), gm ho gI (’gum ho gayee’; lost) is the event. As compared to English, LVCs are more common in Hindi, have different We also address the problem of automatically extracting the MSC representation from a given narrative text. MSC extends the notion of a single event-timeline (Bedi et al., 2017) for a narrative by providing a timeline per actor (entity of interest). MSC representation captures all the actors and interactions in an easy to visualize manner and hence make the text more comprehensible. Further, the representation’s support for inference mechanisms opens up possibilities of tackling natural language understanding problems like text comprehension and question answering. Our work is similar to (Palshikar et al., 2019) to represent a English narrative using MSC. However, due to intricacies of events in Hindi language, their approach cannot be used for construction of MSC of"
2020.nuse-1.11,2018.gwc-1.47,1,0.720307,"rent languages including Hindi. We employed two different NER approaches proposed for Hindi and consider a word as part of a named entity if either or both of them identify it as a named entity. One of the approaches is Polyglot, proposed in (Al-Rfou et al., 2013). It is based on using language agnostic techniques involving Wikipedia and Freebase and no human annotated NER training data. The second approach is proposed by (Murthy et al., 2019) and is based on a supervised deep learning architecture for NER in Hindi. To access the Hindi WordNet (Narayan et al., 2002), we use the pyiwn toolkit (Panjwani et al., 2018) which is a python API to access Indian language WordNets. We use the Sardar dataset as a training set to iteratively revise and improve the extraction algorithms while keeping the other datasets unseen. In order to filter out an irrealis or non-punctual event, we check its respective P-COMP against a manually curated list of verbs such as яAtA ,яAyA, rhF, rh , cAh , cAhtF, honA, hotF indicating either continuity or uncertainty of events. We also ignore events appearing in quotes as they are likely to be authors’ opinions. 4 emergency 56 1373 78 74 Table 2: Dataset Statistics Figure 2: Examp"
2020.nuse-1.11,2020.isa-1.2,0,0.02677,"he Indian Independence Movement in Table 1 and the corresponding MSC in Figure 1. 2 Key Annotation and Extraction Challenges for Events in Hindi Annotation and extraction of events and their arguments from English texts is a challenging task (Mitamura et al., 2015). In case of Hindi there are more challenges as compared to English. Following are the key challenges we observed while processing Hindi narratives. I. Absence of annotation guidelines and labelled data similar to ACE: Only few attempts have been made to define comprehensive event annotation guidelines for Hindi. (Goud et al., 2019; Goel et al., 2020) propose a set of guidelines for annotation of event mentions in Hindi. However, they do not consider arguments of events, which are vital for narrative processing. II. Annotation of events with Light Verb Constructs (LVCs): LVCs are formed from a commonly used verb and usually a noun phrase (NP) in its direct object position, such as have a look or take an action. For example, in the sentence rAm n  EktAb dF (’Ram ne Kitab di’; Ram gave the book), dF (di;give) is the event, however in the sentence rAm n  EktAb Ko dF (’Ram ne Kitab kho di’; Ram lost the book.), Ko dF(’kho di’; lost) is the e"
2020.nuse-1.11,P96-1054,0,0.378358,"happened or not. Following are examples of sentences with realis and irrealis mood events: • Realis: ldn яAkr uho
  b {Er-VrF kF pYAI kF (landan jakar unhone baristari ki padhai ki; He went to London and studied law) • Irrealis: yEd srdAr kC vq aOr яFEvt rht  to BArt kA kAyAkSp ho яAtA (yadi sardar kuch varsh aur jeewit rehte to pure bharat ka kayakalp ho jata; If Sardar remained alive for a few more years, India would have been transformed.) IV. Only punctual events are annotated as events. An event is punctual if it “does not have a transitional phase between its start and end point” (Kay and Aylett, 1996). This implies that a process in continuation is not punctual and hence not marked as a valid event. • Punctual event: EksAno n  ag  }я srkAr s  BArF kr m  C  V kF mAg kF (kisano ne angrejh sarkar se bhari kar me chut ki mang ki; The farmers demanded the British government, a waiver in the heavy taxes) • Non-punctual event: gяrAt kA K XA KX un Edno sK  kF cp V m  TA (gujrat ka kheda khand un dino sukhe ki II. The head verb of an event predicate is tagged as PIVOT. If an event predicate is a conjunct predicate, its noun element is tagged as P-CONJ. In case of a compound predica"
2020.nuse-1.11,H05-1004,0,0.064958,"Missing"
2020.nuse-1.11,P14-5010,0,0.00244224,"t is described (Chatman, 1975). In this paper we focus on visualization of the plot aspect of a Hindi narrative. Hindi is an Indo-Aryan language spoken by around 300 million people in India. Additionally, Hindi is the fourth most-spoken first language in the world1 . In comparison to English, Hindi has different linguistic characteristics leading to a different set of NLP challenges. First of all, Hindi is a Subject-Object-Verb (SOV) language with relatively free word order, as against the SVO order in English. Secondly, Hindi does not have high accuracy NLP toolkits such as Stanford CoreNLP (Manning et al., 2014). In this paper, we make one of the first attempts to facilitate event processing in Hindi by proposing annotation guidelines for events as well as their arguments. We propose to represent a Hindi narrative using Message Sequence Charts (MSC) (Rudolph Introduction Narratives are used to communicate complex ideas, detailed accounts of complex events or arguments about one’s beliefs (Valls Vargas, 2017). Moreover, a narrative is a powerful tool not just from entertainment perspective but is one of the core component of human memory, knowledge and intelligence (Schank and Morson, 1995). Narrative"
2020.nuse-1.11,C16-1125,0,0.028027,"and guidelines for annotation of event arguments, (ii) we propose an approach to identify event predicates and their arguments, (iii) MSC based visu88 found. As pronouns are a closed set of words, we use a manually prepared list of pronouns and corresponding types to identify all pronoun mentions of the actors. It is ensured that the list does not include any demonstrative pronouns (such as yh, vh, un, us). Each pronoun in the text is checked against the list and marked as an actor, if found. characteristics and are used as a preferred method for introducing new predicates into the language (Vaidya et al., 2016). A state-of-the-art approach (Chen et al., 2015) propose a supervised approach to identify LVCs in English using resources like PropBank, the OntoNotes sense groupings, WordNet and the British National Corpus. However, it is difficult to extend this approach for Hindi as it would require extensive efforts to create labelled data based on such resources for Hindi. III. Annotation of nominal events: An event is a nominal event if it is described by a noun. Annotation of nominal events is a challenging task, as eventiveness of a noun depends on the context in which it is used. In this paper, we"
2020.nuse-1.11,M95-1005,0,0.0648782,"e facets like actor identification, actor coreference resolution, event extraction and event argument finding. To assess each of these facets, we carry out evaluation of the proposed approach at multiple levels. As the first level, we check the performance of actor identification and coreference resolution. If an actor predicted by the approach is present in the gold standard, it is marked as a true positive. False positives and false negatives are computed accordingly. We report the F1 scores for actor mention identification for each dataset in Table 3. At this level, we also report the MUC (Vilain et al., 1995), the B 3 (Bagga and Baldwin, 1998) and the Experimentation Details Datasets We carry out our experiments on the four text narratives from Indian History, contributed by (Ramrakhiyani et al., 2018). We obtain the dataset text and gold actor annotations and we carry out the event annotations for these datasets with the help of three annotators. The statistics about the datasets are described in Table 2. 2 https://github.com/tensorflow/models/ tree/master/research/syntaxnet 92 nsubj dobj mhA(mA gADF n  srdAr mahatma gandhi ne sardar |{z } sender pV l patel |{z } receiver ko in EryAsto k  bA"
2020.sdp-1.39,N18-2097,0,0.0225026,"so as to obtain a concise abstractive summary? We investigate answers to these questions in this paper. Multiple survey papers have provided a detailed overview of the automatic text summarization task (Tas and Kiyani, 2007; Nenkova and McKeown, 2012; Allahyari et al., 2017). Most of the practically usable summarization systems are extractive in nature. Also, most summarization studies have focused on summarization of news articles. In this work, we mainly focus on two interesting aspects of text summarization: (1) summarization of scientific research papers, and (2) summarization for laymen. Cohan et al. (2018) propose that section level processing of scientific documents is useful. Further, Collins et al. (2017) conclude that not all sections are equally useful. Also, recent papers have observed that a hierarchical summarization of scientific documents is highly effective where at the first level, an extractive summary of each section is independently generated and at the second level, the sectional output is abstracted into a brief summary (Subramanian et al., 2019; Erera et al., 2019). (Xiao and Carenini, 2019) observe that while summarizing, local context is useful, but global is not. Thus, in o"
2020.sdp-1.39,C08-1018,0,0.0582069,"et al., 2017) have been proposed for extractive summarization. 2.1.2 Abstractive Summarization In abstractive summarization, the model tries to generate the summary instead of extracting sentences or keywords. As compared to extractive summarization, this is more challenging and requires strong language modeling schemes to achieve good results. Traditionally, abstractive summarization techniques have focused on generating short text such as headlines or titles. But more recently, there have been efforts on generation of longer summaries. Older methods have depended on tree transduction rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend and Lapata, 2011) for effective abstractive summarization. Recently, neural summarization approaches have been found to be more effective. Effective neural representative language models are very important for text generation tasks. With the recent breakthrough of Transformer-based (Vaswani et al., 2017) architectures like BERT (Devlin et al., 2018), T5 (Raffel 337 Match Scores Mean R1 Mean R2 summary. A set of 37 research papers were pro40.0 Mean R1 Mean R2 vided as the Mean blindRLtest data. The LaySumm dataset 30.0 comprises of full-text p"
2020.sdp-1.39,K17-1021,0,0.021041,". Multiple survey papers have provided a detailed overview of the automatic text summarization task (Tas and Kiyani, 2007; Nenkova and McKeown, 2012; Allahyari et al., 2017). Most of the practically usable summarization systems are extractive in nature. Also, most summarization studies have focused on summarization of news articles. In this work, we mainly focus on two interesting aspects of text summarization: (1) summarization of scientific research papers, and (2) summarization for laymen. Cohan et al. (2018) propose that section level processing of scientific documents is useful. Further, Collins et al. (2017) conclude that not all sections are equally useful. Also, recent papers have observed that a hierarchical summarization of scientific documents is highly effective where at the first level, an extractive summary of each section is independently generated and at the second level, the sectional output is abstracted into a brief summary (Subramanian et al., 2019; Erera et al., 2019). (Xiao and Carenini, 2019) observe that while summarizing, local context is useful, but global is not. Thus, in our approach at the sectional level, we use extracted information from only within the section text to ob"
2020.sdp-1.39,P19-1204,0,0.0834253,"es of models is crucial for obtaining good textual representations on the target side for neural abstractive summarization. 2.2 Datasets We first describe the datasets which were provided by the organizers of the ‘Workshop on Scholarly Document Processing @EMNLP 2020’1 . 3.1 LaySumm Dataset LongSumm Dataset The corpus for this task includes a training set that consists of 1705 extractive summaries, and 531 abstractive summaries of scientific papers in the domains of Natural Language Processing and Machine Learning. The extractive summaries are based on video talks from associated conferences (Lev et al., 2019) while the abstractive summaries are blog posts created by NLP and ML researchers. The average gold summary length was 767 tokens. The research papers were parsed using the science-parse3 library. A collection of pdfs of 22 research papers served as the blind test set. The LongSumm train and test datasets are publicly accessible on LongSumm’s official GitHub repository4 . 4 Text Style Transfer Neural text style transfer is yet another related area of work where the document in style A is converted to style B without any loss of content or semantics (Syed et al., 2020; Vadapalli et al., 2018)."
2020.sdp-1.39,2020.acl-main.703,0,0.054998,"Missing"
2020.sdp-1.39,W04-1013,0,0.0646398,"om multiple domains. We differentiate between two types of summaries, namely, (a) LaySumm: A very short summary that captures the essence of the research paper in layman terms restricting overtly specific technical jargon and (b) LongSumm: A much longer detailed summary aimed at providing specific insights into various ideas touched upon in the paper. While leveraging latest Transformer-based models, our systems are simple, intuitive and based on how specific paper sections contribute to human summaries of the two types described above. Evaluations against gold standard summaries using ROUGE (Lin, 2004) metrics prove the effectiveness of our approach. On blind test corpora, our system ranks first and third for the LongSumm and LaySumm tasks respectively. 1 Introduction Popularity of data science in recent years has led to a massive growth in the number of published papers online. This has generated an epochal change in the way we retrieve, analyze and consume information from these papers. Also wider interest in data science implies even lay persons (readers outside ∗ The author also works as a researcher at Microsoft the data science community) are significantly interested in keeping up wit"
2020.sdp-1.39,D19-1387,0,0.0201614,"t (Liu, 2019). Most papers in this area focus on the summarization of news articles. But several others focus on specific domains like summarization of medical documents, legal documents, scientific documents, etc. Summarization can also be performed in a query-sensitive manner or a user-centric manner. Sentence-scoring methods include graph-based methods like LexRank (Erkan and Radev, 2004) or TextRank (Mihalcea and Tarau, 2004), machine learning or deep learning techniques and position-based methods. Recently, various deep learning architectures such as HIBERT (Zhang et al., 2019), BERTSUM (Liu and Lapata, 2019), SummaRuNNer (Nallapati et al., 2016), CSTI (Singh et al., 2018) and Hybrid MemNet (Singh et al., 2017) have been proposed for extractive summarization. 2.1.2 Abstractive Summarization In abstractive summarization, the model tries to generate the summary instead of extracting sentences or keywords. As compared to extractive summarization, this is more challenging and requires strong language modeling schemes to achieve good results. Traditionally, abstractive summarization techniques have focused on generating short text such as headlines or titles. But more recently, there have been efforts"
2020.sdp-1.39,W04-3252,0,0.279843,"ext directly from the input document. Extractive Summarization can also be seen as a text classification task where we try to predict whether a given sentence will be part of the summary or not (Liu, 2019). Most papers in this area focus on the summarization of news articles. But several others focus on specific domains like summarization of medical documents, legal documents, scientific documents, etc. Summarization can also be performed in a query-sensitive manner or a user-centric manner. Sentence-scoring methods include graph-based methods like LexRank (Erkan and Radev, 2004) or TextRank (Mihalcea and Tarau, 2004), machine learning or deep learning techniques and position-based methods. Recently, various deep learning architectures such as HIBERT (Zhang et al., 2019), BERTSUM (Liu and Lapata, 2019), SummaRuNNer (Nallapati et al., 2016), CSTI (Singh et al., 2018) and Hybrid MemNet (Singh et al., 2017) have been proposed for extractive summarization. 2.1.2 Abstractive Summarization In abstractive summarization, the model tries to generate the summary instead of extracting sentences or keywords. As compared to extractive summarization, this is more challenging and requires strong language modeling schemes"
2020.sdp-1.39,D14-1162,0,0.0939185,"ature. For comparison, we also present results for a na¨ıve “Lead-150” baseline which outputs the first 150 tokens of the abstract as the summary. As shown in Table 1, surprisingly, this simple baseline leads to impressive results especially on recall metrics. Running SummaRuNNer on the abstract leads to results which are worse than the Lead-150 baseline. 6.2 Long Summary Generation We used the SummaRuNNer (Nallapati et al., 2016) neural extractive summarization system as our base section summarizer. We pretrained this on the training set of the publicly available PubMed dataset (using GloVe (Pennington et al., 2014) 6B 100D word embeddings) to generate the paper abstract as closely as possible from any given section. This grounds the network in a setting where it can easily capture salient points. We plan to explore pretraining with other datasets as part of future work. This was further finetuned using the LongSumm train set as follows. The given LongSumm training dataset was divided into train and validation splits in a 9:1 ratio. We used the same previous settings to finetune on documents in the LongSumm train 340 split. Now, the pretrained SummaRuNNer model was conditioned to extract sentences which"
2020.sdp-1.39,2020.emnlp-main.748,0,0.07042,"Missing"
2020.sdp-1.39,D18-2028,1,0.877627,"Missing"
2020.sdp-1.39,D11-1038,0,0.0340438,"1.2 Abstractive Summarization In abstractive summarization, the model tries to generate the summary instead of extracting sentences or keywords. As compared to extractive summarization, this is more challenging and requires strong language modeling schemes to achieve good results. Traditionally, abstractive summarization techniques have focused on generating short text such as headlines or titles. But more recently, there have been efforts on generation of longer summaries. Older methods have depended on tree transduction rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend and Lapata, 2011) for effective abstractive summarization. Recently, neural summarization approaches have been found to be more effective. Effective neural representative language models are very important for text generation tasks. With the recent breakthrough of Transformer-based (Vaswani et al., 2017) architectures like BERT (Devlin et al., 2018), T5 (Raffel 337 Match Scores Mean R1 Mean R2 summary. A set of 37 research papers were pro40.0 Mean R1 Mean R2 vided as the Mean blindRLtest data. The LaySumm dataset 30.0 comprises of full-text papers with lay summaries, 20.0 in a variety of domains (epilepsy, arc"
2020.sdp-1.39,D19-1298,0,0.0139976,"ization: (1) summarization of scientific research papers, and (2) summarization for laymen. Cohan et al. (2018) propose that section level processing of scientific documents is useful. Further, Collins et al. (2017) conclude that not all sections are equally useful. Also, recent papers have observed that a hierarchical summarization of scientific documents is highly effective where at the first level, an extractive summary of each section is independently generated and at the second level, the sectional output is abstracted into a brief summary (Subramanian et al., 2019; Erera et al., 2019). (Xiao and Carenini, 2019) observe that while summarizing, local context is useful, but global is not. Thus, in our approach at the sectional level, we use extracted information from only within the section text to obtain a section’s extractive summary, ignoring the remaining text of the entire paper. For the LaySumm task, we observe that abstract is the most relevant section of a scientific paper from a layman perspective. We therefore feed the abstract to a Transformer-based model and generate an abstractive summary for the LaySumm task. For the LongSumm task, we first perform extractive summarization for each sectio"
2020.sdp-1.39,P19-1499,0,0.0197351,"l be part of the summary or not (Liu, 2019). Most papers in this area focus on the summarization of news articles. But several others focus on specific domains like summarization of medical documents, legal documents, scientific documents, etc. Summarization can also be performed in a query-sensitive manner or a user-centric manner. Sentence-scoring methods include graph-based methods like LexRank (Erkan and Radev, 2004) or TextRank (Mihalcea and Tarau, 2004), machine learning or deep learning techniques and position-based methods. Recently, various deep learning architectures such as HIBERT (Zhang et al., 2019), BERTSUM (Liu and Lapata, 2019), SummaRuNNer (Nallapati et al., 2016), CSTI (Singh et al., 2018) and Hybrid MemNet (Singh et al., 2017) have been proposed for extractive summarization. 2.1.2 Abstractive Summarization In abstractive summarization, the model tries to generate the summary instead of extracting sentences or keywords. As compared to extractive summarization, this is more challenging and requires strong language modeling schemes to achieve good results. Traditionally, abstractive summarization techniques have focused on generating short text such as headlines or titles. But more re"
2020.semeval-1.157,C16-1234,1,0.831821,"ude both the 2D structure or pixels in an image, or the 1D structure of words in a sentence, paragraph, or document. It is therefore, a great starting point to work on processing memes and other images/videos of this sort. It makes sense that the results received when comparing the accuracy of word-level and characterlevel features are similar as the data is mostly concise monolingual text(the given memes predominantly used English). Character-level features generally tend to show promising improvement when the data is bilingual/code-mixed. Systems like sub-word level compositions with LSTMs (Prabhu et al., 2016) to capture sentiment at morpheme level are effective to a certain degree, but are limited by their dependence on abundant data that is beyond which was provided for the task. Given that the amount of data is scarce, such deep learning models or use of transformers would fall short as they require an abundance of data to train accurately. Hyperparameter tuning worked best for unigram or at most bigram parameters. This may have to do with the length of the input text. Models that perform better when the given text input is short in length would perform better when analysing memes due to the inh"
2021.sdp-1.17,2020.wosp-1.12,0,0.418401,"ntific publications and can be used for multiple downstream tasks like this one. de Andrade and Gonçalves (2020) tackled the same problem of classifying the citations based on purpose and influence. Their solution relies on combining different, potentially complementary, text representations to enhance the final obtained results. A combination of TF-IDF(Term FrequencyInverse Document Frequency) (capturing statistical information), LDA (capturing topical information), and Glove word embeddings (capturing contextual information) was used for the task of classifying the context of the citation. (Kunnath et al., 2020) presented an overview of all approaches used for the previous edition of this shared task, highlighting the data distribution and the results achieved, and has been used as a baseline for our further work. 1500 1250 1000 750 500 250 0 0 1 2 Class 3 4 5 Figure 1: Class distribution for Subtask A for entire training dataset. NLU tasks. Different works (Beltagy et al., 2019; Chalkidis et al., 2020; Nguyen et al., 2020) pretrain a BERT-like model from scratch using largescale in-domain data to learn the domain-specific language patterns. Beltagy et al. (2019) trained the BERT model from scratch u"
2021.sdp-1.17,2020.emnlp-demos.2,0,0.0280657,"information), LDA (capturing topical information), and Glove word embeddings (capturing contextual information) was used for the task of classifying the context of the citation. (Kunnath et al., 2020) presented an overview of all approaches used for the previous edition of this shared task, highlighting the data distribution and the results achieved, and has been used as a baseline for our further work. 1500 1250 1000 750 500 250 0 0 1 2 Class 3 4 5 Figure 1: Class distribution for Subtask A for entire training dataset. NLU tasks. Different works (Beltagy et al., 2019; Chalkidis et al., 2020; Nguyen et al., 2020) pretrain a BERT-like model from scratch using largescale in-domain data to learn the domain-specific language patterns. Beltagy et al. (2019) trained the BERT model from scratch using scientific documents sand saw an improvement in different NLU tasks involving scientific documents. We finetune BERT, RoBERTa, and SciBERT on our training data. We use a linear layer for the classification of sentence embeddings that we got from these models. Experiment results show that SciBERT-uncased performed the best, and thus for our further experiments, we used SciBERT to get our sentence representations."
2021.sdp-1.17,N19-1361,0,0.165809,"Missing"
2021.sdp-1.17,W06-1613,0,0.643666,"f referencing (Moravcsik and Murugesan, 1975; Chubin and Moitra, 1975). However, despite these works, the literature on automating the task of classifying citations has been limited. Garzone and Mercer (2000) treated 1 Introduction the citation classification as a task of sentence categorization. They extracted a sentence that incorpoRecent years have witnessed a massive increase rated citations and then applied manually curated in the amount of scientific literature and research lexical and grammar rules to assign categories to data being published online, providing revelation the citations. Teufel et al. (2006) used supervised about the advancements in different domains. The machine learning techniques to classify citations introduction of aggregator services like CORE has into 12 categories. The authors annotated 2,829 enabled unprecedented levels of open access to citation contexts from 116 articles, using linguistic scholarly publications. The availability of the full text of the research documents facilitates the pos- features, including the cue phrases. Agarwal et al. sibility of extending the bibliometric studies by (2010) used algorithms like Linear Discriminant Analysis(LDA) and Support Vect"
2021.semeval-1.173,P19-1394,0,0.0266794,") have been used for this task (Zhang and Luo, 2018). Recurrent neural networks combined with user-related information have also been used for hate speech detection in Twitter Data (Pitsilis et al., 2018) whereas multilingual transformer architectures were leveraged by (Ghosh Roy et al., 2021) to detect hostile content in English, Hindi and German. 3 Task and dataset overview The task(Meaney et al., 2021) is divided into 4 sub-tasks. Related work There have been many attempts made at computational humour detection. In this section, we briefly describe other work in this area. In this approach(Blinov et al., 2019), the authors have used universal language model fine-tuning method 1. Humour detection: This is a binary classification task where the model needs to predict if the text is humorous or not where the values are either 0 and 1. 2. Humour Rating: This is a regression task where the model needs to rate how humorous the text is where the value can vary between 0 to 5. 3. Controversy detection: This is a binary classification task where the model needs to classify text as controversial or not if it has been classified as humorous. It can be either 0 or 1. 1221 Proceedings of the 15th International"
2021.semeval-1.173,2020.acl-main.740,0,0.0553973,"Missing"
2021.semeval-1.173,2020.semeval-1.98,0,0.0279923,"We achieved an F1 score of 0.959 in the humor classification task and 0.592 in the humor controversy task. For the regression tasks, we achieved a RMSE score of 0.541 and 0.488 in the humor regression and offense regression task respectively. 2 for humour recognition. Convolutional neural networks (CNN) have also been used for this task by (Chen and Soo, 2018) whereas (Weller and Seppi, 2019) used transformers to classify humour. There has also been a lot of shared tasks and workshops related to computational humour. One of them is SemEval-2020 Task 7: Assessing Humor in Edited News Headlines(Hossain et al., 2020) where Zhang(Zhang et al., 2020) used bidirectional neural networks with an attention mechanism and incorporated lexical features to assess humour in edited news headlines. There has been a lot of work done on hate speech and offensive speech detection as well. CNN’s and gated recurrent units (GRU) have been used for this task (Zhang and Luo, 2018). Recurrent neural networks combined with user-related information have also been used for hate speech detection in Twitter Data (Pitsilis et al., 2018) whereas multilingual transformer architectures were leveraged by (Ghosh Roy et al., 2021) to dete"
2021.semeval-1.173,2020.findings-emnlp.445,0,0.0573444,"Missing"
2021.semeval-1.173,2021.ccl-1.108,0,0.0871954,"Missing"
2021.semeval-1.173,2021.semeval-1.9,0,0.0352315,"mechanism and incorporated lexical features to assess humour in edited news headlines. There has been a lot of work done on hate speech and offensive speech detection as well. CNN’s and gated recurrent units (GRU) have been used for this task (Zhang and Luo, 2018). Recurrent neural networks combined with user-related information have also been used for hate speech detection in Twitter Data (Pitsilis et al., 2018) whereas multilingual transformer architectures were leveraged by (Ghosh Roy et al., 2021) to detect hostile content in English, Hindi and German. 3 Task and dataset overview The task(Meaney et al., 2021) is divided into 4 sub-tasks. Related work There have been many attempts made at computational humour detection. In this section, we briefly describe other work in this area. In this approach(Blinov et al., 2019), the authors have used universal language model fine-tuning method 1. Humour detection: This is a binary classification task where the model needs to predict if the text is humorous or not where the values are either 0 and 1. 2. Humour Rating: This is a regression task where the model needs to rate how humorous the text is where the value can vary between 0 to 5. 3. Controversy detect"
2021.semeval-1.173,P17-1161,0,0.0310636,"and regression task. 4.2 Lexical features The structure of humorous and offensive texts can be a bit different from normal texts. We have leveraged a lexical feature set that would help us capture that information and distinguish humorous and offensive texts. The set of lexical features are: • Counting the total number of letters, punctuation, upper case letters and numbers within the text. • Identifying the presence of any named entity. For detecting named entities, we have used the AllenNLP named entity recogniser1 which uses pretrained GloVe vectors for token embeddings and a GRU encoder.(Peters et al., 2017) • Detecting the presence of interrogation by identifying ’?’ symbol or any WH-word • Detecting the number of personal pronouns and what kind of personal pronouns they are: first-person, second-person or third-person. 1 https://demo.allennlp.org/ named-entity-recognition/ named-entity-recognition For detecting the personal pronouns, we have used a pre-defined list of personal pronouns. 4.3 Sentence embeddings For generating the sentence embeddings, we have experimented with 4 different pre-trained transformer models: bert-base-uncased(Devlin et al., 2018), roberta-base(Liu et al., 2019), googl"
2021.semeval-1.173,D19-1372,0,0.0242777,"offensive or controversial humour being posted online. (Weitz, 2017) In this task, we have presented a transformer based approach combined with lexical and hurtlex feature sets to quantify humour and offense of a piece of text. We achieved an F1 score of 0.959 in the humor classification task and 0.592 in the humor controversy task. For the regression tasks, we achieved a RMSE score of 0.541 and 0.488 in the humor regression and offense regression task respectively. 2 for humour recognition. Convolutional neural networks (CNN) have also been used for this task by (Chen and Soo, 2018) whereas (Weller and Seppi, 2019) used transformers to classify humour. There has also been a lot of shared tasks and workshops related to computational humour. One of them is SemEval-2020 Task 7: Assessing Humor in Edited News Headlines(Hossain et al., 2020) where Zhang(Zhang et al., 2020) used bidirectional neural networks with an attention mechanism and incorporated lexical features to assess humour in edited news headlines. There has been a lot of work done on hate speech and offensive speech detection as well. CNN’s and gated recurrent units (GRU) have been used for this task (Zhang and Luo, 2018). Recurrent neural netwo"
2021.semeval-1.173,2020.semeval-1.129,0,0.0583144,"Missing"
bakliwal-etal-2012-hindi,banea-etal-2008-bootstrapping,0,\N,Missing
bakliwal-etal-2012-hindi,kamps-etal-2004-using,0,\N,Missing
bakliwal-etal-2012-hindi,N06-1026,0,\N,Missing
bakliwal-etal-2012-hindi,H05-1044,0,\N,Missing
bakliwal-etal-2012-hindi,E09-1004,0,\N,Missing
bakliwal-etal-2012-hindi,C04-1200,0,\N,Missing
bakliwal-etal-2012-hindi,W03-2102,0,\N,Missing
C16-1234,bakliwal-etal-2012-hindi,1,0.717687,"ion (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan,"
C16-1234,W14-3902,0,0.259828,"nglish speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Reco"
C16-1234,P07-1056,0,0.0156888,"tive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan, December 11-17 2016. Sentence variations Trailer dhannnsu hai bhai Dhannnsu trailer hai bhai Bhai trailer dhannnsu hai Bhai dhannnsu"
C16-1234,W14-3908,0,0.026059,"g interest owing to the rising amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tas"
C16-1234,W10-3208,0,0.0476344,"al. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan, December 11-17 2016. Sentence variations Trailer dhannnsu hai bhai Dhannnsu trailer hai bhai Bhai trailer dhannnsu hai Bhai dhannnsu trailer hai Table 1: Illustra"
C16-1234,W15-3904,0,0.0635959,"Missing"
C16-1234,esuli-sebastiani-2006-sentiwordnet,0,0.0460907,"ng data into 80-20 split to get the final training, validation and testing data. As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence. The architecture of the proposed system (Subword-LSTM) is described in Figure 2. We compare it with a character-level LSTM (Char-LSTM) following the same architecture without the convolutional and maxpooling layers. We use Adamax (Kingma and Ba, 2014) (a variant of Adam based on infinity norm) optimizer to train this setup in an end-to-end fashion using batch size of 128. We use very simplistic architectures because of the constraint on the size of"
C16-1234,N13-1090,0,0.0191484,"esome, brother. Sentiment Polarity Positive Negative Positive Table 4: Examples of Hi-En Code Mixed Comments from the dataset. Our dataset and code is freely available for download 2 to encourage further exploration in this domain. 2 https://github.com/DrImpossible/Sub-word-LSTM 2484 3 Learning Compositionality Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations. 3.1 Word-level models Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to"
C16-1234,D13-1170,0,0.0727234,"kers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar˜aes, 2015). LSTMs have been observed to outperform baselines for language modelling (Kim et al., 2015) and classification (Zhou et al., 2015). In a recent work, (Bojanowski et al., 2016) proposed a skip-gram based"
C16-1234,W14-3907,0,0.0805565,"ted final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar˜aes, 2015). LSTMs have been observed to outperform baselines"
C16-1234,J11-2001,0,0.0210151,"t − C + B = Bat” lacks any linguistic basis. But, groups of characters may serve semantic functions. This is illustrated by U n + Holy = U nholy or Cat + s = Cats which is semantically interpretable by a human. Since sub-word level representations can generate meaningful lexical representations and individually carry semantic weight, we believe that sub-word level representations consisting composition of characters might allow generation of new lexical structures and serve as better linguistic units than characters. 3.3 Sub-word level representations Lexicon based approaches for the SA task (Taboada et al., 2011; Sharma et al., 2015) perform a dictionary look up to obtain an individual score for words in a given sentence and combine these scores to get the sentiment polarity of a sentence. We however want to use intermediate sub-word feature representations learned by the filters during convolution operation. Unlike traditional approaches that add sentiment scores of individual words, we propagate relevant information with LSTM and compute final sentiment of the sentence as illustrated in Figure 1. Hypothesis: We propose that incorporating sub-word level representations into the design of our models"
C16-1234,W13-3512,0,0.0154115,"Table 4: Examples of Hi-En Code Mixed Comments from the dataset. Our dataset and code is freely available for download 2 to encourage further exploration in this domain. 2 https://github.com/DrImpossible/Sub-word-LSTM 2484 3 Learning Compositionality Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations. 3.1 Word-level models Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to be independent entities. However, operating on the lexical"
C16-1234,D14-1105,0,0.0461119,"ing amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging"
C16-1234,P12-2018,0,0.0555154,"ches: Hashtags, User Mentions, Emoticons etc. may not exist in the data. 2486 Figure 1: Illustration of the proposed methodology Figure 3: Training accuracy and loss variation. Figure 2: Schematic overview of the architecture. 4.2 Experimental Setup Our dataset is divided into 3 splits- Training, validation and testing. We first divide the data into randomized 80-20 train test split, then further randomly divide the training data into 80-20 split to get the final training, validation and testing data. As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence. The architecture of th"
D18-2028,W16-6010,0,0.125881,"apply the architectures directly to our problem where the domains and vocabulary are very different. While directly using seq2seq architectures was somewhat helpful in our case - as we will show later; cross domain headline generation requires the consideration of aspects such as style, readability, etc in the two different domains of study. 164 Figure 2: Model of pipeline architecture, describing the two stages in which we model blog title generation Existing literature in non-parallel style transfer assumes the unavailability of sufficient parallel data (Shen et al., 2017; Fu et al., 2018; Kabbara and Cheung, 2016). In first trying to address the problem of style transfer on non-parallel data, Shen et al. (2017) tried to separate the content from the style of the article. It was assumed that a shared latent content distribution exists across different text corpora, and proposed a method that leveraged refined alignment of latent representations to perform style transfer. While Shen et al. (2017) demonstrated their results on sentiment transfer, this cannot be accepted as style transfer from a linguistic point of view. tween the source and the target is not significant which is one of the prime reasons w"
D18-2028,P02-1040,0,0.117005,"same heuristic function or neural generation model might not exhibit the best results for all possible inputs. Thus, it is of exceptional importance to provide the users fine grained control over the individual components of the model. The system does this by allowing the user the freedom to select a heuristic function and neural generation model of their choice. This allows for more flexibility for the users to experiment with various heuristic functions and neural generation models and ensures better results than forcing the user to use one particular configuration for all inputs. 1. BLEU (Papineni et al., 2002): It uses a modified precision to compare generated text against multiple reference texts 2. ROUGE L (Lin, 2004): It is an F-measure that is based on the Longest Common Subsequence (LCS) between the candidate and reference utterances 3. CIDEr (Vedantam et al., 2015): It is based on n-gram overlap 4. Skip Thought Cosine Similarity (Kiros et al., 2015): It is based on a continuous representation of sentences known as skipthought vectors 167 5. Flesch Reading Ease (Flesch, 1948): It measures the readability of the sentence based on the number of syllables and words Table 2 shows the performance o"
D18-2028,P17-1099,0,0.446781,"on Empirical Methods in Natural Language Processing (System Demonstrations), pages 163–168 c Brussels, Belgium, October 31–November 4, 2018. 2018 Association for Computational Linguistics Figure 1: Layout of the web application for our prototype, demonstrating blog title generation generating an entire blog from a given research paper - thereby paving the way for future research in the area. paper and then it it uses the extracted information to generate the blog title. The state-of-theart sequence-to-sequence neural networks for natural language generation like the Pointer Generator Network (See et al., 2017) are used in the second stage of the pipeline. The generated blog titles are evaluated using all standard metrics for natural language generation tasks and the results indicate the viability of the proposed model to produce semantically sound blog titles. Our contributions can be summed up as follows: 1. A new parallel corpus of 87, 328 pairs of research paper titles and abstracts and their corresponding blog titles. 2. Demonstrating the web application, which uses a pipeline-based architecture that can generate blog titles in a step-by-step fashion, while enabling the user to choose between v"
D19-1174,J08-4004,0,0.0274948,"ing distinct accounts of sexism from an entry obtained from Everyday Sexism Project and subsequently tagging each portion with at least one of the 23 categories of sexism, producing over 23000 labeled accounts. In phases 2 and 3, we sought redundancy of annotations for improved quality, as permitted by the availability of annotators adequately knowledgeable about sexism. Over 21000 accounts were categorized again in phase 2 such that the annotators for phases 1 and 2 were different. The inter-annotator agreement across phases 1 and 2, measured by the average of the Cohen’s Kappa (Cohen, 1960; Artstein and Poesio, 2008) scores for the per-category pairs of binary label vectors, is 0.584. Each account for which the label sets annotated across phases 1 and 2 were identical was included in the dataset along with the associated label set. In phase 3, some of the accounts for which there was a mismatch between the phase 1 and phase 2 annotations were selected. For each account, the annotators were presented with only the mismatched categories and asked to select or reject each. Duplicates and records for which the Everyday Sexism Project entry numbers match but the accounts do not fully match were removed at mult"
D19-1174,Q17-1010,0,0.0107708,"assed through bi-LSTM followed by attention-based aggregation, producing q representations for a post collectively. These vectors are then concatenated to produce the overall post representation. The final step involves a fully connected layer with a sigmoid or softmax non-linearity depending on the loss function used, generating the output probabilities. 4.2 Word and Sentence Representations We model a post using both word embeddings and sentence embeddings. We experiment with three distributional word vectors, namely ELMo (Peters et al., 2018), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), and a linguistic feature vector. Our linguistic feature representation comprises a variety of features, namely features from the biased language detection work (assertive verb, implicative verb, hedges, factive verb, report verb, entailment, strong subjective, weak subjective, positive word, and negative word) (Recasens et al., 2013), PERMA (Positive Emotion, Engagement, Relationships, Meaning, and Accomplishments) features for both polarities (Schwartz et al., 2016), associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentim"
D19-1174,D18-2029,0,0.0207455,"t, Relationships, Meaning, and Accomplishments) features for both polarities (Schwartz et al., 2016), associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive) from the NRC emotion lexicon (Mohammad and Turney, 2013), and affect (valence, arousal, and dominance) scores (Mohammad, 2018). Missing values are filled with zero for binary features and with the mean for non-binary ones. We explore the following for creating sentence embeddings: BERT (Devlin et al., 2018), Universal Sentence Encoder (USE) (Cer et al., 2018), and 1646 InferSent (Conneau et al., 2017). Our choice of utilizing these models is warranted by the fact that the corpora that they are trained on are considerably bigger than the textual data that we have for supervised learning and hence likely contain greater semantic diversity. 4.3 Utilizing Unlabeled Data Models such as BERT are not trained to generate representations tuned to a specific domain. We use over 90000 entries crawled from Everyday Sexism Project’s website to tailor a pre-trained BERT model for obtaining more effective representations for our model. After removing the unlabel"
D19-1174,D17-1070,0,0.0241869,"shments) features for both polarities (Schwartz et al., 2016), associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive) from the NRC emotion lexicon (Mohammad and Turney, 2013), and affect (valence, arousal, and dominance) scores (Mohammad, 2018). Missing values are filled with zero for binary features and with the mean for non-binary ones. We explore the following for creating sentence embeddings: BERT (Devlin et al., 2018), Universal Sentence Encoder (USE) (Cer et al., 2018), and 1646 InferSent (Conneau et al., 2017). Our choice of utilizing these models is warranted by the fact that the corpora that they are trained on are considerably bigger than the textual data that we have for supervised learning and hence likely contain greater semantic diversity. 4.3 Utilizing Unlabeled Data Models such as BERT are not trained to generate representations tuned to a specific domain. We use over 90000 entries crawled from Everyday Sexism Project’s website to tailor a pre-trained BERT model for obtaining more effective representations for our model. After removing the unlabeled entries corresponding to the posts in th"
D19-1174,I17-1078,0,0.0143725,"ng any type(s) of sexism without the assumption of the mutual exclusivity of classes. • We provide a dataset consisting of 13023 accounts of sexism by survivors and observers annotated with one or more of 23 carefully formulated categories of sexism. 2 Related Work Substantial work has been directed to hate speech detection in recent years. Since some of it involves the detection of sexism (Badjatiya et al., 2017; Waseem and Hovy, 2016), we review it along with the work on sexism classification. 2.1 Hate Speech Detection Warner and Hirschberg (2012) identify antisemitic hate speech using SVM. Gao et al. (2017) perform hate speech detection in a weakly supervised fashion. Nobata et al. (2016) distinguish abusive comments from clean ones through various NLP and embedding-derived features. Burnap and Williams (2016) classify cyber hate with respect to race, disability, and sexual orientation using text parsing to extract typed dependencies. Waseem and Hovy (2016) explore the role of extra-linguistic features along with character ngrams in classifying tweets as racist, sexist or neither. Badjatiya et al. (2017) experiment with various deep learning approaches for the same threeway classification. Zhang"
D19-1174,W18-5114,0,0.23467,"Missing"
D19-1174,W17-2902,0,0.403354,"gainst ∗ this injustice. Given the rising volume of such information on digital media, automated sexism categorization can aid social scientists and policy makers in combating sexism by conducting such analyses efficiently. While sexism is detected as a category of hate in some of the hate speech classification work (Badjatiya et al., 2017; Waseem and Hovy, 2016), it does not perform sexism classification. Except the work on categorizing sexual harassment by Karlekar and Bansal (2018), the prior work on classifying sexism assumes the categories to be mutually exclusive (Anzovino et al., 2018; Jha and Mamidi, 2017). Moreover, the existing category sets number between 2 to 5. In this paper, we focus on the new problem of the multi-label categorization of an account of sexism reporting any type(s) of sexism. We create a dataset comprising 13023 accounts of sexism, including first-person accounts from survivors, each tagged with at least one of 23 categories of sexism. The categories were defined keeping in mind the discourse and campaigns on gender-related issues along with potential policy implications, under the guidance of a social scientist. Ten annotators, most of whom have formally studied topics re"
D19-1174,D18-1303,0,0.117557,"sexism can play a part in analyzing sexism with a view to developing sensitization programs, systemic safeguards, and other mechanisms against ∗ this injustice. Given the rising volume of such information on digital media, automated sexism categorization can aid social scientists and policy makers in combating sexism by conducting such analyses efficiently. While sexism is detected as a category of hate in some of the hate speech classification work (Badjatiya et al., 2017; Waseem and Hovy, 2016), it does not perform sexism classification. Except the work on categorizing sexual harassment by Karlekar and Bansal (2018), the prior work on classifying sexism assumes the categories to be mutually exclusive (Anzovino et al., 2018; Jha and Mamidi, 2017). Moreover, the existing category sets number between 2 to 5. In this paper, we focus on the new problem of the multi-label categorization of an account of sexism reporting any type(s) of sexism. We create a dataset comprising 13023 accounts of sexism, including first-person accounts from survivors, each tagged with at least one of 23 categories of sexism. The categories were defined keeping in mind the discourse and campaigns on gender-related issues along with p"
D19-1174,D14-1181,0,0.0261579,"t word ems beddings, and (b) g tensors ∈ R|S|×dj constructed using different sentence encoders. First, subsets of the f tensors based on word embeddings are concatenated in a configurable manner (configurable word-level concat in Fig. 2), w producing p tensors ∈ R|S|×|W |×Di , where Diw is the dimension resulting from the ith concatenation. Next, we construct vector representations for the sentences word-embedded in each of the p tensors using CNN-based and/or LSTM-based operations as configured. The CNN-based operations begin with convolutional filters being applied along the word dimension (Kim, 2014) to generate many bigram, trigram and 4-gram based features. This is followed by max-over-time pooling, which picks the largest value for each filter and produces a sentence representing tensor ∈ R|S|×c , where c is the total number of convolutional filters used. The LSTM-based components include biLSTM followed by an attention mechanism (Yang et al., 2016) through which the LSTM outputs across time steps are aggregated into a vector representation for each sentence, resulting in a tensor ∈ R|S|×h , where h is the bi-LSTM output length. At this stage, we have three types of sentence representi"
D19-1174,P18-1017,0,0.0155353,"nguage detection work (assertive verb, implicative verb, hedges, factive verb, report verb, entailment, strong subjective, weak subjective, positive word, and negative word) (Recasens et al., 2013), PERMA (Positive Emotion, Engagement, Relationships, Meaning, and Accomplishments) features for both polarities (Schwartz et al., 2016), associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive) from the NRC emotion lexicon (Mohammad and Turney, 2013), and affect (valence, arousal, and dominance) scores (Mohammad, 2018). Missing values are filled with zero for binary features and with the mean for non-binary ones. We explore the following for creating sentence embeddings: BERT (Devlin et al., 2018), Universal Sentence Encoder (USE) (Cer et al., 2018), and 1646 InferSent (Conneau et al., 2017). Our choice of utilizing these models is warranted by the fact that the corpora that they are trained on are considerably bigger than the textual data that we have for supervised learning and hence likely contain greater semantic diversity. 4.3 Utilizing Unlabeled Data Models such as BERT are not trained to generate rep"
D19-1174,D14-1162,0,0.0823778,"I am pleasing to the eye! Disgusting.” is an experience of sexism wherein the victim was subjected to three types of sexism, namely hyper-sexualization, sexual harassment, and hostile work environment. We develop a novel neural architecture for the multi-label classification of accounts of sexism that enables flexibly combining sentence representations created using models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) with distributional word embeddings like ELMo (Embeddings from Language Models) (Peters et al., 2018) and Global Vectors (GloVe) (Pennington et al., 2014) and a linguistic feature representation through hierarchical convolutional and/or recurrent operations. Leveraging general-purpose models such as BERT for encoding sentences likely makes our model better equipped to capture semantic aspects effectively, since they are trained on substantially larger textual data than the domain-specific labeled data that we have. Moreover, we adapt a BERT model for the domain of instances of sexism using unlabeled data. Embeddings from sentence encoders are complemented by sentence representations built from word embeddings as a function of trainable neural n"
D19-1174,N18-1202,0,0.324495,"should be in more team events and photos because I am pleasing to the eye! Disgusting.” is an experience of sexism wherein the victim was subjected to three types of sexism, namely hyper-sexualization, sexual harassment, and hostile work environment. We develop a novel neural architecture for the multi-label classification of accounts of sexism that enables flexibly combining sentence representations created using models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) with distributional word embeddings like ELMo (Embeddings from Language Models) (Peters et al., 2018) and Global Vectors (GloVe) (Pennington et al., 2014) and a linguistic feature representation through hierarchical convolutional and/or recurrent operations. Leveraging general-purpose models such as BERT for encoding sentences likely makes our model better equipped to capture semantic aspects effectively, since they are trained on substantially larger textual data than the domain-specific labeled data that we have. Moreover, we adapt a BERT model for the domain of instances of sexism using unlabeled data. Embeddings from sentence encoders are complemented by sentence representations built fro"
D19-1174,P13-1162,0,0.0197984,"put probabilities. 4.2 Word and Sentence Representations We model a post using both word embeddings and sentence embeddings. We experiment with three distributional word vectors, namely ELMo (Peters et al., 2018), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), and a linguistic feature vector. Our linguistic feature representation comprises a variety of features, namely features from the biased language detection work (assertive verb, implicative verb, hedges, factive verb, report verb, entailment, strong subjective, weak subjective, positive word, and negative word) (Recasens et al., 2013), PERMA (Positive Emotion, Engagement, Relationships, Meaning, and Accomplishments) features for both polarities (Schwartz et al., 2016), associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive) from the NRC emotion lexicon (Mohammad and Turney, 2013), and affect (valence, arousal, and dominance) scores (Mohammad, 2018). Missing values are filled with zero for binary features and with the mean for non-binary ones. We explore the following for creating sentence embeddings: BERT (Devlin et al., 2018)"
D19-1174,P16-2037,0,0.0480231,"the C-LSTM architecture (Zhou et al., 2015) somewhat related to an approach used by Karlekar and Bansal (2018) for multi-label sexual harassment classification, after applying convolution on the word vectors for a post, the feature maps are stacked along the filter dimension to create a sequence of window vectors, which are then fed to biLSTM. CNN-biLSTM-Attention: For each sentence, convolutional and max-over-time pooling layers are applied on the embeddings of its words. The resultant sentence representations are put through bi-LSTM with the attention mechanism. This approach is similar to (Wang et al., 2016) with the attention scheme from (Yang et al., 2016) added. The architectures of the deep learning baselines have a fully connected layer with the sigmoid or softmax non-linearity (depending on the loss function used) at the end. 5.3 Results Table 2 shows results produced using traditional ML methods (SVM, RF, and LR) across four different feature sets (word n-grams, character ngrams, averaged ELMo vectors, and composite features). We use Label Powerset for these methods, since the direct (non-transformative) formulation cannot be used with them. Among these combinations, logistic regression wi"
D19-1174,W12-2103,0,0.0535749,"our knowledge, this is the first work on classifying an account recounting any type(s) of sexism without the assumption of the mutual exclusivity of classes. • We provide a dataset consisting of 13023 accounts of sexism by survivors and observers annotated with one or more of 23 carefully formulated categories of sexism. 2 Related Work Substantial work has been directed to hate speech detection in recent years. Since some of it involves the detection of sexism (Badjatiya et al., 2017; Waseem and Hovy, 2016), we review it along with the work on sexism classification. 2.1 Hate Speech Detection Warner and Hirschberg (2012) identify antisemitic hate speech using SVM. Gao et al. (2017) perform hate speech detection in a weakly supervised fashion. Nobata et al. (2016) distinguish abusive comments from clean ones through various NLP and embedding-derived features. Burnap and Williams (2016) classify cyber hate with respect to race, disability, and sexual orientation using text parsing to extract typed dependencies. Waseem and Hovy (2016) explore the role of extra-linguistic features along with character ngrams in classifying tweets as racist, sexist or neither. Badjatiya et al. (2017) experiment with various deep l"
D19-1174,N16-2013,0,0.522569,"ly and widely by facilitating anonymity and connecting far-away people. A meaningful categorization of these accounts of sexism can play a part in analyzing sexism with a view to developing sensitization programs, systemic safeguards, and other mechanisms against ∗ this injustice. Given the rising volume of such information on digital media, automated sexism categorization can aid social scientists and policy makers in combating sexism by conducting such analyses efficiently. While sexism is detected as a category of hate in some of the hate speech classification work (Badjatiya et al., 2017; Waseem and Hovy, 2016), it does not perform sexism classification. Except the work on categorizing sexual harassment by Karlekar and Bansal (2018), the prior work on classifying sexism assumes the categories to be mutually exclusive (Anzovino et al., 2018; Jha and Mamidi, 2017). Moreover, the existing category sets number between 2 to 5. In this paper, we focus on the new problem of the multi-label categorization of an account of sexism reporting any type(s) of sexism. We create a dataset comprising 13023 accounts of sexism, including first-person accounts from survivors, each tagged with at least one of 23 categor"
D19-1174,N16-1174,0,0.358862,"construct vector representations for the sentences word-embedded in each of the p tensors using CNN-based and/or LSTM-based operations as configured. The CNN-based operations begin with convolutional filters being applied along the word dimension (Kim, 2014) to generate many bigram, trigram and 4-gram based features. This is followed by max-over-time pooling, which picks the largest value for each filter and produces a sentence representing tensor ∈ R|S|×c , where c is the total number of convolutional filters used. The LSTM-based components include biLSTM followed by an attention mechanism (Yang et al., 2016) through which the LSTM outputs across time steps are aggregated into a vector representation for each sentence, resulting in a tensor ∈ R|S|×h , where h is the bi-LSTM output length. At this stage, we have three types of sentence representing tensors if both CNN-based and RNN-based operations are chosen to be applied on all word embedding tensors: (a) p tensors ∈ R|S|×c from the CNN-based processing, (b) p tensors ∈ R|S|×h from the LSTM-based processs ing, and (c) g tensors ∈ R|S|×dj obtained using general-purpose sentence encoders. From these sentence representing tensors, subsets are concat"
dubey-etal-2014-enrichment,C04-1151,0,\N,Missing
dubey-etal-2014-enrichment,P99-1067,0,\N,Missing
dubey-etal-2014-enrichment,J05-4003,0,\N,Missing
dubey-etal-2014-enrichment,D10-1025,0,\N,Missing
dubey-etal-2014-enrichment,P06-1011,0,\N,Missing
dubey-etal-2014-enrichment,I13-1163,0,\N,Missing
dubey-etal-2014-enrichment,barker-gaizauskas-2012-assessing,0,\N,Missing
dubey-etal-2014-enrichment,W11-3501,0,\N,Missing
I08-1068,J93-2003,0,0.0262577,"layed by search engines can better capture the context of the query. In deed we experimented with different context sizes for Drel . The first is using the whole document i.e., considering the query and concatenation of all the relevant documents as a pair in the parallel texts extracted which is called Ddocuments The second is using just a short text snippet from the document in the context of query instead of the whole document which is called Dsnippets Details are described in the experiments section. 5.1.2 Learning Translation Model According to the standard statistical translation model (Brown et al., 1993), we can find the optimal model M ∗ by maximizing the probability of generating queries from documents or 524 M ∗ = arg max M N Y i=1 P (Qi |Di , M ) qw journal journal journal journal journal journal journal music music music music music music dw kdd conference journal sigkdd discovery mining acm music purchase mp3 listen mp3.com free P(qw|dw,u) 0.0176 0.0123 0.0176 0.0088 0.0211 0.0017 0.0088 0.0375 0.0090 0.0090 0.0180 0.0450 0.0008 user. It represents the probability of generation of the query word qi for a word w in the document. P (w|D) is the probability of the word w in the document an"
I08-1068,J03-1002,0,0.00828552,"not include of language translation including the way word or- documents of type doc and pdf files. To evaluate our approach, we use the 10-fold der tends to differ across languages. Similar to earcross-validation strategy (Mitchell, 1997). We dilier work (Berger and Lafferty, 1999), we use IBM Model1 because we believe it is more suited for IR vide the data of each user into 10 sets each havbecause the subtler aspects of language used for ma- ing (approximately) equal number of search queries chine translation can be ignored for IR. GIZA++ (For example, for user1 had 37 queries in total, we (Och and Ney, 2003), an open source tool which im- divided this into 10 sets with 4 queries each approxplements the IBM Models which we have used in imately). Learning of user profile is done 10 times, our work for computing the translation probabilities. each time leaving out one of the sets from training, but using only the omitted subset for testing. PerA sample user profile learned is shown in Table 1. formance is computed in the testing phase for each time and average of the 10 times is taken. In the 5.2 Re-ranking Re-ranking is a phase in personalized search where testing phase, we take each query and re r"
I08-5010,W03-2201,0,0.0357488,"Missing"
I08-5010,A97-1029,0,0.388408,"ble state sequences, T Zo = ∑ ∗exp( ∑ ∑ λk fk (st−1, st , o,t)) s∈ST t=1 k and that the number of state sequences is exponential in the input sequence length, T. In arbitrarilystructured CRFs, calculating the partition function in closed form is intractable, and approximation methods such as Gibbs sampling, or loopy belief propagation must be used. 69 5 Features There are many types of features used in NER systems. Many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word. (Mikheev, 1997; Wacholder et al., 1997; Bikel et al., 1997). Following are examples of commonly used binary features: All-Caps (IBM), internal capitalization (eBay), initial capital (Abdul Kalam), uncapitalized word (can), 2-digit number (83, 73), 4-digit number (1983, 2007), all digits (8, 28, 1273) etc. The features that correspond to the capitalization are not applicable to Indian languages. Also, we have not used any of the binary features in any of our models. Dictionaries: Dictionaries are used to check if a part of the named entity is present in the dictionary. These dictionaries are called as gazetteers. The problem with the Indian languages i"
I08-5010,J92-1002,0,0.167306,"erformed well in almost all the runs. The rate of learning is more in case of 30K. From Table 8, in all the runs, the bi-gram character model constantly performed the best. Also interestingly the model is able to achieve a least F-measure of 44.75 with just 10K words of training data. But, in case of Telugu,(Table 7) an Fmeasure of 44+ was reached with training data of size 35K i.e the learning rate for english is more for less amount of data. This is due to the reason that Telugu (Entropy=15.625 bits per character) (Bharati et al., 1998) is comparitively a high entropy language than English (Brown and Pietra, 1992). However for Hindi, the relative jump in the performance (compared to Telugu and English)is less. Even the entropy of Hindi (Entorpy=11.088) (Bharati et al., 1998) is more than English. This is also observed from the table (Table 10). The numbers in the second, third and fourth columns are the number of features for English,Telugu and Hindi respectively. words n=2 n=3 n=4 n=5 n=6 English 29145 27707 45580 64284 65248 57297 Telugu 320260 267340 680720 1162320 1359980 1278790 Hindi 685032 647109 1403352 1830438 1735614 1433322 Table 10: Number of features calculated in the word based model for"
I08-5010,W99-0612,0,0.650187,"deva Varma vv@iiit.ac.in Language Technologies Research Centre International Institute of Information Technology Hyderabad, India swering systems (Toral et al., 2005; Molla et al., 2006), and machine translation (Babych and Hartley, 2003). NER is an essential subtask in organizing and retrieving biomedical information (Tsai, 2006). NER can be treated as a two step process Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper nouns in a document as person names, organization names, location names, date & time expressions and miscellaneous. Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. Character n-gram based approach (Klein et al., 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. Applying the same technique on Indian Languages, we experimented with Conditional Random Fields (CRFs), a discriminative model, and evaluated our system on two Indian Languages Telugu and Hindi. The character n-gram based models showed considerable improvement over the word based models. This paper describes the features used and experiment"
I08-5010,W03-0428,0,0.271016,"ral et al., 2005; Molla et al., 2006), and machine translation (Babych and Hartley, 2003). NER is an essential subtask in organizing and retrieving biomedical information (Tsai, 2006). NER can be treated as a two step process Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper nouns in a document as person names, organization names, location names, date & time expressions and miscellaneous. Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. Character n-gram based approach (Klein et al., 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. Applying the same technique on Indian Languages, we experimented with Conditional Random Fields (CRFs), a discriminative model, and evaluated our system on two Indian Languages Telugu and Hindi. The character n-gram based models showed considerable improvement over the word based models. This paper describes the features used and experiments to increase the recall of Named Entity Recognition Systems which is also language independent. • identification of proper nouns. • classif"
I08-5010,J97-3003,0,0.0206642,") is the sum of the scores of all possible state sequences, T Zo = ∑ ∗exp( ∑ ∑ λk fk (st−1, st , o,t)) s∈ST t=1 k and that the number of state sequences is exponential in the input sequence length, T. In arbitrarilystructured CRFs, calculating the partition function in closed form is intractable, and approximation methods such as Gibbs sampling, or loopy belief propagation must be used. 69 5 Features There are many types of features used in NER systems. Many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word. (Mikheev, 1997; Wacholder et al., 1997; Bikel et al., 1997). Following are examples of commonly used binary features: All-Caps (IBM), internal capitalization (eBay), initial capital (Abdul Kalam), uncapitalized word (can), 2-digit number (83, 73), 4-digit number (1983, 2007), all digits (8, 28, 1273) etc. The features that correspond to the capitalization are not applicable to Indian languages. Also, we have not used any of the binary features in any of our models. Dictionaries: Dictionaries are used to check if a part of the named entity is present in the dictionary. These dictionaries are called as gazett"
I08-5010,U06-1009,0,0.0605179,"Missing"
I08-5010,W95-0107,0,0.0125276,"Named entity recognition (NER) can be modelled as a sequence labelling task (Lafferty et al., 2001). Given an input sequence of words W1n = w1 w2 w3 ...wn , the NER task is to construct a label sequence Ln1 = l1 l2 l3 ...ln , where label li either belongs to 68 the set of predefined classes for named entities or is none (representing words which are not proper nouns). The general label sequence l1n has the highest probability of occuring for the word sequence W1n among all possible label sequences, that is Lˆ n1 = argmax {Pr (Ln1 |W1n ) } 3.2 Tagging Scheme We followed the IOB tagging scheme (Ramshaw and Marcus, 1995) for all the three languages (English, Hindi and Telugu). In this scheme each line contains a word at the beginning followed by its tag. The tag encodes the type of named entity and whether the word is in the beginning or inside the NE. Empty lines represent sentence (document) boundaries. An example of the IOB tagging scheme is given in Table 1. Words tagged with O are outside of named entities Token Dr. Talcott led a team of researchers from the National Cancer Institute Named Entity Tag B-PER I-PER O O O O O O O B-ORG I-ORG I-ORG Table 1: IOB tagging scheme. and the I-XXX tag is used for wo"
I08-5010,N06-3009,0,0.0564884,"Missing"
I08-5010,A97-1030,0,\N,Missing
I08-5015,U06-1009,0,0.0696997,"Missing"
I08-5015,W95-0107,0,0.056166,"ere Named Entities(NEs). We divided the corpus into training and testing sets. The training set consisted of 46068 tokens out of which 8485 were NEs. The testing set consisted of 17951 tokens out of which 2407 were NEs. The tagset as mentioned in the release, was based on AUKBC’s ENAMEX,TIMEX and NAMEX, has the following tags: NEP (Person), NED (Designation), NEO (Organization), NEA (Abbreviation), NEB (Brand), NETP (Title-Person), NETO (Title-Object), NEL (Location), NETI (Time), NEN (Number), NEM (Measure) & NETE (Terms). 5.2 Tagging Scheme The corpus is tagged using the IOB tagging scheme (Ramshaw and Marcus, 1995). In this scheme each line contains a word at the beginning followed by its tag. The tag encodes the type of named entity and whether the word is in the beginning or inside the NE. Empty lines represent sentence(document) boundaries. An example is given in table 1. Words tagged with O are outside of named entities and the I-XXX tag is used for words inside a named entity of type XXX. Whenever two entities of type XXX are immediately next to each other, the first word of the second entity will be tagged BXXX in order to show that it starts another entity. This tagging scheme is the IOB scheme o"
I08-5015,N06-3009,0,0.0218565,"in a document is used in many of the language processing tasks. NER was created as a subtask in Message Understanding Conference (MUC) (Chinchor, 1997). This reflects the importance of NER in the area of Information Extraction (IE). NER has many applications in the areas of Natural Language Processing, Information Extraction, Information Retrieval and speech processing. NER is also used in question answering systems (Toral et al., 2005; Molla et al., 2006), and machine translation systems (Babych and Hartley, 2003). It is also a subtask in organizing and re105 trieving biomedical information (Tsai, 2006). The process of NER consists of two steps • identification of boundaries of proper nouns. • classification of these identified proper nouns. The Named Entities(NEs) should be correctly identified for their boundaries and later correctly classified into their class. Recognizing NEs in an English document can be done easily with a good amount of accuracy(using the capitalization feature). Indian Languages are very much different from the English like languages. Some challenges in named entity recognition that are found across various languages are: Many named entities(NEs) occur rarely in the c"
I08-5015,A97-1030,0,0.1216,"scores of all possible state sequences, T Zo = ∑ ∗exp( ∑ ∑ λk fk (st−1 , st , o,t)) s∈ST t=1 k and that the number of state sequences is exponential in the input sequence length,T. In arbitrarilystructure CRFs, calculating the partition function in closed form is intractable, and approximation methods such as Gibbs sampling, or loopy belief propagation must be used. 4 Features There are many types of features used in general NER systems. Many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word. (Mikheev, 1997; Wacholder et al., 1997; Bikel et al., 1997). Following are examples of binary features commonly used. All-Caps (IBM), Internal capitalization (eBay), initial capital (Abdul Kalam), uncapitalized word (can), 2-digit number (83, 28), 4-digit number (1273, 1984), all digits (8, 31, 1228) etc. The features that correspond to the capitalization are not applicable to Telugu. We have not used any binary features in our experiments. Gazetteers are used to check if a part of the named entity is present in the gazetteers. We don’t have proper gazetteers for Telugu. Lexical features like a sliding window [w−2 ,w−1 ,wo ,w1 ,w2"
I08-5015,J97-3003,0,\N,Missing
I08-5015,W03-0428,0,\N,Missing
I08-5015,W03-2201,0,\N,Missing
I08-5015,A97-1029,0,\N,Missing
I08-6006,P97-1017,0,0.576251,"Missing"
I08-6006,P00-1056,0,0.2471,"Missing"
I08-6006,J93-2003,0,0.0225125,"itional probabilities derived from counting the alignments. 1 A source language word can have more than one valid transliteration in target language. For example for the Hindi word below four different transliterations are possible . - gautam, gautham, gowtam, gowtham Therefore, in a CLIR context, it becomes important to generate all possible transliterations to retrieve documents containing any of the given forms. Most current transliteration systems use a generative model for transliteration such as freely available GIZA++1 (Och and Ney , 2000),an implementation of the IBM alignment models (Brown et al., 1993). These systems use GIZA++ (which uses HMM alignment) to get character level alignments (n-gram) from word aligned data. The transliteration system was built by counting up the alignments and converting the counts to conditional probabilities. The readers are strongly encouraged to refer to (Nasreen and Larkey , 2003) to have a detailed understanding of this technique. Introduction In cross language information retrieval (CLIR) a user issues a query in one language to search a document collection in a different language. Out of Vocabulary (OOV) words are problematic in CLIR. These words are a"
I08-6006,P06-1009,0,0.0348935,"Missing"
I08-6006,W98-1005,0,0.471388,"Missing"
I17-1082,N15-1012,1,0.785249,"ized in a cluster graph (CG) as shown in Figure 4 in which nodes represent partial trees, and edge weights represent the combination probability of partial trees represented by the edge nodes calculated using the Equation 4. An edge exists between two nodes if the partial trees at the nodes contain mention about same named entity. From cluster graph we try to extract a connected subgraph (SG) which maximizes the Syntactic Linearization We make use of the syntactic linearization model proposed by Puduppully et al. (2016) to linearize the input set of partial trees. Puduppully et al. (2016) and Liu et al. (2015) propose a transitionbased word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, yields a sentence together with its dependency parse tree that conforms to input syntactic constraints (Zhang, 2013). The system is flexible with respect to input constraints, performing abstract word ordering when no constraints are given, but gives increasingly confined outputs when more POS and dependency relations are specified. We retrain their model1 using autoparsed data obtained using Stanford Dependency Parser. 1 (6) where ShannonEnt is"
I17-1082,W09-1801,0,0.0286989,"rent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences whil"
I17-1082,P13-1020,0,0.0168147,"e to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences while Li (2015) system make use of semantic link network on"
I17-1082,P13-1101,0,0.0178577,"ing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2"
I17-1082,N16-1058,1,0.922299,"and to generate the sentences from scratch. We build a model to this end by leveraging syntactic dependencies. Input for our model is the set of syntactic dependency trees obtained by parsing sentences in the corpus to be summarized. Relevant and noise pruned partial tree structures are extracted from the set of dependency trees and different subsets of maximally relevant partial dependency structures are identified. Partial trees in different subsets are linearized to generate individual summary sentences. In this work, we utilize transition-based syntactic linearization approach proposed by Puduppully et al. (2016) to linearize a combination of partial trees and to generate a noise free summary sentence. The combinability of a set of partial trees to form a full dependency tree of a valid sentence is estimated using a generative model of syntactic dependency trees (Zhang et al., 2016). As a result, the model is allowed to exhibit its own learnt writing style while generating summary sentences. The summaries generated by our system are evaluated on the DUC 2004, DUC 2007 and TAC 2011 muti-document summarization data-sets. In addition, we relied on human evaluation to evaluate factual accuracy and linguis"
I17-1082,P11-1049,0,0.0216954,"2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that th"
I17-1082,E09-1089,0,0.0211778,"on and generates summary sentences exhibiting coher812 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 812–821, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tive summarization with syntactic dependency trees, which entrust the summarization model to generate summary sentences without exploiting any kind of subsentential or phrasal sequential structures originally present in the input corpus. Our code is released at https://bitbucket. org/litton_kurisinkel/tree_sum 2 Related Work Text summarization can be achieved using extractive (Takamura and Okumura, 2009; Lin and Bilmes, 2011; Wang et al., 2008) and abstractive methods (Bing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence struc"
I17-1082,P15-1153,0,0.341417,"l Joint Conference on Natural Language Processing, pages 812–821, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tive summarization with syntactic dependency trees, which entrust the summarization model to generate summary sentences without exploiting any kind of subsentential or phrasal sequential structures originally present in the input corpus. Our code is released at https://bitbucket. org/litton_kurisinkel/tree_sum 2 Related Work Text summarization can be achieved using extractive (Takamura and Okumura, 2009; Lin and Bilmes, 2011; Wang et al., 2008) and abstractive methods (Bing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2"
I17-1082,P16-1046,0,0.0311407,"ty. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences while Li (2015) system make use of semantic link network on basic semantic units (BSUs) to generate summary. Neither of these methods employ a learnt model to generate summary sentences. Instead, they make use sequential structures in the source text itself to construct the summary sentences. Cheng and Lapata (2016) propose a fully data driven approach using neuFigure 1: Overall approach ral network for single document summarization by extracting words. They have treated highlighted text in news articles consisting of very short bulleted lines on the web as summary of the corresponding article. A major challenge of using a fully data driven approach for multi-document abstractive summarization using neural network is the expensive task of creating dataset which can be used to jointly model extraction of relevant content and generation good quality summary sentences. But multidocument abstractive summariz"
I17-1082,D15-1219,0,0.339639,"on Natural Language Processing, pages 812–821, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tive summarization with syntactic dependency trees, which entrust the summarization model to generate summary sentences without exploiting any kind of subsentential or phrasal sequential structures originally present in the input corpus. Our code is released at https://bitbucket. org/litton_kurisinkel/tree_sum 2 Related Work Text summarization can be achieved using extractive (Takamura and Okumura, 2009; Lin and Bilmes, 2011; Wang et al., 2008) and abstractive methods (Bing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-K"
I17-1082,P10-1058,0,0.0273203,"result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences while Li (2015) system make use"
I17-1082,N16-1035,0,0.422268,"res are extracted from the set of dependency trees and different subsets of maximally relevant partial dependency structures are identified. Partial trees in different subsets are linearized to generate individual summary sentences. In this work, we utilize transition-based syntactic linearization approach proposed by Puduppully et al. (2016) to linearize a combination of partial trees and to generate a noise free summary sentence. The combinability of a set of partial trees to form a full dependency tree of a valid sentence is estimated using a generative model of syntactic dependency trees (Zhang et al., 2016). As a result, the model is allowed to exhibit its own learnt writing style while generating summary sentences. The summaries generated by our system are evaluated on the DUC 2004, DUC 2007 and TAC 2011 muti-document summarization data-sets. In addition, we relied on human evaluation to evaluate factual accuracy and linguistic quality of generated summary sentences. To our knowledge this is the first work on multi-document abstracExisting work for abstractive multidocument summarization utilise existing phrase structures directly extracted from input documents to generate summary sentences. Th"
I17-1082,W04-1013,0,\N,Missing
I17-1082,W03-1101,0,\N,Missing
I17-1082,P11-1052,0,\N,Missing
I17-2034,D15-1075,0,0.0432176,"external discrete vocabulary information (Miller et al., 1990). Also they are less equipped to conceive the accurate semantic inference from long sequences of summary text. For instance, consider the following statements. A: Mary lived through an era of liberating reform for women. B: Mary’s life spanned years of incredible change for women. C: Mary lived through an era of suppression of women. Considering A as the reference summary element, most of the previous metrics give higher score to C than B even when C is clearly contradicting A. Actual scores for above samples are shown in Table 1. Bowman et al. (2015) mention that understanding entailment and contradiction is fundamental to understanding natural language. The lack of consideration of semantics when evaluating summarization automatically, motivates us to propose a new metric focused on semantic matching between system and human summaries. Ideally a metric evaluating an abstract system summary should represent the extent to which the system-generated summary approximates the semantic inference conceived by the reader using a humanwritten reference summary. Most of the previous approaches relied upon word or syntactic sub-sequence overlap to"
I17-2034,P12-1091,0,0.0243056,"3 0.05 0.45 0.65 C 0.66 0.4 0.66 0.45 0.33 0.48 given weights which are equal to the number of reference summaries they occur in. After this, a generated summary is given a score which is equal to the normalized sum of the weights of the overlapping SCUs. Pyramid score does not evaluate the semantic overlap in a continuous space, and also requires manual efforts when performing evaluation. Autopyramid (Passonneau et al., 2013) automates a part of the pyramid based evaluation which checks whether an SCU is present in the generated summary. Though they use various generic dense representations (Guo and Diab, 2012) for estimating semantic similarity between SCUs, Autopyramid cannot explicitly quantify the quality of a summary based on its agreement or contradiction with a reference summary. The PEAK (Yang et al., 2016) method for evaluation automates the extraction part of the SCUs, and they use the ADW (Align, Disambiguate and Walk) algorithm (Pilehvar et al., 2013) to compute semantic similarity. However, their approach fails to model contradiction, paraphrase identification and other features like natural language inference. Table 1: Scores given to Samples B and C by Various Metrics Our main contrib"
I17-2034,W04-1013,0,0.0433381,"– Hyderabad Hyderabad India {raghuram.vadapalli, litton.jkurisinkel}@research.iiit.ac.in {manish.gupta, vv}@iiit.ac.in Abstract on abstractive summarization, to establish a metric which can judge the quality of a system-generated abstractive summary. An ideal metric should be able to represent the similarity of semantic inference perceived by a reader from system-generated summary to that from a human-written reference summary. Most of the existing summarization metrics are well-suited for extractive summaries, and are directly or indirectly based upon word or syntactic substructure overlap (Lin, 2004). Evaluation of abstractive summarization needs a semantic overlap based method. Although there are some metrics which attempt to evaluate the summary at semantic level (Nenkova and Passonneau, 2004; Passonneau et al., 2013; Yang et al., 2016), they either demand high level of human involvement or rely on external discrete vocabulary information (Miller et al., 1990). Also they are less equipped to conceive the accurate semantic inference from long sequences of summary text. For instance, consider the following statements. A: Mary lived through an era of liberating reform for women. B: Mary’s"
I17-2034,N04-1019,0,0.357028,"which can judge the quality of a system-generated abstractive summary. An ideal metric should be able to represent the similarity of semantic inference perceived by a reader from system-generated summary to that from a human-written reference summary. Most of the existing summarization metrics are well-suited for extractive summaries, and are directly or indirectly based upon word or syntactic substructure overlap (Lin, 2004). Evaluation of abstractive summarization needs a semantic overlap based method. Although there are some metrics which attempt to evaluate the summary at semantic level (Nenkova and Passonneau, 2004; Passonneau et al., 2013; Yang et al., 2016), they either demand high level of human involvement or rely on external discrete vocabulary information (Miller et al., 1990). Also they are less equipped to conceive the accurate semantic inference from long sequences of summary text. For instance, consider the following statements. A: Mary lived through an era of liberating reform for women. B: Mary’s life spanned years of incredible change for women. C: Mary lived through an era of suppression of women. Considering A as the reference summary element, most of the previous metrics give higher scor"
I17-2034,P15-1034,0,0.0284337,"mbine these measures into a single score such that the score maximally correlates with human evaluation of summaries. • We experimentally show the robustness and effectiveness of SSAS. 3 The rest of the paper is organized as follows. Section 2 describes previous attempts at evaluating summarization systems. Section 3 describes our approach in detail. We discuss our experimental results in Section 4. We conclude with a summary in Section 5. 2 We first extract SCUs using the automatic SCU extraction scheme introduced by PEAK model I, which in turn relies on Open Information Extraction (OpenIE) (Angeli et al., 2015) for the extraction process. Given a reference summary R and a system summary S, we obtain SCU sets SCU s(R) and SCU s(S) with cardinality n and m respectively. Next, we derive a set of natural language inference and paraphrasing features from the text pieces. Computation of these features is explained in Section 3.1. After that, we use a ranking model to learn the weights for combining these features to obtain a score. Finally, we normalize the obtained score. Ranking and Normalization are discussed in detail in Section 3.2. Related Work The following are two broad approaches popular for eval"
I17-2034,P13-2026,0,0.0684362,"f a system-generated abstractive summary. An ideal metric should be able to represent the similarity of semantic inference perceived by a reader from system-generated summary to that from a human-written reference summary. Most of the existing summarization metrics are well-suited for extractive summaries, and are directly or indirectly based upon word or syntactic substructure overlap (Lin, 2004). Evaluation of abstractive summarization needs a semantic overlap based method. Although there are some metrics which attempt to evaluate the summary at semantic level (Nenkova and Passonneau, 2004; Passonneau et al., 2013; Yang et al., 2016), they either demand high level of human involvement or rely on external discrete vocabulary information (Miller et al., 1990). Also they are less equipped to conceive the accurate semantic inference from long sequences of summary text. For instance, consider the following statements. A: Mary lived through an era of liberating reform for women. B: Mary’s life spanned years of incredible change for women. C: Mary lived through an era of suppression of women. Considering A as the reference summary element, most of the previous metrics give higher score to C than B even when C"
I17-2034,P13-1132,0,0.0296204,"efforts when performing evaluation. Autopyramid (Passonneau et al., 2013) automates a part of the pyramid based evaluation which checks whether an SCU is present in the generated summary. Though they use various generic dense representations (Guo and Diab, 2012) for estimating semantic similarity between SCUs, Autopyramid cannot explicitly quantify the quality of a summary based on its agreement or contradiction with a reference summary. The PEAK (Yang et al., 2016) method for evaluation automates the extraction part of the SCUs, and they use the ADW (Align, Disambiguate and Walk) algorithm (Pilehvar et al., 2013) to compute semantic similarity. However, their approach fails to model contradiction, paraphrase identification and other features like natural language inference. Table 1: Scores given to Samples B and C by Various Metrics Our main contributions are as follows. • We propose a novel metric SSAS for semantic assessment of abstractive summaries. • The method includes computing various semantic and lexical similarity measures between reference summary and system summary, and learning a weight vector to combine these measures into a single score such that the score maximally correlates with human"
L18-1507,P10-1095,0,0.0367635,"oss-lingual summarization. Most recently, Zhang et al. (2016) proposed abstractive crosslingual summarization. Yao et al. (2015) proposed compressive cross-lingual summarization inspired by phrasebased translation models. Wan (2011) proposed summarization using information from both source and translated article, while Wan et al. (2010) proposed to summarize considering the translation quality prediction. Most extractive cross-lingual summarization systems have a sequential pipeline architecture. Additionally, most of them output a proposed mono-lingual summary and its translation at the end (Litvak et al., 2010; Orasan and Chiorean, 2008; Pingali et al., 2007; Wan et al., 2010; Wan, 2011; Yao et al., 2015). 2 http://www.sdltrados.com/ https://www.matecat.com/ 4 http://omegat.org/ 3 This motivates the design of the workbench where the annotator can edit the mono-lingual summaries and its translation easily to get publishable cross-lingual summaries, and which can also collect various logs. 3. The Workbench The workbench is a flexible, language independent tool for editing automatically generated cross-lingual summaries. The main features of the workbench are: • The workbench provides a unique user-fr"
L18-1507,orasan-chiorean-2008-evaluation,0,0.029507,"tion. Most recently, Zhang et al. (2016) proposed abstractive crosslingual summarization. Yao et al. (2015) proposed compressive cross-lingual summarization inspired by phrasebased translation models. Wan (2011) proposed summarization using information from both source and translated article, while Wan et al. (2010) proposed to summarize considering the translation quality prediction. Most extractive cross-lingual summarization systems have a sequential pipeline architecture. Additionally, most of them output a proposed mono-lingual summary and its translation at the end (Litvak et al., 2010; Orasan and Chiorean, 2008; Pingali et al., 2007; Wan et al., 2010; Wan, 2011; Yao et al., 2015). 2 http://www.sdltrados.com/ https://www.matecat.com/ 4 http://omegat.org/ 3 This motivates the design of the workbench where the annotator can edit the mono-lingual summaries and its translation easily to get publishable cross-lingual summaries, and which can also collect various logs. 3. The Workbench The workbench is a flexible, language independent tool for editing automatically generated cross-lingual summaries. The main features of the workbench are: • The workbench provides a unique user-friendly environment for anno"
L18-1507,L16-1095,0,0.0455758,"Missing"
L18-1507,2006.amta-papers.25,0,0.0599669,"vaktA ne kahA ki nausenA ke AdhAra aDDe para DyUTI para ke daurAn ek nAvika ko apane kartavya hathiyAra kI golIbarI se golI calane kI vajaha se golI kI ghAtaka buleTa kI coTa meM hai coTa lagI thI. • Text input: For all items, all the changes in text are logged, along with all copy/cut/paste events. This is specifically important as keystroke logging doesn’t provide accurate and wholesome information in case of complex scripts and use of IMEs. Figure 6: Edits made to the Hindi translation of the summary. Removed, Added For translation post-editing, Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), along with information about insertion, deletion, and substitution of single words as well as shifts of word sequences can be calculated and stored when the translation is finalized. For editing of mono-lingual summary, along with the sentence level editing logs, following summary-level events are also logged by the workbench. The total time taken for editing can be calculated as the time difference between completion of editing and article load time, and reducing the difference by the amount of time the annotator was marked away. In addition to these logs, annotator’s browser and platform i"
L18-1507,P10-1094,0,0.024007,"e support editing of cross-lingual summaries as well. b) A few of them allow the recording of various kinds of logs about the translation post-editing process, while we allow recording comprehensive logs about the human editing of summary and translations. Some work exists on cross-lingual summarization. Most recently, Zhang et al. (2016) proposed abstractive crosslingual summarization. Yao et al. (2015) proposed compressive cross-lingual summarization inspired by phrasebased translation models. Wan (2011) proposed summarization using information from both source and translated article, while Wan et al. (2010) proposed to summarize considering the translation quality prediction. Most extractive cross-lingual summarization systems have a sequential pipeline architecture. Additionally, most of them output a proposed mono-lingual summary and its translation at the end (Litvak et al., 2010; Orasan and Chiorean, 2008; Pingali et al., 2007; Wan et al., 2010; Wan, 2011; Yao et al., 2015). 2 http://www.sdltrados.com/ https://www.matecat.com/ 4 http://omegat.org/ 3 This motivates the design of the workbench where the annotator can edit the mono-lingual summaries and its translation easily to get publishable"
L18-1507,P11-1155,0,0.0199721,". They compare with our system in following ways. a) While they support translation post-editing, we support editing of cross-lingual summaries as well. b) A few of them allow the recording of various kinds of logs about the translation post-editing process, while we allow recording comprehensive logs about the human editing of summary and translations. Some work exists on cross-lingual summarization. Most recently, Zhang et al. (2016) proposed abstractive crosslingual summarization. Yao et al. (2015) proposed compressive cross-lingual summarization inspired by phrasebased translation models. Wan (2011) proposed summarization using information from both source and translated article, while Wan et al. (2010) proposed to summarize considering the translation quality prediction. Most extractive cross-lingual summarization systems have a sequential pipeline architecture. Additionally, most of them output a proposed mono-lingual summary and its translation at the end (Litvak et al., 2010; Orasan and Chiorean, 2008; Pingali et al., 2007; Wan et al., 2010; Wan, 2011; Yao et al., 2015). 2 http://www.sdltrados.com/ https://www.matecat.com/ 4 http://omegat.org/ 3 This motivates the design of the workb"
N16-2014,D10-1047,0,0.0632496,"a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important"
N16-2014,P04-1051,0,0.045439,"ontext in the original corpus, placing it in the summary in a different context can render a wrong inference to the reader of the summary. The main intuition behind our approach begins with a crucial question about the linguistic nature of a text. Is text a bag of words every time? Psycholinguistic studies suggest that local coherence plays a vital role in inference formation while reading a text (McKoon and Ratcliff, 1992). Local coherence is undoubtedly necessary for global coherence and has received considerable attention in Computational Linguistics. ((Marcu, 2000), (Foltz et al., 1998), (Althaus et al., 2004), (Karamanis et al., 2004)). Linguistically, every sentence is uttered not in isolation but within a context in a given discourse. To make a coherent reading, sentences use various discourse connectives that bind one sentence with another. A set of such structurally related sentences forms a Locally Coherent Discourse Unit (hereafter referred to as LDU). In the current work, we suggest that it is important to leverage this structural coherence to improve the comprehensibility of the generated summary. It should be noted that the concept of LDU is different from the elementary discourse units ("
N16-2014,J08-1001,0,0.0185835,"endence score of individual sentences. Comprehensibility index for the generated summary is the average contextual independence score of a sentence in the summary. We verified, through human evaluators, whether the comprehensibility index is actually representative of the human comprehensibility. 2 Previous Work Identification of locally coherent discourse unit (LDU) and combining the information to create a comprehensible summary is a novel problem which is not attempted by any of the previous works in the field of natural language processing to the best of our knowledge. Barzilay and Lapata(Barzilay and Lapata, 2008) attempt to measure the global coherence in terms of local coherence which is measured in terms of entity role switch while GFlow(Christensen et al., 2013) came up with a metric to measure the coherence of the generated summary with respect to a corpus level discourse graph. Still, these two works are not directly relevant to local discourse unit identification per se. Substantial work has been done on extractive summarization which tries to achieve a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998),"
N16-2014,P10-1084,0,0.0549254,"Missing"
N16-2014,N13-1136,0,0.137296,"omprehensibility. The problem 95 of extractive summarization can be formulated as a function maximization problem in the space of all candidate summaries as follows. X S ∗ ∈ argmaxS⊆V F (S)subject to ci <= b i∈S (1) where F is an objective function, is the summary which maximizes F with an adopted optimization method, S is a candidate summary, ci is the cost of selecting a sentence i into summary, b is the upper bound on the total cost and V is the set of total number of sentences in the corpus. The current work is inspired by two of the previous works namely (Lin and Bilmes, 2011) and GFlow (Christensen et al., 2013). Lin & Bilmes observed that if the objective function to score candidate summaries is monotone sub-modular, a greedy approach can ensure the approximation of the summary at the global maximum by a factor of 0.632 as follows. S∗ ˆ ≥ (1−1/e)∗F (Sopt ) ≈ 0.632∗F (Sopt ) (2) F (S) where Sˆ is the summary obtained from monotone sub-modular function F and Sopt is the summary at the global maximum of F. G-Flow aimed at generating coherent summaries by constructing a sentence-level discourse graph for the entire corpus and the information from the graph is utilized to quantify the coherence of candid"
N16-2014,C12-1056,0,0.0186027,"while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summ"
N16-2014,N09-1041,0,0.0360369,"ourse unit identification per se. Substantial work has been done on extractive summarization which tries to achieve a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Con"
N16-2014,I11-1118,0,0.0550466,"Missing"
N16-2014,D11-1105,0,0.0585663,"Missing"
N16-2014,P11-1052,0,0.245467,"s of readability, coherence and comprehensibility. The problem 95 of extractive summarization can be formulated as a function maximization problem in the space of all candidate summaries as follows. X S ∗ ∈ argmaxS⊆V F (S)subject to ci <= b i∈S (1) where F is an objective function, is the summary which maximizes F with an adopted optimization method, S is a candidate summary, ci is the cost of selecting a sentence i into summary, b is the upper bound on the total cost and V is the set of total number of sentences in the corpus. The current work is inspired by two of the previous works namely (Lin and Bilmes, 2011) and GFlow (Christensen et al., 2013). Lin & Bilmes observed that if the objective function to score candidate summaries is monotone sub-modular, a greedy approach can ensure the approximation of the summary at the global maximum by a factor of 0.632 as follows. S∗ ˆ ≥ (1−1/e)∗F (Sopt ) ≈ 0.632∗F (Sopt ) (2) F (S) where Sˆ is the summary obtained from monotone sub-modular function F and Sopt is the summary at the global maximum of F. G-Flow aimed at generating coherent summaries by constructing a sentence-level discourse graph for the entire corpus and the information from the graph is utilize"
N16-2014,C10-1101,0,0.0805044,"Missing"
N16-2014,C10-1111,0,0.0233701,"tric to measure the coherence of the generated summary with respect to a corpus level discourse graph. Still, these two works are not directly relevant to local discourse unit identification per se. Substantial work has been done on extractive summarization which tries to achieve a proper content coverage while reducing the redundancy. Approaches include the use of Maximum Marginal Relevance (Carbonell and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summ"
N16-2014,E09-1089,0,0.375041,"2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summarization. By Contextual Independence of a sentence, we mean that the sentence can be globally understood even when the sentences preceding/following it are not available"
N16-2014,C08-1124,0,0.0366908,"Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summarization. By Contextual Independence of a sentence, we mean that the sentence can be globally understood even when the sentences preceding/following it are not available to the reader. Contextual dependence, signifies only the structural dependence of a sent"
N16-2014,D12-1022,0,0.0318867,"ll and Goldstein, 1998), Centroidbased Summarization (Radev et al., 2002), Summarization through Keyphrase Extraction (Qazvinian et 96 al., 2010) and Formulation as Minimum Dominating Set problem (Shen and Li, 2010), Graph centrality to estimate the salience of a sentence (Erkan and Radev, 2004). Approaches to content analysis include generative topic models (Haghighi and Vanderwende, 2009), (Celikyilmaz and HakkaniTur, 2010), (Li et al., 2011b) and Discriminative models (Aker et al., 2010), ILP2 (Galanis et al., 2012) Joint Optimization of the Importance and Diversity of summary’s sentences (Woodsend and Lapata, 2012), Language Model based scoring function (Takamura and Okumura, 2009) as a maximum concept coverage problem with knapsack constraint(MCKP) (Wong et al., 2008). Lin and Bilmes formulated summarization as a sub-modular function maximization problem in the possible set of candidate summaries with due respect to the summary space constraint (Lin and Bilmes, 2011). 3 Contextual Independence Identifying whether a sentence is contextually independent or not is an important step in our approach to summarization. By Contextual Independence of a sentence, we mean that the sentence can be globally underst"
N16-2014,P04-1050,0,\N,Missing
N18-1167,P16-1179,0,0.0936546,"effective graph densification method which may be applied to improve EL involving any KG. • By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance. • We have made ELDEN’s code and data publicly available4 . 2 Related Work Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of EL"
N18-1167,D11-1098,0,0.0219611,"ng external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measures for word association including PMI, Jaccard (Dice, 1945) and Co-occurrence Significance Ratio (CSR). Damani (2013) proves that considering corpus level significant co-occurrences, PMI is better than others. Budiu et al. (2007) compare Latent Semantic Analysis (LSA), PMI and Generalized Latent Semantic Analysis (GLSA) and conclude that for large corpora like web corpus, PMI works best on word similarity tests. Hence, we chose PMI to refine co-occurring mentions of entities in web corpus. 3 Definitions and Problem Formulation In this section, we present a f"
N18-1167,D13-1184,0,0.0324844,"ased on number of common edges in KG. 3 This paper focuses on mention disambiguation. We assume mention and candidate entities are detected already. We make the following contributions: • ELDEN presents a simple yet effective graph densification method which may be applied to improve EL involving any KG. • By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance. • We have made ELDEN’s code and data publicly available4 . 2 Related Work Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hof"
N18-1167,Q15-1011,0,0.382817,"mbiguation performance using KG densification: edges to WWW conference, a sparsely-connected entity in the KG, is increased by adding edges from pseudo entity Program Committee whose mention cooccurs with it in web corpus. ELDEN, the system proposed in this paper, uses such densified KG to successfully link ambiguous mention WWW to the correct entity WWW conference, instead of the more popular entity World Wide Web. Introduction Entity Linking (EL) is the task of mapping mentions of an entity in text to the corresponding entity in Knowledge Graph (KG) (Hoffart et al., 2011; Dong et al., 2014; Chisholm and Hachey, 2015). EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text. Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity (Milne and Witten, 2008; Globerson et al., 2016). In the stateof-the-art EL system by (Yamada et al., 2016), coherence is measured as distance between embeddings of entities. This system performs well on entiti"
N18-1167,P89-1010,0,0.0527623,"Missing"
N18-1167,W13-3503,0,0.0194452,"(Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measures for word association including PMI, Jaccard (Dice, 1945) and Co-occurrence Significance Ratio (CSR). Damani (2013) proves that considering corpus level significant co-occurrences, PMI is better than others. Budiu et al. (2007) compare Latent Semantic Analysis (LSA), PMI and Generalized Latent Semantic Analysis (GLSA) and conclude that for large corpora like web corpus, PMI works best on word similarity tests. Hence, we chose PMI to refine co-occurring mentions of entities in web corpus. 3 Definitions and Problem Formulation In this section, we present a few definitions and formulate the EL problem. Knowledge Graph (KG): A Knowledge Graph is defined as G = (E, F ) with entities E as nodes and F as edges. I"
N18-1167,K16-1026,0,0.0377315,"hod shows how unstructured data (web corpus about the entity to be linked) can be effectively used for entity disambiguation. Entity Embeddings: ELDEN presents a method to enhance embedding of entities and words in a 4 https://github.com/ priyaradhakrishnan0/ELDEN 5 (Shen et al., 2015) presents a survey of EL systems. 6 Named Entity Disambiguation (NED) and EL are synonymous terms in research (Hoffart et al., 2011) 1845 common vector space. Word embedding methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis e"
N18-1167,D17-1277,0,0.240235,"s://github.com/ priyaradhakrishnan0/ELDEN 5 (Shen et al., 2015) presents a survey of EL systems. 6 Named Entity Disambiguation (NED) and EL are synonymous terms in research (Hoffart et al., 2011) 1845 common vector space. Word embedding methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entit"
N18-1167,N13-1122,0,0.0314971,"ost EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of ELDEN. Enhancing entity disambiguation: Among methods proposed in literature to enhance entity disambiguation utilizing KG (Bhattacharya and Getoor) uses additional relational information between database references; (Han and Zhao, 2010) uses semantic relatedness between entities in other KGs; and (Shen et al., 2018) uses paths consisti"
N18-1167,P10-1006,0,0.027817,"ed are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of ELDEN. Enhancing entity disambiguation: Among methods proposed in literature to enhance entity disambiguation utilizing KG (Bhattacharya and Getoor) uses additional relational information between database references; (Han and Zhao, 2010) uses semantic relatedness between entities in other KGs; and (Shen et al., 2018) uses paths consisting of defined relations between entities in the KG (IMDB and DBLP). All these methods utilize structured information, while our method shows how unstructured data (web corpus about the entity to be linked) can be effectively used for entity disambiguation. Entity Embeddings: ELDEN presents a method to enhance embedding of entities and words in a 4 https://github.com/ priyaradhakrishnan0/ELDEN 5 (Shen et al., 2015) presents a survey of EL systems. 6 Named Entity Disambiguation (NED) and EL are s"
N18-1167,D13-1041,0,0.172608,"te entities are detected already. We make the following contributions: • ELDEN presents a simple yet effective graph densification method which may be applied to improve EL involving any KG. • By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance. • We have made ELDEN’s code and data publicly available4 . 2 Related Work Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedd"
N18-1167,D15-1241,1,0.531835,"g methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measur"
N18-1167,D14-1162,0,0.0874171,"Missing"
N18-1167,K16-1025,0,0.136506,"xt to the corresponding entity in Knowledge Graph (KG) (Hoffart et al., 2011; Dong et al., 2014; Chisholm and Hachey, 2015). EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text. Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity (Milne and Witten, 2008; Globerson et al., 2016). In the stateof-the-art EL system by (Yamada et al., 2016), coherence is measured as distance between embeddings of entities. This system performs well on entities which are densely-connected in KG, but not so well on sparsely-connected entities in the KG. We demonstrate this problem using the example sentence in Figure 1. This sentence has two mentions: Andrei Broder and WWW. The figure also shows mention-entity linkages, i.e., mentions and their candidate entities in KG. Using a conventional EL system, the first mention Andrei Broder1 can be easily linked to Andrei Broder using string similarity between the mention and candidate entity strings. Str"
N18-1167,Q15-1023,0,0.0879102,"aking this TAC candidate set publicly available. Baseline: Yamada16 Our baseline is the Yamada et al. system explained in Section 3. Entity embedding distance measured using ve trained on the 16 This is a tunable parameter. These recent datasets consist of other evaluations, e.g., mention detection, multilinguality etc. which is beyond the scope of the paper and hence we didnt focus on them in the paper. 18 https://github.com/masha-p/PPRforNED 19 http://lucene.apache.org/solr/ 17 Method CONLL CONLL TAC (P-micro) (P-micro) (P-macro) (Hoffart et al., 2011) 82.5 81.7 (He et al., 2013) 85.6 84.0 (Ling et al., 2015) 67.5 (Barrena et al., 2016) 88.32 (Chisholm and Hachey, 2015) - 81.0 - 88.7 - (Pershina et al., 2015) 91.8 89.9 91.7 - - (Globerson et al., 2016) (Yamada et al., 2016) 93.1 92.6 85.2 ELDEN 93.0 93.7 89.6 86.8 80.7 87.2 Table 4: Performance comparison with other recent EL approaches. ELDEN matches best results in CoNLL and outperforms the state-of-the-art in TAC dataset. (Please see Section 6.1 for details and ψELDEN++ row of Table 5 for ELDEN results.) input KG G is ψYamada . 6 6.1 Results Does ELDEN’s selective densification help in disambiguation in EL? In Table 4, we compare ELDEN’s EL per"
N19-2017,P14-5010,0,0.0026459,"demarcating lines separating the normal and alternate set of messages. In the example (Table 1), based on the condition of the station at which the vehicle has arrived, the step mentioned as alteranate flow or the steps 6 to 10 of the normal flow need to be followed. This branching is depicted using the altFigure 1: MSC corresponding to the use-case description in Table 1 In this paper, we describe our approach to extract MSCs from use-cases based on Open Information Extraction (OpenIE) (Mausam, 2016). OpenIE extracts structured information from a 1 In this paper, we use the Stanford CoreNLP (Manning et al., 2014) pipeline for dependency parsing. 131 Processing input using OpenIE: Use-case descriptions generally use well-written English. Over such text, generation of candidate messages requires a simple relation and argument extractor. We thus propose use of the OpenIE framework which provides tuples of the form (left argument, relation, right argument 1, right argument 2, . . .) along with confidence scores with each tuple. We first process the input use-cases through the OpenIE technique described in (Mausam et al., 2012; Mausam, 2016) and obtain a list of candidate messages. box shown in Figure 1. T"
N19-2017,W17-5912,1,0.792108,"ith multiple verbs, conjunctions and passive voice. Kof (2007a) also proposes a set of heuristics to handle missing sender or receiver of a message. A key limitation of this method is that it ignores syntactic as well as semantic relations between the verb and its corresponding sender or receiver. Hence it is possible that some of the <sender, message label, receiver> tuples may not be valid semantically. It is important to note that Kof (2007a; 2007b) do not focus on other features of MSC such as conditions, timers and altboxes. 3 Defining Actors in Software Engineering UseCases: In general (Bedi et al., 2017; Patil et al., 2018; Palshikar et al., 2019), the notion of actors in MSC is based on named entities of the types - PERSON, LOCATION and ORGANIZATION. However, in the software requirements domain, these may not be the only entities interacting with each other. This criteria is extended and is proposed to include: • PERSONS (Human SYSTEM users), ORGANIZATIONS and LOCATIONS • SYSTEMS (such as Supervisory System, Library System) • Persistent components of SYSTEMS (such as servers, databases, customer accounts) • Persistent processes in the SYSTEM (such as schedulers, daemons) • SYSTEM components"
N19-2017,D12-1048,0,0.0223572,"ts structured information from a 1 In this paper, we use the Stanford CoreNLP (Manning et al., 2014) pipeline for dependency parsing. 131 Processing input using OpenIE: Use-case descriptions generally use well-written English. Over such text, generation of candidate messages requires a simple relation and argument extractor. We thus propose use of the OpenIE framework which provides tuples of the form (left argument, relation, right argument 1, right argument 2, . . .) along with confidence scores with each tuple. We first process the input use-cases through the OpenIE technique described in (Mausam et al., 2012; Mausam, 2016) and obtain a list of candidate messages. box shown in Figure 1. The paper is organized as follows. Section 2 covers the related work; Section 3 describes the MSC extraction approach; Section 4 describes the experiments and discusses one of the use-cases in detail; Section 5 concludes the paper. 2 Related Work Feijs (2000) studies the relationship between natural language use cases and message sequence charts. He uses context-free generative grammars for natural language description of use-cases that use object-oriented system development methods. However, this study focuses on"
N19-2017,P18-2011,1,0.840609,"conjunctions and passive voice. Kof (2007a) also proposes a set of heuristics to handle missing sender or receiver of a message. A key limitation of this method is that it ignores syntactic as well as semantic relations between the verb and its corresponding sender or receiver. Hence it is possible that some of the <sender, message label, receiver> tuples may not be valid semantically. It is important to note that Kof (2007a; 2007b) do not focus on other features of MSC such as conditions, timers and altboxes. 3 Defining Actors in Software Engineering UseCases: In general (Bedi et al., 2017; Patil et al., 2018; Palshikar et al., 2019), the notion of actors in MSC is based on named entities of the types - PERSON, LOCATION and ORGANIZATION. However, in the software requirements domain, these may not be the only entities interacting with each other. This criteria is extended and is proposed to include: • PERSONS (Human SYSTEM users), ORGANIZATIONS and LOCATIONS • SYSTEMS (such as Supervisory System, Library System) • Persistent components of SYSTEMS (such as servers, databases, customer accounts) • Persistent processes in the SYSTEM (such as schedulers, daemons) • SYSTEM components (GUI elements like"
P09-2027,W04-1013,0,0.0138706,"Missing"
P09-2027,W03-0508,0,0.0733619,"Missing"
P18-2011,W97-1301,0,0.147112,"ate them using knowledge sources like Wikipedia. These in64 • Ea = {hu, vi : undirected edge, which connects nodes u and v which are headwords of aliases of the same participant }; e.g., hhim, Bonapartei Our approach has been summarized in Algorithm 1. Its input is an ULDG G(V, Ed , Ep , Ea ) for a set S of given sentences. We initialize V , Ed , Ep and Ea using any standard dependency parser, NER and coreference resolution techniques3 . volves resolution of bridging descriptions which study relationships between a definite description and its antecedent. As noted in (Vieira and Teufel, 1997; Poesio et al., 1997), bridging descriptions consider many different types of relationships between a definite description (definite generic NP) and its antecedent; e.g., synonymy, hyponymy, meronymy, events, compound nouns, etc. However, in this paper we focus on identity type of relationships only. Further, Vieira and Teufel (1997) use WordNet to identify these relationship types between definite descriptions. As described in Phase-I of algorithm 1 (Section 3), we use WordNet for a completely different purpose of identifying participant type.2 Gardent and Kow (2003) presented a corpus study of bridging definite"
P18-2011,W17-5912,1,0.427438,"An inIntroduction Identifying aliases of participants in a narrative is crucial for many NLP applications like timeline creation, question-answering, summarization, and information extraction. For instance, to answer a question (in the context of Table 1) When did Napoleon defeat the royalist rebels?, we need to identify Napoleon and the young lieutenant as aliases of Napoleon Bonaparte. Similarly, timeline for Napoleon Bonaparte will be inconsistent with the text, if the young lieutenant is not identified as an alias Napoleon Bonaparte. This will further affect any analysis of the timeline (Bedi et al., 2017). In the context of narrative analysis, we define – • A participant as an entity of type PERSON (PER), LOCATION (LOC), or ORGANIZATION (ORG). A participant has a canonical mention, ∗ When [he]A1 1 These authors contributed equally. NP with a common noun headword 63 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 63–68 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics dependent composite mention is created by recursively merging all its dependent mentions. For instance, for the phrases his par"
P18-2011,P14-2006,0,0.0571736,"Missing"
P18-2011,de-marneffe-etal-2014-universal,0,0.0313162,"Missing"
P18-2011,W03-2410,0,0.116214,"Missing"
P18-2011,D12-1114,0,0.0246045,"1. New Ea edges: hman, Bonapartei, hman, himi, & hman, Hisi are added. Newly added Ep edges are highlighted with thick, filled arrows. Participant types of man & school are changed to PER & ORG respectively; type of France is changed to OTH. stances of Predicate Schemas are then compiled into constraints in an Integer Linear Programming (ILP) based formulation to resolve coreferences. In addition to pronouns, our approach focuses on identification of common noun based aliases of a participant using MLN. MLN has been used to solve the problem of coreference resolution (Poon and Domingos, 2008; Song et al., 2012). Our work differs from them as we build upon output of off-the-shelf coreference resolution system, rather than identifying aliases/coreferences from scratch. This helps in exploiting the strengths (such as linking pronoun mentions to their antecedents) of the existing systems and overcome the weaknesses (such as resolving generic NP mentions) by incorporating additional linguistic knowledge. A more general and challenging problem inRelated Work Traditionally, alias detection restricts the focus on aliases of named entities which occur as proper nouns (Sapena et al., 2007; Hsiung et al., 2005"
P18-2011,P97-1072,0,0.327682,"dicate level and instantiate them using knowledge sources like Wikipedia. These in64 • Ea = {hu, vi : undirected edge, which connects nodes u and v which are headwords of aliases of the same participant }; e.g., hhim, Bonapartei Our approach has been summarized in Algorithm 1. Its input is an ULDG G(V, Ed , Ep , Ea ) for a set S of given sentences. We initialize V , Ed , Ep and Ea using any standard dependency parser, NER and coreference resolution techniques3 . volves resolution of bridging descriptions which study relationships between a definite description and its antecedent. As noted in (Vieira and Teufel, 1997; Poesio et al., 1997), bridging descriptions consider many different types of relationships between a definite description (definite generic NP) and its antecedent; e.g., synonymy, hyponymy, meronymy, events, compound nouns, etc. However, in this paper we focus on identity type of relationships only. Further, Vieira and Teufel (1997) use WordNet to identify these relationship types between definite descriptions. As described in Phase-I of algorithm 1 (Section 3), we use WordNet for a completely different purpose of identifying participant type.2 Gardent and Kow (2003) presented a corpus study"
P18-2011,M95-1005,0,0.574618,"oreference resolution system based on (Peng et al., 2015a,b). M is our proposed alias detection approach (Algorithm 1). Evaluation: The performance of all the approaches is evaluated at two levels: all independent participant mentions (i.e., participant detection) and their links with canonical mentions (i.e., participant linking). We use the standard F1 metric to measure performance of participant detection. For participant linking, we evaluate (Pradhan et al., 2014) the combined performance of participant mention identification and alias detection using the standard evaluation metrics, MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe) (Luo, 2005) and their average. Results: Results of the quantitative evaluation are summarized in Table 3. We observe that the proposed approach outperforms other baselines on all datasets. Dataset & Approach B1 ACEnw B2 M B1 Nap B2 M B1 BoH B2 M B1 Fas B2 M B1 Mao B2 M Participant mentions 53.1 62.9 70.2 60.5 73.9 86.4 61.7 65.6 73.5 56.8 61.6 70.3 60.1 49.1 78.9 posed approach succeeds as it exploits MLN rule CopulaConnect(x, y) ⇒ Alias(x, y). As an illustration of the proposed approach, Table 4 shows the participant mentions and the"
P18-2011,H05-1004,0,0.0685315,"lias detection approach (Algorithm 1). Evaluation: The performance of all the approaches is evaluated at two levels: all independent participant mentions (i.e., participant detection) and their links with canonical mentions (i.e., participant linking). We use the standard F1 metric to measure performance of participant detection. For participant linking, we evaluate (Pradhan et al., 2014) the combined performance of participant mention identification and alias detection using the standard evaluation metrics, MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), Entity-based CEAF (CEAFe) (Luo, 2005) and their average. Results: Results of the quantitative evaluation are summarized in Table 3. We observe that the proposed approach outperforms other baselines on all datasets. Dataset & Approach B1 ACEnw B2 M B1 Nap B2 M B1 BoH B2 M B1 Fas B2 M B1 Mao B2 M Participant mentions 53.1 62.9 70.2 60.5 73.9 86.4 61.7 65.6 73.5 56.8 61.6 70.3 60.1 49.1 78.9 posed approach succeeds as it exploits MLN rule CopulaConnect(x, y) ⇒ Alias(x, y). As an illustration of the proposed approach, Table 4 shows the participant mentions and their corresponding canonical mentions for the example text in Table 1. Se"
P18-2011,P14-5010,0,0.00694833,"OT H Drop from Ep all outgoing edges from y foreach clique c in subgraph (V, Ea ) ⊂ G do foreach n ∈ c.nodes do n.a := earliest participant mention in c.nodes Algorithm 1: identif y participants & aliases Our algorithm modifies the input ULDG inplace by updating node labels, Ep and Ea . Figure 1 shows an example of initialized input ULDG, which gets transformed by our algorithm to the output ULDG shown in Figure 2. Phase-I: In this phase, we update participant type 2 Further details are available in Figure A.1 and Table A.2 in the supplementary material. 3 65 We use Stanford CoreNLP Toolkit (Manning et al., 2014) Predicates N ET ype(x, y) CopulaConnect(x, y) Description y is entity type of participant x Participants x and y are connected through a copula verb or a “copula-like” verb in Ed (e.g., become) Conj(x, y) Participants x and y are connected by a conjunction in Ed Dif f V erbConnect(x, y) Participants x and y are connected through a “differentiating” verb or a copula-like verb in Ed (e.g. tell) LexSim(x, y) Participants x and y are lexically similar, i.e. having low edit distance Alias(x, y) Participants x and y are aliases of each other (used as a query predicate) Hard rules Description Alias("
P18-2011,L16-1695,0,0.0170009,"Table 2: MLN Predicates and Rules of headword h of a generic NP if its WordNet hypernyms contain PER/ORG/LOC indicating synsets. We also add new Ep edges from h to dependent nodes of h using dependency relations compound, amod or det (de Marneffe et al., 2014) to get corresponding mention boundaries. The function resolveP articipantT ypeConf lict() ensures that participant types of all nodes in a single clique in Ea are same by giving higher priority to NER-induced type than WordNet-induced type. Phase-II: In this phase, we encode linguistic rules in MLN to add new Ea edges. As elaborated by Mojica and Ng (2016), MLN gives the benefits of (i) ability to employ soft constraints, (ii) compact representation, and (iii) ease of specification of domain knowledge. The predicates and key first-order logic rules are described in Table 2. Here, Alias(x, y) is the only query predicate. Others are evidence predicates, whose observed groundings are specified using G. As we use a combination of hard rules (i.e., rules with infinite weight) and soft rules (i.e., rules with finite weights), probabilistic inference in MLN is necessary to get find most likely groundings of the predicate-Alias(x, y). As the goal is to"
P18-2011,K15-1002,0,0.0364984,"Missing"
P18-2011,N15-1082,0,0.0378355,"Missing"
R09-1019,W01-1201,0,0.0134553,"REtrieval Conference, http://trec.nist.gov 99 International Conference RANLP 2009 - Borovets, Bulgaria, pages 99–102 Where v is a term in the vocabulary V and UA (v), UR (v), UN (v) are the unigram probabilities of v in the passage, relevant and non-relevant text respectively. With the increase in the divergence between passage and relevant text, the probability of passage being relevant decreases. So, the prior probabilities are estimated as follows: p(A|R) = 1 1 + D(UA ||UR ) p(A|N ) = 1 1 + D(UA ||UN ) As KL divergence is always non-negative, both p(A|R) and p(A|N ) always lie in the range [0, 1]. This satisfies the basic law of probability i.e., the probability of an event should always lie in the range [0, 1]. p(A|R) = 1 when UA = UR , as the divergence of two equivalent distributions is zero. Similarly, p(A|N ) = 1 when UA = UN . Substituting the above estimates for prior probabilities in equation 1 gives the final ranking ranking function for passage retrieval. log rank(A) = log p(Q|A, R) − log 3 1 + D(UA ||UR ) 1 + D(UA ||UN ) Identifying relevant and nonrelevant text In the previous section we have assumed that the relevant and non-relevant text for a given question is known. He"
R09-1019,P03-1054,0,0.00419747,"6 test set [4]. We observed that on an average about 6 snippets out of top 10 snippets provided by the search engine are relevant to the question. As the quality of snippets is considerably high, we use them as relevant text to a given question. 3.2 Non-relevant text The methodology for extracting non-relevant text is independent of the size of a text collection unlike the methodology for relevant text. Here the structure of a question is used to extract the required information. An input question is parsed to get POS tags corresponding to all the terms in it. We have used the stanford parser [6] [7] to get POS tag sequence corresponding to a question. Based on POS tags, all keywords in a question are splitted into two sets: Topic and Keyword. Topic: Typically, questions ask for a specific information within a broad topic. For example, the question “Which position did Warren Moon play in professional football?”, asks for a specific information regarding “Warren Moon”. A topic can be a person, location, organization, event or any other entity, which are proper nouns. So, a topic set consists of all the proper nouns within a question. And, in questions where there are no proper nouns li"
R09-1019,H05-1086,0,0.0290407,"nts should be ranked using the following equation: log rank(D) = log p(Q|D, R) + log p(D|R) p(D|N ) (1) Here the first term p(Q|D, R) measures the likelihood of the query given a document that is relevant and Language Modeling is being used to estimate this value. The second term measures the prior probabilities of document being relevant, and non relevant. But, document retrieval assumes that a document is independent of its relevance, and non-relevance. So, documents are just ranked based on Language Modeling i.e., the probability of a query being generated by a document. Previous works [9] [10] showed that the same approach is being used even for passage retrieval in the context of QA. Previously Jagadeesh et al. [5] used prior probabilities in Query-Based Multi-Document Summarization task. They defined an entropy based measure called Information Measure to capture the prior of a sentence. This information measure was computed using external information sources like the Web and Wikipedia. Their experimental results showed that prior probabilities are necessary for ranking sentences in the summarization task. We use a similar approach to exploit the use of prior probabilities for pas"
R09-1020,W08-1805,0,0.0892398,"Missing"
R09-1020,U06-1013,0,0.024573,"e categories and the links between articles signify a semantic relationship between source and target articles. 2 Related Work Different query expansion methods have been studied to enhance the performance of passage retrieval in the context of QA. Monz [6] tested a blind relevance feedback technique which selects terms based on standard Rocchio term weighting from top N documents. His experiments in the context of QA, showed a reduction in the performance compared to original query’s performance. On the other hand, the same technique was found effective for the ad-hoc retrieval task. Pizzato [7] employed a blind relevance feedback technique which uses the named entities of the relevant answer type from the top ranked documents to form an expanded query. His experiments on PERSON type factoid questions have not shown a considerable improvement. Yang et al. [10] used WordNet and Web to expand queries for QA. Only marginal improvements were attained when Web was used to extract expansion terms and when WordNet was used to rank these extracted terms the improvement was reduced. On semantic 103 International Conference RANLP 2009 - Borovets, Bulgaria, pages 103–106 grouping the candidate"
S13-2087,baccianella-etal-2010-sentiwordnet,0,0.0568223,"Missing"
S13-2087,C10-2005,0,0.0720111,"Missing"
S13-2087,P11-2008,0,0.0755415,"Missing"
S13-2087,C04-1200,0,0.0130794,"Missing"
S13-2087,pak-paroubek-2010-twitter,0,0.1152,"Missing"
S13-2087,P04-1035,0,0.00732574,"Missing"
S13-2087,H05-1044,0,0.195748,"Missing"
S13-2087,S13-2052,0,\N,Missing
S15-2087,E09-1004,0,0.0231485,"d Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 201"
S15-2087,W11-0705,0,0.0535961,"Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 • Emoticon Dictionary: We use the emoticons list 2 and manually annotate the related sentiment. We categorize the emoticons into four classes as follows: (1) Extremely- Positive, (2) Positive, (3) Extremely- Negative, and (4) Negative. • Acronym Dictionary: We crawl the noslang.com website 3 in order to obtain the acronym expansion of the most com"
S15-2087,W12-3704,1,0.834629,"y in the model formation. Also, stop words do not carry any sentiment information and thus are of no use. • Handle Negative Mentions Negation plays a very important role in determining the sentiment of the tweet. Tweets consist of various notions of negation. Words which are either ‘no’, ‘not’ or ending with ‘n’t’ are replaced by a common word indicating negation. • Expand Acronyms As described in Section 3 we use an acronym expansion list. In the pre-processing step we expand the acronyms if they are present in the tweet. 4.2 Baseline Model We first generate a baseline model as discussed in (Bakliwal et al., 2012). We perform the preprocessing steps listed in Section 4.1 and learn the positive, negative and neutral frequencies of unigrams, bigrams and trigrams in our training data. Every token is given three probability scores: Positive Probability (Pp ), Negative Probability (Np ) and Neutral Probability (N Ep ). Given a token, let Pf denote the frequency in positive training set, Nf denote the frequency in negative training set and N Ef denote the frequency in neutral training set. The probability scores are then computed as follows. Pp = Pf Pf + Nf + N Ef (1) Np = Nf Pf + Nf + N Ef (2) N Ep = N Ef P"
S15-2087,esuli-sebastiani-2006-sentiwordnet,0,0.0295606,"listed in Table 2 for our experiments. There are a total of 34 features. We calculate these features for the whole tweet in case of message based sentiment analysis. We can divide the features into two classes: a) Tweet Based Features, and b) Lexicon Based Features. Table 2 summarizes the features used in our experiment. Here features f1 − f22 are tweet based features while features f23 − f34 are lexicon based features. A number of our features are based on prior polarity score of the tweet. For obtaining the prior polarity of words, we use AFINN dictionary 4 and extend it using SENTIWORDNET (Esuli and Sebastiani, 2006). We first look up the tokens in the tweet in the AFINN lexicon. This dictionary of about 2490 English language words assigns every word a pleasantness score between -5 (Negative) and +5 (Posi4 http://www2.imm.dtu.dk/pubdb/views/ publication_details.php?id=6010 Table 2: Description of the Features used in the Model. Feature Description Prior Polarity Score of the Tweet Brown Clusters Percentage of Capitalised Words # of Positive Capitalised Words # of Negative Capitalised Words Presence of Capitalised Words # of Positive Hashtags # of Negative Hashtags # of Positive Emoticons # of Extremely Po"
S15-2087,P11-2008,0,0.12169,"Missing"
S15-2087,S13-2053,0,0.0207922,"training and the dev-test datasets, while the organizers provided us with 9684 and 1654 tweet IDs respectively, we were able to retrieve only 7966 and 1368 tweets, respectively. 3.2 Sentiment Lexicons It has been found that lexicons play an important role in determining the polarity of a message. Several lexicons have been proposed in the past which are used popularly in the field of sentiment analysis. We use the following lexicons to generate our lexicon based features: (1) Bing Liu’s Opinion Lexicon1 , (2) MPQA Subjectivity Lexicon (Wilson et al., 2005), (3) NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), and (4) Sentiment140 Lexicon (Mohammad et al., 2013). 3.3 Dictionary Besides the above sentiment lexicons, we used two other dictionaries described as follows. 1 http://www.cs.uic.edu/˜liub/FBS/ opinion-lexicon-English.rar 521 Other than this we also use Tweet NLP (Owoputi et al., 2013), a Twitter specific tweet tokenizer and tagger which provides a fast and robust Java-based tokenizer and part-of-speech tagger for Twitter. 4 System Overview Figure 1 gives a brief overview of our system. In the offline stage, the system takes the tweet IDs and the N-Gram model as inputs (shown in red) to lea"
S15-2087,S13-2052,0,0.0376434,"wal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 • Emoticon Dictionary: We use the emoticons list 2 and manually annotate the related sentiment. We categorize the emoticons into four classes as follows: (1) Extremely- Positive, (2) Positive, (3) Extremely- Negative, and (4) Negative. • Acronym Dictionary: We crawl the noslang.com website 3 in order to obtain the acronym expansion of the most commonly used acronyms on the web. The acronym dictionary helps in expanding the tweet text and thereby improves the overall sentiment score. The acronym dictionary has 5297 entries. For example, asap has the translation As soon as possible. Resource"
S15-2087,N13-1039,0,0.0667497,"Missing"
S15-2087,P04-1035,0,0.00662084,"ources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experimental results and the ranking of our system for different datasets in Section 5. The paper is summarized in Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative cl"
S15-2087,S14-2009,0,0.0198683,"ilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 • Emoticon Dictionary: We use the emoticons list 2 and manually annotate the related sentiment. We categorize the emoticons into four classes as follows: (1) Extremely- Positive, (2) Positive, (3) Extremely- Negative, and (4) Negative. • Acronym Dictionary: We crawl the noslang.com website 3 in order to obtain the acronym expansion of the most commonly used acronyms on the web. The acronym dictionary helps in expanding the tweet text and thereby improves the overall sentiment score. The acronym dictionary has 5297 entries. For example, asap has the translation As soon"
S15-2087,S15-2078,0,0.0154292,"sidents. Therefore, it is possible to collect text posts of users from different social and interest groups. However, analyzing Twitter data comes with its own bag of difficulties. Tweets are small in length, thus ambiguous. The informal style of writing, a distinct usage of orthography, acronymization and a different set of elements like hashtags, user mentions demand a different approach to solve this problem. In this work we present the description of the supervised machine learning system developed while participating in the shared task of message based sentiment analysis in SemEval 2015 (Rosenthal et al., 2015). The system takes as input a tweet message, pre-processes it, extracts features and finally classifies it as either positive, negative or neutral. Tweets in the positive and negative classes are subjective in nature. However, the neutral class consists of both subjective tweets which do not have any polarity as well as objective tweets. Our paper is organized as follows. We discuss related work in Section 2. In Section 3, we discuss the existing resources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experiment"
S15-2087,H05-1044,0,0.0732019,"of all the tweets IDs provided by the organizers. For the training and the dev-test datasets, while the organizers provided us with 9684 and 1654 tweet IDs respectively, we were able to retrieve only 7966 and 1368 tweets, respectively. 3.2 Sentiment Lexicons It has been found that lexicons play an important role in determining the polarity of a message. Several lexicons have been proposed in the past which are used popularly in the field of sentiment analysis. We use the following lexicons to generate our lexicon based features: (1) Bing Liu’s Opinion Lexicon1 , (2) MPQA Subjectivity Lexicon (Wilson et al., 2005), (3) NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), and (4) Sentiment140 Lexicon (Mohammad et al., 2013). 3.3 Dictionary Besides the above sentiment lexicons, we used two other dictionaries described as follows. 1 http://www.cs.uic.edu/˜liub/FBS/ opinion-lexicon-English.rar 521 Other than this we also use Tweet NLP (Owoputi et al., 2013), a Twitter specific tweet tokenizer and tagger which provides a fast and robust Java-based tokenizer and part-of-speech tagger for Twitter. 4 System Overview Figure 1 gives a brief overview of our system. In the offline stage, the system takes the twe"
S15-2087,J09-3003,0,0.0457662,"n Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al.,"
S15-2098,W11-0705,0,0.0287093,"linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further"
S15-2098,baccianella-etal-2010-sentiwordnet,0,0.0902886,"Missing"
S15-2098,W12-3704,1,0.854817,"ed Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features"
S15-2098,P05-1045,0,0.0235427,"Missing"
S15-2098,S14-2076,0,0.0139472,"e SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of e"
S15-2098,S13-2053,0,0.0201563,"numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 201"
S15-2098,S13-2052,0,0.0312022,"ervices can gauge the public sentiment towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗ The first two authors made equal contribution to this work Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple"
S15-2098,pak-paroubek-2010-twitter,0,0.0600791,"a data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗ The first two authors made equal contribution to this work Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes b"
S15-2098,S14-2004,0,0.0805018,"Missing"
S15-2098,S14-2009,0,0.0154444,"nt towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being user-generated, the data is noisy; there are misspellings, unreliable capitalization, widespread use ∗ The first two authors made equal contribution to this work Related Work SemEval 2013 (Nakov et al., 2013) and 2014 tasks (Rosenthal et al., 2014) on Sentiment Analysis in Twitter not only contributed to this field by making huge amounts of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses pr"
S15-2098,S15-2078,0,0.0148721,"Eval too and attracted an unprecedented number of participations. This task comprises of four sub-tasks. We participated in subtask 2 — Message polarity classification. Although we lie a few notches down from the top system, we present a very simple yet effective approach to handle this problem that can be implemented in a single day! 1 2 Introduction Social media not only acts as a proxy for the real world society, it also offers a treasure trove of data for different types of analyses like Trend Analysis, Event Detection and Sentiment Analysis, to name a few. SemEval 2015 Task 10 subtask B (Rosenthal et al., 2015) specifically deals with the task of Sentiment Analysis in Twitter. Sentiment Analysis in social media in general and Twitter in particular has a wide range of applications — Companies/services can gauge the public sentiment towards the new product or service they launched, political parties can estimate their chances of winning the upcoming elections by monitoring what people are saying on Twitter about them, and so on. In spite of the availability of huge amount of data and the huge promises they entail, working with social media data is far more challenging than regular text data. Being use"
S15-2098,D13-1170,0,0.00367562,"features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics kernel to obviate the need for tedious feature engineering. (Kouloumpis et al., 2011) evaluate the usefulness of existing lexical resources as well as features that capture information about the informal and creative language used in microblogging. Recent publication from (Socher et al., 2013) has further raised the bar for Sentiment Analysis in general, but it is not specifically designed to tackle tweets data. 3 3.1 Approach Preprocessing We acquire a list of acronyms and their expanded forms 1 . We use this list as a look-up table and replace all occurrences of acronyms in our data by their expanded forms. We normalize all numbers that find a place in our data by replacing them with the string ‘0’. We do not remove stop words because they often contribute heavily towards expressing sentiment/emotion. We do not also stem the words because stemming leads to the loss of the parts o"
S15-2098,H05-1044,0,0.165906,"of annotated datasets available for research, but also encouraged researchers to come up with better solutions for this challenging problem. There has been numerous initiatives outside SemEval too. (Pak and Paroubek, 2010) is one of the early attempts at using Twitter as a corpus for Sentiment Analysis, which shows how to automatically collect a corpus for the same and performs linguistic analysis of the collected corpus. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. (Wilson et al., 2005) demontrates an efficient technique for automatically identifying the contextual polarity for a large subset of sentiment expressions. (Mohammad et al., 2013) and (Kiritchenko et al., 2014) establishes benchmark in Sentiment Analyis in Twitter as well as in the field of Aspect Based Sentiment Analysis by incorporating various innovative linguistic features. (Agarwal et al., 2011) introduced POS-specific prior polarity features and (Kouloumpis et al., 2011) explored the use of a tree 590 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 590–594, c Denver"
S15-2098,P06-4018,0,\N,Missing
S15-2129,W12-3704,1,0.861206,"Missing"
S15-2129,S14-2149,0,0.0529627,"Missing"
S15-2129,S14-2145,0,0.0428515,"Missing"
S15-2129,S14-2135,0,0.0386611,"Missing"
S15-2129,P05-1045,0,0.0133495,"Missing"
S15-2129,D10-1101,0,0.102655,"Missing"
S15-2129,S14-2076,0,0.0461655,"Missing"
S15-2129,W02-0109,0,0.0947969,"Missing"
S15-2129,S13-2053,0,0.0588788,"Missing"
S15-2129,P04-1035,0,0.0418711,"Missing"
S15-2129,S14-2004,0,0.0462049,"Missing"
S15-2129,S15-2082,0,0.0548958,"Missing"
S15-2129,S14-2009,0,0.0478206,"Missing"
S15-2129,D13-1170,0,0.0121678,"Missing"
S15-2129,S14-2038,0,0.0429146,"Missing"
S15-2129,S14-2036,0,0.0380869,"Missing"
S15-2129,H05-1044,0,0.119918,"Missing"
S15-2129,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-2129,P06-4018,0,\N,Missing
S19-2009,W17-3007,0,0.0228899,"Missing"
S19-2009,S19-2007,0,0.0867251,"considers the sub-word information. • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. 4 Dataset The data collection methods used to compile the dataset used in HatEval is described in (Basile et al., 2019). We did not use any external datasets to augment the data for training our models. 5 Results and Analysis The official test set results scored on CodaLab have been presented below in Table 2. • Deep Contextualized Word Representations (ELMo) (Peters et al., 2018) use language models to get the embeddings for individual words. The entire sentence or paragraph is taken into consideration while calculating these embedding representations. ELMo uses a pre-trained bi-directional LSTM language model. For the input supplied, the ELMo architecture extracts the hidden state of each layer. A weighted s"
S19-2009,W17-3013,0,0.0617531,"Missing"
S19-2009,W18-4401,0,0.0370188,"Missing"
S19-2009,malmasi-zampieri-2017-detecting,0,0.120451,"chine learning classifiers for detection (Dinakar et al., 2011; Waseem and Hovy, 2016; Nobata et al., 2016). Deep learning methods for hate speech detection were used by Badjatiya et al. (2017) wherein the authors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and"
S19-2009,D14-1162,0,0.0955796,"ds. 3.1 Word Embeddings Word embeddings have been widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While w"
S19-2009,N18-1202,0,0.250349,"widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for"
S19-2009,W18-1209,0,0.0219416,"task organizers. Using each of the sentence embeddings we have mentioned above, we seek to evaluate how each of them performs when the vector representations are supplied for classification with various off-theshelf machine learning algorithms. For each of the evaluation tasks, we perform experiments using each of the sentence embeddings mentioned above and show our classification performance on the dev set given by the task organizers. considerably closes the gap to state-of-theart methods mono-lingually and substantially outperforms many complex techniques crosslingually. • Lexical Vectors (Salle and Villavicencio, 2018) is another word embedding similar to fastText with slightly modified objective. FastText (Bojanowski et al., 2016) is another word embedding model which incorporates character n-grams into the skipgram model of Word2Vec and considers the sub-word information. • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of"
S19-2009,W17-1101,0,0.0324117,"e early works related to hate speech detection employed the use of features like bag of words, word and character n-grams with relatively off-the-shelf machine learning classifiers for detection (Dinakar et al., 2011; Waseem and Hovy, 2016; Nobata et al., 2016). Deep learning methods for hate speech detection were used by Badjatiya et al. (2017) wherein the authors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlig"
S19-2009,W17-3003,0,0.0165797,"tures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Introduction Microblogging platforms like Twitter provide channels to exchange ideas using short messages called tweets. While such a platform can be used for constructive ideas, a small group of peopl"
S19-2009,W17-3012,0,0.0450659,"hors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Introduction Microblogging platforms like Twitter provide channels to exchange ideas using short messages called tweets. While such a"
S19-2009,N16-2013,0,0.0529118,"sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task. 1 2 In this section we briefly describe other work in this area. A few of the early works related to hate speech detection employed the use of features like bag of words, word and character n-grams with relatively off-the-shelf machine learning classifiers for detection (Dinakar et al., 2011; Waseem and Hovy, 2016; Nobata et al., 2016). Deep learning methods for hate speech detection were used by Badjatiya et al. (2017) wherein the authors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al.,"
S19-2109,W18-4401,0,0.0553136,"Missing"
S19-2109,malmasi-zampieri-2017-detecting,0,0.0316093,"y which can be misused to target offensive comments to targeted parties. Users may engage in generating offensive content on social media which may show aggressive behaviour and may also include hate speech. As a result, it is imperative for social media platforms to invest heavily in creating solutions which can identify offensive language and to prevent such behaviour on social media. Using computational methods to identify offense, aggression and hate speech in user generated content has been gaining attention in the recent years as evidenced in (Waseem et al., 2017; Davidson et al., 2017; Malmasi and Zampieri, 2017; Kumar et al., 2018) and workshops such as Abusive Language Workshop (ALW) 1 and Work1 Related Work 3 Methodology 3.1 Word Embeddings Word embeddings have been widely used in modern Natural Language Processing applications as 2 https://sites.google.com/view/trac1 http://ta-cos.org/ 4 https://sites.google.com/site/alw2018 5 https://sites.google.com/view/trac1 3 https://sites.google.com/view/alw2018 611 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 611–616 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics s"
S19-2109,D18-2029,0,0.0233732,"ing techniques. A common approach of obtaining a sentence representation using word embeddings is by the simple and na¨ıve way of using the simple arithmetic mean of all the embeddings of the words present in the sentence. Smooth inverse frequency, which uses weighted averages and modifies it using Singular Value Decomposition (SVD), has been a strong contender as a baseline over traditional averaging technique (Arora et al., 2016). Other sentence embedding techniques include pmeans (R¨uckl´e et al., 2018), InferSent (Conneau et al., 2017), SkipThought (Kiros et al., 2015), Universal Encoder (Cer et al., 2018). We formulate each of the sub-tasks of OffensEval as a text classification task. In this paper, we evaluate various pre-trained sentence embeddings for identifying the offense, hate and aggres• The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length"
S19-2109,D14-1162,0,0.107946,"ub-tasks as defined in this task. In the following, we discuss various popular sentence embedding methods in brief. they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 • InferSent (Conneau et al."
S19-2109,N18-1202,0,0.379919,", we discuss various popular sentence embedding methods in brief. they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 • InferSent (Conneau et al., 2017) is a set of embeddings proposed by Fac"
S19-2109,D17-1070,0,0.146605,"et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 • InferSent (Conneau et al., 2017) is a set of embeddings proposed by Facebook. InferSent embeddings have been trained using the popular language inference corpus. Given two sentences the model is trained to infer whether they are a contradiction, a neutral pairing, or an entailment. The output is an embedding of 4096 dimensions. • Concatenated Power Mean Word Embedding (R¨uckl´e et al., 2018) generalizes the concept of average word embeddings to power mean word embeddings. The concatenation of different types of power mean word embeddings considerably closes the gap to state-of-theart methods mono-lingually and substantially"
S19-2109,W18-1209,0,0.0222762,"the popular language inference corpus. Given two sentences the model is trained to infer whether they are a contradiction, a neutral pairing, or an entailment. The output is an embedding of 4096 dimensions. • Concatenated Power Mean Word Embedding (R¨uckl´e et al., 2018) generalizes the concept of average word embeddings to power mean word embeddings. The concatenation of different types of power mean word embeddings considerably closes the gap to state-of-theart methods mono-lingually and substantially outperforms many complex techniques crosslingually. Sentence Embeddings • Lexical Vectors (Salle and Villavicencio, 2018) is another word embedding similar to fastText with slightly modified objective. FastText (Bojanowski et al., 2016) is another word embedding model which incorporates character n-grams into the skipgram model of Word2Vec and considers the sub-word information. While word embeddings can produce representations for words which can capture the linguistic properties and the semantics of the words, the idea of representing sentences as vectors is an important and open research problem (Conneau et al., 2017). Finding a universal representation of a sentence which works with a variety of downstream t"
S19-2109,W17-1101,0,0.0647868,"Missing"
S19-2109,W17-3007,0,0.0251414,"Missing"
S19-2109,W17-3003,0,0.0303558,"al leaderboard. 2 In this section we briefly describe other work in this area. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018; Badjatiya et al., 2017). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity vs. hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Online Safety (TA-COS) 3 , Abusive Language Workshop 4 , and TRAC 5 . Related shared tasks include GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018). Through the paper we provide a detailed de"
S19-2109,W17-3012,0,0.0544584,"uses SVM (RBF kernel) for training, scored third position on the official leaderboard. 2 In this section we briefly describe other work in this area. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018; Badjatiya et al., 2017). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity vs. hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Online Safety (TA-COS) 3 , Abusive Language Workshop 4 , and TRAC 5 . Related shared tasks include GermEval (Wiegand et al., 2018) and TR"
S19-2109,W17-3013,0,0.0399184,"Missing"
S19-2109,N19-1144,0,0.0419401,"f each layer. A weighted sum is computed of the hidden states to obtain an embedding for each sentence. Using each of the sentence embeddings we have mentioned above, we seek to evaluate how each of them performs when the vector representations are supplied for classification with various off-theshelf machine learning algorithms. For each of the evaluation tasks, we perform experiments using each of the sentence embeddings mentioned above and show our classification performance on the dev set given by the task organizers. 4 Figure 1: Distribution of label combinations in the data (taken from (Zampieri et al., 2019)) posts as offensive (OFF) vs not (NOT). Subtask B (Categorization of Offensive Language) deals with categorization of offense as: targeted (TIN) and untargeted (INT). Sub-task C (Offensive Language Target Identification) categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH). The overall dataset across the three sub-tasks consists of 14100 posts. Fig. 1 (reproduced from (Zampieri et al., 2019)) shows dataset size details. Dataset The data collection methods used to compile the dataset used in OffensEval is described in (Zampieri et al., 2019). Sub-tas"
S19-2203,S16-1130,0,0.0294083,"is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic"
S19-2203,D18-2029,0,0.0348251,"the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for words which can capture the linguistic properties and the semantics of the words, the idea of representing sentences as vectors is an important and open research problem (Conneau et al., 2017). Finding a universal representation of a sentence which works with a variety of downstream tasks 1161 • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. is the major goal of many sentence embedding techniques. A common approach of obtaining a sentence representation using word embeddings is by the simple and na¨ıve way of using the simple"
S19-2203,D17-1070,0,0.123195,"d embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for words which can capture the linguistic properties and the semantics of the words, the idea of representing sentences as vectors is an important and open research problem (Conneau et al., 2017). Finding a universal representation of a sentence which works with a variety of downstream tasks 1161 • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. is the major goal of many sentenc"
S19-2203,S17-2045,0,0.0168754,"ed (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features such as Bag of Words (BoW) (FrancoSalvador et al., 2016); Bag of vectors (BoV) (Mohtarami et al., 2016); Lexical features (for example, Cosine"
S19-2203,S16-1126,0,0.0313487,"uestions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features such as Bag of Words (BoW) (FrancoSalvador et al., 2016); Bag of vectors (BoV) (Mohtarami et al., 2016); Lexical features (for example, Cosine Similarity, Word Overlap, Noun Overlap, N-gram Overlap, Longest Common Substring/Subsequence, Keyword and Named Entity features etc.) (Franco-Salvador et al., 2016; Mohtarami et al., 2016; Nandi et al., 2017); Semantic features (for example, Distributed representations of text, Knowledge Graphs, Distributed word alignments, Word Cluster Similarity, etc.) (Franco-Salvador et al., 2016); Word Embedding Features (like Word2vec, GloVe etc.) (Wang and Poupart, 2016; Mohtarami et al., 2016; Nandi et al., 2017); and Metadata-based features (Mohtarami et al., 2016; Mihaylova et al., 2016; Xie Methodology and Data The data supplied by organizers is used for the task at hand. Specifically, for sub-task A, the subject and body for each question is provided by the"
S19-2203,S19-2149,0,0.0683836,"Missing"
S19-2203,S16-1128,0,0.176937,"erform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al.,"
S19-2203,S17-2009,0,0.0878099,"shelf machine learning algorithms for the multi-class prediction problem. The approach is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016)"
S19-2203,D14-1162,0,0.0840945,"ly. 3.1 Word Embeddings Word embeddings have been widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While w"
S19-2203,S17-2055,0,0.0640604,"Missing"
S19-2203,S16-1133,0,0.103328,"on problem. The approach is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and"
S19-2203,S17-2047,0,0.0230645,"ng algorithms for the multi-class prediction problem. The approach is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in t"
S19-2203,S17-2052,0,0.0310138,"like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features"
S19-2203,P11-1066,0,0.0355737,"perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features such as Bag of Words (BoW) (FrancoSalvador et al., 2016); Bag of vectors (BoV) (Mohtarami et al., 2016); Lexical features (for example, Cosine Similarity, Word Overlap, Noun Overlap, N-gram Overlap, Longest Common Substring/Subsequence, Keyword and Named Entity features etc.) (Franco-Salvador et al., 2016; Mohtarami et al., 2016; Nandi et al., 2017); Semantic features (for example, Distributed representations of text, Knowledge Graphs, Distributed"
S19-2203,N18-1202,0,0.0658627,"widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for"
S19-2203,W18-1209,0,0.0254185,"been trained using the popular language inference corpus. Given two sentences the model is trained to infer whether they are a contradiction, a neutral pairing, or an entailment. The output is an embedding of 4096 dimensions. • Concatenated Power Mean Word Embedding (R¨uckl´e et al., 2018) generalizes the concept of average word embeddings to power mean word embeddings. The concatenation of different types of power mean word embeddings considerably closes the gap to state-of-theart methods mono-lingually and substantially outperforms many complex techniques crosslingually. • Lexical Vectors (Salle and Villavicencio, 2018) is another word embedding similar to fastText with slightly modified objective. FastText (Bojanowski et al., 2016) is another word embedding model which incorporates character n-grams into the skipgram model of Word2Vec and considers the sub-word information. • Deep Contextualized Word Representations (ELMo) (Peters et al., 2018) use language models to get the embeddings for individual words. The entire sentence or paragraph is taken into consideration while calculating these embedding representations. ELMo uses a pre-trained bi-directional LSTM language model. For the input supplied, the ELM"
W09-1607,A97-1042,0,0.0324739,"ization based on the SPP in Section 3, and show evaluation results. Next, in Section 4, we explain the current baselines and evaluation for Multi-Document Summarization and finally in Section 5, we discuss the need for an older baseline in the current context of the short summary task of update summarization. 2 Sub-Optimal Sentence Position Policy Given a large text collection and a way to approximate the relevance for a reasonably large subset of sentences, we could identify significant positional attributes for the genre of the collection. Our experiments are based on the work described in (Lin and Hovy, 1997), whose experiments using the ZiffDavis corpus gave great insights on the selective power of the position method. 2.1 Sentence Position Yield and Optimal Position Policy (OPP) Lin and Hovy (1997) provide an empirical validation for the position hypothesis. They describe a method of deriving an Optimal Position Policy for a collection of texts within a genre, as long as a small set of topic keywords is defined for each text. They defined sentence yield (strength of relevance) of a sentence based on the mention of topic keywords in the sentence. The positional yield is defined as the average sen"
W09-1607,W04-1013,0,0.00517813,"We have applied it to Query-Focused Multi-Document Summarization (QF-MDS) task of DUC 2007 and QueryFocused Update Summarization task of TAC 2008. 3.1 Query-Focused Multi-Document Summarization The query-focused multi-document summarization task at DUC models the real world complex question answering task. Given a topic and a set of 25 relevant documents, this task is to synthesize a fluent, well-organized 250 word summary of the documents that answers the question(s) in the topic statement/narration. The summaries from the above algorithm for the QF-MDS were evaluated based on ROUGE metrics (Lin, 2004). The average4 recall scores are reported for ROUGE-2 and ROUGE-SU4 in Table 1. Also reported are the performance of the top performing system and the official baseline(s). This algorithm performed worse than most systems participating in the task that year and performed better5 than only the ‘first x words’ baseline and 3 other systems. system ‘first x words’ baseline ‘generic’ baseline SPP algorithm system 15 (top system) ROUGE-2 0.06039 0.09382 0.06913 0.12448 ROUGE-SU4 0.10507 0.14641 0.12492 0.17711 Table 1: ROUGE 2, SU4 Recall scores for two baselines, the SPP algorithm and a top perform"
W09-3507,P04-1021,0,0.463924,"ocess is repeated for all the five language pairs. 3. The alignment consist of N U LLs on source language i.e., a target language unigram is aligned to N U LL on the source language. These N U LLs are problematic during online phase (as positions of N U LLs are unknown). So, these N U LLs are removed by appending the target language unigram to the unigram of its previous alignment. For example, the following alignment, 6 Results In this section, we present the results of our participation in the NEWS-2009 shared task. We conducted our experiments on five language pairs namely English-Chinese (Li et al., 2004), English{Hindi, Kannada, Tamil, Russian} (Kumaran and Kellner , 2007). As specified in NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we submitted our standard runs on all the five language pairs. Table 1 shows the results of our system. k−K N U LL − A transforms to k − KA 2 Training Phase http://crfpp.sourceforge.net/ 42 Language Pair English-Tamil English-Hindi English-Russian English-Chinese English-Kannada Accuracy in top-1 0.406 0.407 0.548 0.493 0.350 Mean F-score 0.894 0.877 0.916 0.804 0.864 MRR 0.542 0.544 0.640 0.600 0.482 M APref 0.399 0.402 0.548 0.493 0.344 M AP"
W09-3507,P97-1017,0,0.149536,"Missing"
W09-3507,P00-1056,0,0.276555,"Missing"
W09-3507,J93-2003,0,0.033902,"ges are written in different scripts, these named entities must be transliterated. Transliteration is defined as the process of obtaining the phonetic translation of names across languages. A source language word can have more than one valid transliteration in the target language. In areas like Cross Language Information Retrieval (CLIR), it is important to generate all possible transliterations of a Named Entity. Most current transliteration systems use a generative model for transliteration such as freely available GIZA++1 (Och and Ney , 2000), an implementation of the IBM alignment models (Brown et al., 1993) and HMM alignment model. These systems use GIZA++ to get character level alignments from word aligned data. The 1 Previous work http://www.fjoch.com/GIZA++.html 40 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 40–43, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP evaluation of the transliterations in terms of their reasonableness according to human judges. None of these studies measures their performance on a retrieval task or on other NLP tasks. Fujii and Ishikawa (Fujii and Ishikawa , 2001) describe a transliteration system for English-Japanese CLIR that re"
W09-3507,P06-1009,0,0.053793,"Missing"
W09-3507,W98-1005,0,0.223435,"Missing"
W09-3507,W02-0505,0,\N,Missing
W09-3507,W09-3501,0,\N,Missing
W09-3507,W09-3502,0,\N,Missing
W11-1219,W04-2209,0,0.0854986,"ny pair of languages present in Wikipedia. The remainder of paper is organized as follows. Section 2 shows the related work. Proposed method is discussed in Section 3. Results and Discussion are in Section 4. We finally conclude in Section 5. 2 RELATED WORK We discuss the related work of the two stages are involved in our system of language-independent context aware query translation, • Resource building/ collection (Dictionaries in our case) • Query formation 146 Dictionary building can be broadly classified into two approaches, manual and automatic. At initial stages, various projects like (Breen, 2004) try to build dictionaries manually, taking lot of time and effort. Though manual approaches perform well, they lag behind when recent vocabulary is considered. To reduce the effort involved, automatic extraction of dictionaries has been envisioned. The approach followed by (Kay and Roscheisen, 1999) and (Brown et al., 1990) were towards statistical machine translation, that can also be applied to dictionary building. The major requirement for using statistical methods is the availability of bilingual parallel corpora, that again is limited for underresourced languages. Factors like sentence s"
W11-1219,J90-2002,0,0.695603,"our system of language-independent context aware query translation, • Resource building/ collection (Dictionaries in our case) • Query formation 146 Dictionary building can be broadly classified into two approaches, manual and automatic. At initial stages, various projects like (Breen, 2004) try to build dictionaries manually, taking lot of time and effort. Though manual approaches perform well, they lag behind when recent vocabulary is considered. To reduce the effort involved, automatic extraction of dictionaries has been envisioned. The approach followed by (Kay and Roscheisen, 1999) and (Brown et al., 1990) were towards statistical machine translation, that can also be applied to dictionary building. The major requirement for using statistical methods is the availability of bilingual parallel corpora, that again is limited for underresourced languages. Factors like sentence structure, grammatical differences, availability of language resources and the amount of parallel corpus available further hamper the recall and coverage of the dictionaries extracted. After parallel corpora, attempts have been made to construct bilingual dictionaries using various types of corpora like comparable corpus (Sad"
W11-1219,2010.eamt-1.10,0,0.0225201,"us (Fung and McKeown, 1997). Though there exist various approaches, most of them make use of the language resources. Wikipedia has also been used to mine dictionaries. (Tyers and Pienaar, 2008), (Erdmann et al., 2008), (Erdmann et al., 2009) have built bilingual dictionaries using Wikipedia and language resources. We have mined our dictionaries similarly considering the cross lingual links present. Our approach to dictionary building is detailed in section 3. Wikipedia has been used for CLIA at various stages including query formation. Most recently, Wikipedia structure has been exploited in (Gaillard et al., 2010) for query translation and disambiguation. In (Sch¨onhofen et al., 2008), Wikipedia has been exploited at all the stages of building a CLIA system. We tread the same path of (Sch¨onhofen et al., 2008) in harnessing Wikipedia for dictionary building and query formation. Similar to them we extract concept words for each Wikipedia article and use them to disambiguate and form the query. For evaluation purposes, we adapted evaluation measures based on Wikipedia and existing dictionaries (Bharadwaj and Varma, 2011). The authors have proposed a classification based technique, using Wikipedia article"
W11-1219,P03-2025,0,0.0283101,"90) were towards statistical machine translation, that can also be applied to dictionary building. The major requirement for using statistical methods is the availability of bilingual parallel corpora, that again is limited for underresourced languages. Factors like sentence structure, grammatical differences, availability of language resources and the amount of parallel corpus available further hamper the recall and coverage of the dictionaries extracted. After parallel corpora, attempts have been made to construct bilingual dictionaries using various types of corpora like comparable corpus (Sadat et al., 2003) and noisy parallel corpus (Fung and McKeown, 1997). Though there exist various approaches, most of them make use of the language resources. Wikipedia has also been used to mine dictionaries. (Tyers and Pienaar, 2008), (Erdmann et al., 2008), (Erdmann et al., 2009) have built bilingual dictionaries using Wikipedia and language resources. We have mined our dictionaries similarly considering the cross lingual links present. Our approach to dictionary building is detailed in section 3. Wikipedia has been used for CLIA at various stages including query formation. Most recently, Wikipedia structure"
W11-1219,W06-2810,0,\N,Missing
W11-1219,oard-1998-comparative,0,\N,Missing
W11-1219,N07-1025,0,\N,Missing
W11-1219,R09-1026,0,\N,Missing
W11-1219,N09-2024,0,\N,Missing
W11-1219,P08-1001,0,\N,Missing
W11-1219,P09-4008,0,\N,Missing
W11-1219,N10-1063,0,\N,Missing
W11-1219,J93-1006,0,\N,Missing
W11-1219,W10-4004,0,\N,Missing
W11-1219,N04-1034,0,\N,Missing
W11-1219,P09-2057,0,\N,Missing
W11-3715,P07-1056,0,0.138369,"Missing"
W11-3715,N07-1039,0,0.0306683,"Missing"
W11-3715,P97-1023,0,0.0828553,"Missing"
W11-3715,W02-1011,0,0.0348007,"Missing"
W11-3715,P04-1035,0,0.218348,"Missing"
W11-3715,P02-1053,0,0.0370665,"se Mutual Information) from the strong positive word “excellent” and also from the strong negative word “poor”, and the difference will give you the semantic orientation of the phrase. Dave et al.(Dave et al., 2003) devised their own scoring function which was probability based. They performed some lexical substitutions to negation handling and used rainbow classifiers to decide the class of the review. Our work is motivated from each of these works. Pang et al.(Pang et al., 2002) used POS information with unigram, we extended this work using POS information with bigrams and trigrams. Turney (Turney, 2002) also used POS1 information with trigrams but he restricted trigram formation with some rules. He used PMI to evaluate the classification and here in this research we propose a new scoring function to classify. Dave et al.(Dave et al., 2003) devised some rules for negation handling and thus motivated us to work on negation handling. fication various methodologies have been applied earlier. Following are Unsupervised approaches. 1. Syntactic approach towards sentiment classification using Ngrams. This approach was used by Pang et al.(Pang et al., 2002) in their work. 2. Semantic approach using"
W12-3704,W11-0705,0,0.148638,"itive and negative probability scores of all the constituent unigrams, and use their difference (positive - negative) to find the overall score of the tweet. If tweet score is &gt; 0 then it is positive otherwise negative. Pf = F requency in P ositive T raining Set Nf = F requency in N egative T raining Set Pp = P ositive P robability of the token. = Pf /(Pf + Nf ) Np = N egative P robability of the token. = Nf /(Pf + Nf ) (1) 3.2 Emoticons and Punctuations Handling We make slight changes in the pre-processing module for handling emoticons and punctuations. We use the emoticons list provided by (Agarwal et al., 2011) in their research. This list2 is built from wikipedia list of emoticons3 and is hand tagged into five classes (extremely positive, positive, neutral, negative and extremely negative). In this experiment, we replace all the emoticons which are tagged positive or extremely positive with ‘zzhappyzz’ and rest all other emoticons with ‘zzsadzz’. We append and prepend ‘zz’ to happy and sad in order to prevent them from mixing into tweet text. At the end, ‘zzhappyzz’ is scored +1 and ‘zzsadzz’ is scored -1. Exclamation marks (!) and question marks (?) also carry some sentiment. In general, ‘!’ is us"
W12-3704,C10-2028,0,0.122065,"Missing"
W12-3704,W02-1011,0,0.0272372,"per: Related work is discussed in Section 2. In Section 3, we describe our approach to address the problem of Twitter sentiment classification along with pre-processing steps.Datasets used in this research are discussed in Section 4. Experiments and Results are presented in Section 5. In Section 6, we present the feature vector approach to twitter sentiment classification. Section 7 presents as discussion on the methods and we conclude the paper with future work in Section 8. 2 Related Work Research in Sentiment Analysis of user generated content can be categorized into Reviews (Turney, 2002; Pang et al., 2002; Hu and Liu, 2004), Blogs (Draya et al., 2009; Chesley, 2006; He et al., 2008), 11 Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 11–18, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics News (Godbole et al., 2007), etc. All these categories deal with large text. On the other hand, Tweets are shorter length text and are difficult to analyse because of its unique language and structure. (Turney, 2002) worked on product reviews. Turney used adjectives and adverbs for performing opinion classificatio"
W12-3704,P02-1053,0,0.00403678,"rest of the paper: Related work is discussed in Section 2. In Section 3, we describe our approach to address the problem of Twitter sentiment classification along with pre-processing steps.Datasets used in this research are discussed in Section 4. Experiments and Results are presented in Section 5. In Section 6, we present the feature vector approach to twitter sentiment classification. Section 7 presents as discussion on the methods and we conclude the paper with future work in Section 8. 2 Related Work Research in Sentiment Analysis of user generated content can be categorized into Reviews (Turney, 2002; Pang et al., 2002; Hu and Liu, 2004), Blogs (Draya et al., 2009; Chesley, 2006; He et al., 2008), 11 Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 11–18, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics News (Godbole et al., 2007), etc. All these categories deal with large text. On the other hand, Tweets are shorter length text and are difficult to analyse because of its unique language and structure. (Turney, 2002) worked on product reviews. Turney used adjectives and adverbs for performing op"
W12-3902,D07-1074,0,0.319666,"ch concentrates on exploiting structured and semi-structured parts of Wikipedia and hence yielding better results. The approach used is simple, efficient, easily reproducible and can be extended to any language as it doesn’t use any of the language specific resources. 2 Related Work Wikipedia has been the subject of a considerable amount of research in recent years including Gabrilovich and Markovitch (2005), Milne et al. (2006), Zesch et al. (2007), Timothy Weale (2006) and Richman and Schone (2008). The most relevant work to this paper are Kazama and Torisawa (2007), Toral and Munoz (2006), Cucerzan (2007), Richman and Schone (2008). More details follow, however it is worth noting that all known prior research is fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resources. Toral and Munoz (2006) used Wikipedia to create lists of NE’s. They used the first sentence of Wikipedia articles as likely definitions of the article titles, and used them in attempting to classify the titles as people, locations, organizations, or none. Unlike the method presented in our paper, their algorithm relied on WordNet (or"
W12-3902,D07-1073,0,0.0208062,"tured parts which results in less recall. Our approach concentrates on exploiting structured and semi-structured parts of Wikipedia and hence yielding better results. The approach used is simple, efficient, easily reproducible and can be extended to any language as it doesn’t use any of the language specific resources. 2 Related Work Wikipedia has been the subject of a considerable amount of research in recent years including Gabrilovich and Markovitch (2005), Milne et al. (2006), Zesch et al. (2007), Timothy Weale (2006) and Richman and Schone (2008). The most relevant work to this paper are Kazama and Torisawa (2007), Toral and Munoz (2006), Cucerzan (2007), Richman and Schone (2008). More details follow, however it is worth noting that all known prior research is fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resources. Toral and Munoz (2006) used Wikipedia to create lists of NE’s. They used the first sentence of Wikipedia articles as likely definitions of the article titles, and used them in attempting to classify the titles as people, locations, organizations, or none. Unlike the method presented in our pape"
W12-3902,W06-2809,0,0.0206832,"less recall. Our approach concentrates on exploiting structured and semi-structured parts of Wikipedia and hence yielding better results. The approach used is simple, efficient, easily reproducible and can be extended to any language as it doesn’t use any of the language specific resources. 2 Related Work Wikipedia has been the subject of a considerable amount of research in recent years including Gabrilovich and Markovitch (2005), Milne et al. (2006), Zesch et al. (2007), Timothy Weale (2006) and Richman and Schone (2008). The most relevant work to this paper are Kazama and Torisawa (2007), Toral and Munoz (2006), Cucerzan (2007), Richman and Schone (2008). More details follow, however it is worth noting that all known prior research is fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resources. Toral and Munoz (2006) used Wikipedia to create lists of NE’s. They used the first sentence of Wikipedia articles as likely definitions of the article titles, and used them in attempting to classify the titles as people, locations, organizations, or none. Unlike the method presented in our paper, their algorithm relie"
W12-3902,P08-1001,0,0.146986,"exploit Wikipedia for recognizing NEs concentrates only on the structured parts which results in less recall. Our approach concentrates on exploiting structured and semi-structured parts of Wikipedia and hence yielding better results. The approach used is simple, efficient, easily reproducible and can be extended to any language as it doesn’t use any of the language specific resources. 2 Related Work Wikipedia has been the subject of a considerable amount of research in recent years including Gabrilovich and Markovitch (2005), Milne et al. (2006), Zesch et al. (2007), Timothy Weale (2006) and Richman and Schone (2008). The most relevant work to this paper are Kazama and Torisawa (2007), Toral and Munoz (2006), Cucerzan (2007), Richman and Schone (2008). More details follow, however it is worth noting that all known prior research is fundamentally monolingual, often developing algorithms that can be adapted to other languages pending availability of the appropriate semantic resources. Toral and Munoz (2006) used Wikipedia to create lists of NE’s. They used the first sentence of Wikipedia articles as likely definitions of the article titles, and used them in attempting to classify the titles as people, locat"
W12-3902,E06-1002,0,0.060451,"o the classical NE tags (PERSON, LOCATION, etc.), they used a noun phrase following the verb forms ’to be’ to derive a label. For example, they used the sentence ’Franz Fischler ... is an Austrian politician’ to associate the label ’politician’ to the surface form ’Franz Fischler’. They proceeded to show that the dictionaries generated by their method are useful when integrated into an NER system. It is to be noted that their technique relies upon a part-of-speech tagger. Cucerzan (2007), by contrast to the above, used Wikipedia primarily for Named Entity Disambiguation, following the path of Bunescu and Pasca (2006). As in our paper, and unlike the above mentioned works, Cucerzan (2007) made use of the explicit Category information found within Wikipedia. In particular, Category and related list derived data were key pieces of information used to differentiate between various meanings of an ambiguous surface form. Cucerzan (2007) did not make use of the Category information in identifying the class of a given entity. It is to be noted that the NER component was not the focus of their research, and was specific to the English language. Richman and Schone (2008) emphasized on the use of links between artic"
W12-3902,I08-5005,0,0.0281224,"inks. The output of Hierarchical GAAC clustering on this subset was observed to be 345 clusters. We have manually tagged Hindi articles of 50 random clusters (as cluster size can dictate accuracies) with three NE tags (i.e., Person, Organization, Location), resulting in 2,328 Hindi articles with around 11,000 NE tags. All further experiments were performed on this tagged dataset. Precision, Recall and F-measure are the evaluation metrics used to estimate the performance of our system. In order to compare our system performance with a baseline, we have availed the Hindi NER system developed by Gali et al. (2008) at LTRC (Language Technologies Research Center) 1 that recognizes and annotates Hindi NEs in a given text using Conditional Random Fields (CRF) as the sequential labeling mechanism. Their system is reproduced on our dataset with a 5-fold cross validation using spell variations, pattern of suffixes and POS tagging as the features. 6 Experiments and Results: The experiments conducted are broadly classified as follows: Experiment 1: Using the structure of Wikipedia namely Category terms, we can cluster the articles which are having similar category terms. Another approach for clustering is to co"
W12-5306,P02-1053,0,0.00852311,"the blog along with his view points on these objects. In this work, named entities are potential objects for opinion mining. We perform opinion mining for each of these objects by linking modifiers to each of these objects and deciding the orientation of these modifiers using a pre-constructed subjective lexicon. And finally, we generate two different concept summaries: an object wise opinionated summary of the document and opinionated summary of the object across the dataset. 2 Related Work The research we propose here is a combination of Opinion Mining and Summarization. (Pang et al., 2002; Turney, 2002) started the work in the direction of document level sentiment analysis. Major work in phrase level sentiment analysis was initially performed in (Agarwal et al., 2009; Wilson, 2005). (Hu and Liu, 2004; Liu and Hu, 2004; Popescu and Etzioni, 2005) concentrated on feature level product review mining. They extracted features from product reviews and generated a feature wise opinionated summary. Blog sentiment classification is primarily performed at document and sentence level. (Ku et al., 2006) used TREC and NTCIR blogs for opinion extraction. (Chesley, 2006) performed topic and genre independe"
W12-5306,H05-1044,0,0.00949143,"nking modifiers to each of these objects and deciding the orientation of these modifiers using a pre-constructed subjective lexicon. And finally, we generate two different concept summaries: an object wise opinionated summary of the document and opinionated summary of the object across the dataset. 2 Related Work The research we propose here is a combination of Opinion Mining and Summarization. (Pang et al., 2002; Turney, 2002) started the work in the direction of document level sentiment analysis. Major work in phrase level sentiment analysis was initially performed in (Agarwal et al., 2009; Wilson, 2005). (Hu and Liu, 2004; Liu and Hu, 2004; Popescu and Etzioni, 2005) concentrated on feature level product review mining. They extracted features from product reviews and generated a feature wise opinionated summary. Blog sentiment classification is primarily performed at document and sentence level. (Ku et al., 2006) used TREC and NTCIR blogs for opinion extraction. (Chesley, 2006) performed topic and genre independent blog classification, making novel use of linguistic features. (Zhang and et al., 2007) divided the document into sentences and used Pang (Pang et al., 2002) hypothesis to decide o"
W12-5306,radev-etal-2004-mead,0,\N,Missing
W12-5306,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W12-5306,H05-2017,0,\N,Missing
W12-5306,H05-1043,0,\N,Missing
W12-5306,E09-1004,0,\N,Missing
W12-5306,W02-1011,0,\N,Missing
W12-5306,L10-1000,0,\N,Missing
W14-5125,W03-0428,0,0.07729,"Missing"
W14-5125,P10-3015,0,0.0310825,"Missing"
W14-5125,dinh-etal-2008-word,0,\N,Missing
W14-5125,W03-0432,0,\N,Missing
W19-2404,P14-5010,0,0.0052004,"h empty, we ignore that interaction. If only one of them is empty, we add a special actor Environment (ENV) to that set. Once such sets are identified, a message is created for each unique combination of a sender and a receiver for a particular interaction verb. Dependency parsing-based Approaches: We developed two approaches for message creation based on dependency parsing output: i) Baseline B1 which directly maps the dependencies output to messages and ii) Approach M1 (Algorithm 1) which builds on the dependencies output by applying additional linguistic knowledge. We use Stanford CoreNLP (Manning et al., 2014) for dependency parsing. Baseline B1 simply maps each interaction verb in the dependency tree to a set of messages. Actors directly connected to an interaction verb with certain dependency relations (nsubj, nmod:agent) are identified as senders whereas actors directly connected to the verb with certain other dependency relations (dobj, nsubjpass, xcomp, iobj, advcl:to, nmod:∗) are identified as receivers. Interaction Identification Typically the input text mentions many different interactions, and identifying each verbal interaction is required, omitting non-interactions as discussed in Sectio"
W19-2404,W17-5912,1,0.76547,"sages M 1 (Algorithm 1), M2: create messages M 2 (Algorithm 2), M3: Combined approach using SRL and dependencies. Setting S1 corresponds to using gold actors and interaction verbs, Setting S2 uses predicted actors and interaction verbs Jurafsky (2009) on modelling of narrative schemas and their participants. They need a corpus of narratives to identify prototypical schemas which try to capture common sequence of events. We address a different problem of extracting MSC from a single narrative and do not need a corpus. MSC has been proposed as a knowledge representation for a narrative text in (Bedi et al., 2017). We extend their work to automatically construct MSC. Open Information Extraction (OpenIE) systems aim to extract tuples consisting of relation phrases and their multiple associated argument phrases from an input sentence (Mausam et al., 2012). The predicate-argument structures in OpenIE seem similar to SRL and dependency parsing. However, in dependency parsing the relations are fixed, while SRL systems require deeper semantic analysis of a sentence and hence they depend on lexical resources like PropBank and FrameNet. On the other hand, the predicate-argument structures in OpenIE are not res"
W19-2404,D12-1048,0,0.262806,"ii) Setting S2 : using predicted actors and interaction verbs. We use the approach proposed by Patil et al. (2018) for predicting actor mentions and identifying canonical mentions; and a simple algorithm for predicting interaction verbs. For evaluating our temporal ordering approach, we use Kendall’s τ rank correlation coefficient (Kendall, 1938) to compare predicted and gold time-lines of a key actor in each dataset (e.g., Mao Zedong in the Mao dataset). As goal of Kof’s work (Kof, 2007) is same as our work on message extraction, we use it as one of the baselines (B-Kof). We also use OpenIE (Mausam et al., 2012) as another baseline (BOIE). To avoid unnecessarily penalizing B-OIE, we consider only those extractions where relations fit our definition of interaction verbs and arguments fit our definition of actors. We compare our temporal ordering approach with the default text order based baseline (Text-Order). Table 2 shows comparative performance of the proposed approaches for message extraction and temporal ordering. 4.3 5 Related Work Though there has been some work in applying MSC for Software Engineering domain, less attention is given to the automatic construction of MSC using NLP. Feijs (2000)"
W19-2404,P09-1068,0,0.0492721,"that actor has participated. Message Sequence Chart (MSC) is an intuitive visual notation with rigorous mathematical semantics that can help to precisely represent and analyze (Alur et al., 1996) such scenarios. Feijs (2000), and Li (2000) propose techniques to convert software requirements to MSC. Event timeline construction is a related task about inferring the temporal ordering among events, but where events are not necessarily interactions among actors (Do et al., 2012). Another related line of research is storyline or plot generation from narrative texts such as news stories or fiction (Chambers and Jurafsky, 2009; Vossen 2 Problem Definition The input is a document D containing narrative text, and the desired output is an MSC depicting the interactions among the actors. No information about the actors or interactions is given as input; they need to be identified. For history narratives, we define an actor as an entity of type Person, Organization (ORG) or Location (LOC), which actively participates in various interactions 28 Proceedings of the First Workshop on Narrative Understanding, pages 28–36 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics Figure 1: MSC for"
W19-2404,P18-2011,1,0.923163,"ugh the text and identify all the actors who are involved in one or more interactions. We group all co-referring mentions of an actor into a set, and choose one canonical mention as a representative on the MSC. One complication can occur due to complex actors, which is an actor that contains multiple actors, one of which is independent and the others are dependent and serve to elaborate on the independent actor; e.g., his parents, military school, the army of the new government. We need to identify a complex actor as a whole, and not its constituent actors separately. We use the algorithm in (Patil et al., 2018) to identify an actor and all its coreferents. 3.2 3.3 We need to map each identified interaction to one or more messages in the output MSC. We also need to identify the sender (initiator of the interaction) and receiver (other actors involved in the interaction) for each message. We have developed several approaches for identifying a set of senders (SX) and a set of receivers (RX) for each valid interaction verb. If SX and RX are both empty, we ignore that interaction. If only one of them is empty, we add a special actor Environment (ENV) to that set. Once such sets are identified, a message"
W19-2404,D12-1062,0,0.0342475,"set of inter-related timelines, one for each actor, where the timeline of an actor specifies the temporal order of interactions in which that actor has participated. Message Sequence Chart (MSC) is an intuitive visual notation with rigorous mathematical semantics that can help to precisely represent and analyze (Alur et al., 1996) such scenarios. Feijs (2000), and Li (2000) propose techniques to convert software requirements to MSC. Event timeline construction is a related task about inferring the temporal ordering among events, but where events are not necessarily interactions among actors (Do et al., 2012). Another related line of research is storyline or plot generation from narrative texts such as news stories or fiction (Chambers and Jurafsky, 2009; Vossen 2 Problem Definition The input is a document D containing narrative text, and the desired output is an MSC depicting the interactions among the actors. No information about the actors or interactions is given as input; they need to be identified. For history narratives, we define an actor as an entity of type Person, Organization (ORG) or Location (LOC), which actively participates in various interactions 28 Proceedings of the First Worksh"
W19-2404,Q15-1032,0,0.0279814,"er. But as S2 indicates that the actor (Mao) is willingly performing the action of moving, we expect Mao to be a sender. Hence, for an ergative verb, even if the SRL assigns A1 role to an actor, we consider such an actor for being sender if no A0 role is assigned for the ergative verb by the SRL (lines 9-13 in Algorithm 2). SRL-based Approaches: We developed two approaches for message creation based on SRL: i) Baseline B2 which directly maps the SRL output to messages and ii) Approach M2 (Algorithm 2) which builds on the SRL output by applying additional linguistic knowledge. We use MatePlus (Roth and Lapata, 2015) for SRL which produces predicate-argument structures as per PropBank (Kingsbury and Palmer, 2002). The baseline B2 simply maps each verbal predicate corresponding to an interaction verb to a set of messages. Actors corresponding to A0 arguments of a verbal predicate are identified as senders whereas actors corresponding to other arguments are identified as receivers. Combined SRL and Dependency parsing based Approach (M3): SRL tools are useful to identify senders and receivers of a message, but they do have a few important limitations. E.g. (i) SRL tool may fail to identify any A0 even when i"
W19-2404,D15-1063,0,0.0626977,"Missing"
W19-2404,D10-1008,0,0.0822706,"Missing"
W19-2404,W15-4507,0,0.0509534,"Missing"
W19-2404,J95-2003,0,0.516851,"Missing"
W19-2404,W16-5702,0,0.0438654,"Missing"
W19-2404,kingsbury-palmer-2002-treebank,0,0.271159,"expect Mao to be a sender. Hence, for an ergative verb, even if the SRL assigns A1 role to an actor, we consider such an actor for being sender if no A0 role is assigned for the ergative verb by the SRL (lines 9-13 in Algorithm 2). SRL-based Approaches: We developed two approaches for message creation based on SRL: i) Baseline B2 which directly maps the SRL output to messages and ii) Approach M2 (Algorithm 2) which builds on the SRL output by applying additional linguistic knowledge. We use MatePlus (Roth and Lapata, 2015) for SRL which produces predicate-argument structures as per PropBank (Kingsbury and Palmer, 2002). The baseline B2 simply maps each verbal predicate corresponding to an interaction verb to a set of messages. Actors corresponding to A0 arguments of a verbal predicate are identified as senders whereas actors corresponding to other arguments are identified as receivers. Combined SRL and Dependency parsing based Approach (M3): SRL tools are useful to identify senders and receivers of a message, but they do have a few important limitations. E.g. (i) SRL tool may fail to identify any A0 even when it is present or when it assumes the verb does not require A0 in the sentence; (ii) the identified"
W19-2404,P15-2059,0,0.15776,"n in Figure 1, which can be used to answer questions like ""Whom did Napoleon defend the National Convention from?"". To the best of our knowledge, this is the first work that uses MSC to represent knowledge about actors and their interactions in narrative history text. Our approach is general, and can represent interactions among actors in any narrative text (e.g., news, fiction and screenplays). We propose unsupervised approaches using dependency parsing and Semantic Role Labelling for extracting interactions and corresponding senders/receivers. We use a state-ofthe-art tense based technique (Laparra et al., 2015) to temporally order the interactions to create the MSC. In this paper, we advocate the use of Message Sequence Chart (MSC) as a knowledge representation to capture and visualize multiactor interactions and their temporal ordering. We propose algorithms to automatically extract an MSC from a history narrative. For a given narrative, we first identify verbs which indicate interactions and then use dependency parsing and Semantic Role Labelling based approaches to identify senders (initiating actors) and receivers (other actors involved) for these interaction verbs. As a final step in MSC extrac"
Y09-2014,J90-2002,0,0.494478,"density based passage retrieval approaches. Similarly Wu et al. (2005) extracted surface relation patterns from both the query and the passages to perform relation based matching. The above two techniques are ineffective for short queries which have very less query terms and relation paths. Apart from the above methodologies, there are several other passage retrieval methodologies which used variants of standard retrieval methodologies including vector space models and language modeling. Murdock and Croft (2005) used the SMT model for sentence retrieval in QA. Their approach used IBM model 1 (Brown et al., 1990) to build a translation model for all the question-sentence pairs in the training corpus. The constructed translation model is used in the language modeling framework to retrieve a ranked set of passages. Their experimental results on TREC data showed that their approach performed better than retrieval based on query likelihood. Our approach for passage retrieval is very similar to the above approach, but we construct more sophisticated multiple translation models which are perceived as answer type profiles i.e., questions from distinct categories (answer types) have distinct translation model"
Y09-2014,J93-2003,0,0.0185534,"pproach, we build multiple translation models, each one for a category (answer type) of questions. We perceive each such translation model as an ATP. Statistical alignment models which maximize the probability of the observed (question, sentence) text pairs using Expectation Maximization algorithm, are used to construct these ATPs. After the maximization process is completed, the word level alignments are set to maximum posterior predictions of the model to produce triples: question word, sentence word, probability. We used GIZA++ (Och and Ney, 2000) an implementation of IBM alignment models (Brown et al., 1993), for building ATPs. Sample profiles for LOCATION and NUMBER types are shown in Table 3 and Table 4 respectively. Table 3: Translations for word born in LOCATION profile Word hometown immigrant birthplace competitor career birthday Probability 0.081337 0.0322747 0.0244121 0.0244121 0.0242433 0.0108326 Table 4: Translations for word born in NUMBER profile Word born youngest grandson nursing biography birthdate 4.4 Probability 0.330707 0.0147641 0.0147641 0.0134934 0.00987116 0.00492135 Passage Ranking This is the ranking or on-line phase of our approach. In this phase, using the ATPs, passages"
Y09-2014,kaisser-lowe-2008-creating,0,0.0217279,"odology includes two steps: estimation and ranking. The estimation step includes: construction of parallel corpus, semantic categorization of questions based on their answer types, and building answer type profiles. In the ranking step, answer type profiles are incorporated into the SMT framework to retrieve a ranked set of passages given a question. The detailed description of individual steps in estimation and ranking are described below. 561 4.1 Parallel corpus A parallel corpus consisting of questions and sentences with answers to those questions is required to learn answer type profiles. Kaisser and Lowe (2008) developed a Question Answer Sentence Pair (QASP) corpus to foster research in QA. They identified sentences which contain answers using Amazon’s Mechanical Trunk, an “artificial artificial intelligence” web service. The corpus consists of questions from Text Retrieval Conference (TREC) - QA track test sets for the years 2002 to 2006, and sentences consisting of answers from AQUAINT corpus. Table 1 shows the quantitative overview of QASP parallel corpus. Table 1: Quantitative overview of QASP parallel corpus. Year 2002 2003 2004 2005 2006 No. factoid questions 429 354 204 319 352 No. sentence"
Y09-2014,C02-1150,0,0.159901,"Missing"
Y09-2014,H05-1086,0,0.0955793,"terms and key terms within passages. This approach produced significant improvements when compared to the density based passage retrieval approaches. Similarly Wu et al. (2005) extracted surface relation patterns from both the query and the passages to perform relation based matching. The above two techniques are ineffective for short queries which have very less query terms and relation paths. Apart from the above methodologies, there are several other passage retrieval methodologies which used variants of standard retrieval methodologies including vector space models and language modeling. Murdock and Croft (2005) used the SMT model for sentence retrieval in QA. Their approach used IBM model 1 (Brown et al., 1990) to build a translation model for all the question-sentence pairs in the training corpus. The constructed translation model is used in the language modeling framework to retrieve a ranked set of passages. Their experimental results on TREC data showed that their approach performed better than retrieval based on query likelihood. Our approach for passage retrieval is very similar to the above approach, but we construct more sophisticated multiple translation models which are perceived as answer"
Y09-2014,P00-1056,0,0.0661192,"rds as feature. 562 4.3 Learning Answer type profiles In our approach, we build multiple translation models, each one for a category (answer type) of questions. We perceive each such translation model as an ATP. Statistical alignment models which maximize the probability of the observed (question, sentence) text pairs using Expectation Maximization algorithm, are used to construct these ATPs. After the maximization process is completed, the word level alignments are set to maximum posterior predictions of the model to produce triples: question word, sentence word, probability. We used GIZA++ (Och and Ney, 2000) an implementation of IBM alignment models (Brown et al., 1993), for building ATPs. Sample profiles for LOCATION and NUMBER types are shown in Table 3 and Table 4 respectively. Table 3: Translations for word born in LOCATION profile Word hometown immigrant birthplace competitor career birthday Probability 0.081337 0.0322747 0.0244121 0.0244121 0.0242433 0.0108326 Table 4: Translations for word born in NUMBER profile Word born youngest grandson nursing biography birthdate 4.4 Probability 0.330707 0.0147641 0.0147641 0.0134934 0.00987116 0.00492135 Passage Ranking This is the ranking or on-line"
Y12-1018,C10-1039,0,0.0817499,"Missing"
Y12-1018,P09-2040,0,0.044776,"Missing"
Y12-1018,P07-1123,0,0.248925,"Missing"
Y12-1018,D08-1014,0,0.117452,"Missing"
Y12-1018,C10-1004,0,0.236602,"Missing"
Y12-1018,D09-1140,0,0.090201,"Missing"
Y12-1018,I11-1129,0,0.278023,"Missing"
Y12-1018,W06-1642,0,0.492065,"Missing"
Y12-1018,P09-1027,0,0.0253584,"action tasks of NTCIR-6 (Y. Wu.et.al, 2007) performed subjectivity and sentiment analysis in languages other than English. (C. Banea.et.al, 172 2.3 Supervised Furthermore, tools developed for English were used to determine sentiment or subjectivity labeling for a given target language by transferring the text to English and applying an English classifier on the resulting data. The labels were then transfered back into the target language (M. Bautin.et.al, 2008). These experiments are carried out in Arabic, Chinese, English, French, German, Italian, Japanese, Korean, Spanish, and Romanian. (X. Wan, 2009) who constructs a polarity co-training system by using the multi-lingual views obtained through the automatic translation of product-reviews into Chinese and English. 3 3.1 Approach Feature Extraction and Weighing Features are categorized into syntactic, semantic, link-based, and stylistic features (G. Forman, 2003) from the previous subjective and sentiment studies. Here, we concentrate more on feature weighing methods based on syntactic and stylistic properties of the text to maintain language independence. Unigrams and Bigrams extracted as features are weighed as given below. Syntactic Feat"
Y12-1018,C04-1121,0,0.144608,"multi-lingual views obtained through the automatic translation of product-reviews into Chinese and English. 3 3.1 Approach Feature Extraction and Weighing Features are categorized into syntactic, semantic, link-based, and stylistic features (G. Forman, 2003) from the previous subjective and sentiment studies. Here, we concentrate more on feature weighing methods based on syntactic and stylistic properties of the text to maintain language independence. Unigrams and Bigrams extracted as features are weighed as given below. Syntactic Feature Weighing Syntactic features used in earlier works (M. Gamon, 2004) where word n-grams and part-of-speech (POS) tags. But, POS tagging create dependency on language specific tools. In order to eliminate the language specific dependencies we will use only word n-grams. Sentence Representation with Unigram (UF.ISF) This feature extraction is inspired from vector space model (G. Salton, 1975) used for flat documents. UF represents the unigram frequency at word level in a sentence. While ISF represent the inverse sentence frequency of the unigram. For a given collection S of subjective and objective sentences, an Index I = {u1 , u2 , ...., u|I |}, where |I |denot"
Y12-1018,I08-1040,0,\N,Missing
Y18-1053,D14-1058,0,0.208151,"MT & NLP Lab, KCIS LTRC, IIIT-Hyderabad Vasudeva Varma IREL Lab, KCIS LTRC, IIIT-Hyderabad knowledge, co-reference resolution and a large vocabulary. Simple keyword or pattern matching is less equipped to take up such a challenge (Bakman, 2007). There are 112 short trees and 119 tall trees currently in the park . Park workers will plant 105 short trees today . How many short trees will the park have when the workers are finished ? Answer : 112 + 105 Table 1: Mathematical Word Problem Example Previous systems either rely heavily on specific set of problem abstractions based on verb categories (Hosseini et al., 2014) or learning equations from pre-defined set of templates (Kushman et al., 2014). Deep neural solvers (Wang et al., 2017) proposed a combination of sequence-to-sequence model and information retrieval system. However, an ideal equation generation system for a word problem should be able to identify components of the equation and form the equation in an orderly fashion independent of syntax or vocabulary of the sentences. In this work, we introduce EquGener - an equation generator using a memory network with an equation decoder. Intuitively, a human math solver collects relevant details from the"
Y18-1053,P16-1084,0,0.0122247,"r. Wang et al. (2017) used a hybrid model of RNN and similarity-based information retrieval methods to outperform existing solvers. They used a 5 layer deep network - one word embedding layer, two-layer GRU (Chung et al., 2014) on the encoder side and two-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the help of some hand-written rules and modified the softmax function on the decoder side. There have been a concerted effort to create large scale and diversified datasets. Huang et al. (2016) analyzed the existing datasets, and created a large-scale dataset from community question answering web pages. Most of the neural network frameworks suffer from lack of performance in presence of a limited training dataset. (Weston et al., 2014) introduced long term memory components in neural networks for better reasoning called “memory networks”. The memories can be retrieved and written multiple times and can be used for prediction in multiple tasks. (Sukhbaatar et al., 2015) came up with an end-to-end memory network which required less supervision in giving answers to a question asked fro"
Y18-1053,D17-1084,0,0.0129782,"rb categorization, CFG rules can be used to solve word problems. Liang et al. (2018) suggested that there exists mainly two kinds of approaches for solving MWPs - one involving understanding and the other without understanding. Kushman et al. (2014) system was a joint log linear distribution over the full set of equations and alignments between the variables and text. The number of equations was dependent on the number of training equation templates. The number slots were filled by the numbers present in the text while the unknown or variable slot were filled by the nouns in the problem text. Huang et al. (2017) also extracted relevant templates and did fine grained inferencing to solve word problems. Illinois Math Solver (Roy and Roth, 2016a) used two modules to solve any arithmetic word problem. The first module was a CFG based Semantic Parser and other module solved the problem by decomposing it into a series of classification problems (Roy and Roth, 2016b) with formation of an expression tree through constrained inference. Hosseini et al. (2014) ‘s system used verb categorization for identifying relevant variables, their values and mapping them into a set of linear equations which can be easily s"
Y18-1053,P18-1039,0,0.0112001,"ed an arithmetic word problem solver that learned how to use formulas to solve simple addition and subtraction problems. Templates corresponding to particular formulas were manually modeled with pre-defined slots. All possible applications of different formulas were passed through a log-linear model to pick the best solution with highest score. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet (Miller, 1995) features. Recently there have been renewed interest in solving word problem through deep learning techniques. Huang et al. (2018) used intermediate meaning representation to generate equations. There has been attempts (Ling et al., 2017) to generate answer rationales for arriving at the final answer. Wang et al. (2017) used a hybrid model of RNN and similarity-based information retrieval methods to outperform existing solvers. They used a 5 layer deep network - one word embedding layer, two-layer GRU (Chung et al., 2014) on the encoder side and two-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the"
Y18-1053,Q15-1042,0,0.0420806,"Missing"
Y18-1053,N16-1136,0,0.013987,"anguage, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 from the encoder. The decoder also uses an LSTM to predict the next output. The decoder predicts the output distribution using teacher forcing (Lamb et al., 2016). The hidden and cell states are computed according to the LSTM equations defined in section 2.2. In Figure 1, the output tokens are referred to as Op1, Op2 and Opr which stand for the operands and the operator of the equation. 3 Experimental Setup 3.1 Data We used 1314 arithmetic problems with a single operation present in MAWPS Koncel-Kedziorski et al. (2016) as our training set. The operations include all basic mathematical operation addition (+), subtraction (-), multiplication (*) and division (/). The two benchmark datasets for evaluation are MA1 and IXL dataset Mitra and Baral (2016) which are subsets of the AI2 dataset Hosseini et al. (2014) released as a part of project Euclid 2 . We chose problems with only single operation from these two datasets - 103 from MA1 and 81 from IXL dataset. 3.2 Setting We used publicly available glove pre-trained embeddings 3 that were trained on Common Crawl containing 42 billion tokens, with a vocabulary siz"
Y18-1053,N18-1060,0,0.0130832,"Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 asked about the rulers. The verb “place” appears in the context of the rulers, so it also receives higher weights. 7 Previous Work Mukherjee and Garain (2008) explained different techniques used for word problem solving in their survey paper. Bobrow (1964) represented word problems in terms of a relational model. Bakman (2007) touched upon understanding of word problems involving extraneous information. Multiple approaches like template alignment, verb categorization, CFG rules can be used to solve word problems. Liang et al. (2018) suggested that there exists mainly two kinds of approaches for solving MWPs - one involving understanding and the other without understanding. Kushman et al. (2014) system was a joint log linear distribution over the full set of equations and alignments between the variables and text. The number of equations was dependent on the number of training equation templates. The number slots were filled by the numbers present in the text while the unknown or variable slot were filled by the nouns in the problem text. Huang et al. (2017) also extracted relevant templates and did fine grained inferenci"
Y18-1053,P17-1015,0,0.0227863,"n problems. Templates corresponding to particular formulas were manually modeled with pre-defined slots. All possible applications of different formulas were passed through a log-linear model to pick the best solution with highest score. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet (Miller, 1995) features. Recently there have been renewed interest in solving word problem through deep learning techniques. Huang et al. (2018) used intermediate meaning representation to generate equations. There has been attempts (Ling et al., 2017) to generate answer rationales for arriving at the final answer. Wang et al. (2017) used a hybrid model of RNN and similarity-based information retrieval methods to outperform existing solvers. They used a 5 layer deep network - one word embedding layer, two-layer GRU (Chung et al., 2014) on the encoder side and two-layer LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. A modified version of the sequence-to-sequence model was used to validate each generated output symbol with the help of some hand-written rules and modified the softmax function on the decoder side. There have been a con"
Y18-1053,D15-1166,0,0.00783601,"re compared to arrive at the alignment. at = align(ht , h˜s ) exp(ht T .h˜s ) = Σ 0 exp(h T .h˜0 ) s t s The context vector ct is computed as the weighted combination of the hidden states from the word sequence: Approach ct = Σt at × ht Base model The encoder-decoder architecture (Sutskever et al., 2014) has proved beneficial in many applications. But, this architecture has its limitation in handling long input sequences. This limitation results from a fixed size internal representation of the encoded sequence where the target sequence is decoded from this representation. Attention mechanism (Luong et al., 2015) has been widely used where the network learns the relative importance on which parts to attend to. In this architecture, the input sequence is encoded as a sequence of vectors and the decoder has access to all these vectors instead of a single vector. We modeled the input sequence as a sequence of word vectors. Each word vector is a concatenated vector representation of pre-trained glove (Pennington et al., 2014) embeddings and the embeddings learned by the network from the training corpus. The equation generation for a word problem requires the identification of words which indicate the pres"
Y18-1053,P16-1202,0,0.0144985,"oy and Roth, 2016a) used two modules to solve any arithmetic word problem. The first module was a CFG based Semantic Parser and other module solved the problem by decomposing it into a series of classification problems (Roy and Roth, 2016b) with formation of an expression tree through constrained inference. Hosseini et al. (2014) ‘s system used verb categorization for identifying relevant variables, their values and mapping them into a set of linear equations which can be easily solved. The system identified 7 kinds of verbs used in the problems which was predicted by support vector machines. Mitra and Baral (2016) created an arithmetic word problem solver that learned how to use formulas to solve simple addition and subtraction problems. Templates corresponding to particular formulas were manually modeled with pre-defined slots. All possible applications of different formulas were passed through a log-linear model to pick the best solution with highest score. The features to the model were dependency labels by running Stanford dependency parser, POS tags, some linguistic cues, Wordnet (Miller, 1995) features. Recently there have been renewed interest in solving word problem through deep learning techni"
Y18-1053,D14-1162,0,0.0819292,"ut sequences. This limitation results from a fixed size internal representation of the encoded sequence where the target sequence is decoded from this representation. Attention mechanism (Luong et al., 2015) has been widely used where the network learns the relative importance on which parts to attend to. In this architecture, the input sequence is encoded as a sequence of vectors and the decoder has access to all these vectors instead of a single vector. We modeled the input sequence as a sequence of word vectors. Each word vector is a concatenated vector representation of pre-trained glove (Pennington et al., 2014) embeddings and the embeddings learned by the network from the training corpus. The equation generation for a word problem requires the identification of words which indicate the presence of operands and operators. So an attention based encoder-decoder has been used as a baseline for our equation generation system. Both the encoder and decoder employ Long-term short-term memory (LSTM) to represent the input and target sequence respectively. hj = f(hj−1 , sj ) (1) The j th hidden state of the encoder is computed as equation 1 using an LSTM. The decoder is initialized with the hidden and cell st"
Y18-1053,Y18-1000,0,0.263852,"Missing"
Y18-1053,D17-1088,0,0.0554704,"n and a large vocabulary. Simple keyword or pattern matching is less equipped to take up such a challenge (Bakman, 2007). There are 112 short trees and 119 tall trees currently in the park . Park workers will plant 105 short trees today . How many short trees will the park have when the workers are finished ? Answer : 112 + 105 Table 1: Mathematical Word Problem Example Previous systems either rely heavily on specific set of problem abstractions based on verb categories (Hosseini et al., 2014) or learning equations from pre-defined set of templates (Kushman et al., 2014). Deep neural solvers (Wang et al., 2017) proposed a combination of sequence-to-sequence model and information retrieval system. However, an ideal equation generation system for a word problem should be able to identify components of the equation and form the equation in an orderly fashion independent of syntax or vocabulary of the sentences. In this work, we introduce EquGener - an equation generator using a memory network with an equation decoder. Intuitively, a human math solver collects relevant details from the problem description which equip her to solve the problem (Dellarosa, 1986). Driven by this intuition, we learn a dense"
