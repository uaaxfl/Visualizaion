2021.findings-acl.55,Deep Learning against {COVID}-19: Respiratory Insufficiency Detection in {B}razilian {P}ortuguese Speech,2021,-1,-1,9,0,7629,edresson casanova,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.winlp-1.6,{SIMPLEX}-{PB} 2.0: A Reliable Dataset for Lexical Simplification in {B}razilian {P}ortuguese,2020,-1,-1,3,1,14003,nathan hartmann,Proceedings of the The Fourth Widening Natural Language Processing Workshop,0,"Most research on Lexical Simplification (LS) addresses non-native speakers of English, since they are numerous and easy to recruit. This makes it difficult to create LS solutions for other languages and target audiences. This paper presents SIMPLEX-PB 2.0, a dataset for LS in Brazilian Portuguese that, unlike its predecessor SIMPLEX-PB, accurately captures the needs of Brazilian underprivileged children. To create SIMPLEX-PB 2.0, we addressed all limitations of the old SIMPLEX-PB through multiple rounds of manual annotation. As a result, SIMPLEX-PB 2.0 features much more reliable and numerous candidate substitutions to complex words, as well as word complexity rankings produced by a group underprivileged children."
2020.lrec-1.317,Evaluating Sentence Segmentation in Different Datasets of Neuropsychological Language Tests in {B}razilian {P}ortuguese,2020,-1,-1,4,0,7629,edresson casanova,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Automatic analysis of connected speech by natural language processing techniques is a promising direction for diagnosing cognitive impairments. However, some difficulties still remain: the time required for manual narrative transcription and the decision on how transcripts should be divided into sentences for successful application of parsers used in metrics, such as Idea Density, to analyze the transcripts. The main goal of this paper was to develop a generic segmentation system for narratives of neuropsychological language tests. We explored the performance of our previous single-dataset-trained sentence segmentation architecture in a richer scenario involving three new datasets used to diagnose cognitive impairments, comprising different stories and two types of stimulus presentation for eliciting narratives {---} visual and oral {---} via illustrated story-book and sequence of scenes, and by retelling. Also, we proposed and evaluated three modifications to our previous RCNN architecture: (i) the inclusion of a Linear Chain CRF; (ii) the inclusion of a self-attention mechanism; and (iii) the replacement of the LSTM recurrent layer by a Quasi-Recurrent Neural Network layer. Our study allowed us to develop two new models for segmenting impaired speech transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set."
2020.coling-main.512,"Using Eye-tracking Data to Predict the Readability of {B}razilian {P}ortuguese Sentences in Single-task, Multi-task and Sequential Transfer Learning Approaches",2020,-1,-1,5,1,16976,sidney leal,Proceedings of the 28th International Conference on Computational Linguistics,0,"Sentence complexity assessment is a relatively new task in Natural Language Processing. One of its aims is to highlight in a text which sentences are more complex to support the simplification of contents for a target audience (e.g., children, cognitively impaired users, non-native speakers and low-literacy readers (Scarton and Specia, 2018)). This task is evaluated using datasets of pairs of aligned sentences including the complex and simple version of the same sentence. For Brazilian Portuguese, the task was addressed by (Leal et al., 2018), who set up the first dataset to evaluate the task in this language, reaching 87.8{\%} of accuracy with linguistic features. The present work advances these results, using models inspired by (Gonzalez-Gardu{\~n}o and S{\o}gaard, 2018), which hold the state-of-the-art for the English language, with multi-task learning and eye-tracking measures. First-Pass Duration, Total Regression Duration and Total Fixation Duration were used in two moments; first to select a subset of linguistic features and then as an auxiliary task in the multi-task and sequential learning models. The best model proposed here reaches the new state-of-the-art for Portuguese with 97.5{\%} accuracy 1 , an increase of almost 10 points compared to the best previous results, in addition to proposing improvements in the public dataset after analysing the errors of our best model."
C18-1034,A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in {P}ortuguese,2018,0,0,3,1,16976,sidney leal,Proceedings of the 27th International Conference on Computational Linguistics,0,"Effective textual communication depends on readers being proficient enough to comprehend texts, and texts being clear enough to be understood by the intended audience, in a reading task. When the meaning of textual information and instructions is not well conveyed, many losses and damages may occur. Among the solutions to alleviate this problem is the automatic evaluation of sentence readability, task which has been receiving a lot of attention due to its large applicability. However, a shortage of resources, such as corpora for training and evaluation, hinders the full development of this task. In this paper, we generate a nontrivial sentence corpus in Portuguese. We evaluate three scenarios for building it, taking advantage of a parallel corpus of simplification, in which each sentence triplet is aligned and has simplification operations annotated, being ideal for justifying possible mistakes of future methods. The best scenario of our corpus PorSimplesSent is composed of 4,888 pairs, which is bigger than a similar corpus for English; all the three versions of it are publicly available. We created four baselines for PorSimplesSent and made available a pairwise ranking method, using 17 linguistic and psycholinguistic features, which correctly identifies the ranking of sentence pairs with an accuracy of 74.2{\%}."
W17-6615,{P}ortuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks,2017,13,24,6,1,14003,nathan hartmann,Proceedings of the 11th {B}razilian Symposium in Information and Human Language Technology,0,"Word embeddings have been found to provide meaningful representations for words in an efficient way; therefore, they have become common in Natural Language Processing sys- tems. In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants. We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec. We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks. The obtained results suggest that word analogies are not appropriate for word embedding evaluation; task-specific evaluations appear to be a better option."
W17-6618,Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts,2017,29,3,3,1,8607,marcos treviso,Proceedings of the 11th {B}razilian Symposium in Information and Human Language Technology,0,"This paper is motivated by the automation of neuropsychological tests involving discourse analysis in the retellings of narratives by patients with potential cognitive impairment. In this scenario the task of sentence boundary detection in speech transcripts is important as discourse analysis involves the application of Natural Language Processing tools, such as taggers and parsers, which depend on the sentence as a processing unit. Our aim in this paper is to verify which embedding induction method works best for the sentence boundary detection task, specifically whether it be those which were proposed to capture semantic, syntactic or morphological similarities."
W17-1215,Discriminating between Similar Languages with Word-level Convolutional Neural Networks,2017,11,1,2,0,32082,marcelo criscuolo,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"Discriminating between Similar Languages (DSL) is a challenging task addressed at the VarDial Workshop series. We report on our participation in the DSL shared task with a two-stage system. In the first stage, character n-grams are used to separate language groups, then specialized classifiers distinguish similar language varieties. We have conducted experiments with three system configurations and submitted one run for each. Our main approach is a word-level convolutional neural network (CNN) that learns task-specific vectors with minimal text preprocessing. We also experiment with multi-layer perceptron (MLP) networks and another hybrid configuration. Our best run achieved an accuracy of 90.76{\%}, ranking 8th among 11 participants and getting very close to the system that ranked first (less than 2 points). Even though the CNN model could not achieve the best results, it still makes a viable approach to discriminating between similar languages."
P17-1118,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,2017,29,4,6,0,28671,leandro santos,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose. Linguistic features, mainly from parsers, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts. In this paper, we modeled transcripts into complex networks and enriched them with word embedding (CNE) to better represent short texts produced in neuropsychological assessments. The network measurements were applied with well-known classifiers to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets: DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using only complex networks, while Support Vector Machine was superior to other classifiers. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using linguistic features yielded higher accuracy if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that complex networks enriched with embedding is promising for detecting MCI in large-scale assessments."
E17-1030,Sentence Segmentation in Narrative Transcripts from Neuropsychological Tests using Recurrent Convolutional Neural Networks,2017,13,6,3,1,8607,marcos treviso,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks in order to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech as well as normal, prepared speech and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL."
W15-5610,Portal {M}in@s: Uma Ferramenta Geral de Apoio ao Processamento de C{\\'o}rpus de Prop{\\'o}sito Geral (Portal {M}in@s: A General Purpose Support Tool for Corpora Processing),2015,0,0,6,0,36444,arnaldo junior,Proceedings of the 10th {B}razilian Symposium in Information and Human Language Technology,0,None
W15-5624,Semi-Automatic Construction of a Textual Entailment Dataset: Selecting Candidates with Vector Space Models,2015,14,0,2,1,13913,erick fonseca,Proceedings of the 10th {B}razilian Symposium in Information and Human Language Technology,0,None
W15-1508,A Deep Architecture for Non-Projective Dependency Parsing,2015,20,4,2,1,13913,erick fonseca,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"Graph-based dependency parsing algorithms commonly employ features up to third order in an attempt to capture richer syntactic relations. However, each level and each feature combination must be defined manually. Besides that, input features are usually represented as huge, sparse binary vectors, offering limited generalization. In this work, we present a deep architecture for dependency parsing based on a convolutional neural network. It can examine the whole sentence structure before scoring each head/modifier candidate pair, and uses dense embeddings as input. Our model is still under ongoing work, achieving 91.6% unlabeled attachment score in the Penn Treebank."
S15-1026,Automatic Generation of a Lexical Resource to support Semantic Role Labeling in {P}ortuguese,2015,12,1,2,1,30756,magali duran,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"This paper reports an approach to automatically generate a lexical resource to support incremental semantic role labeling annotation in Portuguese. The data come from the corpus Propbank-Br (Propbank of Brazilian Portuguese) and from the lexical resource of English Propbank, as both share the same structure. In order to enable the strategy, we added extra annotation to Propbank-Br. This approach is part of a previous decision to invert the process of implementing a Propbank project, by first annotating a core corpus and only then generating a lexical resource to enable further annotation tasks. The reasoning behind such inversion is to explore the task empirically before distributing the annotation task and to provide simultaneously: 1) a first training corpus for SRL in Brazilian Portuguese and 2) annotated examples to compose a lexical resource to support SRL. The main contribution of this paper is to point out to what extent linguistic effort may be reduced, thereby speeding up the construction of a lexical resource to support SRL for less resourced languages. The corpus Propbank-Br, with the extra annotation described herein, is publicly available."
W14-0404,Some Issues on the Normalization of a Corpus of Products Reviews in {P}ortuguese,2014,12,9,3,1,30756,magali duran,Proceedings of the 9th Web as Corpus Workshop ({W}a{C}-9),0,"This paper describes the analysis of different kinds of noises in a corpus of products reviews in Brazilian Portuguese. Case folding, punctuation, spelling and the use of internet slang are the major kinds of noise we face. After noting the effect of these noises on the POS tagging task, we propose some procedures to minimize them."
sepulveda-torres-etal-2014-generating,Generating a Lexicon of Errors in {P}ortuguese to Support an Error Identification System for {S}panish Native Learners,2014,11,0,3,1,21011,lianet torres,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Portuguese is a less resourced language in what concerns foreign language learning. Aiming to inform a module of a system designed to support scientific written production of Spanish native speakers learning Portuguese, we developed an approach to automatically generate a lexicon of wrong words, reproducing language transfer errors made by such foreign learners. Each item of the artificially generated lexicon contains, besides the wrong word, the respective Spanish and Portuguese correct words. The wrong word is used to identify the interlanguage error and the correct Spanish and Portuguese forms are used to generate the suggestions. Keeping control of the correct word forms, we can provide correction or, at least, useful suggestions for the learners. We propose to combine two automatic procedures to obtain the error correction: i) a similarity measure and ii) a translation algorithm based on aligned parallel corpus. The similarity-based method achieved a precision of 52{\%}, whereas the alignment-based method achieved a precision of 90{\%}. In this paper we focus only on interlanguage errors involving suffixes that have different forms in both languages. The approach, however, is very promising to tackle other types of errors, such as gender errors."
hartmann-etal-2014-large,A Large Corpus of Product Reviews in {P}ortuguese: Tackling Out-Of-Vocabulary Words,2014,19,12,7,1,14003,nathan hartmann,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Web 2.0 has allowed a never imagined communication boom. With the widespread use of computational and mobile devices, anyone, in practically any language, may post comments in the web. As such, formal language is not necessarily used. In fact, in these communicative situations, language is marked by the absence of more complex syntactic structures and the presence of internet slang, with missing diacritics, repetitions of vowels, and the use of chat-speak style abbreviations, emoticons and colloquial expressions. Such language use poses severe new challenges for Natural Language Processing (NLP) tools and applications, which, so far, have focused on well-written texts. In this work, we report the construction of a large web corpus of product reviews in Brazilian Portuguese and the analysis of its lexical phenomena, which support the development of a lexical normalization tool for, in future work, subsidizing the use of standard NLP products for web opinion mining and summarization purposes."
W13-4820,Um reposit{\\'o}rio de verbos para a anota{\\c{c}}{\\~a}o de pap{\\'e}is sem{\\^a}nticos dispon{\\'\\i}vel na web (A Verb Repository for Semantic Role Labeling Available in the Web) [in {P}ortuguese],2013,0,1,3,1,30756,magali duran,Proceedings of the 9th {B}razilian Symposium in Information and Human Language Technology,0,None
W13-4822,Approaches for Helping {B}razilian Students Improve their Scientific Writings,2013,8,0,3,0,40608,ethel schuster,Proceedings of the 9th {B}razilian Symposium in Information and Human Language Technology,0,"Writing well in English is a challenge for non-native English speakers. When readers are unable to comprehend what they read they just give up reading and fail to get to the content. In this paper we describe problems encountered in the English writing of scientific abstracts by Brazilian Portuguese speakers. We collected and analyzed a corpus of 115 abstracts in which we identified specific language-related errors that affect comprehension. We show that students who must write scientific papers may benefit significantly with practice exercises, and computer-based tools. The use of such tools can enhance the students' level of confidence and thus enable them to improve their writing."
W13-4829,An Evaluation of the {B}razilian {P}ortuguese {LIWC} Dictionary for Sentiment Analysis,2013,9,29,3,0,37014,pedro filho,Proceedings of the 9th {B}razilian Symposium in Information and Human Language Technology,0,This work presents an evaluation of the Brazilian Portuguese LIWC dictionary for Sentiment Analysis. This evaluation is conducted by comparison against two other sentiment resources for Portuguese language: Opinion Lexicon and SentiLex. We conducted an intrinsic and an extrinsic evaluations and show how LIWC dictionary could be used in sentiment analysis projects.
W13-1014,Identifying Pronominal Verbs: Towards Automatic Disambiguation of the Clitic {`}se{'} in {P}ortuguese,2013,10,1,3,1,30756,magali duran,Proceedings of the 9th Workshop on Multiword Expressions,0,"A challenging topic in Portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se, which impacts NLP tasks such as syntactic parsing, semantic role labeling and machine translation. Aiming to give a step forward towards the automatic disambiguation of se, our study focuses on the identification of pronominal verbs, which correspond to one of the six uses of se as a clitic pronoun, when se is considered a CONSTITUTIVE PARTICLE of the verb lemma to which it is bound, as a multiword unit. Our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se. This process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task. The availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources, such as the Portuguese versions of Wordnet, Propbank and VerbNet. Moreover, it will allow the revision of parsers and dictionaries already in use."
duran-aluisio-2012-propbank,{P}ropbank-Br: a {B}razilian Treebank annotated with semantic role labels,2012,18,21,2,1,30756,magali duran,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper reports the annotation of a Brazilian Portuguese Treebank with semantic role labels following Propbank guidelines. A different language and a different parser output impact the task and require some decisions on how to annotate the corpus. Therefore, a new annotation guide â called Propbank-Br - has been generated to deal with specific language phenomena and parser problems. In this phase of the project, the corpus was annotated by a unique linguist. The annotation task reported here is inserted in a larger projet for the Brazilian Portuguese language. This project aims to build Brazilian verbs frames files and a broader and distributed annotation of semantic role labels in Brazilian Portuguese, allowing inter-annotator agreement measures. The corpus, available in web, is already being used to build a semantic tagger for Portuguese language."
dayrell-etal-2012-rhetorical,Rhetorical Move Detection in {E}nglish Abstracts: Multi-label Sentence Classifiers and their Annotated Corpora,2012,20,6,8,0,35163,carmen dayrell,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The relevance of automatically identifying rhetorical moves in scientific texts has been widely acknowledged in the literature. This study focuses on abstracts of standard research papers written in English and aims to tackle a fundamental limitation of current machine-learning classifiers: they are mono-labeled, that is, a sentence can only be assigned one single label. However, such approach does not adequately reflect actual language use since a move can be realized by a clause, a sentence, or even several sentences. Here, we present MAZEA (Multi-label Argumentative Zoning for English Abstracts), a multi-label classifier which automatically identifies rhetorical moves in abstracts but allows for a given sentence to be assigned as many labels as appropriate. We have resorted to various other NLP tools and used two large training corpora: (i) one corpus consists of 645 abstracts from physical sciences and engineering (PE) and (ii) the other corpus is made up of 690 from life and health sciences (LH). This paper presents our preliminary results and also discusses the various challenges involved in multi-label tagging and works towards satisfactory solutions. In addition, we also make our two training corpora publicly available so that they may serve as benchmark for this new task."
W11-4506,Caracter{\\'\\i}sticas do jornalismo popular: avalia{\\c{c}}{\\~a}o da inteligibilidade e aux{\\'\\i}lio {\\`a} descri{\\c{c}}{\\~a}o do g{\\^e}nero (Characteristics of Popular News: the Evaluation of Intelligibility and Support to the Genre Description) [in {P}ortuguese],2011,-1,-1,4,0,15680,maria finatto,Proceedings of the 8th {B}razilian Symposium in Information and Human Language Technology,0,None
W11-4508,Using machine learning methods to avoid the pitfall of cognates and false friends in {S}panish-{P}ortuguese word pairs,2011,14,5,2,1,21011,lianet torres,Proceedings of the 8th {B}razilian Symposium in Information and Human Language Technology,0,"The fact that 85% of the Portuguese lexicon contains Spanish cognates and that the linguistic structures of both languages are highly coincident is believed to be an advantage for the Spanish speaker who learns Portuguese. However, these similarities have some negative aspects in the learning of Portuguese, such as, the pitfall of false friends, since about 20% of cognates are false. The aim of this article is to identify cognates and false friends between Spanish and Portuguese automatically to build dictionaries of these words. One of the uses for these dictionaries is to support scientific writing tools, which can help lower barriers for Spanish speakers when they write in Portuguese."
W11-4519,{P}ropbank-Br: a {B}razilian {P}ortuguese corpus annotated with semantic role labels,2011,-1,-1,2,1,30756,magali duran,Proceedings of the 8th {B}razilian Symposium in Information and Human Language Technology,0,None
W11-2315,Towards an on-demand Simple {P}ortuguese {W}ikipedia,2011,20,1,4,0,7636,arnaldo jr,Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies,0,"The Simple English Wikipedia provides a simplified version of Wikipedia's English articles for readers with special needs. However, there are fewer efforts to make information in Wikipedia in other languages accessible to a large audience. This work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a Simple Portuguese Wikipedia on demand, based on user interactions with the main Portuguese Wikipedia. Our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words, while our system can correctly simplify 22,200 occurrences, with estimated f-measure 77.2%."
W11-0812,Identifying and Analyzing {B}razilian {P}ortuguese Complex Predicates,2011,14,13,3,1,30756,magali duran,Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,0,"Semantic Role Labeling annotation task depends on the correct identification of predicates, before identifying arguments and assigning them role labels. However, most predicates are not constituted only by a verb: they constitute Complex Predicates (CPs) not yet available in a computational lexicon. In order to create a dictionary of CPs, this study employs a corpus-based methodology. Searches are guided by POS tags instead of a limited list of verbs or nouns, in contrast to similar studies. Results include (but are not limited to) light and support verb constructions. These CPs are classified into idiomatic and less idiomatic. This paper presents an in-depth analysis of this phenomenon, as well as an original resource containing a set of 773 annotated expressions. Both constitute an original and rich contribution for NLP tools in Brazilian Portuguese that perform tasks involving semantics."
W10-1607,Fostering Digital Inclusion and Accessibility: The {P}or{S}imples project for Simplification of {P}ortuguese Texts,2010,17,31,1,1,7637,sandra aluisio,Proceedings of the {NAACL} {HLT} 2010 Young Investigators Workshop on Computational Approaches to Languages of the {A}mericas,0,"In this paper we present the PorSimples project, whose aim is to develop text adaptations tools for Brazilian Portuguese. The tools developed cater for both people at poor literacy levels and authors that want to produce texts for this audience. Here we describe the tools and resources developed over two years of this project and point directions for future work and collaboration. Since Portuguese and Spanish have many aspects in common, we believe our main point for collaboration lies in transferring our knowledge and experience to researches willing to developed simplification and elaboration tools for Spanish."
W10-1001,Readability Assessment for Text Simplification,2010,27,68,1,1,7637,sandra aluisio,Proceedings of the {NAACL} {HLT} 2010 Fifth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We describe a readability assessment approach to support the process of text simplification for poor literacy readers. Given an input text, the goal is to predict its readability level, which corresponds to the literacy level that is expected from the target reader: rudimentary, basic or advanced. We complement features traditionally used for readability assessment with a number of new features, and experiment with alternative ways to model this problem using machine learning methods, namely classification, regression and ranking. The best resulting model is embedded in an authoring tool for Text Simplification."
N10-2011,{SIMPLIFICA}: a tool for authoring simplified texts in {B}razilian {P}ortuguese guided by readability assessments,2010,6,6,5,0.952381,7140,carolina scarton,Proceedings of the {NAACL} {HLT} 2010 Demonstration Session,0,"SIMPLIFICA is an authoring tool for producing simplified texts in Portuguese. It provides functionalities for lexical and syntactic simplification and for readability assessment. This tool is the first of its kind for Portuguese; it brings innovative aspects for simplification tools in general, since the authoring process is guided by readability assessment based on the levels of literacy of the Brazilian population."
duran-etal-2010-assigning,Assigning Wh-Questions to Verbal Arguments: Annotation Tools Evaluation and Corpus Building,2010,8,5,3,1,30756,magali duran,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This work reports the evaluation and selection of annotation tools to assign wh-question labels to verbal arguments in a sentence. Wh-question assignment discussed herein is a kind of semantic annotation which involves two tasks: making delimitation of verbs and arguments, and linking verbs to its arguments by question labels. As it is a new type of semantic annotation, there is no report about requirements an annotation tool should have to face it. For this reason, we decided to select the most appropriated tool in two phases. In the first phase, we executed the task with an annotation tool we have used before in another task. Such phase helped us to test the task and enabled us to know which features were or not desirable in an annotation tool for our purpose. In the second phase, guided by such requirements, we evaluated several tools and selected a tool for the real task. After corpus annotation conclusion, we report some of the annotation results and some comments on the improvements there should be made in an annotation tool to better support such kind of annotation task."
W09-2105,Supporting the Adaptation of Texts for Poor Literacy Readers: a Text Simplification Editor for {B}razilian {P}ortuguese,2009,17,41,6,0,46973,arnaldo candido,Proceedings of the Fourth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper we investigate the task of text simplification for Brazilian Portuguese. Our purpose is three-fold: to introduce a simplification tool for such language and its underlying development methodology, to present an on-line authoring system of simplified text based on the previous tool, and finally to discuss the potentialities of such technology for education. The resources and tools we present are new for Portuguese and innovative in many aspects with respect to previous initiatives for other languages."
aluisio-etal-2004-lacio,The L{\\'a}cio-Web: Corpora and Tools to Advance {B}razilian {P}ortuguese Language Investigations and Computational Linguistic Tools,2004,6,17,1,1,7637,sandra aluisio,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we discuss the five requirements for building large publicly available corpora which geared the construction of the L{\'a}cio-Web corpora and their environments: 1) a comprehensive text typology; 2) text copyright clearance, compilation and annotation scheme; 3) a friendly and didactic interface; 4) the need to serve as support for several types of research; 5) the need to offer an array of associated tools. Also, we present the features that make L{\'a}cio-Web corpora interesting and novel as well as the limitations of this project, such as corpora size and balance, and the non-inclusion of spoken texts in the project{'}s reference corpus."
aires-etal-2004-style,What is my Style? Using Stylistic Features of {P}ortuguese Web Texts to Classify Web Pages According to Users{'} Needs,2004,4,4,3,0,52188,rachel aires,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we investigate the use of stylistic features of Web texts in Portuguese to classify web pages according to usersxe2x80x99 needs, in order to improve Web Information Retrieval. We first describe a seven categories classification of users needs, which was the outcome of a qualitative analysis of two TodoBr logs (a major Brazilian search engine). We describe 46 shallow linguistic features, inspired by the works of Biber and Karlgren, and proceed describing the compilation of the corpus employed on the classifier training. Our aim is to obtain rules that can be applied on the classification of Web texts according to those seven users needs. Some experiments are reported, showing that it is possible, at least for some of the categories, to identify them reliably."
