2020.acl-main.641,D10-1049,0,0.558515,"e 3 4 FamilyFriendly → (is a child-free) 6 5 → (, expensive) PriceRange 8 7 → (pub.) EatType Figure 1: Generation from our model on the E2E dataset. Decoding is performed segment-by-segment. 1 Each segment realizes one data record. ˜ 8 mark the decision order in the generation process. Introduction Data-to-text generation aims at automatically producing natural language descriptions of structured database (Reiter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental struc"
2020.acl-main.641,P19-1080,0,0.134841,"with neural attention, the proposed model has the following advantages: (1) We can monitor the corresponding data record for every segment to be generated. This allows us to easily control the output structure and verify its correctness1 . (2) Explicitly building the correspondence between segments and data records can potentially reduce the hallucination, as noted in (Wu et al., 2018; Deng et al., 2018) that hard alignment usually outperforms soft attention. (3) When decoding each segment, the model pays attention only to the 1 For example, we can perform a similar constrained decoding as in Balakrishnan et al. (2019) to rule out outputs with undesired patterns. 7155 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7155–7165 c July 5 - 10, 2020. 2020 Association for Computational Linguistics selected data record instead of averaging over the entire input data. This largely reduces the memory and computational costs 2 . To train the model, we do not rely on any human annotations for the segmentation and correspondence, but rather marginalize over all possibilities to maximize the likelihood of target text, which can be efficiently done within polynomial time by"
2020.acl-main.641,W05-0909,0,0.112008,"input data). The intuition is that every text is expected to realize the content of all K input records. It is natural to assume every text can be roughly segmented into K fragments, each corresponding to one data record. A deviation of 7159 5 nlp.stanford.edu/data/glove.6B.zip K±1 is allowed for noisy data or text with complex structures. Metrics We measure the quality of system outputs from three perspectives: (1) word-level overlap with human references, which is a commonly used metric for text generation. We report the scores of BLEU-4 (Papineni et al., 2002), ROUGEL (Lin, 2004), Meteor (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) . (2) human evaluation. Word-level overlapping scores usually correlate rather poorly with human judgements on fluency and information accuracy (Reiter and Belz, 2009; Novikova et al., 2017a). Therefore, we passed the input data and generated text to human annotators to judge if the text is fluent by grammar (scale 1-5 as in Belz and Reiter (2006)), contains wrong fact inconsistent with input data, repeats or misses information. We report the averaged score for fluency and definite numbers for others. The human is conducted on a sampled subset from the test d"
2020.acl-main.641,H05-1042,0,0.180871,"engineered constraints to control the outputs. 2 Related Work Data-to-text generation is traditionally dealt with using a pipeline structure containing content planning, sentence planning and linguistic realization (Reiter and Dale, 1997). Each target text is split into meaningful fragments and aligned with corresponding data records, either by handengineered rules (Kukich, 1983; McKeown, 1992) or statistical induction (Liang et al., 2009; KoncelKedziorski et al., 2014; Qin et al., 2018). The segmentation and alignment are used as supervision signals to train the content and sentence planner (Barzilay and Lapata, 2005; Angeli et al., 2010). The linguistic realization is usually implemented by template mining from the training corpus (Kondadadi et al., 2013; Oya et al., 2014). Our model adopts a similar pipeline generative process, but 2 Coarse-to-fine attention (Ling and Rush, 2017; Deng et al., 2017) was proposed for the same motivation, but they resort to reinforcement learning which is hard to train, and the performance is sacrificed for efficiency. integrates all the sub-steps into a single end-toend trainable neural architecture. It can be considered as a neural extension of the PCFG system in Konstas"
2020.acl-main.641,E06-1040,0,0.0258429,"puts from three perspectives: (1) word-level overlap with human references, which is a commonly used metric for text generation. We report the scores of BLEU-4 (Papineni et al., 2002), ROUGEL (Lin, 2004), Meteor (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) . (2) human evaluation. Word-level overlapping scores usually correlate rather poorly with human judgements on fluency and information accuracy (Reiter and Belz, 2009; Novikova et al., 2017a). Therefore, we passed the input data and generated text to human annotators to judge if the text is fluent by grammar (scale 1-5 as in Belz and Reiter (2006)), contains wrong fact inconsistent with input data, repeats or misses information. We report the averaged score for fluency and definite numbers for others. The human is conducted on a sampled subset from the test data. To ensure the subset covers inputs with all possible number of records (K ∈ [3, 8] for E2E and K ∈ [1, 7] for WebNLG), we sample 20 instances for every possible K. Finally,we obtain 120 test cases for E2E and 140 for WebNLG 6 . (3) Diversity of outputs. Diversity is an important concern for many real-life applications. We measure it by the number of unique unigrams and trigram"
2020.acl-main.641,J93-2003,0,0.154018,"e constraint to prohibit selftransition, which can be easily done by zeroing out the transition probability in Equation 3 when c(so ) = c(so−1 ). This forces the model to group together text describing the same data record. Since Equation 3 conditions on all previously generated text, it is able to capture more complex dependencies as in semi-markov models (Liang et al., 2009; Wiseman et al., 2018). Null Record In our task, we find some frequent phrases, e.g., “it is”, “and”, tend to be wrongly aligned with some random records, similar to the garbage collection issue in statistical alignment (Brown et al., 1993). This hurt the model interpretability. Therefore, we introduce an additional null record r0 to attract these non-content phrases. The context vector when aligned to r0 is a zero vector so that the decoder will decode words based solely on the language model without relying on the input data. The final likelihood of P the target text can be comK puted as p(y1:m |X) = rj=r α(m, j). As the for0 ward algorithm is fully differentiable, we maximize the log-likelihood of the target text by backpropagating through the dynamic programming. The process is essentially equivalent to the generalized EM al"
2020.acl-main.641,W16-6626,0,0.064989,"Missing"
2020.acl-main.641,J05-4004,0,0.158114,"Missing"
2020.acl-main.641,W18-6539,0,0.0255158,"Missing"
2020.acl-main.641,P02-1001,0,0.218156,"use the content of 7158 one data record is realized in separate pieces 3 . When it is too coarse, the alignment might become less accurate (as in Example 4, “pub” is wrongly merged with previous words and aligned together to the “Food” record). In practice, we expect the segmentation to stay with accurate alignment yet avoid being too brokenly separated. To control the granularity as we want, we utilize posterior regularization (Ganchev et al., 2010) to constrain the expected number of segments for each text 4 , which can be calculated by going through a similar forward pass as in Equation 4 (Eisner, 2002). Most computation is shared without significant extra burden. The final loss function is: − log ESy p(s1:τs |X)+max( ESy τs − η , γ) (5) log ESy p(s1:τs |X) is the log-likelihood of target text after marginalizing over all valid segmentations. ESy τs is the expected number of segments and η, γ are hyperparameters. We use the maxmargin loss to encourage ESy τs to stay close to η under a tolerance range of γ. Decoding The segment-by-segment generation process allows us to easily constrain the output structure. Undesirable patterns can be rejected before the whole text is generated. We adopt thr"
2020.acl-main.641,W16-5901,0,0.0204655,"rt the model interpretability. Therefore, we introduce an additional null record r0 to attract these non-content phrases. The context vector when aligned to r0 is a zero vector so that the decoder will decode words based solely on the language model without relying on the input data. The final likelihood of P the target text can be comK puted as p(y1:m |X) = rj=r α(m, j). As the for0 ward algorithm is fully differentiable, we maximize the log-likelihood of the target text by backpropagating through the dynamic programming. The process is essentially equivalent to the generalized EM algorithm (Eisner, 2016). By means of the modern automatic differentiation tools, we avoid the necessity to calculate the posterior distribution manually (Kim et al., 2018). To speed up training, we set a threshold L to the maximum length of a segment as in Liang et al. (2009); Wiseman et al. (2018). This changes the complexity in Equation 4 to a constant O(LK) instead of scaling linearly with the length of the target text. Moreover, as pointed out in Wang et al. (2017), the computation for the longest segment can be reused for shorter segments. We therefore first compute the generation and transition probability for"
2020.acl-main.641,D19-1052,0,0.0517223,"Missing"
2020.acl-main.641,N18-1014,0,0.0362743,"rately estimate the sentence likelihood itself without paying extra cost of switching between segments, then it tends to reduce the number of transitions. Vice versa, the number of transitions will grow if the transition module is stronger (larger embedding size). With the regularization we proposed, the granularity remains what we want regardless of the hyperparameters. We can thereby freely decide the model capacity without worrying about the difference of segmentation behavior. Results on E2E On the E2E dataset, apart from our implementations, we also compare agianst outputs from the SLUG (Juraska et al., 2018), the overall winner of the E2E challenge (seq2seqbased), DANGNT (Nguyen and Tran, 2018), the best grammar rule based model, TUDA (Puzikov and Gurevych, 2018), the best template based model, and the autoregressive neural template model (N TEMP) from Wiseman et al. (2018). SLUG uses a heuristic slot aligner based on a set of handcrafted rules and combine a complex pipeline of data augmentation, selection, model ensemble and reranker, while our model has a simple end-toend learning paradigm with no special delexicalizing, training or decoding tricks. Table 2 reports the evaluated results. Seq2se"
2020.acl-main.641,C10-2062,0,0.108697,"→ (is a child-free) 6 5 → (, expensive) PriceRange 8 7 → (pub.) EatType Figure 1: Generation from our model on the E2E dataset. Decoding is performed segment-by-segment. 1 Each segment realizes one data record. ˜ 8 mark the decision order in the generation process. Introduction Data-to-text generation aims at automatically producing natural language descriptions of structured database (Reiter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specific"
2020.acl-main.641,D18-3004,0,0.0176831,"n aligned to r0 is a zero vector so that the decoder will decode words based solely on the language model without relying on the input data. The final likelihood of P the target text can be comK puted as p(y1:m |X) = rj=r α(m, j). As the for0 ward algorithm is fully differentiable, we maximize the log-likelihood of the target text by backpropagating through the dynamic programming. The process is essentially equivalent to the generalized EM algorithm (Eisner, 2016). By means of the modern automatic differentiation tools, we avoid the necessity to calculate the posterior distribution manually (Kim et al., 2018). To speed up training, we set a threshold L to the maximum length of a segment as in Liang et al. (2009); Wiseman et al. (2018). This changes the complexity in Equation 4 to a constant O(LK) instead of scaling linearly with the length of the target text. Moreover, as pointed out in Wang et al. (2017), the computation for the longest segment can be reused for shorter segments. We therefore first compute the generation and transition probability for the whole sequence in one pass. The intermediate results are then cached to efficiently proceed the forward algorithm without any re-computation. O"
2020.acl-main.641,D14-1043,0,0.0708342,"Missing"
2020.acl-main.641,P13-1138,0,0.0171102,"taining content planning, sentence planning and linguistic realization (Reiter and Dale, 1997). Each target text is split into meaningful fragments and aligned with corresponding data records, either by handengineered rules (Kukich, 1983; McKeown, 1992) or statistical induction (Liang et al., 2009; KoncelKedziorski et al., 2014; Qin et al., 2018). The segmentation and alignment are used as supervision signals to train the content and sentence planner (Barzilay and Lapata, 2005; Angeli et al., 2010). The linguistic realization is usually implemented by template mining from the training corpus (Kondadadi et al., 2013; Oya et al., 2014). Our model adopts a similar pipeline generative process, but 2 Coarse-to-fine attention (Ling and Rush, 2017; Deng et al., 2017) was proposed for the same motivation, but they resort to reinforcement learning which is hard to train, and the performance is sacrificed for efficiency. integrates all the sub-steps into a single end-toend trainable neural architecture. It can be considered as a neural extension of the PCFG system in Konstas and Lapata (2013), with a more powerful transition probability considering inter-segment dependence and a state-of-the-art attention-based l"
2020.acl-main.641,P83-1022,0,0.7137,"rks, our model is able to produce significantly higher-quality outputs while being several times computationally cheaper. Due to its fully interpretable segmental structure, it can be easily reconciled with heuristic rules or hand-engineered constraints to control the outputs. 2 Related Work Data-to-text generation is traditionally dealt with using a pipeline structure containing content planning, sentence planning and linguistic realization (Reiter and Dale, 1997). Each target text is split into meaningful fragments and aligned with corresponding data records, either by handengineered rules (Kukich, 1983; McKeown, 1992) or statistical induction (Liang et al., 2009; KoncelKedziorski et al., 2014; Qin et al., 2018). The segmentation and alignment are used as supervision signals to train the content and sentence planner (Barzilay and Lapata, 2005; Angeli et al., 2010). The linguistic realization is usually implemented by template mining from the training corpus (Kondadadi et al., 2013; Oya et al., 2014). Our model adopts a similar pipeline generative process, but 2 Coarse-to-fine attention (Ling and Rush, 2017; Deng et al., 2017) was proposed for the same motivation, but they resort to reinforce"
2020.acl-main.641,D16-1128,0,0.0976118,"Missing"
2020.acl-main.641,P09-1011,0,0.708957,"- 10, 2020. 2020 Association for Computational Linguistics selected data record instead of averaging over the entire input data. This largely reduces the memory and computational costs 2 . To train the model, we do not rely on any human annotations for the segmentation and correspondence, but rather marginalize over all possibilities to maximize the likelihood of target text, which can be efficiently done within polynomial time by dynamic programming. This is essentially similar to traditional methods of inducing segmentation and alignment with semi-markov models (Daum´e III and Marcu, 2005; Liang et al., 2009). However, they make strong independence assumptions thus perform poorly as a generative model (Angeli et al., 2010). In contrast, the transition and generation in our model condition on all previously generated text. By integrating an autoregressive neural network structure, our model is able to capture unbounded dependencies while still permitting tractable inference. The training process is stable as it does not require any sampling-based approximations. We further add a soft statistical constraint to control the segmentation granularity via posterior regularization (Ganchev et al., 2010)."
2020.acl-main.641,W17-4505,0,0.107249,"ingful fragments and aligned with corresponding data records, either by handengineered rules (Kukich, 1983; McKeown, 1992) or statistical induction (Liang et al., 2009; KoncelKedziorski et al., 2014; Qin et al., 2018). The segmentation and alignment are used as supervision signals to train the content and sentence planner (Barzilay and Lapata, 2005; Angeli et al., 2010). The linguistic realization is usually implemented by template mining from the training corpus (Kondadadi et al., 2013; Oya et al., 2014). Our model adopts a similar pipeline generative process, but 2 Coarse-to-fine attention (Ling and Rush, 2017; Deng et al., 2017) was proposed for the same motivation, but they resort to reinforcement learning which is hard to train, and the performance is sacrificed for efficiency. integrates all the sub-steps into a single end-toend trainable neural architecture. It can be considered as a neural extension of the PCFG system in Konstas and Lapata (2013), with a more powerful transition probability considering inter-segment dependence and a state-of-the-art attention-based language model as the linguistic realizer. Wiseman et al. (2018) tried a similar neural generative model to induce templates. How"
2020.acl-main.641,D15-1166,0,0.071677,"k with an encode-attend-decode process (Bahdanau et al., 2015). The input X is processed into a sequence of x1 , x2 , . . . , xn , normally by flatten7156 ing the data records (Wiseman et al., 2017). The encoder encodes each xi into a vector hi . At each time step, the decoder attends to encoded vectors and outputs the probability of the next token by p(yt |y1:t−1 , At ). At is a weighted average of source vectors: X At = αt,i hi i αt,i ef (hi ,dt ) = P f (h ,d ) j t je (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 4 Approach Suppose the input data X contains a set of records r1 , r2 , ..., rK . Our assumption is that the target text y1:m can be segmented into a sequence of fragments. Each fragment corresponds to one data record. As the ground-truth segmentation and correspondence are not available, we need to enumerate over all possibilities to compute the likelihood of y1:m . Denote by Sy the set containing all valid segmentation of y1:m . For any valid segmentation s1:τs ∈ Sy , π(s1:τs ) = y1:m , where π means concatenation and τs is the number of segments. For example, let m = 5 and τs = 3. One pos"
2020.acl-main.641,N16-1086,0,0.0346329,"1 Each segment realizes one data record. ˜ 8 mark the decision order in the generation process. Introduction Data-to-text generation aims at automatically producing natural language descriptions of structured database (Reiter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specifically, we assume the target text is formed from a sequence of segments. Every segment is the result of a twostage decision: (1) Select a proper data record to be desc"
2020.acl-main.641,N19-1236,0,0.104213,"Missing"
2020.acl-main.641,D17-1238,0,0.0353651,"Missing"
2020.acl-main.641,W17-5525,0,0.117296,"Missing"
2020.acl-main.641,W99-0604,0,0.724133,"Missing"
2020.acl-main.641,W14-4407,0,0.125109,"5 → (, expensive) PriceRange 8 7 → (pub.) EatType Figure 1: Generation from our model on the E2E dataset. Decoding is performed segment-by-segment. 1 Each segment realizes one data record. ˜ 8 mark the decision order in the generation process. Introduction Data-to-text generation aims at automatically producing natural language descriptions of structured database (Reiter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specifically, we assume the"
2020.acl-main.641,P02-1040,0,0.107102,"γ = 1 (recall that K is the number of records in the input data). The intuition is that every text is expected to realize the content of all K input records. It is natural to assume every text can be roughly segmented into K fragments, each corresponding to one data record. A deviation of 7159 5 nlp.stanford.edu/data/glove.6B.zip K±1 is allowed for noisy data or text with complex structures. Metrics We measure the quality of system outputs from three perspectives: (1) word-level overlap with human references, which is a commonly used metric for text generation. We report the scores of BLEU-4 (Papineni et al., 2002), ROUGEL (Lin, 2004), Meteor (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) . (2) human evaluation. Word-level overlapping scores usually correlate rather poorly with human judgements on fluency and information accuracy (Reiter and Belz, 2009; Novikova et al., 2017a). Therefore, we passed the input data and generated text to human annotators to judge if the text is fluent by grammar (scale 1-5 as in Belz and Reiter (2006)), contains wrong fact inconsistent with input data, repeats or misses information. We report the averaged score for fluency and definite numbers for others. The"
2020.acl-main.641,D14-1162,0,0.0836455,"g of three to eight slot-value pairs. WebNLG contains 25k instances describing entities belonging to fifteen distinct DBpedia categories. The inputs are up to seven RDF triples of the form (subject, relation, object). Implementation Details We use a bi-directional LSTM encoder and uni-directional LSTM decoder for all experiments. Input data records are concatenated into a sequence and fed into the encoder. We choose the hidden size of encoder/decoder as 512 for E2E and 256 for WebNLG. The word embedding is with size 100 for both datasets and initialized with the pre-trained Glove embedding 5 (Pennington et al., 2014). We use a drop out rate of 0.3 for both the encoder and decoder. Models are trained using the Adam optimizer (Kingma and Ba, 2014) with batch size 64. The learning rate is initialized to 0.01 and decays an order of magnitude once the validation loss increases. All hyperparameters are chosen with grid search according to the validation loss. Models are implemented based on the open-source library PyTorch (Paszke et al., 2019). We set the hyperparameters in Eq. 5 as η = K, γ = 1 (recall that K is the number of records in the input data). The intuition is that every text is expected to realize t"
2020.acl-main.641,D18-1463,1,0.852952,"ision order in the generation process. Introduction Data-to-text generation aims at automatically producing natural language descriptions of structured database (Reiter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specifically, we assume the target text is formed from a sequence of segments. Every segment is the result of a twostage decision: (1) Select a proper data record to be described and (2) Generate corresponding text by paying atten"
2020.acl-main.641,W18-6557,0,0.0152044,"s. Vice versa, the number of transitions will grow if the transition module is stronger (larger embedding size). With the regularization we proposed, the granularity remains what we want regardless of the hyperparameters. We can thereby freely decide the model capacity without worrying about the difference of segmentation behavior. Results on E2E On the E2E dataset, apart from our implementations, we also compare agianst outputs from the SLUG (Juraska et al., 2018), the overall winner of the E2E challenge (seq2seqbased), DANGNT (Nguyen and Tran, 2018), the best grammar rule based model, TUDA (Puzikov and Gurevych, 2018), the best template based model, and the autoregressive neural template model (N TEMP) from Wiseman et al. (2018). SLUG uses a heuristic slot aligner based on a set of handcrafted rules and combine a complex pipeline of data augmentation, selection, model ensemble and reranker, while our model has a simple end-toend learning paradigm with no special delexicalizing, training or decoding tricks. Table 2 reports the evaluated results. Seq2seq-based models are more diverse than rule-based models at the cost of higher chances of making errors. As rule-based systems are by design always faithful to"
2020.acl-main.641,P19-1003,1,0.876916,"Missing"
2020.acl-main.641,D18-1411,0,0.0185729,"onally cheaper. Due to its fully interpretable segmental structure, it can be easily reconciled with heuristic rules or hand-engineered constraints to control the outputs. 2 Related Work Data-to-text generation is traditionally dealt with using a pipeline structure containing content planning, sentence planning and linguistic realization (Reiter and Dale, 1997). Each target text is split into meaningful fragments and aligned with corresponding data records, either by handengineered rules (Kukich, 1983; McKeown, 1992) or statistical induction (Liang et al., 2009; KoncelKedziorski et al., 2014; Qin et al., 2018). The segmentation and alignment are used as supervision signals to train the content and sentence planner (Barzilay and Lapata, 2005; Angeli et al., 2010). The linguistic realization is usually implemented by template mining from the training corpus (Kondadadi et al., 2013; Oya et al., 2014). Our model adopts a similar pipeline generative process, but 2 Coarse-to-fine attention (Ling and Rush, 2017; Deng et al., 2017) was proposed for the same motivation, but they resort to reinforcement learning which is hard to train, and the performance is sacrificed for efficiency. integrates all the sub-"
2020.acl-main.641,W18-6535,0,0.0191154,"ore than once (except for the null record). 3. The generation will not finish until all data records have been realized. Constraint 2 and 3 directly address the information repetition and missing problem. When segments are incrementally generated, the constraints will be checked against for validity. Note that adding the constraints hardly incur any cost, the decoding process is still finished in one pass. No post-processing or reranking is needed. 3 The finer-grained segmentation might be useful if the focus is on modeling the detailed discourse structure instead of the information accuracy (Reed et al., 2018; Balakrishnan et al., 2019), which we leave for future work. 4 We can also utilize some heuristic rules to help segmentation. For example, we can prevent breaking syntactic elements obtained from an external parser (Yang et al., 2019) or match entity names with handcrafted rules (Chen et al., 2018). The interpretability of the segmental structure allows easy combination with these rules. We focus on a general domain-agnostic method in this paper, though heuristic rules might bring further improvement under certain cases. Computational Complexity Suppose the input data has M records and each r"
2020.acl-main.641,J09-4008,0,0.0234784,"one data record. A deviation of 7159 5 nlp.stanford.edu/data/glove.6B.zip K±1 is allowed for noisy data or text with complex structures. Metrics We measure the quality of system outputs from three perspectives: (1) word-level overlap with human references, which is a commonly used metric for text generation. We report the scores of BLEU-4 (Papineni et al., 2002), ROUGEL (Lin, 2004), Meteor (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) . (2) human evaluation. Word-level overlapping scores usually correlate rather poorly with human judgements on fluency and information accuracy (Reiter and Belz, 2009; Novikova et al., 2017a). Therefore, we passed the input data and generated text to human annotators to judge if the text is fluent by grammar (scale 1-5 as in Belz and Reiter (2006)), contains wrong fact inconsistent with input data, repeats or misses information. We report the averaged score for fluency and definite numbers for others. The human is conducted on a sampled subset from the test data. To ensure the subset covers inputs with all possible number of records (K ∈ [3, 8] for E2E and K ∈ [1, 7] for WebNLG), we sample 20 instances for every possible K. Finally,we obtain 120 test cases"
2020.acl-main.641,P17-1099,0,0.0264398,"d dashed arrows indicate the generation model. Every segment so is generated by attending only to the corresponding data record c(so ). can only pay attention to its corresponding data record. The attention scores of other records are masked out when decoding so : ef (hi ,dt ) 1(xi ∈ c(so )) αt,i = P f (h ,d ) j t 1(x ∈ c(s )) j o je where 1 is the indicator function. This forces the model to learn proper correspondences and enhances the connection between each segment and the data record it describes. Following the common practice, we define the output probability with the pointer generator (See et al., 2017; Wiseman et al., 2017): pgen = σ(MLPg ([dt ◦ At ])) pvocab = softmax(W1 dt + W2 At ) pθ (yt |y<t ) = pgen pvocab (yt ) X + (1 − pgen ) at,i i:yt =xi dt is the decoder’s hidden state at time step t. ◦ denotes vector concatenation. At is the context vector. MLP indicates multi-layer perceptron and σ normalizes the score between (0, 1). W1 and W2 are trainable matrices. pgen is the probability that the word is generated from a fixed vocabulary distribution pvocab instead of being copied. The final decoding probability pθ (yt ) is marginalized over pvocab and the copy distribution. The generation"
2020.acl-main.641,D19-1054,1,0.911309,"ter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specifically, we assume the target text is formed from a sequence of segments. Every segment is the result of a twostage decision: (1) Select a proper data record to be described and (2) Generate corresponding text by paying attention only to the selected data record. This decision is repeated until all desired records have been realized. Figure 1 illustrates this process. Compared with neural"
2020.acl-main.641,D19-1390,1,0.917236,"ter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specifically, we assume the target text is formed from a sequence of segments. Every segment is the result of a twostage decision: (1) Select a proper data record to be described and (2) Generate corresponding text by paying attention only to the selected data record. This decision is repeated until all desired records have been realized. Figure 1 illustrates this process. Compared with neural"
2020.acl-main.641,P18-2060,0,0.0152788,"tion. Our model is also in spirit related to recent attempts at separating content planning and surface realization in neural data-to-text models (Zhao et al., 2018; Puduppully et al., 2019; Moryossef et al., 2019; Ferreira et al., 2019). Nonetheless, all of them resort to manual annotations or hand-engineered rules applicable only for a narrow domain. Our model, instead, automatically learn the optimal content planning via exploring over exponentially many segmentation/correspondence possibilities. There have been quite a few neural alignment models applied to tasks like machine translation (Wang et al., 2018; Deng et al., 2018), character transduction (Wu et al., 2018; Shankar and Sarawagi, 2019) and summarization (Yu et al., 2016; Shen et al., 2019b). Unlike word-to-word alignment, we focus on learning the alignment between data records and text segments. Some works also integrate neural language models to jointly learn the segmentation and correspondence, e.g., phrase-based machine translation (Huang et al., 2018), speech recognition (Wang et al., 2017) and vision-grounded word segmentation (Kawakami et al., 2019). Data-to-text naturally fits into this scenario since each data record is normall"
2020.acl-main.641,D18-1356,0,0.417144,"ructured database (Reiter and Dale, 1997). Traditional statistical methods usually tackle this problem by breaking the generation process into a set of local decisions that are learned separately (Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Oya et al., 2014). Recently, neural attention models conflate all steps into a single end-to-end system and largely simplify the training process (Mei et al., 2016; Lebret et al., 2016; Shen et al., 2017; Su et al., 2018, 2019; Chang et al., 2020). However, the black-box conflation also renders the generation uninterpretable and hard to control (Wiseman et al., 2018; Shen et al., 2019a). Verifying the generation correctness in a principled way is nontrivial. In practice, it often suffers from the problem In this work, we propose to explicitly exploit the segmental structure of text. Specifically, we assume the target text is formed from a sequence of segments. Every segment is the result of a twostage decision: (1) Select a proper data record to be described and (2) Generate corresponding text by paying attention only to the selected data record. This decision is repeated until all desired records have been realized. Figure 1 illustrates this process. Co"
2020.acl-main.641,D18-1473,0,0.0450041,"Missing"
2020.acl-main.641,D19-1197,0,0.0253329,"ly generated, the constraints will be checked against for validity. Note that adding the constraints hardly incur any cost, the decoding process is still finished in one pass. No post-processing or reranking is needed. 3 The finer-grained segmentation might be useful if the focus is on modeling the detailed discourse structure instead of the information accuracy (Reed et al., 2018; Balakrishnan et al., 2019), which we leave for future work. 4 We can also utilize some heuristic rules to help segmentation. For example, we can prevent breaking syntactic elements obtained from an external parser (Yang et al., 2019) or match entity names with handcrafted rules (Chen et al., 2018). The interpretability of the segmental structure allows easy combination with these rules. We focus on a general domain-agnostic method in this paper, though heuristic rules might bring further improvement under certain cases. Computational Complexity Suppose the input data has M records and each record contains N tokens. The computational complexity for neural attention models is O(M N ) at each decoding step where the whole input is retrieved. Our model, similar to chunkwise attention (Chiu and Raffel, 2018) or coarse-to-fine"
2020.acl-main.641,D16-1138,0,0.0624568,"ata-to-text models (Zhao et al., 2018; Puduppully et al., 2019; Moryossef et al., 2019; Ferreira et al., 2019). Nonetheless, all of them resort to manual annotations or hand-engineered rules applicable only for a narrow domain. Our model, instead, automatically learn the optimal content planning via exploring over exponentially many segmentation/correspondence possibilities. There have been quite a few neural alignment models applied to tasks like machine translation (Wang et al., 2018; Deng et al., 2018), character transduction (Wu et al., 2018; Shankar and Sarawagi, 2019) and summarization (Yu et al., 2016; Shen et al., 2019b). Unlike word-to-word alignment, we focus on learning the alignment between data records and text segments. Some works also integrate neural language models to jointly learn the segmentation and correspondence, e.g., phrase-based machine translation (Huang et al., 2018), speech recognition (Wang et al., 2017) and vision-grounded word segmentation (Kawakami et al., 2019). Data-to-text naturally fits into this scenario since each data record is normally verbalized in one continuous text segment. 3 Background: Data-to-Text Let X, Y denote a source-target pair. X is structured"
2020.acl-main.641,W04-1013,0,\N,Missing
2020.acl-main.641,D17-1239,0,\N,Missing
2020.blackboxnlp-1.29,N16-3020,0,0.0411319,"his entire setup can be placed in an environment responsible for learning models about the individual recipients’ preferences. For each query, multiple explanations will be generated and ranked by the learning environment. Based on recipient’s choice, the corresponding model will be refined for future queries. Each component of this architecture brings forward a different challenge. The most intriguing ones are from the section of content determination and the learning environment. Feature Importance Extraction has been researched vividly in recent years with black box solutions such as LIME (Ribeiro et al., 2016a), LEAFAGE (Adhikari, 320 Figure 3: A possible architecture of a generic explanation system based on the proposed definition. This is a slightly modified setup of the classical NLG pipeline. This setup will be placed in an iterative learning environment for refining individual recipients’ models. 2018), XEMP 2 , etc. However, very little attention has been given to bridging the gap between black box inputs and human interpretable features in a generic way. 5 The suggested architecture enables us to partly mimic Overton’s structure of explanations and Malle’s structure of social explanations ("
2020.blackboxnlp-1.7,W19-4825,0,0.0454661,"Missing"
2020.blackboxnlp-1.7,W16-2524,0,0.0195974,"suggest that changes in probing performance can not exclusively be attributed to an improved or deteriorated encoding of linguistic knowledge and should be carefully interpreted. We present further evidence for this interpretation by investigating changes in the attention distribution and language modeling capabilities of fine-tuned models which constitute alternative explanations for changes in probing accuracy. 2 Related Work Probing A large body of previous work focuses on analyses of the internal representations of neural models and the linguistic knowledge they encode (Shi et al., 2016; Ettinger et al., 2016; Adi et al., 2016; Belinkov et al., 2017; Hupkes et al., 2018). In a similar spirit to these first works on probing, Conneau et al. (2018) were the first to compare different sentence embedding methods for the linguistic knowledge they encode. Krasnowska-Kiera´s and Wr´oblewska (2019) extended this approach to study sentence-level probing tasks on English and Polish sentences. Alongside sentence-level probing, many recent works (Peters et al., 2018; Liu et al., 2019a; Tenney et al., 2019b; Lin et al., 2019; Hewitt and Manning, 2019) have focused on token-level probing tasks investigating more"
2020.blackboxnlp-1.7,N19-1112,0,0.365506,"asure linguistic knowledge encoded by a model for two reasons: 1) during fine-tuning we explicitly train a model to represent sentence-level context in its representations and 2) we are interested in the extent to which this affects existing sentence-level linguistic knowledge already present in a pre-trained model. We find that while, indeed, fine-tuning affects a model’s sentence-level probing accuracy and these effects are typically larger for higher layers, changes in probing accuracy vary dependIntroduction Transformer-based contextual embeddings like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and ALBERT (Lan et al., 2020) recently became the state-of-the-art on a variety of NLP downstream tasks. These models are pre-trained on large amounts of text and subsequently fine-tuned on task-specific, supervised downstream tasks. Their strong empirical performance triggered questions concerning the linguistic knowledge they encode 68 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 68–82 c Online, November 20, 2020. 2020 Association for Computational Linguistics Fine-tuning While fine-tuning pre-trained language models leads to a"
2020.blackboxnlp-1.7,W07-1401,0,0.0252945,"crease or decrease in probing performance.1 However, we argue that encoding or forgetting linguistic knowledge is not necessarily the only explanation for observed changes in probing accuracy. Hence, the goal of our work is to test the abovestated hypotheses assessing the interaction between fine-tuning and probing tasks across three different encoder models. 3.1 RTE The Recognizing Textual Entailment dataset is a collection of sentence-pairs in either neutral or entailment relationship collected from a series of annual textual entailment challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The task requires a deeper understanding of the relationship of two sentences, hence, fine-tuning on RTE might affect the accuracy on a discourse-level probing task. SQuAD The Stanford Questions Answering Dataset (Rajpurkar et al., 2016) is a popular extractive reading comprehension dataset. The task involves a broader discourse understanding as a model trained on SQuAD is required to extract the answer to a question from an accompanying paragraph. 3.2 Probing Tasks We select three sentence-level probing tasks from the SentEval probing suit (Conneau et al., 2018), t"
2020.blackboxnlp-1.7,D19-1275,0,0.0155272,"be found in Table 1. coordination-inversion is a collection of sentences made out of two coordinate clauses. In half of the sentences, the order of the clauses is inverted. Coordinate-inversion tests for a model’s broader discourse understanding. 3.3 Pre-trained Models It is unclear to which extent findings on the encoding of certain linguistic phenomena generalize from one pre-trained model to another. Hence, we examine three different pre-trained encoder models in our experiments. Probing For probing, our setup largely follows that of previous works (Tenney et al., 2019b; Liu et al., 2019a; Hewitt and Liang, 2019) where a probing classifier is trained on top of the contextualized embeddings extracted from a pre-trained or – as in our case – fine-tuned encoder model. Notably, we train linear (logistic regression) probing classifiers and use two different pooling methods to obtain sentence embeddings from the encoder hidden states: CLS-pooling, which simply returns the hidden state corresponding to the first token of the sentence and mean-pooling which computes a sentence embedding as the mean over all hidden states. We do this to assess the extent to which the CLS token captures sentence-level context."
2020.blackboxnlp-1.7,2021.ccl-1.108,0,0.103032,"Missing"
2020.blackboxnlp-1.7,S19-1026,0,0.0451437,"Missing"
2020.blackboxnlp-1.7,N18-1202,0,0.0460674,"f previous work focuses on analyses of the internal representations of neural models and the linguistic knowledge they encode (Shi et al., 2016; Ettinger et al., 2016; Adi et al., 2016; Belinkov et al., 2017; Hupkes et al., 2018). In a similar spirit to these first works on probing, Conneau et al. (2018) were the first to compare different sentence embedding methods for the linguistic knowledge they encode. Krasnowska-Kiera´s and Wr´oblewska (2019) extended this approach to study sentence-level probing tasks on English and Polish sentences. Alongside sentence-level probing, many recent works (Peters et al., 2018; Liu et al., 2019a; Tenney et al., 2019b; Lin et al., 2019; Hewitt and Manning, 2019) have focused on token-level probing tasks investigating more recent contextualized embedding models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), and BERT (Devlin et al., 2019). Two of the most prominent works following this methodology are Liu et al. (2019a) and Tenney et al. (2019b). While Liu et al. (2019a) use linear probing classifiers as we do, Tenney et al. (2019b) use more expressive, non-linear classifiers. However, in contrast to our work, most studies that investigate pre-trained"
2020.blackboxnlp-1.7,P19-1573,0,0.0515376,"Missing"
2020.blackboxnlp-1.7,2020.acl-main.467,0,0.0196575,"e, November 20, 2020. 2020 Association for Computational Linguistics Fine-tuning While fine-tuning pre-trained language models leads to a strong empirical performance across various supervised NLP downstream tasks (Wang et al., 2019b), fine-tuning itself (Dodge et al., 2020) and its effects on the representations learned by a pre-trained model are poorly understood. As an example, Phang et al. (2018) show that downstream accuracy can benefit from an intermediate fine-tuning task, but leave the investigation of why certain tasks benefit from intermediate task training to future work. Recently, Pruksachatkun et al. (2020) extended this approach using eleven diverse intermediate fine-tuning tasks. They view probing task performance after finetuning as an indicator of the acquisition of a particular language skill during intermediate task finetuning. This is similar to our work in the sense that probing accuracy is used to understand how finetuning affects a pre-trained model. Talmor et al. (2019) try to understand whether the performance on downstream tasks should be attributed to the pre-trained representations or rather the fine-tuning process itself. They fine-tune BERT and RoBERTa on a large set of symbolic"
2020.blackboxnlp-1.7,P19-1439,0,0.0478689,"Missing"
2020.blackboxnlp-1.7,D16-1264,0,0.0394904,"ng the interaction between fine-tuning and probing tasks across three different encoder models. 3.1 RTE The Recognizing Textual Entailment dataset is a collection of sentence-pairs in either neutral or entailment relationship collected from a series of annual textual entailment challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The task requires a deeper understanding of the relationship of two sentences, hence, fine-tuning on RTE might affect the accuracy on a discourse-level probing task. SQuAD The Stanford Questions Answering Dataset (Rajpurkar et al., 2016) is a popular extractive reading comprehension dataset. The task involves a broader discourse understanding as a model trained on SQuAD is required to extract the answer to a question from an accompanying paragraph. 3.2 Probing Tasks We select three sentence-level probing tasks from the SentEval probing suit (Conneau et al., 2018), testing for syntactic, semantic and broader discourse information on the sentence-level. Fine-tuning tasks We study three fine-tuning tasks taken from the GLUE benchmark (Wang et al., 2019b). All the tasks are sentence-level classification tasks and cover different"
2020.blackboxnlp-1.7,2020.tacl-1.54,0,0.0242377,"d Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers Marius Mosbach Anna Khokhlova Michael A. Hedderich Dietrich Klakow Spoken Language Systems (LSV) Department of Language Science and Technology Saarland Informatics Campus, Saarland University, Germany {mmosbach,akhokhlova,mhedderich,dklakow}@lsv.uni-saarland.de Abstract in their representations and how it is affected by the training objective and model architecture (Kim et al., 2019; Wang et al., 2019a). One prominent technique to gain insights about the linguistic knowledge encoded in pre-trained models is probing (Rogers et al., 2020). However, works on probing have so far focused mostly on pre-trained models. It is still unclear how the representations of a pre-trained model change when fine-tuning on a downstream task. Further, little is known about whether and to what extent this process adds or removes linguistic knowledge from a pre-trained model. Addressing these issues, we are investigating the following questions: Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by"
2020.blackboxnlp-1.7,D16-1159,0,0.011312,"thod. Our findings suggest that changes in probing performance can not exclusively be attributed to an improved or deteriorated encoding of linguistic knowledge and should be carefully interpreted. We present further evidence for this interpretation by investigating changes in the attention distribution and language modeling capabilities of fine-tuned models which constitute alternative explanations for changes in probing accuracy. 2 Related Work Probing A large body of previous work focuses on analyses of the internal representations of neural models and the linguistic knowledge they encode (Shi et al., 2016; Ettinger et al., 2016; Adi et al., 2016; Belinkov et al., 2017; Hupkes et al., 2018). In a similar spirit to these first works on probing, Conneau et al. (2018) were the first to compare different sentence embedding methods for the linguistic knowledge they encode. Krasnowska-Kiera´s and Wr´oblewska (2019) extended this approach to study sentence-level probing tasks on English and Polish sentences. Alongside sentence-level probing, many recent works (Peters et al., 2018; Liu et al., 2019a; Tenney et al., 2019b; Lin et al., 2019; Hewitt and Manning, 2019) have focused on token-level probing t"
2020.blackboxnlp-1.7,D13-1170,0,0.00570702,"task which tests a model’s knowledge of grammatical concepts. We expect that fine-tuning on CoLA results in changes in accuracy on a syntactic probing task.2 Table 1: Fine-tuning performance on the development set on selected down-stream tasks. For comparison we also report the fine-tuning accuracy of BERT-basecased as reported by Devlin et al. (2019) on the test set of each of the tasks taken from the GLUE and SQuAD leaderboards. We report Matthews correlation coefficient for CoLA, accuracy for SST-2 and RTE, and exact match (EM) and F1 score for SQuAD. SST-2 The Stanford Sentiment Treebank (Socher et al., 2013). We use the binary version where the task is to categorize movie reviews to have either positive or negative valence. Making sentiment judgments requires knowing the meanings of isolated words and combining them on the sentence and discourse level (e.g. in case of irony). Hence, we expect to see a difference for semantic and/or discourse probing tasks when fine-tuning on SST-2. to focus more on syntactic, semantic or discourse information. The extent to which knowledge of a particular linguistic level is needed to perform well differs from task to task. For instance, to judge if the syntactic"
2020.emnlp-demos.8,W16-4011,0,0.0503598,"Missing"
2020.emnlp-demos.8,S19-2010,0,0.0211219,"N) follows a highly modular concept, which makes it easy to adapt to a specific annotation scenario. It uses an internal deterministic state machine (DSM) to guide the annotator through the pre-defined annotation task(s). This usage of a DSM allows annotation tasks to be chained in any order needed and makes it easy to implement entirely new annotation tasks and custom features in the future. This modular nature is especially useful when single task annotations do not capture the reality of a problem or when several dependencies exist in the annotations. One example being hate speech corpora (Zampieri et al., 2019; Struß et al., A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phenomenon is easily captured by a single task, the high specialisation of most annotation tools can result in having to switch to another tool if the task only slightly changes. We introduce HUMAN, a novel web-based annotation tool that addresses the above problems by a) covering a variety of annotation tasks on both textual and image"
2020.emnlp-demos.8,E17-3004,0,0.0217006,"e that simplifies the annotation task and management. 1 Introduction Access to suitable annotated data constitutes a fundamental prerequisite for R&D of machine learning algorithms and models. The demand for new annotated data is growing as new data is collected or existing data is being re-annotated from a new perspective. In the area of natural language processing (NLP) alone, there is a large variety of tasks that require different types of annotations to be covered by a tool. This includes named-entity recognition or part-of-speech tagging, which require a tool to cover sequence labeling (Kiesel et al., 2017), coreference and dependency parsing requiring relational annotations (Stenetorp et al., 2012; Eckart de Castilho et al., 2016; Shindo et al., 2018), or any ∗ 1 https://www.lighttag.io https://prodi.gy/ 3 https://github.com/Tarlanc/angrist 2 Equal contribution. 55 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 55–61 c November 16-20, 2020. 2020 Association for Computational Linguistics Administrators 2019), where the target of hate is only supposed to be annotated if a comment has been previously annotated as hateful. Further, HUMAN covers a variety of annotation tasks, includin"
2020.emnlp-demos.8,L18-1175,0,0.0139677,"D of machine learning algorithms and models. The demand for new annotated data is growing as new data is collected or existing data is being re-annotated from a new perspective. In the area of natural language processing (NLP) alone, there is a large variety of tasks that require different types of annotations to be covered by a tool. This includes named-entity recognition or part-of-speech tagging, which require a tool to cover sequence labeling (Kiesel et al., 2017), coreference and dependency parsing requiring relational annotations (Stenetorp et al., 2012; Eckart de Castilho et al., 2016; Shindo et al., 2018), or any ∗ 1 https://www.lighttag.io https://prodi.gy/ 3 https://github.com/Tarlanc/angrist 2 Equal contribution. 55 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 55–61 c November 16-20, 2020. 2020 Association for Computational Linguistics Administrators 2019), where the target of hate is only supposed to be annotated if a comment has been previously annotated as hateful. Further, HUMAN covers a variety of annotation tasks, including the often lacking multi-label annotations and document-level classification, but also sequence labeling on textual data as well as image labeling;"
2020.emnlp-demos.8,E12-2021,0,0.134038,"Missing"
2020.emnlp-main.204,2020.lrec-1.335,1,0.804429,"Missing"
2020.emnlp-main.204,D19-1031,0,0.0215534,"ithout having to label every instance manually. The expert can, e.g. create a set of rules that are then used to label the data automatically (Ratner et al., 2020) or information from an external knowledge source can be used (Rijhwani et al., 2020). This kind of (semi-) automatic supervision tends to contain more errors which can hurt the performance of classifiers (see e.g. (Fang and Cohn, 2016)). To avoid this, it can be combined with label noise handling techniques. This pipeline has been shown to be effective for several NLP tasks (Lange et al., 2019; Paul et al., 2019; Wang et al., 2019; Chen et al., 2019; Mayhew et al., 2019), however, mostly for RNN based approaches. As we have seen in Section 4 that these have a lower baseline performance, we are interested in whether distant supervision is still useful for the better performing transformer models. Several of the past works evaluated their approach only on 2582 noise handling, combining the confusion matrix with the smoothing approach might be beneficial because the estimated confusion matrix is flawed when only small amounts of labeled data are given. When more manually labeled data is available, the noisy annotations lose their benefit an"
2020.emnlp-main.204,P19-3031,0,0.0445541,"Missing"
2020.emnlp-main.204,D14-1179,0,0.0560279,"Missing"
2020.emnlp-main.204,P19-4007,0,0.0309385,"Missing"
2020.emnlp-main.204,N19-3005,1,0.812867,"ain expert to insert their knowledge without having to label every instance manually. The expert can, e.g. create a set of rules that are then used to label the data automatically (Ratner et al., 2020) or information from an external knowledge source can be used (Rijhwani et al., 2020). This kind of (semi-) automatic supervision tends to contain more errors which can hurt the performance of classifiers (see e.g. (Fang and Cohn, 2016)). To avoid this, it can be combined with label noise handling techniques. This pipeline has been shown to be effective for several NLP tasks (Lange et al., 2019; Paul et al., 2019; Wang et al., 2019; Chen et al., 2019; Mayhew et al., 2019), however, mostly for RNN based approaches. As we have seen in Section 4 that these have a lower baseline performance, we are interested in whether distant supervision is still useful for the better performing transformer models. Several of the past works evaluated their approach only on 2582 noise handling, combining the confusion matrix with the smoothing approach might be beneficial because the estimated confusion matrix is flawed when only small amounts of labeled data are given. When more manually labeled data is available, the n"
2020.emnlp-main.204,P19-1493,0,0.0183213,"en-data (zero-shot) RCNN 300 700 1340 number of clean sentences (b) Topic Class. Yor`ub´a test F1 0.8 0.7 0.6 0.5 XLM-R XLM-R + en-data (few-shot) XLM-R + en-data (zero-shot) 10 100 250 400 650 800 1014 number of clean sentences 6 (c) Transfer Learn NER Hausa Figure 1: Comparing to RNNs (a+b) and using transfer learning (b+c). Additional plots in the Appendix. tionally, the multilingual models can be fine-tuned on task-specific, supervised data but from a different, high-resource language. There is evidence that the multilingual transformer models can learn parallel concepts across languages (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). This allows to then apply or evaluate the model directly without having been fine-tuned on any labeled data in the target language (zero-shot) or on only a small amount of labeled data in the target language (fewshot). For NER, we pre-train on the English CoNLL03 NER dataset (Tjong Kim Sang and De Meulder, 2003). For topic classification, the models are preDistant Supervision Distant and weak supervision are popular techniques when labeled data is lacking. It allows a domain expert to insert their knowledge without having to label every instance manuall"
2020.emnlp-main.204,2020.acl-main.722,0,0.0198632,"t) or on only a small amount of labeled data in the target language (fewshot). For NER, we pre-train on the English CoNLL03 NER dataset (Tjong Kim Sang and De Meulder, 2003). For topic classification, the models are preDistant Supervision Distant and weak supervision are popular techniques when labeled data is lacking. It allows a domain expert to insert their knowledge without having to label every instance manually. The expert can, e.g. create a set of rules that are then used to label the data automatically (Ratner et al., 2020) or information from an external knowledge source can be used (Rijhwani et al., 2020). This kind of (semi-) automatic supervision tends to contain more errors which can hurt the performance of classifiers (see e.g. (Fang and Cohn, 2016)). To avoid this, it can be combined with label noise handling techniques. This pipeline has been shown to be effective for several NLP tasks (Lange et al., 2019; Paul et al., 2019; Wang et al., 2019; Chen et al., 2019; Mayhew et al., 2019), however, mostly for RNN based approaches. As we have seen in Section 4 that these have a lower baseline performance, we are interested in whether distant supervision is still useful for the better performing"
2020.emnlp-main.204,L16-1521,0,0.0261579,"Missing"
2020.emnlp-main.204,W19-6808,0,0.053317,"Missing"
2020.emnlp-main.204,D19-1655,0,0.0165418,"t their knowledge without having to label every instance manually. The expert can, e.g. create a set of rules that are then used to label the data automatically (Ratner et al., 2020) or information from an external knowledge source can be used (Rijhwani et al., 2020). This kind of (semi-) automatic supervision tends to contain more errors which can hurt the performance of classifiers (see e.g. (Fang and Cohn, 2016)). To avoid this, it can be combined with label noise handling techniques. This pipeline has been shown to be effective for several NLP tasks (Lange et al., 2019; Paul et al., 2019; Wang et al., 2019; Chen et al., 2019; Mayhew et al., 2019), however, mostly for RNN based approaches. As we have seen in Section 4 that these have a lower baseline performance, we are interested in whether distant supervision is still useful for the better performing transformer models. Several of the past works evaluated their approach only on 2582 noise handling, combining the confusion matrix with the smoothing approach might be beneficial because the estimated confusion matrix is flawed when only small amounts of labeled data are given. When more manually labeled data is available, the noisy annotations lo"
2020.emnlp-main.204,D19-1077,0,0.0200812,"RCNN 300 700 1340 number of clean sentences (b) Topic Class. Yor`ub´a test F1 0.8 0.7 0.6 0.5 XLM-R XLM-R + en-data (few-shot) XLM-R + en-data (zero-shot) 10 100 250 400 650 800 1014 number of clean sentences 6 (c) Transfer Learn NER Hausa Figure 1: Comparing to RNNs (a+b) and using transfer learning (b+c). Additional plots in the Appendix. tionally, the multilingual models can be fine-tuned on task-specific, supervised data but from a different, high-resource language. There is evidence that the multilingual transformer models can learn parallel concepts across languages (Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020). This allows to then apply or evaluate the model directly without having been fine-tuned on any labeled data in the target language (zero-shot) or on only a small amount of labeled data in the target language (fewshot). For NER, we pre-train on the English CoNLL03 NER dataset (Tjong Kim Sang and De Meulder, 2003). For topic classification, the models are preDistant Supervision Distant and weak supervision are popular techniques when labeled data is lacking. It allows a domain expert to insert their knowledge without having to label every instance manually. The expert can, e."
2020.findings-emnlp.227,S19-1026,0,0.0538857,"Missing"
2020.findings-emnlp.227,P19-1573,0,0.0490022,"Missing"
2020.findings-emnlp.227,W19-4825,0,0.0470604,"Missing"
2020.findings-emnlp.227,N19-1112,0,0.415546,"ery few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of finetuning on probing require a careful interpretation. 1 1. How and where does fine-tuning affect the representations of a pre-trained model? 2. To which extent (if at all) can changes in probing accuracy be attributed to a change in linguistic knowledge encoded by the model? Introduction Transformer-based contextual embeddings like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and ALBERT (Lan et al., 2020) recently became the state-of-the-art on a variety of NLP downstream tasks. These models are pre-trained on large amounts of text and subsequently fine-tuned on task-specific, supervised downstream tasks. Their strong empirical performance triggered questions concerning the linguistic knowledge they encode To answer these questions, we investigate three different pre-trained encoder models, BERT, RoBERTa, and ALBERT. We fine-tune them on sentence-level classification tasks from the GLUE benchmark (Wang et al., 2019b) and evaluate the linguistic knowledge they en"
2020.findings-emnlp.227,2021.ccl-1.108,0,0.175957,"Missing"
2020.findings-emnlp.227,2020.blackboxnlp-1.4,0,0.111561,"ask finetuning. This is similar to our work in the sense that probing accuracy is used to understand how finetuning affects a pre-trained model. Talmor et al. (2019) try to understand whether the performance on downstream tasks should be attributed to the pre-trained representations or rather the fine-tuning process itself. They fine-tune BERT and RoBERTa on a large set of symbolic reasoning tasks and find that while RoBERTa generally outperforms BERT in its reasoning abilities, the performance of both models is highly context dependent. Most similar to our work is the contemporaneous work by Merchant et al. (2020). They investigate how fine-tuning leads to changes in the representations of a pre-trained model. In contrast to our work, their focus, however, lies on edgeprobing (Tenney et al., 2019b) and structural probing tasks (Hewitt and Manning, 2019) and they study only a single pre-trained encoder: BERT. We consider our work complementary to them since we study sentence-level probing tasks, use different analysis methods and investigate the impact of fine-tuning on three different pre-trained encoders: BERT, RoBERTa, and ALBERT. 3 Methodology and Setup The focus of our work is on studying how finet"
2020.findings-emnlp.227,N18-1202,0,0.048683,"f previous work focuses on analyses of the internal representations of neural models and the linguistic knowledge they encode (Shi et al., 2016; Ettinger et al., 2016; Adi et al., 2016; Belinkov et al., 2017; Hupkes et al., 2018). In a similar spirit to these first works on probing, Conneau et al. (2018) were the first to compare different sentence embedding methods for the linguistic knowledge they encode. Krasnowska-Kiera´s and Wr´oblewska (2019) extended this approach to study sentence-level probing tasks on English and Polish sentences. Alongside sentence-level probing, many recent works (Peters et al., 2018; Liu et al., 2019a; Tenney et al., 2019b; Lin et al., 2019; Hewitt and Manning, 2019) have focused on token-level probing tasks investigating more recent contextualized embedding models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), and BERT (Devlin et al., 2019). Two of the most prominent works following this methodology are Liu et al. (2019a) and Tenney et al. (2019b). While Liu et al. (2019a) use linear probing classifiers as we do, Tenney et al. (2019b) use more expressive, non-linear classifiers. However, in contrast to our work, most studies that investigate pre-trained"
2020.findings-emnlp.227,2020.tacl-1.54,0,0.0239466,"d Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers Marius Mosbach Anna Khokhlova Michael A. Hedderich Dietrich Klakow Spoken Language Systems (LSV) Department of Language Science and Technology Saarland Informatics Campus, Saarland University, Germany {mmosbach,akhokhlova,mhedderich,dklakow}@lsv.uni-saarland.de Abstract in their representations and how it is affected by the training objective and model architecture (Kim et al., 2019; Wang et al., 2019a). One prominent technique to gain insights about the linguistic knowledge encoded in pre-trained models is probing (Rogers et al., 2020). However, works on probing have so far focused mostly on pre-trained models. It is still unclear how the representations of a pre-trained model change when fine-tuning on a downstream task. Further, little is known about whether and to what extent this process adds or removes linguistic knowledge from a pre-trained model. Addressing these issues, we are investigating the following questions: Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by"
2020.findings-emnlp.227,D16-1159,0,0.0113746,"thod. Our findings suggest that changes in probing performance can not exclusively be attributed to an improved or deteriorated encoding of linguistic knowledge and should be carefully interpreted. We present further evidence for this interpretation by investigating changes in the attention distribution and language modeling capabilities of fine-tuned models which constitute alternative explanations for changes in probing accuracy. 2 Related Work Probing A large body of previous work focuses on analyses of the internal representations of neural models and the linguistic knowledge they encode (Shi et al., 2016; Ettinger et al., 2016; Adi et al., 2016; Belinkov et al., 2017; Hupkes et al., 2018). In a similar spirit to these first works on probing, Conneau et al. (2018) were the first to compare different sentence embedding methods for the linguistic knowledge they encode. Krasnowska-Kiera´s and Wr´oblewska (2019) extended this approach to study sentence-level probing tasks on English and Polish sentences. Alongside sentence-level probing, many recent works (Peters et al., 2018; Liu et al., 2019a; Tenney et al., 2019b; Lin et al., 2019; Hewitt and Manning, 2019) have focused on token-level probing t"
2020.findings-emnlp.227,D13-1170,0,0.00785855,"soning. They find that fine-tuning on dependency parsing task leads to an improvement on the constituents probing task and attribute this to the improved linguistic knowledge. Similarly, Pruksachatkun et al. (2020) view probing task performance as “an indicator for the acquisition of a particular language skill.” CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is an acceptability task which tests a model’s knowledge of grammatical concepts. We expect that fine-tuning on CoLA results in changes in accuracy on a syntactic probing task.2 SST-2 The Stanford Sentiment Treebank (Socher et al., 2013). We use the binary version where the task is to categorize movie reviews to have either positive or negative valence. Making sentiment judgments requires knowing the meanings of isolated words and combining them on the sentence and discourse level (e.g. in case of irony). Hence, we expect to see a difference for semantic and/or discourse probing tasks when fine-tuning on SST-2. RTE The Recognizing Textual Entailment dataset is a collection of sentence-pairs in either neutral or entailment relationship collected from a series of annual textual entailment challenges (Dagan et al., 2005; Bar-Hai"
2020.findings-emnlp.227,P19-1452,0,0.394936,"he internal representations of neural models and the linguistic knowledge they encode (Shi et al., 2016; Ettinger et al., 2016; Adi et al., 2016; Belinkov et al., 2017; Hupkes et al., 2018). In a similar spirit to these first works on probing, Conneau et al. (2018) were the first to compare different sentence embedding methods for the linguistic knowledge they encode. Krasnowska-Kiera´s and Wr´oblewska (2019) extended this approach to study sentence-level probing tasks on English and Polish sentences. Alongside sentence-level probing, many recent works (Peters et al., 2018; Liu et al., 2019a; Tenney et al., 2019b; Lin et al., 2019; Hewitt and Manning, 2019) have focused on token-level probing tasks investigating more recent contextualized embedding models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), and BERT (Devlin et al., 2019). Two of the most prominent works following this methodology are Liu et al. (2019a) and Tenney et al. (2019b). While Liu et al. (2019a) use linear probing classifiers as we do, Tenney et al. (2019b) use more expressive, non-linear classifiers. However, in contrast to our work, most studies that investigate pre-trained contextualized embedding models focus o"
2020.findings-emnlp.227,P19-1439,0,0.0385157,"Missing"
2020.insights-1.8,W18-4411,0,0.0593078,"Missing"
2020.insights-1.8,S19-2082,0,0.0402489,"Missing"
2020.insights-1.8,S19-2009,0,0.0197217,"data points close to each other tend to have a similar label. These algorithms rely on the representation of data points to create a distance graph which captures their proximity. Recently, pre-trained word embeddings such as Word2Vec, fastText, Global Vectors for Word Representation (GloVe) have been used for representing words for hate speech classification (Waseem et al., 2017; Badjatiya et al., 2017). Furthermore, pre-trained sentence embeddings such as InferSent, Universal Sentence Encoder, Embeddings from Language Models (ELMo) have been used for the task of hate speech classification (Indurthi et al., 2019; Bojkovsk`y and Pikuliak, 2019). These pretrained sentence embeddings are generic representations and are unaware of task-specific classes. Transforming the pre-trained sentence embeddings to task-specific representations can be helpful for label propagation, and our work explores this direction. The contributions of this article are: • evaluation of label propagation based semisupervised learning for hate speech classification; • comparison of label propagation on pretrained and task-specific representations learned from a small labeled corpus. Research on hate speech classification has rece"
2020.insights-1.8,W18-5114,0,0.0639458,"Missing"
2020.lrec-1.783,hofbauer-etal-2008-atcosim,0,0.263075,", which improve the efficiency of the human controller (Helmke et al., 2017). Air traffic control poses a number of challenges for ASR. Utterances are very domain-specific and while communication is conducted in English, most controllers are nonnative speakers. Commands are uttered at high speed to accommodate the heavy workload (Oualil et al., 2015). Commercial off-the-shelf ASR systems perform very poorly under these conditions (Cordero et al., 2012). Instead, the ASR system has to be trained on ATC domain data. Publicly available ATC recordings are very sparse (the largest corpus, ATCOSIM (Hofbauer et al., 2008), has 10 hours), so more data had to be recorded, transcribed and annotated (Oualil et al., 2015). 2.2. (1) Lufthansa four romeo juliett reduce speed two five zero and descend to altitude four thousand. Example (1) shows a transmission with two commands, instructing the airplane DLH4RJ to reduce its speed to 250 knots and its altitude to 4000 feet. 3. This approach has since been expanded upon by the followup projects AcListant-Strips and MALORCA. Annotation To allow ATC assistant systems to make use of the utterances recognised by ASR they need to be converted from natural language text to an"
2020.semeval-1.252,P19-4007,0,0.024649,"Missing"
2020.semeval-1.252,S19-2116,0,0.0112082,"SentiBERT” by Yin et al. (2020), this illustrates that proper pre-processing is often neglected over algorithmic advances. Therefore, judging the impact of algorithmic advances proves difficult. Nikolov and Radivchev (2019) compared the performance of a BERT-Large model to those of more traditional machine learning models, such as a logistic regression model and soft voting classifier (SVC). While the BERT-Large model outperformed the other on the test data, the logistic regression model and the SVC outperformed BERT on the dev data. Another approach from last year’s competition, presented by Han et al. (2019), used a bi-directional recurrent architecture with Gated Recurrent Units (GRU) (Chung et al., 2014) and rule-based sentence offensiveness calculations to categorize tweets. This worked especially well for subtask B, where the submission achieved the highest results. It was less successful for subtasks A and C, where the rule-based nature proved to be detrimental to the classifier’s performance. 3 3.1 Data Overview The data provided by the task organizers consists of social media posts in English, Arabic, Danish, Turkish, and Greek, the non-English data human-annotated using the hierarchical t"
2020.semeval-1.252,S19-2011,0,0.159643,"sh task A and task B (English), we submitted only the XLM-R model predictions. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 1916 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1916–1924 Barcelona, Spain (Online), December 12, 2020. 2 Related Work SemEval 2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval 2019) (Zampieri et al., 2019b) saw many competitors make use of the recently released pre-trained transformer model, BERT. Liu et al. (2019a), the top performing model in OffensEval 2019, combined a BERT model with several pre-processing steps, such as hashtag segmentation, converting emojis to textual descriptions, and lowercasing the text. Compared to runner-up BERT submissions for the 2019 subtask A such as the rank 2 submission by Nikolov and Radivchev (2019), the rank 3 team UM@LING (Zhu et al., 2019) or the rank 7 submission “SentiBERT” by Yin et al. (2020), this illustrates that proper pre-processing is often neglected over algorithmic advances. Therefore, judging the impact of algorithmic advances proves difficult. Nikolo"
2020.semeval-1.252,S19-2123,0,0.0130739,"24 Barcelona, Spain (Online), December 12, 2020. 2 Related Work SemEval 2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval 2019) (Zampieri et al., 2019b) saw many competitors make use of the recently released pre-trained transformer model, BERT. Liu et al. (2019a), the top performing model in OffensEval 2019, combined a BERT model with several pre-processing steps, such as hashtag segmentation, converting emojis to textual descriptions, and lowercasing the text. Compared to runner-up BERT submissions for the 2019 subtask A such as the rank 2 submission by Nikolov and Radivchev (2019), the rank 3 team UM@LING (Zhu et al., 2019) or the rank 7 submission “SentiBERT” by Yin et al. (2020), this illustrates that proper pre-processing is often neglected over algorithmic advances. Therefore, judging the impact of algorithmic advances proves difficult. Nikolov and Radivchev (2019) compared the performance of a BERT-Large model to those of more traditional machine learning models, such as a logistic regression model and soft voting classifier (SVC). While the BERT-Large model outperformed the other on the test data, the logistic regression model and the SVC outperformed BERT on the"
2020.semeval-1.252,2020.lrec-1.629,0,0.0191873,"Missing"
2020.semeval-1.252,2020.lrec-1.430,0,0.0415454,"Missing"
2020.semeval-1.252,2020.acl-main.341,0,0.0247762,"Offensive Language in Social Media (OffensEval 2019) (Zampieri et al., 2019b) saw many competitors make use of the recently released pre-trained transformer model, BERT. Liu et al. (2019a), the top performing model in OffensEval 2019, combined a BERT model with several pre-processing steps, such as hashtag segmentation, converting emojis to textual descriptions, and lowercasing the text. Compared to runner-up BERT submissions for the 2019 subtask A such as the rank 2 submission by Nikolov and Radivchev (2019), the rank 3 team UM@LING (Zhu et al., 2019) or the rank 7 submission “SentiBERT” by Yin et al. (2020), this illustrates that proper pre-processing is often neglected over algorithmic advances. Therefore, judging the impact of algorithmic advances proves difficult. Nikolov and Radivchev (2019) compared the performance of a BERT-Large model to those of more traditional machine learning models, such as a logistic regression model and soft voting classifier (SVC). While the BERT-Large model outperformed the other on the test data, the logistic regression model and the SVC outperformed BERT on the dev data. Another approach from last year’s competition, presented by Han et al. (2019), used a bi-di"
2020.semeval-1.252,N19-1144,0,0.0122567,"rated promising results during experiments and was submitted for all languages in task A except for Danish. For Danish task A and task B (English), we submitted only the XLM-R model predictions. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 1916 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1916–1924 Barcelona, Spain (Online), December 12, 2020. 2 Related Work SemEval 2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval 2019) (Zampieri et al., 2019b) saw many competitors make use of the recently released pre-trained transformer model, BERT. Liu et al. (2019a), the top performing model in OffensEval 2019, combined a BERT model with several pre-processing steps, such as hashtag segmentation, converting emojis to textual descriptions, and lowercasing the text. Compared to runner-up BERT submissions for the 2019 subtask A such as the rank 2 submission by Nikolov and Radivchev (2019), the rank 3 team UM@LING (Zhu et al., 2019) or the rank 7 submission “SentiBERT” by Yin et al. (2020), this illustrates that proper pre-processing is often negl"
2020.semeval-1.252,S19-2010,0,0.0349641,"Missing"
2020.semeval-1.252,S19-2138,0,0.0261639,"Missing"
2020.vardial-1.12,J19-2006,0,0.208804,"points of view, the main idea of this stream of research is to train a single NLP model on many languages whereby the language representation space is learned by exploiting the multilingual signal. For example, Johnson et al. (2017) introduced a multilingual neural machine translation (NMT) model in which the required target language of the translation was specified by the language embedding. Other works have either scaled this approach to a massively multilingual setting (Östling and Tiedemann, 2017; Malaviya et al., 2017) or explored other NLP tasks such as linguistic structure prediction (Bjerva et al., 2019) and grapheme-to-phoneme conversion (Peters et al., 2017). Furthermore, Rabinovich et al. (2017) and Bjerva et al. (2019) have analyzed the learned language representations and shown that the distance in the representation space reflects the phylogenetic distance between Indo-European languages. However, Bjerva et al. (2019) have argued that structural syntactic similarities between languages are a better predictor of the language representation similarities than phylogenetic similarities. The most relevant analysis to ours is the recent work by Cathcart and Wandl (2020), in which the authors"
2020.vardial-1.12,2020.sigmorphon-1.28,0,0.250513,"istic structure prediction (Bjerva et al., 2019) and grapheme-to-phoneme conversion (Peters et al., 2017). Furthermore, Rabinovich et al. (2017) and Bjerva et al. (2019) have analyzed the learned language representations and shown that the distance in the representation space reflects the phylogenetic distance between Indo-European languages. However, Bjerva et al. (2019) have argued that structural syntactic similarities between languages are a better predictor of the language representation similarities than phylogenetic similarities. The most relevant analysis to ours is the recent work by Cathcart and Wandl (2020), in which the authors have trained a neural sequence-to-sequence model on a Slavic etymological dictionary. Their model was trained to consume a reconstructed Proto-Slavic word form and a language embedding, then emit a word form in the modern language specified by the language embedding. The authors have applied a clustering analysis on the learned language embeddings and successfully reconstructed the phylogenetic Slavic family tree. Our work complements this line of research with one fundamental difference: we perform our analysis on contemporary realizations of Slavic speech instead of th"
2020.vardial-1.12,D17-1268,0,0.0218122,"(v ∈ Rd ) is associated with each language. Even though it has been motivated from different points of view, the main idea of this stream of research is to train a single NLP model on many languages whereby the language representation space is learned by exploiting the multilingual signal. For example, Johnson et al. (2017) introduced a multilingual neural machine translation (NMT) model in which the required target language of the translation was specified by the language embedding. Other works have either scaled this approach to a massively multilingual setting (Östling and Tiedemann, 2017; Malaviya et al., 2017) or explored other NLP tasks such as linguistic structure prediction (Bjerva et al., 2019) and grapheme-to-phoneme conversion (Peters et al., 2017). Furthermore, Rabinovich et al. (2017) and Bjerva et al. (2019) have analyzed the learned language representations and shown that the distance in the representation space reflects the phylogenetic distance between Indo-European languages. However, Bjerva et al. (2019) have argued that structural syntactic similarities between languages are a better predictor of the language representation similarities than phylogenetic similarities. The most releva"
2020.vardial-1.12,E17-2102,0,0.0251119,"dings, where a single vector (v ∈ Rd ) is associated with each language. Even though it has been motivated from different points of view, the main idea of this stream of research is to train a single NLP model on many languages whereby the language representation space is learned by exploiting the multilingual signal. For example, Johnson et al. (2017) introduced a multilingual neural machine translation (NMT) model in which the required target language of the translation was specified by the language embedding. Other works have either scaled this approach to a massively multilingual setting (Östling and Tiedemann, 2017; Malaviya et al., 2017) or explored other NLP tasks such as linguistic structure prediction (Bjerva et al., 2019) and grapheme-to-phoneme conversion (Peters et al., 2017). Furthermore, Rabinovich et al. (2017) and Bjerva et al. (2019) have analyzed the learned language representations and shown that the distance in the representation space reflects the phylogenetic distance between Indo-European languages. However, Bjerva et al. (2019) have argued that structural syntactic similarities between languages are a better predictor of the language representation similarities than phylogenetic simil"
2020.vardial-1.12,W17-5403,0,0.0390389,"Missing"
2020.vardial-1.12,P17-1049,0,0.118922,"many languages whereby the language representation space is learned by exploiting the multilingual signal. For example, Johnson et al. (2017) introduced a multilingual neural machine translation (NMT) model in which the required target language of the translation was specified by the language embedding. Other works have either scaled this approach to a massively multilingual setting (Östling and Tiedemann, 2017; Malaviya et al., 2017) or explored other NLP tasks such as linguistic structure prediction (Bjerva et al., 2019) and grapheme-to-phoneme conversion (Peters et al., 2017). Furthermore, Rabinovich et al. (2017) and Bjerva et al. (2019) have analyzed the learned language representations and shown that the distance in the representation space reflects the phylogenetic distance between Indo-European languages. However, Bjerva et al. (2019) have argued that structural syntactic similarities between languages are a better predictor of the language representation similarities than phylogenetic similarities. The most relevant analysis to ours is the recent work by Cathcart and Wandl (2020), in which the authors have trained a neural sequence-to-sequence model on a Slavic etymological dictionary. Their mode"
2021.blackboxnlp-1.32,D19-1593,0,0.0264495,"ess to word phonology (i.e., PGE) across different languages. 6 6.1 Discussion Relevance to Related Work Although the idea of representational similarity analysis (RSA) has originated in the neuroscience literature (Kriegeskorte et al., 2008), researchers in NLP and speech technology have employed a similar set of techniques to analyze emergent representations of multi-layer neural networks. For example, RSA has previously been employed to analyze the correlation between neural and symbolic representations (Chrupała and Alishahi, 2019), contextualized word representations (Abnar et al., 2019; Abdou et al., 2019; Lepori and McCoy, 2020; Wu et al., 2020), representations of self-supervised speech models (Chung et al., 2021) and visuallygrounded speech models (Chrupała et al., 2020). We take the inspiration from previous work and 413 0.8 Cross-model CKA (RBF 0.5) Cross-model CKA (linear) 0.8 0.721 0.7 0.6 0.5 0.4 0.3 0.239 0.2 0.1 P GE AE .C vs P GE .C vs 0.230 SE E CA .C vs SE typological features from the ancestor language (see Bjerva et al. (2019) for a discussion on how typological similarity and genetic relationships interact when analyzing neural embeddings). 0.757 0.7 0.6 0.5 0.397 0.4 0.401 0.3"
2021.eacl-srw.14,C18-1139,0,0.024507,"Model hyperparameters 4.1 Low-level speech features. We use 39dimensional MFCC feature vectors as well as frame-level averaged energy as low-level speech features. Frames are extracted from speech segment of 25ms with 10ms overlap between frames. Each speech sample is then scaled with word-level zero mean and unit variance. Choice of word embeddings In this experiment, we train our model on a subset of the SWC consisting of 1500 word types, 20 tokens per type, with each of the following word embeddings: GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), bidirectional Flair (Akbik et al., 2018), and stacked Flair and FastText. The retrieval scores of the model with different word embeddings are reported in Table 1. Although the difference is not dramatic, the best-performing model is the one that uses stacked FastText and Flair embeddings. It seems that stacking the embeddings provides richer semantic representations that benefit the model during training. Therefore, we proceed to the variability experiments with stacked Flair and FastText word embeddings as meaning representations. Speech encoder. We employ three convolutional layers over the temporal dimension with 128, 128, and 2"
2021.eacl-srw.14,Q17-1010,0,0.0403189,"concerning speech variability. 3.2 Experiments Model hyperparameters 4.1 Low-level speech features. We use 39dimensional MFCC feature vectors as well as frame-level averaged energy as low-level speech features. Frames are extracted from speech segment of 25ms with 10ms overlap between frames. Each speech sample is then scaled with word-level zero mean and unit variance. Choice of word embeddings In this experiment, we train our model on a subset of the SWC consisting of 1500 word types, 20 tokens per type, with each of the following word embeddings: GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), bidirectional Flair (Akbik et al., 2018), and stacked Flair and FastText. The retrieval scores of the model with different word embeddings are reported in Table 1. Although the difference is not dramatic, the best-performing model is the one that uses stacked FastText and Flair embeddings. It seems that stacking the embeddings provides richer semantic representations that benefit the model during training. Therefore, we proceed to the variability experiments with stacked Flair and FastText word embeddings as meaning representations. Speech encoder. We employ three convolutional layers over t"
2021.eacl-srw.14,P17-1057,0,0.0714605,"Missing"
2021.eacl-srw.14,L16-1735,0,0.0438541,"Missing"
2021.eacl-srw.14,D14-1162,0,0.0850853,"y suitable for our experimental aims concerning speech variability. 3.2 Experiments Model hyperparameters 4.1 Low-level speech features. We use 39dimensional MFCC feature vectors as well as frame-level averaged energy as low-level speech features. Frames are extracted from speech segment of 25ms with 10ms overlap between frames. Each speech sample is then scaled with word-level zero mean and unit variance. Choice of word embeddings In this experiment, we train our model on a subset of the SWC consisting of 1500 word types, 20 tokens per type, with each of the following word embeddings: GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), bidirectional Flair (Akbik et al., 2018), and stacked Flair and FastText. The retrieval scores of the model with different word embeddings are reported in Table 1. Although the difference is not dramatic, the best-performing model is the one that uses stacked FastText and Flair embeddings. It seems that stacking the embeddings provides richer semantic representations that benefit the model during training. Therefore, we proceed to the variability experiments with stacked Flair and FastText word embeddings as meaning representations. Speech encoder. We empl"
2021.eacl-srw.14,P19-1171,0,0.0213862,"e model, f (.) is the model presented as a parametric function, x1:T is the observed acoustic word segment, and θ are the model’s parameters learned in a supervised approach. Training. Given a training dataset of N tu1 1 N ples {(x1:T , v ), (x21:T , v2 ), . . . , (xN 1:T , v )}, our model is trained to take an acoustic word token x1:T as input, build up a meaning representation u, and then minimize the distance between the computed representation u and the embedding of the Meaning representation Following previous studies that adopted the distributional approach to represent lexical meaning (Pimentel et al., 2019; Williams et al., 2020), we use pre-trained distributed word representations, or word embeddings, as a proxy for the stored lexical representations of word forms. This modeling 97 word v. This learning objective can be realized by projecting the acoustic word token into the word embedding space in such a way that an acoustic segment and embedding of the same word type are encouraged to end up closer in space than mismatched word embeddings. Concretely, we use a triplet margin loss function as follows: L= N X GloVe FastText (FT) Flair FT + Flair dim R@1 R@5 R@10 300 300 4096 4396 0.159 0.176 0"
2021.eacl-srw.14,P15-2119,0,0.0158017,"Our model is trained on naturalistic data that consists of actual acoustic realizations of spoken words extracted from the German portion of the Spoken Wikipedia Corpus. And (2) we investigate the degree to which the emergent representations from the model can generalize with respect to two sources of variability in speech signals — interspeaker variability and cross-lingual variability. 2 Figure 1: A schematic view of our proposed model for spoken word recognition. choice can be justified since word embeddings have been shown to reliably encode lexical features such as taxonomic information (Rubinstein et al., 2015). 2.3 Architecture. Similar to the architecture presented in work of Maas et al. (2012), our proposed spoken word recognition model is based on a multilayer convolutional neural network that maps an acoustic input onto a meaning representation (depicted in Figure 1). However, instead of vector regression as the objective function, the training procedure of our model builds on the ideas of visuallygrounded learning of spoken language (Harwath et al., 2016; Chrupała et al., 2017a). While in previous work models have been trained to project an image and its corresponding spoken caption onto a sha"
2021.eacl-srw.14,2020.acl-main.597,0,0.0133115,"odel presented as a parametric function, x1:T is the observed acoustic word segment, and θ are the model’s parameters learned in a supervised approach. Training. Given a training dataset of N tu1 1 N ples {(x1:T , v ), (x21:T , v2 ), . . . , (xN 1:T , v )}, our model is trained to take an acoustic word token x1:T as input, build up a meaning representation u, and then minimize the distance between the computed representation u and the embedding of the Meaning representation Following previous studies that adopted the distributional approach to represent lexical meaning (Pimentel et al., 2019; Williams et al., 2020), we use pre-trained distributed word representations, or word embeddings, as a proxy for the stored lexical representations of word forms. This modeling 97 word v. This learning objective can be realized by projecting the acoustic word token into the word embedding space in such a way that an acoustic segment and embedding of the same word type are encouraged to end up closer in space than mismatched word embeddings. Concretely, we use a triplet margin loss function as follows: L= N X GloVe FastText (FT) Flair FT + Flair dim R@1 R@5 R@10 300 300 4096 4396 0.159 0.176 0.216 0.227 0.451 0.461 0"
2021.eacl-srw.15,S19-2007,0,0.216109,"the learned representations in the encoder. Transfer learning has been applied to sentiment analysis (SA) using parameter transfer methods such as pretrained sentiment embeddings (Dong and de Melo, 2018) or machine translation-based context vectors (McCann et al., 2017). Our approach forms part of the parameter transfer approach, as we use encoder representations learned using emoji-based source tasks and transfer these to sentiment target tasks. Hate speech classification and sentiment analysis have in recent years been the object of many shared tasks (Rosenthal et al., 2017; Wiegand, 2018; Basile et al., 2019; Mandl et al., 2019; Ogrodniczuk and Łukasz Kobyli´nski, 2019). Classification models for these tasks often rely on feature engineering and statistical methods such as naivebayes (Saleem et al., 2016), logistic regression over subwords (Waseem and Hovy, 2016) or neural approaches including convolutional neural networks (Park and Fung, 2017) or, as in our case, the representations of large LMs (Yang et al., 2019). 3 Method: Emoji-Prediction For our parameter transfer, we rely on a single transformer-based LM which is shared among different tasks. A sequence x ∈ X is featurized by reading it in"
2021.eacl-srw.15,P04-3031,0,0.305417,"above, we use the original train/test splits. While the HA tasks have different label names, we normalize these to be hate/none across all tasks. For all SA, the labels to be predicted are positive/negative/neutral. In Table 1, we report the label distribution, hate/none for HS and positive/negative/neutral for SA, across all TT training and test sets, as well as ST Twitter corpora sizes. For both ST and TT corpora, we also report the percentage as well as total number of tweets containing emojis. Preprocessing All data sets undergo the same preprocessing. Tweets are tokenized using the NLTK (Bird and Loper, 2004) TweetTokenizer and user mentions, retweets and punctuation are removed. Repeated characters are shortened. We use token frequencies to determine the standard orthography of a word (e.g. coooool → cool instead of col). 4.2 Figure 1: Happy (left) and unhappy (right) emoji clusters obtained by KMeans on TW-DE. and multilingual experiments respectively. In both cases, this yielded clusters that can be manually categorized as happy, love, fun, nature, unhappy, other (Figure 1). For KMeans-3, {happy, fun, love} were merged to positive, {other, nature} to neutral and {unhappy} was used as the negati"
2021.eacl-srw.15,W17-1106,0,0.104793,"Missing"
2021.eacl-srw.15,2020.acl-main.747,0,0.0960641,"Missing"
2021.eacl-srw.15,S16-1173,0,0.0650931,"Missing"
2021.eacl-srw.15,N19-1423,0,0.0177684,"103–110 April 19 - 23, 2021. ©2021 Association for Computational Linguistics work does not pre-define the emotion classes found in emojis and instead learns these classes, or clusters, from the data itself. While our and the above approaches focus on exploiting emojis as additional labelled data, e.g. in a transfer setting, emoji embeddings (Eisner et al., 2016) have been used as additional features in downstream tasks such as sarcasm detection (Subramanian et al., 2019). Transfer learning has recently been driven by transformer-based (Vaswani et al., 2017) language models (LM) such as BERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020). When learning a source task on these models, the representations in the encoder change to become informative to the task at hand. In a parameter transfer setting, a new but related target task then profits from the learned representations in the encoder. Transfer learning has been applied to sentiment analysis (SA) using parameter transfer methods such as pretrained sentiment embeddings (Dong and de Melo, 2018) or machine translation-based context vectors (McCann et al., 2017). Our approach forms part of the parameter transfer approach, as we use encoder repre"
2021.eacl-srw.15,P18-1235,0,0.0628143,"Missing"
2021.eacl-srw.15,W16-6208,0,0.0588116,"Missing"
2021.eacl-srw.15,D17-1169,0,0.0835003,"ourced. On the other hand, noisy social media content is available in abundance and many sentiment tasks are based on user comments on such platforms. Emojis can be a valuable source for the distant supervision of sentiment tasks, as they correlate with the underlying emotion of a comment. In this study, we aim to exploit the emotional information encoded in emojis to improve the performance on various sentiment tasks using a transfer learning approach from an emoji-based source task (ST) to a sentiment target task (TT). Previous work has focused on the transfer from predicting single emojis (Felbo et al., 2017) or strictly pre-defined emoji-clusters (Deriu et al., 2016). However, predefined emoji clusters do not take into account the culturally diverse usage of emojis (Park et al., 2012; Kaneko et al., 2019). We therefore introduce datadriven supervised and unsupervised emoji clusters and compare these with single emoji prediction tasks. Specifically, we analyze the efficacy of the transfer from a single emoji or (un)supervised emoji cluster prediction ST to a sentiment TT under three conditions, i.e. i) low vs. high amount of emoji content present in TT, ii) balanced vs. unbalanced label distributi"
2021.eacl-srw.15,W17-3006,0,0.013956,"encoder representations learned using emoji-based source tasks and transfer these to sentiment target tasks. Hate speech classification and sentiment analysis have in recent years been the object of many shared tasks (Rosenthal et al., 2017; Wiegand, 2018; Basile et al., 2019; Mandl et al., 2019; Ogrodniczuk and Łukasz Kobyli´nski, 2019). Classification models for these tasks often rely on feature engineering and statistical methods such as naivebayes (Saleem et al., 2016), logistic regression over subwords (Waseem and Hovy, 2016) or neural approaches including convolutional neural networks (Park and Fung, 2017) or, as in our case, the representations of large LMs (Yang et al., 2019). 3 Method: Emoji-Prediction For our parameter transfer, we rely on a single transformer-based LM which is shared among different tasks. A sequence x ∈ X is featurized by reading it into the encoder of the LM and retrieving its last hidden state. A linear layer is then used as a predictive function f : X → Y to predict labels y ∈ Y . A task T = {Y, f (x)} is then a set of labels Y and the predictive function f over the instances in X. We follow a transfer learning approach, where source task TS is an emoji-based classific"
2021.eacl-srw.15,S17-2088,0,0.0714973,"Missing"
2021.eacl-srw.15,N16-2013,0,0.0146637,"et al., 2017). Our approach forms part of the parameter transfer approach, as we use encoder representations learned using emoji-based source tasks and transfer these to sentiment target tasks. Hate speech classification and sentiment analysis have in recent years been the object of many shared tasks (Rosenthal et al., 2017; Wiegand, 2018; Basile et al., 2019; Mandl et al., 2019; Ogrodniczuk and Łukasz Kobyli´nski, 2019). Classification models for these tasks often rely on feature engineering and statistical methods such as naivebayes (Saleem et al., 2016), logistic regression over subwords (Waseem and Hovy, 2016) or neural approaches including convolutional neural networks (Park and Fung, 2017) or, as in our case, the representations of large LMs (Yang et al., 2019). 3 Method: Emoji-Prediction For our parameter transfer, we rely on a single transformer-based LM which is shared among different tasks. A sequence x ∈ X is featurized by reading it into the encoder of the LM and retrieving its last hidden state. A linear layer is then used as a predictive function f : X → Y to predict labels y ∈ Y . A task T = {Y, f (x)} is then a set of labels Y and the predictive function f over the instances in X. We fo"
2021.eacl-srw.3,goldhahn-etal-2012-building,0,0.00987273,"facilitatory orthographic effect, we eliminate the difference between consistent and inconsistent words by training the models on Finnish data. Finnish has a grapheme to phoneme mapping that is nearly one to one which leads to little to no inconsistent words (Joshi and Aaron, 2016). Excluding the factor of consistency For the Finnish training data, the 2378 most frequent words are extracted from the vocabulary of the Finnish fastText embeddings (Grave et al., 2018). For the input of the models, Finnish phoneme embeddings are trained on the transcription of Finnish news texts (Newscrawl 2017, Goldhahn et al., 2012). Finnish fastText embeddings are used as meaning representations, as well as 540-dimensional localist orthographic representations within the online model. Four balanced samples of size 70 that correspond to the four neighborhood groups are drawn from the training data to then monitor the mean error score of each model per condition (see Figure 4). The results after training the offline model on Finnish data show an inverse pattern compared to the German results. The offline model would, therefore, predict that no facilitatory orthographic effect can be observed in a lexical decision task wit"
2021.eacl-srw.3,L18-1550,0,0.0455837,"uman-like behavior. (3) We provide testable hypotheses for future research based on the models’ behavior, which allows us to further validate the online or offline hypothesis. 2 2.1 transcription of the NEGRA corpus (Skut et al., 1997). The transcription is generated with the grapheme-to-phoneme converter tool provided by the Bavarian Archive for Speech Signals (BAS) (Reichel, 2012, 2014). The cbow model and negative sampling is used with window size 1 to obtain 30-dimensional phoneme embeddings. Word meanings are approximated by word embeddings. We use pre-trained German fastText embeddings (Grave et al., 2018) as the output meaning representations of our models (see also Baayen et al., 2019; Chuang et al., 2020; Hendrix and Sun, 2020, for the similar use of word embeddings as semantic representations in models of word recognition). Methodology The offline model The first architecture implements the theoretical assumption that a prelexical phonemic representation is mapped onto a lexical meaning representation, without incorporating explicit orthographic representations at the prelexical level. The offline model, which instantiates the offline hypothesis, processes one phoneme per time step. After t"
2021.eacl-srw.3,W19-4219,0,0.0123162,"aus (mouse) is illustrated in Figure 1. First, the model takes the respective phonemic sequence of [/m/, /aU/, /s/] as input. Then, it should build a vector representation that corresponds to a phoneme sequence, thus the phonological form of the entire word, to then produce a word meaning representation as output. This meaning representation should be as close as possible to the actual ground truth, which is the word embedding of Maus (mouse). Phoneme embeddings learn the phonemic distribution well and implicitly capture articulatory distinctive features of phonemes (Silfverberg et al., 2018; Kolachina and Magyar, 2019). Therefore, phoneme vector representations are trained using word2vec (Mikolov et al., 2013) on the phonetic The online model The second proposed model architecture includes explicit orthographic information at the prelexical level, instantiating the online hypothesis. The online model processes two kinds of inputs – a sequence of 30-dimensional phoneme representations and a localist orthographic representation of a word that is based on character bigrams (818 units). The first input layer (30 units) is connected to an LSTM cell (400 units) which is fully connected to an intermediate layer (4"
2021.eacl-srw.3,J80-3005,0,0.573773,"xical and a lexical level (Scharenborg and Boves, 2010). The prelexical level contains prelexical representations, like phonological units, which are the result of having processed the raw acoustic signal. These units are assumed to be activated before accessing meaning representations of words in the lexical level. By instantiating the process of SWR in a computational model the underlying theory can then be validated or further refined based on insights into the model’s architecture and its behavior. Influential models of SWR are for example the Cohort model (Marslen-Wilson and Welsh, 1978; Marslen-Wilson and Tyler, 1980; Marslen-Wilson, 1987), the TRACE model (McClelland and Elman, 1986) or the Shortlist model (Norris, 1994). These models typically have a connectionist architecture with localist or feature-based representations as their inputs and outputs (Weber and Scharen16 Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Linguistics: Student Research Workshop, pages 16–22 April 19 - 23, 2021. ©2021 Association for Computational Linguistics In what follows, we present two models of SWR using a long short-term memory (LSTM) architecture (Hochreiter and Schmidhub"
2021.eacl-srw.3,W18-0314,0,0.0171651,"g, e.g., the German word Maus (mouse) is illustrated in Figure 1. First, the model takes the respective phonemic sequence of [/m/, /aU/, /s/] as input. Then, it should build a vector representation that corresponds to a phoneme sequence, thus the phonological form of the entire word, to then produce a word meaning representation as output. This meaning representation should be as close as possible to the actual ground truth, which is the word embedding of Maus (mouse). Phoneme embeddings learn the phonemic distribution well and implicitly capture articulatory distinctive features of phonemes (Silfverberg et al., 2018; Kolachina and Magyar, 2019). Therefore, phoneme vector representations are trained using word2vec (Mikolov et al., 2013) on the phonetic The online model The second proposed model architecture includes explicit orthographic information at the prelexical level, instantiating the online hypothesis. The online model processes two kinds of inputs – a sequence of 30-dimensional phoneme representations and a localist orthographic representation of a word that is based on character bigrams (818 units). The first input layer (30 units) is connected to an LSTM cell (400 units) which is fully connecte"
2021.emnlp-main.660,W19-1909,0,0.0522682,"Missing"
2021.emnlp-main.660,2021.naacl-main.201,1,0.768703,"Missing"
2021.emnlp-main.660,L18-1473,0,0.101128,"Missing"
2021.emnlp-main.660,P19-1027,0,0.0405841,"Missing"
2021.emnlp-main.660,P82-1020,0,0.381609,"Missing"
2021.emnlp-main.660,D19-1138,0,0.0328587,"Missing"
2021.emnlp-main.660,D18-1176,0,0.243662,"sed Adversarial Meta-Embeddings for Robust Input Representations Jannik Str¨otgen1 Dietrich Klakow2 Lukas Lange1,2,3 Heike Adel1 1 Bosch Center for Artificial Intelligence, Renningen, Germany 2 Spoken Language Systems (LSV), Saarland University, Saarbr¨ucken, Germany 3 Saarbr¨ucken Graduate School of Computer Science, Saarbr¨ucken, Germany {Lukas.Lange,Heike.Adel,Jannik.Stroetgen}@de.bosch.com dietrich.klakow@lsv.uni-saarland.de Abstract embeddings are most effective, combinations of different embedding models are likely to be benefiCombining several embeddings typically imcial (Tsuboi, 2014; Kiela et al., 2018; Lange et al., proves performance in downstream tasks as 2019b), even when using already powerful largedifferent embeddings encode different inforscale pre-trained language models (Akbik et al., mation. It has been shown that even models us2018; Yu et al., 2020). Word-based embeddings, ing embeddings from transformers still benefor instance, are strong in modeling frequent words fit from the inclusion of standard word embedwhile character-based embeddings can model outdings. However, the combination of embeddings of different types and dimensions is chalof-vocabulary words. Similarly, domain-"
2021.emnlp-main.660,L16-1530,0,0.0122618,"conduct experiments for concept extraction on two datasets from the clinical domain, the English i2b2 2010 data (Uzuner et al., 2011) and the Spanish PharmaCoNER task (GonzalezAgirre et al., 2019), as well as experiments on the materials science domain (Friedrich et al., 2020). For POS tagging, we use the universal dependencies treebanks version 1.2 (UPOS tag) and use the 27 languages for which Yasunaga et al. (2018) reported numbers. Sentence Classification. We experiment with three question classifications tasks, namely the TREC corpus (Voorhees and Tice, 1999) with 6 or 50 labels and GARD (Kilicoglu et al., 2016, clinical domain). 5.2 Evaluation Results We now present the results of our experiments. All reported numbers are the averages of three runs. Sequence Labeling. Tables 3 and 4 show the results for sequence labeling in comparison to the state of the art.4 Our models consistently set the new state of the art for English and Dutch NER, for domain-specific concept extraction as well as for all 27 languages for POS tagging. The comparison with XML-R on NER shows that our FAME method can also improve upon already powerful transformer representations. In domain-specific concept extraction, we outper"
2021.emnlp-main.660,D14-1181,0,0.015151,"ding low-resource settings. (iv) Moreover, we show that even representations from large-scale pretrained transformer models can benefit from our meta-embeddings approach. The code for FAME is publicly available1 and compatible with the flair framework (Akbik et al., 2018). Meta-Embeddings. Previous work has seen performance gains by, for example, combining various types of word embeddings (Tsuboi, 2014) or the same type trained on different corpora (Luo et al., 2014). For the combination, some alternatives have been proposed, such as different input channels of a convolutional neural network (Kim, 2014; Zhang et al., 2016), concatenation followed by dimensionality reduction (Yin and Sch¨utze, 2016) or averaging of embeddings (Coates and Bollegala, 2018), e.g., for combining embeddings from multiple languages (Lange et al., 2020b; Reid et al., 2020). More recently, auto-encoders (Bollegala and Bao, 2018; Wu et al., 2020), ensembles of sentence encoders (Poerner et al., 2020) and attentionbased methods (Kiela et al., 2018; Lange et al., 2019a) have been introduced. The latter allows a dynamic (input-based) combination of multiple embeddings. Winata et al. (2019) and Priyadharshini et al. (202"
2021.emnlp-main.660,N16-1030,0,0.311098,"ays, a large number of differ- put granularities are combined, e.g., subwords and words. Infrequent in-domain tokens, for instance, ent embedding models are available with different are hard to detect when using subword-based emcharacteristics, such as different input granularities beddings as they can model any token. Moreover, (word-based (e.g., Mikolov et al., 2013; Pennington both average and attention-based meta-embeddings et al., 2014) vs. subword-based (e.g., Heinzerling and Strube, 2018; Devlin et al., 2019) vs. character- require a mapping of all embeddings into the same based (e.g., Lample et al., 2016; Ma and Hovy, space which can be challenging for a set of embeddings with different dimensions. 2016; Peters et al., 2018)), or different data used for pre-training (general-world vs. specific domain). In this paper, we propose feature-based adverSince those characteristics directly influence when sarial meta-embeddings (FAME) that (1) align the 8382 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8382–8395 c November 7–11, 2021. 2021 Association for Computational Linguistics embedding spaces with adversarial training, and (2) use attention for co"
2021.emnlp-main.660,D19-5705,1,0.89376,"Missing"
2021.emnlp-main.660,2020.acl-main.621,1,0.877429,"Missing"
2021.emnlp-main.660,2020.repl4nlp-1.13,1,0.865759,"Missing"
2021.emnlp-main.660,2020.repl4nlp-1.14,1,0.876423,"Missing"
2021.emnlp-main.660,D19-1022,0,0.034744,"Missing"
2021.emnlp-main.660,P18-2005,0,0.0247026,"et al., 2017b; Li et al., 2019). However, in contrast to these works, we use task-independent features derived from the token itself. Thus, we can use the same attention function for different tasks. Adversarial Training. Further, our method is motivated by the usage of adversarial training (Goodfellow et al., 2014) for creating input representations that are independent of a specific domain or feature. This is related to using adversarial train2 Related Work ing for domain adaptation (Ganin et al., 2016) or This section surveys related work on meta- coping with bias or confounding variables (Li et al., 2018; Raff and Sylvester, 2018; Zhang et al., 2018; embeddings, attention and adversarial training. Barrett et al., 2019; McHardy et al., 2019). Follow1 ing Ganin et al. (2016), we use gradient reversal https://github.com/boschresearch/ adversarial_meta_embeddings training in this paper. Recent studies use adversar8383 8384 - Frequency: High-frequency words can typically be modeled well by word-based embeddings, while low-frequency words are better captured with subword-based embeddings. Moreover, frequency is domain-dependent and can thus help to decide between embeddings from different domains."
2021.emnlp-main.660,W16-3920,0,0.0151318,"ional one-hot vector. - Word Shape: Word shapes capture certain linguistic features and are often part of manually designed feature sets, e.g., for CRF classifiers (Lafferty et al., 2001). For example, uncommon word shapes can be indicators for domain-specific words, which can benefit from domain-specific embeddings. We create 12 binary features that capture information on the word shape, including whether the first, any or all characters are uppercased, alphanumerical, digits or punctuation marks. - Word Shape Embeddings: In addition, we train word shape embeddings (25 dimensions) similar to Limsopatham and Collier (2016). For this, the shape of each word is converted by replacing letters with c or C (depending on the capitalization), digits with n and punctuation marks with p. For instance, Dec. 12th would be converted to Cccp nncc. The resulting shapes are one-hot encoded and a trainable randomly initialized linear layer is used to compute the shape representation. All sparse feature vectors (binary or one-hot encoded) are fed through a linear layer to generate a dense representation. Finally, all features are concatenated into a single feature vector f of 77 dimensions which is used in the attention functio"
2021.emnlp-main.660,P16-1101,0,0.112208,"Missing"
2021.emnlp-main.660,D14-1162,0,0.088083,"Missing"
2021.emnlp-main.660,N18-1202,0,0.0545009,"instance, ent embedding models are available with different are hard to detect when using subword-based emcharacteristics, such as different input granularities beddings as they can model any token. Moreover, (word-based (e.g., Mikolov et al., 2013; Pennington both average and attention-based meta-embeddings et al., 2014) vs. subword-based (e.g., Heinzerling and Strube, 2018; Devlin et al., 2019) vs. character- require a mapping of all embeddings into the same based (e.g., Lample et al., 2016; Ma and Hovy, space which can be challenging for a set of embeddings with different dimensions. 2016; Peters et al., 2018)), or different data used for pre-training (general-world vs. specific domain). In this paper, we propose feature-based adverSince those characteristics directly influence when sarial meta-embeddings (FAME) that (1) align the 8382 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8382–8395 c November 7–11, 2021. 2021 Association for Computational Linguistics embedding spaces with adversarial training, and (2) use attention for combining embeddings with a layer that is guided by features reflecting wordspecific properties, such as the shape or frequen"
2021.emnlp-main.660,P16-2067,0,0.0656888,"Missing"
2021.emnlp-main.660,2020.acl-main.628,0,0.057668,"Missing"
2021.emnlp-main.660,P15-1150,0,0.128362,"Missing"
2021.emnlp-main.660,C16-1116,0,0.0289272,"Missing"
2021.emnlp-main.660,D19-1450,0,0.0301199,"Missing"
2021.emnlp-main.660,W19-4320,0,0.0189274,"annels of a convolutional neural network (Kim, 2014; Zhang et al., 2016), concatenation followed by dimensionality reduction (Yin and Sch¨utze, 2016) or averaging of embeddings (Coates and Bollegala, 2018), e.g., for combining embeddings from multiple languages (Lange et al., 2020b; Reid et al., 2020). More recently, auto-encoders (Bollegala and Bao, 2018; Wu et al., 2020), ensembles of sentence encoders (Poerner et al., 2020) and attentionbased methods (Kiela et al., 2018; Lange et al., 2019a) have been introduced. The latter allows a dynamic (input-based) combination of multiple embeddings. Winata et al. (2019) and Priyadharshini et al. (2020) used similar attention functions to combine embeddings from different languages for NER in code-switching settings. Liu et al. (2021) explored the inclusion of domain-specific semantic structures to improve meta-embeddings in nonstandard domains. In this paper, we follow the idea of attention-based meta-embeddings and propose task-independent methods for improving them. Extended Attention. Attention has been introduced in the context of machine translation (Bahdanau et al., 2015) and is since then widely used in NLP (i.a., Tai et al., 2015; Xu et al., 2015; Ya"
2021.emnlp-main.660,W02-2024,0,0.187062,"a carbon-neutral GPU cluster. NER Model Schweter and Akbik (2020) Yu et al. (2020) XLM-R (Conneau et al., 2020) FAME (our model) En De Es Nl 93.69 93.5 92.92 94.11 92.29 90.3 85.81 92.28 89.93 90.3 89.72 89.90 94.66 93.7 92.53 95.42 Model Concept Extraction ClinEn ClinEs SofcEn Alsentzer et al. (2019) Lange et al. (2020a) Friedrich et al. (2020) FAME (our model) 87.7 88.9 – 90.08 91.4 – 92.68 – 81.5 83.68 Table 3: NER and concept extraction results (F1 ). XLM-R is a fine-tuned transformer (Conneau et al., 2020). benchmark datasets from the news domain (English/German/Dutch/Spanish) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). In addition, we conduct experiments for concept extraction on two datasets from the clinical domain, the English i2b2 2010 data (Uzuner et al., 2011) and the Spanish PharmaCoNER task (GonzalezAgirre et al., 2019), as well as experiments on the materials science domain (Friedrich et al., 2020). For POS tagging, we use the universal dependencies treebanks version 1.2 (UPOS tag) and use the 27 languages for which Yasunaga et al. (2018) reported numbers. Sentence Classification. We experiment with three question classifications tasks, namely the TREC corpus"
2021.emnlp-main.660,2020.emnlp-main.282,0,0.0518836,"Missing"
2021.emnlp-main.660,D14-1101,0,0.185477,"ME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations Jannik Str¨otgen1 Dietrich Klakow2 Lukas Lange1,2,3 Heike Adel1 1 Bosch Center for Artificial Intelligence, Renningen, Germany 2 Spoken Language Systems (LSV), Saarland University, Saarbr¨ucken, Germany 3 Saarbr¨ucken Graduate School of Computer Science, Saarbr¨ucken, Germany {Lukas.Lange,Heike.Adel,Jannik.Stroetgen}@de.bosch.com dietrich.klakow@lsv.uni-saarland.de Abstract embeddings are most effective, combinations of different embedding models are likely to be benefiCombining several embeddings typically imcial (Tsuboi, 2014; Kiela et al., 2018; Lange et al., proves performance in downstream tasks as 2019b), even when using already powerful largedifferent embeddings encode different inforscale pre-trained language models (Akbik et al., mation. It has been shown that even models us2018; Yu et al., 2020). Word-based embeddings, ing embeddings from transformers still benefor instance, are strong in modeling frequent words fit from the inclusion of standard word embedwhile character-based embeddings can model outdings. However, the combination of embeddings of different types and dimensions is chalof-vocabulary words"
2021.emnlp-main.660,N16-1174,0,0.0517471,"9) and Priyadharshini et al. (2020) used similar attention functions to combine embeddings from different languages for NER in code-switching settings. Liu et al. (2021) explored the inclusion of domain-specific semantic structures to improve meta-embeddings in nonstandard domains. In this paper, we follow the idea of attention-based meta-embeddings and propose task-independent methods for improving them. Extended Attention. Attention has been introduced in the context of machine translation (Bahdanau et al., 2015) and is since then widely used in NLP (i.a., Tai et al., 2015; Xu et al., 2015; Yang et al., 2016; Vaswani et al., 2017). Our approach extends this technique by integrating word features into the attention function. This is similar to extending the source of attention for uncertainty detection (Adel and Sch¨utze, 2017) or relation extraction (Zhang et al., 2017b; Li et al., 2019). However, in contrast to these works, we use task-independent features derived from the token itself. Thus, we can use the same attention function for different tasks. Adversarial Training. Further, our method is motivated by the usage of adversarial training (Goodfellow et al., 2014) for creating input represent"
2021.emnlp-main.660,N18-1089,0,0.0185406,"R is a fine-tuned transformer (Conneau et al., 2020). benchmark datasets from the news domain (English/German/Dutch/Spanish) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). In addition, we conduct experiments for concept extraction on two datasets from the clinical domain, the English i2b2 2010 data (Uzuner et al., 2011) and the Spanish PharmaCoNER task (GonzalezAgirre et al., 2019), as well as experiments on the materials science domain (Friedrich et al., 2020). For POS tagging, we use the universal dependencies treebanks version 1.2 (UPOS tag) and use the 27 languages for which Yasunaga et al. (2018) reported numbers. Sentence Classification. We experiment with three question classifications tasks, namely the TREC corpus (Voorhees and Tice, 1999) with 6 or 50 labels and GARD (Kilicoglu et al., 2016, clinical domain). 5.2 Evaluation Results We now present the results of our experiments. All reported numbers are the averages of three runs. Sequence Labeling. Tables 3 and 4 show the results for sequence labeling in comparison to the state of the art.4 Our models consistently set the new state of the art for English and Dutch NER, for domain-specific concept extraction as well as for all 27 l"
2021.emnlp-main.660,P16-1128,0,0.0321128,"Missing"
2021.emnlp-main.660,2020.acl-main.577,0,0.0158555,"ny 3 Saarbr¨ucken Graduate School of Computer Science, Saarbr¨ucken, Germany {Lukas.Lange,Heike.Adel,Jannik.Stroetgen}@de.bosch.com dietrich.klakow@lsv.uni-saarland.de Abstract embeddings are most effective, combinations of different embedding models are likely to be benefiCombining several embeddings typically imcial (Tsuboi, 2014; Kiela et al., 2018; Lange et al., proves performance in downstream tasks as 2019b), even when using already powerful largedifferent embeddings encode different inforscale pre-trained language models (Akbik et al., mation. It has been shown that even models us2018; Yu et al., 2020). Word-based embeddings, ing embeddings from transformers still benefor instance, are strong in modeling frequent words fit from the inclusion of standard word embedwhile character-based embeddings can model outdings. However, the combination of embeddings of different types and dimensions is chalof-vocabulary words. Similarly, domain-specific lenging. As an alternative to attention-based embeddings can capture in-domain words that do meta-embeddings, we propose feature-based not appear in general domains like news text. adversarial meta-embeddings (FAME) with an Different word representations"
2021.emnlp-main.660,P17-1179,0,0.0292852,"standard domains. In this paper, we follow the idea of attention-based meta-embeddings and propose task-independent methods for improving them. Extended Attention. Attention has been introduced in the context of machine translation (Bahdanau et al., 2015) and is since then widely used in NLP (i.a., Tai et al., 2015; Xu et al., 2015; Yang et al., 2016; Vaswani et al., 2017). Our approach extends this technique by integrating word features into the attention function. This is similar to extending the source of attention for uncertainty detection (Adel and Sch¨utze, 2017) or relation extraction (Zhang et al., 2017b; Li et al., 2019). However, in contrast to these works, we use task-independent features derived from the token itself. Thus, we can use the same attention function for different tasks. Adversarial Training. Further, our method is motivated by the usage of adversarial training (Goodfellow et al., 2014) for creating input representations that are independent of a specific domain or feature. This is related to using adversarial train2 Related Work ing for domain adaptation (Ganin et al., 2016) or This section surveys related work on meta- coping with bias or confounding variables (Li et al., 2"
2021.emnlp-main.660,N16-1178,0,0.0123519,"source settings. (iv) Moreover, we show that even representations from large-scale pretrained transformer models can benefit from our meta-embeddings approach. The code for FAME is publicly available1 and compatible with the flair framework (Akbik et al., 2018). Meta-Embeddings. Previous work has seen performance gains by, for example, combining various types of word embeddings (Tsuboi, 2014) or the same type trained on different corpora (Luo et al., 2014). For the combination, some alternatives have been proposed, such as different input channels of a convolutional neural network (Kim, 2014; Zhang et al., 2016), concatenation followed by dimensionality reduction (Yin and Sch¨utze, 2016) or averaging of embeddings (Coates and Bollegala, 2018), e.g., for combining embeddings from multiple languages (Lange et al., 2020b; Reid et al., 2020). More recently, auto-encoders (Bollegala and Bao, 2018; Wu et al., 2020), ensembles of sentence encoders (Poerner et al., 2020) and attentionbased methods (Kiela et al., 2018; Lange et al., 2019a) have been introduced. The latter allows a dynamic (input-based) combination of multiple embeddings. Winata et al. (2019) and Priyadharshini et al. (2020) used similar atten"
2021.emnlp-main.660,D17-1004,0,0.0249187,"standard domains. In this paper, we follow the idea of attention-based meta-embeddings and propose task-independent methods for improving them. Extended Attention. Attention has been introduced in the context of machine translation (Bahdanau et al., 2015) and is since then widely used in NLP (i.a., Tai et al., 2015; Xu et al., 2015; Yang et al., 2016; Vaswani et al., 2017). Our approach extends this technique by integrating word features into the attention function. This is similar to extending the source of attention for uncertainty detection (Adel and Sch¨utze, 2017) or relation extraction (Zhang et al., 2017b; Li et al., 2019). However, in contrast to these works, we use task-independent features derived from the token itself. Thus, we can use the same attention function for different tasks. Adversarial Training. Further, our method is motivated by the usage of adversarial training (Goodfellow et al., 2014) for creating input representations that are independent of a specific domain or feature. This is related to using adversarial train2 Related Work ing for domain adaptation (Ganin et al., 2016) or This section surveys related work on meta- coping with bias or confounding variables (Li et al., 2"
2021.emnlp-main.684,P19-1041,0,0.0184015,"been evaluated for both privacy and utility preservation in downstream tasks. 2 Related Work Attribute information such as gender, age, or race are being captured in the deep learning models. Traditional approaches prevented this information leakage via lexical substitution of sensitive words (Reddy and Knight, 2016). In recent years, many text style transfer techniques have been proposed to control certain attributes of generated text (e.g., formality or politeness) while preserving the content. A common paradigm is to disentangle the content and style in the latent space (Shen et al., 2017; John et al., 2019; Cheng et al., 2020). Another stream of work treats text style transfer as an analogy of unsupervised machine translation (Zhang et al., 2018; Lample et al., 2019; Zhao et al., 2019; He et al., 2020) to rephrase a sentence while reducing its stylistic properties (Prabhumoye et al., 2018). Beyond the end-to-end training methods, the prototype-based text editing approach also attracts lot of attention (Li et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020), in which attribute markers of input sentences are deleted and then replaced by target attribute markers. These techniques have been w"
2021.emnlp-main.684,2020.emnlp-main.55,0,0.0492809,"Missing"
2021.emnlp-main.684,N18-1169,0,0.063761,"Missing"
2021.emnlp-main.684,W04-1013,0,0.0204367,"Missing"
2021.emnlp-main.684,2020.acl-main.169,0,0.0445274,"liteness) while preserving the content. A common paradigm is to disentangle the content and style in the latent space (Shen et al., 2017; John et al., 2019; Cheng et al., 2020). Another stream of work treats text style transfer as an analogy of unsupervised machine translation (Zhang et al., 2018; Lample et al., 2019; Zhao et al., 2019; He et al., 2020) to rephrase a sentence while reducing its stylistic properties (Prabhumoye et al., 2018). Beyond the end-to-end training methods, the prototype-based text editing approach also attracts lot of attention (Li et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020), in which attribute markers of input sentences are deleted and then replaced by target attribute markers. These techniques have been well studied in the text style transfer community, but have never been evaluated for both privacy and utility preservation in downstream tasks. Shetty et al. (2018) and Xu et al. (2019) make use of adversarial training and evaluate on authorship obfuscation. However, they did not include most recent style transfer methods and predictors based on pretrained language models. 3 Multilingual Back-Translation Pivot Language Translated Back-translated DE FR ZH Danke P"
2021.emnlp-main.684,P02-1040,0,0.110999,"Missing"
2021.emnlp-main.684,P18-1080,0,0.38164,"e resulting data is often overlooked. Conversely, we argue that the privacy-utility dichotomy should be at the heart of all research on this topic because it is fairly easy to consider one of the two but difficult to improve both at the same time. In this paper, we explore a simple yet effective zero-shot text transformation method based on multilingual back-translation. Back-translation (BT) is an alternative approach without the prerequisites of labeled training data. Sensitive user traits can be significantly obfuscated when translated to another language and back (Rabinovich et al., 2017; Prabhumoye et al., 2018) since many concepts cannot easily be mapped across languages. For example, in languages such as Japanese and Korean the speaker’s gender can be inferred from the choice of certain pronouns. When back-translating them via an intermediate language that does not make such differences, such as English, these gender indicators will be largely obfuscated. Results from extensive experiments show that our simple zero-shot text transformer has comparable or even better performance than popular style transfer methods, considering both the privacy and utility of the transformed texts. In summary, we mak"
2021.emnlp-main.684,E17-1101,0,0.0990389,"have on the utility of the resulting data is often overlooked. Conversely, we argue that the privacy-utility dichotomy should be at the heart of all research on this topic because it is fairly easy to consider one of the two but difficult to improve both at the same time. In this paper, we explore a simple yet effective zero-shot text transformation method based on multilingual back-translation. Back-translation (BT) is an alternative approach without the prerequisites of labeled training data. Sensitive user traits can be significantly obfuscated when translated to another language and back (Rabinovich et al., 2017; Prabhumoye et al., 2018) since many concepts cannot easily be mapped across languages. For example, in languages such as Japanese and Korean the speaker’s gender can be inferred from the choice of certain pronouns. When back-translating them via an intermediate language that does not make such differences, such as English, these gender indicators will be largely obfuscated. Results from extensive experiments show that our simple zero-shot text transformer has comparable or even better performance than popular style transfer methods, considering both the privacy and utility of the transformed"
2021.emnlp-main.684,2021.naacl-main.92,0,0.0251785,"extend the zero-shot BT method with some supervision to improve privacy. We highlight a few limitations of our work. First, back-translation transformation remove content style but does not necessarily replace attribute markers like style transfer models, for example, given a text “me and my husband ...”, style trans5 https://github.com/uds-lsv/ author-profiling-prevention-BT fer models are more likely to change “husband” to “wife” but back-translation will not. Second, our back-translation technique also inherit some of the problems of machine translation generated texts like hallucination (Raunak et al., 2021). We provide examples highlighting these issues in Appendix C. 6 Broader Impact Statement and Ethics This paper presents an approach to prevent author profiling of sensitive user attributes. We understand there are many ethical concerns around gender and race, however, our definition and evaluation of user traits are constrained by the available datasets we found in the literature. We did not collect any new data to show the strength of our approach. We hope our research helps to protect the profiling of under-represented groups and communities. Acknowledgements The presented research has been"
2021.emnlp-main.684,D19-1325,0,0.0407397,"Missing"
2021.emnlp-main.684,1983.tc-1.13,0,0.442092,"Missing"
2021.emnlp-main.684,D19-1322,0,0.0173452,"(e.g., formality or politeness) while preserving the content. A common paradigm is to disentangle the content and style in the latent space (Shen et al., 2017; John et al., 2019; Cheng et al., 2020). Another stream of work treats text style transfer as an analogy of unsupervised machine translation (Zhang et al., 2018; Lample et al., 2019; Zhao et al., 2019; He et al., 2020) to rephrase a sentence while reducing its stylistic properties (Prabhumoye et al., 2018). Beyond the end-to-end training methods, the prototype-based text editing approach also attracts lot of attention (Li et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020), in which attribute markers of input sentences are deleted and then replaced by target attribute markers. These techniques have been well studied in the text style transfer community, but have never been evaluated for both privacy and utility preservation in downstream tasks. Shetty et al. (2018) and Xu et al. (2019) make use of adversarial training and evaluate on authorship obfuscation. However, they did not include most recent style transfer methods and predictors based on pretrained language models. 3 Multilingual Back-Translation Pivot Language Translated Back-trans"
2021.emnlp-main.684,tiedemann-2012-parallel,0,0.0146576,"form of “dad” and thereby protects the user privacy. Specifically, we define our text transformation function as: X 0 = TL→en (Ten→L (X)) where L is the pivot language and T is a translation model. We make use of mBART501 — an off-theshelf machine translation model implemented by HuggingFace (Wolf et al., 2020). We consider 6 high-resourced languages as the pivot, so as to ensure a decent quality of machine translation models. The languages chosen are German (DE), Spanish (ES), French (FR), Japanese (JA), Russian (RU), and Chinese (ZH) based on the large amount of resources they have on OPUS (Tiedemann, 2012) and Common Crawl corpora 2 . Problem Scenario In understanding human be1 https://huggingface.co/facebook/ haviors and intents, many machine learning appli- mbart-large-50-many-to-many-mmt 2 cations need to infer important information from https://commoncrawl.org/ 8688 Dataset DIAL (race) VerbMobil (gender) Yelp (gender) Attribute Train Utility Train Style Train Dev Test 80K 5K 2.6M 100K 4977 373K 100K 5K 200K 4K 4K 442 1096 4K 4K Table 2: Data splits for DIAL, VerbMobil, and Yelp. The utility task for Yelp and DIAL is sentiment classification while for VerbMobil is dialog act classification."
2021.emnlp-main.684,Q19-1040,0,0.0435109,"Missing"
2021.emnlp-main.684,weilhammer-etal-2002-multi,0,0.0204556,"58.69 BT (DE) BT (ES) BT (FR) BT (JA) BT (RU) BT (ZH) 81.37 69.44 77.72 73.77 78.81 66.65 73.84 70.33 72.60 72.00 73.00 71.68 47.47 32.76 41.78 34.63 42.98 27.61 51.83 63.50 54.88 62.22 50.38 80.95 47.94 49.29 47.89 48.77 46.89 53.40 Adv SMDSP 65.75 74.85 65.70 69.88 17.03 28.15 _ _ _ _ CAE BST UNMT DLS Tag&Gen 35.37 13.99 18.11 28.13 44.34 61.63 54.16 64.68 66.18 69.74 12.84 5.03 19.95 25.04 42.30 22.08 10.60 43.87 30.28 23.18 40.30 38.95 52.60 48.34 47.72 Method Experiments and Results Datasets In this paper, we conduct experiments on three datasets: DIAL (Blodgett et al., 2016), VerbMobil (Weilhammer et al., 2002) and Yelp (Reddy and Knight, 2016; Shen et al., 2017). These datasets comprise of a variety of domains with either race or gender as the sensitive attribute and they also have annotations for dialog acts and sentiment classification that we use to test the utility of downstream NLP tasks. For Yelp, we find two datasets previously used in the style transfer literature, one for gender (YelpGender) (Reddy and Knight, 2016) and the other for sentiment (YelpSentiment) (Shen et al., 2017). The texts are from the same source but each review do not have both gender and sentiment labels. By automatical"
2021.emnlp-main.684,P19-1216,1,0.831278,"g models. Traditional approaches prevented this information leakage via lexical substitution of sensitive words (Reddy and Knight, 2016). In recent years, many text style transfer techniques have been proposed to control certain attributes of generated text (e.g., formality or politeness) while preserving the content. A common paradigm is to disentangle the content and style in the latent space (Shen et al., 2017; John et al., 2019; Cheng et al., 2020). Another stream of work treats text style transfer as an analogy of unsupervised machine translation (Zhang et al., 2018; Lample et al., 2019; Zhao et al., 2019; He et al., 2020) to rephrase a sentence while reducing its stylistic properties (Prabhumoye et al., 2018). Beyond the end-to-end training methods, the prototype-based text editing approach also attracts lot of attention (Li et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020), in which attribute markers of input sentences are deleted and then replaced by target attribute markers. These techniques have been well studied in the text style transfer community, but have never been evaluated for both privacy and utility preservation in downstream tasks. Shetty et al. (2018) and Xu et al. (201"
2021.emnlp-main.689,W15-2103,0,0.0347435,"Missing"
2021.emnlp-main.689,I13-1041,0,0.0349265,"et of domains is usually limited and the focus is on the single-best source. In contrast, we exploit sources from a larger set of domains and also explore the prediction of sets of sources, as using multiple sources is likely to be beneficial, as also shown by Parvez and Chang (2021) contemporaneously to this work. 3 3.2 Similarity Measures We apply the following measures to rank sources according to their similarity with the target data. Baselines. We use the most promising domain similarity measures reported by Dai et al. (2020): Vocabulary and Annotation overlap, Language model perplexity (Baldwin et al., 2013), Dataset size (Bingel and Søgaard, 2017) and Term distribution (Ruder and Plank, 2017). We also compare to domain similarity via Text embeddings and task similarity using Task embeddings (Vu et al., 2020). Model similarity. As a new strong method, we propose Model similarity that is able to combine domain and task similarity. For this, feature vectors f for a target dataset t are extracted from the last layer of two models ms , mt which have been trained on the source and target datasets, respectively. The features are then aligned by a linear transformation W , a learned mapping, between the"
2021.mtsummit-research.7,P19-1310,0,0.168016,"Missing"
2021.mtsummit-research.7,P17-1042,0,0.017738,"e monolingual Wikipedias to initialize SSNMT. As the monolingual Wikipedia for Yor`ub´a is especially small (65 k sentences), we use the Yor`ub´a side of JW300 (Agi´c and Vuli´c, 2019) as additional monolingual initialization data. For each monolingual data pair en–{af ,...,yo}, the large English monolingual corpus is downsampled to its low(er)-resource counterpart before using the data (Monolingual in Table 1). For the word-embedding-based initialization, we learn CBOW word embeddings using word2vec (Mikolov et al., 2013), which are then projected into a common multilingual space via vecmap (Artetxe et al., 2017) to attain bilingual embeddings between en–{af ,...,yo}. For the weak-supervision of the bilingual mapping process, we use a list of numbers (en–f r only) which is augmented with 200 Swadesh list4 entries for the low-resource experiments. For DAE initialization, we do not use external, highly-multilingual pre-trained language models, since in practical terms these may not cover the language combination of interest5 . We therefore use the monolingual data to train a bilingual (en+{af ,...yo}) DAE using BART-style 1 Dumps were downloaded on February 2021 from dumps.wikimedia.org/ 2 github.com/at"
2021.mtsummit-research.7,P19-1019,0,0.214691,"g autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili). 1 Introduction Neural machine translation (NMT) achieves high quality translations when large amounts of parallel data are available (Barrault et al., 2020). Unfortunately, for most language combinations, parallel data is non-existent, scarce or low-quality. To overcome this, unsupervised MT (UMT) (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019) focuses on exploiting large amounts of monolingual data, which are used to generate synthetic bitext training data via various techniques such as back-translation or denoising. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) learns from smaller amounts of comparable data –i.e. topic-aligned data such as Wikipedia articles– by learning to discover and exploit similar sentence pairs. However, both UMT and SSNMT approaches often do not scale to low-resource languages, for which neither monolingual nor comparable data are available in sufficient quantity (Guzm´an et al., 2019; Espa˜na-Bonet et"
2021.mtsummit-research.7,D18-1549,0,0.230264,"al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on smal"
2021.mtsummit-research.7,P19-1309,0,0.372739,"low-resource similar and distant language pairs, i.e. English (en)–{Afrikaans (af ), Kannada (kn), Burmese (my), Nepali (ne), Swahili (sw), Yor`ub´a (yo)}, chosen based on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation ofte"
2021.mtsummit-research.7,Q19-1038,0,0.168425,"low-resource similar and distant language pairs, i.e. English (en)–{Afrikaans (af ), Kannada (kn), Burmese (my), Nepali (ne), Swahili (sw), Yor`ub´a (yo)}, chosen based on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation ofte"
2021.mtsummit-research.7,P06-4018,0,0.0228013,"5) for word sequence masking. We add one random mask insertion per sequence and perform a sequence permutation. For the multilingual DAE (MDAE) setting, we train a single denoising autoencoder on the monolingual data of all languages, where en is downsampled to match the largest non-English monolingual dataset (kn). In all cases SSNMT training is bidirectional between two languages en–{af ,...,yo}, except for MDAE, where SSNMT is trained multilingually between all language combinations in {af ,en,...,yo}. 4.2 Preprocessing On the Wikipedia corpora, we perform sentence tokenization using NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardso"
2021.mtsummit-research.7,W11-2138,0,0.0261784,"., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional"
2021.mtsummit-research.7,W19-5435,0,0.0270045,"Missing"
2021.mtsummit-research.7,2020.acl-main.747,0,0.0744637,"Missing"
2021.mtsummit-research.7,W17-4715,0,0.0387347,"ed on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been us"
2021.mtsummit-research.7,2020.eamt-1.10,0,0.073161,"Missing"
2021.mtsummit-research.7,D18-1045,0,0.0130344,"d translation. Given a rejected sentence sL1 with tokens wL1 ∈ L1, we replace each token with its nearest neighbor wL2 ∈ L2 in the bilingual word embedding layer of the T WT model to obtain sW L2 . We then train on the synthetic pair in the opposite direction sL2 → sL1 . As with BT, this is applied to both language directions. To ensure sufficient volume of synthetic data (Figure 2, right), WT data is trained on without filtering. Noise (N): To increase robustness and variance in the training data, we add noise, i.e. token deletion, substitution and permutation, to copies of source sentences (Edunov et al., 2018) in parallel pairs identified via SPE, back-translations and word-translated sentences and, as with WT, we use these without additional filtering. Initialization: When languages are related and large amounts of training data is available, the initialization of SSNMT is not important. However, similarly to UMT, initialization becomes crucial in the low-resource setting (Edman et al., 2020). We explore four different initialization techniques: i) no initialization (none), i.e. random initialization for all model parameters, ii) initialization of tied source and target side word embedding layers"
2021.mtsummit-research.7,2020.emnlp-main.480,0,0.0278832,"Missing"
2021.mtsummit-research.7,W19-5315,1,0.881943,"Missing"
2021.mtsummit-research.7,D19-1632,0,0.147595,"Missing"
2021.mtsummit-research.7,W18-2703,0,0.0179433,"amachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results ("
2021.mtsummit-research.7,Q17-1024,0,0.229862,"Missing"
2021.mtsummit-research.7,2020.eamt-1.5,0,0.0358621,"Missing"
2021.mtsummit-research.7,W04-3250,0,0.0618963,"Missing"
2021.mtsummit-research.7,P07-2045,0,0.010509,"(MDAE) setting, we train a single denoising autoencoder on the monolingual data of all languages, where en is downsampled to match the largest non-English monolingual dataset (kn). In all cases SSNMT training is bidirectional between two languages en–{af ,...,yo}, except for MDAE, where SSNMT is trained multilingually between all language combinations in {af ,en,...,yo}. 4.2 Preprocessing On the Wikipedia corpora, we perform sentence tokenization using NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardson, 2018). We prepend a source and a target language token to each sentence. For the en–f r experiments only, we use the data processing by Ruite"
2021.mtsummit-research.7,2021.dravidianlangtech-1.7,0,0.0417202,"n be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolingual data (Guzm´an et al., 2019), when there is a domain mismatch between the two datasets (Kim et al., 2020) or when the languages in a pair are distant (Koneru et al., 2021). Unfortunately, all of this is the case for most truly low-resource language pairs. Self-supervised NMT (Ruiter et al., 2019) jointly learns to extract data and translate from comparable data and works best on 100s of thousands of documents per language, well beyond what is available in true low-resource settings. 3 UMT-Enhanced SSNMT SSNMT jointly learns MT and extracting similar sentences for training from comparable corpora in a loop on-line. Sentence pairs from documents in languages L1 and L2 are fed as input to a bidirectional NMT system {L1, L2} → {L1, L2}, which filters out non-simila"
2021.mtsummit-research.7,D18-2012,0,0.0218435,"NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardson, 2018). We prepend a source and a target language token to each sentence. For the en–f r experiments only, we use the data processing by Ruiter et al. (2020) in order to minimize experimental differences for later comparison. 4.3 Model Specifications and Evaluation Systems are either not initialized, initialized via bilingual word embeddings, or via pre-training using (M)DAE. Our implementation of SSNMT is a transformer base with default parameters. We use a batch size of 50 sentences and a maximum sequence length of 100 tokens. For evaluation, we use BLEU (Papineni et al., 2002) calculated using Sa"
2021.mtsummit-research.7,P19-1017,0,0.0159691,"rocessing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several la"
2021.mtsummit-research.7,2020.findings-emnlp.371,0,0.0206148,". NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolingual data (Guzm´an et al., 2019), when there is a domain mismatch between the two datasets (Kim et al., 2020) or when the languages in a pair are distant (Koneru et al., 2021). Unfortunately, all of this is the case for most truly low-resource language pairs. Self-supervised NMT (Ruiter et al., 2019) jointly learns to extract data and translate fr"
2021.mtsummit-research.7,E17-2002,0,0.0448202,"Missing"
2021.mtsummit-research.7,2020.wmt-1.68,0,0.0126971,"exploiting large amounts of monolingual data, which are used to generate synthetic bitext training data via various techniques such as back-translation or denoising. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) learns from smaller amounts of comparable data –i.e. topic-aligned data such as Wikipedia articles– by learning to discover and exploit similar sentence pairs. However, both UMT and SSNMT approaches often do not scale to low-resource languages, for which neither monolingual nor comparable data are available in sufficient quantity (Guzm´an et al., 2019; Espa˜na-Bonet et al., 2019; Marchisio et al., 2020). To date, UMT data augmentation techniques have not been explored in SSNMT. However, both approaches can benefit from each other, as i) SSNMT has strong internal quality checks on the data it admits for training, which can be Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 76 of use to filter low-quality synthetic data, and ii) UMT data augmentation makes monolingual data available for SSNMT. In this paper we explore and test the effect of combining UMT data augmentation with SSNMT on different data sizes, ranging"
2021.mtsummit-research.7,W18-2710,0,0.018043,"high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). Ho"
2021.mtsummit-research.7,P02-1040,0,0.109861,"Missing"
2021.mtsummit-research.7,W18-6319,0,0.022108,"Missing"
2021.mtsummit-research.7,D17-1039,0,0.161776,"word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and b"
2021.mtsummit-research.7,P19-1178,1,0.848055,"Missing"
2021.mtsummit-research.7,2020.emnlp-main.202,1,0.852975,"Missing"
2021.mtsummit-research.7,2021.eacl-main.115,0,0.354207,"Missing"
2021.mtsummit-research.7,P16-1009,0,0.0936491,"aging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a rel"
2021.mtsummit-research.7,P16-1162,0,0.546366,"aging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a rel"
2021.mtsummit-research.7,P18-1005,0,0.0178722,"zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolin"
2021.mtsummit-research.7,D16-1163,0,0.0451399,"iques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et"
2021.naacl-main.201,P19-1310,0,0.0230194,"Missing"
2021.naacl-main.201,2020.acl-main.692,0,0.0226689,"are is also low-resource. Biljon et al. (2020) showed that low- to medium- ple, Friedrich et al. (2020) showed that a generaldomain BERT model performs well in the materials depth transformer sizes perform better than larger science domain, but the domain-adapted SciBERT models for low-resource languages and Schick performs best. Xu et al. (2020) used in- and out-ofand Schütze (2020) managed to train models with domain data to pre-train a domain-specific model three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT- and adapt it to low-resource domains. Aharoni and Goldberg (2020) found domain-specific clus3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) ters in pre-trained language models and showed how these could be exploited for data selection in showed that simple bag-of-words approaches are domain-sensitive training. better when there are only a few dozen training instances or less for text classification, while more Powerful representations can be achieved by complex transformer models require more training combining high-resource embeddings from the gen2551 eral domain with low-resource embeddings from the target"
2021.naacl-main.201,C18-1139,0,0.0150531,"main-specific clus3 on few-shot task by reformulating the training task and using ensembling. Melamud et al. (2019) ters in pre-trained language models and showed how these could be exploited for data selection in showed that simple bag-of-words approaches are domain-sensitive training. better when there are only a few dozen training instances or less for text classification, while more Powerful representations can be achieved by complex transformer models require more training combining high-resource embeddings from the gen2551 eral domain with low-resource embeddings from the target domain (Akbik et al., 2018; Lange et al., 2019b). Kiela et al. (2018) showed that embeddings from different domains can be combined using attention-based meta-embeddings, which create a weighted sum of all embeddings. Lange et al. (2020b) further improved on this by aligning embeddings trained on diverse domains using an adversarial discriminator that distinguishes between the embedding spaces to generate domain-invariant representations. 5.3 Multilingual Language Models 30 15 14 16 13 12 Language families with &gt;1 mio. speakers BERT RoBERTa 8 2 2 All Asian African 6 5 4 European 3 0 0 3 0 0 South North American America"
2021.naacl-main.201,2020.lrec-1.335,0,0.0174684,"BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019). data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources. 5.2 Domain-Specific Pre-Training The language of a specialized domain can differ tremendously from what is considered the standard language, thus, many text domains are often lessresourced as well. For example, scientific articles can contain formulas and technical terms, which are not observed in news articles. However, the majority of recent language models are pre-trained on general-domain data, such as texts from"
2021.naacl-main.201,2020.coling-demos.1,0,0.0125165,"t al. (2020) take the opposite direction, the training data, e.g., through a probability threshintegrating speakers of low-resource languages old (Jia et al., 2019), a binary classifier (Adel and without formal training into the model development Schütze, 2015; Onoe and Durrett, 2019; Huang and process in an approach of participatory research. Du, 2019), or the use of a reinforcement-based This is part of recent work on how to strengthen agent (Yang et al., 2018; Nooralahzadeh et al., low-resource language communities and grassroot 2019). Alternatively, a soft filtering might be apapproaches (Alnajjar et al., 2020; Adelani et al., plied that re-weights instances according to their 2021). probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). 5 Transfer Learning The noise in the labels can also be modeled. A While distant supervision and data augmentation common model is a confusion matrix estimating generate and extend task-specific training data, the relationship between clean and noisy labels transfer learning reduces the need for labeled tar(Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., get data by"
2021.naacl-main.201,W19-1909,0,0.0464648,"Missing"
2021.naacl-main.201,P19-1134,0,0.0222776,"xt with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng Adversarial methods are often used to find weak- and Sun, 2019; Alt et al., 2019; Ye et al., 2019; nesses in machine learning models (Jin et al., 2020; Lange et al., 2019a; Nooralahzadeh et al., 2019; Garg and Ramakrishnan, 2020). They can, how- Le and Titov, 2019; Cao et al., 2019; Lison et al., ever, also be utilized to augment NLP datasets (Ya- 2020; Hedderich et al., 2021a). The automatic ansunaga et al., 2018; Morris et al., 2020). Instead of notation ranges from simple string matching (Yang manually crafted transformation rules, these meth- et al., 2018) to complex pipelines including classiods learn how to apply small perturbations to the fiers and manual steps (No"
2021.naacl-main.201,2020.lrec-1.309,0,0.0228764,"Missing"
2021.naacl-main.201,2020.acl-main.527,0,0.0162376,"ther areas, like general machine learning and computer vision, can be a useful source for insights and new ideas. We already presented data augmentation and pretraining. Another example is Meta-Learning (Finn et al., 2017), which is based on multi-task learning. Given a set of auxiliary high-resource tasks and a low-resource target task, meta-learning trains a model to decide how to use the auxiliary tasks in the most beneficial way for the target task. For NLP, this approach has been evaluated on tasks such as sentiment analysis (Yu et al., 2018), user intent classification (Yu et al., 2018; Chen et al., 2020b), natural language understanding (Dou et al., 2019), text classification (Bansal et al., 2020) and dialogue generation (Huang et al., 2020). Instead of having a set of tasks, Rahimi et al. (2019) built an ensemble of language-specific NER models which are then weighted depending on the zero- or few-shot target language. Differences in the features between the pretraining and the target domain can be an issue in transfer learning, especially in neural approaches where it can be difficult to control which information the model takes into account. Adversarial discriminators (Goodfellow et al.,"
2021.naacl-main.201,D19-1031,0,0.0207367,"according to their 2021). probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). 5 Transfer Learning The noise in the labels can also be modeled. A While distant supervision and data augmentation common model is a confusion matrix estimating generate and extend task-specific training data, the relationship between clean and noisy labels transfer learning reduces the need for labeled tar(Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., get data by transferring learned representations and 2019a,c; Chen et al., 2019; Wang et al., 2019; Hed- models. A strong focus in recent works on transfer learning in NLP lies in the use of pre-trained landerich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, guage representations that are trained on unlabeled data like BERT (Devlin et al., 2019). Thus, this a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be in- section starts with an overview of these methods (§ 5.1) and then discusses how they can be utilized terpreted as the original classifier being trained on a “c"
2021.naacl-main.201,2020.emnlp-main.413,0,0.0935215,"Missing"
2021.naacl-main.201,Q18-1039,0,0.0155743,"few-shot target language. Differences in the features between the pretraining and the target domain can be an issue in transfer learning, especially in neural approaches where it can be difficult to control which information the model takes into account. Adversarial discriminators (Goodfellow et al., 2014) can prevent the model from learning a feature-representation that is specific to a data source. Gui et al. (2017), Liu et al. (2017), Kasai et al. (2019), Grießhaber et al. (2020) and Zhou et al. (2019) learned domainindependent representations using adversarial training. Kim et al. (2017), Chen et al. (2018) and Lange et al. (2020c) worked with language-independent representations for cross-lingual transfer. These examples show the beneficial exchange of ideas between NLP and the machine learning community. 7 Discussion and Conclusion This can reveal which techniques are expected to be applicable in a specific low-resource setting. More theoretic and experimental work is necessary to understand how approaches compare to each other and on which factors their effectiveness depends. Longpre et al. (2020), for instance, hypothesized that data augmentation and pre-trained language models yield similar"
2021.naacl-main.201,L16-1720,0,0.0551918,"Missing"
2021.naacl-main.201,D18-1217,0,0.0281488,"es found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b). These methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce (Cruz and Cheng, 2019). data. Bhattacharjee et al. (2020) found that crossview training (Clark et al., 2018) leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT. Moreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. Alabi et al. (2020) found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources. 5.2 Domain-Specific Pre-Training The language of a specialized domain can differ tremendously from what is considered the standard l"
2021.naacl-main.201,D18-1330,0,0.0121649,"lingual transformer models. gual model, i.a., in cross-lingual (Schuster et al., 2019; Liu et al., 2019a) or multilingual settings (Cao et al., 2020). Analogously to low-resource domains, lowThis alignment is typically done by computing a resource languages can also benefit from labeled re- mapping between two different embedding spaces, sources available in other high-resource languages. such that the words in both embeddings share simiThis usually requires the training of multilingual lar feature vectors after the mapping (Mikolov et al., language representations by combining monolin- 2013; Joulin et al., 2018). This allows to use differgual representations (Lange et al., 2020a) or train- ent embeddings inside the same model and helps ing a single model for many languages, such as when two languages do not share the same space inmultilingual BERT (Devlin et al., 2019) or XLM- side a single model (Cao et al., 2020). For example, RoBERTa (Conneau et al., 2020) . These models Zhang et al. (2019b) used bilingual representations are trained using unlabeled, monolingual corpora by creating cross-lingual word embeddings using from different languages and can be used in cross- a small set of parallel senten"
2021.naacl-main.201,2020.lrec-1.437,0,0.0267748,"reused for other downstream tasks as proaches learn latent variables (Jie et al., 2019), use well. Subword-based embeddings such as fastText 2550 n-gram embeddings (Bojanowski et al., 2017) and byte-pair-encoding embeddings (Heinzerling and Strube, 2018) addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word. Zhu et al. (2019) showed that these embeddings leveraging subword information are beneficial for lowresource sequence labeling tasks, such as named entity recognition and typing, and outperform wordlevel embeddings. Jungmaier et al. (2020) added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings. In addition, pre-trained embeddings were published for more than 270 languages for both embedding methods. This enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia. More recently, a trend emerged of pre-training large embedding models using a language model objective to create contextaware word representations by predicting the next word or sentence. This includes pre-trained transformer models"
2021.naacl-main.201,D19-1676,0,0.0220878,"or obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013; Wisniewski et al., 2014; Plank and Agi´c, 2018; Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019; Zhang et al., 2019a; Fei et al., 2020; Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015; Marasovi´c et al., 2016; Friedrich and Gateva, 2017). Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both While distant supervision allows obtaining la- labels in a high-resource language and means to beled data more quickly than manually annotat- project them into a low-resou"
2021.naacl-main.201,D18-1176,0,0.0493269,"formulating the training task and using ensembling. Melamud et al. (2019) ters in pre-trained language models and showed how these could be exploited for data selection in showed that simple bag-of-words approaches are domain-sensitive training. better when there are only a few dozen training instances or less for text classification, while more Powerful representations can be achieved by complex transformer models require more training combining high-resource embeddings from the gen2551 eral domain with low-resource embeddings from the target domain (Akbik et al., 2018; Lange et al., 2019b). Kiela et al. (2018) showed that embeddings from different domains can be combined using attention-based meta-embeddings, which create a weighted sum of all embeddings. Lange et al. (2020b) further improved on this by aligning embeddings trained on diverse domains using an adversarial discriminator that distinguishes between the embedding spaces to generate domain-invariant representations. 5.3 Multilingual Language Models 30 15 14 16 13 12 Language families with &gt;1 mio. speakers BERT RoBERTa 8 2 2 All Asian African 6 5 4 European 3 0 0 3 0 0 South North American American 1 0 0 Oceanic Figure 2: Language families"
2021.naacl-main.201,D17-1302,0,0.0184194,"ng on the zero- or few-shot target language. Differences in the features between the pretraining and the target domain can be an issue in transfer learning, especially in neural approaches where it can be difficult to control which information the model takes into account. Adversarial discriminators (Goodfellow et al., 2014) can prevent the model from learning a feature-representation that is specific to a data source. Gui et al. (2017), Liu et al. (2017), Kasai et al. (2019), Grießhaber et al. (2020) and Zhou et al. (2019) learned domainindependent representations using adversarial training. Kim et al. (2017), Chen et al. (2018) and Lange et al. (2020c) worked with language-independent representations for cross-lingual transfer. These examples show the beneficial exchange of ideas between NLP and the machine learning community. 7 Discussion and Conclusion This can reveal which techniques are expected to be applicable in a specific low-resource setting. More theoretic and experimental work is necessary to understand how approaches compare to each other and on which factors their effectiveness depends. Longpre et al. (2020), for instance, hypothesized that data augmentation and pre-trained language"
2021.naacl-main.201,C18-2002,0,0.0517184,"Missing"
2021.naacl-main.201,N18-2072,0,0.0250738,"nyms (Wei and Zou, 2 hours resulting in up to 1-2k tokens. Kann et al. 2019), entities of the same type (Raiman and Miller, (2020) study languages that have less than 10k la- 2017; Dai and Adel, 2020) or words that share the beled tokens in the Universal Dependency project same morphology (Gulordava et al., 2018; Vania (Nivre et al., 2020) and Loubser and Puttkammer et al., 2019). Such replacements can also be guided (2020) report that most available datasets for South by a language model that takes context into considAfrican languages have 40-60k labeled tokens. eration (Fadaee et al., 2017; Kobayashi, 2018). The threshold is also task-dependent and more To go beyond the token level and add more divercomplex tasks might also increase the resource re- sity to the augmented sentences, data augmentation quirements. For text generation, Yang et al. (2019) can also be performed on sentence parts. Operaframe their work as low-resource with 350k la- tions that (depending on the task) do not change the beled training instances. Similar to the task, the label include manipulation of parts of the depenresource requirements can also depend on the lan- dency tree (Sahin ¸ and Steedman, 2018; Vania et al., gu"
2021.naacl-main.201,2020.lifelongnlp-1.3,0,0.0258569,"An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization (Parida and Motlicek, 2019) and table-to-text generation (Ma et al., 2019). Back-translation has also been leveraged for text classification (Xie et al., 2020; Hegde and Patil, 2020). This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets (Kumar et al., 2020; Anaby-Tavor et al., 2020). It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. Ding et al. (2020) extend this idea for token level tasks. leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited. 4.2 Distant & Weak Supervision In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from a"
2021.naacl-main.201,D12-1127,0,0.0201323,"correction task. Karamanolakis et al., 2021). Open Issues: While data augmentation is ubiqWhile distant supervision is popular for inforuitous in the computer vision community and while mation extraction tasks like NER and RE, it is most of the above-presented approaches are task- less prevalent in other areas of NLP. Nevertheless, independent, it has not found such widespread use distant supervision has also been successfully emin natural language processing. A reason might ployed for other tasks by proposing new ways for be that several of the approaches require an in- automatic annotation. Li et al. (2012) leverage a depth understanding of the language. There is dictionary of POS tags for classifying unseen text not yet a unified framework that allows applying with POS. For aspect classification, Karamanolakis data augmentation across tasks and languages. Re- et al. (2019) create a simple bag-of-words classicently, Longpre et al. (2020) hypothesised that data fier on a list of seed words and train a deep neuaugmentation provides the same benefits as pre- ral network on its weak supervision. Wang et al. training in transformer models. However, we argue (2019) use context by transferring a docume"
2021.naacl-main.201,2020.emnlp-demos.16,0,0.0878525,"Missing"
2021.naacl-main.201,N19-3005,1,0.832127,"might be apapproaches (Alnajjar et al., 2020; Adelani et al., plied that re-weights instances according to their 2021). probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). 5 Transfer Learning The noise in the labels can also be modeled. A While distant supervision and data augmentation common model is a confusion matrix estimating generate and extend task-specific training data, the relationship between clean and noisy labels transfer learning reduces the need for labeled tar(Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., get data by transferring learned representations and 2019a,c; Chen et al., 2019; Wang et al., 2019; Hed- models. A strong focus in recent works on transfer learning in NLP lies in the use of pre-trained landerich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, guage representations that are trained on unlabeled data like BERT (Devlin et al., 2019). Thus, this a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be in- section starts with an overview of these methods (§ 5.1) and the"
2021.naacl-main.201,P19-1231,0,0.0239408,"n active or religious texts. Mayhew et al. (2017), Fang and learning scheme. Unfortunately, distant supervi- Cohn (2017) and Karamanolakis et al. (2020) prosion papers rarely provide information on how long pose systems with fewer requirements based on the creation took, making it difficult to compare word translations, bilingual dictionaries and taskthese approaches. Taking the human expert into the specific seed words, respectively. 2549 4.4 Learning with Noisy Labels constrained binary learning (Mayhew et al., 2019) or construct a loss assuming that only unlabeled positive instances exist (Peng et al., 2019). The above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain 4.5 Non-Expert Support more errors. Even though more training data is As an alternative to an automatic annotation proavailable, training directly on this noisily-labeled data can actually hurt the performance. Therefore, cess, annotations might also be provided by nonexperts. Similar to distant supervision, this results many recent approaches for distant supervision use in a trade-off between label quality and availability. a noise handling method to d"
2021.naacl-main.201,D18-1061,0,0.0448115,"Missing"
2021.naacl-main.201,P16-2067,0,0.214737,"e bases or gazetteers. Some approaches require other NLP tools in the target language like machine translation to generate training data. It is essential to consider this as results from one low-resource scenario might not be transferable to another one if the assumptions on the auxiliary data are broken. Given the lack of a hard threshold for lowresource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. For instance, Plank et al. (2016) and Melamud et al. (2019) show that for very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available. 4 Generating Additional Labeled Data Faced with the lack of task-specific labels, a variety of approaches have been developed to find alternative forms of labeled data as substitutes for goldstandard supervision. This is usually done through some form of expert insights in combination with automation. We group the ideas into two main categories: data augmentatio"
2021.naacl-main.201,2020.emnlp-main.517,0,0.0431029,"e difficult to obtain not only labeled data but also such auxiliary data. Kann et al. (2020) find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language. focus connects this research direction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018; Qian et al., 2020). 4.3 Cross-Lingual Annotation Projections For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific"
2021.naacl-main.201,P19-1015,0,0.0194299,"Learning (Finn et al., 2017), which is based on multi-task learning. Given a set of auxiliary high-resource tasks and a low-resource target task, meta-learning trains a model to decide how to use the auxiliary tasks in the most beneficial way for the target task. For NLP, this approach has been evaluated on tasks such as sentiment analysis (Yu et al., 2018), user intent classification (Yu et al., 2018; Chen et al., 2020b), natural language understanding (Dou et al., 2019), text classification (Bansal et al., 2020) and dialogue generation (Huang et al., 2020). Instead of having a set of tasks, Rahimi et al. (2019) built an ensemble of language-specific NER models which are then weighted depending on the zero- or few-shot target language. Differences in the features between the pretraining and the target domain can be an issue in transfer learning, especially in neural approaches where it can be difficult to control which information the model takes into account. Adversarial discriminators (Goodfellow et al., 2014) can prevent the model from learning a feature-representation that is specific to a data source. Gui et al. (2017), Liu et al. (2017), Kasai et al. (2019), Grießhaber et al. (2020) and Zhou et"
2021.naacl-main.201,D17-1111,0,0.0473595,"Missing"
2021.naacl-main.201,2020.coling-main.603,0,0.0371189,"gual data multilingual feature representation 3 7 Adversarial Discriminator (§ 6) additional datasets independent representations 3 3 Meta-Learning (§ 6) multiple auxiliary tasks better target task performance 3 3 unlabeled Table 1: Overview of low-resource methods surveyed in this paper. * Heuristics are typically gathered manually. tions. Table 1 gives an overview of the surveyed techniques along with their requirements a practitioner needs to take into consideration. Related Surveys Recent surveys cover low-resource machine translation (Liu et al., 2019) and unsupervised domain adaptation (Ramponi and Plank, 2020). Thus, we do not investigate these topics further in this paper, but focus instead on general methods for lowresource, supervised natural language processing including data augmentation, distant supervision and transfer learning. This is also in contrast to the task-specific survey by Magueresse et al. (2020) who review highly influential work for several extraction tasks, but only provide little overview of recent approaches. In Table 2 in the appendix, we list past surveys that discuss a specific method or lowresource language family for those readers who seek a more specialized follow-up."
2021.naacl-main.201,P17-1107,0,0.012348,"which shifts the noisy to the (unseen) clean label distribution. This can be in- section starts with an overview of these methods (§ 5.1) and then discusses how they can be utilized terpreted as the original classifier being trained on a “cleaned” version of the noisy labels. In Ye et al. in low-resource scenarios, in particular, regarding the usage in domain-specific (§ 5.2) or multilingual (2019), the prediction is shifted from the noisy to the clean distribution during testing. In Chen et al. low-resource settings (§ 5.3). (2020a), a group of reinforcement agents relabels noisy instances. Rehbein and Ruppenhofer (2017), 5.1 Pre-Trained Language Representations Lison et al. (2020) and Ren et al. (2020) leverage Feature vectors are the core input component of several sources of distant supervision and learn how many neural network-based models for NLP tasks. to combine them. They are numerical representations of words or senIn NER, the noise in distantly supervised la- tences, as neural architectures do not allow the probels tends to be false negatives, i.e., mentions of cessing of strings and characters as such. Collobert entities that have been missed by the automatic et al. (2011) showed that training thes"
2021.naacl-main.201,2020.findings-emnlp.334,0,0.200976,"ernal text (according to a specific score). This approach knowledge sources can be seen as a subset of the is often applied on the level of vector represen- more general approach of labeling rules. These tations. For instance, Grundkiewicz et al. (2019) encompass also other ideas like reg-ex rules or simreverse the augmentation setting by applying trans- ple programming functions (Ratner et al., 2017; formations that flip the (binary) label. In their case, Zheng et al., 2019; Adelani et al., 2020; Hedderich they introduce errors in correct sentences to obtain et al., 2020; Lison et al., 2020; Ren et al., 2020; new training data for a grammar correction task. Karamanolakis et al., 2021). Open Issues: While data augmentation is ubiqWhile distant supervision is popular for inforuitous in the computer vision community and while mation extraction tasks like NER and RE, it is most of the above-presented approaches are task- less prevalent in other areas of NLP. Nevertheless, independent, it has not found such widespread use distant supervision has also been successfully emin natural language processing. A reason might ployed for other tasks by proposing new ways for be that several of the approaches req"
2021.naacl-main.201,D18-1545,0,0.02768,"e et al., 2017; Kobayashi, 2018). The threshold is also task-dependent and more To go beyond the token level and add more divercomplex tasks might also increase the resource re- sity to the augmented sentences, data augmentation quirements. For text generation, Yang et al. (2019) can also be performed on sentence parts. Operaframe their work as low-resource with 350k la- tions that (depending on the task) do not change the beled training instances. Similar to the task, the label include manipulation of parts of the depenresource requirements can also depend on the lan- dency tree (Sahin ¸ and Steedman, 2018; Vania et al., guage. Plank et al. (2016) find that task perfor- 2019; Dehouck and Gómez-Rodríguez, 2020), simmance varies between language families given the plification of sentences by removal of sentence same amount of limited training data. parts (Sahin ¸ and Steedman, 2018) and inversion 2547 of the subject-object relation (Min et al., 2020). For whole sentences, paraphrasing through backtranslation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences (Bojar and Tamchyna, 2011; Hoang et al., 2018). An important as"
2021.naacl-main.201,2020.coling-main.488,0,0.0264791,"n2548 stances. Mekala et al. (2020) leverage meta-data for text classification and Huber and Carenini (2020) build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER (Bach et al., 2019) or from entity lists (Hedderich et al., 2020). For some classification tasks, the labels can be rephrased with simple rules into sentences. A pretrained language model then judges the label sentence that most likely follows the unlabeled input (Opitz, 2019; Schick and Schütze, 2020; Schick et al., 2020). An unlabeled review, for instance, might be continued with &quot;It was great/bad&quot; for obtaining binary sentiment labels. Open Issues: The popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification (Xiao et"
2021.naacl-main.201,2021.naacl-main.185,0,0.0896104,"Missing"
2021.naacl-main.201,N19-1162,0,0.0124197,"a weighted sum of all embeddings. Lange et al. (2020b) further improved on this by aligning embeddings trained on diverse domains using an adversarial discriminator that distinguishes between the embedding spaces to generate domain-invariant representations. 5.3 Multilingual Language Models 30 15 14 16 13 12 Language families with &gt;1 mio. speakers BERT RoBERTa 8 2 2 All Asian African 6 5 4 European 3 0 0 3 0 0 South North American American 1 0 0 Oceanic Figure 2: Language families with more than 1 million speakers covered by multilingual transformer models. gual model, i.a., in cross-lingual (Schuster et al., 2019; Liu et al., 2019a) or multilingual settings (Cao et al., 2020). Analogously to low-resource domains, lowThis alignment is typically done by computing a resource languages can also benefit from labeled re- mapping between two different embedding spaces, sources available in other high-resource languages. such that the words in both embeddings share simiThis usually requires the training of multilingual lar feature vectors after the mapping (Mikolov et al., language representations by combining monolin- 2013; Joulin et al., 2018). This allows to use differgual representations (Lange et al., 20"
2021.naacl-main.201,L16-1521,0,0.067854,"Missing"
2021.naacl-main.201,D12-1042,0,0.0494158,"k Supervision In contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng Adversarial methods are often used to find weak- and Sun, 2019; Alt et al., 2019; Ye et al., 2019; nesses in machine learning models (Jin et al., 2020; Lange et al., 2019a; Nooralahzadeh et al., 2019; Garg and Ramakrishnan, 2020). They can, how- Le and Titov, 2019; Cao et al., 2019; Lison et al., ever, also be utilized to"
2021.naacl-main.201,Q13-1001,0,0.036248,"e unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013; Wisniewski et al., 2014; Plank and Agi´c, 2018; Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019; Zhang et al., 2019a; Fei et al., 2020; Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic ph"
2021.naacl-main.201,tiedemann-2012-parallel,0,0.0594246,"classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013; Wisniewski et al., 2014; Plank and Agi´c, 2018; Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019; Zhang et al., 2019a; Fei et al., 2020; Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015; Marasovi´c et al., 2016; Friedr"
2021.naacl-main.201,W13-2412,0,0.0253294,"Missing"
2021.naacl-main.201,2020.sltu-1.39,0,0.0314477,"Missing"
2021.naacl-main.201,D19-1102,0,0.0596374,"Missing"
2021.naacl-main.201,D19-1655,0,0.0278206,"2021). probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). 5 Transfer Learning The noise in the labels can also be modeled. A While distant supervision and data augmentation common model is a confusion matrix estimating generate and extend task-specific training data, the relationship between clean and noisy labels transfer learning reduces the need for labeled tar(Fang and Cohn, 2016; Luo et al., 2017; Hedderich and Klakow, 2018; Paul et al., 2019; Lange et al., get data by transferring learned representations and 2019a,c; Chen et al., 2019; Wang et al., 2019; Hed- models. A strong focus in recent works on transfer learning in NLP lies in the use of pre-trained landerich et al., 2021b). The classifier is no longer trained directly on the noisily-labeled data. Instead, guage representations that are trained on unlabeled data like BERT (Devlin et al., 2019). Thus, this a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be in- section starts with an overview of these methods (§ 5.1) and then discusses how they can be utilized terpreted as the original classifier being trained on a “cleaned” version of"
2021.naacl-main.201,D19-1670,0,0.0571908,"Missing"
2021.naacl-main.201,D14-1187,0,0.0265561,"data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013; Wisniewski et al., 2014; Plank and Agi´c, 2018; Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019; Zhang et al., 2019a; Fei et al., 2020; Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense"
2021.naacl-main.201,2020.repl4nlp-1.16,0,0.0130272,"pen Issues: While these multilingual models high-resource language is leveraged. A multilinare a tremendous step towards enabling NLP in gual model can be trained on the target task in a many languages, possible claims that these are unihigh-resource language and afterwards, applied to versal language models do not hold. For example, the unseen target languages, such as for named mBERT covers 104 and XLM-R 100 languages, entity recognition (Lin et al., 2019; Hvingelby which is a third of all languages in Wikipedia as et al., 2020), reading comprehension (Hsu et al., outlined earlier. Further, Wu and Dredze (2020) 2019), temporal expression extraction (Lange et al., showed that, in particular, low-resource languages 2020c), or POS tagging and dependency parsing are not well-represented in mBERT. Figure 2 (Müller et al., 2020). Hu et al. (2020) showed, howshows which language families with at least 1 milever, that there is still a large gap between low and lion speakers are covered by mBERT and XLMhigh-resource setting. Lauscher et al. (2020) and RoBERTa2 . In particular, African and American Hedderich et al. (2020) proposed adding a minilanguages are not well-represented within the transmal amount of t"
2021.naacl-main.201,C18-1183,0,0.0162331,"t al., 2019; ing a classifier to make the filtering decision. The Tsygankova et al., 2020). filtering can remove the instances completely from Nekoto et al. (2020) take the opposite direction, the training data, e.g., through a probability threshintegrating speakers of low-resource languages old (Jia et al., 2019), a binary classifier (Adel and without formal training into the model development Schütze, 2015; Onoe and Durrett, 2019; Huang and process in an approach of participatory research. Du, 2019), or the use of a reinforcement-based This is part of recent work on how to strengthen agent (Yang et al., 2018; Nooralahzadeh et al., low-resource language communities and grassroot 2019). Alternatively, a soft filtering might be apapproaches (Alnajjar et al., 2020; Adelani et al., plied that re-weights instances according to their 2021). probability of being correctly labeled (Le and Titov, 2019) or an attention measure (Hu et al., 2019). 5 Transfer Learning The noise in the labels can also be modeled. A While distant supervision and data augmentation common model is a confusion matrix estimating generate and extend task-specific training data, the relationship between clean and noisy labels transfer"
2021.naacl-main.201,D19-1197,0,0.0265539,"niversal Dependency project same morphology (Gulordava et al., 2018; Vania (Nivre et al., 2020) and Loubser and Puttkammer et al., 2019). Such replacements can also be guided (2020) report that most available datasets for South by a language model that takes context into considAfrican languages have 40-60k labeled tokens. eration (Fadaee et al., 2017; Kobayashi, 2018). The threshold is also task-dependent and more To go beyond the token level and add more divercomplex tasks might also increase the resource re- sity to the augmented sentences, data augmentation quirements. For text generation, Yang et al. (2019) can also be performed on sentence parts. Operaframe their work as low-resource with 350k la- tions that (depending on the task) do not change the beled training instances. Similar to the task, the label include manipulation of parts of the depenresource requirements can also depend on the lan- dency tree (Sahin ¸ and Steedman, 2018; Vania et al., guage. Plank et al. (2016) find that task perfor- 2019; Dehouck and Gómez-Rodríguez, 2020), simmance varies between language families given the plification of sentences by removal of sentence same amount of limited training data. parts (Sahin ¸ and S"
2021.naacl-main.201,D19-1397,0,0.0252411,"n the list are automatically labeled as locations. Distant supervision was introduced by Mintz et al. (2009) for relation extraction (RE) with extensions on multi-instance (Riedel et al., 2010) and multi-label learning (Surdeanu et al., 2012). It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources (Luo et al., 2017; Hedderich and Klakow, 2018; Deng Adversarial methods are often used to find weak- and Sun, 2019; Alt et al., 2019; Ye et al., 2019; nesses in machine learning models (Jin et al., 2020; Lange et al., 2019a; Nooralahzadeh et al., 2019; Garg and Ramakrishnan, 2020). They can, how- Le and Titov, 2019; Cao et al., 2019; Lison et al., ever, also be utilized to augment NLP datasets (Ya- 2020; Hedderich et al., 2021a). The automatic ansunaga et al., 2018; Morris et al., 2020). Instead of notation ranges from simple string matching (Yang manually crafted transformation rules, these meth- et al., 2018) to complex pipelines including classiods learn how to apply small perturbations to the fiers and manual steps (Norman et al., 2019"
2021.naacl-main.201,N18-1109,0,0.0282023,"amount of data is not unique to natural language processing. Other areas, like general machine learning and computer vision, can be a useful source for insights and new ideas. We already presented data augmentation and pretraining. Another example is Meta-Learning (Finn et al., 2017), which is based on multi-task learning. Given a set of auxiliary high-resource tasks and a low-resource target task, meta-learning trains a model to decide how to use the auxiliary tasks in the most beneficial way for the target task. For NLP, this approach has been evaluated on tasks such as sentiment analysis (Yu et al., 2018), user intent classification (Yu et al., 2018; Chen et al., 2020b), natural language understanding (Dou et al., 2019), text classification (Bansal et al., 2020) and dialogue generation (Huang et al., 2020). Instead of having a set of tasks, Rahimi et al. (2019) built an ensemble of language-specific NER models which are then weighted depending on the zero- or few-shot target language. Differences in the features between the pretraining and the target domain can be an issue in transfer learning, especially in neural approaches where it can be difficult to control which information the model tak"
2021.naacl-main.201,D19-1092,0,0.0247934,"Missing"
2021.naacl-main.201,P19-1306,0,0.0545381,"data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013; Wisniewski et al., 2014; Plank and Agi´c, 2018; Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019; Zhang et al., 2019a; Fei et al., 2020; Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015; Marasovi´c et al., 2016; Friedrich and Gateva, 2017). Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both While distant supervision allows obtaining la- labels in a high-resource language and means to beled data more quickly than manually annotat- project them into a low-resource language. Espein"
2021.naacl-main.201,H01-1035,0,0.311833,"ction with humancomputer-interaction and human-in-the-loop setups (Klie et al., 2018; Qian et al., 2020). 4.3 Cross-Lingual Annotation Projections For cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the highresource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts (Yarowsky et al., 2001). This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for lowresource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing (Täckström et al., 2013; Wisniewski et al., 2014; Plank and Agi´c, 2018; Eskander et al., 2020). Sources for parallel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resour"
2021.naacl-main.201,P19-1137,0,0.0259178,"nd manual steps (Norman et al., 2019). This input data that do not change the meaning of the distant supervision using information from external text (according to a specific score). This approach knowledge sources can be seen as a subset of the is often applied on the level of vector represen- more general approach of labeling rules. These tations. For instance, Grundkiewicz et al. (2019) encompass also other ideas like reg-ex rules or simreverse the augmentation setting by applying trans- ple programming functions (Ratner et al., 2017; formations that flip the (binary) label. In their case, Zheng et al., 2019; Adelani et al., 2020; Hedderich they introduce errors in correct sentences to obtain et al., 2020; Lison et al., 2020; Ren et al., 2020; new training data for a grammar correction task. Karamanolakis et al., 2021). Open Issues: While data augmentation is ubiqWhile distant supervision is popular for inforuitous in the computer vision community and while mation extraction tasks like NER and RE, it is most of the above-presented approaches are task- less prevalent in other areas of NLP. Nevertheless, independent, it has not found such widespread use distant supervision has also been successfull"
2021.naacl-main.201,N18-1089,0,0.1979,"Missing"
2021.naacl-main.201,W15-2705,0,0.0246381,"lel text can be the OPUS project (Tiedemann, 2012), Bible corpora (Mayer and Cysouw, 2014; Christodoulopoulos and Steedman, 2015) or the recent JW300 corpus (Agi´c and Vuli´c, 2019). Instead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language (Khalil et al., 2019; Zhang et al., 2019a; Fei et al., 2020; Amjad et al., 2020). Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language (Zhou et al., 2015; Marasovi´c et al., 2016; Friedrich and Gateva, 2017). Open issues: Cross-lingual projections set high requirements on the auxiliary data needing both While distant supervision allows obtaining la- labels in a high-resource language and means to beled data more quickly than manually annotat- project them into a low-resource language. Espeing every instance of a dataset, it still requires cially the latter can be an issue as machine transhuman interaction to create automatic annotation lation by itself might be problematic for a specific techniques or to provide labeling rules. This time low-r"
2021.naacl-main.201,K19-1021,0,0.0460285,"Missing"
2021.woah-1.2,S19-2007,0,0.0252601,"motivated a wide range of natural language processing (NLP) research in recent years. However, the issues are far from solved, and the automatic detection of profane and hateful contents in particular faces a number of severe challenges. Pre-trained transformer-based (Vaswani et al., 2017) language models, e.g. BERT (Devlin et al., 2019), play a dominant role today in many NLP tasks. However, they work best when large amounts of training data are available. This is typically not the case for profanity and hate speech detection where few datasets are currently available (Waseem and Hovy, 2016; Basile et al., 2019; We analyze the efficacy of the subspaces to encode the profanity (neutral vs. profane language) aspect and apply the resulting subspace-based representations to a zero-shot transfer classification scenario with both similar (neutral/profane) and distant (neutral/hate) target classification tasks. To study their ability to generalize across languages we evaluate the zero-shot transfer in both a monolingual (German) and a cross-lingual setting 6 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 6–16 August 6, 2021. ©2021 Association for Computational Linguistics with closely-r"
2021.woah-1.2,N19-1062,0,0.0142222,"aluated on a previously unseen task, has recently gained a lot of traction in NLP. Nowadays, this is done using large-scale transformerbased language models such as BERT, that share parameters between tasks. Multilingual varieties such as XLM-R (Conneau et al., 2020) enable the zero-shot cross-lingual transfer of a task. One example is sentence classification trained on a (highresource) language being transferred into another (low-resource) language (Hu et al., 2020). Related Work Semantic subspaces have been used to identify gender (Bolukbasi et al., 2016) or multiclass ethnic and religious (Manzini et al., 2019) bias in word representations. Liang et al. (2020) identify multiclass (gender, religious) bias in sentence representations. Similarly, Niu and Carpuat (2017) identify a stylistic subspace that captures the degree of formality in a word representation. This is done using a list of minimal-pairs, i.e. pairs of words or sentences that only differ in the semantic feature of interest over which they perform principal component analysis (PCA). We take the same general approach in this paper (see Section 3). Conversely, Gonen and Goldberg (2019) show that the methods in Bolukbasi et al. (2016) are n"
2021.woah-1.2,2020.acl-main.747,0,0.0959508,"Missing"
2021.woah-1.2,W17-3008,0,0.0117191,"for which we removed additional classes (insult, abuse etc.) from the original finer-grained data labels and downsampled to the minority class (profane). For German (DE), we use the test sets of GermEval-2019 (Struß et al., 2019) Subtask 1 (Other/Offense) and Subtask 2 (Other/Profanity) for DT and ST respectively. For English (EN), we use the HASOC (Mandl et al., 2019) Subtask A (NOT/HOF) and Subtask B (NOT/PRFN) for DT and ST respectively. French (FR) is tested on the hate speech portion (None/Hate) of the corpus created by Charitidis et al. (2020) for DT only, while Arabic (AR) is tested on Mubarak et al. (2017) for DT (Clean/Obscene+Offense) and ST (Clean/Obscene). As AR has no official train/test splits, we use the last 100 samples for testing. The training data of these corpora is not used. Table 2 summarizes the data used for fine-tuning as well as testing. 5.1 Before moving to the lesser explored sentence-level subspaces, we first verify whether word-level semantic subspaces can also capture complex semantic features such as profanity. Minimal Pairs Staying within the general low-resource setting prevalent in hate speech and profanity domains, and to keep manual annotation effort low, we randoml"
2021.woah-1.2,W17-4903,0,0.335807,"such as BERT, that share parameters between tasks. Multilingual varieties such as XLM-R (Conneau et al., 2020) enable the zero-shot cross-lingual transfer of a task. One example is sentence classification trained on a (highresource) language being transferred into another (low-resource) language (Hu et al., 2020). Related Work Semantic subspaces have been used to identify gender (Bolukbasi et al., 2016) or multiclass ethnic and religious (Manzini et al., 2019) bias in word representations. Liang et al. (2020) identify multiclass (gender, religious) bias in sentence representations. Similarly, Niu and Carpuat (2017) identify a stylistic subspace that captures the degree of formality in a word representation. This is done using a list of minimal-pairs, i.e. pairs of words or sentences that only differ in the semantic feature of interest over which they perform principal component analysis (PCA). We take the same general approach in this paper (see Section 3). Conversely, Gonen and Goldberg (2019) show that the methods in Bolukbasi et al. (2016) are not able to identify and remove the gender bias entirely. Following this, Ravfogel et al. (2020) argue that semantic features such as gender are encoded nonlin"
2021.woah-1.2,N19-1423,0,0.194781,"h tasks. Introduction Profanity and online hate speech have been recognized as crucial problems on social media platforms as they bear the potential to offend readers and disturb communities. The large volume of usergenerated content makes manual moderation very difficult and has motivated a wide range of natural language processing (NLP) research in recent years. However, the issues are far from solved, and the automatic detection of profane and hateful contents in particular faces a number of severe challenges. Pre-trained transformer-based (Vaswani et al., 2017) language models, e.g. BERT (Devlin et al., 2019), play a dominant role today in many NLP tasks. However, they work best when large amounts of training data are available. This is typically not the case for profanity and hate speech detection where few datasets are currently available (Waseem and Hovy, 2016; Basile et al., 2019; We analyze the efficacy of the subspaces to encode the profanity (neutral vs. profane language) aspect and apply the resulting subspace-based representations to a zero-shot transfer classification scenario with both similar (neutral/profane) and distant (neutral/hate) target classification tasks. To study their abili"
2021.woah-1.2,D19-1474,0,0.0218263,"Profanity and Hate Speech in Social Media with Semantic Subspaces Vanessa Hahn, Dana Ruiter, Thomas Kleinbauer, Dietrich Klakow Spoken Language Systems Group Saarland University Saarbr¨ucken, Germany {vhahn|druiter|kleiba|dklakow}@lsv.uni-saarland.de Abstract Struß et al., 2019) with moderate sizes at most. In addition, these tasks are known to be highly subjective (Waseem, 2016). Annotation protocols for hate speech and profanity often rely on different assumptions that make it non-trivial to combine multiple datasets. In addition, such datasets only exist for few languages besides English (Ousidhoum et al., 2019; Abu Farha and Magdy, 2020; Zampieri et al., 2020). Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and nonrelated (Ara"
2021.woah-1.2,W19-3621,0,0.0237565,"olukbasi et al., 2016) or multiclass ethnic and religious (Manzini et al., 2019) bias in word representations. Liang et al. (2020) identify multiclass (gender, religious) bias in sentence representations. Similarly, Niu and Carpuat (2017) identify a stylistic subspace that captures the degree of formality in a word representation. This is done using a list of minimal-pairs, i.e. pairs of words or sentences that only differ in the semantic feature of interest over which they perform principal component analysis (PCA). We take the same general approach in this paper (see Section 3). Conversely, Gonen and Goldberg (2019) show that the methods in Bolukbasi et al. (2016) are not able to identify and remove the gender bias entirely. Following this, Ravfogel et al. (2020) argue that semantic features such as gender are encoded nonlinearly, and suggest an iterative approach to identifying and removing gender features from semantic representations entirely. Addressing the issue of data sparseness, Rothe et al. (2016) use ultradense subspaces to generate task-specific representations that capture semantic features such as abstractness and sentiment and show that these are especially useful for lowresourced downstrea"
2021.woah-1.2,2020.acl-main.647,0,0.0195065,"ender, religious) bias in sentence representations. Similarly, Niu and Carpuat (2017) identify a stylistic subspace that captures the degree of formality in a word representation. This is done using a list of minimal-pairs, i.e. pairs of words or sentences that only differ in the semantic feature of interest over which they perform principal component analysis (PCA). We take the same general approach in this paper (see Section 3). Conversely, Gonen and Goldberg (2019) show that the methods in Bolukbasi et al. (2016) are not able to identify and remove the gender bias entirely. Following this, Ravfogel et al. (2020) argue that semantic features such as gender are encoded nonlinearly, and suggest an iterative approach to identifying and removing gender features from semantic representations entirely. Addressing the issue of data sparseness, Rothe et al. (2016) use ultradense subspaces to generate task-specific representations that capture semantic features such as abstractness and sentiment and show that these are especially useful for lowresourced downstream tasks. While they focus on using small amounts of labeled data of a specific target task to learn the subspaces, we focus our study on learning a ge"
2021.woah-1.2,N16-1091,0,0.0608301,"Missing"
2021.woah-1.2,2020.acl-main.488,0,0.0186336,"ined a lot of traction in NLP. Nowadays, this is done using large-scale transformerbased language models such as BERT, that share parameters between tasks. Multilingual varieties such as XLM-R (Conneau et al., 2020) enable the zero-shot cross-lingual transfer of a task. One example is sentence classification trained on a (highresource) language being transferred into another (low-resource) language (Hu et al., 2020). Related Work Semantic subspaces have been used to identify gender (Bolukbasi et al., 2016) or multiclass ethnic and religious (Manzini et al., 2019) bias in word representations. Liang et al. (2020) identify multiclass (gender, religious) bias in sentence representations. Similarly, Niu and Carpuat (2017) identify a stylistic subspace that captures the degree of formality in a word representation. This is done using a list of minimal-pairs, i.e. pairs of words or sentences that only differ in the semantic feature of interest over which they perform principal component analysis (PCA). We take the same general approach in this paper (see Section 3). Conversely, Gonen and Goldberg (2019) show that the methods in Bolukbasi et al. (2016) are not able to identify and remove the gender bias ent"
2021.woah-1.2,W16-5618,0,0.0493871,"Missing"
2021.woah-1.2,N16-2013,0,0.0189356,"very difficult and has motivated a wide range of natural language processing (NLP) research in recent years. However, the issues are far from solved, and the automatic detection of profane and hateful contents in particular faces a number of severe challenges. Pre-trained transformer-based (Vaswani et al., 2017) language models, e.g. BERT (Devlin et al., 2019), play a dominant role today in many NLP tasks. However, they work best when large amounts of training data are available. This is typically not the case for profanity and hate speech detection where few datasets are currently available (Waseem and Hovy, 2016; Basile et al., 2019; We analyze the efficacy of the subspaces to encode the profanity (neutral vs. profane language) aspect and apply the resulting subspace-based representations to a zero-shot transfer classification scenario with both similar (neutral/profane) and distant (neutral/hate) target classification tasks. To study their ability to generalize across languages we evaluate the zero-shot transfer in both a monolingual (German) and a cross-lingual setting 6 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 6–16 August 6, 2021. ©2021 Association for Computational Lingu"
2021.woah-1.2,2020.semeval-1.188,0,0.0279037,"antic Subspaces Vanessa Hahn, Dana Ruiter, Thomas Kleinbauer, Dietrich Klakow Spoken Language Systems Group Saarland University Saarbr¨ucken, Germany {vhahn|druiter|kleiba|dklakow}@lsv.uni-saarland.de Abstract Struß et al., 2019) with moderate sizes at most. In addition, these tasks are known to be highly subjective (Waseem, 2016). Annotation protocols for hate speech and profanity often rely on different assumptions that make it non-trivial to combine multiple datasets. In addition, such datasets only exist for few languages besides English (Ousidhoum et al., 2019; Abu Farha and Magdy, 2020; Zampieri et al., 2020). Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and nonrelated (Arabic) tasks. We observe that, on both similar and di"
C14-1216,P98-1013,0,0.111955,"ght products. In many cases, the names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item how often it is accompanied by one of them. 4.5 Commerce Cues (COMMERCE) Presumably, brands are more likely to be mentioned in the context of commercial transaction events than types. Therefore, we created a list of words that indicate these types of events. The list was created ad hoc. We used external resources, such as FrameNet (Baker et al., 1998) or GermaNet (Hamp and Feldweg, 1997) (the German version of WordNet (Miller et al., 1990)), and made no attempt to tune that list to our domain-specific food corpus. The final list (85 cues in total) comprises: verbs (and deverbal nouns) that convey the event of a commercial transaction (e.g. buy, purchase or sell), persons involved in a commercial transaction (e.g. customer or shop assistant), means of purchase (e.g. money, credit card or bill), places of purchase (e.g. supermarket or shop) and judgment of price (e.g. cheap or expensive). 4.6 Food Modifier (PATmod ) Even though many mentions"
C14-1216,D12-1124,0,0.02156,"ferent types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that the categorization according to the Food Guide Pyramid helps to decide whether a food item is a brand or not. 7 Conclusion We examined the task of separating types from bran"
C14-1216,chrupala-klakow-2010-named,1,0.792133,"at also employs features below the word level. As many of our food items will be unknown words, a character-level analysis may still be able to make useful predictions. 4.3 Contextual Named-Entity Recognition (NERcontext) We also count the number of other named entities that co-occur with the target food brand within the same sentence. We are only interested in organizations; an organization co-occurring with a brand is likely to be the company producing that brand (e.g. He loves Kellogg’scompany frostiesbrand .) For this feature, we rely on the output of a named-entity recognizer for German (Chrupała and Klakow, 2010). 4.4 Diversification (DIVERS) Once a product has established itself on the market for a substantial amount of time, many companies introduce variants of their brand to further consolidate their market position. The purpose of this diversification is to appeal to customers with special needs. A typical variant of food brands are light products. In many cases, the names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item h"
C14-1216,P12-1057,0,0.0244314,"6), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that the categorization according to the Food Guide Pyramid helps to decide whether a food item is a brand or not. 7 Conclusion We examined the task of separating types from brands in the food domain. Framing the problem as a ranking task, we directly converted predictive features extracted from a domain-specific corpu"
C14-1216,W97-0802,0,0.0341492,"names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item how often it is accompanied by one of them. 4.5 Commerce Cues (COMMERCE) Presumably, brands are more likely to be mentioned in the context of commercial transaction events than types. Therefore, we created a list of words that indicate these types of events. The list was created ad hoc. We used external resources, such as FrameNet (Baker et al., 1998) or GermaNet (Hamp and Feldweg, 1997) (the German version of WordNet (Miller et al., 1990)), and made no attempt to tune that list to our domain-specific food corpus. The final list (85 cues in total) comprises: verbs (and deverbal nouns) that convey the event of a commercial transaction (e.g. buy, purchase or sell), persons involved in a commercial transaction (e.g. customer or shop assistant), means of purchase (e.g. money, credit card or bill), places of purchase (e.g. supermarket or shop) and judgment of price (e.g. cheap or expensive). 4.6 Food Modifier (PATmod ) Even though many mentions of brands are similar to those of ty"
C14-1216,P97-1023,0,0.121095,"address are (mostly) language universal. All examples are given as English translations. We use the term food item to refer to the union of food brands and food types. All food items will be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both written uppercase. In English, both types and brands can be written uppercase or lowercase2 , however, there is a tendency in user-generated content/social media to write mostly lowercase. 2 Motivation & Data Previous research on lexicon induction proposed a widely applicable method based on coordination (Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First, a set of seed expressions that are typical of the categories one wants to induce are defined. Then, additional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the great semantic coherence of conjuncts. This method can also be applied to the food domain. As a domain-specific dataset fo"
C14-1216,D13-1150,0,0.0311416,"focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that"
C14-1216,Y12-1010,0,0.029723,"Amazon. Their work focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization"
C14-1216,Y12-1031,0,0.0294524,"ype, i.e. forum entries, that comprise full 2299 Feature WIKIoracle ranking+GRAPHpyramid ranking+GRAPHpyramid +VSM ranking+GRAPHpyramid +GRAPHbrand ranking+GRAPHpyramid +WIKI P@200 66.00 62.50 60.00 67.50 70.00 AP 0.429 0.626 0.619 0.638 0.688 -2nd reset P@200 AP -N/A- -N/A-N/A- -N/A63.00 0.661 65.50 0.662 73.00 0.718 Table 9: Impact of bootstrapping; -2nd reset: does not apply reset feature for a second time (Figure 1). sentences. Previous work also focuses on traditional (semi-)supervised algorithms. Hence, there are only few additional insights as to the specific properties of brand names. Min and Park (2012) examine the aspect of product instance distinction on the use case of product reviews on jeans from Amazon. Their work focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction"
C14-1216,D11-1144,0,0.165294,"ble 9. The reason for this is that we evaluate in isolation rather than in combination with other features (i.e. parts of the additional benefit included in GRAPHbrand may already be contained in ranking and reset features). Secondly, in a ranking task (Table 9), good performance is usually achieved by classifiers biased towards a high precision. Indeed, the best ranker in Table 9, i.e. WIKI, achieves the highest precision in Table 8. 6 Related Work Ling and Weld (2012) examine named-entity recognition on data that also include brands, however, the class of brands is not explicitly discussed. Putthividhya and Hu (2011) explore brands in the context of product attribute extraction. Entities are extracted from eBay’s clothing and shoe category. Nadeau et al. (2006) explicitly generate gazetteers of car brands obtained from corresponding websites. Those textual data are very restrictive in that they do not represent sentences but category listings or tables. In this paper, we consider as textual source a more general text type, i.e. forum entries, that comprise full 2299 Feature WIKIoracle ranking+GRAPHpyramid ranking+GRAPHpyramid +VSM ranking+GRAPHpyramid +GRAPHbrand ranking+GRAPHpyramid +WIKI P@200 66.00 62."
C14-1216,W97-0313,0,0.12838,"sal. All examples are given as English translations. We use the term food item to refer to the union of food brands and food types. All food items will be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both written uppercase. In English, both types and brands can be written uppercase or lowercase2 , however, there is a tendency in user-generated content/social media to write mostly lowercase. 2 Motivation & Data Previous research on lexicon induction proposed a widely applicable method based on coordination (Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First, a set of seed expressions that are typical of the categories one wants to induce are defined. Then, additional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the great semantic coherence of conjuncts. This method can also be applied to the food domain. As a domain-specific dataset for all our experiments, we u"
C14-1216,P98-2182,0,0.122356,"as English translations. We use the term food item to refer to the union of food brands and food types. All food items will be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both written uppercase. In English, both types and brands can be written uppercase or lowercase2 , however, there is a tendency in user-generated content/social media to write mostly lowercase. 2 Motivation & Data Previous research on lexicon induction proposed a widely applicable method based on coordination (Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First, a set of seed expressions that are typical of the categories one wants to induce are defined. Then, additional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the great semantic coherence of conjuncts. This method can also be applied to the food domain. As a domain-specific dataset for all our experiments, we use a crawl of chefkoch.de3"
C14-1216,I13-1003,1,0.83168,"features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that the categorization accordi"
C14-1216,E14-1071,1,0.904724,"n the target item is likely to be some brand. This is due to the fact that many brands are often mentioned in combination with the food type that they represent, e.g. volvic mineral water, nutella chocolate spread. 4.7 Prepositional Phrase Embedding (PATpp) Instead of appearing as a modifier (§4.6), a brand may also be embedded in some prepositional phrase that has a similar meaning, e.g. We only buy the chocolate spread [by nutella]P P . 4.8 Graph-based Methods (GRAPH) We also employ some semi-supervised graph clustering method in order to assign semantic types to food items as introduced in Wiegand et al. (2014). The underlying data structure is a food graph that is generated automatically from our domain-specific corpus where nodes represent food items and edge weights 2295 Category MEAT BEVERAGE SWEET SPICE VEGE STARCH MILK FRUIT GRAIN FAT EGG Description meat and fish (products) beverages (incl. alcoholic drinks) sweets, pastries and snack mixes spices and sauces vegetables (incl. salads) starch-based side dishes milk products fruits grains, nuts and seeds fat eggs General 19.48 17.19 14.90 10.53 10.38 9.21 6.71 4.48 3.41 2.54 0.92 Brands 1.31 23.96 25.60 2.42 0.00 4.42 23.48 1.14 0.00 20.00 0.00"
C14-1216,I13-1188,0,0.0262032,"Missing"
C14-1216,C98-1013,0,\N,Missing
C14-1216,C98-2177,0,\N,Missing
C16-1194,W13-3520,0,0.0526102,"Missing"
C16-1194,P96-1041,0,0.263918,"erage rates for such words, giving this method added benefit in the context of rare words. 4.2 Experimental Setup Before evaluating the SWordSS embeddings for predicting rare words, we used all the OOVs to expand the corresponding vocabulary. SWordSS embeddings for all the words in the expanded vocabulary were used to initialise LBL framework as described in Section 4. A bigram version of this LBL (LBL2SW ordSS ) was further trained on language corpora before being evaluated. We compare our LBL2SW ordSS model with the conventional Modified-Kneser-Ney five-gram LM (MKN5) (Kneser and Ney, 1995; Chen and Goodman, 1996) and also with the bigram (LBL2) based log-bilinear LM. As a more powerful baseline, we also trained an LSTM based RNN LM to compare with LBL2SW ordSS . Moreover, we compare the LBL2SW ordSS , with a character aware language model (Kim et al., 2015), denoted as CCNN-LSTM. The CCNN-LSTMs were chosen for comparison because of their ability to use character-based features to implicitly handle OOVs and rare words. For training each of these LMs, we used the expanded vocabulary as used by LBL2SW ordSS . In training neural network-based language models, we restricted the number of parameters to have"
C16-1194,P16-1156,0,0.0457888,"Missing"
C16-1194,P14-1006,0,0.0503781,"Missing"
C16-1194,W13-3512,0,0.253531,"Hermann and Blunsom, 2014; Bengio and Heigold, 2014; Yang et al., 2015), and these systems often achieved state-of-the-art performance. This success has been ascribed to embeddings’ ability to capture regularities traditionally represented in core NLP features. Most of these embeddings were trained on large amounts of data, allowing them to have good coverage of the relevant vocabularies. However, embeddings often still cannot satisfactorily represent rare words, i.e. words with few occurrences in training data. To generate useful embeddings for words too rare for standard methods to handle, Luong et al. (2013) and Botha and Blunsom (2014) leveraged the segmentation tool, Morfessor (Creutz and Lagus, 2005), while Cotterell et al. (2016) used morphological lexica to generate rare-word embeddings. In general, these methods added resource-based knowledge to their systems in order to form word vector representations, showing impressive performance gains over methods which did not address the rare words problem. In contrast, Soricut and Och (2015) applied an automatic method to induce morphological rules and transformations as vectors in the same embedding space. More specifically, they exploited automat"
C16-1194,D12-1110,0,0.0237695,"Missing"
C16-1194,N15-1186,0,0.37751,"en Graduate School of Computer Science, Saarland Informatics Campus 3 Collaborative Research Center on Information Density and Linguistic Encoding Saarland University, Saarbr¨ucken, Germany {firstname.lastname}@lsv.uni-saarland.de Abstract Training good word embeddings requires large amounts of data. Out-of-vocabulary words will still be encountered at test-time, leaving these words without embeddings. To overcome this lack of embeddings for rare words, existing methods leverage morphological features to generate embeddings. While the existing methods use computationally-intensive rule-based (Soricut and Och, 2015) or tool-based (Botha and Blunsom, 2014) morphological analysis to generate embeddings, our system applies a computationally-simpler sub-word search on words that have existing embeddings. Embeddings of the sub-word search results are then combined using string similarity functions to generate rare word embeddings. We augmented pre-trained word embeddings with these novel embeddings and evaluated on a rare word similarity task, obtaining up to 3 times improvement in correlation over the original set of embeddings. Applying our technique to embeddings trained on larger datasets led to on-par pe"
chrupala-klakow-2010-named,N04-1043,0,\N,Missing
chrupala-klakow-2010-named,D07-1073,0,\N,Missing
chrupala-klakow-2010-named,W02-1001,0,\N,Missing
chrupala-klakow-2010-named,J92-4003,0,\N,Missing
chrupala-klakow-2010-named,P08-1068,0,\N,Missing
chrupala-klakow-2010-named,W03-0419,0,\N,Missing
chrupala-klakow-2010-named,doddington-etal-2004-automatic,0,\N,Missing
chrupala-klakow-2010-named,C96-1079,0,\N,Missing
chrupala-klakow-2010-named,W09-3301,0,\N,Missing
chrupala-klakow-2010-named,P05-1045,0,\N,Missing
cramer-etal-2006-building,P04-3018,0,\N,Missing
D13-1003,P12-1001,0,0.0275206,"successively computing the Pareto-frontier of the 2-dimensional score vectors (Borzsony et al., 2001; Godfrey et al., 2007). The underlying principle is that all data points (patterns in our case) that are not dominated by another point3 build the frontier and are ranked highest (see Figure 2), with ties broken by linear 3 2 + A data point h1 dominates a data point h2 if h1 ≥ h2 in all metrics and h1 > h2 in at least one metric. The weight vectors are averaged over 20 iterations. 26 combination. Sorting by computing the Paretofrontier has been applied to training machine translation systems (Duh et al., 2012) to combine the translation quality metrics BLEU, RIBES and NTER, each of which is based on different principles. In the context of machine translation it has been found to outperform a linear interpolation of the metrics and to be more stable to non-smooth metrics and noncomparable scalings. We compare non-dominated sorting with a simple linear interpolation with uniform weights. method MLE hier orig hier feature perceptron perc+hier (pareto) perc+hier (itpl) map .253 .270 .318†* .330†* .340†* .344†* gmap .142 .158 .205†* .210†* .220†* .220†* Interpolated Precision/Recall Evaluation MLE hier"
D13-1003,P11-1055,0,0.114636,"2.1 At-Least-One Models The original form of distant supervision (Mintz et al., 2009) assumes all sentences containing an entity pair to be potential patterns for the relation holding between the entities. A variety of models relax this assumption and only presume that at least one of the entity pair occurrences is a textual manifestation of the relation. The first proposed model with an atleast-one learner is that of Riedel et al. (2010) and Yao et al. (2010). It consists of a factor graph that includes binary variables for contexts, and groups contexts together for each entity pair. MultiR (Hoffmann et al., 2011) can be viewed as a multi-label extension of (Riedel et al., 2010). A further extension is MIMLRE (Surdeanu et al., 2012), a jointly trained two-stage classification model. 2.2 Hierarchical Topic Model The hierarchical topic model (HierTopics) by Alfonseca et al. (2012) models the distant supervision data by a generative model. For each corpus match of an entity pair in the knowledge base, the corresponding surface pattern is assumed to be typical for either the entity pair, the relation, or neither. This principle is then used to infer distributions over patterns of one of the following types"
D13-1003,P09-1113,0,0.410381,"ace_of_birth(Michael Jackson, Gary) Introduction Relation extraction is the task of finding relational facts in unstructured text and putting them into a structured (tabularized) knowledge base. Training machine learning algorithms for relation extraction requires training data. If the set of relations is prespecified, the training data needs to be labeled with those relations. Manual annotation of training data is laborious and costly, however, the knowledge base may already partially be filled with instances from the relations. This is utilized by a scheme known as distant supervision (DS) (Mintz et al., 2009): text is automatically labeled by aligning (matching) pairs of entities that are contained in a knowledge base with their textual occurrences. Whenever such a match is encountered, the surrounding context (sentence) is assumed to express the relation. is contained in the knowledge base, one matching context could be: Michael Jackson was born in Gary ... And another possible context: Michael Jackson moved from Gary ... Clearly, only the first context indeed expresses the relation and should be labeled accordingly. Three basic approaches have been proposed to deal with noisy distant supervision"
D13-1003,D12-1042,0,0.305681,"entity pair to be potential patterns for the relation holding between the entities. A variety of models relax this assumption and only presume that at least one of the entity pair occurrences is a textual manifestation of the relation. The first proposed model with an atleast-one learner is that of Riedel et al. (2010) and Yao et al. (2010). It consists of a factor graph that includes binary variables for contexts, and groups contexts together for each entity pair. MultiR (Hoffmann et al., 2011) can be viewed as a multi-label extension of (Riedel et al., 2010). A further extension is MIMLRE (Surdeanu et al., 2012), a jointly trained two-stage classification model. 2.2 Hierarchical Topic Model The hierarchical topic model (HierTopics) by Alfonseca et al. (2012) models the distant supervision data by a generative model. For each corpus match of an entity pair in the knowledge base, the corresponding surface pattern is assumed to be typical for either the entity pair, the relation, or neither. This principle is then used to infer distributions over patterns of one of the following types: 1. For every entity pair, a pair-specific distribution. 25 Generative Model We use a feature-based extension (Roth and"
D13-1003,P12-1076,0,0.307985,"context: Michael Jackson moved from Gary ... Clearly, only the first context indeed expresses the relation and should be labeled accordingly. Three basic approaches have been proposed to deal with noisy distant supervision instances: The discriminative at-least-one approach (Riedel et al., 2010), that requires that at least one of the matches for a relation-entity tuple indeed expresses the relation; The generative approach (Alfonseca et al., 2012) that separates relation-specific distributions from noise distributions by using hierarchical topic models; And the pattern correlation approach (Takamatsu et al., 2012) that assumes that contexts which match argument pairs have a high overlap in argument pairs with other patterns expressing the relation. In this work we combine 1) a discriminative atleast-one learner, that requires high scores for both a dedicated noise label and the matched relation, and 2) a generative topic model that uses a feature-based representation to separate relation-specific patterns from background or pair-specific noise. We score surface patterns and show that combining the two approaches results in a better ranking quality of relational facts. In an end-to-end evaluation we set"
D13-1003,D10-1099,0,0.131553,"ons are shared on the respective levels. Figure 1 shows the plate diagram of the HierTopics model. 3 Model Extensions and Combination 3.1 2 Related Work 2.1 At-Least-One Models The original form of distant supervision (Mintz et al., 2009) assumes all sentences containing an entity pair to be potential patterns for the relation holding between the entities. A variety of models relax this assumption and only presume that at least one of the entity pair occurrences is a textual manifestation of the relation. The first proposed model with an atleast-one learner is that of Riedel et al. (2010) and Yao et al. (2010). It consists of a factor graph that includes binary variables for contexts, and groups contexts together for each entity pair. MultiR (Hoffmann et al., 2011) can be viewed as a multi-label extension of (Riedel et al., 2010). A further extension is MIMLRE (Surdeanu et al., 2012), a jointly trained two-stage classification model. 2.2 Hierarchical Topic Model The hierarchical topic model (HierTopics) by Alfonseca et al. (2012) models the distant supervision data by a generative model. For each corpus match of an entity pair in the knowledge base, the corresponding surface pattern is assumed to b"
D13-1003,P12-2011,0,\N,Missing
D16-1017,P14-1023,0,0.443736,"se sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for development and testing, and the rest were used for training. In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for MaltParser. Because of the heterogeneous nature of web data, this alignment was"
D16-1017,J10-4006,0,0.298302,"Greenberg (patient) Pado+McRae+Ferretti # ratings 414 1444 274 248 720 2380 Roles ARG0, ARG1, ARG2 ARG0, ARG1 ARGM-LOC ARGM-MNR ARG1 NN RF 0.52 (8) 0.38 (20) 0.44 (3) 0.45 (6) 0.61 (8) 0.41 (37) BL2010 0.53 (0) 0.32 (70) 0.23 (3) 0.36 (17) 0.46 (18) 0.35 (90) GSD2015 0.53 (0) 0.36 (70) 0.29 (3) 0.42 (17) 0.48 (18) 0.38 (90) BDK2014 0.41 0.28 - Table 2: Thematic fit evaluation scores, consisting of Spearman’s ρ correlations between average human judgements and model output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM (Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al. (2015a,b) and the neural network predict model described in Baroni et al. (2014). NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980). NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for"
D16-1017,P07-1028,0,0.177316,"y significant for Pado and Ferretti instruments. 4.1 Related work State-of-the-art computational models of thematic fit quantify the similarity between a role filler of a verb and the proto-typical filler for that role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical patient is in turn obtained from averaging representations of words that typically occur as a patient of eat (e.g., Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b). For more than one role, information from both the agent and the predicate can be used to jointly to predict a patient (e.g., Lenci, 2011). 4.2 Data Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Ferretti et al., 2001; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 20"
D16-1017,W15-1106,1,0.918364,"720 2380 Roles ARG0, ARG1, ARG2 ARG0, ARG1 ARGM-LOC ARGM-MNR ARG1 NN RF 0.52 (8) 0.38 (20) 0.44 (3) 0.45 (6) 0.61 (8) 0.41 (37) BL2010 0.53 (0) 0.32 (70) 0.23 (3) 0.36 (17) 0.46 (18) 0.35 (90) GSD2015 0.53 (0) 0.36 (70) 0.29 (3) 0.42 (17) 0.48 (18) 0.38 (90) BDK2014 0.41 0.28 - Table 2: Thematic fit evaluation scores, consisting of Spearman’s ρ correlations between average human judgements and model output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM (Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al. (2015a,b) and the neural network predict model described in Baroni et al. (2014). NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980). NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for Pado and Ferretti instruments. 4.1 Related work State-of-the-art"
D16-1017,P98-1035,0,0.167769,"sad Sayeed and Dietrich Klakow and Stefan Thater Saarland University 66123 Saarbr¨ucken, Germany {vera,asayeed,stth} @coli.uni-sb.de; dietrich.klakow@lsv.uni-sb.de Abstract sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, location, time, manner and the predicate itself. The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general language modelling tasks (e.g., Chelba and Jelinek, 1998; Tan et al., 2012). A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be"
D16-1017,P07-1071,0,0.278611,"inputs, while our model gives a probability distribution over all words for the queried target role. We discuss the components necessary for our model in more detail in section 3. 2 Data source Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, containing approximately 138 million sentences. These sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for de"
D16-1017,C14-1134,0,0.0662196,"Missing"
D16-1017,N15-1003,1,0.727949,"diction, selectional preference models score the inputs, while our model gives a probability distribution over all words for the queried target role. We discuss the components necessary for our model in more detail in section 3. 2 Data source Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, containing approximately 138 million sentences. These sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fo"
D16-1017,J15-1004,0,0.0645383,"however also model the interaction between different roles; see Figure 2 for an example of model predictions. We are only aware of one small dataset that can be used to systematically test the effectiveness of the compositionality for this task. The Bicknell et al. (2010) dataset contains triples like journalist check 178 Evaluation of event representations: sentence similarity To show that our model learns to represent input words and their roles in a useful way that reflects the meaning and interactions between inputs, we evaluate our non-incremental model on a sentence similarity task from Grefenstette and Sadrzadeh (2015). We assign similarity scores to sentence pairs by computing representations for each sentence by tak# ratings 199 NN RF 0.34 Kronecker 0.26 W2V 0.13 Humans 0.62 Table 5: Sentence similarity evaluation scores on GS2013 dataset (Grefenstette and Sadrzadeh, 2015), consisting of Spearman’s ρ correlations between human judgements and model output. Kronecker is the best performing model from Grefenstette and Sadrzadeh (2015). NN RF is the non-incremental model presented in this article, and W2V is the word2vec baseline. Human performance (inter-annotator agreement) shows the upper bound. Figure 2:"
D16-1017,D14-1140,0,0.0613349,"Missing"
D16-1017,P15-1115,0,0.0871915,". The latter could be useful for inferring missing information in entailment tasks or improving identification of thematic roles outside the sentence containing the predicate. Potential applications also include predicate prediction based on arguments and roles, which has been noted to be relevant for simultaneous machine translation for a verb-final to a verb-medial source language (Grissom II et al., 2014). Within cognitive modelling, our model could help to more accurately estimate semantic surprisal for broadcoverage texts, when used in combination with an incremental role labeller (e.g., Konstas and Keller, 2015), or to provide surprisal estimates for content words as a control variable for psycholinguistic experimental materials. In this work, we focus on the predictability of verbs and nouns, and we suggest that the predictability of these words depends to a large extent on the relationship of these words to other nouns and 171 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 171–182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics verbs, especially those connected via the same event. We choose a neural network (NN) mod"
D16-1017,W11-0607,0,0.788422,"t role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical patient is in turn obtained from averaging representations of words that typically occur as a patient of eat (e.g., Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b). For more than one role, information from both the agent and the predicate can be used to jointly to predict a patient (e.g., Lenci, 2011). 4.2 Data Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Ferretti et al., 2001; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 2009; Greenberg et al., 2015a). The datasets include agent, patient, location and instrument roles. For example, in the Pad´o et al. (2009) dataset, the noun sound has a very low rating of 1.1 as the subject of hear a"
D16-1017,J12-3007,0,0.0185998,"akow and Stefan Thater Saarland University 66123 Saarbr¨ucken, Germany {vera,asayeed,stth} @coli.uni-sb.de; dietrich.klakow@lsv.uni-sb.de Abstract sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, location, time, manner and the predicate itself. The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general language modelling tasks (e.g., Chelba and Jelinek, 1998; Tan et al., 2012). A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be used to provide a"
D16-1017,D14-1004,0,0.242497,"Missing"
D16-1017,E09-1094,0,0.680448,"through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for development and testing, and the rest were used for training. In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for MaltParser. Because of the heterogeneous nature of web data, this alignment was not always achievable—we skipp"
D16-1154,J90-2002,0,0.786751,"Missing"
D16-1154,D09-1116,0,0.0825428,"Missing"
D16-1154,W12-2701,0,0.0649009,"Missing"
D16-1154,P15-2081,0,0.0680253,"Missing"
D18-1463,K16-1002,0,0.324347,"I|c) pφ (Fi+1 Fi+1 i+1 (3) ˜ ˜ where Hi−1 and Fi+1 are also assumed to be Gaussian distributed given c with mean and covariance estimated from multi-layer perceptrons. We infer the encoded vectors instead of the original sequences for three reasons. Firstly, inferring dense vectors is parallelizable and computationally much cheaper than autoregressive decoding, especially when the context sequences could be unlimitedly long. Secondly, sequence vectors can capture more holistic semantic-level similarity than individual tokens. Lastly, It can also help alleviate the posterior collapsing issue (Bowman et al., 2016) when training variational inference models on text (Chen et al., 2017; Shen et al., 2018), which we will use later. It can be shown that the above objective maximizes a lower bound of λ1 I(Hi−1 , c) + λ2 I(c, Fi+1 ), given the conditional probability pφ (c|Hi−1 , Fi ). The proof is a direct extension of the derivation in (Chen et al., 2016), followed by the Data Processing Inequality (Beaudry and Renner, 2012) that the encoding function can only reduce the mutual information. As the sampling process contains only Gaussian continuous variables, the above objective can be trained through the re"
D18-1463,N18-2081,0,0.0162528,"ence on Empirical Methods in Natural Language Processing, pages 4316–4327 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics with the surrounding context. To enable efficient training, two challenges exist. The first challenge comes from the discrete nature of language tokens, hindering efficient gradient descent. One strategy is to estimate the gradient by methods like Gumbel-Softmax (Maddison et al., 2017; Jang et al., 2017) or REINFORCE algorithm (Williams, 1992), which has been applied in many NLP tasks (He et al., 2016; Shetty et al., 2017; Gu et al., 2018; Paulus et al., 2018), but the trade-off between bias and variance of the estimated gradient is hard to reconcile. The resulting model usually strongly relies on sensitive hyper-parameter tuning, careful pre-train and taskspecific tricks. Li et al. (2016a); Wang et al. (2017) avoid this non-differentiability problem by learning a separate backward model to rerank candidate responses in the testing phase while still adhering to the MLE objective for training. However, the candidate set normally suffers from low diversity and a huge sample size is needed for good performance (Li et al., 2016b)."
D18-1463,W04-3250,0,0.029811,"Missing"
D18-1463,N16-1014,0,0.293431,"we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations. 1 Figure 1: A conversation in real life suitable for modeling dialogues. Recent research has found that while the seq2seq model generates syntactically well-formed responses, they are prone to being off-context, short, and generic. (e.g., “I dont know” or “I am not sure”) (Li et al., 2016a; Serban et al., 2016). The reason lies in the one-to-many alignments in human conversations, where one dialogue context is open to multiple potential responses. When optimizing with the MLE objective, the model tends to have a strong bias towards safe responses as they can be literally paired with arbitrary dialogue context without semantical or grammatical contradictions. These safe responses break the dialogue flow without bringing any useful information and people will easily lose interest in continuing the conversation. Introduction With the availability of massive online conversational"
D18-1463,D16-1127,0,0.35394,"we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations. 1 Figure 1: A conversation in real life suitable for modeling dialogues. Recent research has found that while the seq2seq model generates syntactically well-formed responses, they are prone to being off-context, short, and generic. (e.g., “I dont know” or “I am not sure”) (Li et al., 2016a; Serban et al., 2016). The reason lies in the one-to-many alignments in human conversations, where one dialogue context is open to multiple potential responses. When optimizing with the MLE objective, the model tends to have a strong bias towards safe responses as they can be literally paired with arbitrary dialogue context without semantical or grammatical contradictions. These safe responses break the dialogue flow without bringing any useful information and people will easily lose interest in continuing the conversation. Introduction With the availability of massive online conversational"
D18-1463,D17-1230,0,0.403999,"ned and the training stage is unstable due to the huge search space. In contrast, our model maximizes the mutual information in the continuous space and trains the prior distribution through the reparamaterization trick. As a result, our model can be more easily trained with a lower variance. Throughout our experiment, the training process of NEXUS network is rather stable and much less data-hungry. The MMI objective of our model is theoretically more sound and no manually-defined rules need to be specified. 4 4.1 Experiments Dataset and Training Details We run experiments on the DailyDialog (Li et al., 2017b) and Twitter corpus (Ritter et al., 2011). DailyDialog contains 13118 daily conversations under ten different topics. This dataset is crawled from various websites for English learner to practice English in daily life, which is high-quality, less noisy but relatively smaller. In contrast, the Twitter corpus is significantly larger but contains more noise. We obtain the dataset as used in Serban et al. (2017) and filter out tweets that have already been deleted, resulting in about 750,000 multi-turn dialogues. The contents have more informal, colloquial expressions which makes the generation"
D18-1463,I17-1099,1,0.939152,"ned and the training stage is unstable due to the huge search space. In contrast, our model maximizes the mutual information in the continuous space and trains the prior distribution through the reparamaterization trick. As a result, our model can be more easily trained with a lower variance. Throughout our experiment, the training process of NEXUS network is rather stable and much less data-hungry. The MMI objective of our model is theoretically more sound and no manually-defined rules need to be specified. 4 4.1 Experiments Dataset and Training Details We run experiments on the DailyDialog (Li et al., 2017b) and Twitter corpus (Ritter et al., 2011). DailyDialog contains 13118 daily conversations under ten different topics. This dataset is crawled from various websites for English learner to practice English in daily life, which is high-quality, less noisy but relatively smaller. In contrast, the Twitter corpus is significantly larger but contains more noise. We obtain the dataset as used in Serban et al. (2017) and filter out tweets that have already been deleted, resulting in about 750,000 multi-turn dialogues. The contents have more informal, colloquial expressions which makes the generation"
D18-1463,W05-0908,0,0.0713008,"Missing"
D18-1463,C04-1072,0,0.0453311,"Missing"
D18-1463,D11-1054,0,0.0393555,"due to the huge search space. In contrast, our model maximizes the mutual information in the continuous space and trains the prior distribution through the reparamaterization trick. As a result, our model can be more easily trained with a lower variance. Throughout our experiment, the training process of NEXUS network is rather stable and much less data-hungry. The MMI objective of our model is theoretically more sound and no manually-defined rules need to be specified. 4 4.1 Experiments Dataset and Training Details We run experiments on the DailyDialog (Li et al., 2017b) and Twitter corpus (Ritter et al., 2011). DailyDialog contains 13118 daily conversations under ten different topics. This dataset is crawled from various websites for English learner to practice English in daily life, which is high-quality, less noisy but relatively smaller. In contrast, the Twitter corpus is significantly larger but contains more noise. We obtain the dataset as used in Serban et al. (2017) and filter out tweets that have already been deleted, resulting in about 750,000 multi-turn dialogues. The contents have more informal, colloquial expressions which makes the generation task harder. These two datasets are randoml"
D18-1463,D16-1230,0,0.223757,"Missing"
D18-1463,P02-1040,0,0.101349,"Missing"
D18-1463,W12-2018,0,0.0743031,"Missing"
D18-1463,N15-1020,0,0.058439,"Missing"
D18-1463,D17-1228,0,0.0630013,"Missing"
D18-1463,P17-1061,0,0.462004,"., 2016b). The second challenge relates to the unknown future context in the testing phase. In our framework, both the history and future context need to be explicitly observed in order to compute the mutual information. When applying it to generating tasks where only the history context is given, there is no way to explicitly take into account the future information. Therefore, reranking-based models do not apply here. (Li et al., 2016c) addresses future information by policy learning, but the model suffers from high variance due to the enormous sequential search space. Serban et al. (2017); Zhao et al. (2017); Shen et al. (2017) adopt the variational inference strategy to reduce the training variance by optimizing over latent continuous variables. However, they all stick to the original MLE objective and no connection with the surrounding context is considered. In this work, we address both challenges by introducing an auxiliary continuous code space which is learned from the whole dialogue flow. At each time step, instead of directly optimizing discrete utterances, the current, past and future utterances are all trained to maximize the mutual information with this code space. Furthermore, a learn"
D19-1054,P18-1063,0,0.241541,"The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trained within the same objective, the generations are thus more faithful to the selected contents than Bottom-up methods. Our model is task-agnostic, end-to-end trainable and can be seamlessly inserted into any encoder-dec"
D19-1054,D18-1443,0,0.369633,"e supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the dec"
D19-1054,D16-1011,0,0.0681628,"Missing"
D19-1054,D18-1205,0,0.079091,"), then decode with pθ (Y |X, β). Source-text pairs are available for training, but the ground-truth content selection for each pair is unknown. 3.1 3.2 Soft-select falls back on a deterministic network to output the likelihood function’s first-order Taylor series approximation expanded at Eβ∼B(γ) β: Bottom-up log Eβ∼B(γ) pθ (Y |X, β) The most intuitive way is training the content selector to target some heuristically extracted contents. For example, we can train the selector to select overlapped words between the source and target (Gehrmann et al., 2018), sentences with higher tf-idf scores (Li et al., 2018) or identified image objects that appear in the caption (Wang et al., 2017). A standard encoder-decoder model is independently trained. In the testing stage, the prediction of the content selector is used to hard-mask the attention vector to guide the text generation in a bottom-up way. Though easy to train, Bottomup generation has the following two problems: (1) The heuristically extracted contents might be coarse and cannot reflect the variety of human languages and (2) The selector and decoder are independently trained towards different objectives thus might not adapt to each other well. β"
D19-1054,P18-2027,0,0.0154788,"and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardless of the low precision, so the controllability over generated text is weak. Fan et al. (2018) control the generation by manually conc"
D19-1054,P18-1013,0,0.0209586,"a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorith"
D19-1054,W17-4505,0,0.299395,"on weight will first be “soft-masked” by γ before being passed to the decoder. soft-select is fully differentiable and can be easily trained by gradient descent. However, this soft-approximation is normally inaccurate, especially when B(γ) has a high entropy, which is common in one-to-many text generation tasks. The gap between log Eβ∼B(γ) pθ (Y |X, β) and log pθ (Y |X, Eβ∼B(γ) ) will be large (Ma et al., 2017; Deng et al., 2018). In practice, this would lead to unrealistic generations when sampling β from the deterministically trained distribution. 3.3 Reinforce-Select Reinforce-select (RS) (Ling and Rush, 2017; Chen and Bansal, 2018) utilizes reinforcement learning to approximate the marginal likelihood. Specifically, it is trained to maximize a lower bound of the likelihood by applying the Jensen inequalily: log Eβ∼B(γ) pθ (Y |X, β) ≥ Eβ∼B(γ) log pθ (Y |X, β) The gradient to γ is approximated with MonteCarlo sampling by applying the REINFORCE algorithm (Williams, 1992; Glynn, 1990). To speed up convergence, we pre-train the selector by some distant supervision, which is a common practice in reinforcement learning. REINFORCE is unbiased but has a high variance. Many research have proposed sophistic"
D19-1054,D15-1166,0,0.0535494,"elector and generated text (Alemi et al., 2018; Zhao et al., 2018). A higher CMI leads to stronger controllability with a bit more risk of text disfluency. In summary, our contributions are (1) systematically studying the problem of controllable content selection for Enc-Dec text generation, (2) proposing a task-agnostic training framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is"
D19-1054,N16-1086,0,0.493898,"in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and g"
D19-1054,N19-1236,0,0.0404738,"ain the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardles"
D19-1054,P17-1098,0,0.0234923,"ve concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e.g., data-to-text, summarization and image captioning, can be naturally divided into two steps: content selection and surface realization. The generations are supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical"
D19-1054,P04-1011,0,0.221726,"Missing"
D19-1054,D14-1162,0,0.0812737,"Missing"
D19-1054,N19-1269,0,0.0384767,"Missing"
D19-1054,D18-1411,0,0.0685899,"elected : sri lankan, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many te"
D19-1054,D15-1044,0,0.0525735,"etup, then present the evaluation results. In practice, we can set  to adjust the degree of controllability we want. Later we will show it leads to a trade-off with performance. The final algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom"
D19-1054,P16-1162,0,0.0204755,"algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the clos"
D19-1054,D15-1199,0,0.0719588,"Missing"
D19-1054,D18-1356,0,0.0459964,"an, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e."
D19-1054,C18-1091,0,0.133731,"raining framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is the bernoulli parameter, which we estimate using a two-layer feedforward network on top of the source encoder. Text are generated by first sampling β from B(γ) to decide which content to cover, then decode with the conditional distribution pθ (Y |X, β). The text is expected to faithfully convey all selected contents an"
D19-1054,P17-1101,0,0.347238,"ge Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trai"
D19-1362,J92-4003,0,0.396559,". Veit et al. (2017) and Luo et al. (2017) use multiple layers of a neural network to model these relationships. However, in low resource settings with only small amounts of clean, supervised data, these more complex models can be difficult to learn. In contrast to that, larger amounts of unlabeled text are usually available even in low-resource settings. Therefore, we propose to use unsupervised clustering techniques to partition the feature space of the input words (and the corresponding instances) before estimating the noise matrices. To create the clusters, we use either Brown clustering (Brown et al., 1992) on the input words or k-means clustering (Lloyd, 1982) on the pretrained word embeddings after applying PCA (Pearson, 1901). In sequence labeling tasks, the features x of an instance usually consist of the input word ι(x) and its context. Given a clustering Π over the input words {ι(x) |(x, y) ∈ C ∪ N } consisting of clusters Π1 , ..., Πp , we can group all clean and noisy instances into groups Gq = {(x, y) ∈ C ∪ N |ι(x) ∈ Πq } (5) For each group, we construct an independent confusion matrix using Formulas 3 and 4. The prediction of the noisy label yˆ (Formula 2) then becomes p(ˆ y = j|x) = k"
D19-1362,N19-3005,1,0.804348,"e clean and yˆ the noisy label. For example, this can be represented as a noise or confusion matrix between the clean and the noisy labels, as explained in Section 3. Having its roots in statistics (Dawid and Skene, 1979), this or similar ideas have been recently studied in NLP (Fang and Cohn, 2016; Hedderich and 3554 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3554–3559, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Klakow, 2018; Paul et al., 2019), image classification (Mnih and Hinton, 2012; Sukhbaatar et al., 2015; Dgani et al., 2018) and general machine learning settings (Bekker and Goldberger, 2016; Patrini et al., 2017; Hendrycks et al., 2018). All of these methods, however, do not take the features into account that are used to represent the instances during classification. In (Xiao et al., 2015) only the noise type depends on x but not the actual noise model. Goldberger and Ben-Reuven (2016) and Luo et al. (2017) use the learned feature representation h to model p(ˆ y |y, h(x)) for image classification and relation extraction re"
D19-1362,K16-1018,0,0.370637,"s like named entity recognition (NER). Similarly, even for high-resource languages, there exists only few labeled data for most entity types beyond person, location and organization. Distantlyor weakly-supervised approaches have been proposed to solve this issue, e.g., by using lists of entities for labeling raw text (Ratinov and Roth, 2009; Dembowski et al., 2017). This allows obtaining large amounts of training data quickly and cheaply. Unfortunately, these labels often contain errors and learning with this noisily-labeled data is difficult and can even reduce overall performance (see, e.g. Fang and Cohn (2016)). A variety of ideas have been proposed to overcome the issues of noisy training data. One popular approach is to estimate the relation between noisy and clean, gold-standard labels and use this noise model to improve the training procedure. However, most of these approaches only assume a dependency between the labels and do not take the features into account when modeling the label noise. This may disregard important information. The global confusion matrix (Hedderich and Klakow, 2018) is a simple model which assumes that the errors in the noisy labels just depend on the clean labels. Our co"
D19-1362,L18-1550,0,0.0753905,"Missing"
D19-1362,W18-3402,1,0.763155,"ction respectively. In the work of Veit et al. (2017), p(y|ˆ y , h(x)) is estimated to clean the labels for an image classification task. The survey by Frenay and Verleysen (2014) gives a detailed overview about other techniques for learning in the presence of noisy labels. Specific to learning noisy sequence labels in NLP, Fang and Cohn (2016) used a combination of clean and noisy data for low-resource POS tagging. Yang et al. (2018) suggested partial annotation learning to lessen the effects of incomplete annotations and reinforcement learning for filtering incorrect labels for Chinese NER. Hedderich and Klakow (2018) used a confusion matrix and proposed to leverage pairs of clean and noisy labels for its initialization, evaluating on English NER. For English NER and Chunking, Paul et al. (2019) also used a confusion matrix but learned it with an EM approach and combined it with multi-task learning. Recently, Rahimi et al. (2019) studied input from different, unreliable sources and how to combine them for NER prediction. 3 Global Noise Model We assume a low-resource setting with a small set of gold standard annotated data C consisting of instances with features x and corresponding, clean labels y. Addition"
D19-1362,2005.mtsummit-papers.11,0,0.106901,"Missing"
D19-1362,P17-1040,0,0.334767,"ages 3554–3559, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Klakow, 2018; Paul et al., 2019), image classification (Mnih and Hinton, 2012; Sukhbaatar et al., 2015; Dgani et al., 2018) and general machine learning settings (Bekker and Goldberger, 2016; Patrini et al., 2017; Hendrycks et al., 2018). All of these methods, however, do not take the features into account that are used to represent the instances during classification. In (Xiao et al., 2015) only the noise type depends on x but not the actual noise model. Goldberger and Ben-Reuven (2016) and Luo et al. (2017) use the learned feature representation h to model p(ˆ y |y, h(x)) for image classification and relation extraction respectively. In the work of Veit et al. (2017), p(y|ˆ y , h(x)) is estimated to clean the labels for an image classification task. The survey by Frenay and Verleysen (2014) gives a detailed overview about other techniques for learning in the presence of noisy labels. Specific to learning noisy sequence labels in NLP, Fang and Cohn (2016) used a combination of clean and noisy data for low-resource POS tagging. Yang et al. (2018) suggested partial annotation learning to lessen the"
D19-1362,W09-1119,0,0.177993,"Missing"
D19-1362,W02-2024,0,0.0908108,"Missing"
D19-1362,W03-0419,0,0.0877804,"Missing"
D19-1362,W13-2412,0,0.0515372,"Missing"
D19-1362,C18-1183,0,0.0450859,"al noise model. Goldberger and Ben-Reuven (2016) and Luo et al. (2017) use the learned feature representation h to model p(ˆ y |y, h(x)) for image classification and relation extraction respectively. In the work of Veit et al. (2017), p(y|ˆ y , h(x)) is estimated to clean the labels for an image classification task. The survey by Frenay and Verleysen (2014) gives a detailed overview about other techniques for learning in the presence of noisy labels. Specific to learning noisy sequence labels in NLP, Fang and Cohn (2016) used a combination of clean and noisy data for low-resource POS tagging. Yang et al. (2018) suggested partial annotation learning to lessen the effects of incomplete annotations and reinforcement learning for filtering incorrect labels for Chinese NER. Hedderich and Klakow (2018) used a confusion matrix and proposed to leverage pairs of clean and noisy labels for its initialization, evaluating on English NER. For English NER and Chunking, Paul et al. (2019) also used a confusion matrix but learned it with an EM approach and combined it with multi-task learning. Recently, Rahimi et al. (2019) studied input from different, unreliable sources and how to combine them for NER prediction."
D19-1390,D17-1040,0,0.0196738,"d is too expensive for training. The complexity grows linearly with the source text length n and each computation of δ(yt |xi ) requires a separate softmax operation. One option is to approximate it by sampling like in hard attention models (Xu et al., 2015; Deng et al., 2017), but the training becomes challenging due to the non-differentiable sampling process. In our work, we take an alternative strategy of marginalizing only over k most likely aligned source words. This top-k approximation is widely adopted when the target distribution is expected to be sparse and only a few modes dominate (Britz et al., 2017; Ke et al., 2018; Shankar et al., 2018). We believe this is a valid assumption in text summarization since most source 3 hi can contain context information from surrounding words and thus not necessarily relates to word xi . It is part of the reason that neural attention has a poor alignment (Koehn and Knowles, 2017). Making it grounded on xi improves alignment and performance is also observed in machine translation (Nguyen and Chiang, 2018; Kuang et al., 2018) 3764 Figure 2: Architecture of the generalized pointer. The same encoder is applied to encode the source and target. When decoding “c"
D19-1390,P17-1183,0,0.0242432,"model the source-target alignment in a probabilistic sense. This makes it difficult to interpret or control model generations through the attention mechanism. In practice, people do find the attention vector is often blurred and suffers from poor alignment (Koehn and Knowles, 2017; Kiyono et al., 2018; Jain and Wallace, 2019). Hard alignment models, on the other hand, explicitly models the alignment relation between each source-target pair. Though theoretically sound, hard alignment models are hard to train. Exact marginalization is only feasible for data with limited length (Yu et al., 2016; Aharoni and Goldberg, 2017; Deng et al., 2018; Backes et al., 2018), or by assuming a simple copy generation process (Vinyals et al., 2015; Gu et al., 2016; See et al., 2017). Our model can be viewed as a combination of soft attention and hard alignment, where a simple top-k approximation is used to train the alignment part (Shankar et al., 2018; Shankar and Sarawagi, 2019). The hard alignment generation probability is designed as a relation summation operation to better fit the sum3765 marization task. In this way, the generalized copy mode acts as a hard alignment component to capture the direct word-to-word transiti"
D19-1390,N16-1012,0,0.0738987,"Missing"
D19-1390,J05-4004,0,0.7337,"Missing"
D19-1390,W18-2706,0,0.0146433,"d by Bayes’ theorem (Deng et al., 2018; Shankar and Sarawagi, 2019) to provide a better tool for interpretation2 and finally (c) explicitly capturing the alignment relation should improve generation performance. (Figure 1 shows an example of how latent alignment can improve the controllability and interpretation. Pointer generators fail to model such alignment relations that are not exact copies.) To eliminate the OOV problem, we utilize the byte-pairencoding (BPE) segmentation (Sennrich et al., 2016) to split rare words into sub-units, which has very few applications in summarization so far (Fan et al., 2018; Kiyono et al., 2018), though being a common technique in machine translation (Wu et al., 2016; Vaswani et al., 2017; Gehring et al., 2017). Our experiments are conducted on three summarization datasets: CNN/dailymail (Hermann et al., 2015), English Gigaword (Rush et al., 2015) and XSum (Narayan et al., 2018) (a newly collected corpus for extreme summarization). We further perform human evaluation and examine the word alignment accuracy on the manually annotated DUC 2004 dataset. Overall we find our model provides the following benefits: 1. It can capture richer latent alignment and improve t"
D19-1390,D18-1443,0,0.0835818,"he same as in Zhou et al. (2017) for Gigaword and See et al. (2017) for CNN/DM and XSum. We train with batch size 256 for gigaword and 32 for the other two. The vocabulary size is set to 30k for all dataset. Word representations are shared between the encoder and decoder. We tokenize words with WordPiece segmentation (Wu et al., 2016) to eliminate the OOV problem. More details are in Appendix A.3 Inference: We decode text using beam Figure 3: Test perplexity when increasing k search (Graves, 2012) with beam size 10. We apply length normalization to rescale the score. Unlike See et al. (2017); Gehrmann et al. (2018), we do not explicitly impose coverage penalty since it brings extra hyper-parameters. Instead, for CNN/Dailymail, we use a simple tri-gram penalty (Paulus et al., 2018) to prevent repeated generations. GPG models use an exact marginalization for testing and decoding, while for training and validation we use the top-k approximation mentioned above. The decoder will first decode sub-word ids then map them back to the normal sentence. All scores are reported on the word level and thus comparable with previous results. When computing scores for multi-sentence summaries. The generations are split"
D19-1390,I17-1004,0,0.0193237,"ed source word, then transform it by a learned relation embedding. The explicit alignment modelling encourages the model to stay close to the source information. Alignment Accuracy: We also manually annotate the word alignment on the same 100 DUC 2004 pairs. Following Daum´e III and Marcu (2005), words are allowed to be aligned with a specific source word, phrase or a “null” anchor meaning that it cannot be aligned with any source word. The accuracy is only evaluated on the target words with a non-null alignment. For each target token, the most attended source word is considered as alignment (Ghader and Monz, 2017). For the pointer generator and GPG, we also induce the posterior alignment by applying the Bayes’ theorem (derivation in appendix A.1). We report the alignment precision (Och and Ney, 2000) in Table 6, i.e., an alignment is considered as valid if it matches one of the human annotated ground truth. The results show that GPG improves the alignment precision by 0.1 compared with the standard pointer generator. The posterior alignment is more accurate than the prior one (also reflected in Figure 1), enabling better human interpretation. 6 Conclusion In this work, we propose generalizing the point"
D19-1390,N19-1357,0,0.0268056,"pointer is in Figure 2. 4 Related Work Neural attention models (Bahdanau et al., 2015) with the seq2seq architecture (Sutskever et al., 2014) have achieved impressive results in text summarization tasks. However, the attention vector comes from a weighted sum of source information and does not model the source-target alignment in a probabilistic sense. This makes it difficult to interpret or control model generations through the attention mechanism. In practice, people do find the attention vector is often blurred and suffers from poor alignment (Koehn and Knowles, 2017; Kiyono et al., 2018; Jain and Wallace, 2019). Hard alignment models, on the other hand, explicitly models the alignment relation between each source-target pair. Though theoretically sound, hard alignment models are hard to train. Exact marginalization is only feasible for data with limited length (Yu et al., 2016; Aharoni and Goldberg, 2017; Deng et al., 2018; Backes et al., 2018), or by assuming a simple copy generation process (Vinyals et al., 2015; Gu et al., 2016; See et al., 2017). Our model can be viewed as a combination of soft attention and hard alignment, where a simple top-k approximation is used to train the alignment part ("
D19-1390,W18-5410,0,0.0287757,"Missing"
D19-1390,W17-3204,0,0.164436,"ng due to the non-differentiable sampling process. In our work, we take an alternative strategy of marginalizing only over k most likely aligned source words. This top-k approximation is widely adopted when the target distribution is expected to be sparse and only a few modes dominate (Britz et al., 2017; Ke et al., 2018; Shankar et al., 2018). We believe this is a valid assumption in text summarization since most source 3 hi can contain context information from surrounding words and thus not necessarily relates to word xi . It is part of the reason that neural attention has a poor alignment (Koehn and Knowles, 2017). Making it grounded on xi improves alignment and performance is also observed in machine translation (Nguyen and Chiang, 2018; Kuang et al., 2018) 3764 Figure 2: Architecture of the generalized pointer. The same encoder is applied to encode the source and target. When decoding “closes”, we first find top-k source positions with the most similar encoded state. For each position, the decoding probability is computed by adding its word embedding and a predicted relation embedding. tokens have a vanishingly small probability to be transferred into a target word. For each target word, how to deter"
D19-1390,D18-1207,0,0.0387018,"Missing"
D19-1390,P16-1154,0,0.469041,"” and “closure”, which can be manually controlled to generate desired summaries. Decoded samples are shown when aligned to “announced” and “closure” respectively. Highlighted source words are those that can be directly aligned to a target token in the gold summary. Introduction Modern state-of-the-art (SOTA) summarization models are built upon the pointer generator architecture (See et al., 2017). At each decoding step, the model generates a sentinel to decide whether to sample words based on the neural attention (generation mode), or directly copy from an aligned source context (point mode) (Gu et al., 2016; Merity et al., 2017; Yang et al., 2017). Though outperforming the vanilla attention models, the pointer generator only captures exact word matches. As shown in Fig. 1, for abstractive summarization, ∗ Correspondence to xshen@mpi-inf.mpg.de The source code is available at https://github. com/chin-gyou/generalized-PG. 1 there exists a large number of syntactic inflections (escalated → escalates) or semantic transformations (military campaign → war), where the target word also has an explicit grounding in the source context but changes its surface. In standard pointer generators, these words ar"
D19-1390,P18-1164,0,0.0249886,"words. This top-k approximation is widely adopted when the target distribution is expected to be sparse and only a few modes dominate (Britz et al., 2017; Ke et al., 2018; Shankar et al., 2018). We believe this is a valid assumption in text summarization since most source 3 hi can contain context information from surrounding words and thus not necessarily relates to word xi . It is part of the reason that neural attention has a poor alignment (Koehn and Knowles, 2017). Making it grounded on xi improves alignment and performance is also observed in machine translation (Nguyen and Chiang, 2018; Kuang et al., 2018) 3764 Figure 2: Architecture of the generalized pointer. The same encoder is applied to encode the source and target. When decoding “closes”, we first find top-k source positions with the most similar encoded state. For each position, the decoding probability is computed by adding its word embedding and a predicted relation embedding. tokens have a vanishingly small probability to be transferred into a target word. For each target word, how to determine the k most likely aligned source words is crucial. An ideal system should always include the gold aligned source word in the top-k selections."
D19-1390,P16-1014,0,0.0188453,"e probability of enabling the generation mode instead of the point 3763 mode. In the generation mode, the model computes the probability over the whole vocabulary as in Eq. 2. In the point mode, the model computes which source word to copy based on the attention distribution at from Eq.1. The final probability is marginalized over at,i : X p(yt ) = pgen pvocab (yt ) + (1 − pgen ) at,i δ(yt |xi ) i ( 1, δ(yt |xi ) = 0, if yt = xi . otherwise. (4) If we know exactly from which mode each word comes from, e.g., by assuming all co-occurred words are copied, then the marginalization can be omitted (Gulcehre et al., 2016; Wiseman et al., 2017), but normally pgen is treated as a latent variable (Gu et al., 2016; See et al., 2017). 3 Generalized Pointer Generator (GPG) As seen in Eq .4, δ(yt |xi ) is a 0-1 event that is only turned on when yt is exactly the same word as xi . This restricts the expressiveness of the point mode, preventing it from paying attention to inflections, POS transitions or paraphrases. This section explains how we generalize pointer networks to cover these conditions. Redefine δ(yt |xi ): We extend δ(yt |xi ) by defining it as a smooth probability distribution over the whole vocabulary."
D19-1390,D17-1222,0,0.0355034,"Missing"
D19-1390,C18-1211,0,0.0178887,"arget entity. The intuition is straightforward: After pointing to xt , humans usually first decide which relation should be applied (inflection, hypernym, synonym, etc) based on the context [dt , hi ], then transform xi to the proper target word yt . Using addition transformation is backed by the observation that vector differences often reflect meaningful word analogies (Mikolov et al., 2013; Pennington et al., 2014) (“man” − “king” ≈ “woman” − “queen”) and they are effective at encoding a great amount of word relations like hypernym, meronym and morphological changes (Vylomova et al., 2016; Hakami et al., 2018; Allen and Hospedales, 2019). These word relations reflect most alignment conditions in text summarization. For example, humans often change the source word to its hypernym (boy → child), to make it more specific (person → man) or apply morphological transformations (liked → like). Therefore, we assume δ(yt |xi ) can be well modelled by first predicting a relation embedding − to be applied, then added to → xi . If xi should be exactly copied like in standard pointer generators, the relation embedding is a zero vector meaning an identity transition. We also tried apply− ing more complex transi"
D19-1390,D15-1166,0,0.132949,"Missing"
D19-1390,E17-2007,0,0.013019,"ten fails to quan6 It also reveals a limit of GPG model in that it only models token-level alignment. For phrases like death row prisoner, it cannot align it based on its compositional meaning. 3768 seq2seq Point.Gen. GPG-ptr GPG Gold Fluency 0.83 0.78 0.79 0.82 0.96 Faithfulness 0.61 0.65 0.78 0.74 0.92 0/1 0.53 0.55 0.67 0.69 0.96 Prec seq2seq Point.Gen. GPG 0.361 0.435 (0.512) 0.533 (0.628) Table 6: Word Alignment Precision on DUC 2004. Number in bracket is the posterior alignment precision. Table 5: Human evaluation results on DUC 2004. 0/1 is the score for the 0/1 Turing test. tify them (Schluter, 2017; Cao et al., 2018). 100 random source-target pairs are sampled from the human-written DUC 2004 data for task 1&2 (Over et al., 2007). Models trained on Gigaword are applied to generate corresponding summaries. The gold targets, together with the model generations are randomly shuffled then assigned to 10 human annotators. Each pair is evaluated by three different people and the most agreed score is adopted. Each pair is assigned a 0-1 score to indicate (1) whether the target is fluent in grammar, (2) whether the target faithfully conveys the source information without hallucination and (3) wh"
D19-1390,W12-3018,0,0.0321932,"Missing"
D19-1390,P17-1099,0,0.21121,"d summaries.1 1 Figure 1: Alignment visualization of our model when decoding “closes”. Posterior alignment is more accurate for model interpretation. In contrast, the prior alignment probability is spared to “announced” and “closure”, which can be manually controlled to generate desired summaries. Decoded samples are shown when aligned to “announced” and “closure” respectively. Highlighted source words are those that can be directly aligned to a target token in the gold summary. Introduction Modern state-of-the-art (SOTA) summarization models are built upon the pointer generator architecture (See et al., 2017). At each decoding step, the model generates a sentinel to decide whether to sample words based on the neural attention (generation mode), or directly copy from an aligned source context (point mode) (Gu et al., 2016; Merity et al., 2017; Yang et al., 2017). Though outperforming the vanilla attention models, the pointer generator only captures exact word matches. As shown in Fig. 1, for abstractive summarization, ∗ Correspondence to xshen@mpi-inf.mpg.de The source code is available at https://github. com/chin-gyou/generalized-PG. 1 there exists a large number of syntactic inflections (escalate"
D19-1390,D18-1206,0,0.148179,"etation. Pointer generators fail to model such alignment relations that are not exact copies.) To eliminate the OOV problem, we utilize the byte-pairencoding (BPE) segmentation (Sennrich et al., 2016) to split rare words into sub-units, which has very few applications in summarization so far (Fan et al., 2018; Kiyono et al., 2018), though being a common technique in machine translation (Wu et al., 2016; Vaswani et al., 2017; Gehring et al., 2017). Our experiments are conducted on three summarization datasets: CNN/dailymail (Hermann et al., 2015), English Gigaword (Rush et al., 2015) and XSum (Narayan et al., 2018) (a newly collected corpus for extreme summarization). We further perform human evaluation and examine the word alignment accuracy on the manually annotated DUC 2004 dataset. Overall we find our model provides the following benefits: 1. It can capture richer latent alignment and improve the word alignment accuracy, enabling better controllability and interpretation. 2. The generated summaries are more faithful to the source context because of the explicit alignment grounding. 3. It improves the abstraction of generations because our model allows editing the pointed token instead of always copy"
D19-1390,P16-1162,0,0.0408775,"e can better control the generation by manipulating the alignment trajectory, (b) posterior alignment can be inferred by Bayes’ theorem (Deng et al., 2018; Shankar and Sarawagi, 2019) to provide a better tool for interpretation2 and finally (c) explicitly capturing the alignment relation should improve generation performance. (Figure 1 shows an example of how latent alignment can improve the controllability and interpretation. Pointer generators fail to model such alignment relations that are not exact copies.) To eliminate the OOV problem, we utilize the byte-pairencoding (BPE) segmentation (Sennrich et al., 2016) to split rare words into sub-units, which has very few applications in summarization so far (Fan et al., 2018; Kiyono et al., 2018), though being a common technique in machine translation (Wu et al., 2016; Vaswani et al., 2017; Gehring et al., 2017). Our experiments are conducted on three summarization datasets: CNN/dailymail (Hermann et al., 2015), English Gigaword (Rush et al., 2015) and XSum (Narayan et al., 2018) (a newly collected corpus for extreme summarization). We further perform human evaluation and examine the word alignment accuracy on the manually annotated DUC 2004 dataset. Over"
D19-1390,N18-1031,0,0.0149399,"st likely aligned source words. This top-k approximation is widely adopted when the target distribution is expected to be sparse and only a few modes dominate (Britz et al., 2017; Ke et al., 2018; Shankar et al., 2018). We believe this is a valid assumption in text summarization since most source 3 hi can contain context information from surrounding words and thus not necessarily relates to word xi . It is part of the reason that neural attention has a poor alignment (Koehn and Knowles, 2017). Making it grounded on xi improves alignment and performance is also observed in machine translation (Nguyen and Chiang, 2018; Kuang et al., 2018) 3764 Figure 2: Architecture of the generalized pointer. The same encoder is applied to encode the source and target. When decoding “closes”, we first find top-k source positions with the most similar encoded state. For each position, the decoding probability is computed by adding its word embedding and a predicted relation embedding. tokens have a vanishingly small probability to be transferred into a target word. For each target word, how to determine the k most likely aligned source words is crucial. An ideal system should always include the gold aligned source word in"
D19-1390,D18-1065,0,0.0893309,"complexity grows linearly with the source text length n and each computation of δ(yt |xi ) requires a separate softmax operation. One option is to approximate it by sampling like in hard attention models (Xu et al., 2015; Deng et al., 2017), but the training becomes challenging due to the non-differentiable sampling process. In our work, we take an alternative strategy of marginalizing only over k most likely aligned source words. This top-k approximation is widely adopted when the target distribution is expected to be sparse and only a few modes dominate (Britz et al., 2017; Ke et al., 2018; Shankar et al., 2018). We believe this is a valid assumption in text summarization since most source 3 hi can contain context information from surrounding words and thus not necessarily relates to word xi . It is part of the reason that neural attention has a poor alignment (Koehn and Knowles, 2017). Making it grounded on xi improves alignment and performance is also observed in machine translation (Nguyen and Chiang, 2018; Kuang et al., 2018) 3764 Figure 2: Architecture of the generalized pointer. The same encoder is applied to encode the source and target. When decoding “closes”, we first find top-k source posit"
D19-1390,C00-2163,0,0.329122,"ally annotate the word alignment on the same 100 DUC 2004 pairs. Following Daum´e III and Marcu (2005), words are allowed to be aligned with a specific source word, phrase or a “null” anchor meaning that it cannot be aligned with any source word. The accuracy is only evaluated on the target words with a non-null alignment. For each target token, the most attended source word is considered as alignment (Ghader and Monz, 2017). For the pointer generator and GPG, we also induce the posterior alignment by applying the Bayes’ theorem (derivation in appendix A.1). We report the alignment precision (Och and Ney, 2000) in Table 6, i.e., an alignment is considered as valid if it matches one of the human annotated ground truth. The results show that GPG improves the alignment precision by 0.1 compared with the standard pointer generator. The posterior alignment is more accurate than the prior one (also reflected in Figure 1), enabling better human interpretation. 6 Conclusion In this work, we propose generalizing the pointer generator to go beyond exact copy operation. At each decoding step, the decoder can either generate from the vocabulary, copy or edit some source words by estimating a relation embedding."
D19-1390,D14-1162,0,0.0820877,"yer-perceptron in our experiments. The com∗ is similar to the classical TransE putation of yt,i model (Bordes et al., 2013) where an entity vector is added by a relation embedding to translate into the target entity. The intuition is straightforward: After pointing to xt , humans usually first decide which relation should be applied (inflection, hypernym, synonym, etc) based on the context [dt , hi ], then transform xi to the proper target word yt . Using addition transformation is backed by the observation that vector differences often reflect meaningful word analogies (Mikolov et al., 2013; Pennington et al., 2014) (“man” − “king” ≈ “woman” − “queen”) and they are effective at encoding a great amount of word relations like hypernym, meronym and morphological changes (Vylomova et al., 2016; Hakami et al., 2018; Allen and Hospedales, 2019). These word relations reflect most alignment conditions in text summarization. For example, humans often change the source word to its hypernym (boy → child), to make it more specific (person → man) or apply morphological transformations (liked → like). Therefore, we assume δ(yt |xi ) can be well modelled by first predicting a relation embedding − to be applied, then ad"
D19-1390,N18-1202,0,0.0205605,"probability to be transferred into a target word. For each target word, how to determine the k most likely aligned source words is crucial. An ideal system should always include the gold aligned source word in the top-k selections. We tried several methods and find the best performance is achieved when encoding each source/target token into a vector, then choosing the k source words that are closest to the target word in the encoded vector space. The closeness is measured by the vector inner product4 . The encoded vector space serves like a contextualized word embedding (McCann et al., 2017; Peters et al., 2018). Intuitively if a target word can be aligned to a source word, they should have similar semantic meaning and surrounding context thus should have similar contextualized word embeddings. The new objective is then defined as in Eq. 6: p(yt ) = pgen pvocab (yt ) + (1 − pgen )ppoint (yt ) X ppoint (yt ) = at,i δ(yt |xi ) i ≈ X at,i δ(yt |xi ) (6) i;hT i e(yt )∈TopK e(yt ) is the encoded vector for yt . The marginalization is performed only over the k chosen source words. Eq. 6 is a lower bound of the data likelihood because it only marginalizes over a subset of X. In general a larger k can tighte"
D19-1390,E17-2025,0,0.0282207,"on In a seq2seq model, each source token xi is encoded into a vector hi . At each decoding step t, the decoder computes an attention distribution at over the encoded vectors based on the current hidden state dt (Bahdanau et al., 2015): at = softmax(f (hi , dt )) (1) f is a score function to measure the similarity between hi and dt . The context vector ct and the probability of next token are computed as below. X ct = hi at,i i yt∗ = [dt ◦ ct ]L (2) pvocab = softmax(yt∗ W T ) ◦ means concatenation and L, W are trainable parameters. We tie the parameters of W and the word embedding matrix as in Press and Wolf (2017); Inan et al. (2017). Namely, a target vector yt∗ is predicted, words having a higher inner product with yt∗ will have a higher probability. 2.2 Pointer Generator The pointer generator extends the seq2seq model to support copying source words (Vinyals et al., 2015). At each time step t, the model first computes a generation probability pgen ∈ [0, 1] by: 2 The induced alignment offers useful annotations for people to identify the source correspondence for each target word. News editors can post-edit machine-generated summaries more efficiently with such annotation. For summary readers, it also"
D19-1390,P16-1158,0,0.0149714,"to translate into the target entity. The intuition is straightforward: After pointing to xt , humans usually first decide which relation should be applied (inflection, hypernym, synonym, etc) based on the context [dt , hi ], then transform xi to the proper target word yt . Using addition transformation is backed by the observation that vector differences often reflect meaningful word analogies (Mikolov et al., 2013; Pennington et al., 2014) (“man” − “king” ≈ “woman” − “queen”) and they are effective at encoding a great amount of word relations like hypernym, meronym and morphological changes (Vylomova et al., 2016; Hakami et al., 2018; Allen and Hospedales, 2019). These word relations reflect most alignment conditions in text summarization. For example, humans often change the source word to its hypernym (boy → child), to make it more specific (person → man) or apply morphological transformations (liked → like). Therefore, we assume δ(yt |xi ) can be well modelled by first predicting a relation embedding − to be applied, then added to → xi . If xi should be exactly copied like in standard pointer generators, the relation embedding is a zero vector meaning an identity transition. We also tried apply− in"
D19-1390,D15-1044,0,0.331312,"e controllability and interpretation. Pointer generators fail to model such alignment relations that are not exact copies.) To eliminate the OOV problem, we utilize the byte-pairencoding (BPE) segmentation (Sennrich et al., 2016) to split rare words into sub-units, which has very few applications in summarization so far (Fan et al., 2018; Kiyono et al., 2018), though being a common technique in machine translation (Wu et al., 2016; Vaswani et al., 2017; Gehring et al., 2017). Our experiments are conducted on three summarization datasets: CNN/dailymail (Hermann et al., 2015), English Gigaword (Rush et al., 2015) and XSum (Narayan et al., 2018) (a newly collected corpus for extreme summarization). We further perform human evaluation and examine the word alignment accuracy on the manually annotated DUC 2004 dataset. Overall we find our model provides the following benefits: 1. It can capture richer latent alignment and improve the word alignment accuracy, enabling better controllability and interpretation. 2. The generated summaries are more faithful to the source context because of the explicit alignment grounding. 3. It improves the abstraction of generations because our model allows editing the poin"
D19-1390,1983.tc-1.13,0,0.564362,"Missing"
D19-1390,D17-1197,0,0.0192885,"controlled to generate desired summaries. Decoded samples are shown when aligned to “announced” and “closure” respectively. Highlighted source words are those that can be directly aligned to a target token in the gold summary. Introduction Modern state-of-the-art (SOTA) summarization models are built upon the pointer generator architecture (See et al., 2017). At each decoding step, the model generates a sentinel to decide whether to sample words based on the neural attention (generation mode), or directly copy from an aligned source context (point mode) (Gu et al., 2016; Merity et al., 2017; Yang et al., 2017). Though outperforming the vanilla attention models, the pointer generator only captures exact word matches. As shown in Fig. 1, for abstractive summarization, ∗ Correspondence to xshen@mpi-inf.mpg.de The source code is available at https://github. com/chin-gyou/generalized-PG. 1 there exists a large number of syntactic inflections (escalated → escalates) or semantic transformations (military campaign → war), where the target word also has an explicit grounding in the source context but changes its surface. In standard pointer generators, these words are not covered by the point mode. This lar"
D19-1390,D16-1138,0,0.0645392,"ion and does not model the source-target alignment in a probabilistic sense. This makes it difficult to interpret or control model generations through the attention mechanism. In practice, people do find the attention vector is often blurred and suffers from poor alignment (Koehn and Knowles, 2017; Kiyono et al., 2018; Jain and Wallace, 2019). Hard alignment models, on the other hand, explicitly models the alignment relation between each source-target pair. Though theoretically sound, hard alignment models are hard to train. Exact marginalization is only feasible for data with limited length (Yu et al., 2016; Aharoni and Goldberg, 2017; Deng et al., 2018; Backes et al., 2018), or by assuming a simple copy generation process (Vinyals et al., 2015; Gu et al., 2016; See et al., 2017). Our model can be viewed as a combination of soft attention and hard alignment, where a simple top-k approximation is used to train the alignment part (Shankar et al., 2018; Shankar and Sarawagi, 2019). The hard alignment generation probability is designed as a relation summation operation to better fit the sum3765 marization task. In this way, the generalized copy mode acts as a hard alignment component to capture the"
D19-1390,D18-1089,0,0.0150354,"(1951 article-summary pairs). R-1 R-2 R-L PPL 29.70 31.89 31.90 31.87 31.49 33.11 9.21 11.54 11.15 11.20 11.02 12.55 23.24 25.75 25.48 25.42 25.37 26.57 22.87 17.83 18.62 15.28 Table 3: ROUGE score on XSum. * marks results from Narayan et al. (2018). Underlined values are significantly better than Point.Gen. with p = 0.05. without the generation mode outperforms standard pointer generators in CNN/DM and Gigaword, implying most target tokens can be generated by aligning to a specific source word. The finding is consistent with previous research claiming CNN/DM summaries are largely extractive (Zhang et al., 2018; Kry´sci´nski et al., 2018). Though the Gigaword headline dataset is more abstractive, most words are simple paraphrases of some specific source word, so pure pointer GPGptr can work well. This is different from the XSum story summarization dataset where many target words require high-level abstraction or inference and cannot be aligned to a single source word, so combining the point and generation mode is necessary for a good performance. The word perplexity results are consistent over all dataset (GPG < GPG-ptr < Point.Gen. < seq2seq). The reduction of perplexity does not necessarily indica"
D19-1390,P17-1101,0,0.0853928,"e. XSum corpus provides a single-sentence summary for each BBC long story. We pick these three dataset as they have different properties for us to compare models. CNN/DM strongly favors extractive summarization (Kry´sci´nski et al., 2018). Gigaword has more one-to-one word direct mapping (with simple paraphrasing) (Napoles et al., 2012) while XSum needs to perform more information fusion and inference since the source is much longer than the target (Narayan et al., 2018). Model: We use single-layer bi-LSTM encoders for all models. For comparison, hidden layer dimensions are set the same as in Zhou et al. (2017) for Gigaword and See et al. (2017) for CNN/DM and XSum. We train with batch size 256 for gigaword and 32 for the other two. The vocabulary size is set to 30k for all dataset. Word representations are shared between the encoder and decoder. We tokenize words with WordPiece segmentation (Wu et al., 2016) to eliminate the OOV problem. More details are in Appendix A.3 Inference: We decode text using beam Figure 3: Test perplexity when increasing k search (Graves, 2012) with beam size 10. We apply length normalization to rescale the score. Unlike See et al. (2017); Gehrmann et al. (2018), we do no"
E12-1033,P08-1034,0,0.0700927,"Missing"
E12-1033,J92-4003,0,0.0657639,"V. Madrid, Dresden, Bordeaux, Istanbul, Caracas, Manila, ... Toby, Betsy, Michele, Tim, Jean-Marie, Rory, Andrew, ... detest, resent, imply, liken, indicate, suggest, owe, expect, ... disappointment, unease, nervousness, dismay, optimism, ... remark, baby, book, saint, manhole, maxim, coin, batter, ... Table 2: Some automatically induced clusters. ETHICS 1.47 SPACE 2.70 FICTION 11.59 Table 3: Percentage of opinion holders as patients. al., 2010). Such a generalization is, in particular, attractive as it is cheaply produced. As a stateof-the-art clustering method, we consider Brown clustering (Brown et al., 1992) as implemented in the SRILM-toolkit (Stolcke, 2002). We induced 1000 clusters which is also the configuration used in (Turian et al., 2010).5 Table 2 illustrates a few of the clusters induced from our unlabeled dataset introduced in Section (§) 2. Some of these clusters represent location or person names (e.g. I. & II.). This exemplifies why clustering is effective for named-entity recognition. We also find clusters that intuitively seem to be meaningful for our task (e.g. III. & IV.) but, on the other hand, there are clusters that contain words that with the exception of their part of speech"
E12-1033,H05-1045,0,0.817911,"including the same generalization methods. 4.1 Conditional Random Fields (CRF) The supervised classifier most frequently used for information extraction tasks, in general, are conditional random fields (CRF) (Lafferty et al., 2001). Using CRF, the task of opinion holder extraction is framed as a tagging problem in which given a sequence of observations x = x1 x2 . . . xn (words in a sentence) a sequence of output tags y = y1 y2 . . . yn indicating the boundaries of opinion holders is computed by modeling the conditional probability P (x|y). The features we use (Table 5) are mostly inspired by Choi et al. (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). They are organized into groups. The basic group Plain does not contain any generalization method. Each other group is dedicated to one specific generalization method that we want to examine (Clus, Induc and Lex). Apart from considering generalization features indicating the presence of generalization types, we also consider those types in conjunction with semantic roles. As already indicated above, semantic roles are especially important for the detection of opinion holders. Unfortunately, the cor328"
E12-1033,W06-1651,0,0.0725387,"on holders that RB misses. CK has the advantage that it is not only bound to the relationship between candidate holder and predicate. It learns further heuristics, e.g. that sentence-initial mentions of persons are likely opinion holders. In (12), for example, this heuristics fires while RB overlooks this instance as to give someone a share of advice is not part of the lexicon. (12) She later gives Charlotte her share of advice on running a household. 7 Related Work The research on opinion holder extraction has been focusing on applying different data-driven approaches. Choi et al. (2005) and Choi et al. (2006) explore conditional random fields, Wiegand and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence. A comparison of 332 Features SPACE (similar target domain) CRF CK Prec Rec F1 Prec Rec Plain +Clus +Induc +Lex +Clus+Induc +Lex+Induc +Clus+Lex All 47.32 49.00 42.92 49.65 46.61 48.75 49.72 49.87 48.62 48.62 49.15 49.07 48.78 50.87 50.87 51.03 47.96 48.81 45.82 49.36 47.67 49.78 50.29 50.44 45.89 49.23 46.66 49.60 48.65 49.92 53.70 51.68 57.07 57"
E12-1033,P05-1045,0,0.0610266,"or example, it would correspond to A1 shock. Therefore, we introduce for each generalization method an additional feature replacing the sparse lexical item by a generalization label, i.e. Clus: A1 CLUSTER-35265, Induc: A1 INDUC-PRED and Lex: A1 LEX-PRED.6 For this learning method, we use CRF++.7 We choose a configuration that provides good performance on our source domain (i.e. ETHICS).8 For semantic role labeling we use SWIRL9 , for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005). convolution kernels, the structures to be compared within the kernel function are not vectors comprising manually designed features but the underlying discrete structures, such as syntactic parse trees or part-of-speech sequences. Since they are directly provided to the learning algorithm, a classifier can be built without taking the effort of implementing an explicit feature extraction. We take the best configuration from (Wiegand and Klakow, 2010) that comprises a combination of three different tree kernels being two tree kernels based on constituency parse trees (one with predicate and an"
E12-1033,C10-1059,0,0.042935,"output of clustering will exclusively be evaluated in the context of learning-based methods, since there is no straightforward way of incorporating this output into a rule-based classifier. 6 Experiments CK and RB have an instance space that is different from the one of CRF. While CRF produces a prediction for every word token in a sentence, CK and RB only produce a prediction for every noun phrase. For evaluation, we project the predictions from RB and CK to word token level in order to ensure comparability. We evaluate the sequential output with precision, recall and F-score as defined in (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). 6.1 Rule-based Classifier Table 6 shows the cross-domain performance of the different rule-based classifiers. RB-Lex performs better than RB-Induc. In comparison to the domains ETHICS and SPACE the difference is larger on FICTION. Presumably, this is due to the fact that the predicates in Induc are extracted from a news corpus (§2). Thus, Induc may slightly suffer from a domain mismatch. A combination of the two classifiers, i.e. RB-Lex+Induc, results in a notable improvement in the FICTION-domain. The approaches that also detect opinion holders as patients (A"
E12-1033,P11-2018,0,0.0146717,"usively be evaluated in the context of learning-based methods, since there is no straightforward way of incorporating this output into a rule-based classifier. 6 Experiments CK and RB have an instance space that is different from the one of CRF. While CRF produces a prediction for every word token in a sentence, CK and RB only produce a prediction for every noun phrase. For evaluation, we project the predictions from RB and CK to word token level in order to ensure comparability. We evaluate the sequential output with precision, recall and F-score as defined in (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). 6.1 Rule-based Classifier Table 6 shows the cross-domain performance of the different rule-based classifiers. RB-Lex performs better than RB-Induc. In comparison to the domains ETHICS and SPACE the difference is larger on FICTION. Presumably, this is due to the fact that the predicates in Induc are extracted from a news corpus (§2). Thus, Induc may slightly suffer from a domain mismatch. A combination of the two classifiers, i.e. RB-Lex+Induc, results in a notable improvement in the FICTION-domain. The approaches that also detect opinion holders as patients (AG+PT) including our novel approa"
E12-1033,W06-0301,0,0.0508218,"luated on the MPQA corpus. Some generalization methods are incorporated but unlike this paper they are neither systematically compared nor combined. The role of resources that provide the knowledge of argument positions of opinion holders is not covered in any of these works. This kind of knowledge should be directly learnt from the labeled training data. In this work, we found, however, that the distribution of argument positions of opinion holders varies throughout the different domains and, therefore, cannot be learnt from any arbitrary out-of-domain training set. Bethard et al. (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003). Bethard et al. (2004) use this resource to acquire labeled training data while in (Kim and Hovy, 2006) FrameNet is used within a rule-based classifier mapping frame-elements of frames to opinion holders. Bethard et al. (2004) only evaluate on an artificial dataset (i.e. a subset of sentences from FrameNet and PropBank (Kingsbury and Palmer, 2002)). The only realistic test set on which Kim and Hovy (2006) evaluate their approach are news texts. Their method is compared against a simple rule-based baseline an"
E12-1033,kingsbury-palmer-2002-treebank,0,0.698244,"throughout the different domains. In (1) and (2), the opinion holders are agents of a predictive predicate, whereas the opinion holder her daughters in (3) is a patient2 of embarrasses. (3) Mrs. Bennet does what she can to get Jane and Bingley together and embarrasses her daughters by doing so. If only sentences, such as (1) and (2), occur in the training data, a classifier will not correctly extract the opinion holder in (3), unless it obtains additional knowledge as to which predicates take opinion holders as patients. 1 By agent we always mean constituents being labeled as A0 in PropBank (Kingsbury and Palmer, 2002). 2 By patient we always mean constituents being labeled as A1 in PropBank. 325 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 325–335, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics In this work, we will consider three different generalization methods being simple unsupervised word clustering, an induction method and the usage of lexical resources. We show that generalization causes significant improvements and that the impact of improvement depends on how much training and test data dif"
E12-1033,P03-1054,0,0.00603371,"the predicate is most likely a sparse feature. For the opinion holder me in (10), for example, it would correspond to A1 shock. Therefore, we introduce for each generalization method an additional feature replacing the sparse lexical item by a generalization label, i.e. Clus: A1 CLUSTER-35265, Induc: A1 INDUC-PRED and Lex: A1 LEX-PRED.6 For this learning method, we use CRF++.7 We choose a configuration that provides good performance on our source domain (i.e. ETHICS).8 For semantic role labeling we use SWIRL9 , for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005). convolution kernels, the structures to be compared within the kernel function are not vectors comprising manually designed features but the underlying discrete structures, such as syntactic parse trees or part-of-speech sequences. Since they are directly provided to the learning algorithm, a classifier can be built without taking the effort of implementing an explicit feature extraction. We take the best configuration from (Wiegand and Klakow, 2010) that comprises a combination of three different tree kernels bein"
E12-1033,P09-1113,0,0.0611495,"inion holder mentions from those corpora. The table shows indeed that on the domains from the MPQA corpus, i.e. ETHICS and SPACE, those opinion holders play a minor role but there is a notably higher proportion on the FICTION-domain. 3.3 3.3.1 Distant Supervision with Prototypical Opinion Holders Lexical resources are potentially much more expressive than word clustering. This knowledge, however, is usually manually compiled, which makes this solution much more expensive. Wiegand and Klakow (2011a) present an intermediate solution for opinion holder extraction inspired by distant supervision (Mintz et al., 2009). The output of that method is also a lexicon of predicates but it is automatically extracted from a large unlabeled corpus. This is achieved by collecting predicates that frequently co-occur with prototypical opinion holders, i.e. common nouns such as opponents (7) or critics (8), if they are an agent of that predicate. The rationale behind this is that those nouns act very much like actual opinion holders and therefore can be seen as a proxy. (4) I always supported this idea. holder:agent. (5) This worries me. holder:patient (6) He disappointed me. holder:patient We follow Wiegand and Klakow"
E12-1033,R11-1028,0,0.0863495,"r from each other. We also address the less common case of opinion holders being realized in patient position and suggest approaches including a novel (linguisticallyinformed) extraction method how to detect those opinion holders without labeled training data as standard datasets contain too few instances of this type. 1 Introduction Opinion holder extraction is one of the most important subtasks in sentiment analysis. The extraction of sources of opinions is an essential component for complex real-life applications, such as opinion question answering systems or opinion summarization systems (Stoyanov and Cardie, 2011). Common approaches designed to extract opinion holders are based on data-driven methods, in particular supervised learning. In this paper, we examine the role of generalization for opinion holder extraction in both indomain and cross-domain classification. Generalization may not only help to compensate the availability of labeled training data but also conciliate domain mismatches. In order to illustrate this, compare for instance (1) and (2). (1) Malaysia did not agree to such treatment of Al-Qaeda soldiers as they were prisoners-of-war and should be accorded treatment as provided for under"
E12-1033,P10-1059,0,0.0346307,"ation would suit our purpose. It is henceforth called SPACE. Domain ETHICS SPACE FICTION The cluster is the union of documents with the following MPQA-topic labels: axisofevil, guantanamo, humanrights, mugabe and settlements. # Holders in sentence (average) 0.79 0.28 1.49 Table 1: Statistics of the different domain corpora. In addition to these two (sub)domains, we chose some text type that is not even news text in order to have a very distant domain. Therefore, we had to use some text not included in the MPQA corpus. Existing text collections containing product reviews (Kessler et al., 2010; Toprak et al., 2010), which are generally a popular resource for sentiment analysis, were not found suitable as they only contain few distinct opinion holders. We finally used a few summaries of fictional work (two Shakespeare plays and one novel by Jane Austen4 ) since their language is notably different from that of news texts and they contain a large number of different opinion holders (therefore opinion holder extraction is a meaningful task on this text type). These texts make up our third domain FICTION. We manually labeled it with opinion holder information by applying the annotation scheme of the MPQA cor"
E12-1033,P10-1040,0,0.0328556,", liken, indicate, suggest, owe, expect, ... disappointment, unease, nervousness, dismay, optimism, ... remark, baby, book, saint, manhole, maxim, coin, batter, ... Table 2: Some automatically induced clusters. ETHICS 1.47 SPACE 2.70 FICTION 11.59 Table 3: Percentage of opinion holders as patients. al., 2010). Such a generalization is, in particular, attractive as it is cheaply produced. As a stateof-the-art clustering method, we consider Brown clustering (Brown et al., 1992) as implemented in the SRILM-toolkit (Stolcke, 2002). We induced 1000 clusters which is also the configuration used in (Turian et al., 2010).5 Table 2 illustrates a few of the clusters induced from our unlabeled dataset introduced in Section (§) 2. Some of these clusters represent location or person names (e.g. I. & II.). This exemplifies why clustering is effective for named-entity recognition. We also find clusters that intuitively seem to be meaningful for our task (e.g. III. & IV.) but, on the other hand, there are clusters that contain words that with the exception of their part of speech do not have anything in common (e.g. V.). 3.2 Manually Compiled Lexicons (Lex) The major shortcoming of word clustering is that it lacks an"
E12-1033,J04-3002,0,0.0473889,"Missing"
E12-1033,N10-1121,1,0.859655,"datasets contain too few instances of them. In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set. We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method. 2 Data As a labeled dataset we mainly use the MPQA 2.0 corpus (Wiebe et al., 2005). We adhere to the definition of opinion holders from previous work (Wiegand and Klakow, 2010; Wiegand and Klakow, 2011a; Wiegand and Klakow, 2011b), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2005) is considered an opinion holder. This corpus contains almost exclusively news texts. In order to divide it into different domains, we use the topic labels from (Stoyanov et al., 2004). By inspecting those topics, we found that many of them can grouped to a cluster of news items discussing human rights issues mostly in the context of combating global terrorism. This means that there is little point in considering every single topic as a distinct (sub)do"
E12-1033,R11-1039,1,0.426157,"nstances of them. In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set. We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method. 2 Data As a labeled dataset we mainly use the MPQA 2.0 corpus (Wiebe et al., 2005). We adhere to the definition of opinion holders from previous work (Wiegand and Klakow, 2010; Wiegand and Klakow, 2011a; Wiegand and Klakow, 2011b), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2005) is considered an opinion holder. This corpus contains almost exclusively news texts. In order to divide it into different domains, we use the topic labels from (Stoyanov et al., 2004). By inspecting those topics, we found that many of them can grouped to a cluster of news items discussing human rights issues mostly in the context of combating global terrorism. This means that there is little point in considering every single topic as a distinct (sub)domain and, therefore, we co"
E12-1033,W11-4004,1,0.432624,"nstances of them. In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set. We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method. 2 Data As a labeled dataset we mainly use the MPQA 2.0 corpus (Wiebe et al., 2005). We adhere to the definition of opinion holders from previous work (Wiegand and Klakow, 2010; Wiegand and Klakow, 2011a; Wiegand and Klakow, 2011b), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2005) is considered an opinion holder. This corpus contains almost exclusively news texts. In order to divide it into different domains, we use the topic labels from (Stoyanov et al., 2004). By inspecting those topics, we found that many of them can grouped to a cluster of news items discussing human rights issues mostly in the context of combating global terrorism. This means that there is little point in considering every single topic as a distinct (sub)domain and, therefore, we co"
E12-1033,H05-1044,0,0.0160849,"-occur with prototypical opinion holders, i.e. common nouns such as opponents (7) or critics (8), if they are an agent of that predicate. The rationale behind this is that those nouns act very much like actual opinion holders and therefore can be seen as a proxy. (4) I always supported this idea. holder:agent. (5) This worries me. holder:patient (6) He disappointed me. holder:patient We follow Wiegand and Klakow (2011b) who found that those predicates can be best obtained by using a subset of Levin’s verb classes (Levin, 1993) and the strong subjective expressions of the Subjectivity Lexicon (Wilson et al., 2005). For those predicates it is also important to consider in which argument position they usually take an opinion holder. Bethard et al. (2004) found the 5 We also experimented with other sizes but they did not produce a better overall performance. Task-Specific Lexicon Induction (Induc) (7) Opponents say these arguments miss the point. (8) Critics argued that the proposed limits were unconstitutional. This method reduces the human effort to specifying a small set of such prototypes. Following the best configuration reported in (Wiegand and Klakow, 2011a), we extract 250 verbs, 100 nouns and 100"
E14-1071,N13-1121,0,0.0414127,"Missing"
E14-1071,W06-1642,0,0.0452757,"at individually manages to significantly outperform word is graph. The traditional features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. The task of data-driven lexicon expansion has also been explored before (Kanayama and Nasukawa, 2006; Das and Smith, 2012), however, our paper presents the first attempt to carry out a comprehensive categorization for the food domain. For the first time, we also show that type information can effectively improve the extraction of very common relations. For the twitter domain, the usage of type information based on clustering has already been found effective for supervised learning (Bergsma et al., 2013). 6 Conclusion 5 Related Work We presented an induction method to assign semantic information to food items. We considered two types of categorizations being food-type information and informat"
E14-1071,P08-1119,0,0.039423,"Missing"
E14-1071,J92-4003,0,0.0622751,"currence is co-incidental. On a subset of 200 sentences, we measured a substantial interannotation agreement of Cohen’s κ = 0.67 (Landis and Koch, 1977). We train a supervised classifier and incorporate the knowledge induced from our domain-specific corpus as features. We chose Support Vector Machines with 5-fold cross-validation using SVMlight multi-class (Joachims, 1999). Table 13 displays all features that we examine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke,"
E14-1071,P99-1016,0,0.273419,"Missing"
E14-1071,W03-0415,0,0.0841167,"Missing"
E14-1071,N12-1086,0,0.0218792,"gnificantly outperform word is graph. The traditional features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. The task of data-driven lexicon expansion has also been explored before (Kanayama and Nasukawa, 2006; Das and Smith, 2012), however, our paper presents the first attempt to carry out a comprehensive categorization for the food domain. For the first time, we also show that type information can effectively improve the extraction of very common relations. For the twitter domain, the usage of type information based on clustering has already been found effective for supervised learning (Bergsma et al., 2013). 6 Conclusion 5 Related Work We presented an induction method to assign semantic information to food items. We considered two types of categorizations being food-type information and information about whether a fo"
E14-1071,W97-0802,0,0.283746,"For that we choose a pattern-based representation that outperforms a distributionalbased representation. For initialization, we examine some manually compiled seed words and 2 Data & Annotation 2.1 Domain-Specific Text Corpus In order to generate a dataset for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012b) consisting of 418, 558 webpages of foodrelated forum entries. chefkoch.de is the largest German web portal for food-related issues. 2.2 Food Categorization As a food vocabulary, we employ a list of 1888 food items: 1104 items were directly extracted from GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990). The items were identified by extracting all hyponyms of the synset Nahrung (English: food). By 1 www.chefkoch.de 673 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Class Description Size Perc. Class MEAT BEVERAGE VEGE SWEET SPICE STARCH MILK FRUIT GRAIN FAT EGG meat and fish (products) beverages (incl. alcoholic drinks) vegetables (incl. salads) sweets, pastries and snack mixes"
E14-1071,C92-2082,0,0.240777,"ks and all configurations. It is a setting that provided reasonable results without any notable bias for any particular configuration we examine. Figure 1: Illustration of the similarity graph. 3.1.2 P √1 F − q1 Fj + µ n i=1 kFi − Yi k i,j=1 Wij δi i δj Pn 3.1.3 Manually vs. Automatically Extracted Seeds We explore two types of seed initializations: (a) a manually compiled list of seed food items and (b) a small set of patterns (Table 4) by the help of which such seeds are automatically extracted. In order to extract seeds for Task I with the pattern-based approach, we apply the patterns from Hearst (1992). These patterns have been designed for the acquisition of hyponyms. Task I can also be regarded as some type of hyponym extraction. The food types (fruit, meat, sweets) represent the hypernyms for which we extract seed hyponyms (banana, beef, chocolate). In order to extract seeds for Task II, we apply two domain-specific sets of patterns (pattdish and pattatom ). We rank the food items according to the frequency of occurring with the respective pattern set. Since food items may occur in both rankings, we merge the two rankings in the following way: j and Wii = 0. The normalization weight λ is"
E14-1071,P98-2127,0,0.474451,"Missing"
E14-1071,P13-1147,0,0.0223673,"tures are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). Task Corpus +POSTP graph Acc F1 Acc F1 Food Type Wikipedia chefkoch.de X X 40.3 65.8 49.4 71.0 61.4 80.2 59.8 77.7 Dish Wikipedia chefkoch.de X X 50.4 66.5 53.1 71.3 75.4 83.0 71.1 80.1 Table 11: Comparison of Wikipedia and domainspecific corpus as a source for the similarity graph. dientOf follow the lexical pattern food item1 with food item2 (1). Howeve"
E14-1071,W08-1006,0,0.0591793,"Missing"
E14-1071,W97-0313,0,0.715377,"Missing"
E14-1071,P10-1040,0,0.0165206,"xamine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). Task Corpus +POSTP graph Acc F1 Acc F1 Food Type Wikipedia chefkoch.de X X 40.3 65.8 49.4 71.0 61.4 80.2 59.8 77.7 Dish Wikipedia chefkoch.de X X 50.4 66.5 53.1 71.3 75.4 83.0 71.1 80.1 Table 11: Comparison of Wikipedia and domainspecific corpus as a source for the similarity graph. dientOf follow the le"
E14-1071,wiegand-etal-2012-gold,1,0.899461,"Missing"
E14-1071,D09-1097,0,0.0452431,"Missing"
E14-1071,C04-1146,0,0.0982564,"Missing"
E14-1071,C02-1114,0,0.117582,"Missing"
E14-1071,N03-1036,0,0.064969,"Missing"
E14-1071,P10-1029,0,\N,Missing
E14-1071,C98-2122,0,\N,Missing
E14-1071,S12-1012,0,\N,Missing
E14-2023,D12-1042,0,0.0530024,"5000 LE Tilburg, The Netherlands † {beroth|tbarth|mgropp|dietrich.klakow}@lsv.uni-saarland.de * g.chrupala@uvt.nl Abstract does an organization have). A perfect system would have to return all relevant information (and only this) contained in the text corpus. TAC KBP aims at giving a realistic picture of not only precision but also recall of relation extraction systems on big corpora, and is therefore an advancement over many other evaluations done for relation extraction that are often precision oriented (Suchanek et al., 2007) or restrict the gold key to answers from a fixed candidate set (Surdeanu et al., 2012) or to answers contained in a data base (Riedel et al., 2010). Similar to the classical TREC evaluation campaigns in document retrieval, TAC KBP aims at approaching a true recall estimate by pooling, i.e. merging the answers of a timed-out manual search with the answers of all participating systems. The pooled answers are then evaluated by human judges. It is a big advantage of TAC KBP that the endto-end setup (from the query, through retrieval of candidate contexts and judging whether a relation is expressed, to normalizing answers and putting them into a knowledge base) is realistic. At the"
E14-4020,P12-2011,0,0.107231,"Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et 100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100–105, c Gothenburg, Sweden, April 26-30 20"
E14-4020,J93-2004,0,0.045338,"g and Evaluation From our retrieved set of sentences, we took those with a maximum length of 10 tokens and transformed them to POS sequences. We trained DMV only on this dataset of short POS sequences, which we expect to form mentions of a modeled relation. Therefore, we suspect that DMV training assigns an increased amount of probability mass to dependency paths along structures which are truly related to these relations. We used the DMV implementation from Cohen and Smith (2009) 4 . For the supervised Nivre arc-eager parser we used MALT (Nivre et al., 2007) with a pre-trained Penn Treebank (Marcus et al., 1993) model5 . As a baseline, we tested left branching parses i.e. 3 http://www.freebase.com as of Nov. 2013 publicly available at http://www.ark.cs.cmu. edu/DAGEEM/ as of Nov. 2013 (parser version 1.0). 5 http://www.maltparser.org/mco/ english_parser/engmalt.linear-1.7.mco as of Nov. 2013 4 http://www.maltparser.org as of Nov. 2013 102 0.2 0.5 lbranch dmv surface dmv dep-graph malt surface malt dep-graph 0.18 0.4 0.14 0.12 precision micro-average KBP F1 0.16 0.1 0.08 0.3 0.2 0.06 lbranch dmv surface dmv dep-graph malt surface malt dep-graph 0.04 0.02 0.1 0 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 threshold"
E14-4020,P09-1113,0,0.149998,"Missing"
E14-4020,H05-1091,0,0.0712668,"sides requiring less resources, compares favorably in terms of extraction quality. 1 Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et 100 Proceedings of the 14th Conference of the European Chapter of the Association"
E14-4020,N09-1009,0,0.0126355,"a parser state transition, which, in conjunction with other decisions, determines where phrases begin and end. 2 Experiments 3.1 Training and Evaluation From our retrieved set of sentences, we took those with a maximum length of 10 tokens and transformed them to POS sequences. We trained DMV only on this dataset of short POS sequences, which we expect to form mentions of a modeled relation. Therefore, we suspect that DMV training assigns an increased amount of probability mass to dependency paths along structures which are truly related to these relations. We used the DMV implementation from Cohen and Smith (2009) 4 . For the supervised Nivre arc-eager parser we used MALT (Nivre et al., 2007) with a pre-trained Penn Treebank (Marcus et al., 1993) model5 . As a baseline, we tested left branching parses i.e. 3 http://www.freebase.com as of Nov. 2013 publicly available at http://www.ark.cs.cmu. edu/DAGEEM/ as of Nov. 2013 (parser version 1.0). 5 http://www.maltparser.org/mco/ english_parser/engmalt.linear-1.7.mco as of Nov. 2013 4 http://www.maltparser.org as of Nov. 2013 102 0.2 0.5 lbranch dmv surface dmv dep-graph malt surface malt dep-graph 0.18 0.4 0.14 0.12 precision micro-average KBP F1 0.16 0.1 0."
E14-4020,W03-3017,0,0.0460323,"is model. DMV is a generative head-outward parsing model which is trained by expectation maximization on part-ofspeech (POS) sequences of the input sentences. Starting from a single root token, head tokens generate dependants by a probability conditioned on the direction (left/right) from the head and the head’s token type. Each head node generates tokens until a stop event is generated with a probability dependent on the same criteria plus a flag whether some dependant token has already been generated in the same direction. For comparison of unsupervised and supervised parsing, we apply the (Nivre, 2003) deterministic incremental parsing algorithm Nivre arc-eager, the default algorithm of the MALT framework2 (Nivre et al., 2007). In this model, for each word token, an SVM classifier decides for a parser state transition, which, in conjunction with other decisions, determines where phrases begin and end. 2 Experiments 3.1 Training and Evaluation From our retrieved set of sentences, we took those with a maximum length of 10 tokens and transformed them to POS sequences. We trained DMV only on this dataset of short POS sequences, which we expect to form mentions of a modeled relation. Therefore,"
E14-4020,D09-1001,0,0.0347819,"ch patterns can capture a conjunction of token presence conditions to the left, between, and to the right of the arguments. In cases where argument entities are not parsed as a single complete phrase, we generate patterns for each possible combination of outgoing edges from the two arguments. We dismiss patterns generated for less than four distinct argument entity pairs of Related Work Unsupervised and weakly supervised training methods have been applied to relation extraction (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phrases for measuring parsing consistency. But this count is not clearly relate"
E14-4020,W09-1120,0,0.0214921,"tion (Mintz et al., 2009; Banko et al., 2007; Yates and Etzioni, 2009) and similar applications such as semantic parsing (Poon and Domingos, 2009) and paraphrase acquisition (Lin and Pantel, 2001). However, in such systems, parsing is commonly applied as a separately trained subtask1 for which supervision is used. H¨anig and Schierle (2009) have applied unsupervised parsing to a relation extraction task but their task-specific data prohibits supervised parsing for comparison. Unsupervised parsing is traditionally only evaluated intrinsically by comparison to gold-standard parses. In contrast, Reichart and Rappoport (2009) count POS token sequences inside sub-phrases for measuring parsing consistency. But this count is not clearly related to application qualities. 2 Pattern Extraction Methodology A complete relation extraction system consists of multiple components. Our system follows the architecture described by Roth et al. (2012). In short, the system retrieves queries in the form of entity names for which all relations captured by the system are to be returned. The entity names are expanded by alias-names extracted from Wikipedia link anchor texts. An information retrieval component retrieves documents cont"
E14-4020,J13-4006,0,0.0140447,"iltering retains only sentences where a named entity tagger labeled an occurrence of the queried entity as being of a suitable type and furthermore found a possible entity for the relation’s second argument. For each candidate sentence, a classifier component then identifies whether one of the captured relation types is expressed and, if so, which one it is. Postprocessing then outputs the classified relation according to task-specific format requirements. Here, we focus on the relation type classifier. 1 An exception is the joint syntactic and semantic (supervised) parsing model inference by Henderson et al. (2013) 101 DMV root Milton Friedman , a conservative economist who died in 2006 at age 94 , received the Nobel Prize for economics in 1976 . nn punct det amod nsubj rcmod prep pobj prep pobj num det dobj prep appos MALT root punct nn pobj pobj prep punct nsubj Figure 1: Comparison of a DMV (above text) and a MALT parse (below text) of the same sentence. 3 the same relation type. For each pattern, we calculate the precision on the training set and retain only patterns above a certain precision threshold. 2.2 We used the plain text documents of the English Newswire and Web Text Documents provided for"
E14-4020,P04-1061,0,0.0600454,"es instantiating 41 TAC KBP relation types. Supervised and Unsupervised Parsing Typical applications which require syntactic analyses make use of a parser that has been trained under supervision of a labeled corpus conforming to a linguistically engineered grammar. In contrast, unsupervised parsing induces a grammar from frequency structures in plain text. Various algorithms for unsupervised parsing have been developed in the past decades. Headden (2012) gives a rather recent and extensive overview of unsupervised parsing models. For our work, we use the Dependency Model with Valence (DMV) by Klein and Manning (2004). Most of the more recent unsupervised dependency parsing research is based on this model. DMV is a generative head-outward parsing model which is trained by expectation maximization on part-ofspeech (POS) sequences of the input sentences. Starting from a single root token, head tokens generate dependants by a probability conditioned on the direction (left/right) from the head and the head’s token type. Each head node generates tokens until a stop event is generated with a probability dependent on the same criteria plus a flag whether some dependant token has already been generated in the same"
E14-4020,P06-1104,0,0.0300823,"rably in terms of extraction quality. 1 Introduction Relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form. In natural language, there are infinitely many ways to creatively express a set of semantic relations in accordance to the syntax of the language. Languages vary across domains and change over time. It is therefore impossible to statically capture all ways of expressing a relation. Most relation extraction systems (Bunescu and Mooney, 2005; Snow et al., 2005; Zhang et al., 2006; Mintz et al., 2009; Alfonseca et al., 2012; Min et al., 2012) generalize semantic relations by taking into account statistics about the syntactic construction of sentences. Usually supervised parsers are applied for parsing sentences. Statistics are then utilized to machine-learn how textual mentions of relations can be identified. Many researchers avoid the need for expensive corpora with manually labeled relations by applying a scheme called distant supervision (Mintz et 100 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1"
I05-1045,J96-1002,0,0.0058907,"grees. T: BNP O: null R1: true R2: false T_ac#target t1 t2 Dallas-Fort T: NNP O: CAPALL R1: false R2: false Worth T: NNP O: CAPALL R1: false R2: false t0 t3 International T: JJ O: CAPALL R1: false R2: false t4 Airport T: NNP O: CAPALL R1: false R2: true Fig. 2. An example of the pattern tree T_ac#target 5 ME-Based Answer Extraction In addition to the syntactic relation patterns, many other evidences, such as named entity tags, may help to detect the proper answers. Therefore, we use maximum entropy to integrate the syntactic relation patterns and the common features. 5.1 Maximum Entropy Model [1] gave a good description of the core idea of maximum entropy model. In our task, we use the maximum entropy model to rank the answer candidates for a question, 514 D. Shen, G.-J.M. Kruijff, and D. Klakow which is similar to [12]. Given a question q and a set of possible answer candidates {ac1 , ac2 ...acn } , the model outputs the answer ac ∈ {ac1 , ac2 ...acn } with the maximal probability from the answer candidate set. We define M feature functions f m ( ac,{ac1 , ac2 ...acn }, q ), m=1,...,M . The probability is modeled as M P ( ac |{ac1 , ac2 ...acn }, q ) = exp[ ∑ λm f m ( ac,{ac1 , ac2 ."
I05-1045,P96-1025,0,0.0290853,"rds indicate the expected answer types, such as “party” in “Q1967: What party led …?”. 2. Head Words, which are extracted from how questions. Such words indicate the expected answer heads, such as “dog” in the “Q210: How many dogs pull …?” 3. Subject Phrases, which are extracted from all types of questions. They are the base noun phrases of the questions except the target words and the head words. 4. Verbs, which are the main verbs extracted from non-definition questions. The key words described above are identified and classified based on the question parse tree. We employ the Collins Parser [2] to parse the questions and the answer sentences. 3.2 Question Key Words Mapping From this section, we start to introduce the AE module. Firstly, the answer sentences are tagged with named entities and parsed. Secondly, the parse trees are transformed 510 D. Shen, G.-J.M. Kruijff, and D. Klakow to the dependency trees based on a set of rules. To simplify a dependency tree, some special rules are used to remove the non-useful nodes and dependency information. The rules include 1. Since the question key words are always NPs and verbs, only the syntactic relations between NP and NP / NP and verb"
I05-1045,P02-1034,0,0.0110366,"rom the data sparseness problem. So a partial matching method is required. In this section, we will propose a QAspecific tree kernel to match the patterns. A kernel function K ( x1 , x2 ) : X × X → [0, R ] , is a similarity measure between two objects x1 and x2 with some constraints. It is the most important component of kernel methods [16]. Tree kernels are the structure-driven kernels used to calculate the similarity between two trees. They have been successfully accepted in the natural language processing applications, such as parsing [4], part of speech tagging and named entity extraction [3], and information extraction [5, 17]. To our knowledge, tree kernels have not been explored in answer extraction. Suppose that a pattern is defined as a tree T with nodes {t0 , t1 , ..., t n } and each node ti is attached with a set of attributes {a0 , a1 ,..., am } , which represent the local characteristics of ti . In our task, the set of the attributes include Type attributes, Orthographic attributes and Relation Role attributes, as shown in Table 2. Figure 2 shows an example of the pattern tree T_ac#target. The core idea of the tree kernel K (T1, T2 ) is that the similarity between two tre"
I05-1045,P04-1054,0,0.045354,"So a partial matching method is required. In this section, we will propose a QAspecific tree kernel to match the patterns. A kernel function K ( x1 , x2 ) : X × X → [0, R ] , is a similarity measure between two objects x1 and x2 with some constraints. It is the most important component of kernel methods [16]. Tree kernels are the structure-driven kernels used to calculate the similarity between two trees. They have been successfully accepted in the natural language processing applications, such as parsing [4], part of speech tagging and named entity extraction [3], and information extraction [5, 17]. To our knowledge, tree kernels have not been explored in answer extraction. Suppose that a pattern is defined as a tree T with nodes {t0 , t1 , ..., t n } and each node ti is attached with a set of attributes {a0 , a1 ,..., am } , which represent the local characteristics of ti . In our task, the set of the attributes include Type attributes, Orthographic attributes and Relation Role attributes, as shown in Table 2. Figure 2 shows an example of the pattern tree T_ac#target. The core idea of the tree kernel K (T1, T2 ) is that the similarity between two trees T1 and T2 is the sum of the simil"
I05-1045,W03-1209,0,0.0691384,"CAPALL R1: false R2: true Fig. 2. An example of the pattern tree T_ac#target 5 ME-Based Answer Extraction In addition to the syntactic relation patterns, many other evidences, such as named entity tags, may help to detect the proper answers. Therefore, we use maximum entropy to integrate the syntactic relation patterns and the common features. 5.1 Maximum Entropy Model [1] gave a good description of the core idea of maximum entropy model. In our task, we use the maximum entropy model to rank the answer candidates for a question, 514 D. Shen, G.-J.M. Kruijff, and D. Klakow which is similar to [12]. Given a question q and a set of possible answer candidates {ac1 , ac2 ...acn } , the model outputs the answer ac ∈ {ac1 , ac2 ...acn } with the maximal probability from the answer candidate set. We define M feature functions f m ( ac,{ac1 , ac2 ...acn }, q ), m=1,...,M . The probability is modeled as M P ( ac |{ac1 , ac2 ...acn }, q ) = exp[ ∑ λm f m ( ac,{ac1 , ac2 ...acn }, q ))] m =1 M ∑ exp[ ∑ λm f m ( ac ',{ac1 , ac2 ...acn }, q )] ac ' m =1 where, λm (m=1,...,M) are the model parameters, which are trained with Generalized Iterative Scaling [6]. A Gaussian Prior is used to smooth the ME"
I05-1045,P02-1006,0,0.0783108,"tween answers and question words have been explored by many successful QA systems based on certain sentence representations, such as word sequence, logic form, parse tree, etc. In the simplest case, a sentence is represented as a sequence of words. It is assumed that, for certain type of questions, the proper answers always have certain surface relations with the question words. For example, “Q: When was X born?”, the proper answers often have such relation “<X&gt; ( <Answer&gt;--“ with the question phrase X . [14] first used a predefined pattern set in QA and achieved a good performance at TREC10. [13] further developed a bootstrapping method to learn the surface patterns automatically. When testing, most of them make the partial matching using regular expression. However, such surface patterns strongly depend on the word ordering and distance in the text and are too specific to the question type. LCC [9] explored the syntactic relations, such as subject, object, prepositional attachment and adjectival/adverbial adjuncts, based on the logic form transformation. Furthermore they used a logic prover to justify the answer candidates. The prover is accurate but costly. Most of the QA systems ex"
I13-1003,R11-1019,0,0.0667058,"Missing"
I13-1003,D12-1124,0,0.103539,"n this paper, we take a first step towards this endeavour. We try to identify mentions that a food item is healthy (1) or unhealthy (2). 2 Related Work In the food domain, the most prominent research addresses ontology or thesaurus alignment (van Hage et al., 2010), a task in which concepts from different sources are related to each other. In this context, hyponymy relations (van Hage et al., 2005) and part-whole relations (van Hage et al., 2006) have been explored. More recently, Wiegand et al. (2012a) examined extraction methods for relations involved in customer advice in a supermarket. In Chahuneau et al. (2012), sentiment information has been related to food prices with the help of a large corpus consisting of restaurant menus and reviews. In the health/medical domain, the majority of research focus on domain-specific relations involving entities, such as genes, proteins and (1) There is not a healthy diet without a lot of fruits, vegetables and salads. (2) The day already began unhealthy: I had a piece of cake for breakfast. This task is a pre-requisite of more complex tasks, such as finding food items that are suitable for certain groups of people with a particular health condition (3) or identify"
I13-1003,R09-1034,0,0.0174061,"ion word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detect irony. comp We check for typical inflectional word forms (i.e. healthier and healthiest) and constructions, such as as healthy as. Does the context of healthy suggest another sense of the word? sense Contexts in which healthy has a different meaning (using online dictionaries, such as www.duden.de/rechtschreibung/gesund and de.wiktionary.org/wiki/gesund). Usage of the German PolArt sentiment lexicon (Klenner et al., 2009). Number of positive/negative polar expressions (excluding mentions of healthy) Number of near synonyms of (un)healthy polar* Number of diseases disease* syno* Examples for healthy: high in vitamin, tonic, etc.; examples for unhealthy: carcinogenic, harmful, etc. (manually compiled list of 99 synonyms by an annotator not involved in feature engineering). 411 entries, created with the help of the web (bildung.wikia.com/ wiki/Alphabetische Liste der Krankheiten). Task-specific Knowledge-based Features using a Healthiness Lexicon Feature Abbrev. Illustration/Further Information Is target food ite"
I13-1003,W11-4004,1,0.787551,"100g per day; in moderation; a teaspoon of; a of 75 quantifying expressions was collected from web (rezepte.nit.at/kuechenmasse.html de.wikibooks.org/wiki/Kochbuch/ Maßangaben). steamed vegetables; fried potatoes list the and breadtarget made of whole grains; caketarget with low-fat ingredients; Complementary feature to attrNoH (feature detects specifications in the form of contact clauses or prepositional phrases immediately attached to the target food item). Some people claim that chocolate is healthy. This feature relies on a set of predicates indicating the presence of an opinion holder (Wiegand and Klakow, 2011). question Is chocolate healthy? irrealis If honey were healthy; I wonder, whether honey is healthy. Translation of the cues used in hedge classification (Morante and Daelemans, 2009). modal Honey might be healthy. negTarget No cake is healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). negHealth Chocolate is not healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detec"
I13-1003,P09-1113,0,0.00839799,"althy express this relation. This may already indicate that its extraction is difficult. 2 This is the only part of the dataset which was annotated by both annotators in parallel. www.chefkoch.de 20 Type Frequency Percentage Is-Healthy Is-Unhealthy Abbrev. HLTH UNHLTH 488 171 20.00 7.01 OTHER: No Relation Restricted Relation Unspecified Intersection Embedding Comparison Relation Unsupported Claim Other Sense Irony Question NOREL RESTR INTERS EMB COMP CLAIM SENSE IRO Q 788 312 198 157 121 87 77 25 16 32.30 12.79 8.11 6.43 4.96 3.57 3.16 1.02 0.66 considered an important cue (Zhou et al., 2005; Mintz et al., 2009). In particular, the specific type of syntactic relation needs to be considered. If in our task healthy is an attributive adjective of the target food item (16), this is not an indication of a genuine Is-Healthy relation that we are looking for. With this construction, one usually refers to all those entities that share the two properties (intersection) of being the target food item and being healthy. This case is different from both HLTH (17) and RESTR (18). Table 1: Statistics of the different (linguistic) phenomena. 4.2 (16) I usually buy the healthy fat. (17) Fat is healthy. (18) I usually"
I13-1003,wiegand-etal-2012-gold,1,0.890759,"Missing"
I13-1003,W09-1304,0,0.0229975,"teamed vegetables; fried potatoes list the and breadtarget made of whole grains; caketarget with low-fat ingredients; Complementary feature to attrNoH (feature detects specifications in the form of contact clauses or prepositional phrases immediately attached to the target food item). Some people claim that chocolate is healthy. This feature relies on a set of predicates indicating the presence of an opinion holder (Wiegand and Klakow, 2011). question Is chocolate healthy? irrealis If honey were healthy; I wonder, whether honey is healthy. Translation of the cues used in hedge classification (Morante and Daelemans, 2009). modal Honey might be healthy. negTarget No cake is healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). negHealth Chocolate is not healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detect irony. comp We check for typical inflectional word forms (i.e. healthier and healthiest) and constructions, such as as healthy as. Does the context of healthy suggest another sense"
I13-1003,H05-1044,0,0.0297792,"ects specifications in the form of contact clauses or prepositional phrases immediately attached to the target food item). Some people claim that chocolate is healthy. This feature relies on a set of predicates indicating the presence of an opinion holder (Wiegand and Klakow, 2011). question Is chocolate healthy? irrealis If honey were healthy; I wonder, whether honey is healthy. Translation of the cues used in hedge classification (Morante and Daelemans, 2009). modal Honey might be healthy. negTarget No cake is healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). negHealth Chocolate is not healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detect irony. comp We check for typical inflectional word forms (i.e. healthier and healthiest) and constructions, such as as healthy as. Does the context of healthy suggest another sense of the word? sense Contexts in which healthy has a different meaning (using online dictionaries, such as www.duden.de/rechtschreibung/gesund and de.wiktion"
I13-1003,P05-1053,0,0.0164625,"et food item and healthy express this relation. This may already indicate that its extraction is difficult. 2 This is the only part of the dataset which was annotated by both annotators in parallel. www.chefkoch.de 20 Type Frequency Percentage Is-Healthy Is-Unhealthy Abbrev. HLTH UNHLTH 488 171 20.00 7.01 OTHER: No Relation Restricted Relation Unspecified Intersection Embedding Comparison Relation Unsupported Claim Other Sense Irony Question NOREL RESTR INTERS EMB COMP CLAIM SENSE IRO Q 788 312 198 157 121 87 77 25 16 32.30 12.79 8.11 6.43 4.96 3.57 3.16 1.02 0.66 considered an important cue (Zhou et al., 2005; Mintz et al., 2009). In particular, the specific type of syntactic relation needs to be considered. If in our task healthy is an attributive adjective of the target food item (16), this is not an indication of a genuine Is-Healthy relation that we are looking for. With this construction, one usually refers to all those entities that share the two properties (intersection) of being the target food item and being healthy. This case is different from both HLTH (17) and RESTR (18). Table 1: Statistics of the different (linguistic) phenomena. 4.2 (16) I usually buy the healthy fat. (17) Fat is he"
I13-1003,W08-1006,0,0.030243,"e present. 7 Experiments In this section we present the results on automatic classification. 7.1 Classification of Individual Utterances In this subsection, we evaluate the performance of the different feature sets on sentence-level classification using supervised learning and rule-based classification. We investigate the detection of the two classes HLTH (§4.1) and UNHLTH (§4.2). Each instance to be classified is a sentence in which there is a co-occurrence of a target food item and a mention of healthy along its respective context sentences. The dataset was parsed using the Stanford Parser (Rafferty and Manning, 2008). We carry out a 5-fold cross-validation on our manually labeled dataset. As a supervised classifier, we use Support Vector Machines (SVMlight (Joachims, 1999) with a linear kernel). For each class, we train a binary classifier where positive instances represent the class to be extracted while negative instances are the remaining instances of the entire dataset (§4). (27) Chocolate is healthy as it’s high in magnesium and provides vitamin E. We use this knowledge as a baseline. If we cannot exceed the classification performance of prior (alone), then acquiring the knowledge of healthiness with"
konstantopoulos-etal-2012-task,C10-2166,1,\N,Missing
konstantopoulos-etal-2012-task,M98-1004,0,\N,Missing
konstantopoulos-etal-2012-task,M98-1012,0,\N,Missing
konstantopoulos-etal-2012-task,M98-1014,0,\N,Missing
konstantopoulos-etal-2012-task,M98-1021,0,\N,Missing
konstantopoulos-etal-2012-task,callmeier-etal-2004-deepthought,0,\N,Missing
konstantopoulos-etal-2012-task,I05-1033,0,\N,Missing
konstantopoulos-etal-2012-task,fragkou-etal-2008-boemie,1,\N,Missing
konstantopoulos-etal-2012-task,C10-1066,0,\N,Missing
konstantopoulos-etal-2012-task,C94-1070,0,\N,Missing
konstantopoulos-etal-2012-task,W09-1101,0,\N,Missing
konstantopoulos-etal-2012-task,W07-2207,0,\N,Missing
konstantopoulos-etal-2012-task,P05-1053,0,\N,Missing
konstantopoulos-etal-2012-task,P03-1054,0,\N,Missing
konstantopoulos-etal-2012-task,H05-1045,0,\N,Missing
L16-1017,W13-0507,1,0.867464,"2 dialogue act annotation scheme (ISO, 2012) serves these purposes. The ISO dialogue act annotation scheme has been already deployed in some dialogue projects, e.g. the ToMA project (Blache et al., 2009) where the Corpus of Interactional Data (CID) was labelled according ISO 24617-2, and the DBox project dialogue gaming data (Petukhova et al., 2014). Another possibility is to convert the annotations in existing corpora to annotations that are compatible with the ISO standard. For example, this approach has been applied to the SWBDDAMSL annotations in the Switchboard corpus (Fang et al., 2012; Bunt et al., 2013). The third way of achieving interoperability is to manually map various annotation schemes’ concepts to those of ISO 24617-2 (Petukhova, 2011). All described approaches lead to the creation of annotated interoperable resources which become rather expensive. The analyses reported in (Petukhova and Bunt, 2007) showed that the ratio of annotation time to real dialogue time was approximately 19:1 when coding by expert annotators. Moreover, time and resources are required to train annotators and evaluate their work. In this paper we propose a method how to reduce dialogue act annotation costs by a"
L16-1017,2007.sigdial-1.26,1,0.771465,", POS Chunks, word tokens Chunks, POS, word tokens POS POS, word tokens word tokens unigrams 0.45 0.63 0.66 0.79 0.62 0.82 0.74 bi-grams 0.71 0.75 0.68 0.84 0.58 0.79 0.81 tri-grams 0.41 0.55 0.60 0.74 0.64 0.76 0.67 Table 2: Dialogue act classification results in terms of Fscore on different feature set and with n-gram range computed(best results reported). communicative behaviour to which only one dialogue act tag is assigned, we cleaned the data by removing all segment internal hesitations and disfluencies. METALOGUE data was segmented in multiple dimensions using the approach reported in (Geertzen et al., 2007) resulting in minimal meaningful segments5 that do not contain any irrelevant material such as, for example, turn internal stallings, restarts or other flaws in speech production. 4. Classification Experiments Set Up In order to train classifiers that are able to operate on data collected in various domains, along with commonly used n-grams and bag-of-words models, we used Part-of-Speech (POS) information and shallow syntactic parsing features, and combinations of those. Linguistic features are expected to contribute to higher cross-domain portability of trained prediction models. For POS tagg"
L16-1017,W14-4340,0,0.0206052,"Missing"
L16-1017,W98-0319,0,0.492476,"Missing"
L16-1017,petukhova-etal-2014-dbox,1,0.782741,"Missing"
L16-1665,J96-4002,0,0.134381,"most frequent correspondences, and many approaches from the past few decades are based on Levensthein alignments (Levenshtein, 1966). However, such approaches suﬀer from several drawbacks. Firstly, there is a combinatorial number of possible correspondences for any given set of cognate pairs, and therefore, distinguishing useful patterns from noise in the data is hard. Secondly, the use of Levenshtein distance is not well suited to comparing languages with diﬀering alphabets. There have also been a few attempts at implementing tools to aid in correspondence discovery (Lowe and Mazaudon, 1994; Covington, 1996). However, these tools only assist in ﬁnding examples for hypothesized correspondences and do not allow to easily ﬁnd completely new ones. We are looking for correspondences at diﬀerent linguistic levels, so we require our model to be able to identify both grapheme-to-grapheme and morpheme-to-morpheme correspondences. There is a combinatory number of possible correspondences of this kind and designing a model is far from trivial. For our model, we employ the Minimum Description Length principle (MDL) (Grünwald, 2007). MDL 4203 proposes that good rules are those that succinctly describe the dat"
L16-1665,J94-3004,0,\N,Missing
L18-1120,bunt-etal-2012-iso,1,0.927227,"Missing"
L18-1120,C10-2100,0,0.0284004,"atalogue in second half of 2018. Keywords: debate argumentation, multimodal data collection, ISO standard annotations 1. Introduction We currently observe a steadily growing interest of researchers and practitioners in natural argumentation modelling and in developing argumentation technologies. There are systems developed and deployed for legal domains to assist the lawyer in his search for similar past cases, (Teufel, 1999; Br¨uninghaus and Ashley, 2005); for mining arguments in social media with the goal to predict consumers sentiment (Bai, 2011), to analyse opinions in public discussions (Murakami and Raymond, 2010), to study citizen engagement (Purpura et al., 2008) and to recognize stance in political online debates (Somasundaran and Wiebe, 2010; Walker et al., 2012a). Argumentation constitutes an important component of human intelligence. Educational studies have shown that constructing arguments and engaging in argumentative discussion enhance conceptual understanding of the subject matter (Wiley and Voss, 1999; Zohar and Nemet, 2002). Argumentation training systems are designed for the legal domains, e.g. to training hypothetical reasoning (Ashley et al., 2007). The TruthMapping1 web application fac"
L18-1120,W17-0802,0,0.0237282,"of the Centre for Argument Technology, University of Dundee includes data harvested and analysed from ArguBlogging3 , BBC Radio programmes (e.g. 1 https://www.truthmapping.com/ http://debategraph.org/ 3 http://www.argublogging.com/ 2 MM2012), Araucaria argument database (Reed, 2006). There is the Internet Argument Corpus (IAC) (Walker et al., 2012b) of political debates on internet forums, consisting of about 11,000 discussions and 390,000 posts. Subsets of the data have been annotated for topic, stance, agreement, sarcasm, and nastiness among others. The Yahoo News Annotated Comments Corpus (Napoles et al., 2017) is one of the largest annotated corpora of online human argumentative dialogues, with the most detailed set of annotations to identify argumentative, respectful exchanges containing persuasive, informative, and/or sympathetic comments. Larger projects have been used successfully as resources to study written and spoken argumentative discourse, e.g. Online Debate Forum4 , CE-EMNLP-2015, also known as IBM corpus, a selection of annotated arguments from Wikipedia articles (Rinott et al., 2015), documents of the European Court of Human Rights5 , UK Youth Parliament (UKYP)6 debates (Petukhova et a"
L18-1120,L16-1500,1,0.523814,"Missing"
L18-1120,prasad-etal-2008-penn,0,0.160994,"Missing"
L18-1120,D15-1050,0,0.0272403,"pic, stance, agreement, sarcasm, and nastiness among others. The Yahoo News Annotated Comments Corpus (Napoles et al., 2017) is one of the largest annotated corpora of online human argumentative dialogues, with the most detailed set of annotations to identify argumentative, respectful exchanges containing persuasive, informative, and/or sympathetic comments. Larger projects have been used successfully as resources to study written and spoken argumentative discourse, e.g. Online Debate Forum4 , CE-EMNLP-2015, also known as IBM corpus, a selection of annotated arguments from Wikipedia articles (Rinott et al., 2015), documents of the European Court of Human Rights5 , UK Youth Parliament (UKYP)6 debates (Petukhova et al., 2016), the American Presidency Project (APP)7 , and many more. For the application designed within the Metalogue project8 - Virtual Debate Coach - an interactive system used to train young parliamentarians to debate efficiently (Petukhova et al., 2017b), the Debate Trainees Corpus (DTC) of ‘natural’ multimodal arguments was collected. Trainees were trained to make choices from a wide range of rhetorical, lexical, syntactic, pragmatic and prosodic devices to deliver strong persuasive spee"
L18-1120,W10-0214,0,0.0432432,"We currently observe a steadily growing interest of researchers and practitioners in natural argumentation modelling and in developing argumentation technologies. There are systems developed and deployed for legal domains to assist the lawyer in his search for similar past cases, (Teufel, 1999; Br¨uninghaus and Ashley, 2005); for mining arguments in social media with the goal to predict consumers sentiment (Bai, 2011), to analyse opinions in public discussions (Murakami and Raymond, 2010), to study citizen engagement (Purpura et al., 2008) and to recognize stance in political online debates (Somasundaran and Wiebe, 2010; Walker et al., 2012a). Argumentation constitutes an important component of human intelligence. Educational studies have shown that constructing arguments and engaging in argumentative discussion enhance conceptual understanding of the subject matter (Wiley and Voss, 1999; Zohar and Nemet, 2002). Argumentation training systems are designed for the legal domains, e.g. to training hypothetical reasoning (Ashley et al., 2007). The TruthMapping1 web application facilitates collaborative learning through argumentation. DebateGraph2 used to train how to prevent opinion manipulation marking inconsis"
L18-1120,walker-etal-2012-corpus,0,0.0205778,"ly growing interest of researchers and practitioners in natural argumentation modelling and in developing argumentation technologies. There are systems developed and deployed for legal domains to assist the lawyer in his search for similar past cases, (Teufel, 1999; Br¨uninghaus and Ashley, 2005); for mining arguments in social media with the goal to predict consumers sentiment (Bai, 2011), to analyse opinions in public discussions (Murakami and Raymond, 2010), to study citizen engagement (Purpura et al., 2008) and to recognize stance in political online debates (Somasundaran and Wiebe, 2010; Walker et al., 2012a). Argumentation constitutes an important component of human intelligence. Educational studies have shown that constructing arguments and engaging in argumentative discussion enhance conceptual understanding of the subject matter (Wiley and Voss, 1999; Zohar and Nemet, 2002). Argumentation training systems are designed for the legal domains, e.g. to training hypothetical reasoning (Ashley et al., 2007). The TruthMapping1 web application facilitates collaborative learning through argumentation. DebateGraph2 used to train how to prevent opinion manipulation marking inconsistent arguments. These"
N10-1046,J92-4003,0,0.0957293,"s For class-based unigrams, P (Q|S) is computed using only the cluster labels of the query terms as Y P (Q|S) = P (qi |Cqi , S)P (Cqi |S), (2) i=1...M where Cqi is the cluster that contains qi and P (qi |Cqi , S) is the emission probability of the ith query term given its cluster and the sentence. P (Cqi |S) is analogous to the sentence model P (qi |S) in (1), but is based on clusters instead of terms. To calculate P (Cqi |S), each cluster is considered an atomic entity, with Q and S interpreted as sequences of such entities. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002). The algorithm requires an input corpus statistics in the form hw, w0 , fww0 i, where fww0 is the number of times the word w0 is seen in the context w. Both w and w0 are assumed to come from a common vocabulary. Beginning with each vocabulary item in a separate cluster, a bottom-up approach is used to merge the pair of clusters that minimizes the loss in Average Mutual Information (AMI) between the word cluster Cw0 and its context cluster Cw . Different words seen in the same contexts are good candidates for merger, as are different context"
N10-1046,J90-1003,0,0.417354,"not consider whole documents, and only concerns itself with the number of times that two words occur in the same sentence. Co-occurrence in a Window of Text The window-wise co-occurrence statistic is an even narrower notion of context, considering only terms in a window surrounding w0 . Specifically, a window of a fixed size is moved along the text, and fww0 is set as the number of times both w and w0 appear in the window. Since the window size is a free parameter, different sizes may be applied. In our experiments we use two window sizes, 2 and 5, that have been studied in related research (Church and Hanks, 1990). Co-occurrence in a Syntactic Relationship Another notion of word similarity derives from having the same syntactic relationship with the context w. This syntax-wise co-occurrence statistic is similar to the sentence-wise co-occurrence, in that co-occurrence is defined at the sentence level. However, in contrast to the sentence-wise model, w and w0 are said to co-occur only if there is a syntactic relation between them in that sentence. E.g., this type of co-occurrence can help cluster nouns that are used as objects of same verb, such as ‘tea’, ‘water’, and ‘cola,’ which all are used with the"
N10-1046,kaisser-lowe-2008-creating,0,0.0252002,"4,084,473 12,343,947 46,307,650 14,093,661 Google n-grams Google n-grams window-5 window-2 12,005,479 328,431,792 Table 1: Statistics for different notions of co-occurrence. Derivatives of the TREC QA Data Sets The set of questions from the TREC 2006 QA track1 was used as the test data to evaluate our models, while the TREC 2005 set was used for development. The TREC 2006 QA task contains 75 questionseries, each on one topic, for a total of 403 factoid questions which is used as queries for sentence retrieval. For sentence-level relevance judgments, the Question Answer Sentence Pair corpus of Kaisser and Lowe (2008) was used. All the documents that contain relevant sentences are from the NIST AQUAINT1 corpus. QA systems typically employ sentence retrieval after initial, high quality document retrieval. To simulate this, we created a separate search collection for each question using all sentences from all documents relevant to the topic (question-series) from which the question was derived. On average, there are 17 relevant documents per topic, many not relevant to the question itself: they may be relevant to another question. So the sentence search collection is realistic, even if somewhat optimistic. 4"
N10-1121,H05-1045,0,0.808081,"ion of boundaries of the structures for the convolution kernels is less straightforward in opinion holder extraction. The aim of this paper is to explore in how far convolution kernels can be beneficial for effective opinion holder detection. We are not only interested in how far different kernel types contribute to this extraction task but we also contrast the performance of these kernels with a manually designed feature set used as a standard vector kernel. Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic"
N10-1121,W06-1651,0,0.378973,"rmation and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder label"
N10-1121,P05-1045,0,0.0302596,"ith the SVM-Light-TK toolkit7 . We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are marked with CAN D . up nouns in NOMLEX (Macleod et al., 1998). 5.1 Each Notation kernel"
N10-1121,W06-0301,0,0.176676,"syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In ou"
N10-1121,kingsbury-palmer-2002-treebank,0,0.104533,"G in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMW RD and GRAMP OS , can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (P AS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which training data instances ~x are separated by a hyperplane H(~x) = w ~ · ~x + b = 0 where w ∈ Rn and b ∈ R. One advantage of SVMs is that kernel methods can be applied which map the data to other feature spaces in which they can be separated more easily. Given a feature function φ : O → R, where O is the set of the objects, the kernel trick allows the decision hyperplane to be rewritten as: ! X yi αi ~xi · ~x + b = H(~x) = i=1...l X i=1...l yi"
N10-1121,P03-1054,0,0.00360156,"is of a paired t-test using 0.05 as the significance level. All experiments were done with the SVM-Light-TK toolkit7 . We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are mark"
N10-1121,J08-2003,0,0.0658684,"tion. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang 796 et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic infor"
N10-1121,D09-1143,0,0.285997,"e about [Y]?”). Such systems need to be able to distinguish which entities in a candidate answer sentence are the sources of opinions (= opinion holder) and which are the targets. On other NLP tasks, in particular, on relation extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues"
N10-1121,H05-1044,0,0.0969332,"omial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are marked with CAN D . up nouns in NOMLEX (Macleod et al., 1998). 5.1 Each Notation kernel is represented as a triple hlevelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel (Table 2)i, e.g. hCON ST, SEN T, ST Ki is a Subset Tree Kernel of a constituency parse having"
N10-1121,N06-1037,0,0.239088,"extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues need not be member of a particular word group, e.g. they can be opinion words (see Sentences 1-3), communication words, such as maintained in Sentence 2, or other lexical cues, such as according in Sentence 3. 1. The U.S. co"
N10-1121,W08-2126,0,0.0340879,"matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are marked with CAN D . up nouns in NOMLEX (Macleod et al., 1998). 5.1 Each Notation kernel is represented as a triple hlevelOfRepresentation (Table 1), Scope (Tabl"
N10-1121,P02-1034,0,\N,Missing
N13-1059,P05-1045,0,0.00752314,"ive) (10) [Evil witches are stereotypically dressed in black] and [good fairies in white]. We also experimented with other related weaklysupervised extraction methods, such as mutual information of two adjectives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards"
N13-1059,P97-1023,0,0.533226,"ves from the nonsubjective adjectives. In this work, we are interested in an out-ofcontext assessment of adjectives and therefore evaluate them with the help of sentiment lexicons. We examine the property of being a predicative adjective as an extraction criterion. Predicative adjectives are adjectives that do not modify the head of a noun (3) (4) (5) (6) Her idea was brilliant. This is a financial problem. She came up with a brilliant idea. ?The problem is financial. 2 Related Work The extraction of subjective adjectives has already attracted some considerable attention in previous research. Hatzivassiloglou and McKeown (1997) extract polar adjectives by a weakly supervised method in which subjective adjectives are found by searching for adjectives that are conjuncts of a pre-defined set of polar seed adjectives. Wiebe (2000) induces subjective adjectives with the help of distributional similarity. Hatzivassiloglou and Wiebe (2000) examine the properties of dynamic, gradable and polar adjectives as a means to detect subjectivity. Vegnaduzzo (2004) presents another bootstrapping method of extracting subjective adjectives with the help of head nouns of the subjective candidates and distributional similarity. Baroni a"
N13-1059,C00-1044,0,0.53182,"ot modify the head of a noun (3) (4) (5) (6) Her idea was brilliant. This is a financial problem. She came up with a brilliant idea. ?The problem is financial. 2 Related Work The extraction of subjective adjectives has already attracted some considerable attention in previous research. Hatzivassiloglou and McKeown (1997) extract polar adjectives by a weakly supervised method in which subjective adjectives are found by searching for adjectives that are conjuncts of a pre-defined set of polar seed adjectives. Wiebe (2000) induces subjective adjectives with the help of distributional similarity. Hatzivassiloglou and Wiebe (2000) examine the properties of dynamic, gradable and polar adjectives as a means to detect subjectivity. Vegnaduzzo (2004) presents another bootstrapping method of extracting subjective adjectives with the help of head nouns of the subjective candidates and distributional similarity. Baroni and Vegnaduzzo 534 Proceedings of NAACL-HLT 2013, pages 534–539, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics (2004) employ Web-based Mutual information for this task and largely outperform the results produced by Vegnaduzzo (2004). 3 Method In the following, we present dif"
N13-1059,J11-2001,0,0.0323326,"ctives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards a particular gold standard (which would challenge the general validity of the proposed method). ALL PRD other new last many first such next political federal own several few good∗ former same economic public"
N13-1059,J04-3002,0,0.0545981,"xtract subjective adjectives. We do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives, i.e. another highly subjective subset of adjectives that can be extracted in an unsupervised fashion. In order to prove the robustness of this extraction method, we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons (as a gold standard). 1 Introduction Since the early work on sentiment analysis, it has been established that the part of speech with the highest proportion of subjective words are adjectives (Wiebe et al., 2004) (see Sentence (1)). However, not all adjectives are subjective (2). (1) A grumpy guest made some impolite remarks to the insecure and inexperienced waitress. (2) The old man wearing a yellow pullover sat on a plastic chair. This justifies the exploration of criteria to automatically separate the subjective adjectives from the nonsubjective adjectives. In this work, we are interested in an out-ofcontext assessment of adjectives and therefore evaluate them with the help of sentiment lexicons. We examine the property of being a predicative adjective as an extraction criterion. Predicative adject"
N13-1059,H05-1044,0,0.0764966,"such as mutual information of two adjectives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards a particular gold standard (which would challenge the general validity of the proposed method). ALL PRD other new last many first such next political federal own sever"
N19-2023,Y17-1038,0,0.0285788,"higher layers, without transferring intermediate layers though. In Yang et al. (2017) it is suggested to transfer only the character embeddings and the character RNN weights between languages. The reason for this is likely that many languages written in the Latin alphabet have a large charset overlap, but far less vocabulary overlap. Another question of interest concerns the pair of languages between which TL can be achieved. Past work has shown transferring to a related language to help more than to an unrelated one for NER, POS tagging, and NMT (Zirikly and Hagiwara, 2015; Kim et al., 2017; Dabre et al., 2017). In Yang et al. (2017) it is mentioned that without additional resources, it is “very difficult for transfer learning between languages with disparate alphabets”. This background suggests TL from En183 glish to Japanese to be non-trivial. Finally, another consideration with TL is the size of the target dataset. For one NER task, TL gains were shown to decrease to nearly zero as the size of the target training data increased to around 50k tokens (Lee et al., 2017). Similarly, for domain adaptation, a “phase transition” was observed in the amount of used target data, such that using source data"
N19-2023,W16-2706,0,0.0203837,"get, use over two hundred distinct tags, which are not evenly distributed. In fact, Figure 2 (in log-log scale) shows a very long tail, with the most frequently observed tags belonging to a very small subset of all possible tags. This characteristic makes the internal data a challenging case. External data As external English data, we use the English CoNLL 2003 NER dataset (Tjong Kim Sang and De Meulder, 2003) which contains four named entity (NE) categories. We make use of three external datasets for Japanese NER. The first one is the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Iwakura et al., 2016), containing a variety of writing types, such as blogs and magazine articles. In addition, we created a dataset by combining two small Japanese datasets annotated with NEs: i) a small dataset included in the CRF++ tool, and ii) the Kyoto University and NTT Blog Corpus (KNBC) with data from blogs on topics such as tourism, sports, and technology. We are referring to this dataset as “CRF-KNBC”. Most Japanese NER datasets use IREX tags. Similar to CoNLL 2003, IREX 1999 was a shared task for NER and contains eight tags, three of which are the same as in CoNLL. The remaining tags can be viewed as a"
N19-2023,D17-1302,0,0.0193331,"mbining lower and higher layers, without transferring intermediate layers though. In Yang et al. (2017) it is suggested to transfer only the character embeddings and the character RNN weights between languages. The reason for this is likely that many languages written in the Latin alphabet have a large charset overlap, but far less vocabulary overlap. Another question of interest concerns the pair of languages between which TL can be achieved. Past work has shown transferring to a related language to help more than to an unrelated one for NER, POS tagging, and NMT (Zirikly and Hagiwara, 2015; Kim et al., 2017; Dabre et al., 2017). In Yang et al. (2017) it is mentioned that without additional resources, it is “very difficult for transfer learning between languages with disparate alphabets”. This background suggests TL from En183 glish to Japanese to be non-trivial. Finally, another consideration with TL is the size of the target dataset. For one NER task, TL gains were shown to decrease to nearly zero as the size of the target training data increased to around 50k tokens (Lee et al., 2017). Similarly, for domain adaptation, a “phase transition” was observed in the amount of used target data, such t"
N19-2023,N16-1030,0,0.0489713,"hand (Collobert et al., 2011). Currently, mainly recurrent and/or convolutional neural networks are applied. In Chiu and Nichols (2015), the authors combined a Bi-LSTM to learn long-distance relationships with a CNN to generate character-level representations. A Bi-LSTM-CNN-CRF showed state-of-the-art performance on NER (Ma and Hovy, 2016). CNNs have been shown to be less useful for languages like Japanese, in which average NEs are quite short at around two characters on average (Misawa et al., 2017). Bi-LSTMCRF models without any CNN layer have also performed well on NER (Huang et al., 2015; Lample et al., 2016). Using this architecture with a novel type of embeddings termed “contextual string embeddings” has recently led to state-of-the-art results (Akbik et al., 2018). For our baseline NER system we use a BiLSTM architecture that takes word and character embeddings as input. The same architecture is used both for the source and the target languages to allow for transfer of weights when the crosslingual TL is applied. This architecture largely resembles the model in Lample et al. (2016), except for the final CRF layer. For every token, word and character embeddings are generated. The latter are pass"
N19-2023,C18-1139,0,0.0178307,"-LSTM to learn long-distance relationships with a CNN to generate character-level representations. A Bi-LSTM-CNN-CRF showed state-of-the-art performance on NER (Ma and Hovy, 2016). CNNs have been shown to be less useful for languages like Japanese, in which average NEs are quite short at around two characters on average (Misawa et al., 2017). Bi-LSTMCRF models without any CNN layer have also performed well on NER (Huang et al., 2015; Lample et al., 2016). Using this architecture with a novel type of embeddings termed “contextual string embeddings” has recently led to state-of-the-art results (Akbik et al., 2018). For our baseline NER system we use a BiLSTM architecture that takes word and character embeddings as input. The same architecture is used both for the source and the target languages to allow for transfer of weights when the crosslingual TL is applied. This architecture largely resembles the model in Lample et al. (2016), except for the final CRF layer. For every token, word and character embeddings are generated. The latter are passed through a character Bi-LSTM, the output of which is concatenated with the word embeddings. This combined representation is then passed into the word Bi-LSTM,"
N19-2023,P16-1101,0,0.0267102,"L-HLT 2019, pages 182–189 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2 NER model The growth in neural approaches spurred a push towards “NLP from scratch”, that is, without engineering task- or language-specific features by hand (Collobert et al., 2011). Currently, mainly recurrent and/or convolutional neural networks are applied. In Chiu and Nichols (2015), the authors combined a Bi-LSTM to learn long-distance relationships with a CNN to generate character-level representations. A Bi-LSTM-CNN-CRF showed state-of-the-art performance on NER (Ma and Hovy, 2016). CNNs have been shown to be less useful for languages like Japanese, in which average NEs are quite short at around two characters on average (Misawa et al., 2017). Bi-LSTMCRF models without any CNN layer have also performed well on NER (Huang et al., 2015; Lample et al., 2016). Using this architecture with a novel type of embeddings termed “contextual string embeddings” has recently led to state-of-the-art results (Akbik et al., 2018). For our baseline NER system we use a BiLSTM architecture that takes word and character embeddings as input. The same architecture is used both for the source"
N19-2023,W13-3520,0,0.0294606,"ights (compare rows “Word” and “Char”). In addition, combining the weights at word or character levels with the next dense layer weights improves further the results (rows “Word+Dense” and “Char+Dense”) indicating that this dense layer still captures some language-independent information. Due to these results, we use the “Char+Dense” combination in the following experiments. feature that strongly predisposes a word to be an NE, we did not lowercase the character-level input. No pre-trained word embeddings were used with internal datasets, while external datasets used Polyglot word embeddings (Al-Rfou et al., 2013). The word embedding dimensionality was 50, except where Polyglot pre-trained embeddings were used, in which case it was 64. The word LSTM size was set to 300. Character embeddings were 50-dimensional and character bigrams were used. The character LSTM was of size 100 for external datasets and 30 for internal ones. Dropout was set to 0.5. We used the evaluation script from the CoNLL shared task to compute F1 score. During the parameter tuning phase, development set performance stabilized after 10 epochs for external models and 20 epochs for internal models. Therefore, we conduct experiments on"
N19-2023,W17-4114,0,0.0606212,"hes spurred a push towards “NLP from scratch”, that is, without engineering task- or language-specific features by hand (Collobert et al., 2011). Currently, mainly recurrent and/or convolutional neural networks are applied. In Chiu and Nichols (2015), the authors combined a Bi-LSTM to learn long-distance relationships with a CNN to generate character-level representations. A Bi-LSTM-CNN-CRF showed state-of-the-art performance on NER (Ma and Hovy, 2016). CNNs have been shown to be less useful for languages like Japanese, in which average NEs are quite short at around two characters on average (Misawa et al., 2017). Bi-LSTMCRF models without any CNN layer have also performed well on NER (Huang et al., 2015; Lample et al., 2016). Using this architecture with a novel type of embeddings termed “contextual string embeddings” has recently led to state-of-the-art results (Akbik et al., 2018). For our baseline NER system we use a BiLSTM architecture that takes word and character embeddings as input. The same architecture is used both for the source and the target languages to allow for transfer of weights when the crosslingual TL is applied. This architecture largely resembles the model in Lample et al. (2016)"
N19-2023,P18-2020,0,0.0360932,"Missing"
N19-2023,W03-0419,0,0.669014,"Missing"
N19-2023,C00-2137,0,0.0405182,"t set by training for these respective number of epochs. The scores reported for each model reflect the highest F1 value among all epochs. 5 5.1 Results 5.2 Layer combinations for TL We first investigate which layer combination yields best results when being transferred. The layer groups defined in Section 3.3 are combined and experiments are conducted on the two external JP datasets as well as on a subset of the JP “Medium” internal one. The results are presented in Table 4 as absolute gains against the baseline without TL. Approximate randomization is used for each experiment (Noreen, 1989; Yeh, 2000), and all TL gains were found to be significant to p<0.001. The results reported are the average of running experiments five times with different random seeds. In all experiments, the system configuration detailed in Section 4.2 is followed and the Effect of romanization of Japanese on TL The effect of romanization of Japanese is evaluated on one external (“BCCWJ”) and a subset of an internal (“Med.-10k”) JP dataset. Results are presented in Table 5 with and without romanization before and after TL, and consistent gains are shown when MOM is used with TL. In addition, there are significant gai"
N19-2023,P15-2064,0,0.0255015,"ce our source model in English is not character-based. Figure 1: NER model: an English example 3 Transfer Learning Cross-lingual TL is applied to transfer knowledge from the source to the target language. Working with neural network-based models, this is achieved by initializing some layers of the target network using the weights of the source network, which is assumed to be already trained using a (large) available annotated training corpus. 3.1 Related work One of the first works on cross-lingual TL for NER that did not rely on parallel corpora used a CRF and included hand-crafted features (Zirikly and Hagiwara, 2015). Currently, most work on TL is done with neural models. Because neural models often consist of multiple layers, one important design decision is which layers to transfer from source to target. Much related work involves only transferring a single layer or specific combination of layers. In Lee et al. (2017) the authors present more thorough results combining lower and higher layers, without transferring intermediate layers though. In Yang et al. (2017) it is suggested to transfer only the character embeddings and the character RNN weights between languages. The reason for this is likely that"
N19-3005,W13-3520,0,0.0135632,"liminate some of the negative effects of the noisy data and to leverage the additional data effectively. Combining MTL with NLNN results in small improvements at best and can decrease perforImplementation Details During training, we minimize the cross entropy loss which sums over the entire sentence. The networks are trained with Stochastic Gradient Descent (SGD). To determine the number of iterations for both the NN model and the EM algorithm we use the development data. All models are trained with word embeddings of dimensionality 64 that are initialized with pre-trained Polygot embeddings (Al-Rfou et al., 2013). We add Dropout (Srivastava et al., 2014) with p=0.1 in between the word embedding layer and the LSTM. 6 575 4 I-LOC 83 766 I-PER C 10 1 I-MIS 49 7 I-ORG 3. Iteration Results In Figure 2, we present the F1 scores of the models introduced in the previous section. We perform experiments on Chunking and NER with various amounts of added, automatically-labeled data. In general, adding additional, noisy data tends to improve the performance for all mod32 9 mance, especially on Chunking. The best results are achieved with our combined MTL+CNLNN model as it outperforms all other models. Even when ad"
N19-3005,W03-0407,0,0.253006,"ion techniques are an option to obtain labels for this raw data, but they often require additional external resources like humangenerated lexica which might not be available in a low-resource context. Self-training is a popular technique to automatically label additional text. There, a classifier is trained on a small amount of labeled data and then used to obtain labels for 2 Related Work Self-training has been applied to various NLP tasks, e.g. Steedman et al. (2003) and Sagae and Tsujii (2007). While McClosky et al. (2006) are able to leverage self-training for parsing, Charniak (1997) and Clark et al. (2003) obtain only minimal improvements at best on parsing and POS-tagging § This work was started while the authors were at Saarland University. 29 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 29–34 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics respectively. In some cases, the results even deteriorate. Other successful approaches of automatically labeling data include using a different classifier trained on out-of-domain data (Petrov et al., 2010) or le"
N19-3005,D07-1111,0,0.0211515,"unlabeled data, on the other hand, is usually available even in these scenarios. Automatic annotation or distant supervision techniques are an option to obtain labels for this raw data, but they often require additional external resources like humangenerated lexica which might not be available in a low-resource context. Self-training is a popular technique to automatically label additional text. There, a classifier is trained on a small amount of labeled data and then used to obtain labels for 2 Related Work Self-training has been applied to various NLP tasks, e.g. Steedman et al. (2003) and Sagae and Tsujii (2007). While McClosky et al. (2006) are able to leverage self-training for parsing, Charniak (1997) and Clark et al. (2003) obtain only minimal improvements at best on parsing and POS-tagging § This work was started while the authors were at Saarland University. 29 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 29–34 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics respectively. In some cases, the results even deteriorate. Other successful approaches of auto"
N19-3005,W18-3402,1,0.397987,"ts even deteriorate. Other successful approaches of automatically labeling data include using a different classifier trained on out-of-domain data (Petrov et al., 2010) or leveraging external knowledge (Dembowski et al., 2017). A detailed review of learning in the presence of noisy labels is given in (Fr´enay and Verleysen, 2014). Recently, several approaches have been proposed for modeling the noise using a confusion matrix in a neural network context. Many works assume that all the data is noisy-labeled (Bekker and Goldberger, 2016; Goldberger and Ben-Reuven, 2017; Sukhbaatar et al., 2015). Hedderich and Klakow (2018) and Hendrycks et al. (2018) propose a setting where a mix of clean and unlabeled data is used. However, they require external knowledge sources for labeling the data or evaluate on synthetic noise. Alternatively, instances with incorrect labels might be filtered out, e.g. in the work by Guan et al. (2011) or Han et al. (2018), but this involves the risk of also filtering out difficult but correct instances. Another orthogonal approach is the use of noise-robust loss functions (Zhang and Sabuncu, 2018). 3 Figure 1: A representation of NLNN (left) compared to our proposed CNLNN model. The compl"
N19-3005,W00-0726,0,0.203449,"Missing"
N19-3005,P16-2038,0,0.0208608,"xplicit noise handling of NLNN and CNLNN, we also apply MTL for implicit noise handling. Here, we use NN as the base architecture and POS-tagging as an auxiliary task. We hypothesise that this low-level task helps the model to generalise its representation and that the POS-tags are helpful because e.g. many named entities are proper nouns. The auxiliary task is trained jointly with the first LSTM layer of NN for Chunking and with the second LSTM layer for NER. In our low-resource setting, we use the first 10k tokens of section 0 of Penn Treebank for the auxiliary POS-tagging task for the MTL (Søgaard and Goldberg, 2016). This data is disjunct from the other datasets. Additionally, we combine both the explicit and implicit noise handling. In the low-resource setting, in general, such a combination addresses the data scarcity better than the individual models. NLNN and CNLNN combinations with MTL are labeled as MTL+NLNN and MTL+CNLNN respectively. 2 7 5 0 6 3 211 28 5 540 28 5 I-LOC 87 745 0 I-PER 6 14 6 20 C 61 8 151 16 19 I-MIS 110 91 47 71 72 I-ORG R 4 0 6 12 7153 R 9 4 7 6 7149 OTHE OTHE I-LOC I-PER I-MISCI-ORGOTHER I-LOC I-PER I-MISCI-ORGOTHER Predicted Label 43 265 27 Figure 3: NLNN confusion matrices on"
N19-3005,P17-1040,0,0.148713,"Missing"
N19-3005,E03-1008,0,0.132947,"labeled data exist. Raw or unlabeled data, on the other hand, is usually available even in these scenarios. Automatic annotation or distant supervision techniques are an option to obtain labels for this raw data, but they often require additional external resources like humangenerated lexica which might not be available in a low-resource context. Self-training is a popular technique to automatically label additional text. There, a classifier is trained on a small amount of labeled data and then used to obtain labels for 2 Related Work Self-training has been applied to various NLP tasks, e.g. Steedman et al. (2003) and Sagae and Tsujii (2007). While McClosky et al. (2006) are able to leverage self-training for parsing, Charniak (1997) and Clark et al. (2003) obtain only minimal improvements at best on parsing and POS-tagging § This work was started while the authors were at Saarland University. 29 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 29–34 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics respectively. In some cases, the results even deteriorate. Other s"
N19-3005,J93-2004,0,0.0648724,"Missing"
N19-3005,N06-1020,0,0.103027,"and, is usually available even in these scenarios. Automatic annotation or distant supervision techniques are an option to obtain labels for this raw data, but they often require additional external resources like humangenerated lexica which might not be available in a low-resource context. Self-training is a popular technique to automatically label additional text. There, a classifier is trained on a small amount of labeled data and then used to obtain labels for 2 Related Work Self-training has been applied to various NLP tasks, e.g. Steedman et al. (2003) and Sagae and Tsujii (2007). While McClosky et al. (2006) are able to leverage self-training for parsing, Charniak (1997) and Clark et al. (2003) obtain only minimal improvements at best on parsing and POS-tagging § This work was started while the authors were at Saarland University. 29 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 29–34 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics respectively. In some cases, the results even deteriorate. Other successful approaches of automatically labeling data includ"
N19-3005,D10-1069,0,0.0170577,"7) and Clark et al. (2003) obtain only minimal improvements at best on parsing and POS-tagging § This work was started while the authors were at Saarland University. 29 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 29–34 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics respectively. In some cases, the results even deteriorate. Other successful approaches of automatically labeling data include using a different classifier trained on out-of-domain data (Petrov et al., 2010) or leveraging external knowledge (Dembowski et al., 2017). A detailed review of learning in the presence of noisy labels is given in (Fr´enay and Verleysen, 2014). Recently, several approaches have been proposed for modeling the noise using a confusion matrix in a neural network context. Many works assume that all the data is noisy-labeled (Bekker and Goldberger, 2016; Goldberger and Ben-Reuven, 2017; Sukhbaatar et al., 2015). Hedderich and Klakow (2018) and Hendrycks et al. (2018) propose a setting where a mix of clean and unlabeled data is used. However, they require external knowledge sour"
P06-1112,J96-1002,0,0.00209848,"..., hi } and a set of modifiers M = {m1 , ...mj }. Some heuristic rules are applied to judge heads and modifiers: 1. If BNP is a named entity, all words are heads. 2. The last word of BNP is head. 3. Rest words are modifiers. The similarity between two BNPs Sim(BN Pq , BN Ps ) is defined as: According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate"
P06-1112,C02-1167,0,0.0186118,"hi = hj after format alternation; • Sim = SemSim(hi , hj ) These items consider morphological, format and semantic variations respectively. 1. The morphological variations match words after stemming, such as ”Rhodes scholars” and ”Rhodes scholarships”. 2. The format alternations cope with special characters, such as ”-” for ”Ice-T” and ”Ice T”, ”&” for ”Abercrombie and Fitch” and ”Abercrombie & Fitch”. 3. The semantic similarity SemSim(hi , hj ) is measured using WordNet and eXtended WordNet. We use the same semantic path finding algorithm, relation weights and semantic similarity measure as (Moldovan and Novischi, 2002). For efficiency, only hypernym, hyponym and entailment relations are considered and search depth is set to 2 in our experiments. Particularly, the semantic variations are not considered for NE heads and modifiers. Modifier similarity Sim(mi , mj ) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v1 , v2 ) is the same as head similarity measure Sim(hi , hj ). 893 Then, We manually check the sentences and remove those in which answers cannot be supported. As to build candidate sentence sets for testing, we retrieve all of the sentences from relevant d"
P06-1112,W03-1209,0,0.0256282,"fiers: 1. If BNP is a named entity, all words are heads. 2. The last word of BNP is head. 3. Rest words are modifiers. The similarity between two BNPs Sim(BN Pq , BN Ps ) is defined as: According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate answer sets. Orthographic features, similar to (Shen et al., 2005), are extracted to capture word format informati"
P06-1112,I05-1045,1,0.935049,"udge heads and modifiers: 1. If BNP is a named entity, all words are heads. 2. The last word of BNP is head. 3. Rest words are modifiers. The similarity between two BNPs Sim(BN Pq , BN Ps ) is defined as: According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate answer sets. Orthographic features, similar to (Shen et al., 2005), are extracted to c"
P06-1112,C94-1079,0,\N,Missing
P16-2029,W02-0603,0,0.396808,"the method on seven morphologically rich languages from the Babel (Harper, 2013) corpus and compare to the previously suggested approaches. 2 Suggested method We present a combination of unsupervised morph segmentation and statistical language models for unsupervised vocabulary expansion. The suggested approach operates in four steps: unsupervised morph segmentation, statistical language model training, sampling of new word types and reranking of the sampled words. The phases are described in more detail in the corresponding subsections. 2.1 Unsupervised morph segmentation Morfessor Baseline (Creutz and Lagus, 2002) is a method for unsupervised morphological segmentation. The algorithm optimizes a two-part minimum description length code, finding a balance between the cost of encoding the training corpus and the lexicon, as in Formula 1. arg min L(x, θ) = arg min L(x|θ) + L(θ) (1) θ The corpus encoding is based on a unigram model. A so-called α-term may be used for finetuning the corpus encoding cost. For the experiments in this work, a recent Python implementation Morfessor 2.0 (Smit et al., 2014) was used. 2.2 Statistical language models over morphs As statistical language models, two state-of-theart m"
P16-2029,C14-1111,0,0.0376772,"Missing"
P16-2029,P14-1127,0,0.131821,"hybrid word-subword language modelling for OOV word detection (Yazgan and Sarac¸lar, 2004). Speech recognition by directly using optimized subword units has also (Kneissler and Klakow, 2001) proven a good approach for speech recognition of a morphologically rich language. In this work, we study unsupervised vocabulary expansion for conversational speech recognition of morphologically rich languages in a lessresourced setting. We expand the recognition vocabulary, and thus lower the OOV rate, by generating new word forms. Two recent works also target the unsupervised vocabulary expansion. In (Rasooli et al., 2014), an unsupervised morphological segmentation was inferred from the training corpus using the Morfessor CategoriesMAP (Creutz and Lagus, 2007) method. The prefix-stem-suffix structure estimated by the model was then represented as a finite-statetransducer for sampling new word forms. Different reranking schemes using a bigram language model and a letter trigraph language model were evaluated. The Kaldi speech recognition package (Povey et al., 2011) includes an approach (Trmal et al., 2014) for vocabulary expansion. In this approach, the provided syllable segmented pronunciation lexicon is used"
P16-2029,E14-2006,0,0.0375364,"Missing"
petukhova-etal-2014-dbox,C02-1150,0,\N,Missing
petukhova-etal-2014-dbox,bunt-etal-2012-using,1,\N,Missing
R11-1039,H05-1045,0,0.746629,"different ways of harnessing mentions of protoOHs for OH extraction. We compare their usage as labeled training data for supervised learning with a rule-based classifier that relies on a lexicon of predictive predicates that have been extracted from the contexts of protoOHs. Moreover, we investigate in how far the knowledge gained from these contexts can compensate the lack of large amounts of actually labeled training data in supervised classification by considering various amounts of labeled training sets. 2 Related Work There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling com"
R11-1039,C10-1059,0,0.173728,"earch on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007), for example, use a list of manually compiled communication verbs. 1 Introduction Building an opinion holder (OH) extraction system on the basis of supervised classifiers requires large amounts of labeled training data which are expensive to obtain. Therefore, alternative methods requiring less human effort are required. Such methods would be particularly valuable for languages other than English as for most other"
R11-1039,W06-0301,0,0.210514,"from the contexts of protoOHs. Moreover, we investigate in how far the knowledge gained from these contexts can compensate the lack of large amounts of actually labeled training data in supervised classification by considering various amounts of labeled training sets. 2 Related Work There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007), for example, use a list of manually compiled communication verbs. 1 Introduction Building an opinio"
R11-1039,D09-1012,0,0.0255372,"s, and adjectives). That is, we rank every predicate according to its correlation, i.e. we use Pointwise Mutual Information, of having agentive protoOHs as an argument. The highly ranked predicates are used as predictive cues. The resulting rule-based classifier always classifies an NP as an OH if its head is an agent of a highly ranked discriminative predicate (as illustrated in the right half of Table 2). The supervised kernel-based classifier from §4.1 learns from a rich set of features. In a previous study on reverse engineering making implicit features within convolution kernels visible (Pighin and Moschitti, 2009), it has been shown that the learnt features are usually fairly small subtrees. There are plenty of structures which just contain one or two leaf nodes, i.e. sparse lexical information, coupled with some further structural nodes from the parse tree. These structures are fairly similar to low-level features, such as bag of words or bag of ngrams, in the sense that they are weak predictors and that there are plenty of them. For such types of features, it has been shown in both subjectivity detection (Lambov et al., 2009) and polarity classification (Andreevskaia and Bergler, 2008) that they gene"
R11-1039,P08-1034,0,0.0822727,"n kernels visible (Pighin and Moschitti, 2009), it has been shown that the learnt features are usually fairly small subtrees. There are plenty of structures which just contain one or two leaf nodes, i.e. sparse lexical information, coupled with some further structural nodes from the parse tree. These structures are fairly similar to low-level features, such as bag of words or bag of ngrams, in the sense that they are weak predictors and that there are plenty of them. For such types of features, it has been shown in both subjectivity detection (Lambov et al., 2009) and polarity classification (Andreevskaia and Bergler, 2008) that they generalize poorly across different domains. On the other hand, very few high-level features describing the presence of certain semantic classes or opinion words perform consistently well across different domains. These features can either be incorporated within a supervised learner (Lambov et al., 2009) or a lexicon-based rule-based classifier (Andreevskaia and Bergler, 2008). We assume that our rule-based classifier 4.2.1 Self-training A shortcoming of the rule-based classifier is that it incorporates no (or hardly any) domain knowledge. In other related sentiment classification ta"
R11-1039,P10-1040,0,0.145188,"ce, if doubt is such a predicate, we would replace the subtree [V BP doubt] by [V BP [P REDOH doubt]]. Moreover, we devise a simple vector kernel incorporating the prediction of the rule-based classifier. All kernels are combined by plain summation. domain-specific features the supervised classifier may learn could be useful prior weights towards some of these domain-specific NPs as to whether they might be an OH or not. 4.2.2 Generalization with Clustering and Knowledge Basis We also examine in how far the coverage of the discriminant predicates can be increased with the usage of clustering. Turian et al. (2010) have shown that in semi-supervised learning for namedentity recognition, i.e. a task which bears some resemblance to the present task, features referring to the clusters corresponding to groups of specific words with similar properties (induced in an unsupervised manner) help to improve performance. In the context of our rule-based classifier, we augment the set of discriminant predicates by all words which are also contained in the cluster associated with these discriminant predicates. Hopefully, due to the strong similarity among the words within the same cluster, the additional words will"
R11-1039,N10-1121,1,0.291454,"d training data in supervised classification by considering various amounts of labeled training sets. 2 Related Work There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007), for example, use a list of manually compiled communication verbs. 1 Introduction Building an opinion holder (OH) extraction system on the basis of supervised classifiers requires large amounts of labeled training data which are expensive to obtain. Therefore, alternative"
R11-1039,H05-1044,0,0.0692487,"o-)supervised classifier does not perform as well as our best rule-based classifier (induced by protoOHs). The fact that, in addition to that, our proposed method also largely outperforms the rule-based classifier relying on both AL and SL when no heuristics are used and is still slightly better when they are incorporated supports the effectiveness of our method. quality of the extracted words, we mark the words which can also be found in task-specific resources, i.e. communication verbs from the Appraisal Lexicon (AL) (Bloom et al., 2007) and opinion words from the Subjectivity Lexicon (SL) (Wilson et al., 2005). Both resources have been found predictive for OH extraction (Bloom et al., 2007; Wiegand and Klakow, 2010). Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in §4."
R11-1039,J92-4003,0,0.434474,"01 44.35 54.42 48.87 39.14 62.71 48.20 44.38 59.61 50.88 Table 5: Performance of extended rule-based classifiers. the entire set of protoOHs. The performance of the different subsets is very similar (i.e. 46.44, 46.28, and 46.17), so we may conclude that the configuration that we proposed, namely to consider all protoOHs, is more or less the optimal configuration for this method. 5.2.2 Self-training and Generalization Table 5 shows the performance of our method when extended by either self-training (SelfTr) or generalization. For generalization by clustering (Clus), we chose Brown clustering (Brown et al., 1992) which is the best performing algorithm in (Turian et al., 2010). The clusters are induced on our unlabeled corpus (see §3). We induced 1000 clusters (optimal size). For the knowledge-based generalization (WN), we used synonyms from WordNet 3. For both Clus and WN, we display the results extending only the most highly ranked V100+N50+A50 since it provided notably better results than extending all predicates, i.e. V250+N100+A100 (our baseline). The table shows that only self-training consistently improves the results. The impact of generalization is less advantageous since by increasing recall"
R11-1039,W08-2126,0,0.0237466,"discriminant predicates. Unlike our extraction phase for OH extraction in which only the correlation between predicates and protoOHs are considered (Table 2), we may find additional predicates as the clustering is induced from completely unrestricted text. The extension of discriminant predicates can also be done by taking into account manually built general-purpose lexical resources, such as WordNet.3 One simply adds the entire set of synonyms of each of the predicates. 4.3 5 Experiments The documents were parsed using the Stanford Parser.4 Semantic roles were obtained by using the parser by Zhang et al. (2008). 5.1 Supervised Learning All experiments using convolution kernels were done with the SVM-Light-TK toolkit.5 We test two versions of the supervised classifier. The first considers any mention of a protoOH as an OH, while the second is restricted to only those mentions of a protoOH which are an agent of some predicate. We also experimented with different amounts of (pseudo-)labeled training data from our unlabeled corpus varying from 12500 to 150000 instances. We found that from 25000 instances onwards the classifier does not notably improve when further training data are added. The results of"
R11-1039,E06-1015,0,\N,Missing
R19-1094,W06-1108,0,0.0801886,"contain duplicates. Further, we 1 ∅ plays an important role when computing alignments. We will also refer to it as nothing 811 Levenshtein distance (LD) (Levenshtein, 1966) is, it its basic implementation, a symmetric similarity measure between two strings – in our case words – wi ∈ L1 and wj ∈ L2 . Levenshtein distance quantifies the number of operations one has to perform in order to transform wi into wj . Levenshtein distance allows to measure the orthographic distance between two words and has been successfully used in previous works for measuring the linguistic distance between dialects (Heeringa et al., 2006) as well as the phonetic distance between Scandinavian language varieties (Gooskens, 2007). When computing Levenshtein distance between two words LD(wi , wj ), three different character transformations are considered: character deletion, character insertion, and character substitution. In the following we use T = {insert, delete, substitute} to denote the set of possible transformations. A cost c(t) is assigned to each transformation t ∈ T and setting c(t) = 1 ∀t ∈ T results in the most simple implementation. incom.py allows computing LD(wi , wj ) based on a user-defined cost matrix M, which c"
R19-1154,S13-2108,0,0.076308,"Missing"
R19-1154,D13-1079,0,0.0173016,"nd drug names and other medical or clinical entities (Settles, 2004; Shen et al., 2003). Biomedical named entity recognition is a key step in biomedical language processing. Early (BM-)NER approaches were largely rulebased detecting entities based on the observed contextual and orthographic patterns. Such systems are especially useful if no or little training examples are available (Sekine and Nobata, 2004), are often straightforward to implement, suited for the entity classes or domains where the regularities in orthography or morphology can be exploited, and have other important advantages (Chiticariu et al., 2013). Although they achieve a rather high precision, recall is often low as the rule sets are rarely exhaustive. AbGene system of Tanabe and Wilbur (2002) uses a POS tagger extended to include gene and protein names as tag types. The system was trained on the manually labelled biomedical text. In its second iteration, it applies manually defined post-processing rules. Another successful approach underlies the socalled dictionary-based systems. Here, the decision whether an entity is of an interest is made by matching against the entries in a dictionary, i.e. gazetteer or word list. To expand the c"
R19-1154,C96-1079,0,0.227215,"al design of a medical domain. Section 4 defines the medical term extraction task, assesses various resources for medical information extraction and presents the overall QA system architecture. Section 5 proposes the experimental design by specifying the collected and simulated data, and discusses the obtained results. Finally, Section 6 summarizes our findings and outlines directions for the future research and development. 2 Biomedical Named Entity Recognition In 1995, the 6th Message Understanding Conference (MUC-6) focused on the Information Extraction (IE) from unstructured textual data (Grishman and Sundheim, 1996) and defined the Named Entity Recognition and Classification (NERC) task, see (Nadeau and Sekine, 2007) for a comprehensive overview. The relevant entities comprised names of persons, organizations and locations defined as E NAMEX (Entity Name Expression), extended later with T IMEX (Time Expressions) and N UMEX (Numerical Expressions). In the early 2000s, the interest in bioinformatics lead to enriching the categories with concepts from biomedical domains focusing on the recognition of biological and genetic terms, disease and drug names and other medical or clinical entities (Settles, 2004;"
R19-1154,P09-1113,0,0.0115472,"(Bj¨orne et al., 2013), Conditional Random Fields (Settles, 2004) and Neural Networks (Sahu and Anand, 2017) are reported to show the state-of-the-art performance. These approaches rely on large amounts of the annotated training data. To perform BM-NER some resources are created: the NCBI Disease Corpus (Do˘gan et al., 2014), the GENIA corpus (Kim et al., 2003) for molecular biology, the i2b21 corpus of clinical notes. The data for languages other than English is still an issue. Techniques which allow to automatically generate labelled training data like bootstrapping and distant supervision (Mintz et al., 2009) methods are proposed to build models in semi-supervised or weakly supervised way. For example, Dembowski et al. (2017) extract word lists from Wikipedia to label the data for an NER model training. The trained classifier outperforms the simple dictionary baseline. Unsupervised approaches do not require any labelled training data, but rely on external resources like knowledge-bases or semantic nets (Alfonseca and Manandhar, 2002), lexical patterns (Evans and Street, 2003), and distributional semantics. Zhang and Elhadad (2013) applied a distributional semantics method to clinical notes and bio"
R19-1154,W12-4304,0,0.0270614,"encounters, Patient Education Forms (PEFs) need to be filled in and the patient’s informed consent signed. It is of chief importance that the forms are properly understood, medical procedures and risks are explained. PEFs contain many medical terms including those in Latin and as abbreviations. These terms have to be detected and corresponding definitions retrieved from available electronic medical documents. Although a number of biomedical entities recognition systems (Zhang and Elhadad, 2013; Bj¨orne et al., 2013; Sahu and Anand, 2017) and medical resources exist (Gurulingappa et al., 2010; Ohta et al., 2012), they are mostly built for English. We explore a language-agnostic approach to medical concepts mining based on the existing 1346 Proceedings of Recent Advances in Natural Language Processing, pages 1346–1355, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_154 de-facto standard terminologies and dictionaries, and the collection of the corresponding German seed terms in a distant supervision setting. The extracted concepts and terms are used to search and retrieve definitions from the verified online free text sources. The proof-of-concept definition retrieval syste"
R19-1154,sekine-nobata-2004-definition,0,0.0228723,"merical Expressions). In the early 2000s, the interest in bioinformatics lead to enriching the categories with concepts from biomedical domains focusing on the recognition of biological and genetic terms, disease and drug names and other medical or clinical entities (Settles, 2004; Shen et al., 2003). Biomedical named entity recognition is a key step in biomedical language processing. Early (BM-)NER approaches were largely rulebased detecting entities based on the observed contextual and orthographic patterns. Such systems are especially useful if no or little training examples are available (Sekine and Nobata, 2004), are often straightforward to implement, suited for the entity classes or domains where the regularities in orthography or morphology can be exploited, and have other important advantages (Chiticariu et al., 2013). Although they achieve a rather high precision, recall is often low as the rule sets are rarely exhaustive. AbGene system of Tanabe and Wilbur (2002) uses a POS tagger extended to include gene and protein names as tag types. The system was trained on the manually labelled biomedical text. In its second iteration, it applies manually defined post-processing rules. Another successful"
R19-1154,J88-4003,0,0.208826,"ured data to retrieve an answer in a supervised or rule-based way. 3 Conceptual Domain Modelling To facilitate an accurate information extraction from and reasoning with large amounts of (un)structured data, it is important to specify and model real world entities and relations between them. This knowledge is often represented as ontologies, terminologies with semantic concepts groupings and taxonomic relations between them, and semantic networks. In many knowledge-based QA systems, high level semantic representations are used to query databases or other types of structured data. For example, Wilensky et al. (1988) developed the Berkeley Unix Consultant, for the domain related to the UNIX operating system where questions are analysed and transformed into an internal representation which are used to generate hypothesis about the user’s information needs. A knowledge-based QA system as used by Apple Siri2 and Wolfram Alpha3 also first builds a query representation and then maps it to structured data like ontologies, gazeteers, etc. The Watson a DeepQA system of IBM Research (Ferrucci et al., 2010) incorporates content acquisition, question analysis, hypothesis generation, etc. Inside the hypotheses genera"
R19-1154,P02-1060,0,0.127194,"nst the entries in a dictionary, i.e. gazetteer or word list. To expand the coverage, linguistic methods (e.g. stemming or lemmatization), as well as fuzzy or exact matching strategies are used. cTakes of Savova et al. (2010) is an opensource information extraction tool from Electronic Health Records (EHR) which NER component is based on a dictionary look-up approach. Dictionaries are also used supplementary to machine learning approaches (Tsuruoka and Tsujii, 2003), which are particularly useful if there is a high variability in entities observed. Supervised models like Hidden Markov Models (Zhou and Su, 2002), Support Vector Machines (Bj¨orne et al., 2013), Conditional Random Fields (Settles, 2004) and Neural Networks (Sahu and Anand, 2017) are reported to show the state-of-the-art performance. These approaches rely on large amounts of the annotated training data. To perform BM-NER some resources are created: the NCBI Disease Corpus (Do˘gan et al., 2014), the GENIA corpus (Kim et al., 2003) for molecular biology, the i2b21 corpus of clinical notes. The data for languages other than English is still an issue. Techniques which allow to automatically generate labelled training data like bootstrapping"
R19-1154,W04-1221,0,0.528809,"Sundheim, 1996) and defined the Named Entity Recognition and Classification (NERC) task, see (Nadeau and Sekine, 2007) for a comprehensive overview. The relevant entities comprised names of persons, organizations and locations defined as E NAMEX (Entity Name Expression), extended later with T IMEX (Time Expressions) and N UMEX (Numerical Expressions). In the early 2000s, the interest in bioinformatics lead to enriching the categories with concepts from biomedical domains focusing on the recognition of biological and genetic terms, disease and drug names and other medical or clinical entities (Settles, 2004; Shen et al., 2003). Biomedical named entity recognition is a key step in biomedical language processing. Early (BM-)NER approaches were largely rulebased detecting entities based on the observed contextual and orthographic patterns. Such systems are especially useful if no or little training examples are available (Sekine and Nobata, 2004), are often straightforward to implement, suited for the entity classes or domains where the regularities in orthography or morphology can be exploited, and have other important advantages (Chiticariu et al., 2013). Although they achieve a rather high preci"
R19-1154,W03-1307,0,0.123647,"and defined the Named Entity Recognition and Classification (NERC) task, see (Nadeau and Sekine, 2007) for a comprehensive overview. The relevant entities comprised names of persons, organizations and locations defined as E NAMEX (Entity Name Expression), extended later with T IMEX (Time Expressions) and N UMEX (Numerical Expressions). In the early 2000s, the interest in bioinformatics lead to enriching the categories with concepts from biomedical domains focusing on the recognition of biological and genetic terms, disease and drug names and other medical or clinical entities (Settles, 2004; Shen et al., 2003). Biomedical named entity recognition is a key step in biomedical language processing. Early (BM-)NER approaches were largely rulebased detecting entities based on the observed contextual and orthographic patterns. Such systems are especially useful if no or little training examples are available (Sekine and Nobata, 2004), are often straightforward to implement, suited for the entity classes or domains where the regularities in orthography or morphology can be exploited, and have other important advantages (Chiticariu et al., 2013). Although they achieve a rather high precision, recall is ofte"
R19-1154,W03-1306,0,0.135793,"Another successful approach underlies the socalled dictionary-based systems. Here, the decision whether an entity is of an interest is made by matching against the entries in a dictionary, i.e. gazetteer or word list. To expand the coverage, linguistic methods (e.g. stemming or lemmatization), as well as fuzzy or exact matching strategies are used. cTakes of Savova et al. (2010) is an opensource information extraction tool from Electronic Health Records (EHR) which NER component is based on a dictionary look-up approach. Dictionaries are also used supplementary to machine learning approaches (Tsuruoka and Tsujii, 2003), which are particularly useful if there is a high variability in entities observed. Supervised models like Hidden Markov Models (Zhou and Su, 2002), Support Vector Machines (Bj¨orne et al., 2013), Conditional Random Fields (Settles, 2004) and Neural Networks (Sahu and Anand, 2017) are reported to show the state-of-the-art performance. These approaches rely on large amounts of the annotated training data. To perform BM-NER some resources are created: the NCBI Disease Corpus (Do˘gan et al., 2014), the GENIA corpus (Kim et al., 2003) for molecular biology, the i2b21 corpus of clinical notes. The"
W05-0409,P96-1025,0,0.0145839,"Missing"
W05-0409,P02-1034,0,0.0336653,"ot only have the same syntactic tag sequences but also have the same linking symbols. For example, for the node sequences NP↑VP↑VP ↑S↓ NP and NP↑ NP↑VP↓ NP , there is a matched substring (k = 2): NP ↑ VP. 6.3 Tree Kernel The third method keeps the original representation of the syntactic relation in the parse tree and incorporates a tree kernel in SVM. Tree kernels are the structure-driven kernels to calculate the similarity between two trees. They have been successfully accepted in the NLP applications. (Collins and Duffy, 2002) defined a kernel on parse tree and used it to improve parsing. (Collins, 2002) extended the approach to POS tagging and named entity recognition. (Zelenko et al., 2003; Culotta and Sorensen, 2004) further explored tree kernels for relation extraction. We define an object (a relation tree) as the smallest tree which covers one answer candidate node and one question key word node. Suppose that a relation tree T has nodes {t0 , t1 , ..., t n } and each node ti is attached with a set of attribFigure 1: An example of the path from the answer candidate node to the question subject word node utes {a0 , a1 , ..., am } , which represents the local char(Haussler, 1999) first desc"
W05-0409,P04-1054,0,0.043125,"e node sequences NP↑VP↑VP ↑S↓ NP and NP↑ NP↑VP↓ NP , there is a matched substring (k = 2): NP ↑ VP. 6.3 Tree Kernel The third method keeps the original representation of the syntactic relation in the parse tree and incorporates a tree kernel in SVM. Tree kernels are the structure-driven kernels to calculate the similarity between two trees. They have been successfully accepted in the NLP applications. (Collins and Duffy, 2002) defined a kernel on parse tree and used it to improve parsing. (Collins, 2002) extended the approach to POS tagging and named entity recognition. (Zelenko et al., 2003; Culotta and Sorensen, 2004) further explored tree kernels for relation extraction. We define an object (a relation tree) as the smallest tree which covers one answer candidate node and one question key word node. Suppose that a relation tree T has nodes {t0 , t1 , ..., t n } and each node ti is attached with a set of attribFigure 1: An example of the path from the answer candidate node to the question subject word node utes {a0 , a1 , ..., am } , which represents the local char(Haussler, 1999) first described a convolution kernel over the strings. (Lodhi et al., 2000) applied the string kernel to the text classification"
W05-0409,P03-1003,0,0.0280658,"machine learning framework, it is crucial to capture the useful evidences for the task and integrate them effectively in the model. Many researchers have explored the rich textual features for the answer extraction. IBM (Ittycheriah and Roukos, 2002; Ittycheriah, 2001) used a Maximum Entropy model to integrate the rich features, including query expansion features, focus matching features, answer candidate co-occurrence features, certain word frequency features, named entity features, dependency relation features, linguistic motivated features and surface patterns. ISI’s (Echihabi et al. 2003; Echihabi and Marcu, 2003) statistical-based AE module implemented a noisy-channel model to explain how a given sentence tagged with an answer can be rewritten into a question through a sequence of sto66 chastic operations. (Ravichandran et al., 2003) compared two maximum entropy-based QA systems, which view the AE as a classification problem and a re-ranking problem respectively, based on the word frequency features, expected answer class features, question word absent features and word match features. BBN (Xu et al. 2002) used a HMM-based IR system to score the answer candidates based on the answer contexts. They fur"
W05-0409,W03-1209,0,0.354089,", databases, are incorporated. The various techniques and resources may provide the indicative evidences to find the correct answers. These evidences are further combined by using a pipeline structure, a scoring function or a machine learning method. In the machine learning framework, it is critical but not trivial to generate the features from the various resources which may be represented as surface texts, syntactic structures and logic forms, etc. The complexity of feature generation strongly depends on the complexity of data representation. Many previous QA systems (Echihabi et al., 2003; Ravichandran, et al., 2003; Ittycheriah and Roukos, 2002; Ittycheriah, 2001; Xu et al., 2002) have well studied the features in the surface texts. In this paper, we will use the answer extraction module of QA as a case study to further explore how to generate the features for the more complex sentence representations, such as parse tree. Since parsing gives the deeper understanding of the sentence, the features generated from the parse tree are expected to improve the performance based on the features generated from the surface text. The answer ex65 Proceedings of the ACL Workshop on Feature Engineering for Machine Lea"
W05-0409,C02-1119,0,0.0302059,"ystems, which view the AE as a classification problem and a re-ranking problem respectively, based on the word frequency features, expected answer class features, question word absent features and word match features. BBN (Xu et al. 2002) used a HMM-based IR system to score the answer candidates based on the answer contexts. They further re-ranked the scored answer candidates using the constraint features, such as whether a numerical answer quantifies the correct noun, whether the answer is of the correct location sub-type and whether the answer satisfies the verb arguments of the questions. (Suzuki et al. 2002) explored the answer extraction using SVM. However, in the previous statistical-based AE modules, most of the features were extracted from the surface texts which are mainly based on the key words/phrases matching and the key word frequency statistics. These features only capture the surface-based information for the proper answers and may not provide the deeper understanding of the sentences. In addition, the contribution of the individual feature has not been evaluated by them. As for the features extracted from the structured texts, such as parse trees, only a few works explored some predef"
W09-4628,P04-1034,0,0.017029,"s assigned the polarity derived from the average of the po1 2 http://www.rateitall.com http://www.wjh.harvard.edu/∼inquirer larity scores of the words occurring within the document. The most recent semi-automatic lexicon is SentiWordNet (Esuli and Sebastiani, 2006) which assigns polarity to word senses in WordNet3 known as synsets. The polarity of manually annotated seed synsets is expanded onto the remaining synsets of the WordNet ontology by measuring the overlap between their respective glosses. The only works dealing with semi-supervised learning on this classification task we know of are Beineke et al. (2004) who combine Turney’s web mining approach with evidence from labeled training data, and Aue and Gamon (2005) who focus on domain adaptation. Neither different algorithms nor feature sets are compared in these works. In this paper, we look into adjectives & adverbs as features in detail. Pang et al. (2002) use feature sets exclusively comprising adjectives for supervised polarity classification but report performance to be worse than a standard bag-of-words representation. However, Ng et al. (2006) increase performance significantly by adding to a standard feature set higher order n-grams in wh"
W09-4628,P06-2079,0,0.247456,"rity lexicons are an alternative option, however, they are expensive to create and their individual effectiveness may vary across different domains. We show that a small list of frequently occurring adjectives & adverbs cheaply extracted from an unlabeled in-domain dataset usually has competitive performance. We consider polarity classification as a binary classification problem. That is, we assume that each document to be classified is subjective. We neglect the distinction between objective and subjective content since this classification is usually solved independently (Pang and Lee, 2004; Ng et al., 2006). Besides Ng et al. (2006) report that document-level subjectivity detection is a rather easy task compared to (binary) document-level poPredictive Features in Semi-Supervised Learning for Polarity Classification larity classification. In our experiments, we primarily use the standard dataset from Pang et al. (2002) comprising movie reviews. To substantiate that our insights carry over to other domains, we also use a multidomain dataset we created from Rate-It-All1 . To the best of our knowledge, this is the first time that several semi-supervised classifiers are evaluated on this learning tas"
W09-4628,P04-1035,0,0.0129909,"classification. Polarity lexicons are an alternative option, however, they are expensive to create and their individual effectiveness may vary across different domains. We show that a small list of frequently occurring adjectives & adverbs cheaply extracted from an unlabeled in-domain dataset usually has competitive performance. We consider polarity classification as a binary classification problem. That is, we assume that each document to be classified is subjective. We neglect the distinction between objective and subjective content since this classification is usually solved independently (Pang and Lee, 2004; Ng et al., 2006). Besides Ng et al. (2006) report that document-level subjectivity detection is a rather easy task compared to (binary) document-level poPredictive Features in Semi-Supervised Learning for Polarity Classification larity classification. In our experiments, we primarily use the standard dataset from Pang et al. (2002) comprising movie reviews. To substantiate that our insights carry over to other domains, we also use a multidomain dataset we created from Rate-It-All1 . To the best of our knowledge, this is the first time that several semi-supervised classifiers are evaluated on"
W09-4628,W02-1011,0,0.0358933,"onsider polarity classification as a binary classification problem. That is, we assume that each document to be classified is subjective. We neglect the distinction between objective and subjective content since this classification is usually solved independently (Pang and Lee, 2004; Ng et al., 2006). Besides Ng et al. (2006) report that document-level subjectivity detection is a rather easy task compared to (binary) document-level poPredictive Features in Semi-Supervised Learning for Polarity Classification larity classification. In our experiments, we primarily use the standard dataset from Pang et al. (2002) comprising movie reviews. To substantiate that our insights carry over to other domains, we also use a multidomain dataset we created from Rate-It-All1 . To the best of our knowledge, this is the first time that several semi-supervised classifiers are evaluated on this learning task in depth, in particular, in combination with various feature sets. 2 Related Work Fully supervised polarity classification has been extensively explored. Both discriminative methods, such as support vector machines (SVMs), and generative methods have been applied (Pang et al., 2002; Salvetti et al., 2006). Discrim"
W09-4628,P07-1056,0,0.0378195,"lassifiers, whether polarity lexicons improve performance, and whether adjectives and adverbs produce classifiers competitive to average polarity lexicons. We do not attempt to carry out detailed domain studies which would be beyond the scope of this section. We chose four domains from the list of Topic Categories of the website which we thought are very different from the movie domain and for which we could extract sufficient training data. We took Computer & Internet (computer), Products (products), Sports & Recreation (sports) and Travel, Food, & Culture (travel). We follow the method from Blitzer et al. (2007) to infer the polarity of the reviews. Ratings with less than 3 stars are considered negative reviews whereas ratings with more than 3 stars are positive reviews. We decided not to consider mixed reviews, i.e. reviews rated with 3 stars. In general, we found far fewer mixed reviews15 . On those domains which provided a reasonable amount of data, our initial supervised learning experiments showed that mixed polarity can only be poorly distinguished from definite polarity16 . Manual inspection of a random sample of reviews also showed that a great part of these documents are actually negative re"
W09-4628,P02-1053,0,0.00407775,"ually, the gain in performance hardly justifies the computational overhead of these methods (Gamon, 2004). There are several domain-independent polarity lexicons containing important polar expressions. The most prominent manual lexicons are General Inquirer2 , the subjectivity lexicon from the MPQA-project (Wilson et al., 2005), and Appraisal Groups (Whitelaw et al., 2005). They have been successfully applied to polarity classification (Kennedy and Inkpen, 2005; Wilson et al., 2005; Whitelaw et al., 2005). Moreover, several methods have been proposed to automatically induce polarity lexicons. Turney (2002) applies Pointwise Mutual Information in order to find similar words to a given list of polar seed words on web data. The polarity scores which are thus computed for each word can be used for a completely unsupervised classification algorithm of documents. A document is assigned the polarity derived from the average of the po1 2 http://www.rateitall.com http://www.wjh.harvard.edu/∼inquirer larity scores of the words occurring within the document. The most recent semi-automatic lexicon is SentiWordNet (Esuli and Sebastiani, 2006) which assigns polarity to word senses in WordNet3 known as synset"
W09-4628,esuli-sebastiani-2006-sentiwordnet,0,0.0216837,", several methods have been proposed to automatically induce polarity lexicons. Turney (2002) applies Pointwise Mutual Information in order to find similar words to a given list of polar seed words on web data. The polarity scores which are thus computed for each word can be used for a completely unsupervised classification algorithm of documents. A document is assigned the polarity derived from the average of the po1 2 http://www.rateitall.com http://www.wjh.harvard.edu/∼inquirer larity scores of the words occurring within the document. The most recent semi-automatic lexicon is SentiWordNet (Esuli and Sebastiani, 2006) which assigns polarity to word senses in WordNet3 known as synsets. The polarity of manually annotated seed synsets is expanded onto the remaining synsets of the WordNet ontology by measuring the overlap between their respective glosses. The only works dealing with semi-supervised learning on this classification task we know of are Beineke et al. (2004) who combine Turney’s web mining approach with evidence from labeled training data, and Aue and Gamon (2005) who focus on domain adaptation. Neither different algorithms nor feature sets are compared in these works. In this paper, we look into"
W09-4628,C04-1121,0,0.0308108,"s been extensively explored. Both discriminative methods, such as support vector machines (SVMs), and generative methods have been applied (Pang et al., 2002; Salvetti et al., 2006). Discriminative methods usually perform significantly better. If sufficient labeled data are available, supervised classifiers offer a reasonable performance even without dedicated feature selection. Various linguistic features, such as part-of-speech information, syntactic dependency information and semantic relations have been shown to increase performance of standard bag-of-words feature sets, (Ng et al., 2006; Gamon, 2004). However, Ng et al. (2006) report that the same improvement can be obtained by using higher order n-grams. We omit advanced linguistic features in this work, since, usually, the gain in performance hardly justifies the computational overhead of these methods (Gamon, 2004). There are several domain-independent polarity lexicons containing important polar expressions. The most prominent manual lexicons are General Inquirer2 , the subjectivity lexicon from the MPQA-project (Wilson et al., 2005), and Appraisal Groups (Whitelaw et al., 2005). They have been successfully applied to polarity classif"
W09-4628,H05-1044,0,0.235367,"tic relations have been shown to increase performance of standard bag-of-words feature sets, (Ng et al., 2006; Gamon, 2004). However, Ng et al. (2006) report that the same improvement can be obtained by using higher order n-grams. We omit advanced linguistic features in this work, since, usually, the gain in performance hardly justifies the computational overhead of these methods (Gamon, 2004). There are several domain-independent polarity lexicons containing important polar expressions. The most prominent manual lexicons are General Inquirer2 , the subjectivity lexicon from the MPQA-project (Wilson et al., 2005), and Appraisal Groups (Whitelaw et al., 2005). They have been successfully applied to polarity classification (Kennedy and Inkpen, 2005; Wilson et al., 2005; Whitelaw et al., 2005). Moreover, several methods have been proposed to automatically induce polarity lexicons. Turney (2002) applies Pointwise Mutual Information in order to find similar words to a given list of polar seed words on web data. The polarity scores which are thus computed for each word can be used for a completely unsupervised classification algorithm of documents. A document is assigned the polarity derived from the averag"
W10-3111,D09-1020,0,0.0238775,"inguistic phenomenon. In this section, we present the limits of negation modeling in sentiment analysis. Earlier in this paper, we stated that negation modeling depends on the knowledge of polar expressions. However, the recognition of genuine polar expressions is still fairly brittle. Many polar expressions, such as disease are ambiguous, i.e. they have a polar meaning in one context (Sentence 12) but do not have one in another (Sentence 13). 12. He is a disease to every team he has gone to. 13. Early symptoms of the disease are headaches, fevers, cold chills and body pain. In a pilot study (Akkaya et al., 2009), it has already been shown that applying subjectivity word sense disambiguation in addition to the featurebased negation modeling approach of Wilson et al. (2005) results in an improvement of performance in polarity classification. Another problem is that some polar opinions are not lexicalized. Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge. 67 14. The next time I hear this song on the radio, I’ll throw my radio out of the window. Moreover, the effectiveness of specific negation models can only b"
W10-3111,D08-1083,0,0.59404,"a more abstract level of representation being verb frames. The advantage of a more abstract level of representation is that it more accurately represents the meaning of the text it describes. Apart from that, Shaikh et al. (2007) design a model for sentence-level classification rather than for headlines or complex noun phrases. The approach by Moilanen and Pulman (2007) is not compared against another established classification method whereas the approach by Shaikh et al. (2007) is evaluated against a non-compositional rule-based system which it outperforms. 3.3.2 Shallow Semantic Composition Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. Their working assumption is that the polarity of a phrase can be computed in two steps: An example rule, such as: P olarity([NP1]− [IN] [NP2]− ) = + (3) may be applied to expressions, such as − [lack]− NP1 [of]IN [crime]NP2 in rural areas. The advantage of these rules is that they restrict the scope of negation to specific constituents rather than using the scope of the entire target expression. Such inference rules are very reminiscent of polarity modification features (Wilson e"
W10-3111,R09-1034,0,0.0161424,"outperforms the (plain) rule-based method. 3.3.3 Scope Modeling In sentiment analysis, the most prominent work examining the impact of different scope models for negation is (Jia et al., 2009). The scope detection method that is proposed considers: • static delimiters • dynamic delimiters • heuristic rules focused on polar expressions Static delimiters are unambiguous words, such as because or unless marking the beginning of another clause. Dynamic delimiters are, however, 4 It is probably due to the latter, that these rules have been successfully re-used in subsequent works, most prominently Klenner et al. (2009). • the assessment of polarity of the constituents 64 ambiguous, e.g. like and for, and require disambiguation rules, using contextual information such as their pertaining part-of-speech tag. These delimiters suitably account for various complex sentence types so that only the clause containing the negation is considered. The heuristic rules focus on cases in which polar expressions in specific syntactic configurations are directly preceded by negation words which results in the polar expression becoming a delimiter itself. Unlike Choi and Cardie (2008), these rules require a proper parse and"
W10-3111,D09-1131,0,0.0065067,"lexicalized, such as flaw-less, and are consequently to be found a polarity lexicon, this phenomenon does not need to be accounted for in sentiment analysis. However, since this process is (at least theoretically) productive, fairly uncommon words, such as not-so-nice, anti-war or offensiveless which are not necessarily contained in lexical resources, may emerge as a result of this process. Therefore, a polarity classifier should also be able to decompose words and carry out negation modeling within words. There are only few works addressing this particular aspect (Moilanen and Pulman, 2008; Ku et al., 2009) so it is not clear how much impact this type of negation has on an overall polarity classification and what complexity of morphological analysis is really necessary. We argue, however, that in synthetic languages where negation may regularly be realized as an affix rather than an individual word, such an analysis is much more important. • the entire sentence 3.5 Negation in Various Languages • a simple negation scope using a fixed window size (similar to the negation feature in (Wilson et al., 2005)) The proposed method consistently outperforms the simpler methods proving that the incorporati"
W10-3111,D09-1017,0,0.0443385,"ruction. 10. Peter mag den Kuchen nicht. Peter likes the cake not. ‘Peter does not like the cake.’ 11. Der Kuchen ist nicht k¨ostlich. The cake is not delicious. ‘The cake is not delicious.’ These items show that, clearly, some more extensive cross-lingual examination is required in order to be able to make statements of the general applicability of specific negation models. 3.6 Bad and Not Good are Not the Same The standard approach of negation modeling suggests to consider a negated polar expression, such as not bad, as an unnegated polar expression with the opposite polarity, such as good. Liu and Seneff (2009) claim, however, that this is an oversimplification of language. Not bad and good may have the same polarity but they differ in their respective polar strength, i.e. not bad is less positive than good. That is why, Liu and Seneff (2009) suggest a compositional model in which for individual adjectives and adverbs (the latter include negations) a prior rating score encoding their intensity and polarity is estimated from pros and cons of on-line reviews. Moreover, compositional rules for polar phrases, such as adverb-adjective or negation-adverb-adjective are defined exclusively using the scores"
W10-3111,W09-1105,0,0.0426758,"hod consistently outperforms the simpler methods proving that the incorporation of linguistic insights into negation modeling is meaningful. Even on polarity document retrieval, i.e. a more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007; Morante et al., 2008; Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The 65 Current research in sentiment analysis mainly focuses on English texts. Since there are significant structural differences among the different languages, some particular methods may only capture the idiosyncratic properties of the English language. This may also affect negation modeling. The previous section already stated that the need for morphological analyses may differ across the different languages. Moreo"
W10-3111,D08-1075,0,0.0762849,"005)) The proposed method consistently outperforms the simpler methods proving that the incorporation of linguistic insights into negation modeling is meaningful. Even on polarity document retrieval, i.e. a more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007; Morante et al., 2008; Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The 65 Current research in sentiment analysis mainly focuses on English texts. Since there are significant structural differences among the different languages, some particular methods may only capture the idiosyncratic properties of the English language. This may also affect negation modeling. The previous section already stated that the need for morphological analyses may differ across"
W10-3111,P06-2079,0,0.0191036,"ion, e.g. it does not matter whether a classifier cannot model a negation if the text to be classified contains twenty polar opinions and only one or two contain a negation. Another advantage of these machine learning approaches on coarsegrained classification is their usage of higher order n-grams. Imagine a labeled training set of documents contains frequent bigrams, such as not appealing or less entertaining. Then a feature set using higher order n-grams implicitly contains negation modeling. This also partially explains the effectiveness of bigrams and trigrams for this task as stated in (Ng et al., 2006). The dataset used for the experiments in (Pang et al., 2002; Ng et al., 2006) has been established as a popular benchmark dataset for sentiment analysis and is publicly available1 . 3.2 Incorporating Negation in Models that Include Knowledge of Polar Expressions - Early Works The previous subsection suggested that appropriate negation modeling for sentiment analysis requires the awareness of polar expressions. One way of obtaining such expressions is by using a 1 http://www.cs.cornell.edu/people/ pabo/movie-review-data 62 polarity lexicon which contains a list of polar expressions and for eac"
W10-3111,P04-1035,0,0.019708,"sk dealing with the automatic detection and classification of opinions expressed in text written in natural language. Subjectivity is defined as the linguistic expression of somebody’s opinions, sentiments, emotions, evaluations, beliefs and speculations (Wiebe, 1994). Subjectivity is opposed to objectivity, which is the expression of facts. It is important to make the distinction between subjectivity detection and sentiment analysis, as they are two separate tasks in natural language processing. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. Although research in this area has started only recently, the substantial growth in subjective information on the world wide web in the past years has made sentiment analysis a task on which constantly growing efforts have been concentrated. The body of research published on sentiment analysis has shown that the task is difficult, not only due to the syntactic and semantic variability of language, but also because it involves the extraction of indirect or implicit assessments of"
W10-3111,W02-1011,0,0.0170526,"m these examples, modeling negation is a difficult yet important aspect of sentiment analysis. 3 The Survey In this survey, we focus on work that has presented novel aspects for negation modeling in sentiment analysis and we describe them chronologically. 3.1 Negation and Bag of Words in Supervised Machine Learning Several research efforts in polarity classification employ supervised machine-learning algorithms, like Support Vector Machines, Na¨ıve Bayes Classifiers or Maximum Entropy Classifiers. For these algorithms, already a low-level representation using bag of words is fairly effective (Pang et al., 2002). Using a bag-of-words representation, the supervised classifier has to figure out by itself which words in the dataset, or more precisely feature set, are polar and which are not. One either considers all words occurring in a dataset or, as in the case of Pang et al. (2002), one carries out a simple feature selection, such as removing infrequent words. Thus, the standard bag-of-words representation does not contain any explicit knowledge of polar expressions. As a consequence of this simple level of representation, the reversal of the polarity type of polar expressions as it is caused by a ne"
W10-3111,P09-1026,0,0.00780997,"r meaning in one context (Sentence 12) but do not have one in another (Sentence 13). 12. He is a disease to every team he has gone to. 13. Early symptoms of the disease are headaches, fevers, cold chills and body pain. In a pilot study (Akkaya et al., 2009), it has already been shown that applying subjectivity word sense disambiguation in addition to the featurebased negation modeling approach of Wilson et al. (2005) results in an improvement of performance in polarity classification. Another problem is that some polar opinions are not lexicalized. Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge. 67 14. The next time I hear this song on the radio, I’ll throw my radio out of the window. Moreover, the effectiveness of specific negation models can only be proven with the help of corpora containing those constructions or the type of language behaviour that is reflected in the models to be evaluated. This presumably explains why rare constructions, such as negations using connectives (Sentence 6 in §2), modals (Sentence 7 in §2) or other phenomena presented in the conceptual model of Polanyi and Zaenen (2004), have not y"
W10-3111,W08-0606,0,0.042933,"more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007; Morante et al., 2008; Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The 65 Current research in sentiment analysis mainly focuses on English texts. Since there are significant structural differences among the different languages, some particular methods may only capture the idiosyncratic properties of the English language. This may also affect negation modeling. The previous section already stated that the need for morphological analyses may differ across the different languages. Moreover, the complexity of scope modeling may also be language dependent. In English, for example, modeling the scope of a negation as a fixed window size of words following the occ"
W10-3111,J94-2004,0,0.0714206,"arious computational approaches modeling negation in sentiment analysis. We will, in particular, focus on aspects, such as level of representation used for sentiment analysis, negation word detection and scope of negation. We will also discuss limits and challenges of negation modeling on that task. 1 Introduction Sentiment analysis is the task dealing with the automatic detection and classification of opinions expressed in text written in natural language. Subjectivity is defined as the linguistic expression of somebody’s opinions, sentiments, emotions, evaluations, beliefs and speculations (Wiebe, 1994). Subjectivity is opposed to objectivity, which is the expression of facts. It is important to make the distinction between subjectivity detection and sentiment analysis, as they are two separate tasks in natural language processing. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. Although research in this area has started only recently, the substantial growth in subjective information on the world wide web in t"
W10-3111,H05-1044,0,0.619923,"on is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts. Final results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers. 3.2.2 Features for Negation Modeling Wilson et al. (2005) carry out more advanced negation modeling on expression-level polarity classification. The work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. The features for negation modeling are organized in three groups: • negation features • shifter features • polarity modification features Negation features directly relate to negation expressions negating a polar expression. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. The other feature accounts for a polar predica"
W10-3111,C08-1135,0,0.0112467,"and negation words. Its advantage is that polarity is treated compositionally and is interpreted as a continuum rather than a binary classification. This approach reflects its meaning in a more suitable manner. 3.7 Using Negations in Lexicon Induction Many classification approaches illustrated above depend on the knowledge of which natural lan66 guage expressions are polar. The process of acquiring such lexical resources is called lexicon induction. The observation that negations co-occur with polar expressions has been used for inducing polarity lexicons on Chinese in an unsupervised manner (Zagibalov and Carroll, 2008). One advantage of negation is that though the induction starts with just positive polar seeds, the method also accomplishes to extract negative polar expressions since negated mentions of the positive polar seeds co-occur with negative polar expressions. Moreover, and more importantly, the distribution of the co-occurrence between polar expressions and negations can be exploited for the selection of those seed lexical items. The model presented by Zagibalov and Carroll (2008) relies on the observation that a polar expression can be negated but it occurs more frequently without the negation. T"
W10-3111,J09-3003,0,\N,Missing
W10-3111,P08-2028,0,\N,Missing
W11-4004,H05-1045,0,0.793315,"n a corpus annotated with opinion holders. Our insights are, in particular, important for situations in which no labeled training data are available and only rule-based methods can be applied. 1 Introduction One of the most important tasks in sentiment analysis is opinion holder extraction in which the entities uttering an opinion, also known as opinion holders, need to be extracted from a natural language text. For example, the opinion holders in (1) and (2) are the vet and Russia, respectively. 2 Related Work There has been much research on supervised learning for opinion holder extraction. Choi et al. (2005) examine opinion holder extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. Bethard et al. (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. The approaches of those two papers have mostly been evaluated on some artificial data sets. More recently, Wiegand and Klakow (2010) explored convolution kernels for opinion holder extraction. Rule-based opinion holder extraction heavily relies on lexical cues. Bloom et al."
W11-4004,P05-1045,0,0.03537,"e 10 most frequent opinion holder predicates. already provides a comparatively high recall. The clue PERSON checks whether the candidate opinion holder is a person. For some ambiguous predicates, such as critical, this would allow a correct disambiguation, i.e. Dr. Ren in (4) would be classified as an opinion holder while the crossstrait balance of military power in (5) would not. 4. Dr. Ren was critical of the government’s decision. 5. In his view, the cross-strait balance of military power is critical to the ROC’s national security. For this clue, we employ Stanford named-entity recognizer (Finkel et al., 2005) for detecting proper nouns and WordNet for recognizing common nouns denoting persons. The second clue SUBJ detects subjective evidence in a sentence. The heuristics applied should filter false positives, such as (6). 6. “We do not have special devices for inspecting large automobiles and cargoes”, Nazarov said. If an opinion holder has been found according to our standard procedure using opinion holder predicates, some additional property must hold so that the classifier predicts an opinion holder. Either the candidate opinion holder phrase contains a subjective expression (7), some subjectiv"
W11-4004,P03-1054,0,0.0198252,"eover, we also carry out a quantitative evaluation of those related phenomena. Unlike Ruppenhofer et al. (2008), we thus try to identify the most immediate problems of this task. By also considering resources in order to solve these problems we hope to be a helpful guide for practitioners building an opinion holder extraction system from scratch. 4.2 The Different Types of Grammatical Relations Table 2 shows the distribution of the most frequent grammatical relations between opinion holder and its related predicate listed separately for each unigram predicate type. We use the Stanford parser (Klein and Manning, 2003) for obtaining all syntactic information. The table displays the percentage of that grammatical relation within the particular predicate type when it is observed as a predicate of an opinion holder in our labeled data set (Perc.)3 , the property of being a fairly reliable relation for a semantic agent (Agent), and the precision of that grammatical relation in conjunction with that opinion holder predicate type for detecting opinion holders (Precision). As a goldstandard of opinion holder predicates we extracted all unigram predicates from our data set that cooccur at least twice with an actual"
W11-4004,P10-1059,0,0.0606293,"is marked as such for the other parts of speech. We found that subjects of predicate nouns can very often be found in constructions like (3). Clearly, this is not an agent of idea. 3 Data As a labeled (test) corpus, we use the MPQA 2.0 corpus1 which is a large text corpus containing fine-grained sentiment annotation. It (mainly) consists of news texts which can be considered as a primary domain for opinion holder extraction. Other popular domains for sentiment analysis, for example, product reviews contain much fewer opinion holders according to the pertaining data sets (Kessler et al., 2010; Toprak et al., 2010). Opinions uttered in those texts usually express the author’s point of view. Therefore, the extraction of sources of opinions is of minor importance. We use the definition of opinion holders as described in (Wiegand and Klakow, 2010), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2003) is considered an opinion holder. This is a very strict definition and the scores produced in this work can only be put into relation to the numbers presented in (Wiegand and Klakow, 2010). The final corpus comprises approximately 11,000 sentences with more than 6,200 opinion h"
W11-4004,N10-1121,1,0.909956,"(1) and (2) are the vet and Russia, respectively. 2 Related Work There has been much research on supervised learning for opinion holder extraction. Choi et al. (2005) examine opinion holder extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. Bethard et al. (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. The approaches of those two papers have mostly been evaluated on some artificial data sets. More recently, Wiegand and Klakow (2010) explored convolution kernels for opinion holder extraction. Rule-based opinion holder extraction heavily relies on lexical cues. Bloom et al. (2007) use a list of manually compiled communication verbs and 1. The owner put down the animal, although the vet had forbidden him to do so. 2. Russia favors creation of “international instruments” to regulate emissions. As this is an entity extraction problem it can be considered as a typical task in information extraction. Though there is much work on that subject, most work focuses on data-driven methods. Thus, to a great extent it fails to fully de"
W11-4004,H05-1044,0,0.391413,"b.motion). We consider the files noun.cognition, noun.communication, verb.cognition and verb.communication. Due to the coarse-grained nature of the WN-LF, the resulting set of words contains 10151 words (7684 nouns and 2467 verbs). Table 5 summarizes the properties of the different resources. Due to the high number of nouns in WN-LF, we will evaluate this lexicon both with and without nouns. For all resources only containing verbs, we also use Nomlex (Macleod et al., 1998) to find corresponding noun predicates, e.g. Subjectivity Lexicon (SL) The Subjectivity Lexicon (SL) from the MPQAproject (Wilson et al., 2005) is one of the most commonly used sentiment lexicons. The lexicon contains 8222 subjective expressions from different parts of speech. For our experiments we will only consider its verbs, nouns and adjectives. This lexicon has been used for various subtasks in sentiment analysis, primarily subjectivity detection and polarity classification (Wilson et al., 2005). It has also been used for opinion holder extraction (Choi et al., 2005; Wiegand and Klakow, 2010) though the lexicon does not contain any annotation specifically designed for this task which is why each entry is considered some clue fo"
W11-4635,J92-4003,0,0.0710569,"d Palmer, 2002). Semantic roles are obtained by using the parser by Zhang et al. (2008). A special property of PAS is that a data instance, i.e. the information regarding one target word and its particular context, is represented by a set of those structures rather than a single structure. Thus, the actual partial tree kernel function we use for this task, P T K, sums over all possible pairs P ASl and P ASm of two xi and xj : P T K(xi , xj ) = Xdata instances X P T Kbasic (P ASl , P ASm ). An P ASl ∈xi P ASm ∈xj illustration of these substructures is given in Figure 1(c). (a) CON (b) DEP ing (Brown et al., 1992) which is the best performing algorithm in (Turian et al., 2010). This algorithm induces clusters with the help of cooccurrence statistics of bigrams. We augment our structures with the clustering information. We add the node with a cluster label in such a way that it directly dominates the pertaining lexical node. As a software we use SRILM (Stolcke, 2002) with the default algorithm. The clusters are induced on the North American News Text Corpus (LDC95T21). We chose this corpus as it contains news texts similar to our evaluation corpus (i.e. MPQA). Following Turian et al. (2010), we induced"
W11-4635,P05-1045,0,0.00996647,"chose this corpus as it contains news texts similar to our evaluation corpus (i.e. MPQA). Following Turian et al. (2010), we induced 1000 clusters. As many names of persons and organizations can be very domain-specific, they may not appear in the corpus from which clusters are induced. Consequently, these expressions cannot be assigned to a cluster. We try to compensate this by incorporating the knowledge about named entities in tree kernels, i.e. instead of assigning some expression to a cluster we assign it to a named entity type. Named-entity information is obtained by the Stanford tagger (Finkel et al., 2005). 4.4 The Different Settings (c) PAS Figure 1: Illustration of the different tree structures employed for convolution kernels derived from Sentence 3 with reactions as the target word. 4.3.4 Augmentation with Clustering A common type of unsupervised generalization is clustering. Words which co-occur with each other are automatically grouped into clusters. Ideally, a cluster thus contains words with similar syntactic/semantic properties. The cluster membership of individual words is induced from a large unlabeled corpus. As the context windows of our target expressions contain fairly sparse lex"
W11-4635,D10-1101,0,0.0264447,"rser (Klein and Manning, 2003) for obtaining constituency parse trees. 4.3.2 Dependency Parse Structures (DEP) Apart from manually designed features, Johansson and Moschitti (2010) also test a tree kernel for subjectivity detection using a dependency parse. However, the entire parse comprising a sentence is considered. The resulting tree kernel does not show any significant improvement (again presumably because of the large amount of irrelevant information). The usefulness of particular (usually direct) relations, however, has been found effective on other related tasks in sentiment analysis (Jakob and Gurevych, 2010; Wiegand and Klakow, 2010). We therefore only consider the subtree exclusively containing the lexical units that are connected to the target word by a direct syntactic dependency relationship (i.e. direct parent and direct children). The precise encoding of the pertaining information (i.e. part-ofspeech, grammatical relation, and lexical information) in the resulting tree is taken from (Johansson and Moschitti, 2010). An illustration of this substructure is given in Figure 1(b). Again, we use the Stanford Parser (Klein and Manning, 2003) for obtaining dependency parse trees. 4.3.3 Predicate-A"
W11-4635,W10-2910,0,0.25335,"lder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The features used in those works have been manually designed and comprise various levels of representation, such as grammatical relations or predicateargument structures. Johansson and Moschitti (2010) also use a tree kernel encoding dependency parse trees, however, there is no significant improvement achieved by that structure. In this work, we not only consider dependency parse trees for convolution kernels but also other linguistic levels of representation. Moreover, we also consider appropriate substructures rather than the structures derived from an entire sentence. The latter approach"
W11-4635,W06-0301,0,0.0202091,"arent and direct children). The precise encoding of the pertaining information (i.e. part-ofspeech, grammatical relation, and lexical information) in the resulting tree is taken from (Johansson and Moschitti, 2010). An illustration of this substructure is given in Figure 1(b). Again, we use the Stanford Parser (Klein and Manning, 2003) for obtaining dependency parse trees. 4.3.3 Predicate-Argument Structures (PAS) Predicate-argument structures (PAS), in particular, semantic role labeling has been shown to be effective for many information extraction tasks, including opinion holder extraction (Kim and Hovy, 2006; Wiegand and Klakow, 2010) and opinion target extraction (Kim and Hovy, 2006). Johansson and Moschitti (2010) also examine this level of representation for subjectivity detection. However, they employ manual features derived from these structures rather than using a corresponding tree kernel. We follow Wiegand and Klakow (2010) for the encoding of these structures as tree kernels, that is we restrict ourselves to structures in which the target word is either a predicate or some argument. We derive our predicate-argument structures from a semantic parse based on the PropBank annotation scheme"
W11-4635,kingsbury-palmer-2002-treebank,0,0.0169191,"Wiegand and Klakow, 2010) and opinion target extraction (Kim and Hovy, 2006). Johansson and Moschitti (2010) also examine this level of representation for subjectivity detection. However, they employ manual features derived from these structures rather than using a corresponding tree kernel. We follow Wiegand and Klakow (2010) for the encoding of these structures as tree kernels, that is we restrict ourselves to structures in which the target word is either a predicate or some argument. We derive our predicate-argument structures from a semantic parse based on the PropBank annotation scheme (Kingsbury and Palmer, 2002). Semantic roles are obtained by using the parser by Zhang et al. (2008). A special property of PAS is that a data instance, i.e. the information regarding one target word and its particular context, is represented by a set of those structures rather than a single structure. Thus, the actual partial tree kernel function we use for this task, P T K, sums over all possible pairs P ASl and P ASm of two xi and xj : P T K(xi , xj ) = Xdata instances X P T Kbasic (P ASl , P ASm ). An P ASl ∈xi P ASm ∈xj illustration of these substructures is given in Figure 1(c). (a) CON (b) DEP ing (Brown et al., 1"
W11-4635,P03-1054,0,0.00555918,"ived from scopes. The best performing subtree is the tree with the predicate scope, i.e. a subtree with the boundaries being the candidate or target expression and the nearest predicate. We also assume that this structure is meaningful for our task. As already discovered in previous work on the detection of subjective expressions (Riloff and Wiebe, 2003; Riloff and Wiebe, 2003; Wilson et al., 2005), discriminant patterns often encode a relation between the target expression and the nearest predicate. An illustration of this substructure is given in Figure 1(a). We use the Stanford 256 Parser (Klein and Manning, 2003) for obtaining constituency parse trees. 4.3.2 Dependency Parse Structures (DEP) Apart from manually designed features, Johansson and Moschitti (2010) also test a tree kernel for subjectivity detection using a dependency parse. However, the entire parse comprising a sentence is considered. The resulting tree kernel does not show any significant improvement (again presumably because of the large amount of irrelevant information). The usefulness of particular (usually direct) relations, however, has been found effective on other related tasks in sentiment analysis (Jakob and Gurevych, 2010; Wieg"
W11-4635,J08-2003,0,0.0162426,"rder to generalize from lexical information, we additionally augment these structures with clustering information and the task-specific knowledge of Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effec"
W11-4635,P06-2079,0,0.0291324,"al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The features used in those works have been manually designed and comprise various levels of representation, such as grammatical relations or predicateargument structures. Johansson and Moschitti (2010) also use a tree kernel encoding dependency parse trees, however, there is no significant improvement achieved by that structure. In this work, we not only consi"
W11-4635,D09-1143,0,0.128445,"tures, and predicate-argument structures. In order to generalize from lexical information, we additionally augment these structures with clustering information and the task-specific knowledge of Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of m"
W11-4635,W03-1014,0,0.0694711,"uces very low performing classifiers. Structures derived from an entire sentence contain too much irrelevant information for such a task at expression level. We assume that the same is true for the detection of subjectivity. Wiegand and Klakow (2010) use subtrees derived from scopes. The best performing subtree is the tree with the predicate scope, i.e. a subtree with the boundaries being the candidate or target expression and the nearest predicate. We also assume that this structure is meaningful for our task. As already discovered in previous work on the detection of subjective expressions (Riloff and Wiebe, 2003; Riloff and Wiebe, 2003; Wilson et al., 2005), discriminant patterns often encode a relation between the target expression and the nearest predicate. An illustration of this substructure is given in Figure 1(a). We use the Stanford 256 Parser (Klein and Manning, 2003) for obtaining constituency parse trees. 4.3.2 Dependency Parse Structures (DEP) Apart from manually designed features, Johansson and Moschitti (2010) also test a tree kernel for subjectivity detection using a dependency parse. However, the entire parse comprising a sentence is considered. The resulting tree kernel does not show"
W11-4635,P10-1040,0,0.151711,"r by Zhang et al. (2008). A special property of PAS is that a data instance, i.e. the information regarding one target word and its particular context, is represented by a set of those structures rather than a single structure. Thus, the actual partial tree kernel function we use for this task, P T K, sums over all possible pairs P ASl and P ASm of two xi and xj : P T K(xi , xj ) = Xdata instances X P T Kbasic (P ASl , P ASm ). An P ASl ∈xi P ASm ∈xj illustration of these substructures is given in Figure 1(c). (a) CON (b) DEP ing (Brown et al., 1992) which is the best performing algorithm in (Turian et al., 2010). This algorithm induces clusters with the help of cooccurrence statistics of bigrams. We augment our structures with the clustering information. We add the node with a cluster label in such a way that it directly dominates the pertaining lexical node. As a software we use SRILM (Stolcke, 2002) with the default algorithm. The clusters are induced on the North American News Text Corpus (LDC95T21). We chose this corpus as it contains news texts similar to our evaluation corpus (i.e. MPQA). Following Turian et al. (2010), we induced 1000 clusters. As many names of persons and organizations can be"
W11-4635,N10-1121,1,0.319936,"Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The featur"
W11-4635,H05-1044,0,0.51944,"has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The features used in those works have been manually designed and comprise various levels of representation, such as grammatical relations or predicateargument structures. Johansson and Moschitti (2010) also use a tree kernel encoding dependency parse trees, however, there is no significant improvement achieved by that structure. In this work, we not only consider dependency parse trees for convolution kernels but also other linguistic levels of representation. Moreover, we also consider appropriate substructures rather than the structure"
W11-4635,N06-1037,0,0.157945,"pendency parse structures, and predicate-argument structures. In order to generalize from lexical information, we additionally augment these structures with clustering information and the task-specific knowledge of Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al.,"
W13-1108,D09-1020,0,0.0125799,"mbination (bow+task) 55 50 Table 10: Comparison of different feature sets (summary of features is displayed in Table 5); ∗ significantly better than bow at p &lt; 0.05 (based on paired t-test). F-score 45 40 35 30 fact the different feature types have different properties. On the one hand, there are unambiguous feature types, such as AUTH, which work fine with a wide scope. But we also have ambiguous feature types that require a fairly narrow context. A typical example are strong (positive) polar expressions (STROPO+ ). (Polar expressions are known to be very ambiguous (Wiebe and Mihalcea, 2006; Akkaya et al., 2009).) 5.2 Classification Table 10 compares the different feature sets with regard to extraction performance. We carry out a 5-fold cross-validation on our manually labeled dataset. As a classifier, we chose Support Vector Machines (Joachims, 1999). As a toolkit, we use SVMLight4 with a linear kernel. Table 10 clearly shows the strength of the highlevel features that we proposed. They do not only represent a strong feature set on their own but they can also usefully be combined with bag-of-words features. Apparently, neither part-of-speech nor parse information are predictive for this task. 5.3 Im"
W13-1108,W97-0802,0,0.132084,"rd features, such as bag of words. 3 Data & Annotation As a corpus for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012a) consisting of 418, 558 webpages of food-related forum entries. chefkoch.de is the largest web portal for food-related issues in the German language. From this dataset, sentences in which some food item co-occurred with some health condition (e.g. pregnancy, diarrhoea or flu) were extracted. (In the following, we will also refer to these entities as target food item and target health condition.) The food items were identified with the help of GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990), and the health conditions were used 1 www.chefkoch.de 70 from Wiegand et al. (2012b). In total, 2604 sentences were thus obtained. For the manual annotation, each target sentence (i.e. a sentence with a co-occurrence of target food item and health condition) was presented in combination with the two sentences immediately preceding and following it. Each target sentence was manually assigned two labels, one specifying the type of suitability (§3.1) and another specifying whether the relation expressed is considered reliable or not (§3.2). 3"
W13-1108,P97-1023,0,0.238863,"orrible− or he sang terribly− ). We employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of a polarity lexicon to be modified by the intensifier, while INTENSadj requires an adjective to be modified. In order to identify polar expressions we use the polarity lexicon underlying the PolArt system (Klenner et al., 2009). We also consider adjectives since we must assume that our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but excellent has a much higher polar intensity than nice. Taboada et al. (2011) introduced an English polarity lexicon SO-CAL in which polar expressions were also assigned an intensity label. As our German polarity lexicon (§4.1.3) does not contain comparable intensity labels,"
W13-1108,C00-1044,0,0.520806,"employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of a polarity lexicon to be modified by the intensifier, while INTENSadj requires an adjective to be modified. In order to identify polar expressions we use the polarity lexicon underlying the PolArt system (Klenner et al., 2009). We also consider adjectives since we must assume that our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but excellent has a much higher polar intensity than nice. Taboada et al. (2011) introduced an English polarity lexicon SO-CAL in which polar expressions were also assigned an intensity label. As our German polarity lexicon (§4.1.3) does not contain comparable intensity labels, we used a German translation of SO"
W13-1108,R09-1034,0,0.0202961,"emely). The second group includes more ambiguous expressions, such as adjectives that only function as an intensifier if they modify a polar expression (e.g. horrible pain or terribly nice) otherwise they function as typical polar expressions (e.g. you are horrible− or he sang terribly− ). We employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of a polarity lexicon to be modified by the intensifier, while INTENSadj requires an adjective to be modified. In order to identify polar expressions we use the polarity lexicon underlying the PolArt system (Klenner et al., 2009). We also consider adjectives since we must assume that our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but exc"
W13-1108,J93-2004,0,0.0432563,"Missing"
W13-1108,Y12-1010,0,0.0310966,"tion of health relations from social media are concerned, the prediction of epidemics (Fisichella et al., 2011; Torii et al., 2011; Diaz-Aviles et al., 2012; Munro et al., 2012) has recently attracted the attention of the research community. Relation extraction involving food items has also been explored in the context of ontology alignment (van Hage et al., 2005; van Hage et al., 2006; van Hage et al., 2010) and also as a means of knowledge acquisition for virtual customer advice in a supermarket (Wiegand et al., 2012a). The works most closely related to this paper are Yang et al. (2011) and Miao et al. (2012). Both of these works address the extraction of food-health relationships. Unlike this work, they extract relations from scientific biomedical texts rather than social media. Yang et al. (2011) also cover the task of strength analysis which bears some resemblance to the task of finding reliable utterances to some extent. However, the features applied to that classification task are only standard features, such as bag of words. 3 Data & Annotation As a corpus for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012a) consisting of 418, 558 webpages of food-related forum entrie"
W13-1108,W09-1304,0,0.0810857,"Missing"
W13-1108,W08-1006,0,0.0215284,"e 4 lists all the variants that we use. These variants are applied to all feature types except the types of suitability (§4.1.9) as this label has only been assigned to an entire target sentence. 4.3 Other Features Table 5 lists the entire set of features that we examine in this work. The simplest classifier that we can construct for our task is a trivial classifier that predicts all statements as reliable statements. The remaining features comprise bag of words, part-ofspeech and syntactic parse information. For the latter two features, we employ the output of the Stanford Parser for German (Rafferty and Manning, 2008). 74 Description trivial classifier that always predicts a reliable statement bag-of-words features: all words between the target food item and target health condition and the words immediately preceding and following each of them part-of-speech features: part-of-speech sequence between target food item and health condition and tags of the words immediately preceding and following each of the target expressions path from syntactic parse tree from target food item to target health condition all task-specific high-level feature types from §4.1 with their respective variants (§4.2) Table 5: Descr"
W13-1108,J11-2001,0,0.0191279,"hat our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but excellent has a much higher polar intensity than nice. Taboada et al. (2011) introduced an English polarity lexicon SO-CAL in which polar expressions were also assigned an intensity label. As our German polarity lexicon (§4.1.3) does not contain comparable intensity labels, we used a German translation of SO-CAL. We identified polar expressions with a high intensity score (i.e. ±4 or ±5) as strong polar expressions. It includes 221 highly positive and 344 highly negative polar expressions. We also distinguish the polarity type (i.e. STROPO+ refers to positive and STROPO− refers to negative polarity). (16) Baking soda is an excellent remedy against heartburn. 4.1.5 Sup"
W13-1108,P06-1134,0,0.0133646,"h-level features (task) combination (bow+task) 55 50 Table 10: Comparison of different feature sets (summary of features is displayed in Table 5); ∗ significantly better than bow at p &lt; 0.05 (based on paired t-test). F-score 45 40 35 30 fact the different feature types have different properties. On the one hand, there are unambiguous feature types, such as AUTH, which work fine with a wide scope. But we also have ambiguous feature types that require a fairly narrow context. A typical example are strong (positive) polar expressions (STROPO+ ). (Polar expressions are known to be very ambiguous (Wiebe and Mihalcea, 2006; Akkaya et al., 2009).) 5.2 Classification Table 10 compares the different feature sets with regard to extraction performance. We carry out a 5-fold cross-validation on our manually labeled dataset. As a classifier, we chose Support Vector Machines (Joachims, 1999). As a toolkit, we use SVMLight4 with a linear kernel. Table 10 clearly shows the strength of the highlevel features that we proposed. They do not only represent a strong feature set on their own but they can also usefully be combined with bag-of-words features. Apparently, neither part-of-speech nor parse information are predictive"
W13-1108,wiegand-etal-2012-gold,1,0.891497,"Missing"
W13-1108,H05-1044,0,0.014602,"ertain relation very frequently or even at all times, then there is a high likelihood that this relation actually holds (14). We use a set of adverbs (18 expressions) that express high frequency (e.g. often, frequently etc.) or constancy (e.g. always, at all times etc.). (14) What always helps me when I have the flu is a hot chicken broth. 4.1.3 Intensifiers (INTENS) Some utterances may also be perceived reliable if their speaker adds some emphasis to them. One way of doing so is by adding intensifiers to a remark (15). 72 The intensifiers we use are a translation of the lexicon introduced in Wilson et al. (2005). For the detection, we divide that list into two groups: The first group INTENSsimple are unambiguous adverbs that always function as intensifiers no matter in which context they appear (e.g. very or extremely). The second group includes more ambiguous expressions, such as adjectives that only function as an intensifier if they modify a polar expression (e.g. horrible pain or terribly nice) otherwise they function as typical polar expressions (e.g. you are horrible− or he sang terribly− ). We employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of"
W15-1809,D08-1097,0,0.0561605,"Missing"
W15-1809,P00-1071,0,0.117996,"Generation and Expansion Question Focus words Expanded focus EAT Query Expanded query What do you do as a job? do as job do [make, perform, cause, practice, act], as, job [activity, occupation, career, employment, position] Title do(do as job) (Z, E, ?X) :: Title do(Z, doAs, ?job) :: QUALITY(String) :: QUANTITY(List) :: FOCUS(do as job) (Z, E, ?X) :: Title do(Z, doAs, ?job) :: QUALITY(String) :: QUANTITY(List) :: FOCUS(do [make, perform, practice, act], as, job [activity, occupation, career, employment, position]) Table 4: Example of an expanded query. 5 Question Focus Extraction In line with Moldovan et al. (2000), the question focus describing the main event is typically expressed by a verb or eventive noun. Despite the fact that the focus is semantically defined, we use the knowledge of syntactic structures, since syntactic parsers are mature enough comparing to semantic ones to be used reliably. The following Query generation is the last data processing operation that is performed in the question interpretation module. The query is generated according to the pre-defined set of rules. It captures the results of the question classification (labels) process as well as the extracted focus words and tran"
W15-1809,petukhova-etal-2014-dbox,1,0.876522,"Missing"
W15-1809,W00-0603,0,0.210128,"Missing"
W15-1809,W00-0600,0,0.117976,"xpansion Question Focus words Expanded focus EAT Query Expanded query What do you do as a job? do as job do [make, perform, cause, practice, act], as, job [activity, occupation, career, employment, position] Title do(do as job) (Z, E, ?X) :: Title do(Z, doAs, ?job) :: QUALITY(String) :: QUANTITY(List) :: FOCUS(do as job) (Z, E, ?X) :: Title do(Z, doAs, ?job) :: QUALITY(String) :: QUANTITY(List) :: FOCUS(do [make, perform, practice, act], as, job [activity, occupation, career, employment, position]) Table 4: Example of an expanded query. 5 Question Focus Extraction In line with Moldovan et al. (2000), the question focus describing the main event is typically expressed by a verb or eventive noun. Despite the fact that the focus is semantically defined, we use the knowledge of syntactic structures, since syntactic parsers are mature enough comparing to semantic ones to be used reliably. The following Query generation is the last data processing operation that is performed in the question interpretation module. The query is generated according to the pre-defined set of rules. It captures the results of the question classification (labels) process as well as the extracted focus words and tran"
W15-1809,C02-1150,0,0.0812778,"real life situations. We can use the hierarchical structure of our taxonomy to better discriminate between different question types. There are at least two possible ways of how it can be implemented: 1. Sequence of classifiers, where classifier#1 predicts coarse class labels and classifier#2 applies these labels as additional features. 2. Hierarchy of classifiers, where classifier#1 decides to which coarse class a question belongs and transfers it to the corresponding classifier trained specifically for these types of questions. In our experiments we followed the first approach. According to (Li and Roth, 2002) who worked on a very similar problem there is no significant difference in performance between flat and hierarchical classifiers. 3.3 Features No matter what learning algorithm or approach is applied, text-based features remain important for the classification task. The bag-of-words (BoW) approach, for example, is by far most widely used in text classification. This approach does not take into account the order of words and their cooccurrences. Therefore, apart from bow-models we constructed models based on bigrams, trigrams, and their combinations to assess their impact on the overall classi"
W15-1809,P09-2082,0,0.0370652,"Missing"
W15-4718,J11-3003,1,0.888211,"Missing"
W15-4718,W08-0130,0,0.049453,"Missing"
W15-4718,N01-1021,0,0.0148096,"ow which utterance is a “complex” one? We can draw on psycholinguistic models of human sentence processing difficulty, such as dependency locality theory (measuring dependency lengths within the sentence; longer dependencies are more difficult), information density (measuring surprisal – the amount of information conveyed in a certain time unit; a higher rate of information per time unit is more difficult) or words-per-concept (how many words are used to convey a concept). In this paper, we focus on the measure of information density, which uses the information-theoretic measure of surprisal (Hale, 2001; Levy, 2008), as well as the ratio of concepts per words. Our aim is to flexibly generate utterances that differ in information density, producing high-density and low-density formulations for the same underlying semantic representation. We evaluate different parametrisations of our approach by evaluating how many different high vs. low density utterances can be generated. We additionally present judgments from human evaluators rating both grammaticality and meaningfulness. We collect a small corpus of utterances from the target domain and have them annotated by naive participants with a very"
W15-4718,P10-1157,0,0.0564733,"Missing"
W15-4718,2007.mtsummit-ucnlg.4,0,0.0555371,"terances that are comprehensible and convey the intended meaning, but language that is adaptive to different users as well as situations (see also (Dethlefs, 2014) for an overview). Adaptation can happen at different levels, concerning content as well as the formulation of generated sentences. We here focus only on sentence formulation with the goal of being able to automatically generate a large variety of different realisations of a given semantic representation. Our study explores the combination of a data-driven approach (Mairesse et al., 2010) with a grammar-based approach using OpenCCG (White et al., 2007). The use of templates is a common and well-performing approach to natural language generation. Usually, either the generation process consists of selecting appropriate fillers for manually-built patterns, or the semantic specification constrains the allowable surface constructions so strongly that it effectively constitutes a form of template as well. While such approaches do guarantee grammaticality when templates (or grammars, respectively) are well-designed, the amount of formulation variation that can be generated based on templates is either very low, or requires a huge manual Proceeding"
W17-2404,Q16-1020,0,0.0201486,"the full ensemble of word vectors and show that this structure is a geometrically meaningful representation of the original relations between the words. This newly obtained representation can be used for better understanding and thus improving the embedding algorithm and exhibits semantic meaning, so it can also be utilized in a variety of language processing tasks like categorization or measuring similarity. 1 Introduction There are different ways to assess word embeddings (Yaghoobzadeh and Sch¨utze, 2016). While some authors focus on general properties, as for example Levy et al. (2015) or Hashimoto et al. (2016), most evaluations are with respect to specific tasks. Examples of the latter include the works by Baroni et al. (2014), Schnabel et al. (2015), or Rothe and Sch¨utze (2016), to name but a few. The objective of this paper is to introduce a method for getting a grasp of the global structure of embeddings, which is different from general schemes for dimensionality reduction like t-SNE (Maaten and Hinton, 2008), the methods summarized by Van Der Maaten et al. (2009), or visualization interfaces such as Roleo (Sayeed et al., 2016) and GoWvis (Tixier et al., 2016). The method presented here is a sp"
W17-2404,Q15-1016,0,0.0395816,"raphical clustering of the full ensemble of word vectors and show that this structure is a geometrically meaningful representation of the original relations between the words. This newly obtained representation can be used for better understanding and thus improving the embedding algorithm and exhibits semantic meaning, so it can also be utilized in a variety of language processing tasks like categorization or measuring similarity. 1 Introduction There are different ways to assess word embeddings (Yaghoobzadeh and Sch¨utze, 2016). While some authors focus on general properties, as for example Levy et al. (2015) or Hashimoto et al. (2016), most evaluations are with respect to specific tasks. Examples of the latter include the works by Baroni et al. (2014), Schnabel et al. (2015), or Rothe and Sch¨utze (2016), to name but a few. The objective of this paper is to introduce a method for getting a grasp of the global structure of embeddings, which is different from general schemes for dimensionality reduction like t-SNE (Maaten and Hinton, 2008), the methods summarized by Van Der Maaten et al. (2009), or visualization interfaces such as Roleo (Sayeed et al., 2016) and GoWvis (Tixier et al., 2016). The me"
W17-2404,P98-1013,0,0.0483581,"embeddings. Nevertheless, as already mentioned above, the findings presented in the previous sections indicate that the NH might be used for NLP tasks beyond visualization of word embeddings or other large high-dimensional datasets, because the neighborhood and macro vertex relations appear to be connected to semantical relations between the words, particularly on the lower levels. Possible tasks that directly come to mind are measuring relatedness or similarity, various kinds of tagging, and classification. In contrast to typical semantical frameworks like WordNet (Miller, 1995) or FrameNet (Baker et al., 1998) whose creation requires extensive human resources, the NH can be created without expert knowledge in a very short time and has the capability of including much more words. Zesch and Gurevych (2007) analyze graphs extracted from Wikipedia3 and summarize a variety of methods for evaluating semantical relations. In this spirit and for a first and quick quantitative view at the NH, similarity between neighbors in the graph and between words and their macro vertex are tested by calculating the respective WuPalmer similarity scores (Wu and Palmer, 1994) on WordNet (Miller, 1995). Other scores basic"
W17-2404,P16-2083,0,0.0609896,"Missing"
W17-2404,P14-1023,0,0.0449419,"al relations between the words. This newly obtained representation can be used for better understanding and thus improving the embedding algorithm and exhibits semantic meaning, so it can also be utilized in a variety of language processing tasks like categorization or measuring similarity. 1 Introduction There are different ways to assess word embeddings (Yaghoobzadeh and Sch¨utze, 2016). While some authors focus on general properties, as for example Levy et al. (2015) or Hashimoto et al. (2016), most evaluations are with respect to specific tasks. Examples of the latter include the works by Baroni et al. (2014), Schnabel et al. (2015), or Rothe and Sch¨utze (2016), to name but a few. The objective of this paper is to introduce a method for getting a grasp of the global structure of embeddings, which is different from general schemes for dimensionality reduction like t-SNE (Maaten and Hinton, 2008), the methods summarized by Van Der Maaten et al. (2009), or visualization interfaces such as Roleo (Sayeed et al., 2016) and GoWvis (Tixier et al., 2016). The method presented here is a specific way of clustering (a field nicely reviewed by Jain et al. (1999)) that works particularly well for the current o"
W17-2404,P79-1022,0,0.134471,"Missing"
W17-2404,P16-4024,0,0.0126103,"ocus on general properties, as for example Levy et al. (2015) or Hashimoto et al. (2016), most evaluations are with respect to specific tasks. Examples of the latter include the works by Baroni et al. (2014), Schnabel et al. (2015), or Rothe and Sch¨utze (2016), to name but a few. The objective of this paper is to introduce a method for getting a grasp of the global structure of embeddings, which is different from general schemes for dimensionality reduction like t-SNE (Maaten and Hinton, 2008), the methods summarized by Van Der Maaten et al. (2009), or visualization interfaces such as Roleo (Sayeed et al., 2016) and GoWvis (Tixier et al., 2016). The method presented here is a specific way of clustering (a field nicely reviewed by Jain et al. (1999)) that works particularly well for the current objective. 2 Properties of Embedding Spaces First, a look at global statistics of the dataset lays a basis for justifying later choices and interpreting the hierarchy. Herein, special care must be taken with respect to effects of the high dimensionality. The distribution of the values of single vector components all look very similar and peak clearly at the origin, but they exhibit relatively heavy tails. The d"
W17-2404,P94-1019,0,0.0384542,"works like WordNet (Miller, 1995) or FrameNet (Baker et al., 1998) whose creation requires extensive human resources, the NH can be created without expert knowledge in a very short time and has the capability of including much more words. Zesch and Gurevych (2007) analyze graphs extracted from Wikipedia3 and summarize a variety of methods for evaluating semantical relations. In this spirit and for a first and quick quantitative view at the NH, similarity between neighbors in the graph and between words and their macro vertex are tested by calculating the respective WuPalmer similarity scores (Wu and Palmer, 1994) on WordNet (Miller, 1995). Other scores basically lead to similar results and are thus not discussed in more detail. Because the number of words in WordNet is much smaller than that in the dataset under consideration, the analysis is limited to those words that can be found in both datasets, which amounts to 54,586 words. For that to be possible, a NH of these words alone is used, which is distinct from the full hierarchy discussed above. The usefulness of these results for a much smaller dataset can be justified by envisioning that the sparser NNG must roughly be a skeleton of the full graph"
W17-2404,P16-1023,0,0.0348377,"Missing"
W17-2404,W07-0201,0,0.0407466,"r other large high-dimensional datasets, because the neighborhood and macro vertex relations appear to be connected to semantical relations between the words, particularly on the lower levels. Possible tasks that directly come to mind are measuring relatedness or similarity, various kinds of tagging, and classification. In contrast to typical semantical frameworks like WordNet (Miller, 1995) or FrameNet (Baker et al., 1998) whose creation requires extensive human resources, the NH can be created without expert knowledge in a very short time and has the capability of including much more words. Zesch and Gurevych (2007) analyze graphs extracted from Wikipedia3 and summarize a variety of methods for evaluating semantical relations. In this spirit and for a first and quick quantitative view at the NH, similarity between neighbors in the graph and between words and their macro vertex are tested by calculating the respective WuPalmer similarity scores (Wu and Palmer, 1994) on WordNet (Miller, 1995). Other scores basically lead to similar results and are thus not discussed in more detail. Because the number of words in WordNet is much smaller than that in the dataset under consideration, the analysis is limited t"
W18-3402,P11-1061,0,0.039065,"regularly done for English, for other languages this is often not the case. And even for English, the corpora are usually limited to certain domains like newspaper articles. For tasks in low-resource areas there tend to be no or only few labeled words available. Distant supervision and automatic labeling approaches are an alternative to manually creating labels. These exploit the fact that frequently large amounts of unannotated texts do exist in the targeted domain, e.g. from web crawls. The labels are then assigned using techniques like transferring information from high-resource languages (Das and Petrov, 2011) or simple look-ups in This technique is applicable to different classification scenarios and in this work, we apply it to an NER task. To obtain a non-synthetic, realistic source of noise, we use look-ups from gazetteers for automatically annotating the data. In the lowresource setting, we show the performance boost obtained from training with both clean and noisy instances and from handling the noise in the data. We also compare to another recent neural network noise-handling approach and we give some more insight into the impact of using additional noisy data and into the learned noise mode"
W18-3402,E09-1060,0,0.0321857,"se. Zhu and Wu (2004) suggested that noise in labels tends to be more harmful than noise in features. Beigman and Beigman Klebanov (2009) showed that annotation noise in difficult instances can deteriorate the performance even on simple instances that would have been classified correctly in the absence of the hard cases. Rehbein and Ruppenhofer (2017) presented a technique for detecting annotation noise in automatically labeled POS and NER tags in an active learning scheme. It requires, however, several sources of automatic annotations and human supervision. Similarly, Rocio et al. (2007) and Loftsson (2009) focused on detecting noisy instances in (semi-) automatically annotated POS corpora, leaving the correction to human annotators. The model proposed by Bekker and Goldberger (2016) assumes that all clean labels pass through a noisy channel. One does only observe the noisy labels. The model of the noise channel, as well as the clean labels, are estimated using an EM algorithm. A neural network is then trained on the estimated labels. van den Berg (2016) applied this model to different tasks, obtaining small improvements on NER with automatically annotated data. A disadvantage of this approach i"
W18-3402,D14-1162,0,0.0762878,"entropy loss, except for the noise cleaning component of the noise-cleaning-model which is trained with the absolute error loss like in the original paper. All models are trained for 40 epochs and the weights of the best performing epoch are selected according to the F1 score on the development set. Adam (Kingma and Ba, 2015) is used for stochastic optimization. tries other than Britain until the scientific” where ”Britain” is the target word with label y = LOC. Sentence boundaries are padded. We encode the words using the 300-dimensional GloVe vectors trained on cased text from Common Crawl (Pennington et al., 2014). The base-model uses a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with state size 300 to encode the input. Then a dense layer is applied with size 100 and ReLU activation (Glorot et al., 2011). Afterwards, the softmax layer is used for classification. This model is only trained on the clean data C. The noise-model is built upon the base model and uses the noise layer architecture explained in section 3. First, the model is trained without noise layer for one epoch on the clean data. Then, we alternate between training with the noise layer on the noisy data and without the noise lay"
W18-3402,P17-1107,0,0.0233171,"cleaning network learns to map noisy labels to clean ones. The second network is used to learn the actual image classification task from clean and cleaned labels. We compare our approach to this idea in the experiments. Existing work showed the importance of handling label noise. Zhu and Wu (2004) suggested that noise in labels tends to be more harmful than noise in features. Beigman and Beigman Klebanov (2009) showed that annotation noise in difficult instances can deteriorate the performance even on simple instances that would have been classified correctly in the absence of the hard cases. Rehbein and Ruppenhofer (2017) presented a technique for detecting annotation noise in automatically labeled POS and NER tags in an active learning scheme. It requires, however, several sources of automatic annotations and human supervision. Similarly, Rocio et al. (2007) and Loftsson (2009) focused on detecting noisy instances in (semi-) automatically annotated POS corpora, leaving the correction to human annotators. The model proposed by Bekker and Goldberger (2016) assumes that all clean labels pass through a noisy channel. One does only observe the noisy labels. The model of the noise channel, as well as the clean labe"
W18-3402,W03-0419,0,0.564074,"Missing"
W18-5425,Q16-1037,0,0.044622,"theoretical baseline in most cases. 1 1.1 Synthetic datasets and formal languages have long been used for checking the ability of RNNs to capture a particular feature. For example, Elman (1990), Das et al. (1992), or Gers and Schmidhuber (2001) already did such investigations. Recent work in this direction was done by Weiss et al. (2017, 2018). More specifically, the interplay of RNNs with certain grammatical constructs, brackets and Dyck languages has been the subject of several studies. Karpathy et al. (2016) show that RNNs are capable of capturing bracket structures on real-world datasets. Linzen et al. (2016) study the application of LSTMs to certain grammatical phenomena. RNNs and their variants have been used for recognizing Dyck words (Kalinke and Lehmann, 1998; Deleu and Dureau, 2016). Li et al. (2017) evaluate their nonlinear weighted finite automata model on a Dyck language. Most recently, Bernardy (2018) conducted a very similar study to ours on Dyck languages with a slightly different focus. Introduction Brackets are a challenge for language models. They regularly appear in texts, they typically produce long-range dependencies, and a failure to properly close them is readily recognized by"
W18-5425,D16-1154,1,0.897049,"Missing"
W18-5425,P18-2117,0,0.0940176,"Missing"
W18-6546,P16-2008,0,0.0339663,"Missing"
W18-6546,C90-3045,0,0.213632,"the equivalent of a CFG derivation for the tree consisting of rules like but → First Next, First → have, have → Arg0 Arg1, and so on. These rules would be very general, but the derivation then requires many more steps. The third option, illustrating the appeal of using a tree substitution grammar, involves elementary trees of intermediate size, like those in Figure 3. The rules in Figure 3 represent a combination Synchronous TSGs Synchronous tree substitution grammars (TSGs) are a subset of synchronous tree adjoining grammars, both of which represent the relationships between pairs of trees (Shieber and Schabes, 1990; Eisner, 2003). A tree substitution grammar consists of a set of elementary trees which can be used to expand non-terminal nodes into a complete tree. Consider the example in Figure 2, which shows the text plan and logical form trees for the sentence, Sonia Rose has very good food quality, but Bienvenue has excellent food quality. The logical form in this figure could be derived 392 5 of small, CFG-like rules (e.g. the elementary tree rooted at but), larger trees representing memorized chunks (i.e. the rule involving Bienvenue), and intermediate trees, like the one including have → quality →"
W18-6546,P03-2041,0,0.139378,"rivation for the tree consisting of rules like but → First Next, First → have, have → Arg0 Arg1, and so on. These rules would be very general, but the derivation then requires many more steps. The third option, illustrating the appeal of using a tree substitution grammar, involves elementary trees of intermediate size, like those in Figure 3. The rules in Figure 3 represent a combination Synchronous TSGs Synchronous tree substitution grammars (TSGs) are a subset of synchronous tree adjoining grammars, both of which represent the relationships between pairs of trees (Shieber and Schabes, 1990; Eisner, 2003). A tree substitution grammar consists of a set of elementary trees which can be used to expand non-terminal nodes into a complete tree. Consider the example in Figure 2, which shows the text plan and logical form trees for the sentence, Sonia Rose has very good food quality, but Bienvenue has excellent food quality. The logical form in this figure could be derived 392 5 of small, CFG-like rules (e.g. the elementary tree rooted at but), larger trees representing memorized chunks (i.e. the rule involving Bienvenue), and intermediate trees, like the one including have → quality → food. In these"
W18-6546,W17-3518,0,0.0314461,"ms as training input. In learning a synchronous TSG, the model presented here aims to avoid using hand-crafted rule templates, which are more dependent on the specific representation chosen for surface realizer input. As mentioned in the introduction, there have been a number of attempts in recent years to learn end-to-end generation systems which produce text directly from database records (Konstas and Lapata, 2012), dialogue acts with slot-value pairs (Mairesse et al., 2010; Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016), or semantic triples like those used in the recent WebNLG challenge (Gardent et al., 2017). In contrast, we assume that content selection and discourse structuring are handled before sentence planning. In principle, however, our methods can be applied to any generation subtask involving tree-to-tree mappings. As a testbed during development we used the SPaRKy Restaurant Corpus (2007), a corpus of restaurant recommendations and comparisons generated by a hand-crafted NLG system. While the controlled nature of this corpus is ideal for testing during development, our future evaluations will also use the more varied Extended SRC (Howcroft et al., 2017).4 After training on about 700 TP-"
W18-6546,D15-1199,0,0.0708543,"Missing"
W18-6546,P06-1064,0,0.0274435,"· ) ∼DP(1.0, Uniform({l· })) 5.3 (8) (9) (10) Sampling Our Gibbs sampler adapts the blocked sampling approach of (Cohn et al., 2010) to synchronous grammars. For each text in the corpus, we resample a synchronous derivation for the entire text before updating the associated model parameters. N |l ∼DP(1.0, Uniform({n ∈ corpus})) (4) 6 A|n ∼DP(1.0, Uniform({a ∈ corpus})) (5) While our pipeline can in principle work with any reversible parser-realizer, our current implementation uses OpenCCG2 (White, 2006; White and Rajkumar, 2012). We use the broad-coverage grammar for English based on CCGbank (Hockenmaier, 2006). The ‘logical forms’ associated with this grammar are more or less syntactic in nature, encoding the lemmas to be used, the dependencies among them, and morphosyntactic annotations in a dependency semantics. Parsing the corpus with OpenCCG provides the LFs we use for training. After training the model, we have a collection of synchronous TSG rules which can be applied to (unseen) text plans to produce new LFs. For this rule application we use Alto3 (Koller and Kuhlmann, 2012) because of its efficient implementation of parsing for synchronous grammars. The final stage in the generation pipelin"
W18-6546,W06-1403,0,0.0560587,"appropriate, while l similarly represents tree locations. 5.2 (7) P (lTP , lLF ) =P (lTP )P (lLF ) P (l· ) ∼DP(1.0, Uniform({l· })) 5.3 (8) (9) (10) Sampling Our Gibbs sampler adapts the blocked sampling approach of (Cohn et al., 2010) to synchronous grammars. For each text in the corpus, we resample a synchronous derivation for the entire text before updating the associated model parameters. N |l ∼DP(1.0, Uniform({n ∈ corpus})) (4) 6 A|n ∼DP(1.0, Uniform({a ∈ corpus})) (5) While our pipeline can in principle work with any reversible parser-realizer, our current implementation uses OpenCCG2 (White, 2006; White and Rajkumar, 2012). We use the broad-coverage grammar for English based on CCGbank (Hockenmaier, 2006). The ‘logical forms’ associated with this grammar are more or less syntactic in nature, encoding the lemmas to be used, the dependencies among them, and morphosyntactic annotations in a dependency semantics. Parsing the corpus with OpenCCG provides the LFs we use for training. After training the model, we have a collection of synchronous TSG rules which can be applied to (unseen) text plans to produce new LFs. For this rule application we use Alto3 (Koller and Kuhlmann, 2012) because"
W18-6546,W15-4704,1,0.878594,"Missing"
W18-6546,W12-4616,0,0.0237976,"ntation uses OpenCCG2 (White, 2006; White and Rajkumar, 2012). We use the broad-coverage grammar for English based on CCGbank (Hockenmaier, 2006). The ‘logical forms’ associated with this grammar are more or less syntactic in nature, encoding the lemmas to be used, the dependencies among them, and morphosyntactic annotations in a dependency semantics. Parsing the corpus with OpenCCG provides the LFs we use for training. After training the model, we have a collection of synchronous TSG rules which can be applied to (unseen) text plans to produce new LFs. For this rule application we use Alto3 (Koller and Kuhlmann, 2012) because of its efficient implementation of parsing for synchronous grammars. The final stage in the generation pipeline is to realize these LFs using OpenCCG, optionally performing reranking on the resulting texts. Some examples of the resulting texts are provided in the next section. HDP for sTSG Derivations Our synchronous TSG model has two additional distributions: (1) a distribution over pairs of TP and LF elementary trees; and (2) a distribution over pairs of tree locations representing the probability of those locations being aligned to each other. Similarly to the generative story for"
W18-6546,D12-1023,0,0.0609381,"Missing"
W18-6546,N12-1093,0,0.139933,"we are working with are the product of document planning, and we use an existing parser-realizer for surface realization. This allows us to constrain the learning problem by limiting our search to the set of tree-to-tree mappings which produce valid input for the surIntroduction Developing and adapting natural language generation (NLG) systems for new domains requires substantial human effort and attention, even when using off-the-shelf systems for surface realization. This observation has spurred recent interest in automatically learning end-to-end generation systems (Mairesse et al., 2010; Konstas and Lapata, 2012; Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016); however, these approaches tend to use shallow meaning representations (Howcroft et al., 2017) and do not make effective use of prior work on surface realization to constrain the learning problem or to ensure grammaticality in the resulting texts. Based on these observations, we propose a Bayesian nonparametric approach to learning sentence planning rules for a conventional NLG system. Making use of existing systems for surface realization along with more sophisticated meaning representations allows us to cast the problem as a grammar inductio"
W19-0421,Q18-1034,0,0.0247086,"mic words the same vector representation, i.e., words that share the same spelling but have different meanings obtain the same representation. For example, the word “kiwi” can signify either a green fruit, a bird or, in informal contexts, the New Zealand dollar, which are three semantically distinct concepts. If only a single vector representation is used, then this representation is likely to primarily reflect the word’s most prominent sense, while neglecting other meanings (see Figure 1). More generally, a word vector may be a linear superposition of features of multiple unrelated meanings (Arora et al., 2018), resulting in incoherent vector spaces. In recent years, several ideas have been proposed to overcome this problem. They have in common that they obtain different vector representations for the different meanings of polysemes or homonyms. Most prior work only evaluates these multi-sense vectors on single word benchmarks, however, and there is comparably little evidence for the benefits of using these embeddings in other applications. One multi-word task that suffers from the presence of polysemy and homonymy is the building of a reverse dictionary that can take definitions of words as input a"
W19-0421,P17-1151,0,0.0233439,"ing prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corresponding part-of-speech or named entity tag. They obtain an embedding that distinguishes e.g. between the location Washington and the person with the same name. The method requires the input data to be tagged with POS or NE tags. Athiwaratkun and Wilson (2017) represent multiple meanings as a mixture of Gaussian distributions. The number of senses per word is fixed globally to the number of Gaussian components. Raganato et al. (2017) and Pesaranghader et al. (2018) use bidirectional LSTMs to learn a mapping between words and multiple senses (not sense vectors) as a supervised sequence prediction task requiring sense-tagged text. An extensive survey on further ideas and work regarding vector representations of meaning is given by Camacho-Collados and Pilehvar (2018). Tang et al. (2018) analyzed different attention mechanisms in the specific context"
W19-0421,Q17-1010,0,0.0482508,"different embeddings, the reverse dictionary task and the corresponding architecture. We also motivate the use of multi-sense embeddings for the target and input vectors with qualitative examples and a quantitative analysis. 2.1 Single- and Multi-Sense Word Embeddings A single-sense word embedding es maps a word or token to an l-dimensional vector representation, i.e. es (wi ) = di ∈ Rl for a word wi . They are often pre-trained on large amounts of unlabeled text and serve as a fundamental building block in many neural NLP models. Popular word embeddings include word2vec, GloVe and fastText (Bojanowski et al., 2017). If a word has several meanings, these are still mapped to just a single vector representation. Multi-sense word embeddings em overcome this limitation by mapping each word wi to a list of sense vectors em (wi ) = (di1 , ..., dik ), where k is the number of senses that one considers wi to have. The vector dij then represents one sense of the given word. This difference is visualized in Figure 2. Often, these embeddings can also be pre-trained on unlabeled text. A discussion of different multi-sense word embeddings is given in Section 5. 2.2 Reverse Dictionaries A reverse dictionary is a tool"
W19-0421,D18-1181,0,0.0234915,"Related Work Hill et al. (2016) proposed to map dictionary definitions to vectors both for the practical application of reverse dictionaries as well as to study representations of phrases and sequences. In this setting, Bastos (2018) experimented with recursive neural networks and additional part-of-speech information. Independently of Hill et al., Scheepers et al. (2018) also used dictionary definitions to evaluate ways to compose sequences of words. They studied different single-sense word embeddings and composition methods such as vector addition and recurrent neural networks. The work by Bosc and Vincent (2018) improves word embeddings with an auto-encoder structure that goes from the target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vecto"
W19-0421,P17-1191,0,0.0213981,"s to evaluate ways to compose sequences of words. They studied different single-sense word embeddings and composition methods such as vector addition and recurrent neural networks. The work by Bosc and Vincent (2018) improves word embeddings with an auto-encoder structure that goes from the target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vectors based on the context for predicting prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corr"
W19-0421,Q16-1002,0,0.294253,"everse Dictionaries A reverse dictionary is a tool for authors and writers seeking a word that is on the tip of their tongue. Given a user-provided definition or description, a reverse dictionary attempts to return the corresponding word (Zock and Bilac, 2004). We create a dataset for this task using the WordNet resource (Miller, 1995). For each word sense in this lexical database, we consider the provided gloss description as the input, and the word as the target. the size of something as given by the distance around it → circumference More details about the dataset are given in Section 4.1. Hill et al. (2016) presented a neural network approach for this task and also set it in the wider context of sequence embeddings. Each instance consists of a description, i.e. a sequence of words (w1 , ..., wn ), and a target vector t. Each word of the input sequence wi is mapped with a single-sense word embedding function es (e.g. word2vec) to a vector representation es (wi ). This sequence of vectors is then transformed into a single vector ˆ t = f (es (w1 ), ..., es (wn )). (1) For f , the authors use—among others—a combination of an LSTM (Hochreiter and Schmidhuber, 1997) and a dense layer. The network is t"
W19-0421,D18-1221,0,0.144282,"• For the single-sense baseline, we use the reverse dictionary architecture proposed by Hill et al. (2016), which also serves as the foundation of all the multi-sense models. • In first multi-sense, we experiment with using the first multi-sense vector for every word as a singlesense vector, i.e. vi = di1 . This is motivated by the fact that the WordNet-based multi-sense vectors tend to be roughly ordered by frequency of occurrence (see analysis in Section 4.7). • Random multi-sense evaluates using a randomly selected multi-sense vector. • The model not-pretrained is based on the approach of Kartsaklis et al. (2018). They recently proposed a method to obtain single-sense and multi-sense vector embeddings during training (in contrast to our use of pre-trained embeddings for both). While one of their experiments also evaluates on a reversedictionary setting, their results are unfortunately not directly comparable, as their targets are WordNet synsets and not words. We, therefore, integrate their proposed technique into our architecture in two ways: For the model not pre-trained, we use their equivalent version of vi . This means that we use their code for the training of the single and multi-sense embeddin"
W19-0421,D14-1162,0,0.086862,". In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well. 1 Introduction One problem with popular word embedding methods such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) is that they assign polysemic or homonymic words the same vector representation, i.e., words that share the same spelling but have different meanings obtain the same representation. For example, the word “kiwi” can signify either a green fruit, a bird or, in informal contexts, the New Zealand dollar, which are three semantically distinct concepts. If only a single vector representation is used, then this representation is likely to primarily reflect the word’s most prominent sense, while neglecting other meanings (see Figure 1). More generally, a word vector may be a linear superposition of f"
W19-0421,N18-1202,0,0.0473007,", therefore, integrate their proposed technique into our architecture in two ways: For the model not pre-trained, we use their equivalent version of vi . This means that we use their code for the training of the single and multi-sense embeddings as well as for the creation of vi based on the context and the multi-sense embedding. The model only es pre-trained differs from this in that we use the pre-trained single-sense embedding instead of training it from scratch. • The BERT model belongs to the class of contextual word embeddings. This approach has been rapidly become popular with works by Peters et al. (2018), Radford et al. (2018), Peters et al. (2018b) and Devlin et al. (2018). Instead of using a direct mapping of words to vector representations, these approaches pre-train a neural language model on a large amount of text. The language model’s internal state for each input word is then used as a corresponding word vector representation for a different task. They can be viewed as inducing word vector representations that are specific to the surrounding context. We compare against the current state-of-the-art model BERT (Devlin et al., 2018). For this, the output of BERT’s last Transformer layer i"
W19-0421,D18-1179,0,0.0464118,"Missing"
W19-0421,P17-1170,0,0.0224723,"he target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vectors based on the context for predicting prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corresponding part-of-speech or named entity tag. They obtain an embedding that distinguishes e.g. between the location Washington and the person with the same name. The method requires the input data to be tagged with POS or NE tags. Athiwaratkun and Wilson (2017) represent multiple meanings a"
W19-0421,D16-1174,0,0.0197688,"also used dictionary definitions to evaluate ways to compose sequences of words. They studied different single-sense word embeddings and composition methods such as vector addition and recurrent neural networks. The work by Bosc and Vincent (2018) improves word embeddings with an auto-encoder structure that goes from the target word embedding back to the definition. We consider these three works complementary to ours, as they study different single-sense architectures. In recent years, several approaches to creating multi-sense vector embeddings have been proposed. Rothe and Sch¨utze (2015), Pilehvar and Collier (2016) and Dasigi et al. (2017) use an existing single-sense word embedding and a lexical resource to induce vectors representing different senses of a word. The latter also employ an attention-based approach for creating vectors based on the context for predicting prepositional phrase attachments. Pilehvar et al. (2017) use the same DeConf multi-sense embedding for integrating them in a downstream application. In contrast to our work, they require, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between differen"
W19-0421,N18-2084,0,0.0318724,"ponent of this architecture. The model that uses the embedding training and multi-sense vector selection of Kartsaklis et al. seems to struggle with building good embeddings in this setting with the 300-dimensional embeddings performing somewhat better but still not well. Providing pre-trained single-sense embeddings improves the performance considerably. Although they are not trained task-specifically, the pre-training of the single-sense embeddings on large amounts of unlabeled data seems to result in a very useful embedding space. This is consistent with other works in the literature, e.g. Qi et al. (2018). Our attention based multi-sense vector approach using pre-trained single- and multi-sense embeddings obtains the best results with respect to all four metrics, with the dot product similarity function performing somewhat better than cosine similarity. This shows that using pre-trained multi-sense vectors and selecting the right sense vectors can be beneficial in sequence embedding tasks. 4.7 Study of Senses and Attention In this section, we present a small study to gain more insight into the different senses occurring in the input sequences as well as into the learned attention. This is also"
W19-0421,D17-1120,0,0.029281,"quire, however, a semantic network to do the disambiguation. In Sense2Vec (Trask et al., 2015), the authors create embeddings that distinguish between different meanings given the corresponding part-of-speech or named entity tag. They obtain an embedding that distinguishes e.g. between the location Washington and the person with the same name. The method requires the input data to be tagged with POS or NE tags. Athiwaratkun and Wilson (2017) represent multiple meanings as a mixture of Gaussian distributions. The number of senses per word is fixed globally to the number of Gaussian components. Raganato et al. (2017) and Pesaranghader et al. (2018) use bidirectional LSTMs to learn a mapping between words and multiple senses (not sense vectors) as a supervised sequence prediction task requiring sense-tagged text. An extensive survey on further ideas and work regarding vector representations of meaning is given by Camacho-Collados and Pilehvar (2018). Tang et al. (2018) analyzed different attention mechanisms in the specific context of ambiguous words in machine translation. They limit their approach, however, to single-sense vectors and the established method of using attention over other parts of the sent"
W19-0421,P15-1173,0,0.0441489,"Missing"
W19-0421,D18-1458,0,0.0298887,"Missing"
W19-0421,W04-2105,0,0.227157,"f sense vectors em (wi ) = (di1 , ..., dik ), where k is the number of senses that one considers wi to have. The vector dij then represents one sense of the given word. This difference is visualized in Figure 2. Often, these embeddings can also be pre-trained on unlabeled text. A discussion of different multi-sense word embeddings is given in Section 5. 2.2 Reverse Dictionaries A reverse dictionary is a tool for authors and writers seeking a word that is on the tip of their tongue. Given a user-provided definition or description, a reverse dictionary attempts to return the corresponding word (Zock and Bilac, 2004). We create a dataset for this task using the WordNet resource (Miller, 1995). For each word sense in this lexical database, we consider the provided gloss description as the input, and the word as the target. the size of something as given by the distance around it → circumference More details about the dataset are given in Section 4.1. Hill et al. (2016) presented a neural network approach for this task and also set it in the wider context of sequence embeddings. Each instance consists of a description, i.e. a sequence of words (w1 , ..., wn ), and a target vector t. Each word of the input s"
W19-6106,J10-4006,0,0.0436137,"lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields. 1 Introduction Knowledge of how conceptual structures change over time and how the hierarchical relations among their components evolve is key to the comprehension of language evolution. Recently, the distributional modelling of relationships between concepts has allowed the community to move a bit further in understanding the true mechanisms of semantic organization (Baroni and Lenci, 2010; Kochmar and Briscoe, 2014; Marelli and Baroni, 2015), as well as in better mapping language change in terms of shifts in continuous semantic values (Hamilton et al., 2016; Hellrich and Hahn, 2017; Stewart and Eisenstein, 2017). In the past decades, extensive work has also gone into creating databases of hierarchical conceptual-semantic relationships, the most famous of these ontologies probably being WordNet (Miller, 1995). These hand-made resources are tools of high quality and precision, but they are difficult to reproduce on a diachronic scale (Bizzoni et al., 2014), due to word form chan"
W19-6106,bizzoni-etal-2014-making,1,0.813876,"emantic organization (Baroni and Lenci, 2010; Kochmar and Briscoe, 2014; Marelli and Baroni, 2015), as well as in better mapping language change in terms of shifts in continuous semantic values (Hamilton et al., 2016; Hellrich and Hahn, 2017; Stewart and Eisenstein, 2017). In the past decades, extensive work has also gone into creating databases of hierarchical conceptual-semantic relationships, the most famous of these ontologies probably being WordNet (Miller, 1995). These hand-made resources are tools of high quality and precision, but they are difficult to reproduce on a diachronic scale (Bizzoni et al., 2014), due to word form changes (De Melo, 2014) and shifts in meaning (Depuydt, 2016), which always make it hard to determine “when”, over a period of time, a new lexical hierarchy is in place (Kafe, 2017). A recent attempt to integrate hierarchical structures, typical of lexical ontologies, and the commutative nature of semantic spaces are hyperbolic embeddings (Nickel and Kiela, 2017). Hyperbolic embeddings have shown to be able to learn hierarchically structured, continuous, and lowdimensional semantic spaces from ordered lists of words: it is easy to see how such technology can be of interest f"
W19-6106,de-melo-2014-etymological,0,0.0748106,"Missing"
W19-6106,W18-4503,1,0.764707,"concepts, both in terms of inner structure and in terms of light comparison with contemporary semantic resources (growing Pearson correlation between norm and WordNet senses). We have shown that while the same trends are visible in all four disciplines, Chemistry and Physiology present more accentuated symptoms of hierarchization, while the group of control had even few or no signs of hierarchization. Specialization in scientific language. This work is part of a larger project aimed to trace the linguistic development of scientific language toward an optimal code for scientific communication (Degaetano-Ortlieb and Teich, 2018, 2019). One mayor assumption is the diachronic development towards specialization – as a scientific field develops, it will become increasingly specialized and expert-oriented. Figure 6: Population of the same area of the hyper-disk for Physiology in the first and last epoch. More specialized and technical terms tend to populate the same level in the “hierarchy”. Epoch Physiology I and II Chemistry I and II Galaxy Botany Control 1650 -0.37 -0.42 -0.50 -0.09 -0.06 1700 -0.20 -0.44 -0.35 -0.05 0.67 1750 -0.40 -0.45 -0.43 -0.24 -0.34 1800 -0.42 -0.46 -0.16 -0.22 -0.17 1850 -0.41 -0.46 -0.37 -0.3"
W19-6106,D16-1229,0,0.0303691,"towards specialization for selected scientific fields. 1 Introduction Knowledge of how conceptual structures change over time and how the hierarchical relations among their components evolve is key to the comprehension of language evolution. Recently, the distributional modelling of relationships between concepts has allowed the community to move a bit further in understanding the true mechanisms of semantic organization (Baroni and Lenci, 2010; Kochmar and Briscoe, 2014; Marelli and Baroni, 2015), as well as in better mapping language change in terms of shifts in continuous semantic values (Hamilton et al., 2016; Hellrich and Hahn, 2017; Stewart and Eisenstein, 2017). In the past decades, extensive work has also gone into creating databases of hierarchical conceptual-semantic relationships, the most famous of these ontologies probably being WordNet (Miller, 1995). These hand-made resources are tools of high quality and precision, but they are difficult to reproduce on a diachronic scale (Bizzoni et al., 2014), due to word form changes (De Melo, 2014) and shifts in meaning (Depuydt, 2016), which always make it hard to determine “when”, over a period of time, a new lexical hierarchy is in place (Kafe,"
W19-6106,P17-4006,0,0.0223918,"for selected scientific fields. 1 Introduction Knowledge of how conceptual structures change over time and how the hierarchical relations among their components evolve is key to the comprehension of language evolution. Recently, the distributional modelling of relationships between concepts has allowed the community to move a bit further in understanding the true mechanisms of semantic organization (Baroni and Lenci, 2010; Kochmar and Briscoe, 2014; Marelli and Baroni, 2015), as well as in better mapping language change in terms of shifts in continuous semantic values (Hamilton et al., 2016; Hellrich and Hahn, 2017; Stewart and Eisenstein, 2017). In the past decades, extensive work has also gone into creating databases of hierarchical conceptual-semantic relationships, the most famous of these ontologies probably being WordNet (Miller, 1995). These hand-made resources are tools of high quality and precision, but they are difficult to reproduce on a diachronic scale (Bizzoni et al., 2014), due to word form changes (De Melo, 2014) and shifts in meaning (Depuydt, 2016), which always make it hard to determine “when”, over a period of time, a new lexical hierarchy is in place (Kafe, 2017). A recent attempt t"
W19-6106,L16-1305,1,0.844069,"1,335,484 1,615,564 1,446,900 1,408,473 2,613,486 2,028,140 4,610,380 5,889,353 31,952,725 lemmas 369,718 687,285 466,795 581,821 615,770 383,186 427,016 473,164 804,523 919,169 734,938 1,146,489 1,052,006 1,043,913 1,298,978 1,136,581 1,064,613 2,035,107 1,565,654 3,585,299 4,474,432 24,866,457 sentences 10,860 17,957 13,230 17,886 23,338 17,510 12,499 16,444 26,673 34,162 27,506 41,412 37,082 36,727 45,666 42,998 43,701 81,500 70,745 146,085 202,488 966,469 Table 1: Corpus statistics of the RSC per decade. Methodology Data As our data set, we use the Royal Society Corpus (RSC; version 4.0; Kermes et al. (2016))1 , containing around 10.000 journal articles of the Transactions and Proceedings of the Royal Society in London (approx. 32 million tokens). The time span covered is from 1665 to 1869 and the corpus is split up into five main periods (1650: 16651699, 1700: 1700-1749, 1750: 1750-1799, 1800: 1800-1849, 1850: 1850-1869). As meta-data annotation, the RSC provides e.g. title, author, year, and journal of publication. Crucial for our investigation is the annotation of sci1 We obtained the RSC from the CLARIN-D repository at http://hdl.handle.net/21.11119/0000-0001-7E8B-6. 2.2 Approach Our approach"
W19-6106,C14-1164,0,0.0156416,"her than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields. 1 Introduction Knowledge of how conceptual structures change over time and how the hierarchical relations among their components evolve is key to the comprehension of language evolution. Recently, the distributional modelling of relationships between concepts has allowed the community to move a bit further in understanding the true mechanisms of semantic organization (Baroni and Lenci, 2010; Kochmar and Briscoe, 2014; Marelli and Baroni, 2015), as well as in better mapping language change in terms of shifts in continuous semantic values (Hamilton et al., 2016; Hellrich and Hahn, 2017; Stewart and Eisenstein, 2017). In the past decades, extensive work has also gone into creating databases of hierarchical conceptual-semantic relationships, the most famous of these ontologies probably being WordNet (Miller, 1995). These hand-made resources are tools of high quality and precision, but they are difficult to reproduce on a diachronic scale (Bizzoni et al., 2014), due to word form changes (De Melo, 2014) and shi"
W19-6106,C18-1117,0,0.0252986,"le retaining a hierarchical structure absent in traditional semantic spaces. In their work Nickel and Kiela (2017) have extensively evaluated hyperbolic embeddings on various tasks (taxonomies, link prediction in networks, lexical entailment), evaluating in particular the ability of these embeddings to infer hierarchical relationships without supervision. This paper is a first attempt in the direction of using hyperbolic semantic spaces to generate diachronic lexical ontologies. While count-based and neural word embeddings have often been applied to historical data sets (Jatowt and Duh, 2014; Kutuzov et al., 2018), and the temporal dimension has even solicited innovative kinds of distributional spaces (Dubossarsky et al., 2015; Bamler and Mandt, 2017), this is to the best of our knowledge the first attempt to model a diachronic corpus through hierarchical, non-euclidean semantic spaces. The literature on hyperbolic embeddings has until now mainly focused on reproducing lexical and social networks from contemporary data (Chamberlain et al., 2017; Nickel and Kiela, 2018). We demonstrate that these kinds of word embeddings, while far from perfect, can capture relevant changes in large scale lexico-semanti"
W19-6106,P19-1313,0,0.0223495,"ean semantic spaces built on the same corpus, so as to attain lists of circa 500 elements, of the kind shown in Table 2. Notwithstanding the predictable amount of noise present in these lists, they keep a relative topical cohesion2 . Based on this selection of words, for each of the five 50-years periods of the RSC, we extract a list of bigrams, i.e. pairs of words of entities of interest. While usually the training input to model hyperbolic word embeddings is based on directional lists of related word pairs (e.g. the Hearst patterns extracted via rule-based text queries (Roller et al., 2018; Le et al., 2019)), we decided to opt for a more “agnostic” method to create input lists for our model. We consider two words as related if they occur in the same sentence, and we do not express any 2 Stop words like adverbs, pronouns, determiners and prepositions are also rare in the lists. hierarchical value or direction between the words constituting the input lists: the input can be viewed as an undirected graph3 . On simple cases, this way of extracting undirected edges appears to work well. As an example, in Figure 1 we show the output space of the Wikipedia article on Maslow’s Hierarchy of Needs (a very"
W19-6106,P18-2057,0,0.0154761,"similarity in euclidean semantic spaces built on the same corpus, so as to attain lists of circa 500 elements, of the kind shown in Table 2. Notwithstanding the predictable amount of noise present in these lists, they keep a relative topical cohesion2 . Based on this selection of words, for each of the five 50-years periods of the RSC, we extract a list of bigrams, i.e. pairs of words of entities of interest. While usually the training input to model hyperbolic word embeddings is based on directional lists of related word pairs (e.g. the Hearst patterns extracted via rule-based text queries (Roller et al., 2018; Le et al., 2019)), we decided to opt for a more “agnostic” method to create input lists for our model. We consider two words as related if they occur in the same sentence, and we do not express any 2 Stop words like adverbs, pronouns, determiners and prepositions are also rare in the lists. hierarchical value or direction between the words constituting the input lists: the input can be viewed as an undirected graph3 . On simple cases, this way of extracting undirected edges appears to work well. As an example, in Figure 1 we show the output space of the Wikipedia article on Maslow’s Hierarch"
wiegand-etal-2012-gold,D10-1101,0,\N,Missing
wiegand-etal-2012-gold,chrupala-klakow-2010-named,1,\N,Missing
wiegand-etal-2012-gold,C92-2082,0,\N,Missing
wiegand-etal-2012-gold,P09-1113,0,\N,Missing
wiegand-etal-2012-gold,P06-1101,0,\N,Missing
wiegand-etal-2012-gold,N03-1011,0,\N,Missing
wiegand-klakow-2010-predictive,H05-1044,0,\N,Missing
wiegand-klakow-2010-predictive,D08-1013,0,\N,Missing
wiegand-klakow-2010-predictive,R09-1034,0,\N,Missing
wiegand-klakow-2010-predictive,P06-2079,0,\N,Missing
wiegand-klakow-2010-predictive,J04-3002,0,\N,Missing
wiegand-klakow-2010-predictive,P02-1053,0,\N,Missing
wiegand-klakow-2010-predictive,D09-1063,0,\N,Missing
wiegand-klakow-2010-predictive,W02-1011,0,\N,Missing
xu-klakow-2010-paragraph,D08-1027,0,\N,Missing
xu-klakow-2010-paragraph,P08-1080,0,\N,Missing
xu-klakow-2010-paragraph,D09-1030,0,\N,Missing
xu-klakow-2010-paragraph,P08-1051,0,\N,Missing
xu-klakow-2010-paragraph,kaisser-lowe-2008-creating,0,\N,Missing
