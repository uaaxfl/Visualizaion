2021.vardial-1.1,Findings of the {V}ar{D}ial Evaluation Campaign 2021,2021,-1,-1,6,0,613,bharathi chakravarthi,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"This paper describes the results of the shared tasks organized as part of the VarDial Evaluation Campaign 2021. The campaign was part of the eighth workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2021. Four separate shared tasks were included this year: Dravidian Language Identification (DLI), Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). DLI was organized for the first time and the other three continued a series of tasks from previous evaluation campaigns."
2021.vardial-1.9,Naive {B}ayes-based Experiments in {R}omanian Dialect Identification,2021,-1,-1,3,1,617,tommi jauhiainen,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,This article describes the experiments and systems developed by the SUKI team for the second edition of the Romanian Dialect Identification (RDI) shared task which was organized as part of the 2021 VarDial Evaluation Campaign. We submitted two runs to the shared task and our second submission was the overall best submission by a noticeable margin. Our best submission used a character n-gram based naive Bayes classifier with adaptive language models. We describe our experiments on the development set leading to both submissions.
2020.wac-1.4,Building Web Corpora for Minority Languages,2020,-1,-1,3,1,616,heidi jauhiainen,Proceedings of the 12th Web as Corpus Workshop,0,"Web corpora creation for minority languages that do not have their own top-level Internet domain is no trivial matter. Web pages in such minority languages often contain text and links to pages in the dominant language of the country. When building corpora in specific languages, one has to decide how and at which stage to make sure the texts gathered are in the desired language. In the {``}Finno-Ugric Languages and the Internet{''} (Suki) project, we created web corpora for Uralic minority languages using web crawling combined with a language identification system in order to identify the language while crawling. In addition, we used language set identification and crowdsourcing before making sentence corpora out of the downloaded texts. In this article, we describe a strategy for collecting textual material from the Internet for minority languages. The strategy is based on the experiences we gained during the Suki project."
2020.vardial-1.1,A Report on the {V}ar{D}ial Evaluation Campaign 2020,2020,-1,-1,6,0,14229,mihaela gaman,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"This paper presents the results of the VarDial Evaluation Campaign 2020 organized as part of the seventh workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with COLING 2020. The campaign included three shared tasks each focusing on a different challenge of language and dialect identification: Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). The campaign attracted 30 teams who enrolled to participate in one or multiple shared tasks and 14 of them submitted runs across the three shared tasks. Finally, 11 papers describing participating systems are published in the VarDial proceedings and referred to in this report."
2020.vardial-1.16,Uralic Language Identification ({ULI}) 2020 shared task dataset and the Wanca 2017 corpora,2020,-1,-1,4,1,617,tommi jauhiainen,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,This article introduces the Wanca 2017 web corpora from which the sentences written in minor Uralic languages were collected for the test set of the Uralic Language Identification (ULI) 2020 shared task. We describe the ULI shared task and how the test set was constructed using the Wanca 2017 corpora and texts in different languages from the Leipzig corpora collection. We also provide the results of a baseline language identification experiment conducted using the ULI 2020 dataset.
2020.vardial-1.21,Experiments in Language Variety Geolocation and Dialect Identification,2020,-1,-1,3,1,617,tommi jauhiainen,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"In this paper we describe the systems we used when participating in the VarDial Evaluation Campaign organized as part of the 7th workshop on NLP for similar languages, varieties and dialects. The shared tasks we participated in were the second edition of the Romanian Dialect Identification (RDI) and the first edition of the Social Media Variety Geolocation (SMG). The submissions of our SUKI team used generative language models based on Naive Bayes and character n-grams."
2020.tlt-1.11,{A}kkadian Treebank for early Neo-Assyrian Royal Inscriptions,2020,-1,-1,4,0,14357,mikko luukko,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,0,None
2020.lrec-1.407,The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope,2020,4,1,34,0.137101,60,georg rehm,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions."
2020.lrec-1.433,Automated Phonological Transcription of {A}kkadian Cuneiform Text,2020,-1,-1,4,0,14358,aleksi sahala,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Akkadian was an East-Semitic language spoken in ancient Mesopotamia. The language is attested on hundreds of thousands of cuneiform clay tablets. Several Akkadian text corpora contain only the transliterated text. In this paper, we investigate automated phonological transcription of the transliterated corpora. The phonological transcription provides a linguistically appealing form to represent Akkadian, because the transcription is normalized according to the grammatical description of a given dialect and explicitly shows the Akkadian renderings for Sumerian logograms. Because cuneiform text does not mark the inflection for logograms, the inflected form needs to be inferred from the sentence context. To the best of our knowledge, this is the first documented attempt to automatically transcribe Akkadian. Using a context-aware neural network model, we are able to automatically transcribe syllabic tokens at near human performance with 96{\%} recall @ 3, while the logogram transcription remains more challenging at 82{\%} recall @ 3."
2020.lrec-1.479,{B}aby{FST} - Towards a Finite-State Based Computational Model of Ancient Babylonian,2020,-1,-1,4,0,14358,aleksi sahala,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Akkadian is a fairly well resourced extinct language that does not yet have a comprehensive morphological analyzer available. In this paper we describe a general finite-state based morphological model for Babylonian, a southern dialect of the Akkadian language, that can achieve a coverage up to 97.3{\%} and recall up to 93.7{\%} on lemmatization and POS-tagging task on token level from a transcribed input. Since Akkadian word forms exhibit a high degree of morphological ambiguity, in that only 20.1{\%} of running word tokens receive a single unambiguous analysis, we attempt a first pass at weighting our finite-state transducer, using existing extensive Akkadian corpora which have been partially validated for their lemmas and parts-of-speech but not the entire morphological analyses. The resultant weighted finite-state transducer yields a moderate improvement so that for 57.4{\%} of the word tokens the highest ranked analysis is the correct one. We conclude with a short discussion on how morphological ambiguity in the analysis of Akkadian could be further reduced with improvements in the training data used in weighting the finite-state transducer as well as through other, context-based techniques."
W19-1409,Language and Dialect Identification of Cuneiform Texts,2019,18,0,4,1,617,tommi jauhiainen,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"This article introduces a corpus of cuneiform texts from which the dataset for the use of the Cuneiform Language Identification (CLI) 2019 shared task was derived as well as some preliminary language identification experiments conducted using that corpus. We also describe the CLI dataset and how it was derived from the corpus. In addition, we provide some baseline language identification results using the CLI dataset. To the best of our knowledge, the experiments detailed here represent the first time that automatic language identification methods have been used on cuneiform data."
W19-1419,Discriminating between {M}andarin {C}hinese and {S}wiss-{G}erman varieties using adaptive language models,2019,-1,-1,2,1,617,tommi jauhiainen,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"This paper describes the language identification systems used by the SUKI team in the Discriminating between the Mainland and Taiwan variation of Mandarin Chinese (DMT) and the German Dialect Identification (GDI) shared tasks which were held as part of the third VarDial Evaluation Campaign. The DMT shared task included two separate tracks, one for the simplified Chinese script and one for the traditional Chinese script. We submitted three runs on both tracks of the DMT task as well as on the GDI task. We won the traditional Chinese track using Naive Bayes with language model adaptation, came second on GDI with an adaptive version of the HeLI 2.0 method, and third on the simplified Chinese track using again the adaptive Naive Bayes."
W18-3907,Iterative Language Model Adaptation for {I}ndo-{A}ryan Language Identification,2018,0,4,3,1,617,tommi jauhiainen,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"This paper presents the experiments and results obtained by the SUKI team in the Indo-Aryan Language Identification shared task of the VarDial 2018 Evaluation Campaign. The shared task was an open one, but we did not use any corpora other than what was distributed by the organizers. A total of eight teams provided results for this shared task. Our submission using a HeLI-method based language identifier with iterative language model adaptation obtained the best results in the shared task with a macro F1-score of 0.958."
W18-3915,{H}e{LI}-based Experiments in Discriminating Between {D}utch and {F}lemish Subtitles,2018,0,2,3,1,617,tommi jauhiainen,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"This paper presents the experiments and results obtained by the SUKI team in the Discriminating between Dutch and Flemish in Subtitles shared task of the VarDial 2018 Evaluation Campaign. Our best submission was ranked 8th, obtaining macro F1-score of 0.61. Our best results were produced by a language identifier implementing the HeLI method without any modifications. We describe, in addition to the best method we used, some of the experiments we did with unsupervised clustering."
W18-3929,{H}e{LI}-based Experiments in {S}wiss {G}erman Dialect Identification,2018,0,3,3,1,617,tommi jauhiainen,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"In this paper we present the experiments and results by the SUKI team in the German Dialect Identification shared task of the VarDial 2018 Evaluation Campaign. Our submission using HeLI with adaptive language models obtained the best results in the shared task with a macro F1-score of 0.686, which is clearly higher than the other submitted results. Without some form of unsupervised adaptation on the test set, it might not be possible to reach as high an F1-score with the level of domain difference between the datasets of the shared task. We describe the methods used in detail, as well as some additional experiments carried out during the shared task."
W17-1212,Evaluating {H}e{LI} with Non-Linear Mappings,2017,10,1,2,1,617,tommi jauhiainen,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"In this paper we describe the non-linear mappings we used with the Helsinki language identification method, HeLI, in the 4th edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the VarDial 2017 workshop. Our SUKI team participated on the closed track together with 10 other teams. Our system reached the 7th position in the track. We describe the HeLI method and the non-linear mappings in mathematical notation. The HeLI method uses a probabilistic model with character n-grams and word-based backoff. We also describe our trials using the non-linear mappings instead of relative frequencies and we present statistics about the back-off function of the HeLI method."
W17-0209,{OCR} and post-correction of historical {F}innish texts,2017,8,2,3,1,28221,senka drobac,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
W17-0221,Evaluation of language identification methods using 285 languages,2017,0,6,2,1,617,tommi jauhiainen,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
W16-4820,"{H}e{LI}, a Word-Based Backoff Method for Language Identification",2016,0,4,2,1,617,tommi jauhiainen,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"In this paper we describe the Helsinki language identification method, HeLI, and the resources we created for and used in the 3rd edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the VarDial 2016 workshop. The shared task comprised of a total of 8 tracks, of which we participated in 7. The shared task had a record number of participants, with 17 teams providing results for the closed track of the test set A. Our system reached the 2nd position in 4 tracks (A closed and open, B1 open and B2 open) and in this paper we are focusing on the methods and data used for those tracks. We describe our word-based backoff method in mathematical notation. We also describe how we selected the corpus we used in the open tracks."
W16-2406,Data-Driven Spelling Correction using Weighted Finite-State Methods,2016,21,3,3,1,1308,miikka silfverberg,Proceedings of the {SIGFSM} Workshop on Statistical {NLP} and Weighted Automata,0,"This paper presents two systems for spelling correction formulated as a sequence labeling task. One of the systems is an unstructured classifier and the other one is structured. Both systems are implemented using weighted finite-state methods. The structured system delivers stateof-the-art results on the task of tweet normalization when compared with the recent AliSeTra system introduced by Eger et al. (2016) even though the system presented in the paper is simpler than AliSeTra because it does not include a model for input segmentation. In addition to experiments on tweet normalization, we present experiments on OCR post-processing using an Early Modern Finnish corpus of OCR processed newspaper text."
W15-5408,Discriminating Similar Languages with Token-Based Backoff,2015,6,9,3,1,617,tommi jauhiainen,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,In this paper we describe the language identification system built within the Finno-Ugric Languages and the Internet project for the Discriminating between Similar Languages (DSL) shared task in LT4VarDial workshop at RANLP-2015. The system reached fourth place in normal closed submissions (94.7% accuracy) and second place in closed submissions with the named entities blinded (93.0% accuracy).
W15-4806,Automated Lossless Hyper-Minimization for Morphological Analyzers,2015,9,0,3,1,28221,senka drobac,"Proceedings of the 12th International Conference on Finite-State Methods and Natural Language Processing 2015 ({FSMNLP} 2015 D{\\\u}sseldorf)""",0,"This paper presents a fully automated lossless hyper-minimization method for finitestate morphological analyzers in Xerox lexc formalism. The method utilizes flag diacritics to preserve the structure of the original lexc description in the finite-state analyzer, which results in reduced size of the analyzer. We compare our method against an earlier solution by Drobac et al. (2014) which requires manual selection of flag diacritics and results in slow lookup. We show that our method gives similar size reductions while maintaining fast lookup without requiring any manual input."
W15-1842,Extracting Semantic Frames using hfst-pmatch,2015,3,2,3,1,14359,sam hardwick,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor. We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text."
P14-2043,Part-of-Speech Tagging using Conditional Random Fields: Exploiting Sub-Label Dependencies for Improved Accuracy,2014,13,9,3,1,1308,miikka silfverberg,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We discuss part-of-speech (POS) tagging in presence of large, fine-grained label sets using conditional random fields (CRFs). We propose improving tagging accuracy by utilizing dependencies within sub-components of the fine-grained labels. These sub-label dependencies are incorporated into the CRF model via a (relatively) straightforward feature extraction scheme. Experiments on five languages show that the approach can yield significant improvement in tagging accuracy in case the labels have sufficiently rich inner structure."
kokkinakis-etal-2014-hfst,{HFST}-{S}we{NER} {---} A New {NER} Resource for {S}wedish,2014,25,2,4,0,24566,dimitrios kokkinakis,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Named entity recognition (NER) is a knowledge-intensive information extraction task that is used for recognizing textual mentions of entities that belong to a predefined set of categories, such as locations, organizations and time expressions. NER is a challenging, difficult, yet essential preprocessing technology for many natural language processing applications, and particularly crucial for language understanding. NER has been actively explored in academia and in industry especially during the last years due to the advent of social media data. This paper describes the conversion, modeling and adaptation of a Swedish NER system from a hybrid environment, with integrated functionality from various processing components, to the Helsinki Finite-State Transducer Technology (HFST) platform. This new HFST-based NER (HFST-SweNER) is a full-fledged open source implementation that supports a variety of generic named entity types and consists of multiple, reusable resource layers, e.g., various n-gram-based named entity lists (gazetteers)."
rehm-etal-2014-strategic,"The Strategic Impact of {META}-{NET} on the Regional, National and International Level",2014,47,2,21,0.137101,60,georg rehm,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiativeÂs work throughout Europe in order to boost progress and innovation in our field."
de-smedt-etal-2014-clara,{CLARA}: A New Generation of Researchers in Common Language Resources and Their Applications,2014,68,0,8,0,17515,koenraad smedt,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"CLARA (Common Language Resources and Their Applications) is a Marie Curie Initial Training Network which ran from 2009 until 2014 with the aim of providing researcher training in crucial areas related to language resources and infrastructure. The scope of the project was broad and included infrastructure design, lexical semantic modeling, domain modeling, multimedia and multimodal communication, applications, and parsing technologies and grammar models. An international consortium of 9 partners and 12 associate partners employed researchers in 19 new positions and organized a training program consisting of 10 thematic courses and summer/winter schools. The project has resulted in new theoretical insights as well as new resources and tools. Most importantly, the project has trained a new generation of researchers who can perform advanced research and development in language resources and technologies."
drobac-etal-2014-heuristic,Heuristic Hyper-minimization of Finite State Lexicons,2014,4,5,2,1,28221,senka drobac,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Flag diacritics, which are special multi-character symbols executed at runtime, enable optimising finite-state networks by combining identical sub-graphs of its transition graph. Traditionally, the feature has required linguists to devise the optimisations to the graph by hand alongside the morphological description. In this paper, we present a novel method for discovering flag positions in morphological lexicons automatically, based on the morpheme structure implicit in the language description. With this approach, we have gained significant decrease in the size of finite-state networks while maintaining reasonable application speed. The algorithm can be applied to any language description, where the biggest achievements are expected in large and complex morphologies. The most noticeable reduction in size we got with a morphological transducer for Greenlandic, whose original size is on average about 15 times larger than other morphologies. With the presented hyper-minimization method, the transducer is reduced to 10,1{\%} of the original size, with lookup speed decreased only by 9,5{\%}."
E14-4015,Accelerated Estimation of Conditional Random Fields using a Pseudo-Likelihood-inspired Perceptron Variant,2014,22,0,4,0,35500,teemu ruokolainen,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We discuss a simple estimation approach for conditional random fields (CRFs). The approach is derived heuristically by defining a variant of the classic perceptron algorithm in spirit of pseudo-likelihood for maximum likelihood estimation. The resulting approximative algorithm has a linear time complexity in the size of the label set and contains a minimal amount of tunable hyper-parameters. Consequently, the algorithm is suitable for learning CRFbased part-of-speech (POS) taggers in presence of large POS label sets. We present experiments on five languages. Despite its heuristic nature, the algorithm provides surprisingly competetive accuracies and running times against reference methods."
W13-5616,Nordic and Baltic Wordnets Aligned and Compared through {``}{W}ord{T}ies{''},2013,16,7,5,0,6194,bolette pedersen,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"During the last few years, extensive wordnets have been built locally for the Nordic and Baltic languages applying very different compilation strategies. The aim of the present investigation is to consolidate and examine these wordnets through an alignment via Princeton Core WordNet and thereby compare them along the measures of taxonomical structure, synonym structure, and assigned relations to approximate to a best practice. A common web interface and visualizer xe2x80x9cWordTiesxe2x80x9d is developed to facilitate this purpose. Four bilingual wordnets are automatically processed and evaluated exposing interesting differences between the wordnets. Even if the alignments are judged to be of a good quality, the precision of the translations vary due to considerable differences in hyponymy depth and interpretation of the synset. All seven monolingual and four bilingual wordnets as well as WordTies have been made available via META-SHARE through the META-NORD project."
W13-5619,Baltic and Nordic Parts of the {E}uropean Linguistic Infrastructure,2013,20,1,4,1,17522,inguna skadicna,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"This paper describes scientific, technical, and legal work done on the creation of the linguistic infrastructure for the Nordic and Baltic countries. The paper describes the research on assessment of language technology support for the languages of the Baltic and Nordic countries, work on establishing a language resource sharing infrastructure, and collection and description of linguistic resources. We present improvements necessary to ensure usability and interoperability of language resources, discuss issues related to intellectual property rights for complex resources, and describe extension of infrastructure through integration of language-resource specific repositories. Work on treebanks, wordnets, terminology resources, and finite-state technology is described in more detail. Finally, our approach on ensuring the sustainability of infrastructure is discussed."
niemi-linden-2012-representing,Representing the Translation Relation in a Bilingual {W}ordnet,2012,12,2,2,1,39608,jyrki niemi,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes representing translations in the Finnish wordnet, FinnWordNet (FiWN), and constructing the FiWN database. FiWN was created by translating all the word senses of the Princeton WordNet (PWN) into Finnish and by joining the translations with the semantic and lexical relations of PWN extracted into a relational (database) format. The approach naturally resulted in a translation relation between PWN and FiWN. Unlike many other multilingual wordnets, the translation relation in FiWN is not primarily on the synset level, but on the level of an individual word sense, which allows more precise translation correspondences. This can easily be projected into a synset-level translation relation, used for linking with other wordnets, for example, via Core WordNet. Synset-level translations are also used as a default in the absence of word-sense translations. The FiWN data in the relational database can be converted to other formats. In the PWN database format, translations are attached to source-language words, allowing the implementation of a Web search interface also working as a bilingual dictionary. Another representation encodes the translation relation as a finite-state transducer."
vasiljevs-etal-2012-creation,Creation of an Open Shared Language Resource Repository in the Nordic and Baltic Countries,2012,12,4,7,1,11079,andrejs vasicljevs,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The META-NORD project has contributed to an open infrastructure for language resources (data and tools) under the META-NET umbrella. This paper presents the key objectives of META-NORD and reports on the results achieved in the first year of the project. META-NORD has mapped and described the national language technology landscape in the Nordic and Baltic countries in terms of language use, language technology and resources, main actors in the academy, industry, government and society; identified and collected the first batch of language resources in the Nordic and Baltic countries; documented, processed, linked, and upgraded the identified language resources to agreed standards and guidelines. The three horizontal multilingual actions in META-NORD are overviewed in this paper: linking and validating Nordic and Baltic wordnets, the harmonisation of multilingual Nordic and Baltic treebanks, and consolidating multilingual terminology resources across European countries. This paper also touches upon intellectual property rights for the sharing of language resources."
voutilainen-etal-2012-specifying,"Specifying Treebanks, Outsourcing Parsebanks: {F}inn{T}ree{B}ank 3",2012,5,5,4,0,28693,atro voutilainen,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Corpus-based treebank annotation is known to result in incomplete coverage of mid- and low-frequency linguistic constructions: the linguistic representation and corpus annotation quality are sometimes suboptimal. Large descriptive grammars cover also many mid- and low-frequency constructions. We argue for use of large descriptive grammars and their sample sentences as a basis for specifying higher-coverage grammatical representations. We present an sample case from an ongoing project (FIN-CLARIN FinnTreeBank) where an grammatical representation is documented as an annotator's manual alongside manual annotation of sample sentences extracted from a large descriptive grammar of Finnish. We outline the linguistic representation (morphology and dependency syntax) for Finnish, and show how the resulting `Grammar Definition Corpus' and the documentation is used as a task specification for an external subcontractor for building a parser engine for use in morphological and dependency syntactic analysis of large volumes of Finnish for parsebanking purposes. The resulting corpus, FinnTreeBank 3, is due for release in June 2012, and will contain tens of millions of words from publicly available corpora of Finnish with automatic morphological and dependency syntactic analysis, for use in research on the corpus linguistics and language engineering."
W11-4620,Do wordnets also improve human performance on {NLP} tasks?,2011,4,2,2,1,42395,kristiina muhonen,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,"FinnWordNet is a wordnet for Finnish that complies with the format of the Princeton WordNet (PWN) (Fellbaum, 1998). It was built by translating the Princeton WordNet 3.0 synsets into Finnish by human translators. It is open source and contains 117000 synsets. The Finnish translations were inserted into the PWN structure resulting in a bilingual lexical database. In natural language processing (NLP), wordnets have been used for infusing computers with semantic knowledge assuming that humans already have a sufficient amount of this knowledge. In this paper we present a case study of using wordnets as an electronic dictionary. We tested whether native Finnish speakers benefit from using a wordnet while completing English sentence completion tasks. We found that using either an English wordnet or a bilingual English-Finnish wordnet significantly improves performance in the task. This should be taken into account when setting standards and comparing human and computer performance on these tasks."
W11-4625,Combining Statistical Models for {POS} Tagging using Finite-State Calculus,2011,15,8,2,1,1308,miikka silfverberg,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,We introduce a framework for POS tagging which can incorporate a variety of different information sources such as statistical models and hand-written rules. The information sources are compiled into a set of weighted finite-state transducers and tagging is accomplished using weighted finite-state algorithms. Our aim is to develop a fast and flexible way for trying out different tagger designs and combining them into hybrid systems. We test the applicability of the framework by constructing HMM taggers with augmented lexical models for English and Finnish. We compare our taggers with two existing statistical taggers TnT and Hunpos and find that we achieve superior accuracy.
W11-3314,{META}-{NORD}: Towards Sharing of Language Resources in Nordic and Baltic Countries,2011,6,3,5,1,17522,inguna skadicna,"Proceedings of the Workshop on Language Resources, Technology and Services in the Sharing Paradigm",0,"This paper introduces the META-NORD project which develops Nordic and Baltic part of the European open language resource infrastructure. META-NORD works on assembling, linking across languages, and making widely available the basic language resources used by developers, professionals and researchers to build specific products and applications. The goals of the project, overall approach and specific action lines on wordnets, terminology resources and treebanks are described. Moreover, results achieved in first five months of the project, i.e. language whitepapers, metadata specification and IPR management, are presented."
W09-4614,Weighted Finite-State Morphological Analysis of {F}innish Compounding with {HFST}-{LEXC},2009,9,5,1,1,618,krister linden,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"Finnish has a very productive compounding and a rich inflectional system, which causes ambiguity in the morphological segmentation of compounds made with finite state transducer methods. In order to disambiguate the compound segmentations, we compare three different strategies, which are all cast in the same probabilistic framework and compared for the first time. We present a method for implementing the probabilistic framework as part of the building process of LexC-style morpheme sub-lexicons creating weighted lexical transducers. To implement the structurally disambiguating morphological analyzer, we use the HFST-LEXC tool which is part of the open source Helsinki Finite-State Technology. Using our Finnish test corpus with 53 270 compounds, we demonstrate that it is possible to use non-compound token probabilities to disambiguate the compounding structure. Non-compound token probabilities are easy to obtain from raw data compared with obtaining the probabilities of prefixes of segmented and disambiguated compounds."
W09-4615,Corpus-based Paradigm Selection for Morphological Entries,2009,12,7,1,1,618,krister linden,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"Language software applications encounter new words, e.g., acronyms, technical terminology, loan words, names or compounds of such words. To add new words to a lexicon, we need to indicate their inflectional paradigm. In this article, we evaluate a lexicon-based method augmented with data from a corpus or the internet for selecting the inflectional paradigm of new words in Finnish. As an entry generator often produces numerous suggestions, it is important that the best suggestions be among the first few, otherwise it may become more efficient to create the entries by hand. By generating paradigm suggestions with an entry guesser and then further generating key word forms for the suggested paradigms, we were able to find support for the paradigms in a corpus. Our method has 79-83 % precision and 86-88 % recall, i.e. an F-score of 83-86 %, i.e. the first correctly generated entry is on the average found as the first or the second candidate."
W09-4625,Conflict Resolution Using Weighted Rules in {HFST}-{TWOLC},2009,5,7,2,1,1308,miikka silfverberg,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"In this article we demonstrate a novel way to resolve conicts in two-level grammars by weighting the rules. The rules are transformed into probabilistic constraints, which are allowed to compete with each other. We demonstrate a method to automatically assign weights to the rules. It acts in a similar way as traditional conict resolution, except that traditionally unresolvable left-arrow rule conicts do not cause lexical forms to be ltered out. The two-level lexicon and probabilistic twolevel grammar are combined using the new transducer operation weighted intersecting composition. The result is a weighted lexical transducer. To the best of our knowledge, this is the rst time probabilistic rules have been used to solve two-level rule conicts. The possible applications of probabilistic lexical transducers range from debugging a wed two-level grammars to computer-assisted language learning. We test our method using a twolevel lexicon and grammar compiled with the open source tools HFST-LEXC and HFST-TWOLC."
W04-1808,Discovering Synonyms and Other Related Words,2004,14,18,1,1,618,krister linden,Proceedings of {C}ompu{T}erm 2004: 3rd International Workshop on Computational Terminology,0,"Discovering synonyms and other related words among the words in a document collection can be seen as a clustering problem, where we expect the words in a cluster to be closely related to one another. The intuition is that words occurring in similar contexts tend to convey similar meaning. We introduce a way to use translation dictionaries for several languages to evaluate the rate of synonymy found in the word clusters. We also apply the information radius to calculating similarities between words using a full dependency syntactic feature space, and introduce a method for similarity recalculation during clustering as a fast approximation of the high-dimensional feature space. Finally, we show that 69-79% of the words in the clusters we discover are useful for thesaurus construction."
