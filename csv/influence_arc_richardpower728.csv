bouayad-agha-etal-2002-pills,P98-2173,1,\N,Missing
bouayad-agha-etal-2002-pills,C98-2168,1,\N,Missing
C00-1033,P98-2173,1,0.825795,"and the ability to construct natural language feedback texts to help the author understand the content and the form of the document while it is still under construction. The concluding section explains what needs to be done to ll the gap between the implemented system and the ideal one. 1 An exception is alfresco which takes natural language input, requiring the system to interpret unconstrained natural language (Stock 1991). Avoiding the need for doing this is an important design motivation for wysiwym-based systems. 2 A WYSIWYM-based System for the Authoring of Textual Documents Elsewhere (Power and Scott 1998, Scott et al. 1998, Scott 1999), a new knowledge-editing method called `wysiwym editing&apos; has been introduced and motivated. Wysiwym editing allows a domain expert to edit a knowledge base (kb) by interacting with a feedback text, generated by the system, which presents both the knowledge already de ned and the options for extending and modifying it. Knowledge is added or modi ed by menu-based choices which directly a ect the knowledge base; the result is displayed to the author by means of an automatically generated feedback text: thus `What You See Is What You Meant&apos;. Wysiwym instantiates a"
C00-1033,W98-1427,1,0.840751,"nstruct natural language feedback texts to help the author understand the content and the form of the document while it is still under construction. The concluding section explains what needs to be done to ll the gap between the implemented system and the ideal one. 1 An exception is alfresco which takes natural language input, requiring the system to interpret unconstrained natural language (Stock 1991). Avoiding the need for doing this is an important design motivation for wysiwym-based systems. 2 A WYSIWYM-based System for the Authoring of Textual Documents Elsewhere (Power and Scott 1998, Scott et al. 1998, Scott 1999), a new knowledge-editing method called `wysiwym editing&apos; has been introduced and motivated. Wysiwym editing allows a domain expert to edit a knowledge base (kb) by interacting with a feedback text, generated by the system, which presents both the knowledge already de ned and the options for extending and modifying it. Knowledge is added or modi ed by menu-based choices which directly a ect the knowledge base; the result is displayed to the author by means of an automatically generated feedback text: thus `What You See Is What You Meant&apos;. Wysiwym instantiates a general recent tren"
C00-1033,C98-2168,1,\N,Missing
C00-2093,W98-1411,0,0.0457664,"ark rhetorical relations. We show that text-structuring can be formulated as a Constraint Satisfaction Problem, so that all solutions respecting constraints on text-structure formation and structural compatibility can be eciently generated. Of the many solutions generated by this method, some are stylistically preferable to others; we show how further constraints can be applied in order to select the best versions. Finally, we discuss some extensions such as the generation of indented text structures. 1 Introduction Much recent work on language generation (Rosner and Stede, 1992; Hovy, 1993; Mellish et al., 1998) has made use of discourse representations based on Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Interest has focussed in particular on the problem of building a rhetorical structure (RS) which organizes elementary propositions hierarchically by means of RST relations (Marcu, 1996). There has been less attention to a second problem in text planning, that of realizing the RS by a text structure (TS), in which the material in the RS is distributed among paragraphs, sentences, vertical lists, etc., perhaps linked up by discourse connectives such as `since&apos; and `however&apos;. This task"
C10-2116,P10-2024,1,0.576161,"ection 2 we show that this is not guaranteed, for three reasons. First, there may be OWL functors that represent logically sophisticated concepts which cannot be expressed in non-technical English. Secondly, an OWL axiom may be hard to verbalise because it lacks the right kind of information structure (i.e., because it fails to make a statement about a recognisable topic such as an individual or atomic class). Finally, since arguments can be nested indefinitely, an axiom might contain so much se4 We have discussed elsewhere whether phrases derived in this way provide suitable lexicalisations (Power, 2010), but this topic lies outside the scope of the present paper. 1007 mantic complexity that it cannot be compressed clearly into a single sentence. We then describe (section 3) an empirical analysis of axiom patterns from about 200 ontologies, which investigates whether these potential problems are common in practice. Section 4 discusses the results, and section 5 concludes. 2 2.1 Potential problems in verbalising axioms tunately, OWL syntax requires that all property arguments for the difficult functors are atomic — for FunctionalObjectProperty, for instance, the argument cannot be a complex pr"
C98-2168,C92-3158,0,0.0461466,"this benefit is gained at the cost of a huge increase in the difficulty of obtaining the source. No longer can the domain expert author the document directly by writing a text in natural language. Defining the source becomes a task akin to building an expert system, requiring collaboration between a domain expert (who understands the subjectmatter of the document) and a knowledge engineer (who understands the knowledge representation formalism). Owing to this cost, M-NLG has been applied mainly in contexts where the knowledge base is already available, having been created for another purpose (Iordanskaja et al., 1992; Goldberg et al., 1994); for discussion see Reiter and Mellish (1993). Is there any way in which a domain expert might author a knowledge base without going through this time-consuming and costly collaboration with a knowledge engineer? Assuming that some kind of mediation is needed between domain expert and knowledge formalism, the only alternative is to provide easier tools for editing knowledge bases. Some knowledge management projects have experimented with graphical presentations which allow editing by direct manipulation, so that there is no need to learn the syntax of a programming lan"
C98-2168,W96-0505,1,0.709381,"ight author a knowledge base without going through this time-consuming and costly collaboration with a knowledge engineer? Assuming that some kind of mediation is needed between domain expert and knowledge formalism, the only alternative is to provide easier tools for editing knowledge bases. Some knowledge management projects have experimented with graphical presentations which allow editing by direct manipulation, so that there is no need to learn the syntax of a programming language see for example Skuce and Lethbridge (1995). This approach has also been adopted in two M-NLG systems: ClST (Power and Cavallotto, 1996), which generates social security forms in English, Italian and German; and D R A F T E R (Paris et al., 1995), which generates instructions for software applications in English and French. These projects were the first attempts to produce symbolic authorin9 systems - that is, systems allowing a domain expert with no training in knowledge engineering to author a knowledge base (or symbolic source) from which texts in many languages can be generated. Although helpful, graphical tools for managing knowledge bases remain at best a compromise solution. Diagrams may be easier to understand than log"
C98-2168,W98-1427,1,0.850544,"Missing"
C98-2168,P83-1023,0,0.134724,"Missing"
E03-2005,bouayad-agha-etal-2002-pills,1,0.549639,"-2 system (Power et al., 1998) showed that the knowledge could be more easily edited through direct manipulation of a text. In DRAFTER-3 (van Deemter and Power, 1998), the interface was extended to allow finer control over coreference, so that given the feedback text was converted into a logical formula for submission to the inference engine; it was web-delivered — the user interface was written as a JAVA applet communicating with a server-side generation engine; and it added support for plurals and groups of objects to the WYSIWYM expressive repertoire (Piwek, 2000). PILLS The PILLS project (Bouayad-Agha et al., 2002) demonstrated the application of WYSIWYM technology to produce documentation about pharmaceuticals in multiple languages. From a single &apos;master model&apos; containing information about a particular product, the program could generate ingredient information in three document-types (patient information leaflets, summaries of product characteristics and European Pharmacopia entries) and three languages (English, French and German). The ontology and lexicons include concepts extracted automatically from a large medical database, the Unified Medical Language System (Schultz and Hahn, 2001). As with CLIM"
E03-2005,W98-0608,1,0.653695,"Missing"
E99-1002,P97-1027,0,0.0136254,"finite description if there is no danger of ambiguity, and if no major structural boundary has been passed since the referent was last mentioned. We are not concerned here with the details of this issue (Hofmann, 1989; Walker et al., 1998); in the examples, we have treated the colon in the feedback text as a major structural boundary, so preferring a definite description in the feedback text and a pronoun in the output text. We concentrate here on two contextual features, f o c u s and p r i o r m e n t i o n s . T h e problem of finding suitable identifying properties (Dale and Reiter, 1995; Horacek, 1997) will not be addressed here, although as will be shown our approach could incorporate this work. 10 Proceedings of EACL '99 3 Incorporating c o n t e x t into t h e grammar essary. However, since they are distractors, indefinite descriptions with ordinals should be provided. A requirement on all WYSIWYM systems has been fast response. Every time that the author selects an editing operation on the feedback text, the knowledge base is updated and a new feedback text is generated. Any tangible delay in presenting the updated feedback text is irritating. In pursuit of efficiency, ICONOCLAST employ"
E99-1002,P98-2173,1,0.819913,"attention; whether different referents of the same type (e.g. other books) have been introduced as well. Taking account of such factors poses a tricky problem for Natural Language Generation (NLG), especially in applications in which efficiency (i.e. fast generation of texts) is a priority. This paper proposes a method that allows efficient generation of referring expressions, through a unification grammar, at the cost of some initial effort in tailoring the phrase-structure rules to the current knowledge base. The method was invented to meet the needs of applications using 'WYSIWYM editing' (Power and Scott, 1998), which allow an author to control the content of an automatically generated text without prior training in knowledge engineering. WYSIWYMis based [ procedurej1 oAL METHOD rj ~ put-on j1 _f-[, patch ] ~ I RESTI Figure 1: Network representation of an instruction on the idea of a 'feedback text', i.e. a text, generated by the system, that presents the current content of the knowledge base (however incomplete) along with the set of permitted operations for extending or otherwise editing the knowledge; these operations are provided through pop-up menus which open on spans of the feedback text. Two"
E99-1002,E95-1025,0,\N,Missing
E99-1002,C98-2168,1,\N,Missing
I05-5010,W03-1601,0,0.125807,"hompson, 1983) in which the leaves are elementary propositions, specified either as semantic formulas or as canned text. The following is a simple example, containing one nucleus-satellite relation (REASON) and one multinuclear relation (CON JUNCTION 1 ): Introduction reason NUCLEUS: recommend(doctors, elixir) SATELLITE: conjunction 1: quick-results(elixir) 2: few-side-effects(elixir) Much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (Barzilay and McKeown, 2001; Carroll et al., 1999; Dras, 1999; Inui and Nogami, 2001; Kozlowski et al., 2003; Langkilde and Knight, 1998; Takahashi et al., 2001; Stede, 1999). Our interest in this paper lies instead with variations at the level of text structuring — the way in which propositions are grouped into units like paragraphs, sections, and bulletted lists, and linked rhetorically by discourse connectives such as ‘since’, ‘nevertheless’, and ‘however’. Elsewhere, we have described a text-structuring method in which the options for organising propositions in a text are laid out as a Ignoring variations in the wording of propositions, ICONOCLAST generates over 20 texts realising this input (or"
I05-5010,P98-1116,0,0.223334,"the leaves are elementary propositions, specified either as semantic formulas or as canned text. The following is a simple example, containing one nucleus-satellite relation (REASON) and one multinuclear relation (CON JUNCTION 1 ): Introduction reason NUCLEUS: recommend(doctors, elixir) SATELLITE: conjunction 1: quick-results(elixir) 2: few-side-effects(elixir) Much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (Barzilay and McKeown, 2001; Carroll et al., 1999; Dras, 1999; Inui and Nogami, 2001; Kozlowski et al., 2003; Langkilde and Knight, 1998; Takahashi et al., 2001; Stede, 1999). Our interest in this paper lies instead with variations at the level of text structuring — the way in which propositions are grouped into units like paragraphs, sections, and bulletted lists, and linked rhetorically by discourse connectives such as ‘since’, ‘nevertheless’, and ‘however’. Elsewhere, we have described a text-structuring method in which the options for organising propositions in a text are laid out as a Ignoring variations in the wording of propositions, ICONOCLAST generates over 20 texts realising this input (or many more if a larger reper"
I05-5010,P98-2173,1,0.776152,"on the current problem, as opposed to the general settings illustrated above. The constraint might state, for example, that the proposition recommend(doctors, elixir) should appear at the beginning of the text, thus eliminating Solution 2. Or it might state that the conjunction relation between the other propositions should be realised by a bulletted list, thus eliminating Solution 1. To support constraints of this kind one would need a user interface in which the user can select part of the semantic input, perhaps by clicking on the corresponding part of the text, as in a WYSIWYM interface (Power and Scott, 1998); a dialogue box would then appear allowing a range of constraints specifically directed to the selected fragment. Such an interface would mimic the typical interaction between a human writer and human critic — e.g., the critic might highlight a paragraph and advise the writer to reformat it as a list. vide an encoding that the current ICONOCLAST text-structurer can use? Can we extract sufficient rhetorical and referential information to allow reasonable paraphrases, without depending on a full semantic analysis of the original text? In this section we consider three stages of interpretation,"
I05-5010,P01-1008,0,0.216427,"o our text-structuring system (ICONOCLAST) is a rhetorical structure tree (Mann and Thompson, 1983) in which the leaves are elementary propositions, specified either as semantic formulas or as canned text. The following is a simple example, containing one nucleus-satellite relation (REASON) and one multinuclear relation (CON JUNCTION 1 ): Introduction reason NUCLEUS: recommend(doctors, elixir) SATELLITE: conjunction 1: quick-results(elixir) 2: few-side-effects(elixir) Much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (Barzilay and McKeown, 2001; Carroll et al., 1999; Dras, 1999; Inui and Nogami, 2001; Kozlowski et al., 2003; Langkilde and Knight, 1998; Takahashi et al., 2001; Stede, 1999). Our interest in this paper lies instead with variations at the level of text structuring — the way in which propositions are grouped into units like paragraphs, sections, and bulletted lists, and linked rhetorically by discourse connectives such as ‘since’, ‘nevertheless’, and ‘however’. Elsewhere, we have described a text-structuring method in which the options for organising propositions in a text are laid out as a Ignoring variations in the wor"
I05-5010,E99-1042,0,0.528617,"m (ICONOCLAST) is a rhetorical structure tree (Mann and Thompson, 1983) in which the leaves are elementary propositions, specified either as semantic formulas or as canned text. The following is a simple example, containing one nucleus-satellite relation (REASON) and one multinuclear relation (CON JUNCTION 1 ): Introduction reason NUCLEUS: recommend(doctors, elixir) SATELLITE: conjunction 1: quick-results(elixir) 2: few-side-effects(elixir) Much work on paraphrase generation has focussed on lexical variation and syntactic transformation within individual sentences (Barzilay and McKeown, 2001; Carroll et al., 1999; Dras, 1999; Inui and Nogami, 2001; Kozlowski et al., 2003; Langkilde and Knight, 1998; Takahashi et al., 2001; Stede, 1999). Our interest in this paper lies instead with variations at the level of text structuring — the way in which propositions are grouped into units like paragraphs, sections, and bulletted lists, and linked rhetorically by discourse connectives such as ‘since’, ‘nevertheless’, and ‘however’. Elsewhere, we have described a text-structuring method in which the options for organising propositions in a text are laid out as a Ignoring variations in the wording of propositions,"
I05-5010,C04-1048,0,\N,Missing
I05-5010,W01-0814,0,\N,Missing
I05-5010,J04-4001,1,\N,Missing
I05-5010,C98-1112,0,\N,Missing
I05-5010,C98-2168,1,\N,Missing
J03-2003,W96-0507,0,0.013135,"wes Road, Brighton BN2 4GJ, UK. Email: {firstname.lastname}@itri.bton.ac.uk. † Departament de Tecnologia, University Pompeu Fabra, Barcelona, Spain. Email: Nadjet.Bouayad@ tecm.upf.es. c 2003 Association for Computational Linguistics  Computational Linguistics Volume 29, Number 2 Hovy 1990; DiMarco et al. 1995; Paris et al. 1995; Power and Cavallotto 1996; Lavoie and Rambow 1997; Mittal et al. 1998). In other cases, the text is mapped onto predetermined genre-specific layout patterns—for example, for verbalizing mathematical proofs (Huang and Fiedler 1997) or producing letters for customers (Coch 1996). If we take, as most do, the level of discourse structure as representative of the underlying message of a text, such systems are subject to a fundamental limitation. Simply put, for each message there will be but one possible form of presentation. As an illustration let us briefly consider the well-known consensus architecture for NLG systems proposed by Reiter (1994). This architecture, based on a survey of NLG systems from the 1980s and early 1990s, takes the form of a “pipeline” in which five modules are applied in sequence: content determination, sentence planning, surface generation, mo"
J03-2003,A97-1039,0,0.0290944,"Missing"
J03-2003,A00-1017,0,0.0245699,"Missing"
J03-2003,J98-3004,0,0.0242646,"gha 2001) and the DArtbio system (Bateman et al. 2001)), this is achieved by mapping directly from the underlying discourse structure (Arens and ∗ Information Technology Research Institute, University of Brighton, Lewes Road, Brighton BN2 4GJ, UK. Email: {firstname.lastname}@itri.bton.ac.uk. † Departament de Tecnologia, University Pompeu Fabra, Barcelona, Spain. Email: Nadjet.Bouayad@ tecm.upf.es. c 2003 Association for Computational Linguistics  Computational Linguistics Volume 29, Number 2 Hovy 1990; DiMarco et al. 1995; Paris et al. 1995; Power and Cavallotto 1996; Lavoie and Rambow 1997; Mittal et al. 1998). In other cases, the text is mapped onto predetermined genre-specific layout patterns—for example, for verbalizing mathematical proofs (Huang and Fiedler 1997) or producing letters for customers (Coch 1996). If we take, as most do, the level of discourse structure as representative of the underlying message of a text, such systems are subject to a fundamental limitation. Simply put, for each message there will be but one possible form of presentation. As an illustration let us briefly consider the well-known consensus architecture for NLG systems proposed by Reiter (1994). This architecture,"
J03-2003,J92-4007,0,0.0100792,"skin infection. Of course, NLG systems that ignore the level of document structure would still be able to produce the text in version (9b) from the RST structure in Figure 5, but they would not be able to produce version (9c). Moreover, they could also end up producing the following incorrect text:6 (9c) # Stop using the cream and tell your doctor as soon as possible because you may be allergic to the cream or have a skin infection if your condition gets worse during treatment. A number of other researchers have identified cases in which “orthodox” RST analysis of a text is problematic (e.g., Moore and Pollack 1992; Moser and Moore 1996; Knott et al. 2001). For example, Knott et al. (2001) report on texts from a corpus of museum labels that violate the RST principle of continuous constituency (i.e., adjacent units must be linked by a relation) but are nonetheless coherent. These are cases in which the satellite of a relation is not adjacent to its own nucleus in the text. In all the texts that Knott et al. discuss, the “dislocated” relation is elaboration, and they attribute the source of the problem to the relation itself: elaboration is not, they claim, a proper relation; it is a very weak relation th"
J03-2003,C00-2093,1,0.856801,".g., Halliday 1967; Chomsky and Halle 1968; Crystal 1969; Bolinger 1972; Pierrehumbert 1980; ’t Hart, Collier, and Cohen 1990; Ladd 1996), the same is not true for text layout. Perhaps not surprisingly, therefore, few natural language understanding (NLU) systems use graphical presentational features to aid interpretation, and few natural language generation (NLG) systems attempt to render the output texts in a principled way. Of course, since all texts have a graphical dimension, all NLG systems will, by definition, produce laid-out texts. In all but a few recent cases (the ICONOCLAST system (Power 2000; Bouayad-Agha, Power, and Scott 2000; Bouayad-Agha, Scott, and Power 2001; Bouayad-Agha 2001) and the DArtbio system (Bateman et al. 2001)), this is achieved by mapping directly from the underlying discourse structure (Arens and ∗ Information Technology Research Institute, University of Brighton, Lewes Road, Brighton BN2 4GJ, UK. Email: {firstname.lastname}@itri.bton.ac.uk. † Departament de Tecnologia, University Pompeu Fabra, Barcelona, Spain. Email: Nadjet.Bouayad@ tecm.upf.es. c 2003 Association for Computational Linguistics  Computational Linguistics Volume 29, Number 2 Hovy 1990; DiMarc"
J03-2003,W96-0505,1,0.845092,"Missing"
J03-2003,W94-0319,0,0.030537,"d Rambow 1997; Mittal et al. 1998). In other cases, the text is mapped onto predetermined genre-specific layout patterns—for example, for verbalizing mathematical proofs (Huang and Fiedler 1997) or producing letters for customers (Coch 1996). If we take, as most do, the level of discourse structure as representative of the underlying message of a text, such systems are subject to a fundamental limitation. Simply put, for each message there will be but one possible form of presentation. As an illustration let us briefly consider the well-known consensus architecture for NLG systems proposed by Reiter (1994). This architecture, based on a survey of NLG systems from the 1980s and early 1990s, takes the form of a “pipeline” in which five modules are applied in sequence: content determination, sentence planning, surface generation, morphology, and formatting. Sentence planning maps “conceptual structures into linguistic ones . . . grouping information into clauses and sentences” (Reiter 1994, page 164), but formatting (specified, for example, by LaTEX markup) occurs only in the final formatting stage. In consequence, the organization of material into paragraphs, bulleted lists, etc., is considered o"
J04-4001,P87-1022,0,0.882958,"Missing"
J04-4001,P02-1012,0,0.423594,"Institute, University of Brighton, Brighton BN2 4GJ, U. K. E-mail: Richard.Power@itri.brighton.ac.uk Submission received: 17 October 2002; Revised submission received: 22 May 2004; Accepted for publication: 6 August 2004 c 2004 Association for Computational Linguistics  Computational Linguistics Volume 30, Number 4 in particular pronouns. These are issues which the well-known centering theory (CT) of Grosz, Joshi, and Weinstein (1995; henceforth GJW) is concerned with. Previous algorithms for pronominalization such as those of McCoy and Strube (1999), Henschel, Cheng, and Poesio (2000), and Callaway and Lester (2002) have addressed the task of deciding whether to realize an entity as a pronoun on the basis of given factors such as its syntactic role and discourse history within a given text structure; what is essentially novel in our approach is that we treat referential coherence as a planning problem, on the assumption that obtaining a favorable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for nonambiguous pronoun use. Centering theory provides the basis for such an integrated approach.1 Of course coherence of a text depends on the realization of rhetorical r"
J04-4001,A00-3001,0,0.0354132,"Missing"
J04-4001,P98-1044,0,0.0228841,"that respect the fundamental centering constraint that Cb(Un ) should be the most salient candidate in Un−1 . As stated earlier, two criteria for determining the predecessor have been implemented; the user can select one or the other criterion, thus using the NLG system to test different approaches. Following a linear criterion, the predecessor is simply the proposition that precedes the current proposition in the text, regardless of structural considerations. Following a hierarchical criterion, the predecessor is the most accessible previous proposition, in the sense defined by Veins Theory (Cristea, Ide, and Romary, 1998). For now we assume the criterion is linear. ΣCb(Un ) (potential Cbs of proposition Un ) is given by the intersection between Cf (Un ) and Cf (Un−1 )—that is, all the referents they have in common. The potential Cps are those referents in the current proposition that can be realized as most salient. Obviously this should depend on the linguistic resources available to the generator; the system actually uses a simpler rule based on argument types within the proposition. Table 3 shows the potential Cbs and Cps for the proposition sequence in solution A presented at the beginning of this section"
J04-4001,J95-2003,0,0.970157,"Missing"
J04-4001,W99-0109,1,0.95981,"3). This is also referred to as the backward-looking center or Cb. (The set of entities mentioned in an utterance Un is defined by Constraint 2 as the set of forward-looking centers or Cfs.) It is not entirely clear whether Constraint 1 is to be taken as an empirical claim or as a stipulation that some entity must be designated as Cb, if necessary by constructing an indirect anaphoric link. 2. There is a preference for consecutive utterances within a discourse segment to keep the same entity as the center and for the center to be realized as the highestranked entity or preferred center (Cp). Kibble (1999) dubbed these principles cohe403 Computational Linguistics Volume 30, Number 4 Table 1 Centering transitions. Continue Retain Smooth Shift Rough Shift Cohesion and Salience both hold; same center (or Cb(Un ) undefined), realized as Cp in Un+1 Cohesion only; that is, center remains the same but is not realized as Cp in Un+1 Salience only; center of Un+1 realized as Cp but not equal to Cb(Un ) Neither cohesion nor salience holds sion and salience, respectively. Combinations of these preferences provide the familiar canonical set of transitions shown in Table 1, ranked in the stipulated order of"
J04-4001,J01-4007,1,0.894512,"esent in this article. This general approach is anticipated by McKeown’s (1985) text-planning system, in which referential coherence is taken to be one of the factors determining fluency, though McKeown’s work predates RST and centering. Mittal et al. (1998) apply what we term salience to sentence planning, with the goal of realizing the Cb as subject, though the text planner does not have a goal of attempting to maintain the same Cb. We regard Cheng’s (2000) work on the interaction of centering preferences and aggregation in text planning as complementary to our enterprise. Karamanis (2001), Kibble (2001), and Beaver (2004), have argued for a ranking of the centering principles as opposed to weighting, and indeed Beaver provides a unified formulation of the centering rules and constraints as a ranked set of OT constraints. However, we believe that such a ranking stands in need of empirical justification, and Beaver’s data actually provide little evidence for strict ranking as opposed to weighting of constraints (see Kibble 2003). Constraint satisfaction search was applied by Marcu (1996, 1997) to the far harder task of constructing RST trees given a set of facts and a repertoire of rhetorical"
J04-4001,W99-0108,0,0.0313238,"K. E-mail: r.kibble@gold.ac.uk † Information Technology Research Institute, University of Brighton, Brighton BN2 4GJ, U. K. E-mail: Richard.Power@itri.brighton.ac.uk Submission received: 17 October 2002; Revised submission received: 22 May 2004; Accepted for publication: 6 August 2004 c 2004 Association for Computational Linguistics  Computational Linguistics Volume 30, Number 4 in particular pronouns. These are issues which the well-known centering theory (CT) of Grosz, Joshi, and Weinstein (1995; henceforth GJW) is concerned with. Previous algorithms for pronominalization such as those of McCoy and Strube (1999), Henschel, Cheng, and Poesio (2000), and Callaway and Lester (2002) have addressed the task of deciding whether to realize an entity as a pronoun on the basis of given factors such as its syntactic role and discourse history within a given text structure; what is essentially novel in our approach is that we treat referential coherence as a planning problem, on the assumption that obtaining a favorable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for nonambiguous pronoun use. Centering theory provides the basis for such an integrated approach.1 Of c"
J04-4001,W98-1411,0,0.0497273,"Missing"
J04-4001,J98-3004,0,0.00882021,"as different realizations of an input sequence can be generated by varying control parameters, and one can very quickly see the results of alternative choices. 1.1 Related Work Other researchers have applied CT to generation, though to our knowledge none have applied it to text planning, sentence planning, and pronominalization in the integrated way that we present in this article. This general approach is anticipated by McKeown’s (1985) text-planning system, in which referential coherence is taken to be one of the factors determining fluency, though McKeown’s work predates RST and centering. Mittal et al. (1998) apply what we term salience to sentence planning, with the goal of realizing the Cb as subject, though the text planner does not have a goal of attempting to maintain the same Cb. We regard Cheng’s (2000) work on the interaction of centering preferences and aggregation in text planning as complementary to our enterprise. Karamanis (2001), Kibble (2001), and Beaver (2004), have argued for a ranking of the centering principles as opposed to weighting, and indeed Beaver provides a unified formulation of the centering rules and constraints as a ranked set of OT constraints. However, we believe th"
J04-4001,C00-2093,1,0.765788,"ng a theory of coherence relations, with the following additional assumptions: • There is a one-to-one correspondence between predicates and verbs, so that the options for syntactic realization can be predicted from the argument structure of predicates. Such “shallow” lexicalization appears to be standard in applied NLG systems (Cahill 1999). • Pronominalization is deferred until grammatical relations and word order have been determined. Our exposition will refer to an implemented document generation system, Iconoclast, which uses the technique of constraint satisfaction (van Hentenryck 1989; Power 2000; Power, Scott, and Bouayad-Agha 2003) with CT principles implemented among a set of soft constraints. The Iconoclast system allows the user to specify content and rhetorical structure through an interactive knowledge-base editor and supports fine-grained control over stylistic and layout features. The user-determined rhetorical structure is transformed into a text structure or a set of candidate text structures which respect various text formation rules encoded as hard constraints. Not all of the resulting text structures will give rise to stylistically acceptable documents, and of those whic"
J04-4001,W94-0319,0,0.0155301,"focussing on anaphora resolution (see, e.g., Brennan, Friedman, and Pollard 1987). As stated above, the novel contribution of this article is an integrated treatment of pronominalization and planning, aiming to determine whether the principles underlying the constraints and rules of the theory can be “turned round” and used as planning operators for generating coherent text. We have assumed some familiarity in the foregoing with terms such as text planning and sentence planning. These are among the distinct tasks identified in Reiter’s “consensus architecture” for natural language generation (Reiter 1994): Text planning/content determination: deciding the content of a message and organizing the component propositions into a text structure (typically a tree) Sentence planning: aggregating propositions into clausal units and choosing lexical items corresponding to concepts in the knowledge base; this is the level at which the order of arguments and choice of referring expressions will be determined Linguistic realization: surface details such as agreement and orthography Reiter observed that these functions can often be identified with discrete modules in applied NLG systems and that a de facto"
J04-4001,J00-2005,0,0.262119,"ogy Research Institute, University of Brighton, Brighton BN2 4GJ, U. K. E-mail: Richard.Power@itri.brighton.ac.uk Submission received: 17 October 2002; Revised submission received: 22 May 2004; Accepted for publication: 6 August 2004 c 2004 Association for Computational Linguistics  Computational Linguistics Volume 30, Number 4 in particular pronouns. These are issues which the well-known centering theory (CT) of Grosz, Joshi, and Weinstein (1995; henceforth GJW) is concerned with. Previous algorithms for pronominalization such as those of McCoy and Strube (1999), Henschel, Cheng, and Poesio (2000), and Callaway and Lester (2002) have addressed the task of deciding whether to realize an entity as a pronoun on the basis of given factors such as its syntactic role and discourse history within a given text structure; what is essentially novel in our approach is that we treat referential coherence as a planning problem, on the assumption that obtaining a favorable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for nonambiguous pronoun use. Centering theory provides the basis for such an integrated approach.1 Of course coherence of a text depends on"
J04-4001,J99-3001,0,0.0401004,"d Pollard 1987 [henceforth BFP]). To summarize, our system incorporates the following constraints: cohesion: Cb(Un−1 ) = Cb(Un ) salience: Cp(Un ) = Cb(Un ) cheapness: Cp(Un−1 ) = Cb(Un ) continuity: Cfs(Un−1 ) ∩ Cfs(Un ) = ∅ 2.5 Preferences: Transitions, Pairs, or Sequences? The original version of GJW’s Rule 2 specified that sequences of Continue transitions are preferred over sequences of Retains, and so on; in BFP’s implementation, however, transitions are evaluated incrementally and the preference applies to individual transitions such as Continue versus Retain rather than to sequences. Strube and Hahn (1999) take an intermediate position: In their formulation, pairs of transitions Ui , Uj , Uj , Uk are preferred that are cheap, that is, Cp(Uj ) = Cb(Uk ). Strube and Hahn intended the preference for cheap transition pairs to replace GJW’s Rule 2 in toto, which seems a rather weak requirement. On the other hand the original GJW formulation is difficult to verify, since as Poesio et al. (2002, page 66) found, sequences of multiple occurrences of the same transition type turn out to be relatively rare. Our position is a little more complex, as we do not directly aim to generate particular transiti"
J04-4001,W00-1429,0,\N,Missing
J04-4001,P05-1018,0,\N,Missing
J04-4001,J99-2001,0,\N,Missing
J04-4001,C98-1044,0,\N,Missing
J04-4001,C00-1045,0,\N,Missing
J04-4001,W02-2111,0,\N,Missing
J07-1006,bouayad-agha-etal-2002-pills,1,0.942375,"Missing"
J07-1006,W06-1414,1,0.863094,"Missing"
J07-1006,P98-2173,1,0.914628,"Missing"
J07-1006,W98-1427,1,0.813,"Missing"
J07-1006,N04-1008,0,0.0202788,"Missing"
J07-1006,A83-1002,0,0.779073,"training in question composition. This scenario is found in most corporate domains, especially in applications that are risk-averse. We present a proof-of-concept system we have developed: a question-answering interface to a large repository of medical histories in the area of cancer. We show that the method allows users to successfully and reliably compose complex queries with minimal training. 1. Introduction Where early attempts to build natural language question-answering systems focused on accessing and presenting information held in (closed domain) databases (e.g., Hendrix et al. 1978; Templeton and Burger 1983; Kaplan 1984; Hafner and Godden 1985), the advent of the World Wide Web has led to a shift towards (open domain) collections of texts. However, despite significant advances in open domain question answering since the simple pattern-matching systems of the first TREC competition in 1999, current systems are still largely restricted to simple questions. They can, for example, successfully find answers to questions like Which is the highest peak in Africa? or Who first climbed Kilimanjaro? but they cannot correctly answer more complex questions like: What is the median height of the top twelve h"
J07-1006,P83-1023,0,0.806172,"Missing"
J07-1006,C98-2168,1,\N,Missing
J12-1004,W09-0626,0,0.027063,"Missing"
J12-1004,W09-0620,1,0.604119,"grade this summer, compared to 25.3 per cent 12 months earlier — and just 12 per cent in 1990. (Daily Telegraph, 14th August 2008) The N UM G EN corpus contains 14 articles reporting this story, mostly from UK newspapers; in total, it has nearly 100 articles covering ten stories. The numerical facts found in the corpus include cardinalities (e.g., “300,000 students”) and measures (“28 years”) as well as proportions, but the project focused on proportions as a convenient subset. Elsewhere we have shown that proportions tend to be expressed differently at different locations within a document (Williams and Power 2009). The phrases “more than a quarter” and “25.9 percent” in the example extract provide a convenient illustration of the nature of these differences. First, there is an obvious disparity in precision. Next, the phrases differ in mathematical form (fraction vs. percentage); we have argued that this distinction is conceptual as well as notational, because fractions are accessible to a wider readership than percentages—as testiﬁed by the levels at which they are introduced in the UK mathematics curriculum (Qualiﬁcation and Curriculum Authority 1999). Finally, one of the phrases contains not only a"
J12-1004,W94-0319,0,\N,Missing
J12-1004,J07-1006,1,\N,Missing
P01-1015,W00-1410,1,0.816566,"which all the data levels can be represented in a common framework consisting of networks of typed ‘objects’ connected by typed ‘arrows’. This lingua franca allows NLG modules to manipulate data flexibly and consistently. It also facilitates modular design of NLG systems, and reusability of modules and data sets. However, it does not in itself say anything about how modules in such a system might interact. This paper describes a concrete realisation of the RAGS object and arrows model, OASYS, as applied to a simple but flexible NLG system called RICHES. This is not the first such realisation: Cahill et al., (2000) describes a partial re-implementation of the ‘Caption Generation System’ (Mittal et al., 1999) which includes an objects and arrows ‘whiteboard’. The OASYS system includes more specific proposals for processing and inter-module communication, and RICHES demonstrates how this can be used to support a modular architecture based on small scale functionally-motivated units. 3 OASYS Quote representations These are used to represent literal unanalysed content used by a generator, such as canned text, pictures or tables. OASYS (Objects and Arrows SYStem) is a software library which provides: The rep"
P01-1015,copestake-flickinger-2000-open,0,0.0196598,"7) and FUF/SURGE (Elhadad et al., 1997). Renderer (REND) The Renderer is the module that puts the concrete document together. Guided by the document structure, it produces HTML formatting for the text and positions and references the pictures. Individual sentences are produced for it by LinGO, via the FLO interface. FLO actually processes sentences independently of REND, so when REND makes a request, either the sentence is there already, or the request is queued, and serviced when it becomes available. LinGO The LinGO realiser uses a widecoverage grammar of English in the LKB HPSG framework, (Copestake and Flickinger, 2000). The tactical generation component accepts input in the Minimal Recursion Semantics formalism and produces the target text using a chartdriven algorithm with an optimised treatment of modification (Carroll et al., 1999). No domainspecific tuning of the grammar was required for the RICHES system, only a few additions to the lexicon were necessary. 5 An example: generation in RICHES In this section we show how RICHES generates the first sentence of the example text, Blow your nose so that it is clear and the picture that accompanies the text. The system starts with a rhetorical representation ("
P01-1015,J97-2001,0,0.0233148,"clauses. In addition, SF decides whether a sentence should be imperative, depending on who the reader of the document is (an input parameter to the system). Finalise Lexical Output (FLO) RICHES uses an external sentence realiser component with its own non-RAGS input specification. FLO provides the interface to this realiser, extracting (mostly syntactic) information from OASYS and converting it to the appropriate form for the realiser. Currently, FLO supports the LinGO realiser (Carroll et al., 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al., 1997). Renderer (REND) The Renderer is the module that puts the concrete document together. Guided by the document structure, it produces HTML formatting for the text and positions and references the pictures. Individual sentences are produced for it by LinGO, via the FLO interface. FLO actually processes sentences independently of REND, so when REND makes a request, either the sentence is there already, or the request is queued, and serviced when it becomes available. LinGO The LinGO realiser uses a widecoverage grammar of English in the LKB HPSG framework, (Copestake and Flickinger, 2000). The ta"
P01-1015,A97-1039,0,0.0462862,"example, combining main and subordinate clauses. In addition, SF decides whether a sentence should be imperative, depending on who the reader of the document is (an input parameter to the system). Finalise Lexical Output (FLO) RICHES uses an external sentence realiser component with its own non-RAGS input specification. FLO provides the interface to this realiser, extracting (mostly syntactic) information from OASYS and converting it to the appropriate form for the realiser. Currently, FLO supports the LinGO realiser (Carroll et al., 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al., 1997). Renderer (REND) The Renderer is the module that puts the concrete document together. Guided by the document structure, it produces HTML formatting for the text and positions and references the pictures. Individual sentences are produced for it by LinGO, via the FLO interface. FLO actually processes sentences independently of REND, so when REND makes a request, either the sentence is there already, or the request is queued, and serviced when it becomes available. LinGO The LinGO realiser uses a widecoverage grammar of English in the LKB HPSG framework, (Co"
P01-1015,A00-1017,0,0.0710182,"sity of Sussex Brighton, BN1 9QH, UK johnca@cogs.susx.ac.uk Abstract The RAGS proposals for generic specification of NLG systems includes a detailed account of data representation, but only an outline view of processing aspects. In this paper we introduce a modular processing architecture with a concrete implementation which aims to meet the RAGS goals of transparency and reusability. We illustrate the model with the RICHES system – a generation system built from simple linguisticallymotivated modules. 1 Introduction As part of the RAGS (Reference Architecture for Generation Systems) project, Mellish et al (2000) introduces a framework for the representation of data in NLG systems, the RAGS ‘data model’. This model offers a formally well-defined declarative representation language, which supports the complex and dynamic data requirements of generation systems, e.g. different levels of representation (conceptual to syntax), mixed representations that cut across levels, partial and shared structures and ‘canned’ representations. However  We would like to acknowledge the financial support of the EPSRC (RAGS – Reference Architecture for Generation Systems: grant GR/L77102 to Donia Scott), as well as the"
P01-1015,C00-2093,1,0.828569,"picture should illustrate it. Pic2 The dashed lines indicate flow of information, solid arrows indicate approximately flow of control between modules, double boxes indicate a completely reused module (from another system), while a double box with a dashed outer indicates a module partially reused. Ellipses indicate information sources, as opposed to processing modules. tures, annotated with their SemReps, are part of the picture library, and Media Selection builds small pieces of DocRep referencing the pictures. Document Planner (DP) The Document Planner, based on the ICONOCLAST text planner (Power, 2000) takes the input RhetRep and produces a document structure (DocRep). This specifies aspects such as the text-level (e.g., paragraph, sentence) and the relative ordering of propositions in the DocRep. Its leaves refer to SynReps corresponding to syntactic phrases. This module is pipelined after MS, to make sure that it takes account of any pictures that have been included in the document. Lexical Choice (LC) Lexical choice happens in two stages. In the first stage, LC chooses the lexical items for the predicate of each SynRep. This fixes the basic syntactic structure of the proposition, and the"
P01-1015,W94-0319,0,0.0336301,"ther colleagues at the ITRI, especially Nedjet BouayadAgha. We would also like to acknowledge the contribution of colleagues who worked on the RICHES system previously: Neil Tipper and Rodger Kibble. We are grateful to our anonymous referees for their helpful comments. RAGS, as described in that paper, says very little about the functional structure of an NLG system, or the issues arising from more complex processing regimes (see for example Robin (1994), Inuie et al., (1992) for further discussion). NLG systems, especially end-to-end, applied NLG systems, have many functionalities in common. Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. More recently Cahill et al (1999) attempted to repeat the analysis, but found that while most systems did implement a pipeline, they did not implement the same pipeline – different functionalities occurred in different ways and different orders in different systems. But this survey did identify a number of core functionalities which seem to occur during the execution of most systems. In order to accommodate this result, a ‘process model’ was sketched which aimed to support both pipelines and more complex control r"
P01-1015,J98-3004,0,\N,Missing
P04-3030,C94-2149,0,0.104344,"Missing"
P04-3030,E03-2005,1,0.844256,"oduction of multilingual technical documentation, and the formulation of queries to a database or expert system. In the first case, Wysiwym editing encodes the desired content of the document in an interlingua, from which versions can be generated in mutliple languages; in the second case, it yields a query encoded in a formal query language such as SQL. The benefit is the same in either context: since editing is mediated through a presentation in natural language, there is no need for the user to be acquainted with the formal details of knowledge representation or query languages. Elsewhere (Evans and Power, 2003) we have described a library package for developing Wysiwym applications. This package was a consolidation of work carried out in a series of early applications (Power and Scott, 1998; Piwek et al., 2000; Bouayad-Agha et al., 2002), requiring a very restricted linguistic coverage, especially as regards the range of clausal and nominal patterns. We present here an extension to this library which allows a coverage 2 Editing with simple types patient ARG−1 take ARG−2 aspirin Figure 1: A-box with simple types In early Wysiwym applications, the editing process served to build an A-box like that sho"
P04-3030,P98-2173,1,0.814796,"new ideas: first, a change to the underlying semantic model, replacing atomic entity types with feature structures; secondly, a corresponding change in the user interface, which now offers an extra editing operation (called reconfiguration) through which complex entity types may be modified. The purpose of this paper (and the accompanying demonstration) is to describe these novelties. Introduction Wysiwym (What You See Is What You Meant) is a user-interface technology through which a domain expert can formally encode knowledge by structured editing of an automatically generated feedback text (Power and Scott, 1998). The technology has hitherto addressed two practical contexts: the automatic production of multilingual technical documentation, and the formulation of queries to a database or expert system. In the first case, Wysiwym editing encodes the desired content of the document in an interlingua, from which versions can be generated in mutliple languages; in the second case, it yields a query encoded in a formal query language such as SQL. The benefit is the same in either context: since editing is mediated through a presentation in natural language, there is no need for the user to be acquainted wit"
P04-3030,bouayad-agha-etal-2002-pills,1,\N,Missing
P04-3030,A97-1039,0,\N,Missing
P04-3030,C98-2168,1,\N,Missing
P06-2090,P97-1005,0,0.0489386,"genre emerges (Crowston and Williams 2000). The multi-facetted model capable of hosting new genres wished for by Kwasnik and Crowston (2004), and the adaptive learning system that can identify genre as they emerge announced by Shepherd et al. (2004) are hard to implement. For this reason, the focus of the method proposed below is not to detect emerging genres, but to show a flexible approach capable of giving account of genre hybridism and individualization. Flexible genre classification systems are uncommon in automatic genre classification studies. Apart from two notable exceptions, namely Kessler et al. (1997) and Rehm (2006) whose implementations require extensive manual annotation (Kessler et al., 1997) or analysis (Rehm, 2006), genres are usually classified as single-label discrete entities, relying on the simplified assumption that a document can be assigned to only one genre. In this paper, we propose a tuple representation that maps onto the theoretical characterization of genre suggested above and that can be implemented without much overhead. The implementable tuple includes the following attributes: (genre(s)) of web pages=&lt;linguistic features, HTML, text types, [...]&gt; This tuple means tha"
P06-2090,2006.jeptalnrecital-long.28,1,0.898957,"). While content words or terms show some drawbacks for automatic genre identification (cf. Boese and Howe, 2005), there are several types of linguistic features that return good results, for instance, Biberian features (Biber, 1988). In the model presented here we use a mixture of Biberian features and additional syntactic traits. The total number of features used in this implementation of the model is 100. These features are available online at: text types&gt; of http://www.nltg.brighton.ac.uk/home/Marina.Santini/ 4 Inferential Model The inferential model presented here (partially discussed in Santini (2006a) combines the advantages of deductive and inductive approaches. It is deductive because the cooccurrence and the combination of features in text types is decided a priori by the linguist on the basis on previous studies, and not derived by a statistical procedure, which is too biased towards high frequencies (some linguistic phenomena can be rare, but they are nonetheless discriminating). It is also inductive because the inference process is corpus-based, which means that it is based on a pool of data used to predict some text types. A few handcrafted if-then rules combine the inferred text"
P98-2173,C92-3158,0,0.0381479,"this benefit is gained at the cost of a huge increase in the difficulty of obtaining the source. No longer can the domain expert author the document directly by writing a text in natural language. Defining the source becomes a task akin to building an expert system, requiring collaboration between a domain expert (who understands the subjectmatter of the document) and a knowledge engineer (who understands the knowledge representation formalism). Owing to this cost, M-NLG has been applied mainly in contexts where the knowledge base is already available, having been created for another purpose (Iordanskaja et al., 1992; Goldberg et al., 1994); for discussion see Reiter and Mellish (1993). Is there any way in which a domain expert might author a knowledge base without going through this time-consuming and costly collaboration with a knowledge engineer? Assuming that some kind of mediation is needed between domain expert and knowledge formalism, the only alternative is to provide easier tools for editing knowledge bases. Some knowledge management projects have experimented with graphical presentations which allow editing by direct manipulation, so that there is no need to learn the syntax of a programming lan"
P98-2173,W96-0505,1,0.778225,"ight author a knowledge base without going through this time-consuming and costly collaboration with a knowledge engineer? Assuming that some kind of mediation is needed between domain expert and knowledge formalism, the only alternative is to provide easier tools for editing knowledge bases. Some knowledge management projects have experimented with graphical presentations which allow editing by direct manipulation, so that there is no need to learn the syntax of a programming language see for example Skuce and Lethbridge (1995). This approach has also been adopted in two M-NLG systems: GIST (Power and Cavallotto, 1996), which generates social security forms in English, Italian and German; and DRAFTER (Paris et al., 1995), which generates instructions for software applications in English and French. These projects were the first attempts to produce symbolic authoring systems - that is, systems allowing a domain expert with no training in knowledge engineering to author a knowledge base (or symbolic source) from which texts in many languages can be generated. Although helpful, graphical tools for managing knowledge bases remain at best a compromise solution. Diagrams may be easier to understand than logical f"
P98-2173,W98-1427,1,0.795644,"supported by the system. As well as producing output texts from complete knowledge bases, these generators will produce feedback texts from knowledge bases in any state of completion. • A user interface which presents output or feedback texts to the author. The feedback texts will include mouse-sensitive 'anchors' allowing the author to make semantic decisions, e.g. by selecting options from pop-up menus. The WYSIWYM system allows a domain expert speaking any one of the supported languages to produce good output texts in all of them. A more detailed description of the architecture is given in Scott et al. (1998). 2 E x a m p l e o f a WYSIWYM s y s t e m The first application of WYSIWYM was DRAFTER-II, a system which generates instuctions for using word processors and diary managers. At present three languages are supported: English, French and Italian. As an example, we will follow a session in which the author encodes instructions for scheduling an appointment with the OpenWindows Calendar Manager. The desired content is shown by the following o u t p u t text, which the system will generate when the knowledge base is complete: procl has a goal attribute for which the value is currently undefined ("
P98-2173,P83-1023,0,0.0700298,"user viewing and modifying the knowledge in his/her own language. • Since the knowledge base is presented as a document, large knowledge bases can be navigated by the methods familiar from books and from complex electronic documents (e.g. contents page, index, hypertext links), obviating any need for special training in navigation. The crucial advantage of WYSIWYM editing, compared with alternative natural language interfaces, is that it eliminates all the usual problems associated with parsing and semantic interpretation. Feedback texts with menus have been used before in the NL-Menu system (Tennant et al., 1983), but only as a means of presenting syntactic options. NL-Menu guides the author by listing the extensions of the current sentence that are covered by its grammar; in this way it makes parsing more reliable, by enforcing adherence to a sub-language, but parsing and interpretation are still required. So far WYSIWYM editing has been implemented in two domains: software instructions (as described here), and patient information leaflets. We are currently evaluating the usability of these systems, partly to confirm that authors do indeed find them easy to use, and partly to investigate issues in th"
W00-1411,P87-1022,0,0.885216,"Missing"
W00-1411,A00-3001,0,0.0297196,"Missing"
W00-1411,P98-1044,0,0.204149,"ollowing CONTINUE, another CONcourse segment and favouring sequences which TINUE would be cheap as well. The RETAIN maintain the same center. Our implementation is motivated as it enables a ""cheap"" SMOOTH incorporates two extensions which have impliSHIFT, and so we need a way of evaluating the cations for more structured discourse: Strube whole sequence CONTINUE-RETAIN-SHIFT verand Hahn's (1999) ""cheapness"" principle, which SUS CONTINUE-CONTINUE-SHIFT. favours transitions that introduce a new topic •in. a s a l i e n t position, ,and .Cristea's Veins The- • : :2~4_~._,:Ceaatering.in :NLG ory (Cristea et al 1998) which redefines tranC T has developed primarily in the context of sitions in terms of rhetorical hierarchy rather natural language interpretation, focussing on than linear sequence of clauses (see section 3.3 anaphora resolution (see e.g., Brennan et al for discussion). 1987). Curiously, NLG researchers have tended ""Cheapness"" is satisfied by a transition pair to overlook G J W ' s proposal that ( ( U n - 1 , Un), (Un, Un+l)) if the preferred center of Un is the Cb of Un+l. For example, this test Rule 2 provides a constraint on speakis satisfied by a RETAIN-SHIFT sequence b u t not ers, and o"
W00-1411,J95-2003,0,0.892232,"Missing"
W00-1411,J98-3004,0,0.0131132,"edicted that the ation systems . . . T o empirically test former pattern will be used to introduce a new the claim made by Rule 2 requires excenter. (This claim is consistent with the findamination of differences in inference ings of Brennan 1998, Brennan et al 1987.) If we load of alternative multi-utterance seconsider examples la-e below, the sequence cquences that differentially realize the d'-e ~, including a RETAIN-SHIFT sequence, reads same content. more fluently than c-d-e even though the latter GJW, p. 215. scores better according to the canonical ranking. With a few exceptions (e.g., Mittal et al 1998, Kibble 1999, Kibble and Power 1999, Cheng . a. John has had trouble arranging his va2000) NLG researchers have interpreted C T as cation. a theory of pronominalisation only (e.g., Dale 1992). In this paper we concentrate on planb. He cannot find anyone to take over his ning, aiming to determine whether the prim responsibilities. ciples underlying the constraints and rules of c. He called up Mike yesterday to work the theory can be ""turned round"" and used as out a plan. CONTINUE planning operators for generating coherent text. Text planning in conformity with CT will need d. He has been prett"
W00-1411,C00-2093,1,0.779992,"N = 4 we would expect around 600 text structures; with perhaps 5-10 ways of assigning centers to each text structure, the total number of solutions would approximate to 5000. Global optimization of the solution therefore becomes impracticable for texts longer than about five propositions; we address this problem by a technique of partial optimization in which a macro-planner fixes the large-scale structure of the text, thus defining a set of micro-planning problems each small enough to be t a c k l e d by t h e methods described here. Stage 1 of the planning procedure is described elsewhere (Power, 2000); we focus here on stages 2 and 3, in which the text planner enumerates the possible assignments of centers and evaluates which is the best. 81 3.1 Choosing centers G i v e n a text structure, we enumerate all permissible centering assignments as foil0ws: "" . . . . . 1. Determine the predecessor each proposition Un. Yn-1 (if any) of 2. List the potential Cbs and Cps of each proposition, henceforth denoted by ECb and ECp. 3. Compute ~li combinations from ECb and ECp that respect the fundamental centering constraint that Cb(Un) should be the most salient candidate in Un-1. Two criteria for deter"
W00-1411,W94-0319,0,0.0199189,"rtain basic grammatical categories; o portability: we claim that text planning needs to be driven in part by the goal of maintaining referential continuity: obtaining a favourable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for non-ambiguous pronoun use. o coherence: The latter claim is not new, but underlies the Centering Theory (CT) of Grosz, Joshi and Weinstein (1995, hereafter ""GJW""). 1.2 Issues in T e x t P l a n n i n g Text Planning is one of the distinct tasks identified in Reiter's ""consensus"" architecture for Natural Language Generation (Reiter 1994, Reiter and Dale 1997): T e x t P l a n n i n g - deciding the content of a message, and organising the component propositions into a text tree; S e n t e n c e P l a n n i n g - aggregating propositions into clausal units and choosing lexical items corresponding to concepts in the knowledge base; Linguistic r e a l i s a t i o n - surface details Such as agreement, orthography etc. Following Scott and de Souza (1990), we assume that the component propositions to be realised in a text are organised in a tree structure in which ternfinal nodes are elementary propositions and non-terminal nodes"
W00-1411,J99-3001,0,\N,Missing
W00-1411,W99-0108,0,\N,Missing
W00-1411,W00-1425,0,\N,Missing
W00-1411,C98-1044,0,\N,Missing
W00-1426,A00-2002,0,0.0605867,"Missing"
W00-1426,J92-4007,0,0.100212,"Missing"
W00-1426,J96-3006,0,0.0417252,"Missing"
W07-2314,P98-2173,1,0.707794,"able to resist the lure of the abstract verb. But how about our programs? Let us look at some output samples from Natural Language Generation 93 Behrens’s principal activities were architecture and industrial design. He made electrical appliances and prototype flasks. He built the high tension plant and the turbine factory for AEG in 1908-1910. He built a housing for the workers of AEG in Henningsdorf. Komet, (Bateman and Teich, 1995) To schedule the appointment: 1. Choose the start time of the appointment. 2. Enter the description of the appointment. 3. Click on the Insert button. DRAFTER-2 (Power and Scott, 1998) This jewel is a necklace and is in the Art Deco style. It was made in 1920. It is made from moonstone and silver rock-crystal. In colour, it is coral. It differs from the previous item, in that whereas that was made by Arthur and Georgie Gaskin, this was made by H.G.Murphy. ILEX, (Oberlander et al., 1998) These typical products of current NLG present a paradox. On the one hand, they conform to the advice given by Joseph Williams and other style gurus. The verbs are concrete and usually denote actions (build, click, make). Noun phrases denote people and things rather than nominalised events or"
W07-2314,J07-1006,1,\N,Missing
W07-2314,C98-2168,1,\N,Missing
W07-2328,I05-5005,0,0.0957989,"dialogue because it opens up new ways of making rhetorical and argumentative information explicit (Piwek et al., 2005). Also, as Craig et al. (2000) and Cox et al. (1999) found, it would help the patient recall the current state of her treatment, it would encourage her to ask questions during the subsequent consultation by reminding her of the exact meaning of technical terms, and would give her more confidence in expressing herself both in technical language and informal language. It could also save doctors’ time by providing an additional source of explanation for patients. The CLEF system (Hallett and Scott, 2005) summarises EHRs of breast cancer patients for clinical staff. We have implemented a pilot system for patients that selects and describes events from the same EHR repository. At present the wording of utterances is somewhat stilted, since our initial focus is on designing a discourse planner for both monologue and dialogue. Below is an extract from a monologue generated by the current system. Introduction Increasingly, health service providers are storing patient medical histories in machine-usable form – i.e., in databases of Electronic Health Records (EHRs) – and are obliged by legislation t"
W08-0508,W98-0106,0,0.042281,"ollows: Span a woman lost her bag a woman her bag her Entity e1 e2 e3 e2 Context ROOT AGENT PATIENT OWNER Note that the same entity may be expressed in multiple contexts (denoted by the incoming arcs in the semantic graph). The relationships between the entities are represented by syntactic dependencies between the spans of the text. For instance, AGENT (e1 ,e2 ) is realised by the clause-subject relation between ‘a woman lost her bag’ and its subspan ‘a woman’. This direct linking of semantic and syntactic dependencies has of course been noted many times, for instance in Meaning-Text Theory (Candito and Kahane, 1998). The structure of the spans of text can be represented by a reconfiguration of the original Abox as an ordered tree, which we will henceforth call an Atree. Figure 2 shows an Atree that fits the example Abox. Note that since this is a tree, the vertex with two incoming edges (e2 ) has to be repeated, and there are two spans referring to the woman. 1 The system is able to generate a referring expression, ‘her’, for the second reference to the woman since it knows that the entity has already been mentioned in the text. This information is available because the Atree, see Figure 2, is an ordered"
W08-0508,P89-1009,0,0.227448,"is used to determine whether or not a referring expression (an anaphoric reference to an entity which has already been mentioned) is required. Because the Atree is ordered and is constructed in order, the system always knows whether an instance is being mentioned for the first time. We currently render subsequent mentions by pruning all of the out-going arcs from the Abox node, which also allows us to manage cycles in the semantic graph. Since the system knows which nodes in the semantic graph have already been mentioned it would also be possible to configure an external call to a GRE system (Dale, 1989) - an application which infers the content of a referring expression given the current semantic context. 43 mapping such as the one depicted in Figure 4 below. <frame concept=”938B” role=”any” subcat=”CatClause-33” bindings=”SUB,D OB,CL COM”> <gr key=”TnxOVnx1s2”> <anchor lemma=”remind” pos=”verb”/> </gr> </frame> Figure 4: A Sample Mapping This mapping tells the system which subcategorisation frame to use, which grammar rule to associate with it, which lexical anchors to pass as arguments to the grammar rule and also how to order the subsidiary arguments of the subcategorisation frame (the bi"
W08-0508,C94-2149,0,0.0852886,"lue if we want to reuse large-scale, generic, curated resources for a small domain and deploy where bandwidth is an issue – for example where language generation is required in a client-heavy internet-based or mobile application. 7.1 Export To test our assumptions about efficiency and scalability we inferred a larger Tbox, subcategorisation frames and mappings using a pre-existing data set of verb frames for English encoded using the COMLEX subcategorisation frame inventory (Grishman et al., 1994). The linguistic resources for the application comprised a generative TAG grammar based on X-TAG (Doran et al., 1994) which we wrote ourA further feature of the system which arises from the proving algorithm is that it supports export behaviour. In an enterprise context we want to be able to reuse linguistic resource components, such as a lexicon, a grammar, a morphological generator and so on, across many different applications. These resources are large and complex and for a 46 8 Testing and Results We unit-tested the mechanics of the framework, such as the graph and tree managers. We then built a proof-of-concept application with a small ontology representing the domain of patient treatment narratives and"
W08-0508,C94-1042,0,0.0973102,"while guaranteeing that the deployed application will never fail because of a missing resource. This is of particular value if we want to reuse large-scale, generic, curated resources for a small domain and deploy where bandwidth is an issue – for example where language generation is required in a client-heavy internet-based or mobile application. 7.1 Export To test our assumptions about efficiency and scalability we inferred a larger Tbox, subcategorisation frames and mappings using a pre-existing data set of verb frames for English encoded using the COMLEX subcategorisation frame inventory (Grishman et al., 1994). The linguistic resources for the application comprised a generative TAG grammar based on X-TAG (Doran et al., 1994) which we wrote ourA further feature of the system which arises from the proving algorithm is that it supports export behaviour. In an enterprise context we want to be able to reuse linguistic resource components, such as a lexicon, a grammar, a morphological generator and so on, across many different applications. These resources are large and complex and for a 46 8 Testing and Results We unit-tested the mechanics of the framework, such as the graph and tree managers. We then b"
W08-0508,P98-2173,1,0.843829,"a textual representation of the input data. The resulting text is conceptually aligned, by which we mean that each component of the text structure (such as words, clauses or sentences, for example) is linked back to the mediating structure from which the text was generated, and from there back to vertices and edges in the semantic graph received as input. The target context for the framework is the construction of semantic web (Berners-Lee et al., 2001) resources using Natural Language Generation (NLG) technology which extends the notion of semantic alignment developed in the WYSIWYM system (Power and Scott, 1998; Power et al., 2003). In this context the text is ephemeral and is generated on demand, while the document content is fully machine-readable, supporting tasks such as automated consistency checking, inferencing and semantic search/query. Since the text is fully linked to the underlying semantic representation it supports a rich user interface encompassing fast and reliable semantic search, inline syntax or anaphora highlighting, knowledge editing, and so on. Finally, the text could be generated in many different natural languages making the information content more widely available. We envisa"
W08-0508,C98-2168,1,\N,Missing
W09-0602,bouayad-agha-etal-2002-pills,1,0.796492,"Missing"
W09-0602,W08-0508,1,0.824723,"antic ‘every parent has one or more children’. In a CNL designed for editing a KB, transparency will have priority, but one can imagine other purposes (e.g., an informal report) for which fluency would matter more. If we propose to use generated CNL as an interface to a knowledge base, it is important that generation should be reliable. A minimal test of reliability is that the grammar and lexicon are complete, in the sense that they produce a text for any well-formed semantic input. Elsewhere, we have described a generation method that allows completeness to be checked by a computer program (Hardcastle and Power, 2008). For any non-trivial DL the set of classes is infinite (e.g., through recursion on C u D or ∃R.C); however, completeness 11 2.5 3 Interpretability This is an essential requirement for knowledge editors that rely on automatic parsing and interpretation of texts typed in by human authors (Schwitter and Tilbrook, 2004; Bernstein and Kaufmann, 2006). A recent innovation has been to pursue the goal of ‘roundtripping’ (Davis et al., 2008), so that a CNL text can be generated from an existing ontology, revised in a text editor, and then interpreted automatically to obtain an updated ontology in the"
W09-0602,W07-2310,0,0.0296803,"nce of axioms, and to start the ball rolling it is seeded with a single vacuous axiom &gt; v &gt;. The program generates a sentence expressing this axiom and adds a list of editing options as follows: Extensibility Ontology development requires that authors should be able to introduce new terms for individuals, classes and properties. The designer of a CNL-based editor cannot foresee what these terms will be, and therefore cannot provide a mapping to suitable lexical entries. This must be done by the ontology developer, and take-up accordingly depends on making this task not only feasible but easy (Hielkema et al., 2007). We will explore two ideas on how this might be done: (a) providing a wide-coverage lexicon from which users can select words to extend the CNL, and (b) using conventions for controlling the naming of classes and properties, so that the two decisions (term name, CNL lexical entry) become essentially a single decision. 1: Every thing/1 is a thing/2. t a A/C A/d Add a new term Add a new axiom Edit class C in axiom A Delete axiom A Note that in every sentence expressing an axiom, the head word of every span denoting a class is given a numerical label; in a simple Prolog interface this allows the"
W09-0602,2006.claw-1.6,0,0.0272333,"nto the ontology, the user also decides how they should be expressed linguistically: thus the lexicon of the Natural Language interface is not predetermined. The purpose of such a tool is to support knowledge editing on the Semantic Web, which at present requires training in graphical user interfaces like Prot´eg´e (Rector et al., 2004), or direct coding in OWL and RDF. Linking OWL to Controlled Natural Language is currently the topic of an OWL1-1 task force, and several groups are already working in this area (Schwitter and Tilbrook, 2004; Thompson et al., 2005; Bernstein and Kaufmann, 2006; Pool, 2006; Dongilli, 2007); the novelty in our approach is that we rely entirely on Natural Language Generation (NLG), extending the W YSIWYM (or Conceptual Authoring) method (Power and Scott, 1998; Hal1.1 Description Logic The theoretical underpinning of OWL (and hence of the semantic web) is a discipline that evolved under various names in the 1980s and 1990s and is now called Description Logic (Baader et al., 2003). This refers not to a single logical language, but to a family of languages. All of these languages allow statements to be built from individuals, classes and properties, but they differ"
W09-0602,P98-2173,1,0.614147,"a tool is to support knowledge editing on the Semantic Web, which at present requires training in graphical user interfaces like Prot´eg´e (Rector et al., 2004), or direct coding in OWL and RDF. Linking OWL to Controlled Natural Language is currently the topic of an OWL1-1 task force, and several groups are already working in this area (Schwitter and Tilbrook, 2004; Thompson et al., 2005; Bernstein and Kaufmann, 2006; Pool, 2006; Dongilli, 2007); the novelty in our approach is that we rely entirely on Natural Language Generation (NLG), extending the W YSIWYM (or Conceptual Authoring) method (Power and Scott, 1998; Hal1.1 Description Logic The theoretical underpinning of OWL (and hence of the semantic web) is a discipline that evolved under various names in the 1980s and 1990s and is now called Description Logic (Baader et al., 2003). This refers not to a single logical language, but to a family of languages. All of these languages allow statements to be built from individuals, classes and properties, but they differ in the resources provided in order to construct classes and properties, thus allowing different balances to be drawn between the conflicting demands of expressiveness and tractability (i.e"
W09-0602,J07-1006,1,\N,Missing
W09-0602,C98-2168,1,\N,Missing
W09-0620,J00-4005,0,\N,Missing
W10-4222,W07-2322,0,0.0758622,"as misleading. For now we have allowed repetition, leaving the problem to future work. 6 Related work Reape and Mellish’s (1999) survey of aggregation in NLG proposed a continuum of definitions ranging from narrow to wide. Our technique fits into the narrow definition, i.e., it is languageindependent, operating on non-linguistic conceptual representations with the aim of minimising redundancy and repetition. It implements the subject and predicate grouping rules and aggregation cues suggested by Dalianis and Hovy (1996). Recent NLG systems that aggregate data from ontologies (Hielkema, 2009; Galanis and Androutsopoulos, 2007; Dongilli, 2008) do not perform aggregation directly on axioms, but only after converting them to linguistic representations. Moreover, their systems generate only from ABox axioms in restricted domains while ours generates English for both ABox and TBox in any domain. The approach most similar to ours is that of Bontcheva and Wilks (2004), who aggregate a subset of RDF triples after domaindependent discourse structuring — a task equivalent to merging axioms that conform to the objectPropertyAssertion pattern in table 4. 7 Conclusion We have demonstrated that for the EL++ DL that underlies ma"
W10-4222,E09-2005,0,0.676474,"cats, dogs, horses and rabbits. In this paper, we show how all axiom patterns in EL++, a DL commonly used in the Semantic Web, can be aggregated without further domain knowledge, and describe a prototype system that performs such aggregations. Our method aggregates axioms while they are still in logical form, i.e., as part of sentence planning but before converting to a linguistic representation and realising as English sentences. This approach is somewhat different from that proposed by other researchers who convert ontology axioms to linguistic structures before aggregating (Hielkema, 2009; Galanis et al., 2009; Dongilli, 2008). We present results from testing our algorithm on over fifty ontologies from the Tones repository3 . 2 Analysis of axiom groupings In this section we analyse which kinds of axioms might be grouped together. Power (2010) anal2 For brevity we use logic notation rather than e.g., OWL Functional Syntax: subClassOf(class(ns:cat) class(ns:animal)) where ns is any valid namespace. The operator v denotes the subclass relation, u denotes class intersection, and ∃P.C the class of individuals bearing the relation P to one or more members of class C. 3 http://owl.cs.manchester.ac.uk/ No."
W10-4222,P10-2024,1,0.650061,"Missing"
W11-2314,C96-2183,0,0.349814,"es not have a particular kind of disability in mind. Rather, we aim to simplify according to levels of difficulty defined in the Mathematics Curriculum of the Qualifications and Curriculum Authority (1999). Adaptation to particular types of users is beyond the scope of this paper. 2 Background Text simplification, a relative new task in Natural Language Processing, has been directed mainly at syntactic constructions and lexical choices that some readers find difficult, such as long sentences, passives, coordinate and subordinate clauses, abstract words, low frequency words, and abbreviations. Chandrasekar et al. (1996) introduced a twostage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence; Siddharthan (2002) instead proposed a threestage process comprising analysis, transformation and generation. In 1998, the project PSET (Carroll et al., 1998) employed lexical as well as syntactic simplifications. Other researchers have focused on the generation of readable texts for readers with low basic skills (Williams and Reiter, 2005), and for teaching foreign languages (Petersen and Ostendorf, 2007). There has been some previous work on numerical expressions but m"
W11-2314,W09-0620,1,0.521551,"achers or adult basic numeracy tutors, all native English speakers. We found them through personal contacts and posts to Internet forums. The task of simplifying numerical expressions is difficult, but it is a task that this group seemed well qualified to tackle since they are highly numerate and accustomed to talking to people who do not understand mathematical concepts very well. Our experimental evaluation involved 34 participants who answered at least one question in our survey (some participants did not complete it). A corpus of numerical expressions was collected for the NUMGEN project (Williams and Power, 2009). The corpus contains 10 sets of newspaper ar- 3.3 Survey Design and Implementation ticles and scientific papers (110 texts in total). Each The survey was divided into three parts as follows: set is a collection of articles on the same topic — e.g., the increased risk of breast cancer in red meat 1. Simplification of numerical expressions for a eaters, and the decline in the puffin population on person who can not understand percentages the Isle of May. Within each set, identical numeri2. Simplification of numerical expressions for a cal facts are presented in a variety of linguistic and perso"
W11-2314,W05-1616,1,0.678475,"icult, such as long sentences, passives, coordinate and subordinate clauses, abstract words, low frequency words, and abbreviations. Chandrasekar et al. (1996) introduced a twostage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence; Siddharthan (2002) instead proposed a threestage process comprising analysis, transformation and generation. In 1998, the project PSET (Carroll et al., 1998) employed lexical as well as syntactic simplifications. Other researchers have focused on the generation of readable texts for readers with low basic skills (Williams and Reiter, 2005), and for teaching foreign languages (Petersen and Ostendorf, 2007). There has been some previous work on numerical expressions but more for experts than for people who have difficulties with numeracy (Ellen Peters and Dieckmann, 2007), (Nathan F. Dieckmann and Peters, 2009), (Ann M. Bisantz and Munch, 2005), (Mishra H, 2011). However, to our knowledge, there have been no previous attempts to automatically simplify numerical information in texts. 3 Experiment Our survey took the form of a questionnaire in which participants were shown a sentence containing one or more numerical expressions whi"
W11-2811,W11-0222,0,0.0265416,"al Language Generation) system that aims to generate a coherent text from an OWL ontology, using only generic methods (i.e., methods that require no additional domain knowledge). How can such a system decide whether two statements from the ontology are related, and if so, classify the relationship in a way that guides their linguistic realisation? An example will clarify both the exact task, and how it might be approached. Suppose that an on1 Examples of ontology verbalisers are SWAT Tools (SWAT Project, 2011), described by Williams et al. (2011), ACE (Attempto Project, 2011), and OntoVerbal (Liang et al., 2011). Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 82–90, c Nancy, France, September 2011. 2011 Association for Computational Linguistics 1 2 3 4 5 6 7 8 OWL statement ClassAssertion(C,I) ObjectPropertyAssertion(P,I,J) ClassAssertion(ObjectSomeValuesFrom(P,C),I) SubClassOf(C,D) SubClassOf(C,ObjectHasValue(P,I)) SubClassOf(C,ObjectSomeValuesFrom(P,D)) DisjointClasses(C,D) EquivalentClasses(C,D) Example of verbalisation Butch is a dog Mary owns Butch Butch lives in a kennel Every dog is a canine Every dog likes Mary Every dog lives in a kennel No dog is a ca"
W11-2811,W09-3714,0,0.0276069,"tically in figure 1 by the arrows labelled A–F. The next question is how these relationships among sets should be classified. Among various possibilities, a plausible method is shown in figure 2: given two sets X and Y , either X will be narrower than Y , or wider, or equal, or distinct, or overlapping. These relations are represented in OWL as follows: (1) narrower by SubClassOf(X,Y); (2) wider by SubClassOf(Y,X); (3) equal by EquivalentClasses(X,Y); (4) distinct by disjointClasses(X,Y); and (5) overlapping, implicitly, by absence of the above. A similar set of relations has been proposed by MacCartney and Manning (2009) for the textual entailment task.4 With this model, the rhetorical relationship between two statements can be profiled by assigning an integer from 1–5 (figure 2) to each of the relation4 MacCartney and Manning actually use seven relations, because they distinguish as a separate case disjoint and overlap relations in which the classes X and Y cover all entities in the domain (i.e., every entity must belong either to X or to Y or both). This refinement is not relevant for our purposes. 84 ships A–F (figure 1); to represent such assignments succinctly we will use a six-number code such as 131231"
W11-2811,C10-2116,1,0.890691,"Missing"
W11-2811,W11-2821,1,0.743288,"asserted. This is precisely the situation that confronts an NLG (Natural Language Generation) system that aims to generate a coherent text from an OWL ontology, using only generic methods (i.e., methods that require no additional domain knowledge). How can such a system decide whether two statements from the ontology are related, and if so, classify the relationship in a way that guides their linguistic realisation? An example will clarify both the exact task, and how it might be approached. Suppose that an on1 Examples of ontology verbalisers are SWAT Tools (SWAT Project, 2011), described by Williams et al. (2011), ACE (Attempto Project, 2011), and OntoVerbal (Liang et al., 2011). Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 82–90, c Nancy, France, September 2011. 2011 Association for Computational Linguistics 1 2 3 4 5 6 7 8 OWL statement ClassAssertion(C,I) ObjectPropertyAssertion(P,I,J) ClassAssertion(ObjectSomeValuesFrom(P,C),I) SubClassOf(C,D) SubClassOf(C,ObjectHasValue(P,I)) SubClassOf(C,ObjectSomeValuesFrom(P,D)) DisjointClasses(C,D) EquivalentClasses(C,D) Example of verbalisation Butch is a dog Mary owns Butch Butch lives in a kennel Every dog is a can"
W11-2821,W07-2322,0,0.0192419,"ink to the headings of their entries. 3.5 Related systems To our knowledge, SWAT T OOLS takes document structuring further than other domain-independent ontology verbalisers. We are aware of only one other domain-independent system that attempts document structuring, ACE (Kaljurand and Fuchs, 2007). ACE lists statements under class, individual and property headings; and it inserts hyperlinks; but it has no intermediate levels of organisation. 160 Regarding domain-dependent systems, most of them aggregate statements and generate referring expressions (Bontcheva and Wilks, 2004; Dongilli, 2008; Galanis and Androutsopoulos, 2007; Hielkema, 2009; Liang et al., 2011). Only one attempts further discourse structuring: Laing et al.’s system for verbalising medical ontologies organises text according to rhetorical structure. 4 Evaluation The evaluation study reported here focusses on the following question: Does the organisation just described help people to understand and navigate a text in spite of its longer length? This is addressed through a navigation task in which people were asked to locate information in either an organised text or an unorganised one and then give a judgement on how difficult the information was t"
W11-2821,W11-0222,0,0.0504402,"ystems To our knowledge, SWAT T OOLS takes document structuring further than other domain-independent ontology verbalisers. We are aware of only one other domain-independent system that attempts document structuring, ACE (Kaljurand and Fuchs, 2007). ACE lists statements under class, individual and property headings; and it inserts hyperlinks; but it has no intermediate levels of organisation. 160 Regarding domain-dependent systems, most of them aggregate statements and generate referring expressions (Bontcheva and Wilks, 2004; Dongilli, 2008; Galanis and Androutsopoulos, 2007; Hielkema, 2009; Liang et al., 2011). Only one attempts further discourse structuring: Laing et al.’s system for verbalising medical ontologies organises text according to rhetorical structure. 4 Evaluation The evaluation study reported here focusses on the following question: Does the organisation just described help people to understand and navigate a text in spite of its longer length? This is addressed through a navigation task in which people were asked to locate information in either an organised text or an unorganised one and then give a judgement on how difficult the information was to find. The study design is between-s"
W11-2821,C10-2116,1,0.581847,"eadings. Within each entry, statements are organised into sub-groups according to their logical type. Subheadings are always generated in a fixed order (Definition, Taxonomy, Description, Distinctions, Examples) similar to that found in encyclopedia entries (Berzlanovich et al., 2008). For classes, EquivalentClasses statements in which the atomic class is the first 6 en.wikipedia.org/wiki/Genusdifferentia definition Theoretically, this could mean that some statements are omitted altogether because their top-level arguments are nonatomic, but in practice such statements are almost never found (Power and Third, 2010). 7 argument occur under the definition subheading, the taxonomy is the superclass (from an OWL SubClassOf statement), descriptive statements correspond to the OWL functor SubClassOf, distinctions to DisjointClasses, and examples to the individuals belonging to the class. For individuals the class is given first (from an OWL ClassAssertion statement), followed by descriptions typically corresponding to ObjectPropertyAssertion. For properties, the descriptive statements specify the domain and range, and features such as functionality and transitivity, and examples are provided by statements abo"
W11-2821,W10-4222,1,0.846766,"s, and examples to the individuals belonging to the class. For individuals the class is given first (from an OWL ClassAssertion statement), followed by descriptions typically corresponding to ObjectPropertyAssertion. For properties, the descriptive statements specify the domain and range, and features such as functionality and transitivity, and examples are provided by statements about individuals or classes in which the property is used. 3.3 Aggregating and truncating A third level of organisation occurs when statements with identical structures and one identical argument are aggregated; see Williams and Power (2010) for more details. For some ontologies, this process can lead to very long lists of subclasses or individuals, so under the ‘Examples’ subheading where these occur we truncate them to a predefined maximum length and add the phrase ‘and so on (N items in total)’. Figure 1 shows an example of aggregation and truncation in the sentence ‘The following are seta appendage cephalothorax: male palpal femoral thorns, female palp femoral thorns and spd 0000203s, and so on (5 items in total)’. An obvious refinement would be to add a facility to view the entire list, if desired. 3.4 Hypertext links The fi"
W13-1505,W11-2314,1,0.827731,"Missing"
W13-1505,C96-2183,0,0.0764274,"valuated and the results are discussed against conclusions obtained from previous empirical survey. 2 Previous work Text simplification, a relative new task in Natural Language Processing, has been directed mainly at syntactic constructions and lexical choices that some readers find difficult, such as long sentences, passives, coordinate and subordinate clauses, abstract words, low frequency words, and abbreviations. The rule-based paradigm has been used in the implementation of some systems for text simplification, each one focusing on a variety of readers (with poor literacy, aphasia, etc) (Chandrasekar et al., 1996; Siddharthan, 2003; Jr. et al., 2009; Bautista et al., 2009). The transformation of texts into easy-to-read versions can also be phrased as a translation problem between two different subsets of language: the original and the easy-to-read version. Corpus-based systems can learn from corpora the simplification operations and also the required degree of simplification for a given task (Daelemans et al., 2004; Petersen and Ostendorf, 2007; Gasperin et al., 2009). A variety of simplification techniques have been used, substituting common words for uncommon words (Devlin and Tait, 1998), activatin"
W13-1505,W09-2105,0,0.0716264,"Missing"
W13-1505,C08-1055,0,0.0727917,"Missing"
W13-1505,J12-1004,1,0.898862,"rsen and Ostendorf, 2007). Previous work on numerical expressions has studied the treatment of numerical information in different areas like health (Peters et al., 2007), forecast (Dieckmann et al., 2009), representation of probabilistic information (Bisantz et al., 2005) or vague information (Mishra et al., 2011). In the NUMGEN project (Williams and Power, 2009), a corpus 40 of numerical expressions was collected and a formal model for planning specifications for proportions (numbers between 0 and 1) was developed. The underlying theory and the design of the working program are described in (Power and Williams, 2012). 3 Experimental identification of simplification strategies for numerical information In order to analyze different simplification strategies for numerical expressions, first we have to study the mathematical complexity of the expressions. Expressions can be classified and a level of difficulty can be assigned. A study about the simplification strategies selected by experts to simplify numerical expressions expressed as decimal percentages in a corpus was carried out in Bautista et al. (2011b). Other important aspect of the simplification task is the use of hedges to simplify numerical expres"
W13-1505,W09-0620,1,0.832692,"tradeoff between brevity and clarity in the context of generating referring expressions. Other researchers have focused on the generation of readable texts for readers with low basic skills (Williams and Reiter, 2005), and for teaching foreign languages (Petersen and Ostendorf, 2007). Previous work on numerical expressions has studied the treatment of numerical information in different areas like health (Peters et al., 2007), forecast (Dieckmann et al., 2009), representation of probabilistic information (Bisantz et al., 2005) or vague information (Mishra et al., 2011). In the NUMGEN project (Williams and Power, 2009), a corpus 40 of numerical expressions was collected and a formal model for planning specifications for proportions (numbers between 0 and 1) was developed. The underlying theory and the design of the working program are described in (Power and Williams, 2012). 3 Experimental identification of simplification strategies for numerical information In order to analyze different simplification strategies for numerical expressions, first we have to study the mathematical complexity of the expressions. Expressions can be classified and a level of difficulty can be assigned. A study about the simplifi"
W13-1505,W05-1616,1,0.753297,"techniques have been used, substituting common words for uncommon words (Devlin and Tait, 1998), activating passive sentences and resolving references (Canning, 2000), reducing multiple-clause sentences to single-clause sentences (Chandrasekar and Srinivas, 1997; Canning, 2000; Siddharthan, 2002) and making appropriate choices at the discourse level (Williams et al., 2003b). Khan et at. (2008) studied the tradeoff between brevity and clarity in the context of generating referring expressions. Other researchers have focused on the generation of readable texts for readers with low basic skills (Williams and Reiter, 2005), and for teaching foreign languages (Petersen and Ostendorf, 2007). Previous work on numerical expressions has studied the treatment of numerical information in different areas like health (Peters et al., 2007), forecast (Dieckmann et al., 2009), representation of probabilistic information (Bisantz et al., 2005) or vague information (Mishra et al., 2011). In the NUMGEN project (Williams and Power, 2009), a corpus 40 of numerical expressions was collected and a formal model for planning specifications for proportions (numbers between 0 and 1) was developed. The underlying theory and the design"
W13-1505,W03-2317,1,0.769923,"ior empirical survey. 1 Richard Power, Sandra Williams Department of Computing, The Open University Milton Keynes, MK76AA, UK r.power@open.ac.uk s.h.williams@open.ac.uk Introduction A surprisingly large number of people have limited access to information because of poor literacy. The most recent surveys of literacy in the United Kingdom reveal that 7 million adults in England cannot locate the reference page for plumbers if given the Yellow Pages alphabetical index. This means that one in five adults has less literacy than the expected literacy in an 11-year-old child (Jama and Dugdale, 2010; Williams et al., 2003a; Christina and Jonathan, 2010). Additionally, almost 24 million adults in the U.K. have insufficient numeracy skills to perform simple everyday tasks such as paying household bills and understanding wage slips. They would be unable to achieve grade C in the GCSE maths examination for 16-year-old school children (Williams et al., 2003a). “The Standard Rules on the Equalization of Opportunities for Persons with Disabilities” by United Nations (1994) state that all public information services and documents should be accessible in such a way that they could be easily understood. If we A possible"
W13-1505,daelemans-etal-2004-automatic,0,\N,Missing
W98-0608,P98-2173,1,0.868426,"Missing"
W98-1427,C96-1043,0,0.0610763,"istics over the given period (Iordanskaja et al, 1992). MODELEXPLAINER takes data from graphical object oriented data models and from this generates a textual description of the model (Lavoie et al, 1996). POSTGRAPHE takes tabular data (of the sort found in a typical spreadsheet) and generates a report integrating both graphics and text (Fasciano and Lapalme, 1996). PLANDOC takes the data from a simulation log file and from this produces a report of the explored simulation options (McKeown et al, 1994). ALETHGEN takes data from a customer database and produces a customised letter (in French) (Coch, 1996). A system developed by Johanna Moore and her colleagues at the University of Pittsburg takes the data from SAGE, a graphics presentation system (Roth et al, 1994), and produces an accompanying natural language caption (Mittal et al, in press). However, it doesn't make the only sense. Some applications require the user to interact rather more closely with the semantic model that drives the generation process, and there is a small, but growing, number of systems that are able to provide this kind of interaction. They achieve this through a common solution: interfaces that allow the user to enga"
W98-1427,A94-1001,0,0.114833,"but growing, number of systems that are able to provide this kind of interaction. They achieve this through a common solution: interfaces that allow the user to engage in symbolic authoring of the generated text. • EXCLASS is an intelligent support tool for personnel officers writing (bilingual English and French) job descriptions. The user builds the job description by composing and editing conceptual representations; these representations are trees of concepts from a structured conceptual dictionary. Concepts are presented to the user through diagrammatic trees with natural language labels (Caldwell and Korelsky, 1994). • DRAFTER-I i s an authoring tool to support technical authors and software developers in writing (bilingual English and French) software manuals. The user directly builds the domain model (semantic knowledge base) describing the procedures for using a selected software application. As it is being constructed, the model is presented to the user through diagrams and fragments of text (Paris et al, 1995). • G I S T is an authoring tool to support forms designers. It generates (multilingual English, German, Italian) forms in the domain of social administration. The user's interaction with G I S"
W98-1427,W96-0406,0,0.0633494,"Missing"
W98-1427,C92-3158,0,0.148439,"al language report on the activities of the stock market over the period• (Kukich, 1983). ANA 256 I ,| I I 1 11 • • • • • • • • GOSSIP takes data from an audit trail of an operating system and produces a report (for a security officer) on user activity over the period (Carcagno and Iordanskaja, 1989). FOG takes data from a time series of weather depiction charts and produces a bilingual (French and English) weather report for the period (Goldberg et al, 1994). L F S takes statistical data from labour force surveys and from this produces a report on employment statistics over the given period (Iordanskaja et al, 1992). MODELEXPLAINER takes data from graphical object oriented data models and from this generates a textual description of the model (Lavoie et al, 1996). POSTGRAPHE takes tabular data (of the sort found in a typical spreadsheet) and generates a report integrating both graphics and text (Fasciano and Lapalme, 1996). PLANDOC takes the data from a simulation log file and from this produces a report of the explored simulation options (McKeown et al, 1994). ALETHGEN takes data from a customer database and produces a customised letter (in French) (Coch, 1996). A system developed by Johanna Moore and h"
W98-1427,A94-1002,0,0.0161479,"1994). L F S takes statistical data from labour force surveys and from this produces a report on employment statistics over the given period (Iordanskaja et al, 1992). MODELEXPLAINER takes data from graphical object oriented data models and from this generates a textual description of the model (Lavoie et al, 1996). POSTGRAPHE takes tabular data (of the sort found in a typical spreadsheet) and generates a report integrating both graphics and text (Fasciano and Lapalme, 1996). PLANDOC takes the data from a simulation log file and from this produces a report of the explored simulation options (McKeown et al, 1994). ALETHGEN takes data from a customer database and produces a customised letter (in French) (Coch, 1996). A system developed by Johanna Moore and her colleagues at the University of Pittsburg takes the data from SAGE, a graphics presentation system (Roth et al, 1994), and produces an accompanying natural language caption (Mittal et al, in press). However, it doesn't make the only sense. Some applications require the user to interact rather more closely with the semantic model that drives the generation process, and there is a small, but growing, number of systems that are able to provide this"
W98-1427,J93-1009,0,0.027255,"generally agreed that the technology of natural language generation has evolved to a stage where it can feasibly be expected to be found in ,real world', applied systems. Indeed, within theiast year there has been a specialist tutorial (Dale and Reiter, 1997) and a journal article (Reiter and Dale, 1997) aimed at guiding interested parties on how to build such systems; a textbook on this subject is also about to appear (Reiter and Dale, forthcoming). A problem that remains outstanding, however, is that of the input to NLG applications: where should we get it from and what should it look like (McDonald, 1993)? A popular school of thought on this issue is echoed in the following quotation: Rules of Thumb for NLG NL generation only makes sense when 1) The data to be communicated is already present in an existing database (knowledge base etcO (emphasis added) 2) The users need the information, and want it presented as text (e.g., instead of graphically) 3) The volume of documentation justifies the expense of building an NLG system. from Ehud Reiter, 'Using Natural LanguageGenerationto ProduceReal-WorldDocuments'. Seminarpresented at the iTRI, 1994 This first rule of thumb clearly does make good gener"
W98-1427,W96-0407,0,0.0282339,"Missing"
W98-1427,P83-1022,0,\N,Missing
W98-1437,P98-2173,1,0.880183,"Missing"
W98-1437,W98-1427,1,0.876063,"Missing"
W98-1437,C98-2168,1,\N,Missing
williams-power-2008-deriving,C04-1007,0,\N,Missing
williams-power-2008-deriving,W01-1605,0,\N,Missing
williams-power-2008-deriving,P02-1047,0,\N,Missing
williams-power-2010-fact,W08-1301,0,\N,Missing
williams-power-2010-fact,J07-1006,1,\N,Missing
williams-power-2010-fact,P01-1008,0,\N,Missing
williams-power-2010-fact,W09-0620,1,\N,Missing
