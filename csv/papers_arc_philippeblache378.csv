2021.starsem-1.1,Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge,2021,-1,-1,6,0,926,paolo pedinotti,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge). Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the dynamic estimation of thematic fit. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and we conducted a detailed error analysis to investigate which factors affect their behavior. Our results show that TLMs can reach performances that are comparable to those achieved by SDM. However, additional analysis consistently suggests that TLMs do not capture important aspects of event knowledge, and their predictions often depend on surface linguistic features, such as frequent words, collocations and syntactic patterns, thereby showing sub-optimal generalization abilities."
2020.lrec-1.85,The Brain-{IHM} Dataset: a New Resource for Studying the Brain Basis of Human-Human and Human-Machine Conversations,2020,-1,-1,6,1,16777,magalie ochs,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper presents an original dataset of controlled interactions, focusing on the study of feedback items. It consists on recordings of different conversations between a doctor and a patient, played by actors. In this corpus, the patient is mainly a listener and produces different feedbacks, some of them being (voluntary) incongruent. Moreover, these conversations have been re-synthesized in a virtual reality context, in which the patient is played by an artificial agent. The final corpus is made of different movies of human-human conversations plus the same conversations replayed in a human-machine context, resulting in the first human-human/human-machine parallel corpus. The corpus is then enriched with different multimodal annotations at the verbal and non-verbal levels. Moreover, and this is the first dataset of this type, we have designed an experiment during which different participants had to watch the movies and give an evaluation of the interaction. During this task, we recorded participant{'}s brain signal. The Brain-IHM dataset is then conceived with a triple purpose: 1/ studying feedbacks by comparing congruent vs. incongruent feedbacks 2/ comparing human-human and human-machine production of feedbacks 3/ studying the brain basis of feedback perception."
2020.coling-main.431,Two-level classification for dialogue act recognition in task-oriented dialogues,2020,-1,-1,1,1,929,philippe blache,Proceedings of the 28th International Conference on Computational Linguistics,0,"Dialogue act classification becomes a complex task when dealing with fine-grain labels. Many applications require such level of labelling, typically automatic dialogue systems. We present in this paper a 2-level classification technique, distinguishing between generic and specific dialogue acts (DA). This approach makes it possible to benefit from the very good accuracy of generic DA classification at the first level and proposes an efficient approach for specific DA, based on high-level linguistic features. Our results show the interest of involving such features into the classifiers, outperforming all other feature sets, in particular those classically used in DA classification."
2020.aacl-main.26,"Comparing Probabilistic, Distributional and Transformer-Based Models on Logical Metonymy Interpretation",2020,-1,-1,4,1,927,giulia rambelli,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"In linguistics and cognitive science, Logical metonymies are defined as type clashes between an event-selecting verb and an entity-denoting noun (e.g. The editor finished the article), which are typically interpreted by inferring a hidden event (e.g. reading) on the basis of contextual cues. This paper tackles the problem of logical metonymy interpretation, that is, the retrieval of the covert event via computational methods. We compare different types of models, including the probabilistic and the distributional ones previously introduced in the literature on the topic. For the first time, we also tested on this task some of the recent Transformer-based models, such as BERT, RoBERTa, XLNet, and GPT-2. Our results show a complex scenario, in which the best Transformer-based models and some traditional distributional models perform very similarly. However, the low performance on some of the testing datasets suggests that logical metonymy is still a challenging phenomenon for computational modeling."
W19-3312,Distributional Semantics Meets Construction Grammar. towards a Unified Usage-Based Model of Grammar and Meaning,2019,0,0,3,1,927,giulia rambelli,Proceedings of the First International Workshop on Designing Meaning Representations,0,"In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Construction Grammar, where grammar and meaning are systematically modeled from language use, and finally, we discuss the kind of contributions that distributional models can provide to CxG representation from a linguistic and cognitive perspective."
W18-4603,Modeling Violations of Selectional Restrictions with Distributional Semantics,2018,0,2,3,1,180,emmanuele chersoni,Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing,0,"Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention: it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies."
L18-1467,A Semi-autonomous System for Creating a Human-Machine Interaction Corpus in Virtual Reality: Application to the {ACORFORM}ed System for Training Doctors to Break Bad News,2018,0,2,2,1,16777,magalie ochs,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-6803,Is Structure Necessary for Modeling Argument Expectations in Distributional Semantics?,2017,32,0,3,1,180,emmanuele chersoni,{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers,0,"Despite the number of NLP studies dedicated to thematic fit estimation, little attention has been paid to the related task of composing and updating verb argument expectations. The few exceptions have mostly modeled this phenomenon with structured distributional models, implicitly assuming a similarly structured representation of events. Recent experimental evidence, however, suggests that human processing system could also exploit an unstructured bag-of-arguments type of event representation to predict upcoming input. In this paper, we re-implement a traditional structured model and adapt it to compare the different hypotheses concerning the degree of structure in our event knowledge, evaluating their relative performance in the task of the argument expectations update."
S17-1021,Logical Metonymy in a Distributional Model of Sentence Comprehension,2017,15,2,3,1,180,emmanuele chersoni,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"In theoretical linguistics, logical metonymy is defined as the combination of an event-subcategorizing verb with an entity-denoting direct object (e.g., The author began the book), so that the interpretation of the VP requires the retrieval of a covert event (e.g., writing). Psycholinguistic studies have revealed extra processing costs for logical metonymy, a phenomenon generally explained with the introduction of new semantic structure. In this paper, we present a general distributional model for sentence comprehension inspired by the Memory, Unification and Control model by Hagoort (2013,2016). We show that our distributional framework can account for the extra processing costs of logical metonymy and can identify the covert event in a classification task."
D17-1068,Measuring Thematic Fit with Distributional Feature Overlap,2017,33,1,4,0.666667,181,enrico santus,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we introduce a new distributional method for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verb-specific semantic roles."
Y16-2021,Testing {APS}yn against Vector Cosine on Similarity Estimation,2016,29,11,5,0.666667,181,enrico santus,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,"In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation."
W16-4102,Towards a Distributional Model of Semantic Complexity,2016,20,2,2,1,180,emmanuele chersoni,Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC}),0,"In this paper, we introduce for the first time a Distributional Model for computing semantic complexity, inspired by the general principles of the Memory, Unification and Control framework(Hagoort, 2013; Hagoort, 2016). We argue that sentence comprehension is an incremental process driven by the goal of constructing a coherent representation of the event represented by the sentence. The composition cost of a sentence depends on the semantic coherence of the event being constructed and on the activation degree of the linguistic constructions. We also report the results of a first evaluation of the model on the Bicknell dataset (Bicknell et al., 2010)."
L16-1245,4{C}ouv: A New Treebank for {F}rench,2016,0,0,1,1,929,philippe blache,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The question of the type of text used as primary data in treebanks is of certain importance. First, it has an influence at the discourse level: an article is not organized in the same way as a novel or a technical document. Moreover, it also has consequences in terms of semantic interpretation: some types of texts can be easier to interpret than others. We present in this paper a new type of treebank which presents the particularity to answer to specific needs of experimental linguistic. It is made of short texts (book backcovers) that presents a strong coherence in their organization and can be rapidly interpreted. This type of text is adapted to short reading sessions, making it easy to acquire physiological data (e.g. eye movement, electroencepholagraphy). Such a resource offers reliable data when looking for correlations between computational models and human language processing."
L16-1370,{M}arsa{G}ram: an excursion in the forests of parsing trees,2016,0,1,1,1,929,philippe blache,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The question of how to compare languages and more generally the domain of linguistic typology, relies on the study of different linguistic properties or phenomena. Classically, such a comparison is done semi-manually, for example by extracting information from databases such as the WALS. However, it remains difficult to identify precisely regular parameters, available for different languages, that can be used as a basis towards modeling. We propose in this paper, focusing on the question of syntactic typology, a method for automatically extracting such parameters from treebanks, bringing them into a typology perspective. We present the method and the tools for inferring such information and navigating through the treebanks. The approach has been applied to 10 languages of the Universal Dependencies Treebank. We approach is evaluated by showing how automatic classification correlates with language families."
D16-1205,Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity,2016,22,1,4,1,180,emmanuele chersoni,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus."
2015.jeptalnrecital-long.20,Typologie automatique des langues {\\`a} partir de treebanks,2015,-1,-1,1,1,929,philippe blache,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La typologie des langues repose sur l{'}{\'e}tude de la r{\'e}alisation de propri{\'e}t{\'e}s ou ph{\'e}nom{\`e}nes linguistiques dans plusieurs langues ou familles de langues. Nous abordons dans cet article la question de la typologie syntaxique et proposons une m{\'e}thode permettant d{'}extraire automatiquement ces propri{\'e}t{\'e}s {\`a} partir de treebanks, puis de les analyser en vue de dresser une telle typologie. Nous d{\'e}crivons cette m{\'e}thode ainsi que les outils d{\'e}velopp{\'e}s pour la mettre en {\oe}uvre. Celle-ci a {\'e}t{\'e} appliqu{\'e}e {\`a} l{'}analyse de 10 langues d{\'e}crites dans le Universal Dependencies Treebank. Nous validons ces r{\'e}sultats en montrant comment une technique de classification permet, sur la base des informations extraites, de reconstituer des familles de langues."
2015.jeptalnrecital-court.25,Cr{\\'e}ation d{'}un nouveau treebank {\\`a} partir de quatri{\\`e}mes de couverture,2015,-1,-1,1,1,929,philippe blache,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons ici 4-couv, un nouveau corpus arbor{\'e} d{'}environ 3 500 phrases, constitu{\'e} d{'}un ensemble de quatri{\`e}mes de couverture, {\'e}tiquet{\'e} et analys{\'e} automatiquement puis corrig{\'e} et valid{\'e} {\`a} la main. Il r{\'e}pond {\`a} des besoins sp{\'e}cifiques pour des projets de linguistique exp{\'e}rimentale, et vise {\`a} rester compatible avec les autres treebanks existants pour le fran{\c{c}}ais. Nous pr{\'e}sentons ici le corpus lui-m{\^e}me ainsi que les outils utilis{\'e}s pour les diff{\'e}rentes {\'e}tapes de son {\'e}laboration : choix des textes, {\'e}tiquetage, parsing, correction manuelle."
W14-0501,Challenging incrementality in human language processing: two operations for a cognitive architecture,2014,9,0,1,1,929,philippe blache,Proceedings of the 5th Workshop on Cognitive Aspects of Computational Language Learning ({C}og{ACLL}),0,"The description of language complexity and the cognitive load related to the different linguistic phenomena is a key issue for the understanding of language processing. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation is done starting from the probability of the possible solutions. On their side, symbolic approaches also provide an estimation of the activation degree, depending on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by human subjects. Eyetracking data show for example that fixations are done by chunks, not by words (Rauzy and Blache, 2012). Similarly, EEG experiments have shown that processing multiword expressions (for example idioms) relies on global mechanisms (Vespignani et al., 2010); (Rommers et al., 2013). Starting from the question of complexity and its estimation, I will address in this presentation the problem of language processing and its organization. I propose more precisely, using computational complexity models, to define a cohesion index between words. Such an index makes it possible to define chunks (or more generally units) that are built directly, by aggregation, instead of syntactic analysis. In this hypothesis, parsing consists in two different processes: aggregation and integration."
F14-2026,"Multilingual Summarization Experiments on {E}nglish, {A}rabic and {F}rench (R{\\'e}sum{\\'e} Automatique Multilingue Exp{\\'e}rimentations sur l{'}Anglais, l{'}Arabe et le Fran{\\c{c}}ais) [in {F}rench]",2014,-1,-1,3,0,21535,houda oufaida,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
F13-1017,Chunks and the notion of activation : a facilitation model for sentence processing (Chunks et activation : un mod{\\`e}le de facilitation du traitement linguistique) [in {F}rench],2013,-1,-1,1,1,929,philippe blache,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-4903,Robustness and processing difficulty models. A pilot study for eye-tracking data on the {F}rench Treebank,2012,19,6,2,0,16764,stephane rauzy,Proceedings of the First Workshop on Eye-tracking and Natural Language Processing,0,"We present in this paper a robust method for predicting reading times. Robustness first comes from the conception of the difficulty model, which is based on a morpho-syntactic surprisal index. This metric is not only a good predictor, as shown in the paper, but also intrinsically robust (because relying on POS-tagging instead of parsing). Second, robustness also concerns data analysis: we propose to enlarge the scope of reading processing units by using syntactic chunks instead of words. As a result, words with null reading time do not need any special treatment or filtering. It appears that working at chunks scale smooths out the variability inherent to the different reader's strategy. The pilot study presented in this paper applies this technique to a new resource we have built, enriching a French treebank with eye-tracking data and difficulty prediction measures."
seinturier-etal-2012-ontological,An ontological approach to model and query multimodal concurrent linguistic annotations,2012,4,0,4,0,42960,julien seinturier,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper focuses on the representation and querying of knowledge-based multimodal data. This work stands in the OTIM project which aims at processing multimodal annotation of a large conversational French speech corpus. Within OTIM, we aim at providing linguists with a unique framework to encode and manipulate numerous linguistic domains (from prosody to gesture). Linguists commonly use Typed Feature Structures (TFS) to provide an uniform view of multimodal annotations but such a representation cannot be used within an applicative framework. Moreover TFS expressibility is limited to hierarchical and constituency relations and does not suit to any linguistic domain that needs for example to represent temporal relations. To overcome these limits, we propose an ontological approach based on Description logics (DL) for the description of linguistic knowledge and we provide an applicative framework based on OWL DL (Ontology Web Language) and the query language SPARQL."
F12-2023,Enrichissement du {FTB} : un treebank hybride constituants/propri{\\'e}t{\\'e}s (Enriching the {F}rench Treebank with Properties) [in {F}rench],2012,0,0,1,1,929,philippe blache,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
Y11-1017,Predicting Linguistic Difficulty by Means of a Morpho-Syntactic Probabilistic Model,2011,13,5,1,1,929,philippe blache,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"We propose in this paper a new contribution to the evaluation of linguistic diffi- culty. At the opposite of classical approaches relying on syntax, we show that a probabilis- tic morpho-syntactic analysis provides information enough to calculate different parameters, including surprisal. This method constitutes an original and robust model to linguistic com- plexity. It constitutes a solution towards complexity experiments using spoken languages."
W10-1829,Multimodal Annotation of Conversational Data,2010,35,18,1,1,929,philippe blache,Proceedings of the Fourth Linguistic Annotation Workshop,0,"We propose in this paper a broad-coverage approach for multimodal annotation of conversational data. Large annotation projects addressing the question of multimodal annotation bring together many different kinds of information from different domains, with different levels of granularity. We present in this paper the first results of the OTIM project aiming at developing conventions and tools for multimodal annotation."
blache-etal-2010-otim,The {OTIM} Formal Annotation Model: A Preliminary Step before Annotation Scheme,2010,9,2,1,1,929,philippe blache,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Large annotation projects, typically those addressing the question of multimodal annotation in which many different kinds of information have to be encoded, have to elaborate precise and high level annotation schemes. Doing this requires first to define the structure of the information: the different objects and their organization. This stage has to be as much independent as possible from the coding language constraints. This is the reason why we propose a preliminary formal annotation model, represented with typed feature structures. This representation requires a precise definition of the different objects, their properties (or features) and their relations, represented in terms of type hierarchies. This approach has been used to specify the annotation scheme of a large multimodal annotation project (OTIM) and experimented in the annotation of a multimodal corpus (CID, Corpus of Interactional Data). This project aims at collecting, annotating and exploiting a dialogue video corpus in a multimodal perspective (including speech and gesture modalities). The corpus itself, is made of 8 hours of dialogues, fully transcribed and richly annotated (phonetics, syntax, pragmatics, gestures, etc.)."
C10-2008,A Formal Scheme for Multimodal Grammars,2010,19,1,1,1,929,philippe blache,Coling 2010: Posters,0,"We present in this paper a formal approach for the representation of multimodal information. This approach, thanks to the to use of typed feature structures and hypergraphs, generalizes existing ones (typically annotation graphs) in several ways. It first proposes an homogenous representation of different types of information (nodes and relations) coming from different domains (speech, gestures). Second, it makes it possible to specify constraints representing the interaction between the different modalities, in the perspective of developing multimodal grammars."
2010.jeptalnrecital-long.9,Un mod{\\`e}le de caract{\\'e}risation de la complexit{\\'e} syntaxique,2010,-1,-1,1,1,929,philippe blache,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente un mod{\`e}le de la complexit{\'e} syntaxique. Il r{\'e}unit un ensemble d{'}indices de complexit{\'e} et les repr{\'e}sente {\`a} l{'}aide d{'}un cadre formel homog{\`e}ne, offrant ainsi la possibilit{\'e} d{'}une quantification automatique : le mod{\`e}le propos{\'e} permet d{'}associer {\`a} chaque phrase un indice refl{\'e}tant sa complexit{\'e}."
W09-3033,A general scheme for broad-coverage multimodal annotation,2009,5,1,1,1,929,philippe blache,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,We present in this paper a formal and computational scheme in the perspective of broad-coverage multimodal annotation. We propose in particular to introduce the notion of annotation hypergraphs in which primary and secondary data are represented by means of the same structure.
2009.jeptalnrecital-long.28,Des relations d{'}alignement pour d{\\'e}crire l{'}interaction des domaines linguistiques : vers des Grammaires Multimodales,2009,-1,-1,1,1,929,philippe blache,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Un des probl{\`e}mes majeurs de la linguistique aujourd{'}hui r{\'e}side dans la prise en compte de ph{\'e}nom{\`e}nes relevant de domaines et de modalit{\'e}s diff{\'e}rentes. Dans la litt{\'e}rature, la r{\'e}ponse consiste {\`a} repr{\'e}senter les relations pouvant exister entre ces domaines de fa{\c{c}}on externe, en termes de relation de structure {\`a} structure, s{'}appuyant donc sur une description distincte de chaque domaine ou chaque modalit{\'e}. Nous proposons dans cet article une approche diff{\'e}rente permettant repr{\'e}senter ces ph{\'e}nom{\`e}nes dans un cadre formel unique, permettant de rendre compte au sein d{'}une m{\^e}me grammaire tous les ph{\'e}nom{\`e}nes concern{\'e}s. Cette repr{\'e}sentation pr{\'e}cise de l{'}interaction entre domaines et modalit{\'e}s s{'}appuie sur la d{\'e}finition de relations d{'}alignement."
blache-etal-2008-creating,Creating and Exploiting Multimodal Annotated Corpora,2008,17,6,1,1,929,philippe blache,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The paper presents a project of the Laboratoire Parole {\&} Langage which aims at collecting, annotating and exploiting a corpus of spoken French in a multimodal perspective. The project directly meets the present needs in linguistics where a growing number of researchers become aware of the fact that a theory of communication which aims at describing real interactions should take into account the complexity of these interactions. However, in order to take into account such a complexity, linguists should have access to spoken corpora annotated in different fields. The paper presents the annotation schemes used in phonetics, morphology and syntax, prosody, gestuality at the LPL together with the type of linguistic description made from the annotations seen in two examples."
sitbon-etal-2008-evaluation,Evaluation of Lexical Resources and Semantic Networks on a Corpus of Mental Associations,2008,8,3,3,1,34164,laurianne sitbon,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"When a user cannot find a word, he may think of semantically related words that could be used into an automatic process to help him. This paper presents an evaluation of lexical resources and semantic networks for modelling mental associations. A corpus of associations has been constructed for its evaluation. It is composed of 20 low frequency target words each associated 5 times by 20 users. In the experiments we look for the target word in propositions made from the associated words thanks to 5 different resources. The results show that even if each resource has a useful specificity, the global recall is low. An experiment to extract common semantic features of several associations showed that we cannot expect to see the target word below a rank of 20 propositions."
sitbon-etal-2008-evaluating,Evaluating Robustness Of A {QA} System Through A Corpus Of Real-Life Questions,2008,7,1,3,1,34164,laurianne sitbon,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the sequential evaluation of the question answering system SQuaLIA. This system is based on the same sequential process as most statistical question answering systems, involving 4 main steps from question analysis to answer extraction.The evaluation is based on a corpus made from 20 questions taken in the set of an evaluation campaign and which were well answered by SQuaLIA. Each of the 20 questions has been typed by 17 native participants, non natives and dyslexics. They were vocally instructed the target of each question. Each of the 4 analysis steps of the system involves a loss of accuracy, until an average of 60 of right answers at the end of the process. The main cause of this loss seems to be the orthographic mistakes users make on nouns."
2008.jeptalnrecital-long.29,Influence de la qualit{\\'e} de l{'}{\\'e}tiquetage sur le chunking : une corr{\\'e}lation d{\\'e}pendant de la taille des chunks,2008,8,14,1,1,929,philippe blache,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous montrons dans cet article qu{'}il existe une corr{\'e}lation {\'e}troite existant entre la qualit{\'e} de l{'}{\'e}tiquetage morpho-syntaxique et les performances des chunkers. Cette corr{\'e}lation devient lin{\'e}aire lorsque la taille des chunks est limit{\'e}e. Nous appuyons notre d{\'e}monstration sur la base d{'}une exp{\'e}rimentation conduite suite {\`a} la campagne d{'}{\'e}valuation Passage 2007 (de la Clergerie et al., 2008). Nous analysons pour cela les comportements de deux analyseurs ayant particip{\'e} {\`a} cette campagne. L{'}interpr{\'e}tation des r{\'e}sultats montre que la t{\^a}che de chunking, lorsqu{'}elle vise des chunks courts, peut {\^e}tre assimil{\'e}e {\`a} une t{\^a}che de {``}super-{\'e}tiquetage{''}."
2007.jeptalnrecital-poster.26,Traitements phrastiques phon{\\'e}tiques pour la r{\\'e}{\\'e}criture de phrases dysorthographi{\\'e}es,2007,-1,-1,3,1,34164,laurianne sitbon,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article d{\'e}crit une m{\'e}thode qui combine des hypoth{\`e}ses graph{\'e}miques et phon{\'e}tiques au niveau de la phrase, {\`a} l{'}aide d{'}une r{\'e}pr{\'e}sentation en automates {\`a} {\'e}tats finis et d{'}un mod{\`e}le de langage, pour la r{\'e}{\'e}criture de phrases tap{\'e}es au clavier par des dysorthographiques. La particularit{\'e} des {\'e}crits dysorthographi{\'e}s qui emp{\^e}che les correcteurs orthographiques d{'}{\^e}tre efficaces pour cette t{\^a}che est une segmentation en mots parfois incorrecte. La r{\'e}{\'e}criture diff{\`e}re de la correction en ce sens que les phrases r{\'e}{\'e}crites ne sont pas {\`a} destination de l{'}utilisateur mais d{'}un syst{\`e}me automatique, tel qu{'}un moteur de recherche. De ce fait l{'}{\'e}valuation est conduite sur des versions filtr{\'e}es et lemmatis{\'e}es des phrases. Le taux d{'}erreurs mots moyen passe de 51 {\%} {\`a} 20 {\%} avec notre m{\'e}thode, et est de 0 {\%} sur 43 {\%} des phrases test{\'e}es."
W06-2304,A Robust and Efficient Parser for Non-Canonical Inputs,2006,15,2,1,1,929,philippe blache,Proceedings of the Workshop on {ROMAND} 2006:Robust Methods in Analysis of Natural language Data,0,"We present in this paper a parser relying on a constraint-based formalism called Property Grammar. We show how constraints constitute an efficient solution in parsing non canonical material such as spoken language transcription or e-mails. This technique, provided that it is implemented with some control mechanisms, is very efficient. Some results are presented, from the French parsing evaluation campaign EASy."
P06-1008,Acceptability Prediction by Means of Grammaticality Quantification,2006,15,12,1,1,929,philippe blache,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We propose in this paper a method for quantifying sentence grammaticality. The approach based on Property Grammars, a constraint-based syntactic formalism, makes it possible to evaluate a grammaticality index for any kind of sentence, including ill-formed ones. We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis. The results show that the derived grammaticality index is a fairly good tracer of acceptability scores."
vanrullen-etal-2006-constraint,Constraint-Based Parsing as an Efficient Solution: Results from the Parsing Evaluation Campaign {EAS}y,2006,11,11,2,1,43435,tristan vanrullen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,This paper describes the unfolding of the EASy evaluation campaign for french parsers as well as the techniques employed for the participation of laboratory LPL to this campaign. Three symbolic parsers based on a same resource and a same formalism (Property Grammars) are described and evaluated. The first results of this evaluation are analyzed and lead to the conclusion that symbolic parsing in a constraint-based formalism is efficient and robust.
2006.jeptalnrecital-poster.5,M{\\'e}canismes de contr{\\^o}le pour l{'}analyse en Grammaires de Propri{\\'e}t{\\'e}s,2006,-1,-1,1,1,929,philippe blache,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Les m{\'e}thodes d{'}analyse syntaxiques hybrides, reposant {\`a} la fois sur des techniques statistiques et symboliques, restent peu exploit{\'e}es. Dans la plupart des cas, les informations statistiques sont int{\'e}gr{\'e}es {\`a} un squelette contextfree et sont utilis{\'e}es pour contr{\^o}ler le choix des r{\`e}gles ou des structures. Nous proposons dans cet article une m{\'e}thode permettant de calculer un indice de corr{\'e}lation entre deux objets linguistiques (cat{\'e}gories, propri{\'e}t{\'e}s). Nous d{\'e}crivons une utilisation de cette notion dans le cadre de l{'}analyse des Grammaires de Propri{\'e}t{\'e}s. L{'}indice de corr{\'e}lation nous permet dans ce cas de contr{\^o}ler {\`a} la fois la s{\'e}lection des constituants d{'}une cat{\'e}gorie, mais {\'e}galement la satisfaction des propri{\'e}t{\'e}s qui la d{\'e}crivent."
2006.jeptalnrecital-long.31,Vers une pr{\\'e}diction automatique de la difficult{\\'e} d{'}une question en langue naturelle,2006,-1,-1,5,1,34164,laurianne sitbon,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous proposons et testons deux m{\'e}thodes de pr{\'e}diction de la capacit{\'e} d{'}un syst{\`e}me {\`a} r{\'e}pondre {\`a} une question factuelle. Une telle pr{\'e}diciton permet de d{\'e}terminer si l{'}on doit initier un dialogue afin de pr{\'e}ciser ou de reformuler la question pos{\'e}e par l{'}utilisateur. La premi{\`e}re approche que nous proposons est une adaptation d{'}une m{\'e}thode de pr{\'e}diction dans le domaine de la recherche documentaire, bas{\'e}e soit sur des machines {\`a} vecteurs supports (SVM) soit sur des arbres de d{\'e}cision, avec des crit{\`e}res tels que le contenu des questions ou des documents, et des mesures de coh{\'e}sion entre les documents ou passages de documents d{'}o{\`u} sont extraits les r{\'e}ponses. L{'}autre approche vise {\`a} utiliser le type de r{\'e}ponse attendue pour d{\'e}cider de la capacit{\'e} du syst{\`e}me {\`a} r{\'e}pondre. Les deux approches ont {\'e}t{\'e} test{\'e}es sur les donn{\'e}es de la campagne Technolangue EQUER des syst{\`e}mes de questions-r{\'e}ponses en fran{\c{c}}ais. L{'}approche {\`a} base de SVM est celle qui obtient les meilleurs r{\'e}sultats. Elle permet de distinguer au mieux les questions faciles, celles auxquelles notre syst{\`e}me apporte une bonne r{\'e}ponse, des questions difficiles, celles rest{\'e}es sans r{\'e}ponses ou auxquelles le syst{\`e}me a r{\'e}pondu de mani{\`e}re incorrecte. A l{'}oppos{\'e} on montre que pour notre syst{\`e}me, le type de r{\'e}ponse attendue (personnes, quantit{\'e}s, lieux...) n{'}est pas un facteur d{\'e}terminant pour la difficult{\'e} d{'}une question."
2005.jeptalnrecital-long.10,Combiner analyse superficielle et profonde : bilan et perspectives,2005,-1,-1,1,1,929,philippe blache,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}analyse syntaxique reste un probl{\`e}me complexe au point que nombre d{'}applications n{'}ont recours qu{'}{\`a} des analyseurs superficiels. Nous faisons dans cet article le point sur les notions d{'}analyse superficielles et profondes en proposant une premi{\`e}re caract{\'e}risation de la notion de complexit{\'e} op{\'e}rationnelle pour l{'}analyse syntaxique automatique permettant de distinguer objets et relations plus ou moins difficiles {\`a} identifier. Sur cette base, nous proposons un bilan des diff{\'e}rentes techniques permettant de caract{\'e}riser et combiner analyse superficielle et profonde."
2005.jeptalnrecital-court.22,"Une plateforme pour l{'}acquisition, la maintenance et la validation de ressources lexicales",2005,-1,-1,2,1,43435,tristan vanrullen,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons une plateforme de d{\'e}veloppement de lexique offrant une base lexicale accompagn{\'e}e d{'}un certain nombre d{'}outils de maintenance et d{'}utilisation. Cette base, qui comporte aujourd{'}hui 440.000 formes du Fran{\c{c}}ais contemporain, est destin{\'e}e {\`a} {\^e}tre diffus{\'e}e et remise {\`a} jour r{\'e}guli{\`e}rement. Nous exposons d{'}abord les outils et les techniques employ{\'e}es pour sa constitution et son enrichissement, notamment la technique de calcul des fr{\'e}quences lexicales par cat{\'e}gorie morphosyntaxique. Nous d{\'e}crivons ensuite diff{\'e}rentes approches pour constituer un sous-lexique de taille r{\'e}duite, dont la particularit{\'e} est de couvrir plus de 90{\%} de l{'}usage. Un tel lexique noyau offre en outre la possibilit{\'e} d{'}{\^e}tre r{\'e}ellement compl{\'e}t{\'e} manuellement avec des informations s{\'e}mantiques, de valence, pragmatiques etc."
2004.jeptalnrecital-long.21,Densit{\\'e} d{'}information syntaxique et gradient de grammaticalit{\\'e},2004,-1,-1,1,1,929,philippe blache,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article propose l{'}introduction d{'}une notion de densit{\'e} syntaxique permettant de caract{\'e}riser la complexit{\'e} d{'}un {\'e}nonc{\'e} et au-del{\`a} d{'}introduire la sp{\'e}cification d{'}un gradient de grammaticalit{\'e}. Un tel gradient s{'}av{\`e}re utile dans plusieurs cas : quantification de la difficult{\'e} d{'}interpr{\'e}tation d{'}une phrase, gradation de la quantit{\'e} d{'}information syntaxique contenue dans un {\'e}nonc{\'e}, explication de la variabilit{\'e} et la d{\'e}pendances entre les domaines linguistiques, etc. Cette notion exploite la possibilit{\'e} de caract{\'e}risation fine de l{'}information syntaxique en termes de contraintes : la densit{\'e} est fonction des contraintes satisfaites par une r{\'e}alisation pour une grammaire donn{\'e}e. Les r{\'e}sultats de l{'}application de cette notion {\`a} quelques corpus sont analys{\'e}s."
W03-3004,Meta-Level Contstraints for Linguistic Domain Interaction,2003,0,2,1,1,929,philippe blache,Proceedings of the Eighth International Conference on Parsing Technologies,0,"This paper presents a technique for the representation and the implementation of interaction relations between different domains of linguistic analysis. This solution relies on the localization of the linguistic objects in the context. The relations are then implemented by means of interaction constraints, each domain information being expressed independently."
2003.jeptalnrecital-poster.5,Vers une th{\\'e}orie cognitive de la langue bas{\\'e}e sur les contraintes,2003,-1,-1,1,1,929,philippe blache,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article fournit des {\'e}l{\'e}ments d{'}explication pour la description des relations entre les diff{\'e}rents domaines de l{'}analyse linguistique. Il propose une architecture g{\'e}n{\'e}rale en vue d{'}une th{\'e}orie form{\'e}e de plusieurs niveaux : d{'}un c{\^o}t{\'e} les grammaires de chacun des domaines et de l{'}autre des relations sp{\'e}cifiant les interactions entre ces domaines. Dans cette approche, chacun des domaines est porteur d{'}une partie de l{'}information, celle-ci r{\'e}sultant {\'e}galement de l{'}interaction entre les domaines."
van-rullen-blache-2002-evaluation,An evaluation of different symbolic shallow parsing techniques,2002,9,5,2,0,53367,tristan rullen,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,This paper presents an evaluation of four shallow parsers The interest of each of these parsers led us to imagine a parameterized multiplexer for syntactic information based on the principle of merging the common boundaries of the outputs given by each of these programs. The question of evaluating the parsers as well as the multiplexer came in the foreground with the problem of not owning reference corpora. We attempt here to demonstrate the interest of observing the xe2x80x98common boundariesxe2x80x99 produced by different parsers as good indices for the evaluation of these algorithms. Such an evaluation is proposed and tested with a set of two experiences.
C02-1104,From Shallow to Deep Parsing Using Constraint Satisfaction,2002,18,12,2,0,50482,jeanmarie balfourier,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We present in this paper a technique allowing to choose the parsing granularity within the same approach relying on a constraint-based formalism. Its main advantage lies in the fact that the same linguistic resources are used whatever the granularity. Such a method is useful in particular for systems such as text-to-speech that usually need a simple bracketing, but in some cases requires a precise syntactic structure. We illustrate this method in comparing the results for three different granularity levels and give some figures about their respective performance in parsing a tagged corpus."
2002.jeptalnrecital-long.18,Variabilit{\\'e} et d{\\'e}pendances des composants linguistiques,2002,-1,-1,1,1,929,philippe blache,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons dans cet article un cadre d{'}explication des relations entre les diff{\'e}rents composants de l{'}analyse linguistique (prosodie, syntaxe, s{\'e}mantique, etc.). Nous proposons un principe sp{\'e}cifiant un {\'e}quilibre pour un objet linguistique donn{\'e} entre ces diff{\'e}rents composants sous la forme d{'}un poids (pr{\'e}cisant l{'}aspect marqu{\'e} de l{'}objet d{\'e}crit) d{\'e}fini pour chacun d{'}entre eux et d{'}un seuil (correspondant {\`a} la somme de ces poids) {\`a} atteindre. Une telle approche permet d{'}expliquer certains ph{\'e}nom{\`e}nes de variabilit{\'e} : le choix d{'}une {``}tournure{''} {\`a} l{'}int{\'e}rieur d{'}un des composants peut varier {\`a} condition que son poids n{'}emp{\^e}che pas d{'}atteindre le seuil sp{\'e}cifi{\'e}. Ce type d{'}information, outre son int{\'e}r{\^e}t purement linguistique, constitue le premier {\'e}l{\'e}ment de r{\'e}ponse pour l{'}introduction de la variabilit{\'e} dans des applications comme les syst{\`e}mes de g{\'e}n{\'e}ration ou de synth{\`e}se de la parole."
W01-1820,Property Grammars: A Flexible Constraint-Based Approach to Parsing,2001,0,19,1,1,929,philippe blache,Proceedings of the Seventh International Workshop on Parsing Technologies,0,None
2001.jeptalnrecital-long.8,D{\\'e}pendances {\\`a} distance dans les grammaires de propri{\\'e}t{\\'e}s : l{'}exemple des disloqu{\\'e}es,2001,-1,-1,1,1,929,philippe blache,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article propose une description des d{\'e}pendances {\`a} distances s{'}appuyant sur une approche totalement d{\'e}clarative, les grammaires de propri{\'e}t{\'e}s, d{\'e}crivant l{'}information linguistique sous la forme de contraintes. L{'}approche d{\'e}crite ici consiste {\`a} introduire de fa{\c{c}}on dynamique en cours d{'}analyse de nouvelles contraintes, appel{\'e}es propri{\'e}t{\'e}s distantes. Cette notion est illustr{\'e}e par la description du ph{\'e}nom{\`e}ne des disloqu{\'e}es en fran{\c{c}}ais."
2000.iwpt-1.30,Property Grammars: a Solution for Parsing with Constraints,2000,-1,-1,1,1,929,philippe blache,Proceedings of the Sixth International Workshop on Parsing Technologies,0,
P98-1019,Parsing Ambiguous Structures using Controlled Disjunctions and Unary Quasi-Trees,1998,12,7,1,1,929,philippe blache,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"The problem of parsing ambiguous structures concerns (i) their representation and (ii) the specification of mechanisms allowing to delay and control their evaluation. We first propose to use a particular kind of disjunctions called controlled disjunctions: these formulae allows the representation and the implementation of specific constraints that can occur between ambiguous values. But an efficient control of ambiguous structures also has to take into account lexical as well as syntactic information concerning this object. We then propose the use of unary quasi-trees specifying constraints at these different levels. The two devices allow an efficient implementation of the control of the ambiguity. Moreover, they are independent from a particular formalism and can be used whatever the linguistic theory."
C98-1019,Parsing Ambiguous Structures using Controlled Disjunctions and Unary Quasi-Trees,1998,12,7,1,1,929,philippe blache,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"The problem of parsing ambiguous structures concerns (i) their representation and (ii) the specification of mechanisms allowing to delay and control their evaluation. We first propose to use a particular kind of disjunctions called controlled disjunctions: these formulae allows the representation and the implementation of specific constraints that can occur between ambiguous values. But an efficient control of ambiguous structures also has to take into account lexical as well as syntactic information concerning this object. We then propose the use of unary quasi-trees specifying constraints at these different levels. The two devices allow an efficient implementation of the control of the ambiguity. Moreover, they are independent from a particular formalism and can be used whatever the linguistic theory."
1997.iwpt-1.5,Disambiguating with Controlled Disjunctions,1997,12,4,1,1,929,philippe blache,Proceedings of the Fifth International Workshop on Parsing Technologies,0,"In this paper, we propose a disambiguating technique called controlled disjunctions. This extension of the so-called named disjunctions relies on the relations existing between feature values (covariation, control, etc.). We show that controlled disjunctions can implement different kind of ambiguities in a consistent and homogeneous way. We describe the integration of controlled disjunctions into a HPSG feature structure representation. Finally, we present a direct implementation by means of delayed evaluation and we develop an example within the functional programming paradigm."
C92-1016,Using Active Constraints to Parse {GPSG}s,1992,9,3,1,1,929,philippe blache,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Active constraints of the constraint logic programming paradigm allow (1) the reduction of the search space of programs and (2) a very concise representation of the problems. These two properties are particularly interesting for parsing problems: they can help us to reduce non-determinism and to use large coverage grammars. In this paper, we describe how to use such constraints for parsing ID/LP grammars and propose an implementation in Prolog III."
C90-2004,Bottom-Up Filtering: a Parsing Strategy for {GPSG},1990,16,6,1,1,929,philippe blache,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper, we propose an optimized strategy, called Bottom-Up Filtering, for parsing GPSGs. This strategy is based on a particular, high level, interpretation of GPSGs. It permits a significant reduction of the non-determinism inherent to the rule selection process."
