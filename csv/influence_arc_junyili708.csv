2020.acl-main.168,D19-3009,0,0.0256445,"Missing"
2020.acl-main.168,D19-1435,0,0.146389,"for disregarding Keep and the span of tokens to which it applies is that we can simply copy the content that is retained between Cold and Cnew , instead of generating it anew. By doing posthoc copying, we simplify learning for the model since it has to only learn what to change rather than also having to learn what to keep. We design a method to deterministically place edits in their correct positions in the absence of 4 Preliminary experiments showed that this performed better than structuring edits at the token-level as in other tasks (Shin et al., 2018; Li et al., 2018; Dong et al., 2019; Awasthi et al., 2019). 1855 spans. For the example in Figure 1, the raw sequence <Insert>in degrees<InsertEnd> does not encode information as to where “in degrees” should be inserted. To address this, we bind an insert sequence with the minimum number of words (aka “anchors”) such that the place of insertion can be uniquely identified. This results in the structure that is shown for Cedit in Figure 2. Here “angle” serves as the anchor point, identifying the insert location. Following the structure of Replace, this sequence indicates that “angle” should be replaced with “angle in degrees,” effectively inserting “in"
2020.acl-main.168,W05-0909,0,0.202241,"C’new ). This model gives preference to comments that are more likely for Mnew and are more consistent with the general style of comments.5 Similarity to Cold . So far, our model is mainly trained to produce accurate edits; however, we also follow intuitions that edits should be minimal (as an analogy, the use of Levenshtein distance in spelling correction). To give preference to predictions that accurately update the comment with minimal modifications, we use similarity to Cold as a heuristic for reranking. We measure similarity between the parsed candidate prediction and Cold using METEOR (Banerjee and Lavie, 2005). Reranking score. The reranking score for each candidate is a linear combination of the original beam score, the generation likelihood, and the similarity to Cold with coefficients 0.5, 0.3, and 0.2 respectively (tuned on validation data). 7 Data We extracted examples from popular, open-source Java projects using GitHub’s commit history. We extract pairs of the form (method, comment) for the same method across two consecutive commits where there is a simultaneous change to both the code and comment. This creates somewhat noisy data for the task of comment update; Appendix B describes filterin"
2020.acl-main.168,D12-1091,0,0.0163893,"text-editing metrics to measure how well our system learns to edit: SARI (Xu et al., 2016), originally proposed to evaluate text simplification, is essentially the average of N-gram F1 scores corresponding to add, delete, and keep edit operations;7 GLEU (Napoles et al., 2015), used in grammatical error correction and style transfer, takes into account the source sentence and deviates from BLEU by giving more importance to n-grams that have been correctly changed. Results: We report automatic metrics averaged across three random initializations for all learned models, and use bootstrap tests (Berg-Kirkpatrick et al., 2012) for statistical significance. Table 2 presents the results. While reranking using Cold appears to help the generation model, it still substantially underperforms all other models, across all metrics. Although this model is trained on considerably more data, it does not have access to Cold during training and uses fewer inputs and consequently has less context than the edit model. Reranking slightly deteriorates the edit model’s 7 Although the original formulation only used precision for the delete operation, more recent work computes F1 for this as well (Dong et al., 2019; Alva-Manchego et al"
2020.acl-main.168,P19-1331,0,0.135615,"ence. The intuition for disregarding Keep and the span of tokens to which it applies is that we can simply copy the content that is retained between Cold and Cnew , instead of generating it anew. By doing posthoc copying, we simplify learning for the model since it has to only learn what to change rather than also having to learn what to keep. We design a method to deterministically place edits in their correct positions in the absence of 4 Preliminary experiments showed that this performed better than structuring edits at the token-level as in other tasks (Shin et al., 2018; Li et al., 2018; Dong et al., 2019; Awasthi et al., 2019). 1855 spans. For the example in Figure 1, the raw sequence <Insert>in degrees<InsertEnd> does not encode information as to where “in degrees” should be inserted. To address this, we bind an insert sequence with the minimum number of words (aka “anchors”) such that the place of insertion can be uniquely identified. This results in the structure that is shown for Cedit in Figure 2. Here “angle” serves as the anchor point, identifying the insert location. Following the structure of Replace, this sequence indicates that “angle” should be replaced with “angle in degrees,” ef"
2020.acl-main.168,Q18-1031,0,0.0330691,"ang et al., 2017; Xu et al., 2019). Instead of generating natural language content from scratch as done in their work, we focus on applying edits to existing natural language text. We also show that generating a comment from scratch does not perform as well as our proposed edit model for the comment update setting. Editing natural language text: Approaches for editing natural language text have been studied extensively through tasks such as sentence simplification (Dong et al., 2019), style transfer (Li et al., 2018), grammatical error correction (Awasthi et al., 2019), and language modeling (Guu et al., 2018). The focus of this prior work is to revise sentences to conform to stylistic and grammatical conventions, and it does not generally consider broader contextual constraints. On the contrary, our goal is not to make cosmetic revisions to a given span of text, but rather amend its semantic meaning to be in sync with the content of a separate body of information on which it is dependent. More recently, Shah et al. (2020) proposed an approach for rewriting an outdated sentence based on a sentence stating a new factual claim, which is more closely aligned with our task. However, in our case, the se"
2020.acl-main.168,passonneau-2006-measuring,0,\N,Missing
2020.acl-main.168,D15-1166,0,\N,Missing
2020.acl-main.168,P02-1040,0,\N,Missing
2020.acl-main.168,P13-2007,0,\N,Missing
2020.acl-main.168,P15-2097,0,\N,Missing
2020.acl-main.168,D16-1230,0,\N,Missing
2020.acl-main.168,P16-1195,0,\N,Missing
2020.acl-main.168,Q16-1029,0,\N,Missing
2020.acl-main.168,N19-1317,0,\N,Missing
2020.acl-main.168,N19-1349,1,\N,Missing
2020.acl-main.471,P15-1162,0,0.0319699,"ntal Setup We consider both traditional neural models and pretrained language models. We implement our models in PyTorch (Paszke et al., 2019) and perform all experiments on an NVIDIA Titan V GPU. Training and optimization hyperparameters are detailed in Appendix C. We report mean performance across 10 runs, each with a different random initialization. Below, we elaborate on our models: Traditional Neural Models. Each is equipped with 200D GloVe embeddings pre-trained on 2B tweets (Pennington et al., 2014): (1) Logistic Regression: We average the word embeddings of each token in the sequence (Iyyer et al., 2015); (2) CNN: A word-level CNN (Kim, 2014) with 100 filters of size [3, 4, 5] obtains representations. They are max-pooled and concatenated row-wise. We also experiment with a character-level CNN with filter sizes [5, 6, 7]; (3) GRU: A one-layer, unidirectional GRU (Cho et al., 2014) with a hidden dimension of 100 obtains features, which are mean pooled. For all models, penultimate representations are projected with a weight matrix W ∈ Rd×2 . Pre-trained Language Models. We fine-tune base versions of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) using the HuggingFace Transformers libr"
2020.acl-main.471,D18-1147,1,0.798599,"Missing"
2020.acl-main.471,D14-1181,0,0.00700917,"odels and pretrained language models. We implement our models in PyTorch (Paszke et al., 2019) and perform all experiments on an NVIDIA Titan V GPU. Training and optimization hyperparameters are detailed in Appendix C. We report mean performance across 10 runs, each with a different random initialization. Below, we elaborate on our models: Traditional Neural Models. Each is equipped with 200D GloVe embeddings pre-trained on 2B tweets (Pennington et al., 2014): (1) Logistic Regression: We average the word embeddings of each token in the sequence (Iyyer et al., 2015); (2) CNN: A word-level CNN (Kim, 2014) with 100 filters of size [3, 4, 5] obtains representations. They are max-pooled and concatenated row-wise. We also experiment with a character-level CNN with filter sizes [5, 6, 7]; (3) GRU: A one-layer, unidirectional GRU (Cho et al., 2014) with a hidden dimension of 100 obtains features, which are mean pooled. For all models, penultimate representations are projected with a weight matrix W ∈ Rd×2 . Pre-trained Language Models. We fine-tune base versions of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) using the HuggingFace Transformers library (Wolf et al., 2019). We 6 We also e"
2020.acl-main.471,passonneau-2004-computing,0,0.0286276,"Missing"
2020.acl-main.471,D14-1162,0,0.0846748,"ards the expression of more than one emotion. The predicate “helped us” implies the user admires Mexico for providing aid, and the exclamation mark is indicative of JOY . In addition, our samples contain a mix of implicit and explicit emotions, which lexical information alone cannot resolve. In the third tweet, there are no particular words that point towards ANGER and ANNOYANCE , but we can infer the user is upset that the media is not prioritizing Hurricane Maria. Finally, our emotion prediction tasks cannot be solved by simply retrofitting pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014) or contextualized representations (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019), which we also empirically show in our experiments (§5). These methods work best for explicit emotion detection as they largely overfit to sparse lex5293 Plutchik-8 Plutchik-24 Emotion Abbrv. Emotion Abbrv. aggressiveness agrsv rage anger annoyance rage anger anyce optimism optsm vigilance anticipation interest vglnc antcp inrst love love ecstasy joy serenity ecsty joy srnty submission sbmsn admiration trust acceptance admrn trust acptn awe awe terror fear apprehension trror fear aprhn disapproval d"
2020.acl-main.471,D11-1141,0,0.217275,"Missing"
2020.acl-main.471,W18-5446,0,0.0785446,"Missing"
2020.acl-main.471,L18-1192,0,0.0193183,"leted by 5 workers. 3.3 Inter-Annotator Agreement In this section, we elaborate on our PEA metric for computing inter-annotator agreement with finegrained emotion labels. Challenges. Fine-grained emotion annotation presents several challenges for evaluating interannotator agreement. First, because a tweet can convey multiple emotions, we allow workers to select more than one Plutchik-24 emotion. This implies an agreement metric must support scoring sets of categorical values. Passonneau (2004) use set distance metrics for capturing agreement between coreference cluster annotations. Similarly, Wood et al. (2018) incorporate Jaccard’s similarity in Krippendorff’s alpha. However, these methods would penalize fine-grained emotions equally, which is not ideal. For the Plutchik wheel, the proximity of any two emotions represents their relatedness. For example, TRUST and ADMIRATION belong to the same emotion group while LOATHING and ADMIRATION are orthogonal to each other. Figure 1: Visualization of the PEA metric. The unit circle is superimposed on the Plutchik wheel, and each Plutchik-8 emotion is assigned a radian value. In this example, the (normalized) distance between the emoπ tions corresponding to"
2020.acl-main.471,S07-1013,0,0.109486,"rprise that many take to social media (e.g., Twitter) to share their feelings. Social media websites, as a result, have become an essential platform for understanding the expression and perception of emotions at a significantly larger scale (Mohammad, 2012; Wang et al., 2012; Mohammad and Kiritchenko, 2015; Volkova and Bachrach, 2016; Abdul-Mageed and Ungar, 2017), with far reaching potential influences from academic research to public policy (Dennis et al., 2006; Fritze et al., 2008; Fraustino et al., 2012). While natural language processing methods have been effective for emotion detection (Strapparava and Mihalcea, 2007), existing resources struggle in disaster-centric domains, in part due to distributional shifts. Emotion detection in natural disasters (e.g., hurricanes) requires implicit reasoning not available as surface-level lexical information. For example, in “of course, [we]1 still have the [storm surge]2 coming,” given the context, we can reasonably infer discontent towards the “storm surge” despite the absence of polarizing words. Therefore, distantly supervised techniques largely based on lexical units (Mohammad and Turney, 2013; Abdul-Mageed and Ungar, 2017) fail to capture this type of deeper sem"
2020.acl-main.471,strapparava-etal-2012-parallel,0,0.0711141,"Missing"
2020.acl-main.471,P16-1148,0,0.162166,"each year (Ritchie and Roser, 2020). These catastrophic events not only induce material destruction but also stir an integral part of being human: our emotions. Disasters adversely affect individuals’ mental states (Fritz and Marks, 1954; Kinston and Rosser, 1974), and therefore it is no surprise that many take to social media (e.g., Twitter) to share their feelings. Social media websites, as a result, have become an essential platform for understanding the expression and perception of emotions at a significantly larger scale (Mohammad, 2012; Wang et al., 2012; Mohammad and Kiritchenko, 2015; Volkova and Bachrach, 2016; Abdul-Mageed and Ungar, 2017), with far reaching potential influences from academic research to public policy (Dennis et al., 2006; Fritze et al., 2008; Fraustino et al., 2012). While natural language processing methods have been effective for emotion detection (Strapparava and Mihalcea, 2007), existing resources struggle in disaster-centric domains, in part due to distributional shifts. Emotion detection in natural disasters (e.g., hurricanes) requires implicit reasoning not available as surface-level lexical information. For example, in “of course, [we]1 still have the [storm surge]2 comin"
2020.acl-main.471,J08-4004,0,\N,Missing
2020.acl-main.471,S12-1033,0,\N,Missing
2020.acl-main.471,W18-6205,0,\N,Missing
2020.acl-main.471,N19-1423,0,\N,Missing
2020.acl-main.471,D19-1433,0,\N,Missing
2020.acl-main.471,D19-6117,1,\N,Missing
2020.acl-main.471,P17-1067,0,\N,Missing
2020.acl-main.471,P18-1099,0,\N,Missing
2020.coling-main.419,L18-1269,0,0.0283767,"asure word embeddings through a semantic analogy task and a syntactic analogy task. Pennington et al. (2014) further expands the testing set to include other word similarity and named entity recognition tasks. Similar evaluation procedures are also used for sentence representations (Kiros et al., 2015). However, as different researchers use different evaluation pipelines on different datasets, results reported in the papers are not always fully comparable, especially in the case where the datasets are small, where a minor change in evaluation can lead to big differences in outcomes. SentEval (Conneau and Kiela, 2018) addresses the above problem by introducing a standard evaluation pipeline using a set of popular sentence embedding evaluation datasets. GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) further improve SentEval by providing benchmarks for natural language understanding tasks, ensuring that results from different models are consistent and comparable. They introduce a set of more difficult datasets and a model-agnostic evaluation pipeline. Along with other reading comprehension tasks like SQuAD (Rajpurkar et al., 2016) and RACE (Lai et al., 2017), GLUE and SuperGLUE have become standa"
2020.coling-main.419,D18-1269,0,0.0230419,"odels to judge whether keywords can summarize the document. OCNLI Original Chinese Natural Language Inference (OCNLI, Hu et al. (2020)) is collected closely following procedures of MNLI (Williams et al., 2018). OCNLI is composed of 56k inference pairs from five genres: news, government, fiction, TV transcripts and Telephone transcripts, where the premises are collected from Chinese sources, and universities students in language majors are hired to write the hypotheses. The annotator agreement is on par with MNLI. We believe the non-translation nature of OCNLI makes it more suitable than XNLI (Conneau et al., 2018) as an NLU task specific for Chinese. 3 https://dc.cloud.alipay.com/index#/topic/intro?id=3 4765 4.3 Machine Reading Comprehension CMRC 2018 CMRC 2018 (Cui et al., 2019) is a span-extraction based dataset for Chinese machine reading comprehension. This dataset contains about 19,071 human-annotated questions from Wikipedia paragraphs. In CMRC 2018, all samples are composed of contexts, questions, and related answers. Furthermore, the answers are the text spans in contexts. ChID ChID (Zheng et al., 2019) is a large-scale Chinese IDiom cloze test dataset, which contains about 498,611 passages wit"
2020.coling-main.419,D19-1600,1,0.884996,"Missing"
2020.coling-main.419,2020.findings-emnlp.58,1,0.8961,"ultiple candidate-context pairs to a shared classifier and get corresponding scores. All the models are implemented in both TensorFlow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019). Models We evaluate CLUE on the following public available pre-trained models: • BERT-base, we use the base model (12 layer, hidden size 768) published by (Devlin et al., 2019), which was pre-trained the on Chinese Wikipedia dump of about 0.4 billion tokens. • BERT-wwm-ext-base, a model with the same configuration of BERT-base except it uses whole word masking and is trained on additional 5 billion tokens (Cui et al., 2020). • ALBERT-tiny/xxlarge, ALBERT (Lan et al., 2019) is a recent language representation model. We use: 1) a tiny version6 with only 4 layers and a hidden size of 312, and 2) an xxlarge version7 with 12 layers and a hidden size of 4096. Both are trained on the CLUE pre-training corpus. • ERNIE-base (Sun et al., 2019c) extends BERT-base with additional training data and leverages knowledge from Knowledge Graphs. • XLNet-mid8 , a model with 24 layers and a hidden size of 768, with sentencepiece tokenzier and other techniques from Yang et al. (2019). • RoBERTa-large uses a 24 layer RoBERTa (Liu et"
2020.coling-main.419,N19-1423,0,0.630609,"y-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text. To establish results on these tasks, we report scores using an exhaustive set of current state-of-the-art pre-trained Chinese models (9 in total). We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on Chinese NLU. Our benchmark is released at https://www.CLUEbenchmarks.com 1 Introduction Full-network pre-training methods such as BERT (Devlin et al., 2019) and their improved versions (Yang et al., 2019; Liu et al., 2019; Lan et al., 2019) have led to significant performance boosts across many natural language understanding (NLU) tasks. One key driving force behind such improvements and rapid iterations of models is the general use of evaluation benchmarks. These benchmarks use a single metric to evaluate the performance of models across a wide range of tasks. However, existing language evaluation benchmarks are mostly in English, e.g., GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019). To the best of our knowledge, there is no general"
2020.coling-main.419,2020.findings-emnlp.314,1,0.582971,"Missing"
2020.coling-main.419,D18-2012,0,0.0306501,"Missing"
2020.coling-main.419,D17-1082,0,0.106167,"ferences in outcomes. SentEval (Conneau and Kiela, 2018) addresses the above problem by introducing a standard evaluation pipeline using a set of popular sentence embedding evaluation datasets. GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) further improve SentEval by providing benchmarks for natural language understanding tasks, ensuring that results from different models are consistent and comparable. They introduce a set of more difficult datasets and a model-agnostic evaluation pipeline. Along with other reading comprehension tasks like SQuAD (Rajpurkar et al., 2016) and RACE (Lai et al., 2017), GLUE and SuperGLUE have become standard testing benchmarks for pre-training methods such as BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019). We believe a similar problem exists in Chinese language understanding evaluation. Although more and more Chinese linguistic tasks (Liu et al., 2018; Cui et al., 2019) have been proposed, there is still a need for a standard evaluation pipeline and an evaluation benchmark with a set of diverse and difficult language understanding tasks. 3 CLUE Overview CLUE consists of 1) nine language understanding tasks in Chinese, 2) a large-scale raw dataset"
2020.coling-main.419,C18-1166,0,0.0264469,"s for natural language understanding tasks, ensuring that results from different models are consistent and comparable. They introduce a set of more difficult datasets and a model-agnostic evaluation pipeline. Along with other reading comprehension tasks like SQuAD (Rajpurkar et al., 2016) and RACE (Lai et al., 2017), GLUE and SuperGLUE have become standard testing benchmarks for pre-training methods such as BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019). We believe a similar problem exists in Chinese language understanding evaluation. Although more and more Chinese linguistic tasks (Liu et al., 2018; Cui et al., 2019) have been proposed, there is still a need for a standard evaluation pipeline and an evaluation benchmark with a set of diverse and difficult language understanding tasks. 3 CLUE Overview CLUE consists of 1) nine language understanding tasks in Chinese, 2) a large-scale raw dataset for pre-training and a small hand-crafted diagnostic dataset for linguistic analysis, and 3) a ranking system, a leaderboard and a toolkit. 3.1 Task Selection For this benchmark, we selected nine different tasks, to ensure that the benchmark tests different aspects of pre-trained models. To ensure"
2020.coling-main.419,P19-1334,0,0.0235889,"le 4. Monotonicity is the hardest, similar to GLUE diagnostics (Wang et al., 2018). It seems that BERT also has a hard time dealing with comparatives. An interesting case is the example of lexical semantics in Table 4, where the two two-character words “sad” (难过 hard-pass) and “ugly” (难看 hard-look) in Chinese have the same first character (难 hard). Thus the premise and hypothesis only differ in the last character, which two out of three models have decided to ignore. One possible explanation is that these models in Chinese are also using the simple lexical overlap heuristic, as illustrated in McCoy et al. (2019) for English. 8 Conclusions and Future Work In this paper, we present a Chinese Language Understanding Evaluation (CLUE) benchmark, which consists of 9 natural language understanding tasks and a linguistically motivated diagnostic dataset, along with an online leaderboard for model evaluation. In addition, we release a large clean crawled raw text corpus that can be directly used for pre-training Chinese models. To the best of our knowledge, CLUE is the first comprehensive language understanding benchmark developed for Chinese. We evaluate several latest language representation models on CLUE"
2020.coling-main.419,D14-1162,0,0.0884601,"ts containing multiple linguistic phenomena, some of which are unique to Chinese. (4) A user-friendly toolkit, as well as an online leaderboard with an auto-evaluation system, supporting all our evaluation tasks and models, with which researchers can reproduce experimental results and compare the performance of different submitted models easily. 2 Related Work It has been a common practice to evaluate language representations on different intrinsic and downstream NLP tasks. For example, Mikolov et al. (2013) measure word embeddings through a semantic analogy task and a syntactic analogy task. Pennington et al. (2014) further expands the testing set to include other word similarity and named entity recognition tasks. Similar evaluation procedures are also used for sentence representations (Kiros et al., 2015). However, as different researchers use different evaluation pipelines on different datasets, results reported in the papers are not always fully comparable, especially in the case where the datasets are small, where a minor change in evaluation can lead to big differences in outcomes. SentEval (Conneau and Kiela, 2018) addresses the above problem by introducing a standard evaluation pipeline using a s"
2020.coling-main.419,D16-1264,0,0.0489621,"in evaluation can lead to big differences in outcomes. SentEval (Conneau and Kiela, 2018) addresses the above problem by introducing a standard evaluation pipeline using a set of popular sentence embedding evaluation datasets. GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) further improve SentEval by providing benchmarks for natural language understanding tasks, ensuring that results from different models are consistent and comparable. They introduce a set of more difficult datasets and a model-agnostic evaluation pipeline. Along with other reading comprehension tasks like SQuAD (Rajpurkar et al., 2016) and RACE (Lai et al., 2017), GLUE and SuperGLUE have become standard testing benchmarks for pre-training methods such as BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019). We believe a similar problem exists in Chinese language understanding evaluation. Although more and more Chinese linguistic tasks (Liu et al., 2018; Cui et al., 2019) have been proposed, there is still a need for a standard evaluation pipeline and an evaluation benchmark with a set of diverse and difficult language understanding tasks. 3 CLUE Overview CLUE consists of 1) nine language understanding tasks in Chinese,"
2020.coling-main.419,Q19-1014,1,0.917436,"19,071 human-annotated questions from Wikipedia paragraphs. In CMRC 2018, all samples are composed of contexts, questions, and related answers. Furthermore, the answers are the text spans in contexts. ChID ChID (Zheng et al., 2019) is a large-scale Chinese IDiom cloze test dataset, which contains about 498,611 passages with 623,377 blanks covered from news, novels, and essays. The candidate pool contains 3,848 Chinese idioms. For each blank in the passage, there are ten candidate idioms with one golden option, several similar idioms, and others are randomly chosen from the dictionary. C3 C3 (Sun et al., 2019b) is the first free-form multiple-choice machine reading comprehension dataset for Chinese. Given a document, either a dialogue or a more formally written mixed-genre text, and a free-form question that is not limited to a single question type (e.g., yes/no questions), the task is to select the correct answer option from all (2 to 4) options associated with the corresponding question. We employ all of the 19,577 general domain problems for 13,369 documents and follow the original data splitting. These problems are collected from language exams carefully designed by educational experts for eva"
2020.coling-main.419,W18-5446,0,0.443949,"w.CLUEbenchmarks.com 1 Introduction Full-network pre-training methods such as BERT (Devlin et al., 2019) and their improved versions (Yang et al., 2019; Liu et al., 2019; Lan et al., 2019) have led to significant performance boosts across many natural language understanding (NLU) tasks. One key driving force behind such improvements and rapid iterations of models is the general use of evaluation benchmarks. These benchmarks use a single metric to evaluate the performance of models across a wide range of tasks. However, existing language evaluation benchmarks are mostly in English, e.g., GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019). To the best of our knowledge, there is no general language understanding evaluation benchmark for Chinese, whose speakers account for one-fourth of the world’s population. Also, Chinese is linguistically very different from English and other Indo-European languages, which necessitates an evaluation benchmark specifically designed for Chinese. Without such a benchmark, it would be difficult for researchers in the field to check how good their Chinese language understanding models are. To address this problem and facilitate studies in Chinese language, we intr"
2020.coling-main.419,Q18-1042,0,0.0624747,"Missing"
2020.coling-main.419,N18-1101,0,0.0356261,"L Chinese Scientific Literature dataset contains Chinese paper abstracts and their keywords from core journals of China, covering multiple fields of natural sciences and social sciences. We generate fake keywords through tf-idf and mix them with real keywords. Given an abstract and some keywords, the task is to tell whether the keywords are all original keywords of a paper. It mainly evaluates the ability of models to judge whether keywords can summarize the document. OCNLI Original Chinese Natural Language Inference (OCNLI, Hu et al. (2020)) is collected closely following procedures of MNLI (Williams et al., 2018). OCNLI is composed of 56k inference pairs from five genres: news, government, fiction, TV transcripts and Telephone transcripts, where the premises are collected from Chinese sources, and universities students in language majors are hired to write the hypotheses. The annotator agreement is on par with MNLI. We believe the non-translation nature of OCNLI makes it more suitable than XNLI (Conneau et al., 2018) as an NLU task specific for Chinese. 3 https://dc.cloud.alipay.com/index#/topic/intro?id=3 4765 4.3 Machine Reading Comprehension CMRC 2018 CMRC 2018 (Cui et al., 2019) is a span-extracti"
2020.coling-main.419,P19-1075,0,\N,Missing
2020.emnlp-main.427,N19-1052,0,0.0460966,"Missing"
2020.emnlp-main.427,C18-1248,1,0.869209,"Missing"
2020.emnlp-main.427,S19-2151,0,0.029602,"Missing"
2020.emnlp-main.427,N19-1423,0,0.544284,", finding the right solution to a problem is difficult, since advice may be spread over multiple posts and pages online. Even within the same post, not * Work done as an undergraduate student at UT Austin. † Work done at UT Austin while on the DREU undergraduate research program. Automatic identification of advice in text would thus be extremely useful. Yet, as we see above, it would also require a deep understanding of semantics and discourse pragmatics. In recent years, NLP systems based on large-scale pre-trained language models have shown impressive gains on several linguistic benchmarks (Devlin et al., 2019; Clark et al., 2020; Yang et al., 2019). However, these same models have been found to struggle at tasks that require higher-level processing (Ettinger, 2020), including giving advice (Zellers et al., 2020). This work aims to advance both our understanding of how people give advice, as well as to provide resources for learning to identify advice. First, we construct a dataset of annotations of advice in English from two advice-focused Reddit commu5295 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5295–5306, c November 16–20, 2020. 2020 Associati"
2020.emnlp-main.427,2020.tacl-1.3,0,0.0242687,"as an undergraduate student at UT Austin. † Work done at UT Austin while on the DREU undergraduate research program. Automatic identification of advice in text would thus be extremely useful. Yet, as we see above, it would also require a deep understanding of semantics and discourse pragmatics. In recent years, NLP systems based on large-scale pre-trained language models have shown impressive gains on several linguistic benchmarks (Devlin et al., 2019; Clark et al., 2020; Yang et al., 2019). However, these same models have been found to struggle at tasks that require higher-level processing (Ettinger, 2020), including giving advice (Zellers et al., 2020). This work aims to advance both our understanding of how people give advice, as well as to provide resources for learning to identify advice. First, we construct a dataset of annotations of advice in English from two advice-focused Reddit commu5295 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5295–5306, c November 16–20, 2020. 2020 Association for Computational Linguistics nities – r/AskParents and r/needadvice, totalling 18456 sentences across 684 posts (§3). These two subreddits are different in"
2020.emnlp-main.427,P18-1019,1,0.840469,"sts which contain the following highly frequent flairs: “Education”, “Career”, “Mental Health”, “Life Decisions”, and “Friendships”. Some flairs were not considered due to the lack of variety in responses. For example, in the “Medical” flair, replies often consisted of telling the original poster to see the doctor. 3.2 Annotation Task We crowdsource advice annotations from Amazon Mechanical Turk. Despite the inherent noise due to crowdsourcing (Parde and Nielsen, 2017), recent work showed that when designed carefully, aggregated crowdsourced annotations are trustworthy even for complex tasks (Nye et al., 2018). As (1) illustrates, not all sentences in a response to an advice-seeking question constitute advice. Thus, we want annotators to highlight which parts of the response to a question are advice, and which Dataset Sentences κmaj κDS AskParents 203 0.620 0.669 needadvice 110 0.680 0.681 Table 1: Gold annotator agreement on the internal task. are not. We also want to find instances of implicit advice, i.e., advice that is given indirectly, like in (2). To ensure that annotators can also identify advice that might be marked using contextual cues, we provide annotators with sufficient context. In o"
2020.emnlp-main.427,N15-1166,0,0.0116773,"drive mine he phone night adult stay set big game doctor fun bring less show love depend activity eat normal put teacher family etc minute teach allow home they area luck degree company college interview hobby student field mental course op sorry job dog anxiety hire eventually position path shit comment human online community shoe thanks note exercise depression slowly To motivate that the language of advice varies systematically from non-advice, we quantify how strongly individual lemmas are associated with advice versus non-advice text. We use the log-odds ratio as a metric of comparison (Nye and Nenkova, 2015). To counteract the tendency of log-odds scores to highlight infrequent lemmas (Monroe et al., 2017), we filter out lemmas that occurred less than 20 times in the train and validation set of our corpus. Table 5 shows the top 30 lemmas (excluding punctuation characters and numbers) from advice and non-advice sentences for each subreddit ranked by their log-odds ratio. We observe that there are fewer verbs among non-advice lemmas than advice lemmas, and that lemmas which are generally used in expressing sentiment (luck, sorry, thanks) are more likely to be found in non-advice sentences. Combined"
2020.emnlp-main.427,D17-1204,0,0.0171856,"for posting and replying to posts. We believe all of these factors contribute to two different “styles” of advice-giving. For r/needadvice, we study posts which contain the following highly frequent flairs: “Education”, “Career”, “Mental Health”, “Life Decisions”, and “Friendships”. Some flairs were not considered due to the lack of variety in responses. For example, in the “Medical” flair, replies often consisted of telling the original poster to see the doctor. 3.2 Annotation Task We crowdsource advice annotations from Amazon Mechanical Turk. Despite the inherent noise due to crowdsourcing (Parde and Nielsen, 2017), recent work showed that when designed carefully, aggregated crowdsourced annotations are trustworthy even for complex tasks (Nye et al., 2018). As (1) illustrates, not all sentences in a response to an advice-seeking question constitute advice. Thus, we want annotators to highlight which parts of the response to a question are advice, and which Dataset Sentences κmaj κDS AskParents 203 0.620 0.669 needadvice 110 0.680 0.681 Table 1: Gold annotator agreement on the internal task. are not. We also want to find instances of implicit advice, i.e., advice that is given indirectly, like in (2). To"
2020.emnlp-main.427,S19-2215,0,0.0852041,"constructed a dataset from the subreddits r/needadvice and r/AskParents as a general purpose resource for studying the breadth of advice-giving strategies. Our modelling experiments aim to establish baseline performance for rule-based models and language models at identifying advice, as well as explore how their performance varies with domain and provided context. We model advice identification as a binary classification task – given a sentence, predict whether the sentence is advice or not. Baselines We test the baseline rule-based model and the top performing rule-based submission (NTUA-IS; Potamias et al., 2019) from SEMEVAL Task 9 2019 on our dataset, and use the results of these rule based models as baselines against which to gauge the performance of more advanced ones based on pre-trained language models. The baseline model provided by Negi et al. (2019) uses search patterns to identify suggestions, including words (suggest, recommend), phrases (.*wouldslike.*if.*), and part-of-speech (POS) tags (modals, past tense verbs). However, some of these rules are naive and not intepretable – such as classifying a sentence as a suggestion if it contains a modal or the base form 5300 Utilizing pre-trained"
2020.emnlp-main.427,E12-2021,0,0.0242135,"Missing"
2020.emnlp-main.427,P19-3007,0,0.0535042,"Missing"
2020.emnlp-main.427,D18-1116,0,0.0469115,"Missing"
2020.emnlp-main.530,2020.tacl-1.30,0,0.0295435,"are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text comprehension. Several other question answering datasets contain questions that are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study lingui"
2020.emnlp-main.530,N19-1423,0,0.0319009,"tions are not simply copied from the training data or the article, though the n-gram scores are comparatively much higher than questions asked by human. 6 Question generation from scratch In this section, we assume that spans are not given, and discuss two approaches to generating questions from scratch: a pipeline approach with span prediction, and a question generation model without Models Pipeline. The pipeline approach consists of 2 stages: span prediction and question generation using Inquirer. To predict the span, we use a model similar to the BERT model for the question answering task (Devlin et al., 2019) on SQuAD 1.1 (Rajpurkar et al., 2016). We replace the concatenation passage and question with a concatenation of the target sentence and the previous sentences in the document. Now, the span to ask a question about is treated analogously to the answer span in SQuAD: we find its position in the “passage” which is now the target sentence.5 Sentence+Context. We also experiment with a Sentence+Context model, where the model has no knowledge of any span information in both training and testing; it is trained based purely on (context, sentence) pairs from our dataset. This baseline evaluates the us"
2020.emnlp-main.530,Q19-1026,0,0.117249,"ligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification in conversation (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017). However, data-driven generation of questions that reflect text understanding in a more general setting is challenging because of the lack of appropriate training data. We introduce I NQUISITIVE, a new,"
2020.emnlp-main.530,D17-1219,0,0.0197226,"these questions could be used to prioritize what information to keep. The spans and the questions could also help probing the specific and vague parts of the text, which can be useful in conversational AI. Because of the high level nature of our questions, this resource can also be useful for building education applications targeting reading comprehension. 2 Related Work Among question generation settings, ours is most related to answer-agnostic, or answer-unaware question generation: generating a question from text without specifying the location of the answer (Du et al., 2017). Recent work (Du and Cardie, 2017; Subramanian et al., 2018; Wang et al., 2019; Nakanishi et al., 2019) trains models that can extract phrases or sentences that are question-worthy, and uses this information to generate better questions. Scialom and Staiano (2019) paired the question with other sentences in the article that do not contain the answers to construct curiosity-driven questions. However, these approaches are trained by re-purposing questionanswering datasets that are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions"
2020.emnlp-main.530,N16-1014,0,0.0374568,"at are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study linguistic theories of discourse. Previous work generating clarification questions (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017) uses questions crawled on forums and product reviews. The answers to the questions were used in the models to improve the utility of the generated questio"
2020.emnlp-main.530,P17-1123,0,0.218638,"and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification in conversation (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017). However, data-driven generation of questions that reflect text understanding in a more general setting is challenging because of the lack of appropriate training data. We introduce I NQUISITIVE, a new, large-scale dataset of questions that target high level processing of document c"
2020.emnlp-main.530,L16-1620,1,0.898424,"Missing"
2020.emnlp-main.530,P14-5010,0,0.00558799,"Missing"
2020.emnlp-main.530,P18-1191,0,0.0209807,"hat lead sentences are known to be the most critical for news articles (Errico et al., 1997). For each sentence, we asked 5 distinct annotators from Amazon Mechanical Turk to ask questions. For quality control, we restrict to workers who are located in English speaking countries, and who have completed at least 100 tasks with an approval rating above 0.98. 3.3 Question validation To ensure that our final corpus contain high quality questions, we design a second crowdsourcing task for the validation of these questions, inspired by prior work that also validated crowdsourced questions manually (FitzGerald et al., 2018). At a high level, we want to ensure that the questions are grammatically correct and semantically plausible, related to the highlighted span, and not already answered in the sentence or any previous sentence in the article. Specifically, for each question gathered in Sec6546 Average length std.dev Question 7.1 Highlighted span 3.2 4.1 3.4 2.3 Table 1: Average length and standard deviation of the questions asked by workers and the chosen span tion 3.2, we show the validation annotators the first few sentences of the article up to the sentence where the question is asked, so that the validation"
2020.emnlp-main.530,J93-2004,0,0.0758434,"t questions as one reads (Section 3.2). We then discuss a second validation step for each question we collected as quality control for the data (Section 3.3). 3.1 Text sources In this work we focus on news articles as our source of documents. News articles consist of rich (yet consistent) linguistic structure around a targeted series of events, and are written to engage the readers, hence they are natural test beds for eliciting inquisitive questions that reflect high level processes. We use 1500 news articles, 500 each from three sources: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), Associated Press articles from the TIPSTER corpus (Harman and Liberman, 1993), and Newsela (Xu et al., 2015), a commonly used source in text simplification (we use the most advanced reading level only). We select articles that are not opinion pieces and contain more than 8 sentences to make sure that they are indeed news stories and that would involve sufficiently complex scenarios. 3.2 Question collection To capture questions that occur as one reads, we design a crowdsourcing task in which the annotators ask questions about what they are reading currently and without access to any upcoming"
2020.emnlp-main.530,R11-1037,0,0.0182659,"Missing"
2020.emnlp-main.530,C00-1044,0,0.366283,"tions across 1,500 documents1 , each question accompanied by the specific span of the text the question is about, illustrated in Figure 1. The questions are verified to ensure that they are grammatically correct, semantically plausible and meaningful, and not already answered in previous context. We show that the questions capture a variety of phenomena related to high-level semantic and discourse processes, e.g., making causal inferences upon seeing an event or a description, being curious about more detailed information, seeking clarification, interpreting the scale of a gradable adjective (Hatzivassiloglou and Wiebe, 2000), seeking information of an underspecified event (where key participants are missing), and seeking background knowledge. Our analyses reveal that the questions have a very different distribution from those in existing factoid and conversational question answering datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Choi et al., 2018), thus enabling research into generating natural, inquisitive questions. We further present question generation models on this data using GPT-2 (Radford et al., 2019), a state-of-the-art pre-trained language model often used in natural language generation task"
2020.emnlp-main.530,D19-5809,0,0.0257896,"ep. The spans and the questions could also help probing the specific and vague parts of the text, which can be useful in conversational AI. Because of the high level nature of our questions, this resource can also be useful for building education applications targeting reading comprehension. 2 Related Work Among question generation settings, ours is most related to answer-agnostic, or answer-unaware question generation: generating a question from text without specifying the location of the answer (Du et al., 2017). Recent work (Du and Cardie, 2017; Subramanian et al., 2018; Wang et al., 2019; Nakanishi et al., 2019) trains models that can extract phrases or sentences that are question-worthy, and uses this information to generate better questions. Scialom and Staiano (2019) paired the question with other sentences in the article that do not contain the answers to construct curiosity-driven questions. However, these approaches are trained by re-purposing questionanswering datasets that are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text compreh"
2020.emnlp-main.530,2020.findings-emnlp.3,0,0.0200645,"In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text comprehension. Several other question answering datasets contain questions that are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study linguistic theories of discourse. Previous work generating clarification questions (Rao and Daum´e"
2020.emnlp-main.530,W17-2623,0,0.030809,"Missing"
2020.emnlp-main.530,P18-2124,0,0.0778419,"T-2 (Radford et al., 2019), a state-of-the-art pre-trained language model often used in natural language generation tasks. Human evaluation reveals that our best model is able to generate high-quality questions, though still falls short of human-generated questions in terms of semantic validity, and the questions it generates are more often already answered. Additionally, our experiments explore the importance of model access to already-established common ground (article context), as well as annotations of which part of the text to ask about. Finally, transfer learning results from SQuAD 2.0 (Rajpurkar et al., 2018) show that generating inquisitive questions is a distinct task from question generation using factoid question answering datasets. 1 Data available at https://github.com/wjko2/ INQUISITIVE The capability for question generation models to simulate human-like curiosity and cognitive processing opens up a new realm of applications. One example for this sort of question generation is guided text writing for either machines or humans: we could use these questions to identify important points of information that are not mentioned yet and should be included. In text simplification and summarization,"
2020.emnlp-main.530,D16-1264,0,0.441465,"of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification in conversation (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017). Howe"
2020.emnlp-main.530,P18-1255,0,0.0634711,"Missing"
2020.emnlp-main.530,N19-1013,0,0.132128,"Missing"
2020.emnlp-main.530,Q19-1016,0,0.0217788,"the location of the answer (Du et al., 2017). Recent work (Du and Cardie, 2017; Subramanian et al., 2018; Wang et al., 2019; Nakanishi et al., 2019) trains models that can extract phrases or sentences that are question-worthy, and uses this information to generate better questions. Scialom and Staiano (2019) paired the question with other sentences in the article that do not contain the answers to construct curiosity-driven questions. However, these approaches are trained by re-purposing questionanswering datasets that are factual (Rajpurkar et al., 2016) or conversational (Choi et al., 2018; Reddy et al., 2019). In contrast, we present a new dataset targeting questions that reflect the semantic and discourse processes during text comprehension. Several other question answering datasets contain questions that are more information-seeking in nature. Some of them are collected from questions that users type in search engines (Yang et al., 2015; Bajaj et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019). Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019"
2020.emnlp-main.530,W10-4234,0,0.0433648,"e meaningful, inquisitive questions is natural to humans. Studies among children (Jirout, 2011) showed that questions serving to better understand natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of"
2020.emnlp-main.530,N18-2090,0,0.0465461,"natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) an"
2020.emnlp-main.530,2020.lrec-1.141,0,0.0403886,"Others are collected given a small amount of information on a topic sentence (Trischler et al., 2017; Clark et al., 2020) or in the context of a conversation (Choi et al., 2018; Reddy et al., 2019; Qi et al., 2020). Our data is collected from news articles and our questions are precisely anchored to spans in the article, making our questions less open-ended than those in past datasets. While Li et al. (2016b) also collected a 6545 small number of reader questions from news articles, their goal was to study underspecified phrases in sentences when considered out-of-context. Contemporaneously, Westera et al. (2020) presented a dataset of 2.4K naturally elicited questions on TED talks, with the goal to study linguistic theories of discourse. Previous work generating clarification questions (Rao and Daum´e III, 2018, 2019; Braslavski et al., 2017) uses questions crawled on forums and product reviews. The answers to the questions were used in the models to improve the utility of the generated question. In our data, clarification is only one of the pragmatic goals. In addition, we focus on news articles which contains more narrative discourse and temporal progression. 3 I NQUISITIVE: A corpus of questions T"
2020.emnlp-main.530,Q15-1021,0,0.026248,"quality control for the data (Section 3.3). 3.1 Text sources In this work we focus on news articles as our source of documents. News articles consist of rich (yet consistent) linguistic structure around a targeted series of events, and are written to engage the readers, hence they are natural test beds for eliciting inquisitive questions that reflect high level processes. We use 1500 news articles, 500 each from three sources: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), Associated Press articles from the TIPSTER corpus (Harman and Liberman, 1993), and Newsela (Xu et al., 2015), a commonly used source in text simplification (we use the most advanced reading level only). We select articles that are not opinion pieces and contain more than 8 sentences to make sure that they are indeed news stories and that would involve sufficiently complex scenarios. 3.2 Question collection To capture questions that occur as one reads, we design a crowdsourcing task in which the annotators ask questions about what they are reading currently and without access to any upcoming context. The annotators start from the beginning of an It's not enough for people to get regular moderate exer"
2020.emnlp-main.530,D15-1237,0,0.084767,"Missing"
2020.emnlp-main.530,W17-2603,0,0.0188337,"o better understand natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Ya"
2020.emnlp-main.530,D18-1424,0,0.0622216,"ext are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspecific knowledge (Yang et al., 2018) and seek clarification"
2020.emnlp-main.530,P17-2060,0,0.0155734,"questions serving to better understand natural language text are an organic reflection of curiosity, which “arise from the perception of a gap in knowledge or understanding” (Loewenstein, 1994). Because of its prominence in human cognition and behavior, being able to formulate the right question is highly sought after in intelligent systems, to reflect the ability to understand language, to gather new information, and to engage with users (Vanderwende, 2007, 2008; Piwek and Boyer, 2012; Rus et al., 2010; Huang et al., 2017). A recent line of work on data-driven question generation techniques (Zhou et al., 2017; Yuan et al., 2017; Song et al., 2018; Zhao et al., 2018) focuses on generating questions for datasets like SQuAD (Rajpurkar et al., 2016, 2018). However, factoid questions generated with an answer in mind after the user has read the full text look very different from more natural questions users might have (Kwiatkowski et al., 2019). This has led to work on “answer-agnostic” question generation (Du et al., 2017; Subramanian et al., 2018; Scialom and Staiano, 2019), but the sources of data still emphasize simple factoid questions. Other prior work used question generation to acquire domainspe"
2020.inlg-1.8,N19-1349,1,0.835157,"challenge. We use 122,499 prompt-response pairs for training and 4,801 pairs for validation. We fine-tune GPT-2 medium (345M parameters). For compatibility with GPT-2’s pre-training, we concatenate the prompt and response (separated by a delimiter) during training. GPT-2 is fine-tuned for 3 epochs using Adam (Kingma and Ba, 2015) with a learning rate of 5e-5. The cross-entropy (language modeling) loss is only calculated for the response. At test-time, the model is conditioned on the prompt (and delimiter) and generates the response. Our approach is similar to Zhang et al. (2020) and we follow Ko et al. (2019) to encourage generation of informative responses.1 For decoding, we experimented with both topk sampling (Fan et al., 2018) and nucleus sampling (Holtzman et al., 2019), and picked the better performing one upon manual inspection of the validation data. We use top-k (k=10) in this scenario. For quality assurance, we manually evaluate GPT-2’s generated responses against SpaceFusion (Gao et al., 2019), a state-of-the-art RNNbased model, re-trained on P ERSONAC HAT. The evaluation is conducted on Amazon Mechanical Turk, where 5 annotators (per HIT) chose between GPT-2 and SpaceFusion responses."
2020.inlg-1.8,D16-1020,0,0.0360521,"Missing"
2020.inlg-1.8,P03-1034,0,0.0820579,". This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustrated that GPT-2 has the ability to generate interesting and coherent text. However, analysis of GPT-2’s outputs from deeper linguistic dimensions (e.g., discourse) has largely remained unexplored. Jazz is good, because my favorite is country music. The importance of generating good discourse connectives are recognized in prior work in NLG (Biran and McKeown, 2015; Callaway, 2003). We examine to what extent does GPT-2 generate texts that uphold plausible discourse relations, once a discourse connective (usually 1-2 tokens) is generated. We present a comprehensive analysis of discourse connectives in both fine-tuned generation— specifically, open domain dialogue generation— and organic generation directly from GPT-2. We find that GPT-2 generates valid discourse connectives when the relation can be inferred by humans 52 Proceedings of The 13th International Conference on Natural Language Generation, pages 52–59, c Dublin, Ireland, 15-18 December, 2020. 2020 Association f"
2020.inlg-1.8,N19-1423,0,0.146507,"ate rhetorical connections between spans in the absence of anaphoric entity mentions (Lascarides and Asher, 2008). Cognitive experiments have repeatedly shown discourse relations to be highly influential in the mental processing of text (Meyer and Freedle, 1984; Horowitz, 1987; Millis et al., 1993; Sanders and Noordman, 2000). Spans joined with incorrect discourse connectives can seem logically incoherent although they are independently grammatical: Introduction Recent progress in NLP has been marked with the emergence of large-scale pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2019). Among these, GPT-2 is particularly suitable in natural language generation due to its underlying left-to-right language modeling objective. Indeed, GPT-based language models have shown impressive results for open-domain dialogue generation (Golovanov et al., 2019; Wolf et al., 2019; Zhang et al., 2020). This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustrated that GPT-2 has the abil"
2020.inlg-1.8,P19-1065,0,0.0419038,"Missing"
2020.inlg-1.8,P18-1082,0,0.106953,"Missing"
2020.inlg-1.8,N19-1125,0,0.0507901,"Missing"
2020.inlg-1.8,P19-1442,0,0.0268541,"Missing"
2020.inlg-1.8,N18-1202,0,0.0124072,"rence. For example, they create rhetorical connections between spans in the absence of anaphoric entity mentions (Lascarides and Asher, 2008). Cognitive experiments have repeatedly shown discourse relations to be highly influential in the mental processing of text (Meyer and Freedle, 1984; Horowitz, 1987; Millis et al., 1993; Sanders and Noordman, 2000). Spans joined with incorrect discourse connectives can seem logically incoherent although they are independently grammatical: Introduction Recent progress in NLP has been marked with the emergence of large-scale pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2019). Among these, GPT-2 is particularly suitable in natural language generation due to its underlying left-to-right language modeling objective. Indeed, GPT-based language models have shown impressive results for open-domain dialogue generation (Golovanov et al., 2019; Wolf et al., 2019; Zhang et al., 2020). This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustr"
2020.inlg-1.8,P19-1608,0,0.0165733,"3; Sanders and Noordman, 2000). Spans joined with incorrect discourse connectives can seem logically incoherent although they are independently grammatical: Introduction Recent progress in NLP has been marked with the emergence of large-scale pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2019). Among these, GPT-2 is particularly suitable in natural language generation due to its underlying left-to-right language modeling objective. Indeed, GPT-based language models have shown impressive results for open-domain dialogue generation (Golovanov et al., 2019; Wolf et al., 2019; Zhang et al., 2020). This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustrated that GPT-2 has the ability to generate interesting and coherent text. However, analysis of GPT-2’s outputs from deeper linguistic dimensions (e.g., discourse) has largely remained unexplored. Jazz is good, because my favorite is country music. The importance of generating good discourse connectives are recognized in prio"
2020.inlg-1.8,prasad-etal-2008-penn,0,0.0911782,"Missing"
2020.inlg-1.8,N16-1037,0,0.0189364,"ectly (Fig. 2(b)) and incorrectly (Fig. 2(c)). This shows that the better performance of the model is not due to simply preferring the most frequent class. The improvement is notably more substantial for the organic case, an indication that fine-tuning GPT-2 nudges the model very close to what the connective prediction model learns. The overall improvement is likely due the connective prediction model having access to text before and after the connective, while the initial language generation model does not. This finding points to future work on considering stronger discourse-related signals (Ji et al., 2016) and stronger models for inferring relations. 5 Figure 2: Confusion matrix for GPT-2 (vertical axis) vs. connective prediction model (horizontal axis). Darker color indicates more instances. (a): all changed connectives; (b): sentences that the GPT-2 connectives are inconsistent with human labels, but the connective prediction model gave correct predictions; (c): sentences that the GPT-2 connectives are consistent with human labels, but the connective prediction model gave incorrect predictions. Changed connectives in the same relation class are also included. from GPT-2. We find that the unde"
2020.inlg-1.8,W17-0803,0,0.0208945,"es with relations that humans judge to hold given the rest of the sentence, as in a masked language modeling task. Specifically, for each output sentence that contains a discourse connective, we mask the connective3 and show the rest of the sentence to annotators (in the case of dialogue generation, we also show the prompt). They are asked to fill in the blank with a connective that most naturally expresses the relation between the arguments, or NONE if they think the two segments are not related. This type of insertion is used previously to crowdsource discourse relations (Yung et al., 2019; Scholman and Demberg, 2017). To reduce label sparsity, we group the connectives into the four top-level discourse relations in the Penn Discourse Treebank (Prasad et al., 2008) (contingency, contrast, expansion, temporal), and the annotators are asked to choose a group if it contains the connective they think most appropriately fills the blank. To further help annotators, we included unambiguous synonyms of connectives to anchor the relations more. For ambiguous connectives in our list, we put them in all possible relations they signal. The specific groupings are listed below: • because, therefore, (CONTINGENCY) Fine-tu"
2020.inlg-1.8,K19-1079,0,0.0393017,"re independently grammatical: Introduction Recent progress in NLP has been marked with the emergence of large-scale pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2019). Among these, GPT-2 is particularly suitable in natural language generation due to its underlying left-to-right language modeling objective. Indeed, GPT-based language models have shown impressive results for open-domain dialogue generation (Golovanov et al., 2019; Wolf et al., 2019; Zhang et al., 2020). This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustrated that GPT-2 has the ability to generate interesting and coherent text. However, analysis of GPT-2’s outputs from deeper linguistic dimensions (e.g., discourse) has largely remained unexplored. Jazz is good, because my favorite is country music. The importance of generating good discourse connectives are recognized in prior work in NLG (Biran and McKeown, 2015; Callaway, 2003). We examine to what extent does GPT-2 generate texts that uphold"
2020.inlg-1.8,D19-1221,0,0.021353,"rammatical: Introduction Recent progress in NLP has been marked with the emergence of large-scale pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2019). Among these, GPT-2 is particularly suitable in natural language generation due to its underlying left-to-right language modeling objective. Indeed, GPT-based language models have shown impressive results for open-domain dialogue generation (Golovanov et al., 2019; Wolf et al., 2019; Zhang et al., 2020). This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustrated that GPT-2 has the ability to generate interesting and coherent text. However, analysis of GPT-2’s outputs from deeper linguistic dimensions (e.g., discourse) has largely remained unexplored. Jazz is good, because my favorite is country music. The importance of generating good discourse connectives are recognized in prior work in NLG (Biran and McKeown, 2015; Callaway, 2003). We examine to what extent does GPT-2 generate texts that uphold plausible discourse rel"
2020.inlg-1.8,K15-2001,0,0.0544011,"Missing"
2020.inlg-1.8,W19-4003,0,0.0187298,"by these connectives with relations that humans judge to hold given the rest of the sentence, as in a masked language modeling task. Specifically, for each output sentence that contains a discourse connective, we mask the connective3 and show the rest of the sentence to annotators (in the case of dialogue generation, we also show the prompt). They are asked to fill in the blank with a connective that most naturally expresses the relation between the arguments, or NONE if they think the two segments are not related. This type of insertion is used previously to crowdsource discourse relations (Yung et al., 2019; Scholman and Demberg, 2017). To reduce label sparsity, we group the connectives into the four top-level discourse relations in the Penn Discourse Treebank (Prasad et al., 2008) (contingency, contrast, expansion, temporal), and the annotators are asked to choose a group if it contains the connective they think most appropriately fills the blank. To further help annotators, we included unambiguous synonyms of connectives to anchor the relations more. For ambiguous connectives in our list, we put them in all possible relations they signal. The specific groupings are listed below: • because, the"
2020.inlg-1.8,P18-1205,0,0.0697924,"Missing"
2020.inlg-1.8,2020.acl-demos.30,0,0.162069,"d with incorrect discourse connectives can seem logically incoherent although they are independently grammatical: Introduction Recent progress in NLP has been marked with the emergence of large-scale pre-trained models, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GPT-2 (Radford et al., 2019). Among these, GPT-2 is particularly suitable in natural language generation due to its underlying left-to-right language modeling objective. Indeed, GPT-based language models have shown impressive results for open-domain dialogue generation (Golovanov et al., 2019; Wolf et al., 2019; Zhang et al., 2020). This has motivated investigations into GPT-2’s generated text (See et al., 2019; Wallace et al., 2019). In particular, using automatic metrics (e.g., cosine similarity, lexical diversity, sentence length), See et al. (2019) illustrated that GPT-2 has the ability to generate interesting and coherent text. However, analysis of GPT-2’s outputs from deeper linguistic dimensions (e.g., discourse) has largely remained unexplored. Jazz is good, because my favorite is country music. The importance of generating good discourse connectives are recognized in prior work in NLG (Biran and McKeown, 2015;"
2020.lrec-1.149,J08-4004,0,0.265856,"भी उसे अब फर से नई जान-पहचान करनी पड़ेगी। Maybe it was clear to him that his daughter must have grown up now, and that he would have to reacquaint with her again. The 53 stories contained 10,472 sentences and all sentences were annotated by the three annotators. We evaluated the annotations in terms of inter-annotator agreements using Krippendorff’s alpha (K-alpha) (Krippendorff, 2011) which is more robust than simple agreement measures because it accounts for chance correction and class distributions. We observed strong inter-annotator agreements (K-alpha of 0.87) and per recommendations in (Artstein and Poesio, 2008) conclude that the annotations are of good quality. We chose a straightforward majority decision for label aggregation: if two or more annotators agreed on a discourse mode for a sentence. In cases where there was no agreement between the annotators, they met in person to discuss and assign the final label. 2.4. Dataset statistics Figure 1 shows a distribution of the number of sentences for each discourse mode. There is fair amount of class imbalance in this domain with the most prevalent class Descriptive having 3,954 samples, and the two low prevalence classes (Informative and Argumentative)"
2020.lrec-1.149,Q17-1010,0,0.0206365,"Missing"
2020.lrec-1.149,K18-2017,0,0.0221198,"Missing"
2020.lrec-1.149,W07-1428,0,0.0452706,"t they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from writt"
2020.lrec-1.149,J81-2001,0,0.470376,"ut, indicating relationship to what they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify diff"
2020.lrec-1.149,P14-2047,1,0.805639,"nd relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 of"
2020.lrec-1.149,W16-3617,1,0.84438,"und Discourse in the context of linguistics is defined as exploitation of language features by speakers to express what they are talking about, indicating relationship to what they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Racha"
2020.lrec-1.149,W09-3029,0,0.0379755,"ration (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances of the language using natural l"
2020.lrec-1.149,prasad-etal-2008-penn,0,0.104598,"mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spo"
2020.lrec-1.149,W11-0414,0,0.0355923,"2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_"
2020.lrec-1.149,P17-1011,0,0.0249855,"of these short stories were originally written in Hindi but some of them were written in other Indian languages and later translated to Hindi. We chose against crowd-sourcing the annotation process because we wanted to directly work with the annotators for qualitative feedback and to also ensure high quality annotations. We employed three native Hindi speakers with college level education for the annotation task. We first selected two random stories from our corpus and had the three annotators work on them independently and classify each sentence based on the discourse mode taxonomy used in (Song et al., 2017). Song et al (Song et al., 2017) developed their taxonomy based on prior works in linguistics (Smith, 2003). This preliminary task helped the annotators familiarize themselves with discourse modes and also understand the scope of this annotation task. More importantly, this also helped us ascertain feedback about the class labels. Based on the annotators’ feedback we first observed that of five discourse modes used in (Song et al., 2017), Emotion was extremely prevalent: most of the sentences in these short stories could be associated with some sort of an emotion. We therefore decided to elimi"
2020.lrec-1.149,tonelli-etal-2010-annotation,0,0.0196186,"as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/"
2020.lrec-1.149,W16-6307,0,0.0226833,"w resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances of the language using natural language processing techniques. The syntax and semantics of Hindi is often different from other high resource languages like English. Dependency of the meaning of expressions on word order, morphological variations, and spelling variations makes Hindi an interesting language to study and also pose additional challenges for linguistic modelling (Kumar et al., 2019). Tripathi et al. (Tripathi et al., 2016), created a Hindi corpus of 1960 sentences extracted from children stories that are annotated with discourse modes for improving storytelling experience using TTS systems. They annotated an already existing story speech corpus (Sarkar et al., 2014) for three discourse modes - dialogue, narrative and descriptive. The main motivation of their work was to develop an automated discourse mode identification system at sentence level that could be further used for enhancing the output of a TTS system by improving the performance of the prosody models. Motivated by the efforts of (Tripathi et al., 201"
2020.lrec-1.149,W10-1844,0,0.0371131,"al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances o"
2020.lrec-1.149,P12-1008,0,0.027183,"ld be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that e"
2020.semeval-1.273,W19-3217,0,0.0292162,"Missing"
2020.semeval-1.273,N19-1144,0,0.0320228,"Missing"
2020.semeval-1.273,S19-2010,0,0.0395664,"Missing"
2020.semeval-1.69,P18-1043,0,0.0834403,"ommons.org/licenses/by/4.0/. 556 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 556–561 Barcelona, Spain (Online), December 12, 2020. The rest of our paper is structured as follows. Section 2 introduces related work. Model and data preparation are described in Section 3. Experiments and evaluation are described in Section 4. The conclusions are drawn in Section 5. 2 Related Work In natural language processing, the problem of common sense verification has always been an important one. The common sense verification problem has a large data set. Such as, Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)"
2020.semeval-1.69,P19-1393,0,0.0629995,"ence have become our common knowledge through summarization and verification. In our daily life, common sense can often tell us some other people’s practical experience, so that we can avoid the repetition of some errors. After scientific verification, a lot of wrong common sense is slowly being corrected. However, it takes a lot of human resources to manually correct normal knowledge. At the same time, in recent years, due to the development of natural language processing and neural networks, common sense revision has entered the era of mechanization. Semeval 2020 task 4 (Wang et al., 2020) (Wang et al., 2019) is designed for common sense verification and interpretation. This task is to directly test whether the system can distinguish meaningful natural language statements from unreasonable natural language statements. In this way, we can save a lot of time to distinguish between common sense and extraordinary knowledge. Subtask A is to choose from two natural language statements with similar wording, one of which is meaningful and the other is meaningless. This subtask is mainly to directly distinguish whether the discourse has common sense. Subtask B is to find out the key reason why this sentenc"
2020.semeval-1.69,2020.semeval-1.39,0,0.0316577,"ge amounts of experience have become our common knowledge through summarization and verification. In our daily life, common sense can often tell us some other people’s practical experience, so that we can avoid the repetition of some errors. After scientific verification, a lot of wrong common sense is slowly being corrected. However, it takes a lot of human resources to manually correct normal knowledge. At the same time, in recent years, due to the development of natural language processing and neural networks, common sense revision has entered the era of mechanization. Semeval 2020 task 4 (Wang et al., 2020) (Wang et al., 2019) is designed for common sense verification and interpretation. This task is to directly test whether the system can distinguish meaningful natural language statements from unreasonable natural language statements. In this way, we can save a lot of time to distinguish between common sense and extraordinary knowledge. Subtask A is to choose from two natural language statements with similar wording, one of which is meaningful and the other is meaningless. This subtask is mainly to directly distinguish whether the discourse has common sense. Subtask B is to find out the key rea"
2020.semeval-1.69,D18-1009,0,0.0545026,"paper is structured as follows. Section 2 introduces related work. Model and data preparation are described in Section 3. Experiments and evaluation are described in Section 4. The conclusions are drawn in Section 5. 2 Related Work In natural language processing, the problem of common sense verification has always been an important one. The common sense verification problem has a large data set. Such as, Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) (Zhang et al., 2018) is a large-scale reading comprehension dataset which requires commonsense reasoning. For these data sets, there are some neural networks that can get good per"
2020.semeval-1.86,D15-1075,0,0.0199023,"t a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) (Zhang et al., 2018) is a large-scale reading comprehension dataset which requires commonsense reasoning. In the field of natural inference, there are also a large number of data sets. Such as, The Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) Corpus contains around 550k hypothesis/premise pairs. The Multi-Genre Natural Language Inference (MultiNLI) (Williams et al., 2017) corpus contains around 433k hypothesis/premise pairs. It is similar to the SNLI corpus, but covers a range of genres of spoken and written text and supports crossgenre evaluation. The SciTail (Khot et al., 2018) entailment dataset consists of 27k. In contrast to the SNLI and MultiNLI, it was not crowd-sourced but created from sentences that already exist in the wild. 3 Data Preparation In this part, we mainly introduce our processing of the data set. At the same"
2020.semeval-1.86,P18-1043,0,0.0126697,"3 describes data preparation. Methods are described in Section 4. Experiments and evaluation are described in Section 5. The conclusions are drawn in Section 6. 2 Related Work In the field of natural language processing, common sense problems and natural inference problems are relevant to our task. These two problems have large data sets. By describing the data set, we can show the development of these two fields from the content of the data set. Meanwhile, the enlightenment of related solutions can also be obtained. In the field of common sense, this question has some data sets. Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD)"
2020.semeval-1.86,2020.semeval-1.40,0,0.0270643,"tion is right or wrong. However, this will consume a lot of labor costs and cause a waste of resources. At the same time, there are usually some misjudgments in human detection. Therefore, we need some new methods to deal. In the past two years, with the rapid development of artificial intelligence, some related problems have emerged in the field of deep learning. For example, common sense discrimination and natural language inference. The emergence of these problems provides some mechanical methods to solve the problem of error messages. Our research is also carried out. Semeval 2020 task 5 (Yang et al., 2020) is to model causal inference in language: detect counterfactuals. This task is through the establishment of a model, and at the same time to detect whether it is a factual statement based on a large amount of text data. The generation of this task can help us to make a machine judgment of information correctness to a great extent. Subtask A is to detect counterfactual statements. This subtask requires us to determine whether a given statement is counterfactual. A counterfactual statement describes an event that has not actually occurred or cannot occur, and the possible consequences of the ev"
2020.semeval-1.86,D18-1009,0,0.0165651,"the field of natural language processing, common sense problems and natural inference problems are relevant to our task. These two problems have large data sets. By describing the data set, we can show the development of these two fields from the content of the data set. Meanwhile, the enlightenment of related solutions can also be obtained. In the field of common sense, this question has some data sets. Event2Mind (Rashkin et al., 2018) is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations. The winograd schema challenge (Trinh and Le, 2018) is a dataset for common sense reasoning. It employs winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) (Zhang et al., 2018) is a large-scale reading comprehension dataset which requires commonsense reasoning. In the field of natural inference, there are also a large number of data"
2021.acl-demo.4,N19-1423,0,0.035597,"ROUGE-n (Lin, 2004) measure the ratios of the overlapping ngrams between the generated and real samples, METEOR (Banerjee and Lavie, 2005) measures the word-to-word matches based on WordNet, CIDEr (Vedantam et al., 2015) computes the TFIDF weights for each n-gram in generated/real samples and CHRF++ (Popovic, 2015) computes Fscore averaged on both character- and word-level n-grams. To evaluate the semantic equivalence between generated and real samples, we include BERTScore (Zhang et al., 2020), a metric based on the similarity of sentence embeddings relied on pretrained language model BERT (Devlin et al., 2019). Moreover, Distinct-n and Unique-n (Li et al., 2016a) measures the degree of diversity of generated text by calculating the number of distinct unigrams and bigrams in generated text. Besides, to evaluate the diversity of unconditionally generated samples, we also take into account the Self-BLEU (Zhu et al., 2018) metric. In summary, users can choose different evaluation protocols towards a specific generation task by setting the hyper-parameter, i.e., metrics. In practice, as the model may generate many text pieces, evaluation efficiency is an important concern. Hence, we integrate efficient"
2021.acl-demo.4,E17-1059,0,0.0528613,"Missing"
2021.acl-demo.4,2020.acl-main.703,0,0.080536,"Missing"
2021.acl-demo.4,N16-1014,0,0.244515,"pache License 2.0 at the link https: //github.com/RUCAIBox/TextBox. 1 Introduction Text generation, which has emerged as an important branch of natural language processing (NLP), is often formally referred as natural language generation (NLG) (Li et al., 2021b). It aims to produce plausible and understandable text in human language from input data (e.g., a sequence, keywords) or machine representation. Because of incredible performance of deep learning models, many classic text generation tasks have achieved rapid progress, such as machine translation (Vaswani et al., 2017), dialogue systems (Li et al., 2016b), text summarization (See et al., 2017), graph-to-text generation (Li et al., 2021a), and more. To facilitate the development of text generation models, a few remarkable open-source libraries † ∗ Equal contribution. Corresponding author. 30 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 30–39, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Applications Generation Training Model Strategy Data E"
2021.acl-demo.4,D16-1127,0,0.209724,"pache License 2.0 at the link https: //github.com/RUCAIBox/TextBox. 1 Introduction Text generation, which has emerged as an important branch of natural language processing (NLP), is often formally referred as natural language generation (NLG) (Li et al., 2021b). It aims to produce plausible and understandable text in human language from input data (e.g., a sequence, keywords) or machine representation. Because of incredible performance of deep learning models, many classic text generation tasks have achieved rapid progress, such as machine translation (Vaswani et al., 2017), dialogue systems (Li et al., 2016b), text summarization (See et al., 2017), graph-to-text generation (Li et al., 2021a), and more. To facilitate the development of text generation models, a few remarkable open-source libraries † ∗ Equal contribution. Corresponding author. 30 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 30–39, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Applications Generation Training Model Strategy Data E"
2021.acl-demo.4,D18-1423,0,0.0488524,"Missing"
2021.acl-demo.4,2021.findings-acl.136,1,0.769204,"architecture, inference, and learning process into highly reusable modules, which allows users to easily incorporate new models into our framework. The above features make TextBox especially suitable for researchers and practitioners to quickly reproduce baseline models and develop new models. TextBox is implemented based on PyTorch, and released under Apache License 2.0 at the link https: //github.com/RUCAIBox/TextBox. 1 Introduction Text generation, which has emerged as an important branch of natural language processing (NLP), is often formally referred as natural language generation (NLG) (Li et al., 2021b). It aims to produce plausible and understandable text in human language from input data (e.g., a sequence, keywords) or machine representation. Because of incredible performance of deep learning models, many classic text generation tasks have achieved rapid progress, such as machine translation (Vaswani et al., 2017), dialogue systems (Li et al., 2016b), text summarization (See et al., 2017), graph-to-text generation (Li et al., 2021a), and more. To facilitate the development of text generation models, a few remarkable open-source libraries † ∗ Equal contribution. Corresponding author. 30 P"
2021.acl-demo.4,P18-4020,0,0.0508612,"Missing"
2021.acl-demo.4,P17-4012,0,0.164256,"extBox: A Unified, Modularized, and Extensible Framework for Text Generation Junyi Li1,3†, Tianyi Tang1†, Gaole He2 , Jinhao Jiang1 , Xiaoxuan Hu2 , Puzhao Xie2 , Zhipeng Chen2 , Zhuohao Yu2 , Wayne Xin Zhao1,3,4∗ and Ji-Rong Wen1,2,3 1 Gaoling School of Artificial Intelligence, Renmin University of China 2 School of Information, Renmin University of China 3 Beijing Key Laboratory of Big Data Management and Analysis Methods 4 Beijing Academy of Artificial Intelligence, Beijing, 100084, China {lijunyi,steven_tang}@ruc.edu.cn batmanfly@gmail.com Abstract have been developed (Britz et al., 2017; Klein et al., 2017b; Miller et al., 2017b; Zhu et al., 2018; Hu et al., 2019). These frameworks are mainly designed for some or a small number of specific tasks, particularly machine translation and dialogue systems. They usually focus on a special kind of techniques for text generation such as generative adversarial networks (GAN), or have limitations in covering commonly-used baseline implementations. Even for an experienced researcher, it is difficult and time-consuming to implement all compared baselines under a unified framework. Therefore, it is highly desirable to re-consider the implementation of text g"
2021.acl-demo.4,2020.findings-emnlp.217,0,0.0310842,"Missing"
2021.acl-demo.4,P11-1015,0,0.10535,"utilize three generation strategies, i.e., topk, greedy, and beam search. The greedy strategy considers the most probable token at each generation step, the top-k search strategy means sorting by probability and zero-ing out the probabiliPerformance Evaluation To evaluate the models in TextBox, we conduct extensive experiments to compare their performance on unconditional and conditional generation tasks. 4.1 Conditional Text Generation Unconditional Text Generation Following previous work, we adopt COCO (Lin et al., 2015), EMNLP2017 WMT News (Chatterjee et al., 2017) and IMDB Movie Reviews (Maas et al., 2011) datasets for comparing the performance of five traditional and state-of-the-art models, i.e., LSTM-VAE, SeqGAN, RankGAN, MaliGAN, and GPT-2, in the unconditional text generation task. In our experiments, we run models with the parameter configurations described in their original 34 Tasks Datasets Models Distinct-1 Distinct-2 BLEU-1 BLEU-2 BLEU-3 BLEU-4 - - 63.97 99.76 99.76 99.71 88.15 46.56 82.32 82.92 81.95 78.13 18.53 51.26 52.46 50.86 55.81 5.97 25.18 26.40 24.87 31.88 Unconditional Generation COCO LSTM-VAE SeqGAN RankGAN MailGAN GPT-2 Attribute-to-Text Generation AMAZON Context2Seq Attr2"
2021.acl-demo.4,D17-2014,0,0.458832,"dularized, and Extensible Framework for Text Generation Junyi Li1,3†, Tianyi Tang1†, Gaole He2 , Jinhao Jiang1 , Xiaoxuan Hu2 , Puzhao Xie2 , Zhipeng Chen2 , Zhuohao Yu2 , Wayne Xin Zhao1,3,4∗ and Ji-Rong Wen1,2,3 1 Gaoling School of Artificial Intelligence, Renmin University of China 2 School of Information, Renmin University of China 3 Beijing Key Laboratory of Big Data Management and Analysis Methods 4 Beijing Academy of Artificial Intelligence, Beijing, 100084, China {lijunyi,steven_tang}@ruc.edu.cn batmanfly@gmail.com Abstract have been developed (Britz et al., 2017; Klein et al., 2017b; Miller et al., 2017b; Zhu et al., 2018; Hu et al., 2019). These frameworks are mainly designed for some or a small number of specific tasks, particularly machine translation and dialogue systems. They usually focus on a special kind of techniques for text generation such as generative adversarial networks (GAN), or have limitations in covering commonly-used baseline implementations. Even for an experienced researcher, it is difficult and time-consuming to implement all compared baselines under a unified framework. Therefore, it is highly desirable to re-consider the implementation of text generation algorithms i"
2021.acl-demo.4,2020.tacl-1.18,0,0.0812399,"Missing"
2021.acl-demo.4,P17-1099,0,0.0450577,"github.com/RUCAIBox/TextBox. 1 Introduction Text generation, which has emerged as an important branch of natural language processing (NLP), is often formally referred as natural language generation (NLG) (Li et al., 2021b). It aims to produce plausible and understandable text in human language from input data (e.g., a sequence, keywords) or machine representation. Because of incredible performance of deep learning models, many classic text generation tasks have achieved rapid progress, such as machine translation (Vaswani et al., 2017), dialogue systems (Li et al., 2016b), text summarization (See et al., 2017), graph-to-text generation (Li et al., 2021a), and more. To facilitate the development of text generation models, a few remarkable open-source libraries † ∗ Equal contribution. Corresponding author. 30 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 30–39, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Applications Generation Training Model Strategy Data Evaluation Task-agnostic Metric Greedy Sea"
2021.acl-demo.4,D17-1066,0,0.0491549,"Missing"
2021.acl-demo.4,P02-1040,0,0.110886,"ve this goal, we set up the evaluation module to implement commonly-used evaluation protocols for text generation models. Our library supports both logit-based and wordbased evaluation metrics. The logit-based met32 rics include perplexity (PPL) (Brown et al., 1992) and negative log-likelihood (NLL) (Huszar, 2015), measuring how well the probability distribution or a probability model predicts a sample compared with the ground-truth. The word-based metrics include the most widely-used generation metrics for evaluating lexical similarity, semantic equivalence and diversity. For example, BLEUn (Papineni et al., 2002) and ROUGE-n (Lin, 2004) measure the ratios of the overlapping ngrams between the generated and real samples, METEOR (Banerjee and Lavie, 2005) measures the word-to-word matches based on WordNet, CIDEr (Vedantam et al., 2015) computes the TFIDF weights for each n-gram in generated/real samples and CHRF++ (Popovic, 2015) computes Fscore averaged on both character- and word-level n-grams. To evaluate the semantic equivalence between generated and real samples, we include BERTScore (Zhang et al., 2020), a metric based on the similarity of sentence embeddings relied on pretrained language model BE"
2021.acl-demo.4,W15-3049,0,0.0109412,"w well the probability distribution or a probability model predicts a sample compared with the ground-truth. The word-based metrics include the most widely-used generation metrics for evaluating lexical similarity, semantic equivalence and diversity. For example, BLEUn (Papineni et al., 2002) and ROUGE-n (Lin, 2004) measure the ratios of the overlapping ngrams between the generated and real samples, METEOR (Banerjee and Lavie, 2005) measures the word-to-word matches based on WordNet, CIDEr (Vedantam et al., 2015) computes the TFIDF weights for each n-gram in generated/real samples and CHRF++ (Popovic, 2015) computes Fscore averaged on both character- and word-level n-grams. To evaluate the semantic equivalence between generated and real samples, we include BERTScore (Zhang et al., 2020), a metric based on the similarity of sentence embeddings relied on pretrained language model BERT (Devlin et al., 2019). Moreover, Distinct-n and Unique-n (Li et al., 2016a) measures the degree of diversity of generated text by calculating the number of distinct unigrams and bigrams in generated text. Besides, to evaluate the diversity of unconditionally generated samples, we also take into account the Self-BLEU"
2021.acl-demo.4,W18-1819,0,0.0310866,"extBox has drawn inspirations from these toolkits when designing relevant functions. Compared with them, TextBox covers more text generation tasks and models, which is useful for reproducibility. Besides, we implement standardized evaluation to compare different models. Also, our library provides various common modules for convenience. It has a proper focus on text generation field, and provide a comprehensive set of modules and functionalities. Related Work 6 Several toolkits have been released focusing on one or a few specific text generation tasks or techniques. For example, Tensor2Tensor (Vaswani et al., 2018), MarianNMT (Junczys-Dowmunt et al., 2018) and OpenNMT (Klein et al., 2017a) are designed for machine translation task, while ParlAI (Miller et al., 2017a) and Plato (Papangelis et al., 2020) specialConclusion This paper presented a unified, modularized, and extensible text generation library, called TextBox. So far, we have implemented 21 text generation models, including VAE-based, GAN-based, pretrained language models, sequence-to-sequence and 9 benchmark datasets for unconditional and 35 References conditional text generation tasks. Moreover, Our library is modularized to easily plug in or"
2021.findings-acl.136,2020.acl-main.18,0,0.280089,"large-scale labelled datasets for a variety of domains in practice. Motivated by this, we propose to study the task of few-shot KG-to-text generation that aims to produce satisfactory output text given only a handful of (several hundred) labelled instances. To fulfil this task, we need to fully understand the complicated semantic relations between entities from various domains, which is challenging with limited labelled data. Our solution is inspired by the excellent few-shot capabilities of pretrained language models (PLMs) on language understanding and generation tasks (Brown et al., 2020; Chen et al., 2020; Li et al., 2021a). Pretrained on the large-scale corpora, PLMs encode vast amounts of world knowledge into their parameters (Li et al., 2021b), which is potentially beneficial to understand and describe the KG facts in our task. However, applying PLMs to few-shot KG-totext generation still faces two challenges. First, PLMs are usually pretrained on natural language text, while the KG inputs for our task are structured graphs. This semantic gap makes it difficult to effectively inject KG representations into PLMs 1558 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"
2021.findings-acl.136,N19-1423,0,0.251067,"ged a graph Transformer encoder to compute node representations by attending over local neighborhoods via self-attention. In contrast, Ribeiro et al. (2020a) focused on combining global and local message passing mechanisms based on GNNs, capturing complementary graph contexts. Guo et al. (2020) presented an unsupervised training method that can iteratively back translate between the text and graph data. Different from them, we explore how to utilize large PLMs for few-shot KG-to-text generation. Pretrained Language Model. Recent years have witnessed prominent achievement of PLMs in NLP tasks (Devlin et al., 2019; Radford et al., 2019). Pretrained on massive corpora, pretrained models showcase unprecedented generalization ability to solve related downstream tasks (Li et al., 2021b). However, most of existing PLMs were conditioned on text data (Radford et al., 2019; Lewis et al., 2020), lacking consideration of structured data input. Ribeiro et al. (2020b) proposed to utilize PLMs for KG-to-text generation by randomly linearizing graph into a sequence of triples. While, these methods do not explicitly model the structural relations of KG, which is critical for generating faithful text. Our work aims to"
2021.findings-acl.136,N16-1087,0,0.0263797,"e semantic correspondence between input KG and output text, based on which our model can generate faithful text about KG. To the best of our knowledge, we are the first study to investigate PLMs for few-shot KG-to-text generation. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model. 2 Related Work In this work, we mainly focus on generating text from knowledge graphs using PLMs. KG-to-Text Generation. Early works mainly centered around statistical methods, applying grammar rules to generate text (Konstas and Lapata, 2013; Flanigan et al., 2016). Recently, neural based approaches have been proposed to generate text from linearized KG triples (Gardent et al., 2017), however, unable to model structural information about KG. Many works explored how to encode the graph structure using Graph Neural Networks (GNNs) or Transformers explicitly. Koncel-Kedziorski et al. (2019) leveraged a graph Transformer encoder to compute node representations by attending over local neighborhoods via self-attention. In contrast, Ribeiro et al. (2020a) focused on combining global and local message passing mechanisms based on GNNs, capturing complementary gr"
2021.findings-acl.136,W17-3518,0,0.10334,"To the best of our knowledge, we are the first study to investigate PLMs for few-shot KG-to-text generation. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model. 2 Related Work In this work, we mainly focus on generating text from knowledge graphs using PLMs. KG-to-Text Generation. Early works mainly centered around statistical methods, applying grammar rules to generate text (Konstas and Lapata, 2013; Flanigan et al., 2016). Recently, neural based approaches have been proposed to generate text from linearized KG triples (Gardent et al., 2017), however, unable to model structural information about KG. Many works explored how to encode the graph structure using Graph Neural Networks (GNNs) or Transformers explicitly. Koncel-Kedziorski et al. (2019) leveraged a graph Transformer encoder to compute node representations by attending over local neighborhoods via self-attention. In contrast, Ribeiro et al. (2020a) focused on combining global and local message passing mechanisms based on GNNs, capturing complementary graph contexts. Guo et al. (2020) presented an unsupervised training method that can iteratively back translate between the"
2021.findings-acl.136,2020.coling-main.217,0,0.0313996,"nd embedding size are set to 1e-5, 20, 2 and 1024, respectively. The weights λ1 , λ2 and λ3 in Eq. 10 are set to 0.7, 0.5 and 0.5, respectively, according to performance on validation set. During inference, we apply the beam search method with a beam size of 8. 5 Experiments In this section, we first set up the experiments, and then report the results and analysis. 5.1 Experimental Setup Datasets. To evaluate our model on few-shot KG-to-text generation, we conduct experiments on three benchmarks, including AGENDA (KoncelKedziorski et al., 2019), WebNLG (Gardent et al., 2017) and GenWiki Fine (Jin et al., 2020). We adopt three large domains (i.e., Airport, Building and Food) for WebNLG and two large domains (i.e., Sports and Games) for GenWiki. Table 1 shows the statistics for each dataset. Each instance of these datasets contains a knowledge graph in the form of triples and a target text describing the graph. The three datasets have originally provided the alignment records from entity mentions to KG entities. Take an example from WebNLG dataset “AGENT-1 is located in PATIENT-1”: the entity mention is tagged as “AGENT-1” and the tag “AGENT-1” maps to the entity “11th_Mississippi_Infantry_Monument”"
2021.findings-acl.136,N19-1238,0,0.22385,"ion model. 2 Related Work In this work, we mainly focus on generating text from knowledge graphs using PLMs. KG-to-Text Generation. Early works mainly centered around statistical methods, applying grammar rules to generate text (Konstas and Lapata, 2013; Flanigan et al., 2016). Recently, neural based approaches have been proposed to generate text from linearized KG triples (Gardent et al., 2017), however, unable to model structural information about KG. Many works explored how to encode the graph structure using Graph Neural Networks (GNNs) or Transformers explicitly. Koncel-Kedziorski et al. (2019) leveraged a graph Transformer encoder to compute node representations by attending over local neighborhoods via self-attention. In contrast, Ribeiro et al. (2020a) focused on combining global and local message passing mechanisms based on GNNs, capturing complementary graph contexts. Guo et al. (2020) presented an unsupervised training method that can iteratively back translate between the text and graph data. Different from them, we explore how to utilize large PLMs for few-shot KG-to-text generation. Pretrained Language Model. Recent years have witnessed prominent achievement of PLMs in NLP"
2021.findings-acl.136,D13-1157,0,0.0219411,"s step further enhances the semantic correspondence between input KG and output text, based on which our model can generate faithful text about KG. To the best of our knowledge, we are the first study to investigate PLMs for few-shot KG-to-text generation. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model. 2 Related Work In this work, we mainly focus on generating text from knowledge graphs using PLMs. KG-to-Text Generation. Early works mainly centered around statistical methods, applying grammar rules to generate text (Konstas and Lapata, 2013; Flanigan et al., 2016). Recently, neural based approaches have been proposed to generate text from linearized KG triples (Gardent et al., 2017), however, unable to model structural information about KG. Many works explored how to encode the graph structure using Graph Neural Networks (GNNs) or Transformers explicitly. Koncel-Kedziorski et al. (2019) leveraged a graph Transformer encoder to compute node representations by attending over local neighborhoods via self-attention. In contrast, Ribeiro et al. (2020a) focused on combining global and local message passing mechanisms based on GNNs, ca"
2021.findings-acl.136,D16-1128,0,0.0468114,"Missing"
2021.findings-acl.136,2020.acl-main.703,0,0.370297,"Guo et al. (2020) presented an unsupervised training method that can iteratively back translate between the text and graph data. Different from them, we explore how to utilize large PLMs for few-shot KG-to-text generation. Pretrained Language Model. Recent years have witnessed prominent achievement of PLMs in NLP tasks (Devlin et al., 2019; Radford et al., 2019). Pretrained on massive corpora, pretrained models showcase unprecedented generalization ability to solve related downstream tasks (Li et al., 2021b). However, most of existing PLMs were conditioned on text data (Radford et al., 2019; Lewis et al., 2020), lacking consideration of structured data input. Ribeiro et al. (2020b) proposed to utilize PLMs for KG-to-text generation by randomly linearizing graph into a sequence of triples. While, these methods do not explicitly model the structural relations of KG, which is critical for generating faithful text. Our work aims to consider the KG structure and bridge the semantic gap between KG encodings and PLMs. 3 Problem Formulation KG-to-text generation (Ribeiro et al., 2020a) aims to automatically generate a natural language text that describes the facts in KG. Formally, the input KG consists of a"
2021.findings-acl.136,P19-1598,0,0.0191959,"(e.g., Stan Lee and Iron Man) represent entities and the edges (e.g., creator and alias) describe the relations between connected entities. In recent years, with the help of crowdsourcing platforms and information extraction (IE) systems, large-scale labelled pairs of KG and its descriptive text have been created, such as WikiBio (Lebret et al., 2016) and WebNLG Challenge (Gardent Corresponding author r ato cre Iron Man alias Introduction ∗ r ato et al., 2017). Based on these datasets, data-driven models have shown impressive capabilities to produce informative and fluent text for a given KG (Logan et al., 2019; Moryossef et al., 2019). However, due to the great expense in annotation process, it is not always feasible to generate large-scale labelled datasets for a variety of domains in practice. Motivated by this, we propose to study the task of few-shot KG-to-text generation that aims to produce satisfactory output text given only a handful of (several hundred) labelled instances. To fulfil this task, we need to fully understand the complicated semantic relations between entities from various domains, which is challenging with limited labelled data. Our solution is inspired by the excellent few-sh"
2021.findings-acl.136,N19-1236,0,0.0377567,"Missing"
2021.findings-acl.136,P02-1040,0,0.109126,"ith training dataset size ranging from 50, 100, 200 to 500. All the comparison methods are optimized based on validation performance. In our model, the entity embeddings of GNN are initialized with pretrained KG embeddings and the GNN weights are transferred from CGE-LW. We also pretrain GNN weights based on the large-scale KG, i.e., Wikipedia. Based on the pretrained entity embeddings and weights, we continue to train our model. Evaluation Metrics. For performance comparison, we adopt five automatic evaluation metrics widely used by previous graph-to-text work (Guo et al., 2020), i.e., BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and CHRF++ (Popovic, 2015). Specifically, BLEU-n and ROUGE-n compute the ratios of overlapping n-grams between generated and real text, CIDEr computes the TF-IDF weights for each n-gram in generated/real text, and CHRF++ computes F-score averaged on both character- and word-level n-grams. 1563 B-4 R-L CIDEr Chrf Ours 31.79 55.13 3.94 57.38 w/o RA w/o GR w/o PG 23.14 27.56 29.30 41.34 46.69 48.66 1.90 2.82 3.58 43.34 48.90 53.44 35 RBFS-Test RDFS-Test FFS-Test RS-Test 30 BLEU Models 25 Table 5: Ablation analysis on W EB NLG dataset. 20 5.2 Tabl"
2021.findings-acl.136,D19-1005,0,0.0196357,"eddings. 6 Conclusion This paper presented a few-shot KG-to-text generation model based on PLMs. We make three important technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input KG representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model. As future work, we will consider adopting KG-enhanced PLMs (Zhang et al., 2019; Peters et al., 2019) for improving the task performance, which explicitly inject knowledge information into PLMs. Acknowledgement This work was partially supported by the National Natural Science Foundation of China under Grant No. 61872369 and 61832017, Beijing Academy of Artificial Intelligence (BAAI) under Grant No. BAAI2020ZJ0301, Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098, the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China under Grant No.18XNLG22 and 19XNQ047. Xin Zhao is the corresponding author. Referenc"
2021.findings-acl.136,W15-3049,0,0.0698852,"s are optimized based on validation performance. In our model, the entity embeddings of GNN are initialized with pretrained KG embeddings and the GNN weights are transferred from CGE-LW. We also pretrain GNN weights based on the large-scale KG, i.e., Wikipedia. Based on the pretrained entity embeddings and weights, we continue to train our model. Evaluation Metrics. For performance comparison, we adopt five automatic evaluation metrics widely used by previous graph-to-text work (Guo et al., 2020), i.e., BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) and CHRF++ (Popovic, 2015). Specifically, BLEU-n and ROUGE-n compute the ratios of overlapping n-grams between generated and real text, CIDEr computes the TF-IDF weights for each n-gram in generated/real text, and CHRF++ computes F-score averaged on both character- and word-level n-grams. 1563 B-4 R-L CIDEr Chrf Ours 31.79 55.13 3.94 57.38 w/o RA w/o GR w/o PG 23.14 27.56 29.30 41.34 46.69 48.66 1.90 2.82 3.58 43.34 48.90 53.44 35 RBFS-Test RDFS-Test FFS-Test RS-Test 30 BLEU Models 25 Table 5: Ablation analysis on W EB NLG dataset. 20 5.2 Table 2, 3, and 4 present the fully-supervised and few-shot results of our model"
2021.findings-acl.136,2020.tacl-1.38,0,0.219323,"rtists Jack Kirby . Iron Man&apos;s alter ego is Tony Stark. He has found the superhero team the Avengers alongside Thor. Figure 1: A knowledge graph (subgraph) with its descriptive text. The underlined words represent the context keywords about entities. Knowledge graphs (KGs), such as Wikidata and DBpedia, are essential for many natural language processing (NLP) applications (Ji et al., 2020). To understand the structured information in KG, the task of KG-to-text generation has been proposed to automatically generate a descriptive text for a given knowledge graph (Koncel-Kedziorski et al., 2019; Ribeiro et al., 2020a). Figure 1 illustrates a KG with the corresponding descriptive text, in which the nodes (e.g., Stan Lee and Iron Man) represent entities and the edges (e.g., creator and alias) describe the relations between connected entities. In recent years, with the help of crowdsourcing platforms and information extraction (IE) systems, large-scale labelled pairs of KG and its descriptive text have been created, such as WikiBio (Lebret et al., 2016) and WebNLG Challenge (Gardent Corresponding author r ato cre Iron Man alias Introduction ∗ r ato et al., 2017). Based on these datasets, data-driven models"
2021.findings-acl.136,P17-1099,0,0.0700958,"a. Finally, through RBFS, we can obtain a linearized entity sequence taken as input of the decoder for text generation. 4.3 KG-enhanced Multi-task Learning After obtaining the linearized entity sequence, we next take it as input and perform text generation. 1 https://en.wikipedia.org/wiki/Breadth-first_search where pgen is the generative probability from PLM. Besides, in KG-to-text generation, some tokens in descriptive text correspond to KG entities shown in Figure 1. The ability to copy entities from KG would enrich the generated text content, which can be achieved by the pointer generator (See et al., 2017). By feeding the hidden states of PLM and the token embedding, the copy probability pjcopy of the j-th token wj can be computed as: pjcopy = σ(W1 sj + W2 vwj + bcopy ), (6) where W1 , W2 , and bcopy are trainable parameters, vwj is the embedding of token wj , and sj is the j-th hidden state from the top layer of PLM. Then, we explicitly “teach” our model how to switch between generation and copy via the copy loss as: X X LP G = pjcopy + (1 − pkcopy ). (7) wj wk Our intuition is aimed at minimizing the copy probability pjcopy of token wj (generated from vocabulary) and maximizing the copy proba"
2021.findings-acl.136,P16-1162,0,0.0256959,"odel optimization. Few-shot Learning. In few-shot KG-to-text generation, the key lies in how to bridge the semantic gap between KG and PLMs with limited dataset. To achieve this goal, we first utilize representation alignment in Section 4.1 to align the semantic space between KG encodings and PLMs, and then introduce a KG reconstruction task in Section 4.3 to further learn the semantic correspondence between input KG and output text. Besides, we observe that KG entities are often multi-word expressions. To deal with unseen entities in few-shot learning, we employ the Byte Pair Encoding (BPE) (Sennrich et al., 2016) and sub-word vocabulary (Radford et al., 2019) to split entity words into smaller semantic units. Our work is also empowered by the excellent few-shot capacities of PLMs with vast amounts of world knowledge learned from largescale corpora. Optimization. For PLM, we employ BART-Large model (Lewis et al., 2020). Specially, we adopt the first 6 layers of BART encoder as the lower layers, and the remaining 6 layers of BART encoder and BART decoder as the higher layers. Note that, the target text and text encoder will not be used at test time. In particular, the target text is just used at trainin"
2021.findings-acl.136,2020.acl-main.712,0,0.0322741,"y pjcopy of the j-th token wj can be computed as: pjcopy = σ(W1 sj + W2 vwj + bcopy ), (6) where W1 , W2 , and bcopy are trainable parameters, vwj is the embedding of token wj , and sj is the j-th hidden state from the top layer of PLM. Then, we explicitly “teach” our model how to switch between generation and copy via the copy loss as: X X LP G = pjcopy + (1 − pkcopy ). (7) wj wk Our intuition is aimed at minimizing the copy probability pjcopy of token wj (generated from vocabulary) and maximizing the copy probability pkcopy of token wk (copied from KG entities). KG Reconstruction. Following Song et al. (2020), we formalize the KG reconstruction task as predicting the relations between any two entities. In detail, given a head entity e and a tail entity e0 in generated text, we can obtain the hidden states of their mentions from the top layer of decoder, i.e., hsse , ..., soe i and hsse0 , ..., soe0 i. Then, the entity hidden states he and te0 can be computed by an average pooling over their mention hidden states. The probability for a relation r is calculated as: p(r|e, e0 ) = softmax(W3 [he ; te0 ; he te0 ] + b2 ), (8) 1561 where W3 and b2 are trainable parameters. The loss for reconstructing KG"
2021.findings-acl.136,2020.webnlg-1.11,0,0.147432,"a given PLM (e.g., GPT-2). 4 Approach For our task, two major challenges are how to learn effective input representations and capture the semantic correspondence between KG and text. To address the two challenges, we propose three major technical contributions, namely representation alignment between KG encodings and PLMs, relation-biased BFS strategy for KG linearization, and multi-task learning with KG reconstruction. Figure 2 presents an illustrative overview of our model. Next we will describe each part in detail. 4.1 Representation Alignment Unlike previous works (Ribeiro et al., 2020b; Yang et al., 2020) that directly transform KG into text sequence, we employ graph neural network (GNN) as knowledge graph encoder to explicitly encode entity relations in KG. Based on the input KG, GNN would produce a set of entity embeddings, which KG Encoder. The GNN-based KG encoder aims to generate entity embeddings for KG. Let ve ∈ RdE denote the entity embedding for a general entity e in KG, where dE is the embedding size. In our work, the entity embeddings are shared across different KGs and initialized with pretrained KG embeddings (Yang et al., 2015). We apply RGCN (Schlichtkrull et al., 2018) to gener"
2021.findings-acl.136,P19-1139,0,0.0258127,"ith the PLM word embeddings. 6 Conclusion This paper presented a few-shot KG-to-text generation model based on PLMs. We make three important technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input KG representations, and multi-task learning for learning the correspondence between KG and text. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our few-shot KG-to-text generation model. As future work, we will consider adopting KG-enhanced PLMs (Zhang et al., 2019; Peters et al., 2019) for improving the task performance, which explicitly inject knowledge information into PLMs. Acknowledgement This work was partially supported by the National Natural Science Foundation of China under Grant No. 61872369 and 61832017, Beijing Academy of Artificial Intelligence (BAAI) under Grant No. BAAI2020ZJ0301, Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098, the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China under Grant No.18XNLG22 and 19XNQ047. Xin Zhao is the correspo"
2021.findings-acl.455,2020.cl-1.4,0,0.0855293,"Pellow and Eskenazi, 2014; Paetzold, 2016), and those with language impairments (Carroll et al., 1998; Rello et al., 2013). Text simplification can also be a useful pre-processing step for other NLP tasks such as machine translation (Chen ˇ et al., 2012; Stajner and Popovic, 2016) and summarization (Vanderwende et al., 2007; Silveira and Branco, 2012). With the introduction of large, parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Xu et al., 2015), text simplification research has rapidly advanced in recent years, especially in sentence simplification (Alva-Manchego et al., 2020). However, document simplification involves rich linguistic phenomena that cannot be easily characterized by sentence-level transformations of text, e.g., the omission and addition of content (Petersen and Ostendorf, 2007; Siddharthan, 2014). This paper presents the first data-driven, dedicated study of elaborative simplification, which involves inserting elaborations in the form of definitions, explanations or clarifications to improve readability by providing readers with necessary additional context. Effective elaborations must provide background in a contextual manner, adding relevant info"
2021.findings-acl.455,C18-1021,0,0.0200127,"rtion (and conversely, that unfamiliar concepts negatively impact reading comprehension), though computational approaches till date have been largely limited to definition retrieval. Scope. We intentionally choose to study how concepts are elaborated, posing a scenario where an author has the freedom to specify where to elaborate, and our system generates an appropriate elaboration. We do this for two main reasons: first, understanding how to elaborate can be utilized in a system where users specify what to elaborate on, in the spirit of personalized simplification (Paetzold and Specia, 2016; Bingel et al., 2018). Second, determining when to elaborate is arguably pragmatically more complex, in that the need for elaboration often relies on the writer’s belief about their readers’ background, knowledge, and reading ability, as well as their own judgments on how often to elaborate. For example, in the extreme case, inserting an elaboration after every sentence could prove 5124 Original Text Simplified Text A new standard would put more areas of the country in violation of air quality standards and place parts of the West in a tough spot between a rising baseline of ozone and stricter federal limits. Limi"
2021.findings-acl.455,P18-1060,0,0.0454284,"Missing"
2021.findings-acl.455,C12-1034,0,0.0473076,"Missing"
2021.findings-acl.455,P11-2117,0,0.030189,"xt accessibility for children (De Belder and Moens, 2010; Kajiwara et al., 2013), language learners (Yano et al., 1994; Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014; Paetzold, 2016), and those with language impairments (Carroll et al., 1998; Rello et al., 2013). Text simplification can also be a useful pre-processing step for other NLP tasks such as machine translation (Chen ˇ et al., 2012; Stajner and Popovic, 2016) and summarization (Vanderwende et al., 2007; Silveira and Branco, 2012). With the introduction of large, parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Xu et al., 2015), text simplification research has rapidly advanced in recent years, especially in sentence simplification (Alva-Manchego et al., 2020). However, document simplification involves rich linguistic phenomena that cannot be easily characterized by sentence-level transformations of text, e.g., the omission and addition of content (Petersen and Ostendorf, 2007; Siddharthan, 2014). This paper presents the first data-driven, dedicated study of elaborative simplification, which involves inserting elaborations in the form of definitions, explanations or clarifications to improve readab"
2021.findings-acl.455,N19-1089,0,0.316921,"Cohen’s Kappa between expert annotations and aggregated student annotations is also substantial, at 0.67. Krippendorff’s alpha among the 13 student annotators is 0.37. As in complex NLP annotations (Nye et al., 2018), although there is subjectivity among individual annotators due to the complicated nature of the task, their aggregated judgment can be of as high quality as trained expert annotators. 2.3 Contextual Specificity At first glance, it seemed that elaborative simplification might simply involve retrieving definitions (Paetzold and Specia, 2016) or crafting informative post modifiers (Kang et al., 2019). However, while annotating candidate elaborations, we noticed that elaborations in our corpus took a variety of forms. To better understand content addition, we conducted an extensive study of elaborations and found that often times, clarification or analysis sentences specific to document context are inserted to aid comprehension or facilitate connections between content in the original text. Notably, elaborations vary in their contextual specificity, i.e., the degree to which an elaboration is specific to the context.1 For example, while simple definitions can be inserted into several diffe"
2021.findings-acl.455,N19-1317,0,0.0134465,"nt or entity with little to no background on the event the document is referring to can prove challenging for pre-trained language models. To that end, generating truly effective elaborations of medium to high contextual specificity may require some type of retrieval module. 6 Related Work Text simplification has been studied extensively (Siddharthan, 2014), especially at the sentence level. Recent progress has largely been driven by adapting monolingual translation for sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Dong et al., 2019; Kriz et al., 2019). This paradigm, while effective at transforming text, does not suffice when new content needs to be generated. A recent survey (Alva-Manchego et al., 2020) identifies explanation generation in simplification as an understudied area in dire need of new resources and methods. We tackle content addition, framed as explanation generation during simplification, and name it broadly as elaborative simplification. The need for elaborative simplification is highlighted in prior hand-coded analysis (Yano et al., 1994), which showed that language learners and other audiences benefit from insertion of re"
2021.findings-acl.455,2020.acl-main.703,0,0.143337,"Missing"
2021.findings-acl.455,N18-1049,0,0.0173527,"es with automatic sentence alignment, then use human annotation to extract true elaborations. Candidate extraction. Each set of articles in the Newsela corpus consists of multiple simplified articles ranging from grades 3–12. We choose the article written for the lowest grade level as our simplified document (we leave investigating simplified documents across higher grade levels as future work). Using the approach from Zhong et al. (2020), we then align sentences from the original and simplified documents by thresholding the cosine similarity of sentence vector representations using Sent2Vec (Pagliardini et al., 2018). We then consider sentences in the simplified document that are not aligned with any sentence in the original document as candidate elaborations. Of the 54,892 sentences across the 1,042 simplified documents (on average, 52 sentences per document), 6,207 were extracted as candidate elaborations. Human verification. Before crowdsourcing, we conducted a pilot study of elaboration verification with two sets of annotators: (1) Expert annotators (one graduate student, one undergraduate, both native speakers of English) who studied the data extensively; (2) 13 trusted undergraduate volunteer annota"
2021.findings-acl.455,2020.emnlp-main.373,0,0.0427364,"Missing"
2021.findings-acl.455,2020.acl-main.168,1,0.866296,"Missing"
2021.findings-acl.455,P02-1040,0,0.113205,"Missing"
2021.findings-acl.455,2020.emnlp-main.89,0,0.0243655,"l specificity, i.e., the degree to which an elaboration is specific to the context.1 For example, while simple definitions can be inserted into several different documents mentioning the same entity (low contextual specificity), some elaborations containing clarifications, commonsense reasoning applied to document content, or explicit inference are more contextually specific, as illustrated in Figure 2. This formulation is inspired by prior work in text specificity (Li et al., 2016; Ko et al., 2019) which is related to how a sentence “stands on its own” or sentence “decontextualization” as in Parikh et al. (2020). As we discuss in §2.4, contextually specific elaborations tend to have slightly lower sentence specificity, thus depending on the surrounding context to enhance understanding. We ask the pair of experts from the previous pilot to annotate 116 randomly chosen verified elaborations for contextual specificity. Each expert was again given the entirety of the original and simplified documents with the highlighted elaboration, and asked to label its contextual specificity on a scale of 1–3 (low/medium/high). Their Fleiss’ Kappa showed moderate agreement (Landis and Koch, 1977) with κ = 0.57. Spear"
2021.findings-acl.455,D11-1038,0,0.0436736,"mportant task, improving text accessibility for children (De Belder and Moens, 2010; Kajiwara et al., 2013), language learners (Yano et al., 1994; Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014; Paetzold, 2016), and those with language impairments (Carroll et al., 1998; Rello et al., 2013). Text simplification can also be a useful pre-processing step for other NLP tasks such as machine translation (Chen ˇ et al., 2012; Stajner and Popovic, 2016) and summarization (Vanderwende et al., 2007; Silveira and Branco, 2012). With the introduction of large, parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Xu et al., 2015), text simplification research has rapidly advanced in recent years, especially in sentence simplification (Alva-Manchego et al., 2020). However, document simplification involves rich linguistic phenomena that cannot be easily characterized by sentence-level transformations of text, e.g., the omission and addition of content (Petersen and Ostendorf, 2007; Siddharthan, 2014). This paper presents the first data-driven, dedicated study of elaborative simplification, which involves inserting elaborations in the form of definitions, explanations or clarif"
2021.findings-acl.455,P12-1107,0,0.0667817,"Missing"
2021.findings-acl.455,Q15-1021,0,0.281952,"ren (De Belder and Moens, 2010; Kajiwara et al., 2013), language learners (Yano et al., 1994; Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014; Paetzold, 2016), and those with language impairments (Carroll et al., 1998; Rello et al., 2013). Text simplification can also be a useful pre-processing step for other NLP tasks such as machine translation (Chen ˇ et al., 2012; Stajner and Popovic, 2016) and summarization (Vanderwende et al., 2007; Silveira and Branco, 2012). With the introduction of large, parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Xu et al., 2015), text simplification research has rapidly advanced in recent years, especially in sentence simplification (Alva-Manchego et al., 2020). However, document simplification involves rich linguistic phenomena that cannot be easily characterized by sentence-level transformations of text, e.g., the omission and addition of content (Petersen and Ostendorf, 2007; Siddharthan, 2014). This paper presents the first data-driven, dedicated study of elaborative simplification, which involves inserting elaborations in the form of definitions, explanations or clarifications to improve readability by providing"
2021.findings-acl.455,Q16-1029,0,0.0207672,"le, generating factually correct details about a certain event or entity with little to no background on the event the document is referring to can prove challenging for pre-trained language models. To that end, generating truly effective elaborations of medium to high contextual specificity may require some type of retrieval module. 6 Related Work Text simplification has been studied extensively (Siddharthan, 2014), especially at the sentence level. Recent progress has largely been driven by adapting monolingual translation for sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Dong et al., 2019; Kriz et al., 2019). This paradigm, while effective at transforming text, does not suffice when new content needs to be generated. A recent survey (Alva-Manchego et al., 2020) identifies explanation generation in simplification as an understudied area in dire need of new resources and methods. We tackle content addition, framed as explanation generation during simplification, and name it broadly as elaborative simplification. The need for elaborative simplification is highlighted in prior hand-coded analysis (Yano et al., 1994), which showed that lan"
2021.findings-acl.455,D17-1062,0,0.0178786,"ctually correct details about a certain event or entity with little to no background on the event the document is referring to can prove challenging for pre-trained language models. To that end, generating truly effective elaborations of medium to high contextual specificity may require some type of retrieval module. 6 Related Work Text simplification has been studied extensively (Siddharthan, 2014), especially at the sentence level. Recent progress has largely been driven by adapting monolingual translation for sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Dong et al., 2019; Kriz et al., 2019). This paradigm, while effective at transforming text, does not suffice when new content needs to be generated. A recent survey (Alva-Manchego et al., 2020) identifies explanation generation in simplification as an understudied area in dire need of new resources and methods. We tackle content addition, framed as explanation generation during simplification, and name it broadly as elaborative simplification. The need for elaborative simplification is highlighted in prior hand-coded analysis (Yano et al., 1994), which showed that language learners and other"
2021.findings-acl.455,C10-1152,0,0.0522843,"). It remains an important task, improving text accessibility for children (De Belder and Moens, 2010; Kajiwara et al., 2013), language learners (Yano et al., 1994; Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014; Paetzold, 2016), and those with language impairments (Carroll et al., 1998; Rello et al., 2013). Text simplification can also be a useful pre-processing step for other NLP tasks such as machine translation (Chen ˇ et al., 2012; Stajner and Popovic, 2016) and summarization (Vanderwende et al., 2007; Silveira and Branco, 2012). With the introduction of large, parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Xu et al., 2015), text simplification research has rapidly advanced in recent years, especially in sentence simplification (Alva-Manchego et al., 2020). However, document simplification involves rich linguistic phenomena that cannot be easily characterized by sentence-level transformations of text, e.g., the omission and addition of content (Petersen and Ostendorf, 2007; Siddharthan, 2014). This paper presents the first data-driven, dedicated study of elaborative simplification, which involves inserting elaborations in the form of definiti"
2021.naacl-main.129,2020.repl4nlp-1.10,0,0.0107468,"he classes. Training We split the data into 5 cross-validation folds, stratified by congressional hearing (to preserve the differing response distributions as seen in Figure 3). We reserve one fold for hyperparameter tuning and use the remaining 4 folds for cross-validation at test time.9 Baselines The A LL P OSITIVE baseline predicts 1 for all labels. This baseline easily outperforms a majority baseline that predicts the most frequent label (answer+direct). L OG R EGRESSION performs logistic regression with bag-of-words representations. CNN is a convolutional neural network as implemented in Adhikari et al. (2020). Other baselines performing lower than CNN are in Appendix C. Pretrained We experiment with several pretrained language models, and find RO BERTA (Liu et al., 2019) performs the best on the held-out development fold. We use the implementation from Hugging Face.10 We feed in the tokenized response text and truncate input to 512 word pieces (additional inputs used in the model variants we describe next are separated by the [SEP] token). Hierarchical We use two classifiers to mimic the hierarchy of our taxonomy: the first classifier predicts the conversation act while the second predicts the com"
2021.naacl-main.129,J08-4004,0,0.105177,"Missing"
2021.naacl-main.129,Q19-1035,0,0.0287432,"Missing"
2021.naacl-main.129,N03-2011,0,0.336484,"Missing"
2021.naacl-main.129,N19-1173,0,0.0125943,"ure 3). We reserve one fold for hyperparameter tuning and use the remaining 4 folds for cross-validation at test time.9 Baselines The A LL P OSITIVE baseline predicts 1 for all labels. This baseline easily outperforms a majority baseline that predicts the most frequent label (answer+direct). L OG R EGRESSION performs logistic regression with bag-of-words representations. CNN is a convolutional neural network as implemented in Adhikari et al. (2020). Other baselines performing lower than CNN are in Appendix C. Pretrained We experiment with several pretrained language models, and find RO BERTA (Liu et al., 2019) performs the best on the held-out development fold. We use the implementation from Hugging Face.10 We feed in the tokenized response text and truncate input to 512 word pieces (additional inputs used in the model variants we describe next are separated by the [SEP] token). Hierarchical We use two classifiers to mimic the hierarchy of our taxonomy: the first classifier predicts the conversation act while the second predicts the complete label (conversation act+intent). We train the classifiers independently, and condition the second classifier on the ground truth of the first classifier during"
2021.naacl-main.129,N19-1177,0,0.0258602,"speaker. Detection of deception is, unlike many other NLP tasks, challenging even for humans (Ott et al., 2011). Most datasets consist of instructed lies (where participants are told to lie). Our work contains naturally-occurring deception where we include not just lying but other more covert mechanisms such as being deliberately vague or evasive (Clementson, 2018), both frequent in political discourse (Bull, 2008). Argumentation mining analyzes non-cooperative conversations, but typically requires expert annotators. Recent work decomposes the task into intuitive questions for crowdsourcing (Miller et al., 2019), inspiring our annotation schemes that assume little to no training. Closer to our setting is argument persuasiveness, where Durmus and Cardie (2018) find prior beliefs of the audience play a strong role in their ability to be persuaded, which further motivates our focus on the annotator’s bias. 3 Dataset item question response #sents/ turn #toks/ total turn sents total toks total spkrs 4.1 2.6 81.5 47.0 82582 48831 91 20 4096 2634 Table 1: Statistics of our 20 U.S. congressional hearings. the response, and the data is plentiful.2 A dataset statement is in Appendix D. 3.1 Dataset creation Con"
2021.naacl-main.129,2020.emnlp-main.734,0,0.0351484,"bligations, such as responding to a question (Traum and Allen, 1994; Potts, 2008). For this reason, we explicitly separate judgments on conversation acts (that usually fulfill a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to understand perception"
2021.naacl-main.129,P11-1032,0,0.0605211,"at the utterance level. Classification models typically combine representations of linguistic units (word, utterance, conversation-level) (Chen et al., 2018). In our work, we employ a hierarchical model to account for the levels in our label taxonomy. Intent detection is traditionally applied to human-computer scenarios for task-specific goals such as booking a flight. Our conversation data is not task-oriented, and we thus define our intents more closely aligned with beliefs in the sincerity of the speaker. Detection of deception is, unlike many other NLP tasks, challenging even for humans (Ott et al., 2011). Most datasets consist of instructed lies (where participants are told to lie). Our work contains naturally-occurring deception where we include not just lying but other more covert mechanisms such as being deliberately vague or evasive (Clementson, 2018), both frequent in political discourse (Bull, 2008). Argumentation mining analyzes non-cooperative conversations, but typically requires expert annotators. Recent work decomposes the task into intuitive questions for crowdsourcing (Miller et al., 2019), inspiring our annotation schemes that assume little to no training. Closer to our setting"
2021.naacl-main.129,Q19-1043,0,0.0269729,"ons of dialogue, or discourse obligations, such as responding to a question (Traum and Allen, 1994; Potts, 2008). For this reason, we explicitly separate judgments on conversation acts (that usually fulfill a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to un"
2021.naacl-main.129,C16-1181,0,0.029574,"Missing"
2021.naacl-main.129,W12-3205,0,0.0272315,"a specific obligation) from communicative intents, which can be perceived as deceptive (or sincere). Prior work examines how writer intentions are often misaligned with reader perceptions (Chang et al., 2020), which further motivates our focus on the reader (our annotator). While our work focuses on subjectivity, ambiguity is studied in many NLP tasks, including Natural Language Inference (Pavlick and Kwiatkowski, 2019; Nie et al., 2020), evaluation of NLG (Schoch et al., 2020), a recent SemEval 2021 shared task,1 as well as several discourse tasks (Asher and Lascarides, 2003; Versley, 2011; Webber and Joshi, 2012; Das et al., 2017; Poesio et al., 2019; Webber et al., 2019). Only one study strives to understand how these ambiguities are resolved: Scholman (2019) shows different interpretations of ambiguous coherence relations can be attributable to different cognitive biases. However, our work focuses more generally on subjecIn summary, the task together with the dataset present a valuable opportunity to understand perceptions of discourse in a non-cooperative environment. More broadly, we show the need and value 1 for considering the subjectivity of NLP tasks. Our https://sites.google.com/view/ work i"
2021.naacl-main.129,W19-0411,0,0.0830517,". congressional hearings. In Figure 1, annotators give conflicting assessments of responses given by the witness Mark Zuckerberg (CEO of Facebook) who is being questioned by Congressman Eliot Engel. Discourse, like many uses of language, has inherent ambiguity, meaning it can have multiple, valid interpretations. Much work has focused on characTo make sense of our setting that has speakers terizing these “genuine disagreements” (Asher and (witness, politicians) and observers (annotators), Lascarides, 2003; Das et al., 2017; Poesio et al., we are inspired by the game-theoretic view of con2019; Webber et al., 2019) and incorporating their uncertainty through concurrent labels (Rohde et al., versation in Asher and Paul (2018). The players (witness, politicians) make certain discourse moves 2018) and underspecified structures (Hanneforth et al., 2003). However, prior work does not exam- in order to influence a third party, who is the judge of the game (the annotator). Importantly, the judge ine the subjectivity of discourse: how you resolve makes biased evaluations about the type of the an ambiguity by applying your personal beliefs and player (e.g., sincere vs. deceptive), which leads preferences. Our wo"
2021.naacl-main.395,W14-1207,0,0.030246,"rk Recent efforts on data-driven text simplification methods have tended to rely on two resources: the Wikipedia-Simple Wikipedia aligned corpus (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and the Newsela simplification corpus (Xu et al., 2015). Yet, there is an urgent need to simplify medical texts due to health literacy levels (World Health Organization, 2013). However, due to a lack of resources with which to train model-based simplification systems in this domain, past work has tended to focus on lexical simplification (Damay et al., 2006; Kandula et al., 2010; Abrahamsson et al., 2014; Mukherjee et al., 2017). Recently, Adduru et al. (2018) and Van den Bercken et al. (2019) introduced sentence-aligned corpora at the scale of thousands of sentence pairs. In contrast to our corpus, these datasets were automatically derived using paraphrase mining or monolingual alignment processes. Furthermore, as these are exclusively sentence corpora, they limit the set of potential approaches to just those that operate over sentences. Grabar and Cardon (2018) created a simplification corpus for medical texts in French, in which a small subset of the text pairs are manually sentence-aligne"
2021.naacl-main.395,2020.cl-1.4,0,0.012795,"l stay. We found no evidence suggesting that giving infants high volumes of milk causes feeding or gut problems, but this finding is not certain. Table 1: Sample excerpts from a technical abstract (top) and corresponding plain-language summary (bottom) from the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automati"
2021.naacl-main.395,N18-3011,0,0.0260975,"l., 2019). is pretrained on technical literature. A paired t-test Capitalizing on this intuition, we consider two confirms that these observed differences between large-scale pre-trained masked language modthe abstracts and PLS distributions are statistically els: (1) BERT (Devlin et al., 2019) trained on significant (with p &lt; 0.01). BooksCorpus (Zhu et al., 2015) and English Wikipedia; and (2) SciBERT (Beltagy et al., 2019), Which metric discriminates better? To better trained on a sample of 1.14 million technical pa- determine how well the proposed masked probabilpers from Semantic Scholar (Ammar et al., 2018) ity outputs discriminate between technical abstracts (mostly biomedical and computer science articles). and PLS, we plot receiver operating characteristic 4975 Figure 1: BERT (left) vs SciBERT (right) probabilities of technical abstracts (blue) and PLS (red). 1.0 True Positive Rate 0.8 0.6 0.4 0.2 0.0 0.0 Flesch-Kincaid (area = 0.68) ARI (area = 0.57) General BERT (area = 0.66) SciBERT (area = 0.70) 0.2 0.4 0.6 False Positive Rate 0.8 1.0 Figure 2: ROC Curves for Readability Metrics. (ROC) curves for the outputs of BERT, SciBERT, Flesch-Kincaid and ARI, coding technical and PLS abstracts as 0"
2021.naacl-main.395,D19-1371,0,0.0177855,"red to a model trained technical abstracts than for those in the plain lanover general lay corpora, as in the original BERT guage versions, as we might expect given that this model (Devlin et al., 2019). is pretrained on technical literature. A paired t-test Capitalizing on this intuition, we consider two confirms that these observed differences between large-scale pre-trained masked language modthe abstracts and PLS distributions are statistically els: (1) BERT (Devlin et al., 2019) trained on significant (with p &lt; 0.01). BooksCorpus (Zhu et al., 2015) and English Wikipedia; and (2) SciBERT (Beltagy et al., 2019), Which metric discriminates better? To better trained on a sample of 1.14 million technical pa- determine how well the proposed masked probabilpers from Semantic Scholar (Ammar et al., 2018) ity outputs discriminate between technical abstracts (mostly biomedical and computer science articles). and PLS, we plot receiver operating characteristic 4975 Figure 1: BERT (left) vs SciBERT (right) probabilities of technical abstracts (blue) and PLS (red). 1.0 True Positive Rate 0.8 0.6 0.4 0.2 0.0 0.0 Flesch-Kincaid (area = 0.68) ARI (area = 0.57) General BERT (area = 0.66) SciBERT (area = 0.70) 0.2 0"
2021.naacl-main.395,P11-2117,0,0.238037,"om the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A"
2021.naacl-main.395,N19-1423,0,0.0115063,"’ of text. In particular, when such models induce distributions over instances from the respecare trained on specialized or technical language tive sets that are clearly different. For example, (e.g., scientific articles) we would expect the likeSciBERT (which yields sharper differences) outlihoods subsequently assigned to ‘jargon’ tokens puts higher likelihoods for tokens comprising the to be relatively high compared to a model trained technical abstracts than for those in the plain lanover general lay corpora, as in the original BERT guage versions, as we might expect given that this model (Devlin et al., 2019). is pretrained on technical literature. A paired t-test Capitalizing on this intuition, we consider two confirms that these observed differences between large-scale pre-trained masked language modthe abstracts and PLS distributions are statistically els: (1) BERT (Devlin et al., 2019) trained on significant (with p &lt; 0.01). BooksCorpus (Zhu et al., 2015) and English Wikipedia; and (2) SciBERT (Beltagy et al., 2019), Which metric discriminates better? To better trained on a sample of 1.14 million technical pa- determine how well the proposed masked probabilpers from Semantic Scholar (Ammar et"
2021.naacl-main.395,P19-1331,0,0.0671541,"rmation that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone online to figure out a medical condition” (Fox and Duggan, 2013)."
2021.naacl-main.395,W18-7002,0,0.0204112,"fication systems in this domain, past work has tended to focus on lexical simplification (Damay et al., 2006; Kandula et al., 2010; Abrahamsson et al., 2014; Mukherjee et al., 2017). Recently, Adduru et al. (2018) and Van den Bercken et al. (2019) introduced sentence-aligned corpora at the scale of thousands of sentence pairs. In contrast to our corpus, these datasets were automatically derived using paraphrase mining or monolingual alignment processes. Furthermore, as these are exclusively sentence corpora, they limit the set of potential approaches to just those that operate over sentences. Grabar and Cardon (2018) created a simplification corpus for medical texts in French, in which a small subset of the text pairs are manually sentence-aligned, resulting in 663 sentence pairs, 112 of which are also from Cochrane. With respect to modeling, recent work has focused on sentence simplification, treating it as a monolingual machine translation task (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016) using encoder-decoder models (Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019). In the medical domain, existing systems tend to adopt lexical and syntactic simplification (Damay et al., 2006;"
2021.naacl-main.395,P19-1356,0,0.0213978,"een abstracts and generated PLS are statistically significant; so are differences in FK and ARI between UL models and No-UL (p &lt; 0.01, paired t-test). puts, however, are at the late-high school/early college levels. This could reflect the relatively small differences in readability scores between abstracts and PLS in general (Section 3.2). 5.2 Style In Section 3.2 we showed that SciBERT masked probability scores are more useful as a discriminator between technical abstracts and PLS than the standard readability metrics, which use surfacelevel cues like word and sentence counts. Experiments by Jawahar et al. (2019) suggest that BERTstyle masked language models encode a wide array of syntactic and semantic features of language, which they then employs for downstream tasks. For this reason, we use SciBERT masked probability scores as our notion of style, with lower scores corresponding to simpler, less technical language. To explore the extent to which the generated summaries stylistically resemble the PLS, we computed the average of the SciBERT masked probability scores of the generated texts for each model. The results are shown in Table 5 along with the readability scores. We see that every model produ"
2021.naacl-main.395,C10-1062,0,0.0511234,"rastically different in length, by keeping only instances where the length ratio between the two falls between 0.2 and 1.3. Our final dataset comprises 4459 pairs of technical abstracts and PLS, all containing ≤1024 tokens (so that they can be fed into the BART model in their entirety). 3.2 Characterizing readability differences Readability metrics. Designing metrics that reliably capture readability remains an open topic of research. In recent years, a host of metrics have been developed that use a wide variety of linguistic features to assess readability in a supervised manner. For example, Kate et al. (2010) developed a metric based on syntactical, semantic, and language model-based features, and Vajjala and Luˇci´c (2018) developed a new readability corpus, on which they trained support vector machines to predict text readability. For this medical text simplification task, however, we considered a couple 1. Background, Objectives, Search Methods, Selection Criestablished heuristics-based readability metrics due teria, Data Collection and Analysis, Main Results, Auto clear domain differences between our Cochrane thors’ Conclusions corpus and those used to train supervised read2. Background, Objec"
2021.naacl-main.395,N19-1317,0,0.0678952,"erving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone online to figure out a medical condition” (Fox"
2021.naacl-main.395,2020.acl-main.703,0,0.518224,"75) and Automated Readability Index (Senter and Smith, 1967), are small. Instead, the differences are better captured using large-scale pretrained masked language models, and this reveals that there is more to the language difference than the shallow cues such as sentence and word lengths that traditional readability metrics focus on. We present baseline methods for automatic text simplification over this data and perform analyses that highlight the challenges of this important simplification task. We find that when naively finetuned for the task, existing encoder-decoder models such as BART (Lewis et al., 2020) tend to prefer deletion over paraphrasing or explaining, and are prone to generating technical words. We propose a new approach to try and mitigate the latter issue by imposing a variant of unlikelihood loss (Welleck et al., 2019) that explicitly penalizes the decoder for production of ‘technical’ tokens. We show that this yields improvements in terms of readability with only a minor tradeoff with content quality. lihood training to explicitly penalize models for producing jargon. We release our code and data at https://github.com/AshOlogn/Paragraphlevel-Simplification-of-Medical-Texts. 2 Rel"
2021.naacl-main.395,2020.acl-main.428,0,0.0383659,"obability distribution constructed by removing the ‘tail’ of probability mass from BART’s output distribution and then renormalizing. This strategy mitigates the awkward repetition typical of greedy methods like beam search while still avoiding incoherence by truncating the unlikely tail in the original model distribution. 4.2 Unlikelihood training As an additional mechanism to encourage simple terminology in the PLS generated by our model, we propose a new method in which we explicitly penalize the model for producing seemingly technical words via unlikelihood training (Welleck et al., 2019; Li et al., 2020). The idea is to add a term to the objective that encourages the model to decrease the probability mass assigned to some set of tokens S. This is realized by adding a term to the (log) P|S| loss: U L = j=1 − log(1−pθ (sj |y&lt;t , x)), where x is the technical abstract input to the encoder, y&lt;t is the prefix of the target summary y input to the decoder at time t, and pθ (sj |y&lt;t , x) is the probability assigned to token sj in the distribution output by BART (with model parameters θ) at time t. This 1 We also considered starting from a checkpoint corresponding to training over CNN/Daily News but p"
2021.naacl-main.395,L16-1505,0,0.0204471,"ion corpus for medical texts in French, in which a small subset of the text pairs are manually sentence-aligned, resulting in 663 sentence pairs, 112 of which are also from Cochrane. With respect to modeling, recent work has focused on sentence simplification, treating it as a monolingual machine translation task (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016) using encoder-decoder models (Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019). In the medical domain, existing systems tend to adopt lexical and syntactic simplification (Damay et al., 2006; Kandula et al., 2010; Llanos et al., 2016). Research on document simplification has been sparse; In sum, this work takes a step towards paragraph- to the best of our knowledge, the few prior works on level simplification of medical texts by: (1) intro- this in English have focused on analysis (Petersen ducing a sizable new dataset, (2) proposing and and Ostendorf, 2007), sentence deletion (Woodvalidating a new masked language model (MLM)- send and Lapata, 2011; Zhong et al., 2020), and based metric for scoring the technicality of texts, localized explanation generation (Srikanth and (3) analyzing and understanding the style of plain L"
2021.naacl-main.395,D18-1206,0,0.0600654,"Missing"
2021.naacl-main.395,P02-1040,0,0.109933,"Missing"
2021.naacl-main.395,W18-0535,0,0.0461471,"Missing"
2021.naacl-main.395,2020.emnlp-demos.6,0,0.0854253,"Missing"
2021.naacl-main.395,D11-1038,0,0.258409,"anguage summary (bottom) from the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical informatio"
2021.naacl-main.395,P12-1107,0,0.0732422,"Missing"
2021.naacl-main.395,Q15-1021,0,0.345603,"al solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of Ame"
2021.naacl-main.395,Q16-1029,0,0.255393,"ccessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone onli"
2021.naacl-main.395,D17-1062,0,0.0978497,"der audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for accessible medical information has never been greater. A Pew Research survey of American’s online health habits in 2013 revealed that “one in three American adults have gone online to figure out a medic"
2021.naacl-main.395,C10-1152,0,0.231048,"responding plain-language summary (bottom) from the Cochrane Library. One potential solution to this problem is text simplification, i.e., editing documents such that they are accessible to a wider audience, while preserving the key information that they contain. Although manual simplification is too expensive to feasibly apply at scale, automatic text simplification (Siddharthan, 2014; Alva-Manchego et al., 2020) provides a potential means of rendering a large volume of specialist knowledge more accessible. Large-scale data-driven simplification systems have mostly been trained on Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) and news (Xu et al., 2015), and focus on sentence simplification (Wubben et al., 2012; Wang et al., 2016; Xu et al., 2016; Zhang and Lapata, 2017; Kriz et al., 2019; Dong et al., 2019; Alva-Manchego et al., 2020); on the other hand, medical text simplification is resource poor. Recent work has involved constructing sentencealigned data automatically using monolingual text alignment methods (Adduru et al., 2018; Van den Bercken et al., 2019), but this process is noisy and constrains the task to sentence-level simplification. The need for ac"
2021.scil-1.39,N01-1021,0,0.0718761,"of the identified jargon phrases. Methods At a high level, we examine the ‘fit’ of jargon words in its context via perplexity from a large-scale pre-trained language model, GPT2 (Radford et al., 2019). As the name suggests, perplexity is a quantitative measurement of how ‘perplexed’ the language model is when confronted with text; formally, it is defined as the exponentiated average log-likelihood of a sequence of words: 381 1 P P L(W ) = 2− n log2 P (w1 ,...,wn ) 1 = 2− n Pn i=1 log2 P (wi |w&lt;i ) (1) Note that − log2 P (wi |w&lt;i ) quantifies the surprisal of the word wi given its context w&lt;i (Hale, 2001; Levy, 2008) following the seminal work of Shannon (1948), who suggested that when a word is likely to occur given a context, it communicates less information, thus taking less time to process. Since GPT-2 is trained using a large amount of general-domain English, examining the perplexity differences in jargon and non-jargon tokens would allow us to understand how surprising tokens in a phrase are given their preceding contexts. This distributional analysis uses t-tests, where t-values represents a potential difference in the mean between two independent distributions. Here, we compare the pe"
2021.scil-1.39,W18-5306,0,0.0612166,"Missing"
2021.scil-1.39,2020.finnlp-1.13,0,0.0596243,"Missing"
2021.sigdial-1.34,N19-1220,0,0.0611918,"Missing"
2021.sigdial-1.34,D15-1263,0,0.0276587,"resolving the incongruity of the first clause being negative and the second clause being positive by illustrating how the negative statement in the subordinate clause is reversed by the positive one in the main clause. The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow information. The predicted relations are then used to draw inferences from the text. The value of predicting the semantic classes of coherence relations has been demonstrated in several applications, including sentiment analysis (Marcu, 2000; Bhatia et al., 2015), machine comprehension (Narasimhan and Barzilay, 2015), summarization (Cohan et al., 2018; Marcu, 1999; Xu et al., 2019; Kikuchi et al., 2014), and predicting instructor intervention in an online course discussion forum (Chandrasekaran 314 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between t"
2021.sigdial-1.34,E17-1028,0,0.146145,"could be said about the relation prediction performance of RST parsers. Such performance hinders the wider application of parsers. If downstream tasks are to use predicted relation senses, the data to which the systems are applied is typically different from their training data—the Wall Street Journal (WSJ) in a 3-year window—to varying degrees. This tends to further aggravate the low performance observed. As a result, often we find that adding parsed discourse relations into models are unhelpful. Although domain difference is a recognized issue in shallow discourse parsing by existing work (Braud et al., 2017; Liu et al., 2016), we still have little understanding of the types of distributional shift that matter and by how much, even within one language. This position paper seeks to shed some light on our current state in discourse parsing in English. Surprisingly, we found that parsers have some issues even within the same news source as the training set (WSJ); the differences in accuracy were not significant between indomain and out-of-domain data for the qualitative examples that we looked at, although the distribution of errors tend to be different. This differs from other NLP tasks such as ent"
2021.sigdial-1.34,D19-1060,0,0.0540713,"Missing"
2021.sigdial-1.34,N18-2097,0,0.0202806,"sitive by illustrating how the negative statement in the subordinate clause is reversed by the positive one in the main clause. The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow information. The predicted relations are then used to draw inferences from the text. The value of predicting the semantic classes of coherence relations has been demonstrated in several applications, including sentiment analysis (Marcu, 2000; Bhatia et al., 2015), machine comprehension (Narasimhan and Barzilay, 2015), summarization (Cohan et al., 2018; Marcu, 1999; Xu et al., 2019; Kikuchi et al., 2014), and predicting instructor intervention in an online course discussion forum (Chandrasekaran 314 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distribution"
2021.sigdial-1.34,N18-1013,0,0.0253871,"nd they also had access to the article text if they ever needed to reference back to it. To assess the inter-rater agreement, we determine Cohen’s κ value (Cohen, 1960). We randomly selected 25 samples from the PDTB and assigned each to the annotators. We obtained a Cohen’s κ of 0.88, which indicates almost perfect agreement. 4.2 Findings More context than the two arguments is needed to determine the correct discourse relation in many cases One potential way to mitigate the impact of domain shift on the performance of shallow discourse parsers is to incorporate context. With a few exceptions (Dai and Huang, 2018; Shi and Demberg, 2019; Zhang et al., 2021), existing models for shallow discourse parsing mostly do not Insights for model development While inspecting the results of the annotations, we found several helpful phenomena for developing future models, including observations regarding the role of context in shallow discourse parsing and errors that current RST parsers are making. 320 Figure 3: RST parse tree containing a segment of the relations that were examined in the qualitative analysis. The discourse sense labels on this tree that were examined in our analysis are marked red and green, whe"
2021.sigdial-1.34,F12-2042,0,0.057482,"Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to. The necessity of discourse research has resulted in several shared tasks (Xue et al., 2015, 2016) and corpora development in multiple languages (Zeyrek and Webber, 2008; Meyer et al., 2011; Danlos et al., 2012; Zhou et al., 2014; Zeyrek et al., 2020). Yet shallow discourse parsing is a very difficult task; more than 10 years after the introduction of the Penn Discourse Treebank (Eleni Miltsakaki, 2004), performance for English implicit discourse relation recognition has gone from 40.2 F-1 (Lin et al., 2009) to 47.8 (Lee et al., 2020), less than 8 percentage points; a similar story could be said about the relation prediction performance of RST parsers. Such performance hinders the wider application of parsers. If downstream tasks are to use predicted relation senses, the data to which the systems ar"
2021.sigdial-1.34,miltsakaki-etal-2004-penn,0,0.129037,"Missing"
2021.sigdial-1.34,P14-1048,0,0.0317324,"cent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to determine the discourse sense without considering further context. Additionally, in RST parsing, standard practice involves classifying only the 18 top-level RST classes (Hernault et al., 2010; Feng and Hirst, 2014; Morey et al., 2017). Thus, all Elaboration relations are lumped together, making it a huge class. We reveal findings about these assumptions in Section 4. Other works evaluating discourse parsers include DiscoEval (Chen et al., 2019), a test suite of evaluation tasks that test the effectiveness of different sentence encoders for discourse parsers, and an improved evaluation protocol for the PDTB-2 (Kim et al., 2020). In contrast, our work aims to analyze and evaluate existing discourse parsers via gradual domain shift. We provide a comparative genrebased analysis on distributionally shifted"
2021.sigdial-1.34,P14-2052,0,0.0230773,"in the subordinate clause is reversed by the positive one in the main clause. The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow information. The predicted relations are then used to draw inferences from the text. The value of predicting the semantic classes of coherence relations has been demonstrated in several applications, including sentiment analysis (Marcu, 2000; Bhatia et al., 2015), machine comprehension (Narasimhan and Barzilay, 2015), summarization (Cohan et al., 2018; Marcu, 1999; Xu et al., 2019; Kikuchi et al., 2014), and predicting instructor intervention in an online course discussion forum (Chandrasekaran 314 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to. The nec"
2021.sigdial-1.34,2020.acl-main.480,0,0.141061,"considered extremely challenging for discourse parsers to automatically identify. There is a need to examine the performance of the proposed discourse parsers, their representational choices, their generalizability, and interpretability both across domains, distributions, and frameworks. One recently developed framework is the PDTB-3. Since its release in 2019, several papers have evaluated the performance of implicit sense classifiers on this new corpus, which includes newly annotated intra-sentential implicit discourse relations. In addition to proposing a new evaluation framework for PDTB, Kim et al. (2020) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB-2 and the PDTB-3. Liang et al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent develop"
2021.sigdial-1.34,2020.lrec-1.145,0,0.0320412,"opment While inspecting the results of the annotations, we found several helpful phenomena for developing future models, including observations regarding the role of context in shallow discourse parsing and errors that current RST parsers are making. 320 Figure 3: RST parse tree containing a segment of the relations that were examined in the qualitative analysis. The discourse sense labels on this tree that were examined in our analysis are marked red and green, where green is correct and red is incorrect use input beyond the two adjacent sentences that comprise the arguments of the relation (Kishimoto et al., 2020; Chen et al., 2019). We found that only considering these two sentences is not sufficient even for our expert linguist annotators. Specifically, while annotating the PDTB, the annotators found several examples where, when they looked at the larger context behind the arguments and the sentences where the arguments were contained, their annotations changed. Below, we describe a few examples that demonstrate the mistakes that can be made without the full context and their implications: (4) In this northern latitude it does n’t get dark in summer until about 10:30 p.m. so lighting is operate exce"
2021.sigdial-1.34,D19-1587,0,0.0366493,"Missing"
2021.sigdial-1.34,W19-5927,0,0.0115507,"tion framework for PDTB, Kim et al. (2020) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB-2 and the PDTB-3. Liang et al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to determine the discourse sense without considering further context. Additionally, in RST parsing, standard practice involves classifying only the 18 top-level RST classes (Hernault et al., 20"
2021.sigdial-1.34,2020.emnlp-main.120,0,0.222499,"exacerbated by the distributional shifts in the texts they need to apply to. The necessity of discourse research has resulted in several shared tasks (Xue et al., 2015, 2016) and corpora development in multiple languages (Zeyrek and Webber, 2008; Meyer et al., 2011; Danlos et al., 2012; Zhou et al., 2014; Zeyrek et al., 2020). Yet shallow discourse parsing is a very difficult task; more than 10 years after the introduction of the Penn Discourse Treebank (Eleni Miltsakaki, 2004), performance for English implicit discourse relation recognition has gone from 40.2 F-1 (Lin et al., 2009) to 47.8 (Lee et al., 2020), less than 8 percentage points; a similar story could be said about the relation prediction performance of RST parsers. Such performance hinders the wider application of parsers. If downstream tasks are to use predicted relation senses, the data to which the systems are applied is typically different from their training data—the Wall Street Journal (WSJ) in a 3-year window—to varying degrees. This tends to further aggravate the low performance observed. As a result, often we find that adding parsed discourse relations into models are unhelpful. Although domain difference is a recognized issue"
2021.sigdial-1.34,2020.codi-1.14,0,0.488525,"e proposed discourse parsers, their representational choices, their generalizability, and interpretability both across domains, distributions, and frameworks. One recently developed framework is the PDTB-3. Since its release in 2019, several papers have evaluated the performance of implicit sense classifiers on this new corpus, which includes newly annotated intra-sentential implicit discourse relations. In addition to proposing a new evaluation framework for PDTB, Kim et al. (2020) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB-2 and the PDTB-3. Liang et al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Dem"
2021.sigdial-1.34,P19-1410,0,0.015608,"mplicit sense classifiers on this new corpus, which includes newly annotated intra-sentential implicit discourse relations. In addition to proposing a new evaluation framework for PDTB, Kim et al. (2020) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB-2 and the PDTB-3. Liang et al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to determine the disco"
2021.sigdial-1.34,D09-1036,0,0.148192,"Missing"
2021.sigdial-1.34,J93-2004,0,0.0795284,"relations between discourse segments in PDTB (Rashmi Prasad, 2008) to hierarchical constituent structures in RST (Carlson et al., 2003) or discourse graphs in Segmented Discourse Representation Theory (SDRT) (Asher et al., 2003) and the Discourse Graphbank (Wolf and Gibson, 2005). Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) provides a hierarchical structure for analyzing text that describes relations between text spans known as elementary discourse units (EDUs). The RST Discourse Treebank (Carlson et al., 2003) contains 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) which have been split into elementary discourse units and annotated according to Rhetorical Structure Theory, where discourse relations are annotated in a tree structure across the whole document. A full list of these relations can be found in Carlson and Marcu (2001). The Penn Discourse Treebank (PDTB) (Eleni Miltsakaki, 2004; Rashmi Prasad, 2008; Prasad et al., 2018), which also uses Penn Treebank Wall Street Journal articles, contains discourse relations annotated in a shallow, non-hierarchical manner. For each relation between two arguments, each argument and the discourse connective (wor"
2021.sigdial-1.34,W11-2022,0,0.0124672,"oup on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to. The necessity of discourse research has resulted in several shared tasks (Xue et al., 2015, 2016) and corpora development in multiple languages (Zeyrek and Webber, 2008; Meyer et al., 2011; Danlos et al., 2012; Zhou et al., 2014; Zeyrek et al., 2020). Yet shallow discourse parsing is a very difficult task; more than 10 years after the introduction of the Penn Discourse Treebank (Eleni Miltsakaki, 2004), performance for English implicit discourse relation recognition has gone from 40.2 F-1 (Lin et al., 2009) to 47.8 (Lee et al., 2020), less than 8 percentage points; a similar story could be said about the relation prediction performance of RST parsers. Such performance hinders the wider application of parsers. If downstream tasks are to use predicted relation senses, the data to"
2021.sigdial-1.34,D17-1136,0,0.0169348,"as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to determine the discourse sense without considering further context. Additionally, in RST parsing, standard practice involves classifying only the 18 top-level RST classes (Hernault et al., 2010; Feng and Hirst, 2014; Morey et al., 2017). Thus, all Elaboration relations are lumped together, making it a huge class. We reveal findings about these assumptions in Section 4. Other works evaluating discourse parsers include DiscoEval (Chen et al., 2019), a test suite of evaluation tasks that test the effectiveness of different sentence encoders for discourse parsers, and an improved evaluation protocol for the PDTB-2 (Kim et al., 2020). In contrast, our work aims to analyze and evaluate existing discourse parsers via gradual domain shift. We provide a comparative genrebased analysis on distributionally shifted text data and present"
2021.sigdial-1.34,P15-1121,0,0.02222,"e being negative and the second clause being positive by illustrating how the negative statement in the subordinate clause is reversed by the positive one in the main clause. The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow information. The predicted relations are then used to draw inferences from the text. The value of predicting the semantic classes of coherence relations has been demonstrated in several applications, including sentiment analysis (Marcu, 2000; Bhatia et al., 2015), machine comprehension (Narasimhan and Barzilay, 2015), summarization (Cohan et al., 2018; Marcu, 1999; Xu et al., 2019; Kikuchi et al., 2014), and predicting instructor intervention in an online course discussion forum (Chandrasekaran 314 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the pars"
2021.sigdial-1.34,2020.tacl-1.15,0,0.041198,"Missing"
2021.sigdial-1.34,D08-1020,0,0.0878938,"ot disappoint me at all.” Introduction Discourse analysis is a crucial analytic level in NLP. In natural language discourse, speakers and writers often rely on implicit inference to signal the kind of contribution they are making to the conversation, as well as key relationships that justify their point of view. While early AI literature is full of case studies suggesting that this inference is complex, open-ended and knowledge-heavy (e.g., Charniak (1973); Schank and Abelson (1977)), recent work on computational discourse coherence offers a different approach. Take the following example from Pitler and Nenkova (2008): (1) “Alice thought the story was predictable. She found it boring.” This represents a Concession relation according to both Rhetorical Structure Theory and the Penn Discourse Treebank (where it is notated as Comparison.Concession), resolving the incongruity of the first clause being negative and the second clause being positive by illustrating how the negative statement in the subordinate clause is reversed by the positive one in the main clause. The importance of discourse has led to active research based on predicting what coherence relations are present in text based on shallow informatio"
2021.sigdial-1.34,W18-4710,0,0.0282254,"ture for analyzing text that describes relations between text spans known as elementary discourse units (EDUs). The RST Discourse Treebank (Carlson et al., 2003) contains 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) which have been split into elementary discourse units and annotated according to Rhetorical Structure Theory, where discourse relations are annotated in a tree structure across the whole document. A full list of these relations can be found in Carlson and Marcu (2001). The Penn Discourse Treebank (PDTB) (Eleni Miltsakaki, 2004; Rashmi Prasad, 2008; Prasad et al., 2018), which also uses Penn Treebank Wall Street Journal articles, contains discourse relations annotated in a shallow, non-hierarchical manner. For each relation between two arguments, each argument and the discourse connective (word or phrase that indicates the discourse relation) are labeled. The PDTB also annotates whether a 315 1 Our data is located here: https://github.com/ katherine-atwell/discourse-domain-shift relation is explicit or non-explicit, the latter type of which has three subtypes: Implicit, AltLex, and EntRel. In this paper, we focus on implicit relations, where a connective can"
2021.sigdial-1.34,D19-1233,0,0.0180252,"the performance of implicit sense classifiers on this new corpus, which includes newly annotated intra-sentential implicit discourse relations. In addition to proposing a new evaluation framework for PDTB, Kim et al. (2020) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB-2 and the PDTB-3. Liang et al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to d"
2021.sigdial-1.34,D19-1586,0,0.257288,"al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to determine the discourse sense without considering further context. Additionally, in RST parsing, standard practice involves classifying only the 18 top-level RST classes (Hernault et al., 2010; Feng and Hirst, 2014; Morey et al., 2017). Thus, all Elaboration relations are lumped together, making it a huge class. We reveal findings about these ass"
2021.sigdial-1.34,2020.acl-main.569,0,0.0162132,"sifiers on this new corpus, which includes newly annotated intra-sentential implicit discourse relations. In addition to proposing a new evaluation framework for PDTB, Kim et al. (2020) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB-2 and the PDTB-3. Liang et al. (2020) identify locating the position of relations as a new challenge in the PDTB-3, due to the significantly increased number of intra-sentential implicit relations annotated. Techniques of discourse parsing range from supervised (Liu et al., 2019; Mabona et al., 2019; Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) and weakly supervised and unsupervised approaches (Lee et al., 2020; Nishida and Nakayama, ¨ 2020; Kurfalı and Ostling, 2019); recent developments such as word/contextual embeddings have improved parser performance, although not as significantly as other tasks (Shi and Demberg, 2019; Chen et al., 2019) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality. For instance, most shallow discourse parsers use only the argument pairs to determine the discourse sense without c"
2021.sigdial-1.34,2021.naacl-main.126,0,0.0267258,"if they ever needed to reference back to it. To assess the inter-rater agreement, we determine Cohen’s κ value (Cohen, 1960). We randomly selected 25 samples from the PDTB and assigned each to the annotators. We obtained a Cohen’s κ of 0.88, which indicates almost perfect agreement. 4.2 Findings More context than the two arguments is needed to determine the correct discourse relation in many cases One potential way to mitigate the impact of domain shift on the performance of shallow discourse parsers is to incorporate context. With a few exceptions (Dai and Huang, 2018; Shi and Demberg, 2019; Zhang et al., 2021), existing models for shallow discourse parsing mostly do not Insights for model development While inspecting the results of the annotations, we found several helpful phenomena for developing future models, including observations regarding the role of context in shallow discourse parsing and errors that current RST parsers are making. 320 Figure 3: RST parse tree containing a segment of the relations that were examined in the qualitative analysis. The discourse sense labels on this tree that were examined in our analysis are marked red and green, where green is correct and red is incorrect use"
2021.sigdial-1.34,K15-2002,0,0.375662,"notations. Setup. To examine the impact of changing the linguistic distribution by introducing intra-sentential discourse relations, we run the model developed by Chen et al. (2019) using the same train-test split as the authors and training/testing on discourse senses which contain 10 or more examples. To get results for the PDTB-2, we train and test the model on the PDTB-2; to get results for the PDTB-3 and intrasentential relations in the PDTB-3, we train the model on the PDTB-3 and evaluate its performance on both of these sets. To parse plain-text documents for PDTB relations, we use the Wang and Lan (2015) parser as our end-to-end parser and the Chen et al. (2019) DiscoEval parser as our implicit sense classifier. The former is needed in order to parse unlabeled text, and the latter is a more accurate BERT-based implicit sense classifier (implicit sense classification is the most difficult PDTB parsing task). To evaluate these parsers, we look at quantitative as316 Base Large PDTB-2 PDTB-3 PDTB-3 Intra-Sent 0.4236 0.4358 0.4897 0.5094 0.6251 0.6251 Table 1: Accuracy of the BERT-based model described in Chen et al. (2019) on implicit relations in the PDTB. pects of their output (e.g. the distrib"
2021.sigdial-1.34,P17-2029,0,0.14605,"er to parse unlabeled text, and the latter is a more accurate BERT-based implicit sense classifier (implicit sense classification is the most difficult PDTB parsing task). To evaluate these parsers, we look at quantitative as316 Base Large PDTB-2 PDTB-3 PDTB-3 Intra-Sent 0.4236 0.4358 0.4897 0.5094 0.6251 0.6251 Table 1: Accuracy of the BERT-based model described in Chen et al. (2019) on implicit relations in the PDTB. pects of their output (e.g. the distributions) and qualitative aspects (manual annotation and inspection of parser output). For our RST experiments, we use the state-ofthe-art (Wang et al., 2017) parser. We evaluate the performance of this parser on the standard RST Discourse Treebank test set with a 90-10 split (347 training documents and 38 test documents). We also evaluate it on the gold labels from the GUM corpus (but trained on the RST). Because GUM is annotated with 20 different discourse relations which do not precisely map to the conventional 18 types used in the Wang et al. (2017) parser, we map the ones that don’t match these types or the more fine-grained relations in the following manner, following Braud et al. (2017): preparation to BACKGROUND, justify and motivation to E"
2021.sigdial-1.34,J05-2005,0,0.160347,"this is the case. We urge future researchers to consider developing contextaware models for shallow discourse parsing moving forward. We release our dataset to facilitate further discourse analysis under domain shift. 1 2 Related Work There are various frameworks for studying inferential links between discourse segments, from local shallow relations between discourse segments in PDTB (Rashmi Prasad, 2008) to hierarchical constituent structures in RST (Carlson et al., 2003) or discourse graphs in Segmented Discourse Representation Theory (SDRT) (Asher et al., 2003) and the Discourse Graphbank (Wolf and Gibson, 2005). Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) provides a hierarchical structure for analyzing text that describes relations between text spans known as elementary discourse units (EDUs). The RST Discourse Treebank (Carlson et al., 2003) contains 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) which have been split into elementary discourse units and annotated according to Rhetorical Structure Theory, where discourse relations are annotated in a tree structure across the whole document. A full list of these relations can be found in Carlson and Marc"
2021.sigdial-1.34,K15-2001,0,0.0187423,"sion forum (Chandrasekaran 314 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to. The necessity of discourse research has resulted in several shared tasks (Xue et al., 2015, 2016) and corpora development in multiple languages (Zeyrek and Webber, 2008; Meyer et al., 2011; Danlos et al., 2012; Zhou et al., 2014; Zeyrek et al., 2020). Yet shallow discourse parsing is a very difficult task; more than 10 years after the introduction of the Penn Discourse Treebank (Eleni Miltsakaki, 2004), performance for English implicit discourse relation recognition has gone from 40.2 F-1 (Lin et al., 2009) to 47.8 (Lee et al., 2020), less than 8 percentage points; a similar story could be said about the relation prediction performance of RST parsers. Such performance hinders the w"
2021.sigdial-1.34,K16-2001,0,0.0628298,"Missing"
2021.sigdial-1.34,I08-7009,0,0.0637215,"f the Special Interest Group on Discourse and Dialogue, pages 314–325 July 29–31, 2021. ©2021 Association for Computational Linguistics et al., 2017). However, it is still the case that few works have so far found discourse relations as key features (Zhong et al., 2020). We argue that one reason for this gap between theory and empirical evidence is the quality of the parsers exacerbated by the distributional shifts in the texts they need to apply to. The necessity of discourse research has resulted in several shared tasks (Xue et al., 2015, 2016) and corpora development in multiple languages (Zeyrek and Webber, 2008; Meyer et al., 2011; Danlos et al., 2012; Zhou et al., 2014; Zeyrek et al., 2020). Yet shallow discourse parsing is a very difficult task; more than 10 years after the introduction of the Penn Discourse Treebank (Eleni Miltsakaki, 2004), performance for English implicit discourse relation recognition has gone from 40.2 F-1 (Lin et al., 2009) to 47.8 (Lee et al., 2020), less than 8 percentage points; a similar story could be said about the relation prediction performance of RST parsers. Such performance hinders the wider application of parsers. If downstream tasks are to use predicted relation"
C14-1055,al-saif-markert-2010-leeds,0,0.0200724,"lied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugu"
C14-1055,P11-1061,0,0.0245884,"otated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and pr"
C14-1055,P97-1011,0,0.238511,"Missing"
C14-1055,D11-1027,0,0.0282846,"dies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we ca"
C14-1055,P09-1042,0,0.0306296,"i (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License detail"
C14-1055,W04-1101,0,0.0901396,"Missing"
C14-1055,P06-2057,0,0.0221681,"and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577 Proceedings of COLING 2014,"
C14-1055,P03-1056,0,0.0521517,"n; multiway classification labels one of the five possible classes: non-discourse use, TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION. This series of classifiers results in a system that can assign the same labels as the classifiers trained for English. To complete our presentation of the approach, we now turn to describe the features used to represent instances of potential discourse connectives. 5.2 Features The following set of features for each expression we need to classify are extracted solely from the Chinese part of the corpus4 . The syntactic parse trees were obtained automatically (Levy and Manning, 2003). Connective The connective expressions themselves. The vast majority of connectives (at least in English) are unambiguous, so using the identity of the connective is a hard-to-beat baseline for sense prediction (Pitler et al., 2008). Categories The syntactic category of the expression itself, as well as that of its parents, and its left and right siblings (if any). These features are adapted from Pitler and Nenkova (2009). Depth Depth of the expressions’s syntactic category in the parse tree for the sentence. POS bigram Bigram of part-of-speech tags of the entire sentence. Production pairs Pa"
C14-1055,li-etal-2010-enriching,0,0.0205623,"Missing"
C14-1055,D09-1036,0,0.435052,"labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been devel"
C14-1055,P11-1100,0,0.0134467,"urse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource"
C14-1055,P97-1013,0,0.177418,"divergence, and discourse connective ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations, using the parallel corpus instead of discourse annotation in the language of interest. Our resulting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification. 1 Introduction The analysis of the way spans of text semantically connect with each other to create a coherent text has a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al., 1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relation"
C14-1055,W12-0117,0,0.146065,"he use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another"
C14-1055,W11-2022,0,0.188077,"t available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 577–587, Dublin, Ireland, August 23-29 2014. The goal of our work is not only to measure the accura"
C14-1055,W09-3029,0,0.0231369,"2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009"
C14-1055,H05-1108,0,0.0116767,"2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577"
C14-1055,W12-1614,0,0.256277,"small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish ("
C14-1055,D08-1020,1,0.814766,"esource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-"
C14-1055,P09-2004,1,0.953807,"ause of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated w"
C14-1055,C08-2022,1,0.929939,"Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, s"
C14-1055,P09-1077,1,0.828179,"annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relatio"
C14-1055,prasad-etal-2008-penn,0,0.146439,"lting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification. 1 Introduction The analysis of the way spans of text semantically connect with each other to create a coherent text has a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al., 1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and c"
C14-1055,C10-2118,0,0.0144162,"were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza e"
C14-1055,P11-2052,0,0.0372337,"Missing"
C14-1055,D07-1010,0,0.0748826,"97; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the r"
C14-1055,P11-2111,0,0.0598275,"Missing"
C14-1055,H01-1035,0,0.0189898,"TB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International Licen"
C14-1055,I08-7009,0,0.0315716,"; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado"
C14-1055,P12-1008,0,0.28866,"sing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations,"
C14-1055,L08-1000,0,\N,Missing
C18-1248,S17-2094,0,0.0323187,"Missing"
C18-1248,W15-2911,1,0.883841,"Missing"
C18-1248,P15-1073,0,0.0257455,"igiosity or social status were found to be related with the rate of using vulgar words (McEnery, 2004). In studying the task of user trait prediction from social media texts, several vulgar words were shown to be characteristic of demographic traits such as political ideology (Preot¸iuc-Pietro et al., 2017). Explicitly modeling demographic (Lynn et al., 2017) and other social factors such as community membership (Yang and Eisenstein, 2017) have to date been successfully used for improving accuracy of several tasks including sentiment analysis (Volkova et al., 2013) or document classification (Hovy, 2015). 3 Data In exploring the role of vulgarity in written interactions, social media and Twitter in particular stands out as an ideal medium for collecting data. Twitter provides vast volumes of naturally occurring text, which are less affected by formal and curated text such as newswire, and can be studied together with socio-demographic attributes of their authors. Data Collection We collect a novel corpus of tweets, all of which contain vulgar words, and annotate this for sentiment, as to the best of our knowledge, no such corpus focusing on vulgar expressions exists in past research. This dat"
C18-1248,L16-1620,1,0.857365,"Missing"
C18-1248,D17-1119,0,0.0193237,"ccupation or social status (Jay and Janschewitz, 2008). The most studied social factor was gender, with several studies finding that males employ profanity much more often than females (Selnow, 1985; Wang et al., 2014). Other social factors such as age, religiosity or social status were found to be related with the rate of using vulgar words (McEnery, 2004). In studying the task of user trait prediction from social media texts, several vulgar words were shown to be characteristic of demographic traits such as political ideology (Preot¸iuc-Pietro et al., 2017). Explicitly modeling demographic (Lynn et al., 2017) and other social factors such as community membership (Yang and Eisenstein, 2017) have to date been successfully used for improving accuracy of several tasks including sentiment analysis (Volkova et al., 2013) or document classification (Hovy, 2015). 3 Data In exploring the role of vulgarity in written interactions, social media and Twitter in particular stands out as an ideal medium for collecting data. Twitter provides vast volumes of naturally occurring text, which are less affected by formal and curated text such as newswire, and can be studied together with socio-demographic attributes o"
C18-1248,S16-1001,0,0.0792973,"Missing"
C18-1248,P17-1068,1,0.895356,"Missing"
C18-1248,N10-1020,0,0.126227,"Missing"
C18-1248,S17-2088,0,0.0226343,"rds are obfuscated in mediums where messages are censored or for other goals, and an analysis of these techniques is presented in Laboreiro and Oliveira (2014). Our paper expands the scope of this research by studying several other demographic traits beyond gender, aiming to identify different uses of swear 2928 words through eliciting sentiment annotations and uses these to improve sentiment prediction methods. Sentiment analysis Sentiment analysis on Twitter is a very popular research topic, anchored in a wide range of industry applications and with several shared tasks over the past years (Rosenthal et al., 2017). Several studies have looked at the effects of intensifiers and words that switch their polarity in sentiment dictionaries (Flekova et al., 2015). Vulgar words are commonly part of popular sentiment and emotion analysis lexicons (Mohammad and Turney, 2013). Demographics & Vulgarity Use of vulgar or swear words is influenced by pragmatic or contextual factors such as the inter-personal relationship between the speakers or their social factors such as gender, occupation or social status (Jay and Janschewitz, 2008). The most studied social factor was gender, with several studies finding that mal"
C18-1248,D13-1187,0,0.139239,"oyed used to express emotion in language and can be used either to express negative sentiment or emotions or to intensify the sentiment present in the tweet (Wang, 2013). In one of the examples, ‘I am stupid as f*ck’ conveys more intense anger, while ‘I am stupid’ conveys a less emotional expression of irritation. Hence, understanding vulgar words is expected to have practical implications in sentiment analysis on social media. Furthermore, vulgar word usage is dependent on the user socio-cultural context with demographic and social information shown to improve sentiment analysis performance (Volkova et al., 2013; Yang and Eisenstein, 2017). Hence, this study aims to address the following research questions: • Is the expression of vulgarity and its function different across author demographic traits? * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2927 Proceedings of the 27th International Conference on Computational Linguistics, pages 2927–2938 Santa Fe, New Mexico, USA, August 20-26, 2018. Tweet Function I have made the awful decision to stay up all night. Why? Because I’m stupi"
C18-1248,W12-2103,0,0.0962017,"signal informality). Wang (2013) identifies four pragmatic roles of swear words: express emotion, express emphasis (either in presence of additional sentiment or not), to signal group identity and for aggression – in this latter case, swearing is a face-threatening act (Brown and Levinson, 1987). Overall, the function and effects of vulgar word usage are highly dependent on the context and the social factors (e.g., inter-personal relationships, setting), which we aim to further study and capitalize on in this study. Recently, there has been a surge in interest in studying hate speech online (Warner and Hirschberg, 2012). A better understanding of vulgar words and the context they are used in can aid models that aim to automatically detect and categorize hate speech, which is harder to distinguish when profanity is employed (Malmasi and Zampieri, 2018). Vulgar words have been quantitatively studied in social media and online communities. Liu et al. (2010) identified and filtered vulgar content distributed via peer-to-peer applications. Wang et al. (2014) presents a quantitative analysis of the frequency of curse word usage on Twitter, their variation with time and location, their gender usage and use in conve"
D15-1148,W14-3338,0,0.0498256,"Missing"
D15-1148,C96-2183,0,0.0999206,"having different rankers for expressing the content in one or multiple sentences. In that case the ranker will need to capture only sentence-level information and the discourse-level decision to use multiple sentences will be treated separately. Text simplification. “Text simplification, defined narrowly, is the process of reducing the linguistic complexity of a text, while still retaining the original information content and meaning” (Siddharthan, 2014). An important aspect of simplification is syntactic transformation in which sentences deemed difficult are re-written as multiple sentences (Chandrasekar et al., 1996; Alu´ısio et al., 2008). Our task may be viewed as identifying sentences in one language that will require simplification when translated, for the benefit of the speakers of the target language. In rule-based simplification systems, splitting is performed al1272 ways when a given syntactic construction such as relative clause, apposition or discourse connective are detected (Chandrasekar et al., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Cost"
D15-1148,W09-0436,0,0.0614197,"Missing"
D15-1148,W09-2307,0,0.020688,"gth can be an indication of too much content that needs to be 1276 VP VP PU VP Figure 2: Multiple VP structures repackaged into multiple sentences. Therefore as our baseline we train a decision tree using the number of words7 in a Chinese sentence. Sentence structure cues. We collect potential signals for structural complexity: punctuation, conjunctions, prepositional phrases and relative clauses. As features we count the number of commas, conjunction, preposition and postposition part-of-speech tags. In Chinese “DE” often marks prepositional phrases or relative clauses among other functions (Chang et al., 2009a). Here we include a simple count the number of “DEG” tags in the sentence. Dependencies. Dependency grammar captures both syntactic and semantic relationship between words and are shown to improve reordering in MT (Chang et al., 2009b). To account for such relational information we include two feature classes: the percentage of each dependency type and the typed dependency pairs themselves. For the latter we use the universal part-of-speech tags (Petrov et al., 2012) for each word rather than the word itself to avoid too detailed and sparse representations. For example, the relation dobj(处理/"
D15-1148,W11-1601,0,0.0274352,"1996; Alu´ısio et al., 2008). Our task may be viewed as identifying sentences in one language that will require simplification when translated, for the benefit of the speakers of the target language. In rule-based simplification systems, splitting is performed al1272 ways when a given syntactic construction such as relative clause, apposition or discourse connective are detected (Chandrasekar et al., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in mac"
D15-1148,E14-1063,0,0.0322252,"Missing"
D15-1148,W04-1101,0,0.0357726,"s by examining multiple reference translations for each Chinese sentence. We define heavy sentences based on agreement of translator choices and reader preferences. Commas in Chinese. Often a comma in a sentence can be felicitously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4"
D15-1148,P13-1151,0,0.0257441,"Missing"
D15-1148,P03-1056,0,0.02862,"Missing"
D15-1148,I05-2002,0,0.0419792,"tiple reference translations for each Chinese sentence. We define heavy sentences based on agreement of translator choices and reader preferences. Commas in Chinese. Often a comma in a sentence can be felicitously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4 (Huang et al., 200"
D15-1148,P14-2047,1,0.837372,"tences in Hindi that need simplification prior to translation. In each of these approaches, the identified sentences are segmented into smaller units. Similar to work in text simplification, the simplification rules are applied to all sentences meeting certain criteria, normally to all sentences longer than a predefined threshold or where certain conjunctions or coordinations are present. In contrast, the model we propose here can be used to predict when segmentation is at all necessary. Our approach to the problem is more compatible with the empirical evidence we presented in our prior work (Li et al., 2014) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality. Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences. This prior work was carried over a dataset containing a single reference translation for each Chinese sentence. In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence. We define heavy sent"
D15-1148,P14-5010,0,0.0062043,"Missing"
D15-1148,W14-5603,0,0.0931268,"al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules involving similar factors to distinguish sentences in Hindi that need simplification prior to translation. In each of these approaches, the identified sentences are segmented into smaller units. Similar to work in text simplification, the simplification rules are applied to all sentences meeting certain criteria, normally to all sentences longer than a predefined threshold or where certain conjunctions or coordinations are present. In contrast, the model we propose here can be used to predict when segmentation is at all necessary. Our approach to the problem is more compatible with"
D15-1148,P14-1041,0,0.0982972,"De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules involving similar factors to dis"
D15-1148,P05-1070,0,0.0108427,"anslation, text simplification and Chinese language processing but it is usually addressed in an implicit or application specific way. In contrast, we focus on identifying heavy sentences as a standalone task, providing a unifying view of the seemingly disparate strands of prior work. We now overview the literature which motivated our work. Sentence planning. In text generation, a sentence planner produces linguistic realizations of a list of propositions (Rambow and Korelsky, 1992). One subtask is to decide whether to package the same content into one or more sentences. In the example below (Pan and Shaw, 2005), the multisentence expression B is much easier to process: [A] This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk and in the Byram Hills school district. [B] This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk with 2 acres of land. The asking price is 1 million dollar and the annual taxes are 8000 dollars. The house is located in the Byram Hills School District. Identifying sentence [A] as heavy would be useful in selecting the best realization. A crucial difference between"
D15-1148,petrov-etal-2012-universal,0,0.015959,"d postposition part-of-speech tags. In Chinese “DE” often marks prepositional phrases or relative clauses among other functions (Chang et al., 2009a). Here we include a simple count the number of “DEG” tags in the sentence. Dependencies. Dependency grammar captures both syntactic and semantic relationship between words and are shown to improve reordering in MT (Chang et al., 2009b). To account for such relational information we include two feature classes: the percentage of each dependency type and the typed dependency pairs themselves. For the latter we use the universal part-of-speech tags (Petrov et al., 2012) for each word rather than the word itself to avoid too detailed and sparse representations. For example, the relation dobj(处理/handle, 事 情/matter) becomes feature dobj(verb, noun). Furthermore, we use dependency trees to extract four features for potentially complex constructions. First, we indicate the presence of noun phrases with heavy modifiers on the left. These are frequently used in Chinese and would require a relative clause or an additional sentence in English. Specifically we record the maximum number of dependents for the nouns in the sentence. The second type of construction is the"
D15-1148,A92-1006,0,0.110643,"). 2 Related work The need for identifying content-heavy sentences arises in many specialized domains, including dialog systems, machine translation, text simplification and Chinese language processing but it is usually addressed in an implicit or application specific way. In contrast, we focus on identifying heavy sentences as a standalone task, providing a unifying view of the seemingly disparate strands of prior work. We now overview the literature which motivated our work. Sentence planning. In text generation, a sentence planner produces linguistic realizations of a list of propositions (Rambow and Korelsky, 1992). One subtask is to decide whether to package the same content into one or more sentences. In the example below (Pan and Shaw, 2005), the multisentence expression B is much easier to process: [A] This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk and in the Byram Hills school district. [B] This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk with 2 acres of land. The asking price is 1 million dollar and the annual taxes are 8000 dollars. The house is located in the Byram Hill"
D15-1148,P04-1011,0,0.0111649,"c information about the type of propositions the system needs to convey, while in our task we have access only to Chinese text. In some dialog systems, content selection is treated as an optimization problem, balancing the placement of full-stops and the insertion or deletion of propositions with the similarity of the resulting output and an existing corpus of acceptable productions (Pan and Shaw, 2005). Others formulate the problem as a supervised ranking task, in which different possible content realizations are generated, including variation in the number of sentences (Walker et al., 2001; Stent et al., 2004). With the introduction of the concept of content-heavy sentences, we can envision dialog systems addressing the sentence realization task in two steps, first predicting if the semantic content will require multiple sentences, then having different rankers for expressing the content in one or multiple sentences. In that case the ranker will need to capture only sentence-level information and the discourse-level decision to use multiple sentences will be treated separately. Text simplification. “Text simplification, defined narrowly, is the process of reducing the linguistic complexity of a tex"
D15-1148,P12-1083,0,0.0144124,"citously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4 (Huang et al., 2002; Huang et al., 2003; Ma, 2004; Ma, 2006), and the Chinese Treebank (Xue et al., 2005). In OpenMT and MTC, multiple reference translations in English are available for each Chinese segment (sentence). To stu"
D15-1148,C10-1152,0,0.0832827,"Chandrasekar et al., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et"
D15-1148,N01-1003,0,0.0504752,"ccess to rich semantic information about the type of propositions the system needs to convey, while in our task we have access only to Chinese text. In some dialog systems, content selection is treated as an optimization problem, balancing the placement of full-stops and the insertion or deletion of propositions with the similarity of the resulting output and an existing corpus of acceptable productions (Pan and Shaw, 2005). Others formulate the problem as a supervised ranking task, in which different possible content realizations are generated, including variation in the number of sentences (Walker et al., 2001; Stent et al., 2004). With the introduction of the concept of content-heavy sentences, we can envision dialog systems addressing the sentence realization task in two steps, first predicting if the semantic content will require multiple sentences, then having different rankers for expressing the content in one or multiple sentences. In that case the ranker will need to capture only sentence-level information and the discourse-level decision to use multiple sentences will be treated separately. Text simplification. “Text simplification, defined narrowly, is the process of reducing the linguisti"
D15-1148,D11-1038,0,0.120773,"., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules invo"
D15-1148,P12-1107,0,0.0937459,"Missing"
D15-1148,2005.eamt-1.37,0,0.0430194,"parately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules involving similar factors to distinguish sentences in Hindi that need simplification prior to translation. In each of these approaches, the identified sentences are segmented into smaller units. Similar to work in text simplification, the simplification rules are applied to all sentences meeting certain criteria, normally to all sentences longer than a predefined threshold or where certain conjunctions or coordinations are present. In contrast, the model we propose here can be used to predict when segmentation is at all ne"
D15-1148,P11-2111,0,0.25413,"sentence can be felicitously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4 (Huang et al., 2002; Huang et al., 2003; Ma, 2004; Ma, 2006), and the Chinese Treebank (Xue et al., 2005). In OpenMT and MTC, multiple reference translations in English are available for each Chinese segme"
D18-1471,J08-4004,0,0.30865,"Missing"
D18-1471,J92-4003,0,0.675036,"s feature group, we utilize the opinion lexicon introduced in Hu and Liu (2004). Part of Speech Context –We encode the part of speech of the target word, the previous word and the next word as one-hot vectors as we expect syntactic information to be an indicator of different functions in context. We extract parts of speech using the Twitter version of the Stanford POS tagger which demonstrated good results on tagging tweets and uses the finer grained Penn Treebank tagset (Derczynski et al., 2013). Brown Clusters –Finally, we include two one-hot feature groups which indicate the Brown Cluster (Brown et al., 1992) membership of word immediately before and immediately after the vulgar term. 5 In preliminary experiments, we attempted to utilize a BiLSTM to encode tweet context, but it did significantly worse than the logistic regression model, possibly due to many parameters and classes compared to the size of the training data. 4410 Method Most Frequent Class All Features – Intention Distribution – Tweet Content – Sentiment Content – Part of Speech Context – Brown Clusters Precision 5.05 68.8 58.3 67.9 68.6 67.8 68.6 Recall 16.6 66.4 53.8 64.0 66.3 64.6 64.9 F1 7.76 67.4 55.3 65.6 67.3 65.9 66.3 Table 7"
D18-1471,C18-1248,1,0.761469,"Missing"
D18-1471,R13-1026,0,0.0167655,"eatures which represent the number of positive and negative valence words in the tweet, normalized by tweet length. For this feature group, we utilize the opinion lexicon introduced in Hu and Liu (2004). Part of Speech Context –We encode the part of speech of the target word, the previous word and the next word as one-hot vectors as we expect syntactic information to be an indicator of different functions in context. We extract parts of speech using the Twitter version of the Stanford POS tagger which demonstrated good results on tagging tweets and uses the finer grained Penn Treebank tagset (Derczynski et al., 2013). Brown Clusters –Finally, we include two one-hot feature groups which indicate the Brown Cluster (Brown et al., 1992) membership of word immediately before and immediately after the vulgar term. 5 In preliminary experiments, we attempted to utilize a BiLSTM to encode tweet context, but it did significantly worse than the logistic regression model, possibly due to many parameters and classes compared to the size of the training data. 4410 Method Most Frequent Class All Features – Intention Distribution – Tweet Content – Sentiment Content – Part of Speech Context – Brown Clusters Precision 5.05"
D18-1471,D14-1162,0,0.0801521,"l and syntactic context surrounding the word and general usage of the word in training data. 5.2 Features We use the following feature types in our experiments: Intention Distribution –We include six features encoding the distribution over intentional classes of the target word in training data, as some words use only several functions and some more predominantly than others. Tweet Content –We derive a tweet-level representation of the entire content of the tweet by averaging vector representations of its constituent words. We utilize 200-dimensional GloVe embeddings pre-trained on 2B tweets (Pennington et al., 2014). Sentiment Content –We include two features which represent the number of positive and negative valence words in the tweet, normalized by tweet length. For this feature group, we utilize the opinion lexicon introduced in Hu and Liu (2004). Part of Speech Context –We encode the part of speech of the target word, the previous word and the next word as one-hot vectors as we expect syntactic information to be an indicator of different functions in context. We extract parts of speech using the Twitter version of the Stanford POS tagger which demonstrated good results on tagging tweets and uses the"
D18-1471,P10-1040,0,0.028527,"– Intention Distribution – Tweet Content – Sentiment Content – Part of Speech Context – Brown Clusters Precision 5.05 68.8 58.3 67.9 68.6 67.8 68.6 Recall 16.6 66.4 53.8 64.0 66.3 64.6 64.9 F1 7.76 67.4 55.3 65.6 67.3 65.9 66.3 Table 7: Performance statistics for our baseline, predictive model with all features and with ablating each feature group. Precision, Recall and F1 score are macro-averaged across the six classes. Brown Clusters are obtained by hierarchical clustering tokens based on contexts in which they immediately co-occur. We use the precomputed cluster representations as seen in Turian et al. (2010). We also experimented with Twitter-specific clusters (Owoputi et al., 2012), but found they did not perform as well on our development set. Additionally, we experimented with a personal pronoun indicator feature in a three word window around the target, a one-hot lexical feature encoding the target vulgar item, and NRC emotion scores (Mohammad and Turney, 2013), but found there to be no improvement in performance as a result. We did not experiment with using the demographic variables as features as these are generally unavailable for use in predictive systems. 5.3 Experimental Results We spli"
D18-1471,W12-2103,0,0.0575172,"djective). The least predictable function is signaling group identity. We observed that this function is usually used as part of larger conversational context and often relies on a shared social context. 6 Hate Speech Prediction Finally, we aim to show that modeling the function of vulgar words explicitly has practical implications by using this in a downstream application. 6.1 Task Automatic hate-speech detection on social media is the task defined as generally identifying abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation (Warner and Hirschberg, 2012) with a clear intention to incite harm, or to promote hatred (Zhang and Luo, 2018). Several data sets and approaches to automatic hate speech detection have been recently proposed (Djuric et al., 2015; Burnap and Williams, 2015; Waseem and Hovy, 2016; Nobata et al., 2016; Davidson et al., 2017). The task of predicting hate-speech is challenging for natural language processing using lexical information as it aims to predict the intention of 4411 the message and several words used in conveying hate speech can have other common uses (Davidson et al., 2017; Malmasi and Zampieri, 2018). Hate speech"
D18-1471,N16-2013,0,0.255446,"modeling the function of vulgar words explicitly has practical implications by using this in a downstream application. 6.1 Task Automatic hate-speech detection on social media is the task defined as generally identifying abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation (Warner and Hirschberg, 2012) with a clear intention to incite harm, or to promote hatred (Zhang and Luo, 2018). Several data sets and approaches to automatic hate speech detection have been recently proposed (Djuric et al., 2015; Burnap and Williams, 2015; Waseem and Hovy, 2016; Nobata et al., 2016; Davidson et al., 2017). The task of predicting hate-speech is challenging for natural language processing using lexical information as it aims to predict the intention of 4411 the message and several words used in conveying hate speech can have other common uses (Davidson et al., 2017; Malmasi and Zampieri, 2018). Hate speech is very often confused with offensive language, as highlighted in the error analyses of past hate speech detection papers (Davidson et al., 2017). Quantitative analysis of the machine learning models suggest that obscene words are very informative f"
D18-1471,P17-1068,1,0.876818,"Missing"
D19-1478,D14-1181,0,0.0652559,"domain samples that are more like the source domain samples. Hence, we sort the target domain mini-batches by year; the learning task becomes progressively harder as opposed to confusing the models during the early stages of training. 4 Model In this section, we introduce a new convolutional neural network (CNN) as the plug-in model for our unsupervised domain adaptation framework. We motivate the use of CNNs (§4.1), formalize the model input (§4.2), and introduce several novel components for our task (§4.3). 4.1 Motivation CNNs have emerged as strong baselines for text classification in NLP (Kim, 2014). CNNs are desirable candidates for our framework as they exhibit a high degree of parameter sharing, significantly reducing the number of parameters to train. In addition, they can be designed to solely optimize the log-likelihood of the training data. Experimentally, we find that models that optimize other distributions (e.g., attention distributions in Transformers (Vaswani et al., 2017) or Hierarchical Attention Networks (Yang et al., 2016)) do not work well with this framework. 4.2 Model Input Given a discrete input x = [w1 , · · · , wn ] and vocabulary V , an embedding matrix E ∈ R|V |×d"
D19-1478,N16-1174,0,0.0349434,"input (§4.2), and introduce several novel components for our task (§4.3). 4.1 Motivation CNNs have emerged as strong baselines for text classification in NLP (Kim, 2014). CNNs are desirable candidates for our framework as they exhibit a high degree of parameter sharing, significantly reducing the number of parameters to train. In addition, they can be designed to solely optimize the log-likelihood of the training data. Experimentally, we find that models that optimize other distributions (e.g., attention distributions in Transformers (Vaswani et al., 2017) or Hierarchical Attention Networks (Yang et al., 2016)) do not work well with this framework. 4.2 Model Input Given a discrete input x = [w1 , · · · , wn ] and vocabulary V , an embedding matrix E ∈ R|V |×d replaces each word wi with its respective ddimensional embedding. The resulting embeddings are stacked row-wise to obtain an input matrix X ∈ Rn×d . Following the notion of input perturbation used in consistency regularization algorithms (Athiwaratkun et al., 2019), we design several methods to inject noise into the input layer. Each input is perturbed with additive, isotropic ˜ = X + N (0, I). Then, we Gaussian noise: X apply dropout on the p"
D19-1478,Q17-1036,0,0.395014,"ces, respectively. We have access to labeled samples (i) (i) {xL , yL }N i=1 from a source domain DS and un(i) labeled samples {xU }M i=1 from a target domain DT . The goal of unsupervised domain adaptation is to learn a function f : X → Y that maximizes the likelihood of the target domain samples by only leveraging supervision from the source domain samples. We also assume the existence of a small amount of labeled target domain samples in order to create a development set, following existing work in unsupervised domain adaptation (Glorot et al., 2011; Chen et al., 2012; French et al., 2018; Zhang et al., 2017). 3.2 Self-Ensembling Our unsupervised domain adaptation framework builds on top of self-ensembling (Laine and Aila, 2017), a semi-supervised learning algorithm based on consistency regularization, whereby models are trained to be robust against injected noise (Athiwaratkun et al., 2019). Self-ensembling is an interplay between two neural networks: a student network f (x; θ) and a teacher network f (x; φ). The inputs to both networks are perturbed separately, and the objective is to measure the consistency of the student network’s predictions against the teacher’s. Both networks share the same"
D19-1478,D14-1162,0,0.0821223,"riculum AE (ours) + curriculum 55.7 57.4 68.2 64.0 66.4 75.1 77.4 46.2 49.7 65.8 59.5 62.3 74.5 77.1 28.8 41.0 50.8 42.7 44.4 46.1 48.2 70.0 63.7 36.3 64.1 71.7 75.3 83.5 39.6 48.1 42.2 51.0 54.5 57.2 61.1 In-Domain 81.7 81.6 86.5 83.5 84.8 Table 2: Framework results for the binary label task (left) and multi-label task (right). For the binary task, we show micro- and macro-averaged F1 scores. For the multi-label task, we show macro-averaged precision, recall, and F1 scores. of 200 and minimum word count from [1, 2, 3] to build the vocabulary. The embedding matrix uses 300-D GloVe embeddings (Pennington et al., 2014) with a dropout rate of 0.5 (Srivastava et al., 2014). We history-pad our input with a zero vector, the state connections are obtained using average pooling, and the time embedding has a dimensionality of 10. The model is optimized with Adam (Kingma and Ba, 2015), learning rate from [10−4 , 5 · 10−5 , 10−5 ], and mini-batch size from [16, 32]. Hyperparameters were discovered using a grid search on the held-out development set. 6.2 Framework Results Using our best model, we benchmark our unsupervised domain adaptation framework against established methods: (1) Marginalized Stacked Denoising Aut"
D19-1478,I17-2072,0,\N,Missing
D19-5717,W16-3009,0,0.0535187,"Missing"
D19-5717,W16-3001,0,0.0492311,"Missing"
D19-5717,W13-2001,0,0.0876743,"Missing"
D19-5717,D11-1142,0,0.0486265,"tation construction, such as word embedding, distance embedding and entity type embedding; and CNN-LSTM model. The F1 value of our participated task on the test data set of all types was 0.342. We achieved the second highest in the task. The results showed that our proposed method performed effectively in the binary relation extraction. 1 Introduction The goal of Information Extraction (IE) (Finkel et al., 2005) is to transform textual information into structured information, and to focus on quickly locating and finding useful information in large amounts of data. Information Extraction (IE) (Fader et al., 2011) is also capable of mining useful data and hiding knowledge from a large number of corpus texts, which has led to some new research methods in many disciplines. For example, with the growing demand for key issues related to life and biology, many biological problems have fallen into the bottleneck due to inadequate methods. Biological information extraction (BioIE) emerges in time and attracts more and more researchers to solve problems. For instance, in the identification of named entities, the classification of relationships between proteins and the extraction of links between drugs. In addi"
D19-5717,P05-1045,0,0.0516624,"rt term memory networks (LSTM). The full text information and context information were collected using the advantages of CNN and LSTM. The model consisted of two main modules: distributed semantic representation construction, such as word embedding, distance embedding and entity type embedding; and CNN-LSTM model. The F1 value of our participated task on the test data set of all types was 0.342. We achieved the second highest in the task. The results showed that our proposed method performed effectively in the binary relation extraction. 1 Introduction The goal of Information Extraction (IE) (Finkel et al., 2005) is to transform textual information into structured information, and to focus on quickly locating and finding useful information in large amounts of data. Information Extraction (IE) (Fader et al., 2011) is also capable of mining useful data and hiding knowledge from a large number of corpus texts, which has led to some new research methods in many disciplines. For example, with the growing demand for key issues related to life and biology, many biological problems have fallen into the bottleneck due to inadequate methods. Biological information extraction (BioIE) emerges in time and attracts"
D19-5717,W16-3013,0,0.0256573,"et al., 2016) aims to promote complex event extraction on regulations in plants from scientific articles. It focuses on events describing genetic and molecular mechanisms involved in seed development of the model plant, Arabidopsis thaliana. It involves n-ary and binary relation extraction. Meanwhile, the SeeDev task was proposed for the first time at BioNLP Shared Task 2016(N´edellec et al., 2016) (Mehryary et al., 2016). This 2019 edition is a rerun of the task, with an evaluation methodology more focused on the biological contribution. Many teams participated in the BioNLP 2016 Shared Task(He et al., 2016). For example, VERSE uses a support vector machine (SVM) and k-fold cross-validation to identify the best parameters.(Lever and Jones, 2016) DUTIR uses a deep learning method that utilizes a convolutional neuWe participated in the BioNLP 2019 Open Shared Tasks: binary relation extraction of SeeDev task. The model was constructed using convolutional neural networks (CNN) and long short term memory networks (LSTM). The full text information and context information were collected using the advantages of CNN and LSTM. The model consisted of two main modules: distributed semantic representation con"
D19-5717,P82-1020,0,0.778993,"Missing"
D19-5717,W16-3012,0,0.0311938,") emerges in time and attracts more and more researchers to solve problems. For instance, in the identification of named entities, the classification of relationships between proteins and the extraction of links between drugs. In addition, information extraction in the field of biology, especially event extraction, has entered people’s views. This will be a far-reaching task and a major biological 110 Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, pages 110–114 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics the preprocessed data. ral network(Li et al., 2016). Motivated by the previous study, based on CNN, we have integrated LSTM(Hochreiter and Schmidhuber, 1997) to solve the defect that convolutional neural networks can not obtain context information. After improving the method, we got good results. The rest of our paper is structured as follows. Section 2 introduces models. Section 3 describes results and discussion. Conclusions are described in Section 4. 2 2.2 We use the context of two entities to predict the type of relationship. In our task, the context is represented by words between two entities in a sentence. Then, by analyzing the data,"
D19-5717,P14-5010,0,0.00406116,", < W T >type(E1 ) ] LTW d (S) = [< W d >d(E1 ,E1 ) , ..., < W d >d(E2 ,E1 ) , 0, 0] where S stands for the sentences. E1 and E2 are the type 1 and type 2 respectively. W1 stands for the first word. W is the word embedding table. W T is type embedding table and W d stands for the distance embedding table. LTW (S) is the representation of word embedding. LTW ,W T (S) is the representation type embedding. LTW d (S) is the distance embedding. In the distance embedding, zero vector(0) is used to pad the sentence. Data preprocessing When doing data preprocessing, first we use the Stanford CoreNLP(Manning et al., 2014) tool to process the task’s data. The text is divided into sentences and tokenized. Parts-of-speech and lemmas are identified and a dependency parse is generated for each sentence. Then, we further process 1 111 https://github.com/cambridgeltl/BioNLP-2016 Figure 1: Our proposed CNN-LSTM based model model CNN CNN-LSTM dropout 0.5 0.5 batch 64 64 epoch 120 120 F1 0.52 0.60 Table 1: The F1 score of CNN and CNN-LSTM on the dev data set for SeeDev-binary task 2.3 Model training F1 0.5 0.25 0.34 0.23 0.35 0.22 Recall 0.6 0.19 0.47 0.24 0.57 0.16 Precision 0.43 0.35 0.27 0.22 0.25 0.33 Table 2: The F"
K19-1064,D17-1134,0,0.0144105,"the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptation with adversarial networks has become popular in recent years; this type of approach generates a representation for the target domain with the"
K19-1064,P13-2013,0,0.0222385,"n adversarial way, to minimize the domain discrepancy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Un"
K19-1064,P13-1047,0,0.0230882,"Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptation with adversarial networks has become popular in recent years; this type of approach generates a represent"
K19-1064,C14-1160,0,0.0209656,"on 3.1), then train the target encoder Mt (initialized with Ms ) and discriminator D in an adversarial way, to minimize the domain discrepancy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrar"
K19-1064,D16-1020,0,0.0145264,"cy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptation with adversarial net"
K19-1064,P02-1047,0,0.0435336,"e-train a source encoder Ms and source classifier C (Section 3.1), then train the target encoder Mt (initialized with Ms ) and discriminator D in an adversarial way, to minimize the domain discrepancy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpo"
K19-1064,Q18-1039,0,0.0651019,"e domain training and representation mapping between source and target. We improve this framework by proposing a reconstruction component to preserve the discriminability of target features, and incorporating techniques for stabler training on textual data. Experimental results show that even with a simple architecture for representation learning, our unsupervised domain adaptation system outperforms prior work by 1.4-2.3 macro F1, with substantial improvements on Temporal and Contingency relations. It is also superior to DANN (Ganin et al., 2016), an adversarial framework widely used in NLP (Chen et al., 2018; Gui et al., 2017; Zhang et al., 2017; Fu et al., 2017; Joty et al., 2017; Xu and Yang, 2017), by 5.7 macro F1. Finally, we extend the system to incorporate in-domain supervision as it is sometimes feasible resource-wise to build a seed corpus that may not be large enough to train a fully supervised system. We simulate this scenario by enabling the system to jointly optimize over a varying number of labeled examples of implicit relations. Our system consistently outperforms two strong baselines. Implicit discourse relations are not only more challenging to classify, but also to annotate, than"
K19-1064,miltsakaki-etal-2004-penn,0,0.0554559,"em to take advantage of labeled data if some are available. 1 Introduction Discourse relations capture the relationship between units of text—e.g., sentences and clauses— and are an important aspect of text coherence. While some relations are expressed explicitly with a discourse connective (e.g., “for example”, “however”), relations are equally often expressed implicitly without an explicit connective (Prasad et al., 2008); in these cases, the relation needs to be inferred. Resources for implicit discourse relations are scarce compared to the explicit ones, since they are harder to annotate (Miltsakaki et al., 2004). For example, among corpora annotated with discourse relations such as Arabic (Al-Saif and Markert, 2010), Czech (Pol´akov´a et al., 2013), Chinese (Zhou and Xue, 2015), English (Prasad et al., 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2013), only the Chinese, English and Hindi corpora include implicit discourse relations (Prasad et al., 2014). In this low-resource scenario, Ji et al. (2015) proposed training with explicit relations via unsupervised domain adaptation, viewing explicit relations as a source domain with labeled training data, and implicit relations 2 Related"
K19-1064,D17-1070,0,0.0305454,"50, producing a representation with dimension 200 for each example. The discriminator D consists of 2 hidden layers with 200 and 200 neurons on each layer. The reconstruction mapping Mr contains 3 hidden layers with 120, 15 and 120 neurons on each layer. The label smoothing parameter  is 0.1. We use Adam (Kingma and Ba, 2015) with learning rate 1e-4 for the base encoder and classifier, and 1e-6 for the adversarial domain adapter. We use SGD optimizer with learning rate 1e-2 for the reconstruction component. All the models were implemented using PyTorch (Paszke et al., 2017) and adapted from Conneau et al. (2017). Unsupervised Domain Adaptation Experiments We first evaluate our model for the default task: unsupervised domain adaptation from explicit discourse relations to implicit discourse relations. 4.1 Relation Table 1: The number of examples of the four top level discourse relations in PDTB 2.0. Mr according to Eq.(3), Eq.(4), Eq.(7). Finally, we test the model using the target encoder Mt and classifier C. The steps (lines 4 and 5) in the Repeat loop execute once in one iteration, and we optimized the model in two-step units. 4 Implicit Settings Data We train and test our model on the PDTB, follow"
K19-1064,N18-1013,0,0.0394929,"Missing"
K19-1064,I17-2072,0,0.0456774,"ce and target. We improve this framework by proposing a reconstruction component to preserve the discriminability of target features, and incorporating techniques for stabler training on textual data. Experimental results show that even with a simple architecture for representation learning, our unsupervised domain adaptation system outperforms prior work by 1.4-2.3 macro F1, with substantial improvements on Temporal and Contingency relations. It is also superior to DANN (Ganin et al., 2016), an adversarial framework widely used in NLP (Chen et al., 2018; Gui et al., 2017; Zhang et al., 2017; Fu et al., 2017; Joty et al., 2017; Xu and Yang, 2017), by 5.7 macro F1. Finally, we extend the system to incorporate in-domain supervision as it is sometimes feasible resource-wise to build a seed corpus that may not be large enough to train a fully supervised system. We simulate this scenario by enabling the system to jointly optimize over a varying number of labeled examples of implicit relations. Our system consistently outperforms two strong baselines. Implicit discourse relations are not only more challenging to classify, but also to annotate, than their explicit counterparts. We tackle situations wher"
K19-1064,D14-1162,0,0.0825663,"velopment set1 . Early stopping happened after around 5 epochs (with lines 4 and 5 executed once in each epoch). Model configuration The hyperparameters, as well as the number of fully connected layers for the classifier C, discriminator D and the reconstruction mapping Mr , are all set according to the performance on the development sets. We first set the hyper-parameters of the encoders Ms , Mt and classifier C based on development performance during the pre-training stage. Then, we set the hyper-parameters of D and Mr based on development performance of the adaptation stage. We use GloVe (Pennington et al., 2014) for word embeddings with dimension 300. The maximum argument length is set to 80. The encoder contains an inner-attention BiLSTM with dimension 50, producing a representation with dimension 200 for each example. The discriminator D consists of 2 hidden layers with 200 and 200 neurons on each layer. The reconstruction mapping Mr contains 3 hidden layers with 120, 15 and 120 neurons on each layer. The label smoothing parameter  is 0.1. We use Adam (Kingma and Ba, 2015) with learning rate 1e-4 for the base encoder and classifier, and 1e-6 for the adversarial domain adapter. We use SGD optimizer"
K19-1064,I13-1011,0,0.0437105,"Missing"
K19-1064,prasad-etal-2008-penn,0,0.286566,"ation Classification Hsin-Ping Huang Department of Computer Science The University of Texas at Austin hsinping@cs.utexas.edu Junyi Jessy Li Department of Linguistics The University of Texas at Austin jessy@austin.utexas.edu Abstract as a target domain with no labeled data. The domain gap between explicit and implicit relations is acknowledged by prior observations that the two types of discourse relations are linguistically dissimilar (Sporleder and Lascarides, 2008; Rutherford and Xue, 2015). We present a new system for the unsupervised domain adaptation setup on the Penn Discourse Treebank (Prasad et al., 2008). Our system is based on Adversarial Discriminative Domain Adaptation (Tzeng et al., 2017), which decouples source domain training and representation mapping between source and target. We improve this framework by proposing a reconstruction component to preserve the discriminability of target features, and incorporating techniques for stabler training on textual data. Experimental results show that even with a simple architecture for representation learning, our unsupervised domain adaptation system outperforms prior work by 1.4-2.3 macro F1, with substantial improvements on Temporal and Conti"
K19-1064,J14-4007,0,0.0257538,"citly without an explicit connective (Prasad et al., 2008); in these cases, the relation needs to be inferred. Resources for implicit discourse relations are scarce compared to the explicit ones, since they are harder to annotate (Miltsakaki et al., 2004). For example, among corpora annotated with discourse relations such as Arabic (Al-Saif and Markert, 2010), Czech (Pol´akov´a et al., 2013), Chinese (Zhou and Xue, 2015), English (Prasad et al., 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2013), only the Chinese, English and Hindi corpora include implicit discourse relations (Prasad et al., 2014). In this low-resource scenario, Ji et al. (2015) proposed training with explicit relations via unsupervised domain adaptation, viewing explicit relations as a source domain with labeled training data, and implicit relations 2 Related Work Sporleder and Lascarides (2008) and Rutherford 686 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 686–695 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics They train an adversarial model using implicit discourse relations with and without expert-inserted connectives. Note again that"
K19-1064,D15-1264,0,0.34663,"omain supervision as it is sometimes feasible resource-wise to build a seed corpus that may not be large enough to train a fully supervised system. We simulate this scenario by enabling the system to jointly optimize over a varying number of labeled examples of implicit relations. Our system consistently outperforms two strong baselines. Implicit discourse relations are not only more challenging to classify, but also to annotate, than their explicit counterparts. We tackle situations where training data for implicit relations are lacking, and exploit domain adaptation from explicit relations (Ji et al., 2015). We present an unsupervised adversarial domain adaptive network equipped with a reconstruction component. Our system outperforms prior works and other adversarial benchmarks for unsupervised domain adaptation. Additionally, we extend our system to take advantage of labeled data if some are available. 1 Introduction Discourse relations capture the relationship between units of text—e.g., sentences and clauses— and are an important aspect of text coherence. While some relations are expressed explicitly with a discourse connective (e.g., “for example”, “however”), relations are equally often exp"
K19-1064,K17-1024,0,0.0602499,"Missing"
K19-1064,N15-1081,0,0.0174842,"imize the domain discrepancy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptati"
K19-1064,W10-4326,0,0.0312174,"d discriminator D in an adversarial way, to minimize the domain discrepancy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for impl"
K19-1064,C10-2172,0,0.020795,"d discriminator D in an adversarial way, to minimize the domain discrepancy distance between the target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for impl"
K19-1064,I17-1049,0,0.0179153,"rce classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptation with adversarial networks has become popular in recent years; this type of approach generates a representation for the target domain with the goal that the discriminator unable to distinguish between the source"
K19-1064,D16-1253,0,0.0188755,"urce, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptation with adversarial networks has become popular in recent years; this type of approach generates a representation for the target domain with the goal that the discriminator unable to distinguish"
K19-1064,P17-2042,0,0.0156624,"target representation distribution Mt (Xt ) and that of the source Ms (Xs ) (Section 3.2). Eventually, the target feature space is trained to match the source, and the source classifier C can be directly used on the target domain. To supplement the training data of implicit discourse relations, prior works have used weak supervision from sentences with discourse connectives (Marcu and Echihabi, 2002; Sporleder and Lascarides, 2008; Braud and Denis, 2014; Ji et al., 2015), by analyzing connectives (Zhou et al., 2010a,b; Biran and McKeown, 2013; Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017), using a multi-task framework with other corpora (Lan et al., 2013; Liu et al., 2016; Lan et al., 2017), or utilizing cross-lingual data (Wu et al., 2016; Shi et al., 2017). The important distinction between this work and the research above is that these are supervised systems that used all of the annotated implicit annotation from PDTB during training, while exploring non-PDTB corpora for additional, noisy discourse cues; on the contrary, our main goal is to assume no labeled training data for implicit discourse relations. Unsupervised domain adaptation with adversarial networks has become p"
K19-1064,P17-1130,0,0.130766,"work by proposing a reconstruction component to preserve the discriminability of target features, and incorporating techniques for stabler training on textual data. Experimental results show that even with a simple architecture for representation learning, our unsupervised domain adaptation system outperforms prior work by 1.4-2.3 macro F1, with substantial improvements on Temporal and Contingency relations. It is also superior to DANN (Ganin et al., 2016), an adversarial framework widely used in NLP (Chen et al., 2018; Gui et al., 2017; Zhang et al., 2017; Fu et al., 2017; Joty et al., 2017; Xu and Yang, 2017), by 5.7 macro F1. Finally, we extend the system to incorporate in-domain supervision as it is sometimes feasible resource-wise to build a seed corpus that may not be large enough to train a fully supervised system. We simulate this scenario by enabling the system to jointly optimize over a varying number of labeled examples of implicit relations. Our system consistently outperforms two strong baselines. Implicit discourse relations are not only more challenging to classify, but also to annotate, than their explicit counterparts. We tackle situations where training data for implicit relations"
K19-1064,N16-1174,0,0.0248108,"ng adaptation. The encoders encode relation arguments into latent representations, and then feeds the representations into a classifier C to predict the discourse relation. Qin et al. (2017) adopted adversarial strategies to supervised implicit discourse classification. 687 Figure 1: The framework of our proposed adversarial domain adaptation model, containing the pre-training stage, the adversarial adaptation stage, and the testing stage. The dashed box shows the supervised component. Encoder The encoder generates a representation for each argument with an inner-attention Bidirectional LSTM (Yang et al., 2016) shared between the two arguments. Then, the representations of the two arguments are concatenated to form the final representation, shown in Figure 2. Specifically, we encode each word in an argument into its word embeddings, which are fed into a BiLSTM, to get the hidden representations zi using a fully-connected layer Wc on top of the concatenated hidden states hi = [~hi , h~i ]. We then apply an attention mechanism to induce a distribution of weights over all tokens in the argument; the final argument representation Arg is a weighted sum of zi based on the attention weights αi : Figure 2:"
K19-1064,Q17-1036,0,0.114594,"mapping between source and target. We improve this framework by proposing a reconstruction component to preserve the discriminability of target features, and incorporating techniques for stabler training on textual data. Experimental results show that even with a simple architecture for representation learning, our unsupervised domain adaptation system outperforms prior work by 1.4-2.3 macro F1, with substantial improvements on Temporal and Contingency relations. It is also superior to DANN (Ganin et al., 2016), an adversarial framework widely used in NLP (Chen et al., 2018; Gui et al., 2017; Zhang et al., 2017; Fu et al., 2017; Joty et al., 2017; Xu and Yang, 2017), by 5.7 macro F1. Finally, we extend the system to incorporate in-domain supervision as it is sometimes feasible resource-wise to build a seed corpus that may not be large enough to train a fully supervised system. We simulate this scenario by enabling the system to jointly optimize over a varying number of labeled examples of implicit relations. Our system consistently outperforms two strong baselines. Implicit discourse relations are not only more challenging to classify, but also to annotate, than their explicit counterparts. We tackl"
L16-1620,P10-1018,0,0.0626835,"Missing"
L16-1620,Q14-1037,0,0.0139044,"specificity and they have high consensus as which text segments within the sentence are 2 Appendix: examples Formatting. Each example includes the sentence to be rated in italic as well as the two consecutive sentences immediately before (i.e., immediate context). The ratings are shown in annotator:rating format. For questions, they are formatted as: “underspecified text” — question body (context status) A Entity co-reference 7. underspecified. We plan to release our dataset and further expand it to enable more sophisticated linguistic analysis. We used the Berkeley Entity Resolution System (Durrett and Klein, 2014). High agreement [Ex1: general] Those forces are made up of about 150,000 troops from the United States and upward of 25,000 from other nations. But Dr. Allawi raised the tantalizing prospect of an eventual American withdrawal while giving little away, insisting that a pullout could not be tied to a fixed timetable, but rather to the Iraqi forces’ progress toward standing on their own. That formula is similar to what President Bush and other senior administration officials have spoken about. Ratings: A1:5, A2:5, A3:5 Questions: Q1: “That formula” — What is the formula? (immediate context) Q2:"
L16-1620,I11-1068,1,0.863716,"the the prior context are more likely to trigger questions about the reason behind events, “why” and “how”. Our data is accessible at http://www.cis.upenn.edu/%7Enlp/corpora/lrec16spec.html Keywords: specificity rating, underspecification, discourse 1. Introduction Louis and Nenkova (2012) introduced a corpus of sentences annotated as general or specific. Their definition of sentence specificity relied mostly on examples and intuition, related to the amount of detail contained by the sentence. They used the corpus of general and specific sentences to evaluate a classifier for the binary task (Louis and Nenkova, 2011a) and showed that changes in sentence and overall text specificity are strongly associated with perceptions of text quality. Science writing of the best quality in the New York Times is overall more general than regular science pieces in NYT and contain fewer stretches of specific content (Louis and Nenkova, 2013). Automatic summaries, which are often judged to be incoherent, are significantly more specific than same length human-written summaries for the same events (Louis and Nenkova, 2011b). Sentence specificity is also more robust than sentence length as indicator of which sentences may p"
L16-1620,W11-1605,1,0.776555,"the the prior context are more likely to trigger questions about the reason behind events, “why” and “how”. Our data is accessible at http://www.cis.upenn.edu/%7Enlp/corpora/lrec16spec.html Keywords: specificity rating, underspecification, discourse 1. Introduction Louis and Nenkova (2012) introduced a corpus of sentences annotated as general or specific. Their definition of sentence specificity relied mostly on examples and intuition, related to the amount of detail contained by the sentence. They used the corpus of general and specific sentences to evaluate a classifier for the binary task (Louis and Nenkova, 2011a) and showed that changes in sentence and overall text specificity are strongly associated with perceptions of text quality. Science writing of the best quality in the New York Times is overall more general than regular science pieces in NYT and contain fewer stretches of specific content (Louis and Nenkova, 2013). Automatic summaries, which are often judged to be incoherent, are significantly more specific than same length human-written summaries for the same events (Louis and Nenkova, 2011b). Sentence specificity is also more robust than sentence length as indicator of which sentences may p"
L16-1620,louis-nenkova-2012-corpus,1,0.868603,"form of free text questions. We present results from a pilot annotation with this new scheme and demonstrate good inter-annotator agreement. We found that the lack of specificity distributes evenly among immediate prior context, long distance prior context and no prior context. We find that missing details that are not resolved in the the prior context are more likely to trigger questions about the reason behind events, “why” and “how”. Our data is accessible at http://www.cis.upenn.edu/%7Enlp/corpora/lrec16spec.html Keywords: specificity rating, underspecification, discourse 1. Introduction Louis and Nenkova (2012) introduced a corpus of sentences annotated as general or specific. Their definition of sentence specificity relied mostly on examples and intuition, related to the amount of detail contained by the sentence. They used the corpus of general and specific sentences to evaluate a classifier for the binary task (Louis and Nenkova, 2011a) and showed that changes in sentence and overall text specificity are strongly associated with perceptions of text quality. Science writing of the best quality in the New York Times is overall more general than regular science pieces in NYT and contain fewer stretc"
L16-1620,R11-1037,0,0.133671,"selling records are sensational or what constitutes a valuable member may differ radically. Similarly when a typical argument of a verb is missing from a sentence (Palmer et al., 2005), the reader may have difficulty understanding the full event that is being described. Word choice can also determine the overall specificity of a sentence, by making more explicit the manner in which an action is performed or the identity of the discourse entity, as shown by the contrast of sentence pairs like “The worker cleaned the floor” vs. “The maid swept the floor” (Stinson and Tracy, 1983; Resnik, 1995; McKinlay and Markert, 2011; Nastase et al., 2012). The annotation we propose indirectly provides mechanisms to analyze which of the above intricate linguistic and semantic phenomena trigger the need for clarification of naive readers interested in gaining good understanding of a text. It is developed with the flexibility and intention to enable further analysis such as the classification of triggers and future refinement of annotation, to provide a practical connection between language-related applications and linguistic phenomena. 3. Methodology and corpus summary The annotation is carried out on news articles. Each a"
L16-1620,D12-1017,0,0.0674223,"Missing"
L16-1620,W13-2313,0,0.0271448,"he cause of underspecification in the form of free text questions, and identify if these questions may be answered by information given in previous context. If the annotator chose not to ask any question, she is asked to distinguish if the sentence is most specific (i.e., no underspecified segments) or most general (i.e., the sentence conveys general information that needs no further specification). The latter types of sentences capture generics such as “Cats have four paws.” that do not refer to specific events or entities (Carlson, 2005). Agreement on annotating generic noun phrases is low (Nedoluzhko, 2013), so we adopt a more high-level annotation at the sentence level that can be done with less training and with higher agreement. There are four types of status concerning previous context: • In the immediate context: the answer to the question can be found in the two immediately preceding sentences, a distance shown to be the median length of pronoun chains in writing (Hindle, 1983). Here we use this as the effective context for pronoun resolution. • In some previous context: the answer to the question can be found in the article but it is in a sentence more than two sentences before the one cu"
L16-1620,J05-1004,0,0.0811825,"d for interpreting these properties, it is impossible to verify if a sentence has the same truth value for both the writer and reader. These issues of ability to verify the truth value of a statement are directly related to Wiebe (2000)’s original definition of adjective subjectivity. Sentences like “He is a publishing sensation” and “He is a valuable member of our team” are subjective because different people’s definitions of what selling records are sensational or what constitutes a valuable member may differ radically. Similarly when a typical argument of a verb is missing from a sentence (Palmer et al., 2005), the reader may have difficulty understanding the full event that is being described. Word choice can also determine the overall specificity of a sentence, by making more explicit the manner in which an action is performed or the identity of the discourse entity, as shown by the contrast of sentence pairs like “The worker cleaned the floor” vs. “The maid swept the floor” (Stinson and Tracy, 1983; Resnik, 1995; McKinlay and Markert, 2011; Nastase et al., 2012). The annotation we propose indirectly provides mechanisms to analyze which of the above intricate linguistic and semantic phenomena tri"
L16-1620,petrov-etal-2012-universal,0,0.0184035,"annotation to broader questions in computational linguistics and semantics. 1 All 1388 419 332 317 242 66 24 Table 5: Number of question interrogatives and percentages of associated context status. 0.4 Token fraction 1.0 1.2 Interrogative what who how why which where when Underspecified tokens and context Previously we observed that about a third of the lack of specificity cannot be resolved in prior context. Here we offer insight into the characteristics of the tokens associated with this category of underspecification. In Table 6, we lay out the percentage of universal part-of-speech tags (Petrov et al., 2012) of tokens in underspecified segments their percentage associated with the following: fully specified, resolved in immediate context, in previous context and no context. We also separated the definite determiner “the” from the main determiner category to distinguish between definite and indefinite references. Each token is counted 3924 once if marked by multiple annotators. These numbers clearly show that most of the underspecification comes from content words. Among them, most of the lack of specificity of pronouns and determiners can be resolved in prior context. The definite expression “the"
L16-1620,P10-1005,0,0.0276331,"based on the information in the sentence and commonly shared background knowledge, and key information about the participants and causes of an event are fully expressed in the sentence. These three requirements cover a broad range of linguistic and semantic phenomena. For example a reference to a discourse entity may not be readily interpretable when the reference is anaphoric, by either a pronoun or definite noun phrase, when the reference is by proper name 3921 with which the reader is not familiar or the reference is generic, not referring to a specific discourse entity at all (Dahl, 1975; Reiter and Frank, 2010). Similarly gradable adjectives (Frazier et al., 2008; de Marneffe et al., 2010) like “tall”, “smart” and “valuable” are interpreted according to an assumed standard. If the standard is unknown or if the writer and the reader do not share the same standard for interpreting these properties, it is impossible to verify if a sentence has the same truth value for both the writer and reader. These issues of ability to verify the truth value of a statement are directly related to Wiebe (2000)’s original definition of adjective subjectivity. Sentences like “He is a publishing sensation” and “He is a"
L16-1620,W15-4631,0,0.0418913,"verall more general than regular science pieces in NYT and contain fewer stretches of specific content (Louis and Nenkova, 2013). Automatic summaries, which are often judged to be incoherent, are significantly more specific than same length human-written summaries for the same events (Louis and Nenkova, 2011b). Sentence specificity is also more robust than sentence length as indicator of which sentences may pose comprehension problems and need to be simplified for given audiences (Li and Nenkova, 2015). It is also a stable predictor in identifying high-quality arguments in online discussions (Swanson et al., 2015). Given the demonstrated importance of sentence and text specificity in practical applications and the known shortcomings of the existing annotation, we set out to develop a more detailed framework for annotation of sentence specificity. In the brief annotation guidelines of Louis and Nenkova (2012), the general vs. specific distinction was defined in the following way: “General sentences are broad statements about a topic. Specific sentences contain details and can be used to support or explain the general sentences further. In other words, general sentences create expectations in the minds o"
N16-1141,P10-1018,0,0.0302197,"Missing"
N16-1141,J95-2003,0,0.487739,"ntences (Louis et al., 2010) and that being a first sentence in an INSTANTIATION relation is the most powerful indicator for content selection related to discourse relation sense. The sentences between which the relation holds also contain more sentiment expressions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment analysis applications. Moreover, INSTANTIATION relations appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few relations that are more often implicit, i.e., expressed without a discourse marker such as “f"
N16-1141,C00-1044,0,0.419769,"Missing"
N16-1141,Q15-1024,0,0.0511403,"Missing"
N16-1141,W14-4320,1,0.851102,"Missing"
N16-1141,W14-4327,1,0.878277,"Missing"
N16-1141,D09-1036,0,0.0492481,"Missing"
N16-1141,N10-1043,1,0.798815,"ation revealed that the first sentences from INSTANTIATION pairs are included in human summaries significantly more often than other sentences (Louis et al., 2010) and that being a first sentence in an INSTANTIATION relation is the most powerful indicator for content selection related to discourse relation sense. The sentences between which the relation holds also contain more sentiment expressions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment analysis applications. Moreover, INSTANTIATION relations appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 200"
N16-1141,W10-4327,1,0.87887,"Missing"
N16-1141,R11-1037,0,0.0186645,"expressions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment analysis applications. Moreover, INSTANTIATION relations appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few relations that are more often implicit, i.e., expressed without a discourse marker such as “for exam1181 Proceedings of NAACL-HLT 2016, pages 1181–1186, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ple”. Identifying implicit discourse relation is an acknowledged difficult task (Braud and Denis, 2015; Ji and Ei"
N16-1141,W12-1614,0,0.0442688,"Missing"
N16-1141,D14-1162,0,0.0775346,"Missing"
N16-1141,D08-1020,1,0.575688,"tions (Pennington et al., 2014). Table 2 shows that s1 of INSTANTIATIONs contain significantly fewer out-of-vocabulary words compared to either s2 and non-INSTANTIATIONs. We also compare the difference in unigram probability2 of content word pairs across sentence pairs, i.e., (wi , wj ), wi ∈ s1 , wj ∈ s2 . Compared to non-INSTANTIATION, words across INSTANTIATION arguments show significantly larger average unigram log probability difference (1.24 vs. 1.22). These numbers show that the first sentences of INSTANTIATION do not involve many unfamiliar words — an indication of higher readability (Pitler and Nenkova, 2008). Gradable adjectives. The use of gradable adjectives (Frazier et al., 2008; de Marneffe et al., 2010)—popular, high, likely— may require further explanation to justify the appropriateness of their use. Here we compute the average percentage of gradable adjectives in a sentence. The list of adjectives is from Hatzivassiloglou and Wiebe (2000) and the respective percentages are shown in Table 3. Compared to other sentences, s1 of INSTANTIATION involves significantly more gradable adjectives. 2 We use a unigram language model on year 2006 of the New York Times Annotated Corpus (Sandhaus, 2008)."
N16-1141,P09-1077,1,0.782287,"Missing"
N16-1141,prasad-etal-2008-penn,0,0.0446494,"and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few relations that are more often implicit, i.e., expressed without a discourse marker such as “for exam1181 Proceedings of NAACL-HLT 2016, pages 1181–1186, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ple”. Identifying implicit discourse relation is an acknowledged difficult task (Braud and Denis, 2015; Ji and Eisenstein, 2015; Rutherford and Xue, 2014; Biran and McKeown, 2013; Park and Cardie, 2012; Lin et al., 2009; Pitler et al., 2009), but the challenge is exacerbated due to the lack of explicit IN STANTIATION s:"
N16-1141,E14-1068,0,0.0258267,"Missing"
N16-1141,N15-1081,0,0.0241629,"Missing"
N16-1141,P10-1040,0,0.0416154,"Missing"
N16-1141,P13-2013,0,\N,Missing
N16-1141,D15-1262,0,\N,Missing
N16-1141,W15-4612,0,\N,Missing
N19-1349,D18-1431,0,0.0594287,"ur system is available at https://git. io/fjkDd. 2 Related work Generic responses is a recognized problem in dialogue generation. Li et al. (2016a) maximized mutual information in decoding or reranking, which practically looks like penalizing responses that are common under a language model. Zhou et al. (2017) promoted diversity by training latent embeddings to represent different response mechanisms. Shao et al. (2017) trained and reranked responses segment by segment with a glimpse model to inject diversity. Another angle is to promote prompt-response coherence using techniques such as LDA (Baheti et al., 2018; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and response frequency have been used as rewards in reinforcement learning (Yao et al., 2016; Li et al., 2016d). Some methods adjusted sample weights in the training data, using a dual encoding model ("
N19-1349,W05-0909,0,0.0745248,"entences from diversity decoding often have similar structure and phrases across candidates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (Ko et"
N19-1349,D12-1091,0,0.060406,"Missing"
N19-1349,J92-1002,0,0.460484,"ord frequency (NIWF) Used in Zhang et al. (2018b), NIWF is the maximum of the Inverse Word Frequency (IWF) of all the words in a response, normalized to 0-1: log(1 + |Y |) max(IW F ) = max fw   (3) where fw denotes the number of responses in the corpus that contain the word w, and |Y |is the number of responses in the corpus. Taking a maximum reflects the assumption that a response is specific as long as it has at least some infrequent word. Perplexity per word (PPW) Perplexity is the exponentiation of the entropy, which estimates the expected number of bits required to encode the sentence (Brown et al., 1992; Goodman, 2001). Thus perplexity is a direct measure of the amount of information in the sentence in information theory; it has also been used as a measure of linguistic complexity (Gorin et al., 2000). To compute perplexity, we train a neural language model (Mikolov et al., 2011) on all gold responses and calculate cross-entropy of each sentence. To represent the amount of information per-token and to prevent the model to simply generate long sentences, we normalize perplexity by sentence length. Linguistically-informed specificity We use the system developed by Ko et al. (2019), which estim"
N19-1349,D17-1070,0,0.0146628,"andomly selected function word in the gold response with another random function word of the same part-ofspeech. For pronouns and determiners, these negative sentences would likely be incohesive; with other word categories such as prepositions, this will target grammatically. One word or phrase is replaced in each synthetic sentence. We train one classifier θj , j ∈{1,2,3,4} for each of the categories above.3 The classifiers take word embeddings as input and predict if the response is real or generated. Each classifier consists of a bi-directional LSTM with a projection layer and max pooling (Conneau et al., 2017), followed by 3 fully connected layers. The posterior probabilities of these classifiers reflect how confident the classifiers are that the sentence is synthetic and prone to be implausible, hence we prefer sentences with lower posterior probabilities. During reranking, we feed each candidate sentence c into the classifiers and aggregate the posterior probabilities from these classifiers by taking the 3 We compare with using one classifier lumping all negative sentences in the experiments. 3460 mean 14 4k=1 P (synthetic|c, θk ). At test time, to encourage diversity, we repeat inference multipl"
N19-1349,P18-1152,0,0.012331,"stributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our approach additionally leverages synthetic synthetic sentences targeting a range of plausibility issues and trains discriminators for reranking. 3 Generating specific responses Our main framework (Figure 1) is an attentionbased SEQ 2 SEQ model (Section 3.1) augmented with the ability to jointly learn embeddings from a target metric (e.g., specificity) with the response (Section 3.2). We then integrate frequencybased, information-theoretic and linguistic notions of"
N19-1349,N18-2008,0,0.0128396,"multiple phenomena, including referring expressions, concreteness of concepts, gradable adjectives, subjectivity and syntactic structure. Researchers have noticed that distributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our approach additionally leverages synthetic synthetic sentences targeting a range of plausibility issues and trains discriminators for reranking. 3 Generating specific responses Our main framework (Figure 1) is an attentionbased SEQ 2 SEQ model (Section 3.1) augmented with the ability to jointly"
N19-1349,J16-4003,0,0.169626,"o discourse relations. Sentence specificity predictors have since been developed (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017; Ko et al., 2019). Insights from these featurerich systems and hand-code analysis (Li et al., 2016e) showed that sentence specificity encompasses multiple phenomena, including referring expressions, concreteness of concepts, gradable adjectives, subjectivity and syntactic structure. Researchers have noticed that distributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our"
N19-1349,N16-1014,0,0.440062,"model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity i"
N19-1349,W17-5546,0,0.0133682,"; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and response frequency have been used as rewards in reinforcement learning (Yao et al., 2016; Li et al., 2016d). Some methods adjusted sample weights in the training data, using a dual encoding model (Lison and Bibauw, 2017) or sentence length and frequency in the corpus (Liu et al., 2018). Zhang et al. (2018b) proposed a Gaussian mixture model using frequency-based specificity values. Their approach involves ensembling the context probability and a specificity probability, whereas our approach conditions on both in a single model. Prediction of sentence specificity following the dictionary definition and pragmatically cast as “level of detail” was first proposed by Louis and Nenkova (2011), who related specificity to discourse relations. Sentence specificity predictors have since been developed (Louis and Nenkov"
N19-1349,D16-1230,0,0.0621724,"idates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (Ko et al., 2019); (3) cosine similarity between input and response representation"
N19-1349,D18-1297,0,0.270373,"e input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can potentially make pr"
N19-1349,I11-1068,0,0.439567,"hey usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can potentially make progress by incorporating more linguistically driven measures of specificity, as opposed to relying solely on frequency. We present a sequence-to-sequence dialogue model that factors out specificity and explicitly conditions on it when generating a response. The decoder takes as input categorized values of"
N19-1349,W17-5006,0,0.109906,"One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can potentially make progress by incorporating more linguistically driven measures of specificity, as opposed to relying solely on frequency. We present a sequence-to-sequence dialogue model that factors out specificity and explicitly conditions on it when generating a response. The decoder takes as input categorized values of several specificity metrics, embeds them, and us"
N19-1349,P02-1040,0,0.104787,"different plausibility levels). On the contrary, sentences from diversity decoding often have similar structure and phrases across candidates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses;"
N19-1349,P16-1094,0,0.256097,"model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity i"
N19-1349,D14-1162,0,0.082107,"122,458 prompt-response pairs for training and 14,602 pairs for testing. For validation, for reasons described in Section 5.1, we opt for human evaluation of overall response quality on a validation set of 60 prompt-response pairs from PersonaChat. Settings We use LSTMs with hidden layers of size 500, Adam optimizer (Kingma and Ba, 2015) with learning rate 0.001, β1 = 0.9, β2 = 0.999, dropout rate 0.2 for both training and testing, metric embedding dimension 300 and 5 training epochs. We train randomly initialized word embeddings of size 500 for the dialog model and use 300 dimentional GloVe (Pennington et al., 2014) embeddings for reranking classifiers. We generate 15 candidates for reranking per input sentence. To train the 4 reranking classifiers, we use 375,996 positive sentences on Opensubtitles and 110,221 on PersonaChat. We generate one negative sentence per word or phrase in the positive sentences. Since specificity is the focus of this study, during testing, we use the embedding of the highest specificity level (5) for NIWF and the linguistically informed specificity predictor. For PPW, we observe that the perplexity of generated sentences does not increase beyond the median level (3) during deve"
N19-1349,D16-1127,0,0.484362,"model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity i"
N19-1349,D17-1230,0,0.0282286,"ate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically-informed specificity (spec) (Ko et al., 2019); (3) cosine similarity between input and response representations, which captures coherence (Zhang et al., 2018a). We follow standards from prior work for human evaluation (Li et al., 2017; Zhang et al., 2018a,b; Xu et al., 2018a). We select 250 prompt-response pairs, and asked 5 judges from MechanicalTurk to rate the responses for each prompt. We evaluate whether the responses are informative (Ko et al., 2019; Wu et al., 2018; Shao et al., 2017) and on topic with the prompt (Shen et al., 2018; Xu et al., 4 Although Tao et al. (2018) proposed an unspervised metric, their code is not available. 2018b; Xing et al., 2017), on a scale of 1-5. Average scores are reported. In addition, we evaluate plausibility by asking judges whether they think the given response sentence without th"
N19-1349,D17-1235,0,0.118753,"her improvement. Our plausibility reranking method not only successfully improved the semantic plausibility of responses, but also improved their informativeness, relevance, and grammaticality. Our system is available at https://git. io/fjkDd. 2 Related work Generic responses is a recognized problem in dialogue generation. Li et al. (2016a) maximized mutual information in decoding or reranking, which practically looks like penalizing responses that are common under a language model. Zhou et al. (2017) promoted diversity by training latent embeddings to represent different response mechanisms. Shao et al. (2017) trained and reranked responses segment by segment with a glimpse model to inject diversity. Another angle is to promote prompt-response coherence using techniques such as LDA (Baheti et al., 2018; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and r"
N19-1349,N16-1141,1,0.834858,"th. Linguistically-informed specificity We use the system developed by Ko et al. (2019), which estimates specificity as a real value. This system adopts a pragmatic notion of specificity—level of details in text—that is originally derived using sentence pairs connected via the INSTANTIATION discourse relation (Louis and Nenkova, 2011). With this relation, one sentence explains in further detail of the content in the other; the explanatory sentence is shown to demonstrate properties of specificity towards particular concepts, entities and objects, while the other sentence is much more general (Li and Nenkova, 2016). We use this particular system since other specificity predictors are trained on news with binary specificity labels (Li and Nenkova, 2015). Ko et al. (2019) is an unsupervised domain adaptation system that predicts continuous specificity values, and was evaluated to be close to human judgments across several domains. We retrain their system using the gold responses in our data as unlabeled sentences in the unsupervised domain adaptation component. Coherence Prior work has shown that the universal response problem can be mitigated by improving the coherence between prompt and response (Zhang"
N19-1349,N15-1020,0,0.0380746,"d factors. Experiments show that our final model using linguistically motivated specificity and plausibility reranking improves the informativeness, reasonableness, and grammatically of responses. 1 Introduction Since the pioneering work in machine translation (Sutskever et al., 2014), sequence-tosequence (SEQ 2 SEQ) models have led much recent progress in open-domain dialogue generation, especially single-turn generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect"
N19-1349,L16-1620,1,0.907373,"Missing"
N19-1349,W04-1013,0,0.00982445,". On the contrary, sentences from diversity decoding often have similar structure and phrases across candidates. We also experimented with reinforcement learning, using policy gradient with the reranking scores as reward. However, during development, we observed that this method produced shorter, less informative sentences compared to reranking. P 5 5.1 Experiments Evaluation metrics Automatic evaluation of dialogue generation systems is a known challenge. Prior work has shown that commonly used metrics for overall quality in other generation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005) and perplexity have poor correlations with human judgment (Liu et al., 2016; Tao et al., 2018)4 or are modeldependent (Liu et al., 2016). Therefore, we adopt several metrics that evaluate multiple aspects of responses, and also conduct human evaluation for each result we present. We use the following automatic evaluation metrics: (1) distinct-1 and distinct-2 (Li et al., 2016a), which evaluates response diversity. They respectively calculate the number of distinct unigrams and bigrams, divided by the total number of words in all responses; (2) linguistically"
N19-1349,N18-2049,1,0.835989,"tors have since been developed (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017; Ko et al., 2019). Insights from these featurerich systems and hand-code analysis (Li et al., 2016e) showed that sentence specificity encompasses multiple phenomena, including referring expressions, concreteness of concepts, gradable adjectives, subjectivity and syntactic structure. Researchers have noticed that distributional semantics largely fail to capture semantic plausibility, especially in terms of discrete properties (e.g., negation) (Kruszewski et al., 2016) and physical properties (Wang et al., 2018). Kruszewski et al. (2016) created a dataset building on synthetically generated sentences for negation plausibility. Methodology-wise, Li et al. (2016b) trained embeddings for different speakers jointly with the dialogue context. Huang et al. (2018) learned embeddings of emotions; we learn embeddings of specificity metrics. Targeting multiple factors this way is broadly similar to the approach of Holtzman et al. (2018), who used multiple cooperative discriminators to model repetition, entailment, rel3457 evance, and lexical style in generation. Our approach additionally leverages synthetic sy"
N19-1349,D18-1432,0,0.52716,"eration. Li et al. (2016a) maximized mutual information in decoding or reranking, which practically looks like penalizing responses that are common under a language model. Zhou et al. (2017) promoted diversity by training latent embeddings to represent different response mechanisms. Shao et al. (2017) trained and reranked responses segment by segment with a glimpse model to inject diversity. Another angle is to promote prompt-response coherence using techniques such as LDA (Baheti et al., 2018; Xing et al., 2017). Cosine similarity between prompt and response has also been used for coherence (Xu et al., 2018b; Baheti et al., 2018). Wu et al. (2018) learn a small vocabulary of words that may be relevant during decoding and generates responses with this vocabulary. Several works tackle the problem by directly controlling response specificity in terms of word and response frequency. IDF and response frequency have been used as rewards in reinforcement learning (Yao et al., 2016; Li et al., 2016d). Some methods adjusted sample weights in the training data, using a dual encoding model (Lison and Bibauw, 2017) or sentence length and frequency in the corpus (Liu et al., 2018). Zhang et al. (2018b) propo"
N19-1349,P18-1102,0,0.0752342,"n generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can"
N19-1349,P18-1205,0,0.0832626,"n generation where the input is a prompt and the output is a response. However, SEQ 2 SEQ methods are known to favor universal responses, e.g., “I don’t know what you are talking about” (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a). These responses tend to be “safe” responses to many input queries, yet they usually fail to provide useful information. One promising line of research tackling this issue is to improve the specificity of responses, building on the intuition that generic responses frequently appear in the training data or consist of frequent words (Yao et al., 2016; Zhang et al., 2018b; Liu et al., 2018). However, past work in sentence specificity—the “quality of belonging or relating uniquely to a particular subject”1 — has shown that word frequency is only one aspect of specificity, and that specificity involves a wide range of phenomena including word usage, sentence structure (Louis and Nenkova, 2011; Li and Nenkova, 2015; Lugini and Litman, 2017) and discourse context (Dixon, 1987; Lassonde and O’Brien, 2009). Frequency-based specificity also does not exactly capture “the amount of information” as an information-theoretic concept. Hence, in dialogue generation, we can"
P14-2047,W13-3306,0,0.135469,"Missing"
P14-2047,D07-1007,1,0.794195,"ces, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation qu"
P14-2047,W13-3303,0,0.10424,"very long sentences which at times require the use of multiple English sentences to express the same content and preserve grammaticality. Similarly discourse connectives like because, but, since and while often relate information expressed in simple sentential clauses. There are a number of possible complications in translating these connectives: they may be ambiguous between possible senses, e.g., English while is ambiguous between COMPARISON and TEMPORAL; explicit discourse connectives may be translated into implicit discourse relations or translated in morphology rather than lexical items (Meyer and Webber, 2013; Meyer and Pol´akov´a, 2013). In our work, we quantify the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004;"
P14-2047,P07-1005,0,0.0458452,"uality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using tra"
P14-2047,P07-1017,0,0.0288128,"y the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited re"
P14-2047,C96-2183,0,0.0371132,"Missing"
P14-2047,P09-2004,1,0.922921,"Missing"
P14-2047,P05-1066,0,0.0606216,"Missing"
P14-2047,prasad-etal-2008-penn,0,0.490232,"Missing"
P14-2047,N04-1035,0,0.0469702,"eyer and Webber, 2013; Meyer and Pol´akov´a, 2013). In our work, we quantify the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English tra"
P14-2047,2006.amta-papers.25,0,0.0432856,"tems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using translated references, especially at the segment level. The data for the analysis is drawn from an extended set of newswire reports in the 2008/2010 NIST Metrics for Machine Translation 283 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 283–288, c Baltimor"
P14-2047,N06-2013,0,0.0340844,"tween information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides m"
P14-2047,P11-2111,0,0.188979,"Missing"
P14-2047,W04-1101,0,0.249046,"Missing"
P14-2047,J93-2004,0,\N,Missing
P17-1028,C02-1025,0,0.0483385,"those from domain experts (Snow et al., 2008). It 1 Soure code and biomedical abstract data: www.github.com/thanhan/seqcrowd-acl17, www.byronwallace.com/EBM_abstracts_data 299 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 299–309 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1028 model from a hidden state to an observation and a transition model from a hidden state to the next hidden state. Later work focused on discriminative models such as Maximum Entropy Models (Chieu and Ng, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). These were able to achieve strong predictive performance by exploiting arbitrary features, but they may not be the best choice for label aggregation. Also, compared to the simple HMM model, discriminative sequentially structured models require more complex optimization and are generally more difficult to extend. Here we argue for the generative HMMs for our first task of aggregating crowd labels. The generative nature of HMMs is a good fit for existing crowd modeling techniques and also enables very efficient parameter estimation."
P17-1028,D15-1261,0,0.2839,"Missing"
P17-1028,D07-1031,0,0.477994,"sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community. In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al."
P17-1028,N15-1089,0,0.0163622,"al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods. Sequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF 2 Related Work 300 model from multiple labels (although the identities of the annotato"
P17-1028,D14-1181,0,0.00550015,"works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al., 2011; Kim, 2014; Zhang and Wallace, 2015) and LSTMs (Lample et al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improvin"
P17-1028,W10-0713,0,0.039515,"Missing"
P17-1028,P07-1094,0,0.0246539,"ask 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community. In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) ("
P17-1028,N16-1030,0,0.434422,"beling of unannotated text is typically preferable, as it is more efficient, scalable, and cost-effective. Given a training set of crowd labels, how can we best predict sequences in unannotated text? Should we: (i) consider Task 1 as a pre-processing step and train the model using consensus labels; or (ii) instead directly train the model on all of the individual annotations, as done by Yang et al. (2010)? We investigate both directions in this work. Our approach is to augment existing sequence labeling models such as HMMs (Rabiner and Juang, 1986) and LSTMs (Hochreiter and Schmidhuber, 1997; Lample et al., 2016) by introducing an explicit ”crowd component”. For HMMs, we model this crowd component by including additional parameters for worker label quality and crowd label variables. For the LSTM, we introduce a vector representation for each annotator. In Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a"
P17-1028,N13-1132,0,0.76565,"by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods. Sequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF 2 Related Work 300 model from multiple labels (although the identities of the annotators or workers were not used). Rodrigues et al. (2014) extended this approach"
P17-1028,P14-2062,0,0.0816103,"ustin, 2 Northeastern University, 3 University of Pennsylvania, atn@cs.utexas.edu, byron@ccs.neu.edu, {ljunyi|nenkova}@seas.upenn.edu, ml@utexas.edu Abstract is therefore essential to model crowdsourced label quality, both to estimate individual annotator reliability and to aggregate individual annotations to induce a single set of “reference standard” consensus labels. While many models have been proposed for aggregating crowd labels for binary or multiclass classification problems (Sheshadri and Lease, 2013), far less work has explored crowdbased annotation of sequences (Finin et al., 2010; Hovy et al., 2014; Rodrigues et al., 2014). In this paper, we investigate two complementary challenges in using sequential crowd labels: how to best aggregate them (Task 1); and how to accurately predict sequences in unannotated text given training data from the crowd (Task 2). For aggregation, one might want to induce a single set of high-quality consensus annotations for various purposes: (i) for direct use at run-time (when a given application requires human-level accuracy in identifying sequences); (ii) for sharing with others; or (iii) for training a predictive model. When human-level accuracy in tagging"
P17-1028,D08-1027,0,0.39838,"Missing"
P17-1028,A97-1029,0,\N,Missing
P18-1019,W15-3817,0,0.0286551,"lability of only small corpora, which have typically provided on the order of a couple hundred annotated abstracts or articles for very complex information extraction tasks. For example, the ExaCT system (Kiritchenko et al., 2010) applies rules to extract 21 aspects of the reported trial. It was developed and validated on a dataset of 182 marked full-text articles. The ACRES system (Summerscales et al., 2011) produces summaries of several trial characteristic, and was trained on 263 annotated abstracts. Hinting at more challenging tasks that can build upon foundational information extraction, Alamri and Stevenson (2015) developed methods for detecting contradictory claims in biomedical papers. Their corpus of annotated claims contains 259 sentences (Alamri and Stevenson, 2016). Larger corpora for EBM tasks have been derived using (noisy) automated annotation approaches. This approach has been used to build, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010). Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016). In contrast to"
P18-1019,N16-1030,0,0.0239175,"g medical literature generally and 204 CRF Participants Interventions Outcomes LSTM-CRF Participants Interventions Outcomes Precision 0.55 0.65 0.83 Precision 0.78 0.61 0.69 Recall 0.51 0.21 0.17 Recall 0.66 0.70 0.58 F-1 0.53 0.32 0.29 F-1 0.71 0.65 0.63 Participants Interventions Outcomes Precision 0.41 0.79 0.24 Precision 0.41 0.59 0.60 Recall 0.20 0.44 0.21 Recall 0.25 0.15 0.51 F-1 0.26 0.57 0.22 F-1 0.31 0.21 0.55 each token index, which is then passed to a CRF layer for prediction. We also exploit characterlevel information by passing a bi-LSTM over the characters comprising each word (Lample et al., 2016); these are appended to the word embedding representations before being passed through the bi-LSTM. 6 Conclusions We have presented EBM-NLP: a new, publicly available corpus comprising 5,000 richly annotated abstracts of articles describing clinical randomized controlled trials. This dataset fills a need for larger scale corpora to facilitate research on NLP methods for processing the biomedical literature, which have the potential to aid the conduct of EBM. The need for such technologies will only become more pressing as the literature continues its torrential growth. The EBM-NLP corpus, acco"
P18-1019,D10-1011,0,0.157081,"ty nye.b@husky.neu.edu jessy@austin.utexas.edu romapatel996@gmail.com Yinfei Yang∗ No affiliation Iain J. Marshall King’s College London Ani Nenkova UPenn yangyin7@gmail.com iain.marshall@kcl.ac.uk nenkova@seas.upenn.edu Byron C. Wallace Northeastern University b.wallace@northeastern.edu Abstract Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task. Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al., 2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models. Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM. The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with r"
P18-1019,P16-1101,0,0.100901,"Missing"
P18-1019,P14-5010,0,0.00291024,"of MeSH terms with an instantiation rate above different thresholds for the respective PIO elements are shown in Table 11. 5 5.1 Identifying P, I and O Spans We consider two baseline models: a linear Conditional Random Field (CRF) (Lafferty et al., 2001) and a Long Short-Term Memory (LSTM) neural tagging model, an LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016). In both models, we treat tokens as being either Inside (I) or Outside (O) of spans. For the CRF, features include: indicators for the current, previous and next words; part of speech tags inferred using the Stanford CoreNLP tagger (Manning et al., 2014); and character information, e.g., whether a token contains digits, uppercase letters, symbols and so on. For the neural model, the model induces features via a bi-directional LSTM that consumes distributed vector representations of input tokens sequentially. The bi-LSTM yields a hidden vector at Tasks & Baselines We outline a few NLP tasks that are central to the aim of processing medical literature generally and 204 CRF Participants Interventions Outcomes LSTM-CRF Participants Interventions Outcomes Precision 0.55 0.65 0.83 Precision 0.78 0.61 0.69 Recall 0.51 0.21 0.17 Recall 0.66 0.70 0.58"
P18-1019,J07-1005,0,0.118292,"d, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010). Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016). In contrast to the corpora discussed above, these automatically derived datasets tend to be relatively large, but they include only shallow annotations. Other work attempts to bypass basic extraction tasks and address more complex biomedical QA and (multi-document) summarization problems to support EBM (Demner-Fushman and Lin, 2007; Moll´a and Santiago-Martinez, 2011; Abacha and 3 Data Collection PubMed provides access to the MEDLINE database3 which indexes titles, abstracts and metadata for articles from selected medical journals dating back to the 1970s. MEDLINE indexes over 24 million abstracts; the majority of these have been manually assigned metadata which we used to retrieved a set of 5,000 articles describing RCTs with an emphasis on cardiovascular diseases, cancer, and autism. These particular topics were selected to cover a range of common conditions. We decomposed the annotation process into two steps, perfor"
P18-1019,W16-6112,1,0.840026,"du romapatel996@gmail.com Yinfei Yang∗ No affiliation Iain J. Marshall King’s College London Ani Nenkova UPenn yangyin7@gmail.com iain.marshall@kcl.ac.uk nenkova@seas.upenn.edu Byron C. Wallace Northeastern University b.wallace@northeastern.edu Abstract Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task. Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al., 2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models. Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM. The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with respect to characteristics of the underlying tri"
P18-1019,P17-4002,1,0.894463,"Missing"
P18-1019,U11-1012,0,0.0658617,"Missing"
P18-1019,P14-2062,0,0.0712477,"indicating redundancy in the mentions of PICO elements. In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource. We present baseline models and associated results for these tasks. 2 2.2 Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012). Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017). Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate. However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017). Related Work We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP. 2.1 Crowdsourcing NLP for EBM Prior work on NLP"
P18-1019,P17-1028,1,0.911942,"ncy in the mentions of PICO elements. In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource. We present baseline models and associated results for these tasks. 2 2.2 Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012). Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017). Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate. However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017). Related Work We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP. 2.1 Crowdsourcing NLP for EBM Prior work on NLP for EBM has been limit"
P18-1019,N10-1024,0,0.0830566,"Missing"
P18-1019,E12-2021,0,0.134122,"Missing"
P18-1019,D12-1053,0,0.0489279,"Missing"
P19-1062,D14-1162,0,0.0846982,"dj ) (1) Models We present two models: one for text classification, and one for sentence ordering. Both are based on the L&L model, with a design change to cause stronger percolation of information up the tree (we also experiment without this change). ci = Text classification The left-hand side of Figure 1 presents an overview of the model: the model operates first at the sentence-level to create sentence representations, and then at the document-level to create a document representation from the previously created sentence representations. In more detail, the model composes GloVe embeddings (Pennington et al., 2014) into a sentence representation using structured attention (from which a tree can be derived), then sentence representations into a single document representation for class predicn X aik ek (3) k=1 where aik is the probability that k is the child of i, and ek is the semantic vector of the child. The children vectors are then passed through a non-linear function, resulting in the updated semantic vector e0i for parent node i. e0i = tanh(Wr [ei , ci ]) 3 (4) https://github.com/nlpyang/structured We found similar results for using both parents and children as well as using parents only. 4 647 Yel"
P19-1062,D13-1158,0,0.0651747,"Missing"
P19-1062,prasad-etal-2008-penn,0,0.0301692,"ically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 2 The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an entire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. 1 Code and data available at https://github.com/ elisaF/structured 646 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 646–653 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics document class document-level max-pooling structured+compose attention sentences sentence structured+compose attention word"
P19-1062,P17-1092,0,0.673652,"ations, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.1 1 Introduction Discourse describes how a document is organized, and how discourse units are rhetorically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 2 The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an entire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. 1 Code and data available"
P19-1062,D18-1548,0,0.0479993,"Missing"
P19-1062,P15-1098,0,0.0174817,"e, and the model from the timestep with highest development performance is chosen. We report accuracies on the test set, and tree analyses on the development set. Our implementation is built on the L&L released implementation, with changes as noted in Section 3. Preprocessing and training details are in Appendix A. We evaluate the model on four text classification tasks and one sentence order discrimination task. 4.1 Settings Datasets Details and statistics are included in Appendix A.5 Yelp (in L&L, 5-way classification) comprises customer reviews from the Yelp Dataset Challenge (collected by Tang et al. (2015)). Each review is labeled with a 1 to 5 rating (least to most positive). Debates (in L&L, binary classification) are transcribed debates on Congressional bills from the U.S. House of Representatives (compiled by Thomas et al. (2006), preprocessed by Yogatama 4.3 Results We report accuracy (as in prior work) in Table 1, and perform two ablations: removing the structured attention at the document level, and removing it at both document and sentence levels. Additionally, we run experiments on the original code 5 Of the document-level datasets used in L&L (SNLI was sentence-level), we omit IMDB an"
P19-1062,J93-2004,0,\N,Missing
P19-1062,W06-1639,0,\N,Missing
P19-1062,W01-1605,0,\N,Missing
P19-1062,P14-1074,0,\N,Missing
P19-1062,J08-1001,0,\N,Missing
P19-1062,Q13-1028,0,\N,Missing
P19-1062,J17-4001,0,\N,Missing
P19-1062,N19-1173,0,\N,Missing
P19-1062,Q18-1005,0,\N,Missing
P19-1062,D07-1015,0,\N,Missing
P19-1190,D14-1179,0,0.0863586,"Missing"
P19-1190,P18-1068,0,0.027964,"exts (Lin et al., 2018; Luo et al., 2018). For addressing this issue, Generative Adversarial Nets (GAN) based approaches have been recently proposed to generate long, diverse and novel text (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a). These methods usually utilize reinforcement learning techniques to deal with the generation of discrete symbols. However, they seldom consider the linguistic information from natural languages, which cannot fully address the difficulties of our task. Our work is inspired by the work of using sketches as intermediate representations (Dong and Lapata, 2018; Wiseman et al., 2018; Xu et al., 2018b; Su et al., 2018). These works usually focus on sentence- or utterance-level generation tasks, in which global aspect semantics and transitions have not been considered. Our work is also related to review data mining, especially the studies on topic or aspect extraction from review data (Qiu et al., 2017; Zhao et al., 2010). 3 Problem Formulation A review is a natural language text written by a user u on a product (or item) i with a rating score of r. Let V denote the vocabulary and y 1:m = {hyj,1 , · · · , yj,t , · · · , yj,nj i}m j=1 denote a review t"
P19-1190,P18-1082,0,0.0291355,"intly utilize aspect semantics, syntactic sketch, and context information. We decompose the entire generation process into three stages. In this way, the generation of long review text becomes more controllable, since we consider a simpler sequence generation task at each stage. Furthermore, we incorporate language characteristics (e.g., Part-of-Speech tags and ngrams) into the aspect-aware decoder to instruct the generation of well-structured text. 2 Related Work In recent years, researchers have made great progress in natural language generation (NLG) (Zhang et al., 2018; Zhou et al., 2018; Fan et al., 2018). As a special NLG task, automatic review generation has been proposed to assist the writing of online reviews for users. RNN-based methods have been proposed to generate the review content conditioned on useful context information (Tang et al., 2016; Zhou et al., 2017). Especially, the task of review generation is closely related to the studies in recommender systems that aim to predict the preference of a user over products. Hence, several studies propose to couple the solutions of the two lines of research work, and utilize the user-product interactions for improving the review generation ("
P19-1190,P82-1020,0,0.744419,"Missing"
P19-1190,W04-1013,0,0.0346444,"Missing"
P19-1190,P18-2027,0,0.0232739,"r users. RNN-based methods have been proposed to generate the review content conditioned on useful context information (Tang et al., 2016; Zhou et al., 2017). Especially, the task of review generation is closely related to the studies in recommender systems that aim to predict the preference of a user over products. Hence, several studies propose to couple the solutions of the two lines of research work, and utilize the user-product interactions for improving the review generation (Ni et al., 2017; Wang and Zhang, 2017; Catherine and Cohen, 2018; Ni and McAuley, 2018). Although Ni and McAuley (2018) have explored aspect information to some extent, they characterize the generation process in a single stage and do not perform the coarse-tofine decoding. Besides, the aspect transition patterns have been not modeled. It has been found that RNN models tend to generate short, repetitive, and dull texts (Lin et al., 2018; Luo et al., 2018). For addressing this issue, Generative Adversarial Nets (GAN) based approaches have been recently proposed to generate long, diverse and novel text (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a). These methods usually utilize reinfo"
P19-1190,D18-1075,0,0.0159049,"ral studies propose to couple the solutions of the two lines of research work, and utilize the user-product interactions for improving the review generation (Ni et al., 2017; Wang and Zhang, 2017; Catherine and Cohen, 2018; Ni and McAuley, 2018). Although Ni and McAuley (2018) have explored aspect information to some extent, they characterize the generation process in a single stage and do not perform the coarse-tofine decoding. Besides, the aspect transition patterns have been not modeled. It has been found that RNN models tend to generate short, repetitive, and dull texts (Lin et al., 2018; Luo et al., 2018). For addressing this issue, Generative Adversarial Nets (GAN) based approaches have been recently proposed to generate long, diverse and novel text (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a). These methods usually utilize reinforcement learning techniques to deal with the generation of discrete symbols. However, they seldom consider the linguistic information from natural languages, which cannot fully address the difficulties of our task. Our work is inspired by the work of using sketches as intermediate representations (Dong and Lapata, 2018; Wiseman et al., 20"
P19-1190,D15-1166,0,0.0154486,"is flexible to incorporate more kinds of useful information using a similar approach. Aspect Decoder. The decoder is built upon the dHA GRU-based RNN network. Let hA dej ∈ R note a dHA -dimensional hidden vector at the j-th time step, which is computed via: A hA j = GRU(hj−1 , vaj−1 ), (3) where vaj−1 ∈ RdA is the embedding of the previous aspect label aj−1 . The hidden vector of the first time step is initialized by the encoding vector hA 0 = vc in Eq. 2. Then, RNNs recurrently compute hidden vectors, and predict the next aspect label (or ID) aj . Additionally, we use an attention mechanism (Luong et al., 2015) to enhance the effect of context information. We compute the attention score of context ck for the current time step of the decoder via: (t) wk = P ck exp(tanh(W1 [hA t ; vck ])) , A exp(tanh(W 1 [ht ; vck0 ])) ∈{u,i,r} 0 (4) where W1 is the parameter matrix to learn, and the attention vector c˜t is obtained by: X c˜t = (t) wk vck (5) ck ∈{u,i,r} Finally, we compute the probability of the j-th aspect label p(at |a<j , c) via: Pr(aj |a<j , c) ˜A h j = ˜A softmax(W4 h j + b1 ), (6) = tanh(W2 c˜j + W3 hA j ), (7) where W2 , W3 , W4 and b1 are learnable parameter matrices or vector. 4.2 Aspect-Aw"
P19-1190,D17-1170,0,0.125937,"g., A MAZON and Y ELP) have been an important kind of information platforms where users post their feedbacks or comments about products (Kim et al., 2016). Usually, writing an informative and wellstructured review will require considerable efforts by users. To assist the writing process, the task of review generation has been proposed to automatically generate review text for a user given a product and her/his rating on it (Tang et al., 2016; Zhou et al., 2017). In the literature, various methods have been developed for review generation (Tang et al., 2016; Zhou et al., 2017; Ni et al., 2017; Wang and Zhang, 2017; Catherine and Cohen, 2018). Most of these methods adopt Recurrent Neural Networks (RNN) based methods, especially ∗ Corresponding author the improved variants of Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). They fulfill the review generation task by performing the decoding conditioned on useful context information. Usually, an informative review is likely to consist of multiple sentences, containing substantive comments from users. Hence, a major problem of existing RNN-based methods is that they have limited capacities i"
P19-1190,D18-1356,0,0.0426269,"Missing"
P19-1190,I17-1079,0,0.190626,"view services (e.g., A MAZON and Y ELP) have been an important kind of information platforms where users post their feedbacks or comments about products (Kim et al., 2016). Usually, writing an informative and wellstructured review will require considerable efforts by users. To assist the writing process, the task of review generation has been proposed to automatically generate review text for a user given a product and her/his rating on it (Tang et al., 2016; Zhou et al., 2017). In the literature, various methods have been developed for review generation (Tang et al., 2016; Zhou et al., 2017; Ni et al., 2017; Wang and Zhang, 2017; Catherine and Cohen, 2018). Most of these methods adopt Recurrent Neural Networks (RNN) based methods, especially ∗ Corresponding author the improved variants of Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). They fulfill the review generation task by performing the decoding conditioned on useful context information. Usually, an informative review is likely to consist of multiple sentences, containing substantive comments from users. Hence, a major problem of existing RNN-based methods is that they hav"
P19-1190,D18-1428,0,0.194072,"nts of Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). They fulfill the review generation task by performing the decoding conditioned on useful context information. Usually, an informative review is likely to consist of multiple sentences, containing substantive comments from users. Hence, a major problem of existing RNN-based methods is that they have limited capacities in producing long and informative text. More recently, Generative Adversarial Net (GAN) based methods (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a) have been proposed to enhance the generation of long, diverse and novel text. However, they still focus on word-level generation, and neglect the importance of topical and syntactic characteristics from natural languages. As found in the literature of linguistics (Pullum, 2010) and writing (Bateman and Zock, 2003), the writing process itself has involved multiple stages focusing on different levels of goals. We argue that an ideal review generation approach should follow the writing procedure of a real user and capture rich characteristics from natural language. With this motivation, we des"
P19-1190,P18-2112,0,0.305179,"osed to assist the writing of online reviews for users. RNN-based methods have been proposed to generate the review content conditioned on useful context information (Tang et al., 2016; Zhou et al., 2017). Especially, the task of review generation is closely related to the studies in recommender systems that aim to predict the preference of a user over products. Hence, several studies propose to couple the solutions of the two lines of research work, and utilize the user-product interactions for improving the review generation (Ni et al., 2017; Wang and Zhang, 2017; Catherine and Cohen, 2018; Ni and McAuley, 2018). Although Ni and McAuley (2018) have explored aspect information to some extent, they characterize the generation process in a single stage and do not perform the coarse-tofine decoding. Besides, the aspect transition patterns have been not modeled. It has been found that RNN models tend to generate short, repetitive, and dull texts (Lin et al., 2018; Luo et al., 2018). For addressing this issue, Generative Adversarial Nets (GAN) based approaches have been recently proposed to generate long, diverse and novel text (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a). Thes"
P19-1190,D18-1462,0,0.211219,"nts of Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). They fulfill the review generation task by performing the decoding conditioned on useful context information. Usually, an informative review is likely to consist of multiple sentences, containing substantive comments from users. Hence, a major problem of existing RNN-based methods is that they have limited capacities in producing long and informative text. More recently, Generative Adversarial Net (GAN) based methods (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a) have been proposed to enhance the generation of long, diverse and novel text. However, they still focus on word-level generation, and neglect the importance of topical and syntactic characteristics from natural languages. As found in the literature of linguistics (Pullum, 2010) and writing (Bateman and Zock, 2003), the writing process itself has involved multiple stages focusing on different levels of goals. We argue that an ideal review generation approach should follow the writing procedure of a real user and capture rich characteristics from natural language. With this motivation, we des"
P19-1190,P02-1040,0,0.103262,"Missing"
P19-1190,E17-2107,0,0.0240321,"ation of discrete symbols. However, they seldom consider the linguistic information from natural languages, which cannot fully address the difficulties of our task. Our work is inspired by the work of using sketches as intermediate representations (Dong and Lapata, 2018; Wiseman et al., 2018; Xu et al., 2018b; Su et al., 2018). These works usually focus on sentence- or utterance-level generation tasks, in which global aspect semantics and transitions have not been considered. Our work is also related to review data mining, especially the studies on topic or aspect extraction from review data (Qiu et al., 2017; Zhao et al., 2010). 3 Problem Formulation A review is a natural language text written by a user u on a product (or item) i with a rating score of r. Let V denote the vocabulary and y 1:m = {hyj,1 , · · · , yj,t , · · · , yj,nj i}m j=1 denote a review text consisting of m sentences, where yj,t ∈ V denotes the t-th word of the j-th review sentence and nj is the length of the j-th sentence. 1970 We assume that the review generation process is decomposed into three different stages. First, a user generates an aspect sequence representing the major content flow for a review. To generate a sentenc"
P19-1190,N18-2010,0,0.0302319,"issue, Generative Adversarial Nets (GAN) based approaches have been recently proposed to generate long, diverse and novel text (Zang and Wan, 2017; Yu et al., 2017; Guo et al., 2018; Xu et al., 2018a). These methods usually utilize reinforcement learning techniques to deal with the generation of discrete symbols. However, they seldom consider the linguistic information from natural languages, which cannot fully address the difficulties of our task. Our work is inspired by the work of using sketches as intermediate representations (Dong and Lapata, 2018; Wiseman et al., 2018; Xu et al., 2018b; Su et al., 2018). These works usually focus on sentence- or utterance-level generation tasks, in which global aspect semantics and transitions have not been considered. Our work is also related to review data mining, especially the studies on topic or aspect extraction from review data (Qiu et al., 2017; Zhao et al., 2010). 3 Problem Formulation A review is a natural language text written by a user u on a product (or item) i with a rating score of r. Let V denote the vocabulary and y 1:m = {hyj,1 , · · · , yj,t , · · · , yj,nj i}m j=1 denote a review text consisting of m sentences, where yj,t ∈ V denotes the"
P19-1190,P18-1102,0,0.0261877,"iew generation model that is able to jointly utilize aspect semantics, syntactic sketch, and context information. We decompose the entire generation process into three stages. In this way, the generation of long review text becomes more controllable, since we consider a simpler sequence generation task at each stage. Furthermore, we incorporate language characteristics (e.g., Part-of-Speech tags and ngrams) into the aspect-aware decoder to instruct the generation of well-structured text. 2 Related Work In recent years, researchers have made great progress in natural language generation (NLG) (Zhang et al., 2018; Zhou et al., 2018; Fan et al., 2018). As a special NLG task, automatic review generation has been proposed to assist the writing of online reviews for users. RNN-based methods have been proposed to generate the review content conditioned on useful context information (Tang et al., 2016; Zhou et al., 2017). Especially, the task of review generation is closely related to the studies in recommender systems that aim to predict the preference of a user over products. Hence, several studies propose to couple the solutions of the two lines of research work, and utilize the user-product interactions"
P19-1190,D10-1006,0,0.178824,"tself has involved multiple stages focusing on different levels of goals. We argue that an ideal review generation approach should follow the writing procedure of a real user and capture rich characteristics from natural language. With this motivation, we design an elaborative coarseto-fine generation process by considering the aspect semantics and syntactic characteristics. Figure 1 presents an illustrative example for our review generation process. First, we conceive the content flow that is characterized as an aspect sequence. An aspect describes some property or attribute about a product (Zhao et al., 2010), such as sound and service in this example. To generate a sentence, we further create a sentence skeleton containing semantic slots given the aspect semantics. The semantic slots denote the placeholders for useful syntactic information (e.g., Part-ofspeech tags). Finally, the semantic slots are filled with the generated words. The process is repeated 1969 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1969–1979 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Black Mini Microphone for iPhone 3G Product"
P19-1190,E17-1059,0,0.486614,". Extensive experiments results have demonstrated the effectiveness of the proposed model. 1 Introduction In the past decades, online review services (e.g., A MAZON and Y ELP) have been an important kind of information platforms where users post their feedbacks or comments about products (Kim et al., 2016). Usually, writing an informative and wellstructured review will require considerable efforts by users. To assist the writing process, the task of review generation has been proposed to automatically generate review text for a user given a product and her/his rating on it (Tang et al., 2016; Zhou et al., 2017). In the literature, various methods have been developed for review generation (Tang et al., 2016; Zhou et al., 2017; Ni et al., 2017; Wang and Zhang, 2017; Catherine and Cohen, 2018). Most of these methods adopt Recurrent Neural Networks (RNN) based methods, especially ∗ Corresponding author the improved variants of Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). They fulfill the review generation task by performing the decoding conditioned on useful context information. Usually, an informative review is likely to consist of"
P19-1190,P18-1061,0,0.0272036,"that is able to jointly utilize aspect semantics, syntactic sketch, and context information. We decompose the entire generation process into three stages. In this way, the generation of long review text becomes more controllable, since we consider a simpler sequence generation task at each stage. Furthermore, we incorporate language characteristics (e.g., Part-of-Speech tags and ngrams) into the aspect-aware decoder to instruct the generation of well-structured text. 2 Related Work In recent years, researchers have made great progress in natural language generation (NLG) (Zhang et al., 2018; Zhou et al., 2018; Fan et al., 2018). As a special NLG task, automatic review generation has been proposed to assist the writing of online reviews for users. RNN-based methods have been proposed to generate the review content conditioned on useful context information (Tang et al., 2016; Zhou et al., 2017). Especially, the task of review generation is closely related to the studies in recommender systems that aim to predict the preference of a user over products. Hence, several studies propose to couple the solutions of the two lines of research work, and utilize the user-product interactions for improving the"
S19-2212,P82-1020,0,0.810616,"Missing"
S19-2212,D14-1181,0,0.0321205,"In this task, suggestion mining that classified sentences into suggestion and non-suggestion classes was defined by the organizer. In this paper, we mainly use an attention-based LSTM model(Hochreiter and Schmidhuber, 1997) for this task. The word-embedding used for all models in this task is Word2Vec. Then, the word vectors are fed into the long short-term memory (LSTM) layer. Finally, an attention mechanism(Luong et al., 2015) is added into the neural networks, and the prediction results are output via the softmax activation. What’s more, we try a number of other models (such as the TextCNN(Kim, 2014), the C-LSTM(Zhou et al., 2015) and the attention-based Bi-LSTM(Lai et al., 2015) ) for comparative experiments. Furthermore we combine all of the above models to get results by soft voting. The rest of our paper is structured as follows. Section 2 introduces models. Section 3 describes data preparation. Experiments and evaluation are described in Section 4. The conclusions are drawn in Section 5. 2 Model For this task, we use 6 models for experiments. Among these models, the attention-based LSTM models can get the best results. This model combines the attention mechanism with the LSTM. The at"
S19-2212,D16-1180,0,0.0304469,"ems, the LSTM and the attention mechanism are more effective than they are used individually. For this task, we have 4 chances to submit our result in the final submission. We use differen1208 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1208–1212 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics t methods that are the attention-based LSTM, CLSTM and ensemble different models. In this task, we not only select some single models but also use the ensemble model architecture(Sarle, 1996). The ensemble model(Kuncoro et al., 2016) architecture, shown in figure 1, is an ensemble of many single models(We call them sub models)(Dietterich, 2000). Because each sub model is independent of each other, their weights are not shared and just use the same word embedding when training each sub model. The process of the whole ensemble model is carried out model by model. First, each model is run independently, and then the result file is saved. After running all the independent models, the result files are taken out and the final result is determined by the soft vote(Rokach, 2010). tor with a convolutional layer and a sub-sampling"
S19-2212,N16-1174,0,0.044737,"btained by calculating the similarity. An attention weight vector is generated by calculating a sentence vector matrix and a label vector matrix. The attention weight vector is then fed to the softmax layer. The attention mechanism allows the model to retain some important hidden information when the sentence is long. In our mission, the information of sentences and tags is kept for a relatively long time. Using the standard LSTM may result in the loss of hidden information. To solve this possible problem, we have facilitated the attentionbased LSTM model. In our task, the attention mechanism(Yang et al., 2016) can get better results. We think that the attention mechanism(Vaswani et al., 2017) can improve the efficiency of task. So, we combine the attention mechanism with the LSTM model. This model can get the best results among the single models, which is the final system we submitted. 1210 3 Data Preparation The organizers provided training, trial, and test sets, containing 8500, 592 and 833 sentences respectively(Negi et al., 2019). Each sentence corresponds to one label, 0 or 1. Although official data is regular, we need to do a further normalization. We want to make it possible to read these se"
S19-2212,D15-1166,0,0.0515968,"remains a relatively young area compared to Sentiment Analysis, especially in the context of recent advancements in neural network based approaches for learning feature representations. In this task, suggestion mining that classified sentences into suggestion and non-suggestion classes was defined by the organizer. In this paper, we mainly use an attention-based LSTM model(Hochreiter and Schmidhuber, 1997) for this task. The word-embedding used for all models in this task is Word2Vec. Then, the word vectors are fed into the long short-term memory (LSTM) layer. Finally, an attention mechanism(Luong et al., 2015) is added into the neural networks, and the prediction results are output via the softmax activation. What’s more, we try a number of other models (such as the TextCNN(Kim, 2014), the C-LSTM(Zhou et al., 2015) and the attention-based Bi-LSTM(Lai et al., 2015) ) for comparative experiments. Furthermore we combine all of the above models to get results by soft voting. The rest of our paper is structured as follows. Section 2 introduces models. Section 3 describes data preparation. Experiments and evaluation are described in Section 4. The conclusions are drawn in Section 5. 2 Model For this task"
S19-2212,S19-2151,0,0.060885,"Missing"
W14-4320,D09-1036,0,0.479635,"Missing"
W14-4320,P13-2013,0,0.542988,"icit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim to develop classifiers to"
W14-4320,W12-1614,0,0.227995,"he task 142 Proceedings of the SIGDIAL 2014 Conference, pages 142–150, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at disc"
W14-4320,C08-2022,1,0.902884,"Missing"
W14-4320,I11-1120,0,0.08792,"Missing"
W14-4320,P09-1077,1,0.903601,"l and linguistically rich features for the task 142 Proceedings of the SIGDIAL 2014 Conference, pages 142–150, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for on"
W14-4320,J95-2003,0,0.732462,"Missing"
W14-4320,D10-1039,0,0.0141057,"ns, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim"
W14-4320,prasad-etal-2008-penn,0,0.0745469,"tion Discourse relations holding between adjacent sentences in text play an essential role in establishing local coherence and contribute to the semantic interpretation of the text. For example, the causal relationship is helpful for textual entailment or question answering while restatement and exemplification are important for automatic summarization. Predicting the type of implicit relations, which are not signaled by any of the common explicit discourse connectives such as because, however, has proven to be a most challenging task in discourse analysis. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provided valuable annotations of implicit relations. Most research to date has focused on developing and refining lexical and linguistically rich features for the task 142 Proceedings of the SIGDIAL 2014 Conference, pages 142–150, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied"
W14-4327,N07-1054,0,0.0670673,"Missing"
W14-4327,C10-2172,0,0.238405,"Missing"
W14-4327,D10-1039,0,0.0325299,"Missing"
W14-4327,D09-1036,0,0.646066,"Missing"
W14-4327,P02-1047,0,0.656618,"Missing"
W14-4327,W12-1614,0,0.802014,"s. For example the Alternative class and Concession class have only 185 and 228 occurrences, respectively, in the 16,224 implicit relation annotations of the PDTB. 2 We use SVMLight (Joachims, 1999) with linear kernel. 200 prior work and downsampled the negative class so the number of positive and negative samples are equal in the training set.3 Our training set consists of PDTB sections 219. The testing set consists of sections 20-24. Like most studies, we do not include sections 0-1 in the training set. We expanded the test set (sections 23 or 23-24) used in previous work (Lin et al., 2014; Park and Cardie, 2012) to ensure the number of examples of the smaller relations, particularly of Temporal or Instantiation, are suitable for carrying out reliable tests for statistical significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 word-pairs binary-lexical #"
W14-4327,P09-1077,1,0.917674,"orm those including the lexical items for 6 of the 7 relations. Notably, production rules without lexical items are among the three worst representations, outperforming only the pure lexical features in some cases. This is a strong indication that being both a sparse syntactic representation and lacking lexical information, these features are not favored in this task. Pure lexical features give the worst or second to worst F scores, significantly worse than the alternatives in most of the cases. In Table 7 we list the binary classification results from prior work: feature selected word pairs (Pitler et al., 2009), aggregated word pairs (Biran and McKeown, 2013), production rules only (Park and Cardie, 2012), and the best combination possible from a variety of features (Park and Cardie, 2012), all of which include production rules. We aim to compare the relative gains in performance with different representations. Note that the absolute results from prior work are not exactly comparable to ours for two reasons — the training Ex. Analyst estimate the value of the BellSouth proposal at about $115 to $125 a share. [Implicit=AND] They value McCaw’s bid at $112 to $118 a share . Here the contrast clearly ha"
W14-4327,prasad-etal-2008-penn,0,0.83214,"prior lexical and syntactic features, and improves significantly the classification of implicit discourse relations. Given these findings, we address the question if any lexical information at all should be preserved in discourse parsers. We find that purely syntactic representations show lower recognition Introduction Implicit discourse relations hold between adjacent sentences in the same paragraph, and are not signaled by any of the common explicit discourse connectives such as because, however, meanwhile, etc. Consider the two examples below, drawn from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), of a causal and a contrast relation, respectively. The italic and bold fonts mark the arguments of the relation, i.e the portions of the text connected by the discourse relation. Ex1: Mrs Yeargin is lying. [Implicit = BECAUSE] They found students in an advanced class a year earlier who said she gave them similar help. 199 Proceedings of the SIGDIAL 2014 Conference, pages 199–207, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics lustrated in the following example where the two arguments are marked in bold and italic: for most relations, indicating that l"
W14-4327,J93-2004,0,\N,Missing
W14-4327,P13-2013,0,\N,Missing
W16-3617,P15-1153,0,0.013616,"ys S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we explore EDUs as a middle ground between fine-grained lexical units and full sentences. While EDUs have been used in prior work to directly assemble output summaries (Marcu, 1999; Hirao et al., 2013; Yoshida et al., 2014), the f"
W16-3617,D07-1001,0,0.0266921,",000 # contributors 15,000 10,000 5,000 0 1 2 3 4 5 6 7 8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the summarization literature for techniques which combine extractive sentence selection with sentence compression (Clarke and Lapata, 2007; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Kikuchi et al., 2014). These approaches are typically evaluated against abstractive summaries and have not been studied with a natural compressive dataset such as the ones proposed here. We do not address techniques to generate compressive summaries in this work but instead attempt to quantify how the omitted content in a summary relates to its EDU segmentation. 3 Contiguous Multi-part 3.1 Data and settings In the DUC 2005–2007 and TAC 2008–2011 shared tasks on multi-document summarization, evaluations are c"
W16-3617,W02-1001,0,0.0527873,"Missing"
W16-3617,P02-1057,0,0.0252912,"Missing"
W16-3617,P14-1048,0,0.056046,"Missing"
W16-3617,W10-4327,0,0.0363832,"d related work CIRCUMSTANCE S Discourse structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two t"
W16-3617,C04-1057,0,0.015236,"tems, has seen accuracy and speed improvements in recent years (Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing whi"
W16-3617,W09-1802,0,0.022774,"Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and s"
W16-3617,D13-1158,0,0.273442,"MSTANCE S Discourse structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which"
W16-3617,D15-1011,0,0.188878,"tence with three human-labeled concepts (SCU contributors). rived from the New York Times (NYT) corpus1 that are orders of magnitude larger than the DUC dataset, featuring thousands of article summaries with varying degrees of extractiveness. Although the summaries in this dataset typically contain fewer than 100 words and are sometimes intended to serve as a teaser for the article rather than a distillation of its content, they were nevertheless created by professional editors for a highly-trafficked news website. Prior work has also demonstrated the utility of this corpus for summarization (Hong et al., 2015; Nye and Nenkova, 2015). This dataset therefore enables the study of summarization in a realistic setting. 20,000 # contributors 15,000 10,000 5,000 0 1 2 3 4 5 6 7 8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the s"
W16-3617,hovy-etal-2006-automated,0,0.060529,"tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we explore EDUs as a middle ground between fine-grained lexical units and full sentences. While EDUs have been used in prior work to directly assemble output summaries (Marcu, 1999; Hirao et al., 2013; Yoshida et al., 2014), the focus was on using discourse structure as features for sentence ranking, while our work is the first t"
W16-3617,N15-1166,0,0.21908,"man-labeled concepts (SCU contributors). rived from the New York Times (NYT) corpus1 that are orders of magnitude larger than the DUC dataset, featuring thousands of article summaries with varying degrees of extractiveness. Although the summaries in this dataset typically contain fewer than 100 words and are sometimes intended to serve as a teaser for the article rather than a distillation of its content, they were nevertheless created by professional editors for a highly-trafficked news website. Prior work has also demonstrated the utility of this corpus for summarization (Hong et al., 2015; Nye and Nenkova, 2015). This dataset therefore enables the study of summarization in a realistic setting. 20,000 # contributors 15,000 10,000 5,000 0 1 2 3 4 5 6 7 8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the summarization literature"
W16-3617,N12-1015,0,0.0607799,"Missing"
W16-3617,petrov-etal-2012-universal,0,0.025344,"Missing"
W16-3617,A00-2024,0,0.0279496,"human-labeled summarization concepts within sentences and also aligns with near-extractive summaries constructed by news editors. Finally, we show that using EDUs as units of content selection instead of sentences leads to stronger summarization performance in near-extractive scenarios, especially under tight budgets. 1 Introduction Document summarization has a wide variety of practical applications and is consequently a focus of much NLP research. When a human summarizes a document, they often edit its constituent sentences in order to succinctly capture the document’s meaning. For instance, Jing and McKeown (2000) observed that summary authors trimmed extraneous content, combined sentences, replaced phrases or clauses with more general or specific variants, etc. These abstractive summaries thus involve sentences which deviate from those of the source document in structure or content. In contrast, automated approaches to summarization generally produce extractive summaries by selecting complete sentences from the source document (Nenkova and McKeown, 2011) in order to ensure that the output is grammatical. • A demonstration that EDU segmentation preserves human-identified conceptual units in the context"
W16-3617,prasad-etal-2008-penn,0,0.0157072,"ntences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which are similar to EDUs. However, unlike EDUs, PDTB relation arguments have flexibility in size, ordering and arrangement and do not form a complete segmentation of the text. They are therefore not easily interpretable as textual units that can be combined to form sentences and su"
W16-3617,J15-3002,0,0.0189641,"gement and do not form a complete segmentation of the text. They are therefore not easily interpretable as textual units that can be combined to form sentences and summaries. In this paper, we focus on EDUs and explore their viability as basic units for summarization. We did not use PDTB-style arguments to make sure each part of a document belongs to a textual unit and that the units are strictly adjacent to each other. EDU segmentation, typically addressed as a tagging problem early in discourse parsing systems, has seen accuracy and speed improvements in recent years (Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes,"
W16-3617,P14-2052,0,0.107706,"structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which are similar to EDUs."
W16-3617,C08-1110,1,0.750944,"rovements in recent years (Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is pres"
W16-3617,D15-1219,0,0.0131793,"ation over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we explore EDUs as a middle ground between fine-grained lexical units and full sentences. While EDUs have been used in prior work to directly assemble output summaries (Marcu, 1999; Hirao et al., 2013; Yoshida et al., 2014), the focus was on using discourse structure as features for sentence ranking, while our work is the first to examine the utility of EDUs themselves. Datasets. In this work, we address singledocument summarization. Standard datasets for the task were created for"
W16-3617,D12-1022,0,0.0123486,"8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the summarization literature for techniques which combine extractive sentence selection with sentence compression (Clarke and Lapata, 2007; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Kikuchi et al., 2014). These approaches are typically evaluated against abstractive summaries and have not been studied with a natural compressive dataset such as the ones proposed here. We do not address techniques to generate compressive summaries in this work but instead attempt to quantify how the omitted content in a summary relates to its EDU segmentation. 3 Contiguous Multi-part 3.1 Data and settings In the DUC 2005–2007 and TAC 2008–2011 shared tasks on multi-document summarization, evaluations are conducted under the pyramid method—a technique which quanti"
W16-3617,P11-1052,0,0.0214048,"ty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we expl"
W16-3617,W04-1013,0,0.121571,"pler problem than human summarization. This leads to a natural question: can extractive summarization techniques be used to produce more human-like summaries? We hypothesize that automated methods can generate a wider range of summaries by extracting over sub-sentential units of meaning from the source documents rather than whole sentences. Specifically, in this paper we investigate whether elementary discourse units (EDUs) from Rhetorical Structure Theory (Mann and Thompson, 1988) comprise viable textual units for summarization. Our focus is on recovering salient summary content under ROUGE (Lin, 2004) while the composition of EDUs into fluent output sentences is left to future work. We investigate this hypothesis in two complementary ways: by studying the compatibility of EDUs with human-labeled summarization units from pyramid evaluations (Nenkova et al., 2007) and by assessing their utility in reconstructing real-world document previews chosen by news editors in the New York Times corpus (Sandhaus, 2008). The contributions of this work include: Although human-written summaries of documents tend to involve significant edits to the source text, most automated summarizers are extractive and"
W16-3617,D14-1196,0,0.231527,"tion Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which are similar to EDUs. However, unlike EDUs, P"
W16-3617,P13-1020,0,\N,Missing
W16-3617,W01-0100,0,\N,Missing
W16-3617,N15-1114,0,\N,Missing
W16-3617,P11-1049,0,\N,Missing
W19-2201,P18-1063,0,0.0468223,"Missing"
W19-2201,N18-1169,0,0.0459221,"Missing"
W19-2201,W04-1006,0,0.150148,"Missing"
W19-2201,W12-0515,0,0.106242,"for not reading these documents were that they are perceived as too long (‘information overload’) and too complicated (‘difficult to understand’). This can be seen in Table 1, where a section of the terms of service for a popular phone app includes a 78-word paragraph that can be distilled down to a 19-word summary. The European Union’s General Data Protection 1 2 https://eugdpr.org/ https://plainlanguage.gov/ 1 Proceedings of the Natural Legal Language Processing Workshop 2019, pages 1–11 c Minneapolis, Minesotta, June 7, 2019. 2019 Association for Computational Linguistics and case reports (Galgani et al., 2012), as well as information extraction on patents (Tseng et al., 2007; Tang et al., 2012). While some companies have conducted proprietary research in the summarization of contracts, this information sits behind a large pay-wall and is geared toward law professionals rather than the general public. In an attempt to motivate advancement in this area, we have collected 446 sets of contract sections and corresponding reference summaries which can be used as a test set for such a task.3 We have compiled these sets from two websites dedicated to explaining complicated legal documents in plain English."
W19-2201,W04-1013,0,0.0195969,"Lead-K, and Random-K, we produce summaries budgeted at the average number of words among all summaries (Rush et al., 2015). However, for the sentence which causes the summary to exceed the budget, we keep or discard the full sentence depending on which resulting summary is closer to the budgeted length. ARI(Wd ) − ARI(Ws ) = 5.66 F K(Wd ) − F K(Ws ) = 6.12 Hence, there is a ∼6-year reading level distinction between the two sets of words, an indication that lexical difficulty is paramount in legal text. 5 Results To gain a quantitative understanding of the baseline results, we employed ROUGE (Lin, 2004). ROUGE is a standard metric used for evaluating summaries based on the lexical overlap between a generated summary and gold/reference summaries. The ROUGE scores for the unsupervised summarization baselines found in this paper can be found in Table 6. In the same table, we also tabulate ROUGE scores of the same baselines run on DUC 2002 (Over et al., 2007), 894 documents with summary lengths of 100 words, following the same settings. Note that our performance is a bit different from reported numbers in Mihalcea and Tarau (2004), as we performed different pre-processing and the summary lengths"
W19-2201,W04-3252,0,0.0355208,"nce summaries, which is significantly higher than the abstractive single-document summaries created for the shared tasks of the Document Understanding Conference (DUC) in 2002 (Over et al., 2007), a standard dataset used for single document news summarization. Additionally, we utilize several common readability metrics to show that there is an average of a 6 year reading level difference between the original documents and the reference summaries in our legal dataset. In initial experimentation using this dataset, we employ popular unsupervised extractive summarization models such as TextRank (Mihalcea and Tarau, 2004) and Greedy KL (Haghighi and Vanderwende, 2009), as well as lead baselines. We show that such methods do not perform well on this dataset when compared to the same methods on DUC 2002. These results highlight the fact that this is a very challenging task. As there is not currently a dataset in this domain large enough for supervised methods, we suggest the use of methods developed for simplification and/or style transfer. In this paper, we begin by discussing how this task relates to the current state of text summarization and similar tasks in Section 2. We then introduce the novel dataset and"
W19-2201,W16-6620,0,0.0194505,"ing simplification systems are trained on (e.g., Wikipedia, news). Second, popular supervised approaches such as treating sentence simplification as monolingual machine translation (Specia, 2010; Zhu et al., 2010; Woodsend and Lapata, 2011; Xu et al., 2016; Zhang and Lapata, 2017) would be difficult to apply due to the lack of sentencealigned parallel corpora. Possible directions include unsupervised lexical simplification utilizing distributed representations of words (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016), unsupervised sentence simplification using rich semantic structure (Narayan and Gardent, 2016), or unsupervised style transfer techniques (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). However, there is not currently a dataset in this domain large enough for unsupervised methods, nor corpora unaligned but comparable in semantics across legal and plain English, which we see as a call for future research. Qualitative Analysis We examined some of the results of the unsupervised extractive techniques to get a better understanding of what methods might improve the results. Select examples can be found in Table 7. As shown by example (1), the extractive systems performed well when"
W19-2201,N15-1166,0,0.0143076,"associated with summaries and top 50 words Wd most associated with the original snippets (described below) and consider the differences of ARI and F-K measures. We chose these two measures because they are a weighted sum of a word and sentential properties; as sentential information is kept the same (50 1word “sentences”), the differences will reflect the change in readability of the words most associated with plain English summaries/original texts. To collect Ws and Wd , we calculate the log odds ratio for each word, a measure used in prior work comparing summary text and original documents (Nye and Nenkova, 2015). The log odds ratio compares the probability of a word w occurring in the set of all summaries S vs. original texts D: Readability To verify that the summaries more accessible to a wider audience, we also compare the readability of the reference summaries and the original texts. Full texts We make a comparison between the original contract sections and respective summaries using four common readability metrics. All readability metrics were implemented using Wim Muskee’s readability calculator library for Python8 . These measurements included: • Flesch-Kincaid formula (F-K): the weighted sum o"
W19-2201,D11-1038,0,0.0322283,"nt style than the original text. 6 Discussion Our preliminary experiments and analysis show that summarizing legal contracts in plain English is challenging, and point to the potential usefulness of a simplification or style transfer system in the summarization pipeline. Yet this is challenging. First, there may be a substantial domain gap between legal documents and texts that existing simplification systems are trained on (e.g., Wikipedia, news). Second, popular supervised approaches such as treating sentence simplification as monolingual machine translation (Specia, 2010; Zhu et al., 2010; Woodsend and Lapata, 2011; Xu et al., 2016; Zhang and Lapata, 2017) would be difficult to apply due to the lack of sentencealigned parallel corpora. Possible directions include unsupervised lexical simplification utilizing distributed representations of words (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016), unsupervised sentence simplification using rich semantic structure (Narayan and Gardent, 2016), or unsupervised style transfer techniques (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). However, there is not currently a dataset in this domain large enough for unsupervised methods, nor corpora unal"
W19-2201,W16-4912,0,0.0284798,"nging. First, there may be a substantial domain gap between legal documents and texts that existing simplification systems are trained on (e.g., Wikipedia, news). Second, popular supervised approaches such as treating sentence simplification as monolingual machine translation (Specia, 2010; Zhu et al., 2010; Woodsend and Lapata, 2011; Xu et al., 2016; Zhang and Lapata, 2017) would be difficult to apply due to the lack of sentencealigned parallel corpora. Possible directions include unsupervised lexical simplification utilizing distributed representations of words (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016), unsupervised sentence simplification using rich semantic structure (Narayan and Gardent, 2016), or unsupervised style transfer techniques (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). However, there is not currently a dataset in this domain large enough for unsupervised methods, nor corpora unaligned but comparable in semantics across legal and plain English, which we see as a call for future research. Qualitative Analysis We examined some of the results of the unsupervised extractive techniques to get a better understanding of what methods might improve the results. Select exampl"
W19-2201,Q15-1021,0,0.0265382,"ere is no precise definition of plain English, the general philosophy is to make a text readily accessible for as many English speakers as possible. (Mellinkoff, 2004; Tiersma, 2000). Guidelines for plain English often suggest a preference for words with Saxon etymologies rather than a Latin/Romance etymologies, the use of short words, sentences, and paragraphs, etc.4 (Tiersma, 2000; Kimble, 2006). In this respect, the proposed task involves some level of text simplification, as we will discuss in Section 4.2. However, existing resources for text simplification target literacy/reading levels (Xu et al., 2015) or learners of English as a second language (Zhu et al., 2010). Additionally, these models are trained us3 The dataset is available at https://github.com/ lauramanor/legal_summarization 4 2 https://plainlanguage.gov/guidelines/ ing Wikipedia or news articles, which are quite different from legal documents. These systems are trained without access to sentence-aligned parallel corpora; they only require semantically similar texts (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). To the best of our knowledge, however, there is no existing dataset to facilitate the transfer of legal langua"
W19-2201,Q16-1029,0,0.023632,"text. 6 Discussion Our preliminary experiments and analysis show that summarizing legal contracts in plain English is challenging, and point to the potential usefulness of a simplification or style transfer system in the summarization pipeline. Yet this is challenging. First, there may be a substantial domain gap between legal documents and texts that existing simplification systems are trained on (e.g., Wikipedia, news). Second, popular supervised approaches such as treating sentence simplification as monolingual machine translation (Specia, 2010; Zhu et al., 2010; Woodsend and Lapata, 2011; Xu et al., 2016; Zhang and Lapata, 2017) would be difficult to apply due to the lack of sentencealigned parallel corpora. Possible directions include unsupervised lexical simplification utilizing distributed representations of words (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016), unsupervised sentence simplification using rich semantic structure (Narayan and Gardent, 2016), or unsupervised style transfer techniques (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). However, there is not currently a dataset in this domain large enough for unsupervised methods, nor corpora unaligned but compara"
W19-2201,D15-1044,0,0.271527,"dom sentence until a word limit is satisfied. For this baseline, the reported numbers are an average of 10 runs on the entire dataset.  The list of words with the highest log odds ratios for the reference summaries (Ws ) and original texts (Wd ) can be found in Table 5. We calculate the differences (in years) of ARI and F-K scores between Ws and Wd : Settings We employ lowercasing and lemmatization, as well as remove stop words and punctuation during pre-processing10 . For TextRank, KLSum, Lead-K, and Random-K, we produce summaries budgeted at the average number of words among all summaries (Rush et al., 2015). However, for the sentence which causes the summary to exceed the budget, we keep or discard the full sentence depending on which resulting summary is closer to the budgeted length. ARI(Wd ) − ARI(Ws ) = 5.66 F K(Wd ) − F K(Ws ) = 6.12 Hence, there is a ∼6-year reading level distinction between the two sets of words, an indication that lexical difficulty is paramount in legal text. 5 Results To gain a quantitative understanding of the baseline results, we employed ROUGE (Lin, 2004). ROUGE is a standard metric used for evaluating summaries based on the lexical overlap between a generated summa"
W19-2201,P17-1099,0,0.493018,"introduce the novel dataset and provide details on the level of abstraction, compression, and readability in Section 3. Next, we provide results and analysis on the performance of extractive summarization baselines on our data in Section 5. Finally, we discuss the potential for unsupervised systems in this genre in Section 6. 2 Related work Given a document, the goal of single document summarization is to produce a shortened summary of the document that captures its main semantic content (Nenkova et al., 2011). Existing research extends over several genres, including news (Over et al., 2007; See et al., 2017; Grusky et al., 2018), scientific writing (TAC, 2014; Jaidka et al., 2016; Yasunaga et al., 2019), legal case reports (Galgani et al., 2012), etc. A critical factor in successful summarization research is the availability of a dataset with parallel document/human-summary pairs for system evaluation. However, no such publicly available resource for summarization of contracts exists to date. We present the first dataset in this genre. Note that unlike other genres where human summaries paired with original documents can be found at scale, e.g., the CNN/DailyMail dataset (See et al., 2017), reso"
W19-2201,D17-1062,0,0.0198638,"n Our preliminary experiments and analysis show that summarizing legal contracts in plain English is challenging, and point to the potential usefulness of a simplification or style transfer system in the summarization pipeline. Yet this is challenging. First, there may be a substantial domain gap between legal documents and texts that existing simplification systems are trained on (e.g., Wikipedia, news). Second, popular supervised approaches such as treating sentence simplification as monolingual machine translation (Specia, 2010; Zhu et al., 2010; Woodsend and Lapata, 2011; Xu et al., 2016; Zhang and Lapata, 2017) would be difficult to apply due to the lack of sentencealigned parallel corpora. Possible directions include unsupervised lexical simplification utilizing distributed representations of words (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016), unsupervised sentence simplification using rich semantic structure (Narayan and Gardent, 2016), or unsupervised style transfer techniques (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). However, there is not currently a dataset in this domain large enough for unsupervised methods, nor corpora unaligned but comparable in semantics across l"
W19-2201,C10-1152,0,0.266288,"losophy is to make a text readily accessible for as many English speakers as possible. (Mellinkoff, 2004; Tiersma, 2000). Guidelines for plain English often suggest a preference for words with Saxon etymologies rather than a Latin/Romance etymologies, the use of short words, sentences, and paragraphs, etc.4 (Tiersma, 2000; Kimble, 2006). In this respect, the proposed task involves some level of text simplification, as we will discuss in Section 4.2. However, existing resources for text simplification target literacy/reading levels (Xu et al., 2015) or learners of English as a second language (Zhu et al., 2010). Additionally, these models are trained us3 The dataset is available at https://github.com/ lauramanor/legal_summarization 4 2 https://plainlanguage.gov/guidelines/ ing Wikipedia or news articles, which are quite different from legal documents. These systems are trained without access to sentence-aligned parallel corpora; they only require semantically similar texts (Shen et al., 2017; Yang et al., 2018; Li et al., 2018). To the best of our knowledge, however, there is no existing dataset to facilitate the transfer of legal language to plain English. 3 Data This section introduces a dataset c"
W19-2201,N09-1041,0,\N,Missing
W19-2201,P15-2011,0,\N,Missing
W19-2201,W01-0100,0,\N,Missing
W19-2201,W16-1511,0,\N,Missing
W19-2704,D15-1263,0,0.0238212,"menters: (1) DPLP3 uses features from syntactic and dependency parses for a linear support vector classifier; (2)T WO - PASS (Feng and Hirst, 2014) is a CRF segmenter that derives features from syntax parses but also uses global features to perform a second pass of segmentation; (3) N EURAL (Wang et al., 2018) is a neural BiLSTM-CRF model that uses ELMo embeddings (Peters et al., 2018). We choose these segmenters because they are widelyused and publicly available (most RST parsers do not include a segmenter). DPLP has been cited in several works showing discourse helps on different NLP tasks (Bhatia et al., 2015). T WO - PASS, until recently, achieved SOTA on discourse segmentation when using parsed (not gold) syntax 3 D OMAIN Table 3: F1, precision (P) and recall (R) of RST discourse segmenters on two domains (best numbers for News are underlined, for Medical are bolded). Agreement. Annotators achieved on average a high level of agreement for identifying EDU boundaries with kappa=0.90 (averaged over 11 texts). However, we note that document length and complexity of the discourse influence this number. On a document of 35 tokens, the annotators exhibited perfect agreement. For the Discussion sections"
W19-2704,P17-1092,0,0.0264977,"the more challenging Discussion section. During training, we recommend using medical-specific word embeddings and preprocessing pipeline.6 Addressing even one of these issues may yield a multiplied effect on segmentation improvements as the Medical domain is by nature highly repetitive and formulaic. However, a future avenue of research would be to first understand what impact these segmentation errors have on downstream tasks. For example, using RST trees generated by the lowest-performing DPLP parser nevertheless provides small gains to text categorization tasks such as sentiment analysis (Ji and Smith, 2017). On the other hand, understanding the verb form, which proved to be difficult in the Medical domain, has been shown to be useful in distinguishing text on experimental results from text describing more abstract concepts (such as background and introductory information), which may be a more relevant task than sentiment analysis (de Waard and Maat, 2012). Table 5: Average inter-annotator agreement per section, ordered from highest to lowest, the corresponding average F1 of the NEURAL segmenter, and number of tokens (there are 2 documents per section, except 1 for Summary). fit. This finding is"
W19-2704,P17-2037,0,0.0685629,"for understanding both the strengths and limits of existing RST segmenters, and the next concrete steps towards a better segmenter for the medical domain. 2 Corpus #docs #tokens #sents #EDUs RST-DT SMALL M EDICAL 11 11 4009 3356 159 169 403 399 Table 2: Corpus statistics. pus of RST-segmented medical articles in English. Unlike several other works, we include all parts of the article, and not just the abstract. Segmenters in non-news domains. While corpora have expanded to other domains, most automated discourse segmenters remain focused (and trained) on news. An exception is the segmenter in Braud et al. (2017a) which was trained on different domains for the purpose of developing a segmenter for under-resourced languages. However, they make the simplifying assumption that a single corpus represents a single (and distinct) domain, and do not include the medical domain. In this work, we study the viability of using news-trained segmenters on the medical domain. 3 Corpus Creation Medical Corpus. The M EDICAL corpus consists of 2 clinical trial reports from PubMed Central, randomly selected for their shorter lengths for ease of annotation. We expect the language and discourse to be representative of th"
W19-2704,J15-3002,0,0.246117,"Missing"
W19-2704,D17-1258,0,0.0777561,"for understanding both the strengths and limits of existing RST segmenters, and the next concrete steps towards a better segmenter for the medical domain. 2 Corpus #docs #tokens #sents #EDUs RST-DT SMALL M EDICAL 11 11 4009 3356 159 169 403 399 Table 2: Corpus statistics. pus of RST-segmented medical articles in English. Unlike several other works, we include all parts of the article, and not just the abstract. Segmenters in non-news domains. While corpora have expanded to other domains, most automated discourse segmenters remain focused (and trained) on news. An exception is the segmenter in Braud et al. (2017a) which was trained on different domains for the purpose of developing a segmenter for under-resourced languages. However, they make the simplifying assumption that a single corpus represents a single (and distinct) domain, and do not include the medical domain. In this work, we study the viability of using news-trained segmenters on the medical domain. 3 Corpus Creation Medical Corpus. The M EDICAL corpus consists of 2 clinical trial reports from PubMed Central, randomly selected for their shorter lengths for ease of annotation. We expect the language and discourse to be representative of th"
W19-2704,P14-5010,0,0.00410734,"all documents were re-segmented by both annotators, and disagreements were resolved by discussion. F1 P R DPLP News Medical 82.56 75.29 81.75 78.69 83.37 72.18 TWO - PASS News Medical 95.72 84.69 97.19 86.23 94.29 83.21 N EURAL News Medical 97.32 91.68 95.68 94.86 99.01 88.70 trees. N EURAL now holds SOTA in RST discourse segmentation. We evaluate the segmenter’s ability to detect all EDU boundaries present in the gold data (not just intra-sentential) using the metrics of precision (P), recall (R) and F1. The DPLP and TWO - PASS segmenters, both of which employ the Stanford Core NLP pipeline (Manning et al., 2014), were updated to use the same version of this software (2018-10-05). 5 Results Table 3 lists our results on News and Medical for correctly identifying EDU boundaries using the three discourse segmenters. As expected, the News domain outperforms the Medical domain, regardless of which segmenter is used. In the case of the DPLP segmenter, the gap between the two domains is about 7.4 F1 points. Note that the performance of DPLP on News lags considerably behind the state of the art (-14.76 F1 points). When switching to the TWO - PASS segmenter, the performance on News increases dramatically (+13"
W19-2704,W17-3610,0,0.0210038,"e medical documents. The corpus statistics are summarized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the medical domain, only two corpora exist, neither of which are in English. Da Cunha et al. (2012) annotate a small corpus of Spanish medical articles, and the RST Basque Treebank (Iruskieta et al., 2013) includes a small set of medical article abstracts. Our work aims to fill this gap by creating the first cor2 A future direction of research could revisit this domain of science if the differing segmentation schemes are ade"
W19-2704,N18-1202,0,0.00903885,"News and is also able to more successfully close the gap on Medical, with only a 5.64 F1 difference, largely attributable to lower recall. Experiment We automatically segment the documents in RSTDT SMALL and MEDICAL using three segmenters: (1) DPLP3 uses features from syntactic and dependency parses for a linear support vector classifier; (2)T WO - PASS (Feng and Hirst, 2014) is a CRF segmenter that derives features from syntax parses but also uses global features to perform a second pass of segmentation; (3) N EURAL (Wang et al., 2018) is a neural BiLSTM-CRF model that uses ELMo embeddings (Peters et al., 2018). We choose these segmenters because they are widelyused and publicly available (most RST parsers do not include a segmenter). DPLP has been cited in several works showing discourse helps on different NLP tasks (Bhatia et al., 2015). T WO - PASS, until recently, achieved SOTA on discourse segmentation when using parsed (not gold) syntax 3 D OMAIN Table 3: F1, precision (P) and recall (R) of RST discourse segmenters on two domains (best numbers for News are underlined, for Medical are bolded). Agreement. Annotators achieved on average a high level of agreement for identifying EDU boundaries wit"
W19-2704,W01-1605,0,0.234423,"ds arbitrary and uninformative analyses (Taboada and Mann, 2006). XML formatting was stripped, and figures and tables were removed. The sections for Acknowledgements, Competing Interests, and Prepublication History were not included. For comparison with the News domain, we created RST-DT-SMALL by sampling an equal number of Wall Street Journal articles from the “Test” portion of the RST-DT that were similar in length to the medical documents. The corpus statistics are summarized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the med"
W19-2704,P09-2004,0,0.0394657,"nals the start of an EDU (e.g., in the RST discourse relations of temporal and circumstance), but is not a boundary in this case because there is no verbal element. Other problematic words include “that”, signalling relative clauses (often, but not always treated as embedded EDUs), and “and” which may indicate a coordinated sentence or clause (treated as a separate EDU) but also a coordinated verb phrase (not a separate EDU). Note this phenomenon is different from distinguishing between discourse vs. nondiscourse usage of a word, or sense disambiguation of a discourse connective as studied in Pitler and Nenkova (2009). infinitival “to” The syntactic construction of to+verb can act either as a verbal complement (treated as the same EDU) or a clausal complement (separate EDU). In the Table 4 example, the infinitival “to buy” is a complement of the verb “move” and should remain in the same EDU, but the segmenter incorrectly segmented it. tokenization This error type covers cases where the tokenizer fails to detect token boundaries, specifically punctuation. These tokenization errors lead to downstream segmentation errors since punctuation marks, often markers of EDU boundaries, are entirely missed when mangle"
W19-2704,redeker-etal-2012-multi,0,0.0794475,"Missing"
W19-2704,N03-1030,0,0.248968,"Missing"
W19-2704,W17-3604,0,0.0219226,"arized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the medical domain, only two corpora exist, neither of which are in English. Da Cunha et al. (2012) annotate a small corpus of Spanish medical articles, and the RST Basque Treebank (Iruskieta et al., 2013) includes a small set of medical article abstracts. Our work aims to fill this gap by creating the first cor2 A future direction of research could revisit this domain of science if the differing segmentation schemes are adequately resolved in the forthcoming shared task of the"
W19-2704,D18-1116,0,0.0238744,"Missing"
W19-2704,P18-2071,0,0.0249392,"statistics are summarized in Table 2. Related Work Corpora in non-news domains. The seminal RST resource, the RST Discourse Treebank (RSTDT) (Carlson et al., 2001), consists of news articles in English. With the wide adoption of RST, corpora have expanded to other languages and domains. Several of these corpora include sciencerelated texts, a domain that is closer to medical, but unfortunately also use segmentation guidelines that differ sometimes considerably from RST-DT2 (research articles in Basque, Chinese, English, Russian, Spanish (Iruskieta et al., 2013; Cao et al., 2017; Zeldes, 2017; Yang and Li, 2018; Toldova et al., 2017; Da Cunha et al., 2012); encyclopedias and science news web pages in Dutch (Redeker et al., 2012)). Specifically in the medical domain, only two corpora exist, neither of which are in English. Da Cunha et al. (2012) annotate a small corpus of Spanish medical articles, and the RST Basque Treebank (Iruskieta et al., 2013) includes a small set of medical article abstracts. Our work aims to fill this gap by creating the first cor2 A future direction of research could revisit this domain of science if the differing segmentation schemes are adequately resolved in the forthcomi"
W19-2704,P07-1062,0,\N,Missing
