2007.iwslt-1.18,P07-2045,0,0.00498809,"ting spontaneous conversations in the travel domain from Italian to English. Decode Input Reranker Post-Process Extra Features Output Figure 1: System 1. Introduction This paper describes the machine translation system used by INESC-ID in its first participation on the evaluation campaign of the International Workshop on Spoken Language Translation 2007. We submitted translation results for manual and first-best transcriptions in the Italian-to-English language pair. The statistical machine translation system consists of a first-pass phrase-based system using moses machine translation toolkit [1], followed by a reranking step. In section 2 we describe the system as well as the corpora and the baseline results; in section 3 we present several experiments we did in order to improve the results. Then, in section 4 we show the results we obtained. Finally, section 5 concludes and discusses future work. 2. Overall System Description 2.1. Architecture The INESC-ID IWSLT07 Statistical Machine Translation (SMT) System architecture is shown in Figure 1. It consists of a pipeline with the following steps: preprocessing, phrase-based first pass decoding, n-best reranker and postprocessing. The f"
2007.iwslt-1.18,P03-1021,0,0.00857045,"Missing"
2007.iwslt-1.18,W07-0733,0,0.0276067,"Missing"
2007.iwslt-1.18,C04-1045,0,0.0640603,"Missing"
2007.iwslt-1.18,N04-1023,0,0.0668971,"Missing"
2007.iwslt-1.18,N04-1021,0,0.0601345,"Missing"
2007.iwslt-1.18,2006.iwslt-evaluation.7,0,\N,Missing
2010.iwslt-evaluation.9,N03-1017,0,0.0100386,"ents models than the IBM M4 alignment model, using the posterior distribution over alignments instead of the single best alignment. Moreover, we filtered phrase pairs from being extracted based on punctuation and phrase length differences. This paper is organized as follows: in Section 2 we will present the methods we used to improve the phrase extraction algorithm and Section 3 will describe the corpus used and data preparation. In Section 4, we will report the experimental results and, in Section 5, we will conclude the paper. 2. Phrase Extraction The most common phrase extraction algorithm [1] uses word alignment information to constraint the possible phrases that can be extracted. Given a word alignment, all phrase pairs 0 1 2 3 4 5 0 1 2 z p p p x p p p p p p p p p 3 p p z p p p p 4 p p p 5 p p p 6 p p p 7 p but p then p mr. z z z p p baldwin p p z p said p p p p p z: ce , m ba av en de´ : . pe ld ait su cla wi nd ite re´ n an t Figure 2: Example of a word alignment suffering from the garbage collector effect. consistent with that word alignment are extracted from the parallel sentence (a phrase pair is consistent with a word alignment if all words in one language contained in th"
2010.iwslt-evaluation.9,2006.amta-papers.11,0,0.0210516,"if there are too many incorrect alignment points forming a cluster, the correct phrases cannot be extracted without the spurious words, leading to missing words/phrases from the phrase table. In addition, unaligned words act as wild cards that can be aligned to every word in the neighborhood, thus increasing the size of the phrase table. Another undesirable effect of unaligned words is that they will only appear (in the phrase table) in the context of the surrounding words. Moreover, the spurious phrase pairs will change both the phrase probability and the lexical weight feature. The work by [2] conclude that the factor with most impact was the degradation of the translation probabilities due to noisy phrase pairs. Figure 1 shows the phrase tables extracted from two word alignments for the same sentence pair. These alignments only differ in one point: y-b. However, the nonexistence of this point in the second word alignment, results in the removal of the phrase y-b in the second phrase table. Hence we would not be able to translate y as b except in the contexts shown in that table. Figure 2 shows an example of a word alignment where a rare source word is aligned to a group of target"
2010.iwslt-evaluation.9,J10-3007,1,0.871261,"z 0 w 1 x y • 2 w a b c z Foreign x xy x xy xyz z yz z yz Source a a ab ab abc c c bc bc Points 0-0 0-0 0-0 0-0 0-0 2-2 2-2 2-2 2-2 2-2 Figure 1: Machine Translation phrase extraction from word alignments example. which leads that the word baldwin cannot be extracted without the incorrect surrounding context. This will make the pair baldwin, baldwin unavailable outside the given context. These led us to the work described in the following sections. 2.1. Constrained Alignments Rather than using the IBM M4 alignment model, we use the posterior regularization framework with symmetric constraints [3], which produces better overall results. This constraint takes into account that if a certain unit a is aligned to unit b in the source to target alignment model, b should also be aligned to a in the target to source alignment model. We also made some tests with bijective constrains [3], which had better overall results compared to the IBM M4 model, but that were not as good as the symmetric constraints. In both of these alignments we use a threshold of 0.4 for accepting an alignment point and we train them using the conditions described in [3]. For each model, we initialize the translation ta"
2010.iwslt-evaluation.9,D09-1106,0,0.021721,"from the original paper for an explanation of the meaning of these parameters). 2.2. Weighted Alignment Matrixes We use information about the posterior distributions in the alignments, rather than the single best alignment to obtain better results. Given the posterior distribution for an alignment link, we use the soft union heuristic (the average of each link) to obtain a symmetrized alignment with link posteriors. Given these alignments links, we calculate the phrase translation probability and the link probability using the approach proposed for weighted alignment matrixes, as described in [4]. We only accept a phrase if its phrase posterior probability is above a particular threshold. For both the BTEC and DIALOG corpora we use a threshold of 0.1. We set the values based on the results of the original paper and leave the tuning of this particular threshold as future work, as lowering it does not always yields better results. 2.3. Phrase Pair Filtering For each phrase pair that is extracted from a sentence pair, we apply an acceptor to decide whether that phrase pair is accepted or not. We build special acceptors that deal with punctuation. The idea is that punctuation normally tra"
2010.iwslt-evaluation.9,W09-0439,0,0.0139838,"er than 4 are cut from the translation table, we could not perform such a translation. 4.1. Automatic Evaluation Results We observe from the preliminary automatic evaluation results that our main problem in this evaluation was the fact we only used BLEU as our tuning and evaluation metric, rather than a combination of metrics, which limited the quality of our results. In fact, in many instances our system yielded better BLEU scores in the evaluation, but the overall score was worse. It is also important notice that the tuning process we used does not use any stabilization methods described in [6], which states that there is a high variance between different runs of the tuning process for the same translation model. Thus, there is also the possibility that we obtained a bad set of weights for one of our translation models. In the DIALOG task, we performed specially well in the IWSLT09 testset, probably due to the fact that this set bears more similarity to the data set we used for tuning. In the BTEC task, we were ranked worse. We think that one of the reasons was, as stated above, that we obtained a worse set of weights during the tuning procedure. 5. Conclusions We have described the"
2010.iwslt-evaluation.9,P02-1040,0,\N,Missing
2010.iwslt-papers.14,N03-1017,0,0.631324,"vious approaches fit in this algorithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively. 1. Introduction Modern statistical translation models depend crucially on the minimal translation units that are available during decoding. However, the evolution of statistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9],"
2010.iwslt-papers.14,N04-1035,0,0.0312417,"ithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively. 1. Introduction Modern statistical translation models depend crucially on the minimal translation units that are available during decoding. However, the evolution of statistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over t"
2010.iwslt-papers.14,P08-1112,1,0.927508,"tistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strat"
2010.iwslt-papers.14,D07-1006,0,0.0135809,"tistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strat"
2010.iwslt-papers.14,P07-1003,0,0.0667469,", 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation e"
2010.iwslt-papers.14,D09-1106,0,0.495182,"good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare severa"
2010.iwslt-papers.14,W05-0827,0,0.0165105,"ion process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and"
2010.iwslt-papers.14,N07-2053,0,0.0158559,", the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results."
2010.iwslt-papers.14,P08-1010,0,0.0128993,", the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results."
2010.iwslt-papers.14,D07-1103,0,0.104809,"[3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results. Our experiments ran on two differe"
2010.iwslt-papers.14,W02-1018,0,0.0354354,"table, the translation process can be broken down into three steps: segment the source sentence into phrases, translate each source phrase into a target phrase, and reorder the target phrases. The quality of Pb systems depend directly on the quality of their phrase-table and therefore, phrase extraction is always a fundamental step in Statistical MT. Extracting all possible phrase-pairs is not a valid option since an exponential number of phrases would be extracted, most of which linguistically irrelevant. Learning a phrase table directly from a bilingual corpus has also been tried previously [15, 16], but these methods failed to compete with heuristic methods that we will describe briefly in the following section. In [16] some problems that result from learning a phrase table directly from data using the EM algorithm are identified. In general terms, phrase pairs with different segmentations (potentially all equally good) compete for probability mass (as opposed to learning bilingual lexicons where the competition is only based on bilingual word pairs). The most common phrase extraction algorithm [3] 313 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, D"
2010.iwslt-papers.14,W06-3105,0,0.0713912,"table, the translation process can be broken down into three steps: segment the source sentence into phrases, translate each source phrase into a target phrase, and reorder the target phrases. The quality of Pb systems depend directly on the quality of their phrase-table and therefore, phrase extraction is always a fundamental step in Statistical MT. Extracting all possible phrase-pairs is not a valid option since an exponential number of phrases would be extracted, most of which linguistically irrelevant. Learning a phrase table directly from a bilingual corpus has also been tried previously [15, 16], but these methods failed to compete with heuristic methods that we will describe briefly in the following section. In [16] some problems that result from learning a phrase table directly from data using the EM algorithm are identified. In general terms, phrase pairs with different segmentations (potentially all equally good) compete for probability mass (as opposed to learning bilingual lexicons where the competition is only based on bilingual word pairs). The most common phrase extraction algorithm [3] 313 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, D"
2010.iwslt-papers.14,2006.amta-papers.11,0,0.0529946,"re are too many incorrect alignment points forming a cluster, the correct phrases cannot be extracted without the spurious words, leading to missing words/phrases from the phrase table. In addition, unaligned words act as wild cards that can be aligned to every word in the neighborhood, thus increasing the size of the phrase table. Another undesirable effect of unaligned words is that they will only appear (in the phrase table) in the context of the surrounding words. Moreover, the spurious phrase pairs will change both the phrase probability as well as the lexical weight feature. The work by [17] concludes that the factor with most impact was the degradation of the translation probabilities due to noisy phrase pairs. Figure 1 shows the phrase tables extracted from two word alignments for the same sentence pair. These alignments only differ in one point: y-b. However, the nonexistence of this point in the second word alignment results in the removal of the phrase y-b in the second phrase table. Hence we would not be able to translate y as b except in the contexts shown in that table. Figure 2 shows an example of a word alignment where a rare source word is aligned to a group of target"
2010.iwslt-papers.14,2005.iwslt-1.8,0,0.0239954,"ery different alignments. For phrase extraction we 0 1 2 3 4 5 0 z p p p p p 1 p x p p p p 2 p p 3 p p 4 p p 5 p p 6 p p 7 p but p then z p p p p p mr. z z z p p baldwin p p p p p z p said p p p p p z: ce , m ba av en de´ : . pe ld ait su cla wi nd ite re´ n an t Figure 2: Example of a word alignment suffering from the garbage collector effect. are interested in a single alignment per sentence so the two directional alignments are combined to form a single alignment. Several approaches have been proposed to symmetrize these word alignments. The most commonly used is called grow diagonal final [18]. It starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbag"
2010.iwslt-papers.14,J10-3007,1,0.702323,"sed is called grow diagonal final [18]. It starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbage collector effect and hence increasing the phrase table coverage, there is evidence that the two are only indirectly connected [17, 20]. There are several reasons that explain these facts. First during the pipeline, one keeps committing to the best options available: best word alignment, best symmetrized alignment, etc. Second, the symmetrization heuristics tend to obfuscate the quality of the resulting alignment and the original ones by creating alignments close to the diagonal. Third, current phrase extraction weights each phrase independently of the quality o"
2010.iwslt-papers.14,2006.iwslt-papers.7,0,0.0390249,"two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbage collector effect and hence increasing the phrase table coverage, there is evidence that the two are only indirectly connected [17, 20]. There are several reasons that explain these facts. First during the pipeline, one keeps committing to the best options available: best word alignment, best symmetrized alignment, etc. Second, the symmetrization heuristics tend to obfuscate the quality of the resulting alignment and the original ones by creating alignments close to the diagonal. Third, current phrase extraction weights each phrase independently of the quality of the underlying align314 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 ments (all phrases are given t"
2010.iwslt-papers.14,2008.amta-papers.18,0,0.562826,"hop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 ments (all phrases are given the weight of 1). Several approaches have been proposed to mitigate this problem. Soft Union [9] uses the knowledge about the posterior distributions of each directional model. It includes a point in the final alignment if the average of the posteriors under the two models for that point is above a certain threshold. However, this approach still produces a single alignment to be used by the phrase extraction. An alternative is to not commit to any particular alignment, but either use n-best lists [21] or the posterior distribution over the alignments to extract phrases [7, 10]. Both of these approaches increase the coverage of the phrase table. Moreover, by using the posterior weight of each phrase as its score instead of using 1 for all sentences, one can better estimate the phrase probabilities. The method computeGlobalPhraseStats calculates features, such as the phrase translation probability and the lexical probability based on the counts collected for each occurrence of each phrase pair. This is also the point where different types of smoothing for the phrase table can be applied (for"
2010.iwslt-papers.14,W06-1607,0,0.0287577,"ese approaches increase the coverage of the phrase table. Moreover, by using the posterior weight of each phrase as its score instead of using 1 for all sentences, one can better estimate the phrase probabilities. The method computeGlobalPhraseStats calculates features, such as the phrase translation probability and the lexical probability based on the counts collected for each occurrence of each phrase pair. This is also the point where different types of smoothing for the phrase table can be applied (for instance, we can implement at this step the Knesser-Ney smoothing for bilingual phrases [22]). In this paper, we use as features both the phrase probability and lexical probability in the general phrase extraction [3] [10]. Algorithm 1 General Phrase Table Extraction Require: Bilingual Corpus Require: MaximumPhraseSize - max for each sentence pair (s, t) in Corpus do extractedPhrasePairs = extractPhrasePairs(s, t, max) for each phrase pair p in extractedPhrasePairs do phraseTable.add(p) end for end for computeGlobalPhraseStats pruneGlobalPhraseStats savePhraseTable Algorithm 2 Extract Phrase Pairs Require: Bilingual sentence s fl = s.foreignLen sl = s.sourceLen extractedPhrasePairs ="
2010.iwslt-papers.14,P02-1040,0,0.0797175,", “？”, “！”) with the respective latin punctuation. Furthermore, we leave the segmentation of Chinese characters as the one given in the corpus. At the end of the pipeline, we detokenize and recase the translation, so that the evaluation is performed according to the IWLST task. The recasing is done using a maximum entropy-based capitalization system [23]. For all experiments we use the Moses decoder (http://www.statmt. org/moses/), and before decoding the test set, we tune the weights of the phrase table using Minimum Error Rate Training (MERT). The results are evaluated using the BLEU metric [24]. We also follow the baseline described in the workshop above, which creates directional alignments produced by GIZA++ using the IBM M4 model (the train conditions are the same as the default moses training scripts: 5 iterations of IBM M1, 5 iterations of HMM and 5 iterations of IBM M4). Also, we combine these alignments using the grow-diagonalfinal heuristic, and use the default phrase extraction algorithm [3]. We will refer to the baseline as “Moses IBM M4”. Moreover, we also compare three different alignment models: the regular HMM [25] (referred to as “HMM”), the same model, but using the"
2010.iwslt-papers.14,C96-2141,0,0.331127,"MERT). The results are evaluated using the BLEU metric [24]. We also follow the baseline described in the workshop above, which creates directional alignments produced by GIZA++ using the IBM M4 model (the train conditions are the same as the default moses training scripts: 5 iterations of IBM M1, 5 iterations of HMM and 5 iterations of IBM M4). Also, we combine these alignments using the grow-diagonalfinal heuristic, and use the default phrase extraction algorithm [3]. We will refer to the baseline as “Moses IBM M4”. Moreover, we also compare three different alignment models: the regular HMM [25] (referred to as “HMM”), the same model, but using the posterior regularization framework with bijective constraints (refered to as “BHMM”), and with symmetry constraints (refered to as “SHMM”) [19]. The latter constraint takes into account that if a certain unit a is aligned to unit b in the source to target alignment model, b should also be aligned to a in the target to source alignment model. In these alignments we use a threshold of 0.4 for accepting an alignment point. We trained these models using the conditions described in [19]. For each model, we initialize the translation tables with"
2010.iwslt-papers.14,E03-1009,0,0.0161308,"slation for the word “disait”. The phrases extracted with the default extraction method only contain the following right context “disait six heures” and, therefore, this context does not allow the translation of the sentence above. The words that are left unknown are due to the threshold being too high. We also add new features and new acceptors to address some observed problems. In the first experiment we add a part of speech feature that calculates the phrase probabilities and the lexical probabilities based on the part of speech of each word. We use the unsupervised POS system described by [26] (using the source code available at the authors website), and cluster the words into 50 different groups. We then tag each word with the attained tag. The intuition behind this feature is that a given word can be translated differently if it is being used as a noun or as a verb, and different POS sequences tend to generate different translations even if the words are the same. However, this approach produces worse results than the baseline. Two possible reasons are the use of an unsupervised system, whose accuracy is not very high. Furthermore, this system does not allows word ambiguity (the"
2010.iwslt-papers.14,2005.mtsummit-papers.36,0,0.0164424,"2) and 4 (length-diff4). Figure 3 shows the distribution of length difference of the phrase pairs in the phrase table, and Figure 4 shows the distributions of length difference of the phrase pairs actually used by the decoder. Finally, Table 6 shows the percentual reduction in the phrase table size using each heuristic. As expected, although there are a lot of phrase pairs with large length differences, the decoder only uses 1 sentence 317 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 70000 is similar to the approach proposed in [27] which only includes phrases of length larger than a given threshold if they cannot be realized by using smaller phrases. 60798 60000 50000 35945 40000 30000 28897 20000 15188 10000 5394 1725 453 5 6 0 0 1 2 3 4 Figure 3: Distribution of phrase-pairs in the phrase table by the difference between lenghts for source and target phrases for the BTEC corpus. with length difference of 6, as well as 4 phrase-pairs with a difference of 5. However, looking at Table 5 we see that both heuristics (length-diff2 and length-diff4) hurt the performance. Table 7 show the number of times phrase-pairs with a gi"
2010.iwslt-papers.14,J93-2003,0,\N,Missing
2010.iwslt-papers.14,P01-1067,0,\N,Missing
2010.iwslt-papers.14,J04-4002,0,\N,Missing
2010.iwslt-papers.14,J07-2003,0,\N,Missing
2010.iwslt-papers.14,P08-1000,0,\N,Missing
2011.eamt-1.19,J84-3009,0,0.408033,"Missing"
2011.eamt-1.19,almeida-etal-2010-bigorna,0,0.0592641,"Missing"
2011.eamt-1.19,D09-1030,0,0.0172107,"the EN→EP/BP/BP2EP models on the EN-EP test set 8 system, because it gave us several lexical / morphological entries used to resolve differences between BP to EP. We have used this to extract the lexical rules, where one entity in BP is written differently in EP. As the translation lexicon created automatically has a margin of error, it is necessary to manually filter spurious entries. If the language pair of varieties and/or dialects is well covered by the workers of crowdsourcing systems such as Amazon’s Mechanical turk (AMT), it is a viable option to avoid manually filtering these entries (Callison-Burch, 2009). Another possible improvement to extraction of multiword lexical contrastive pairs algorithm is the inclusion of the translational entropy to help to identify idiomatic multiword expressions (Moir´on and Tiedemann, 2006). We have also got encouraging results (about 5 BLEU points) when we evaluated the BP2EP output against manually created EP corpora. However, the system still needs some improvements to handle particular cases. The incomplete lexical pair coverage is one of the reasons. But there are exceptions to the rules that are hard to capture by rules. Also, the usage of handmade rules t"
2011.eamt-1.19,2001.mtsummit-papers.14,0,0.0386023,"Missing"
2011.eamt-1.19,A00-1002,0,0.726678,"Missing"
2011.eamt-1.19,N03-1017,0,0.0234167,"probability and scores, e.g.: “Eu achei” → “Eu pensei”, “Eu achei” → “Pensei”, “Eu achei” → “Pensava”. • Number based Removal: Numbers are generally translated one to one e.g.: “609 bilh˜oes em 2008” → “609 bili˜oes em 2008” • Identical Translation Removal : Many words in EP are translated equally to BP, which is done by default. Furthermore, if any word in the source phrase is contained in the target one or vice versa, the phrase pair is also removed, e.g.: “Eu”→ “E Eu”. • Confidence based Entries Filtering: Removes phrase pairs with low confidence based on their features, which are used in (Koehn et al., 2003). We remove entries having probabilities lower than 1 (to trim ambiguous entries) and the respective weights are lower than 0.5 (empiric threshold). We plan in future work to improve this filter by using a linear combination of the 4 parameters. • Lexicon based Filtering: Removes phrase pairs where the source or the target contain words that are not present in the lexicon of the respective language. • Number of Words Filtering: Removes phrase pairs where the number of words in the source is different from the number of words in the target. In our experiments, phrase pairs were limited to 2 wor"
2011.eamt-1.19,2010.iwslt-papers.14,1,0.883624,"Missing"
2011.eamt-1.19,W06-2405,0,0.0264081,"Missing"
2011.eamt-1.19,moore-2002-fast,0,0.0416943,"Our second experiment was formulated to analyze how the system can translate from BP to EP. It was used the manually parallel corpora of EP and BP. Our third experiment evaluated the usage of the BP2EP output in SMT. Our goal was to determine whether it is observed translation quality gains when adding the BP2EP output, created from the BP texts, to the EP models. The parallel corpora used in the SMT evaluation was created from TED talks. Since the audio transcriptions and translations available at the TED website are not aligned at the sentence level, we used the Bilingual Sentence Aligner (Moore, 2002) to accomplish this task. Table 5 shows some details about the EPEN, BP-EN, BP2EP-EN, PT-&-BP-EN and PT&BP2EP-EN. The BP2EP corpus corresponds to the output of the BP2EP system with the BP corpus as input. The EP-&-BP and EP-&-BP2EP corpus is the concatenation of the EP corpus with the BP and BP2EP corpus, respectively. 7.1 Evaluation of the extraction of Multiword Lexical Contrastive pairs from SMT Phrase-Tables We run the phrase table extraction algorithm for pairs of translations containing 1 and 2 words. The corpus was retrieved from the set of TED talks, that had both the Brazilian Portug"
2011.eamt-1.19,D09-1141,0,0.400395,"with identical approaches to translate from Czech to Slovak (Hajiˇc et al., 2000), from Spanish to Catalan (Canals-Marote et al., 2001)(Navarro et al., 2004), and from Irish to Gaelic Scottish (Scannell, 2006). Looking at Scannel’s system (2006) gives us a better understanding of the common system architecture. This architecture consists of a pipeline of components, such as a Part-of-Speech tagger (POS-tagger), a Na¨ıve Bayes word sense disambiguator, and a set of lexical and grammatical transfer rules, based on bilingual contrastive lexical and grammatical differences. On a different level, (Nakov and Ng, 2009) describes a way of building MT systems for lessresourced languages by exploring similarities with closely related and languages with much more resources. More than allowing translation for lessresourced languages, this work also aims at allowing translation from groups of similar languages to other groups of similar languages just like stated earlier. This method proposes the merging of bilingual texts and phrase-table combination in the training phase of the MT system. Merging bilingual texts from similar languages (on the source side), one with the less-resourced language and the other (muc"
2011.eamt-1.19,P02-1040,0,0.0834116,"arallel corpus on the EP translation, we made several experiments. Using the EP→EN and EN→EP models as baseline, we compared them with BP and BP2EP models, and also EP-&-BP and EP-&BP2EP. All experiments were performed using the Moses decoder 9 . Before decoding the test set (shown in Table 5), we tune the weights of the phrase table using Minimum Error Rate Training (MERT) using the devel corpus shown in Table 5. The devel and test set are in EP and EN and are the same among the several experiments. The language model was created only with EP texts. The results were evaluated using the BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009) metrics. 9 http://www.statmt.org/moses/ Tables 6 and 7 shows the results for the EP/BP/BP2EP→EN and EN→EP/BP/BP2EP models, respectively. We observed that BP models generate better results than using only the EP ones. The larger amount of parallel data for BP explains these differences. The BP2EP results were systematically better than using BP models. This shows that our hypothesis of converting BP to EP using this approach led to consistent improvements to the translation between EN and EP. Data Train Devel Test Lang. Pairs EP EN BP EN BP2EP EN EP EN EP"
2011.iwslt-papers.3,E03-1035,0,0.30853,"Missing"
2011.iwslt-papers.3,J03-3002,0,0.159638,"Missing"
2011.iwslt-papers.3,N04-1036,0,0.0818659,"Missing"
2011.iwslt-papers.3,2010.iwslt-papers.14,1,0.832985,"Missing"
2011.iwslt-papers.3,P07-2045,0,0.0045147,"Missing"
2011.iwslt-papers.3,P06-1010,0,0.081669,"Missing"
2011.iwslt-papers.3,O08-2010,0,\N,Missing
2020.lrec-1.669,Q17-1010,0,0.0180878,"A-BDE, willing to provide some insights on the data and set some baselines for possible applications of the corpus. In the first experiment, several unsupervised approaches were used for matching variations with their original question. The second experiment targeted the automatic identification of the source of a variation. 4.1. Matching Variations to Original Questions To better understand the challenge underlying the matching of variations with their original questions, we applied the following unsupervised approaches for this purpose: 2 https://cloud.google.com/translate/docs/ – FastText (Bojanowski et al., 2017) word embeddings, pre-trained for Portuguese5 ; – ELMo (Peters et al., 2018) contextual word embeddings, pre-trained for Portuguese6 (Gardner et al., 2018); – BERT (Devlin et al., 2019) contextual word embeddings, pre-trained for multiple languages7 . The application of Chatterbot followed two simple steps: (i) training with all the original questions and answers; (ii) checking the responses given for each variation. Since the default implementation only provides an answer for a given interaction, we only compute its accuracy for the first hit. For the remaining approaches, we also compute acc"
2020.lrec-1.669,D15-1075,0,0.0404154,"user queries. Their manual creation was also a time-consuming process but, once it has been created, the corpus can be used several times for measuring progress and comparing different approaches for matching user interactions with FAQs. Furthermore, once available, it may also be used by other researchers, in other projects. Despite its original goal, AIA-BDE may further be seen as a benchmark for assessing how other systems of this kind adapt to this domain, or even as the starting point for related tasks, such as Semantic Textual Similarity (Cer et al., 2017) or Natural Language Inference (Bowman et al., 2015) in a QA scenario. To the best of our knowledge, there is no corpus with domain questions and their paraphrases in Portuguese, so this is also a gap we aim to fill. In Section 2. we overview some related work on the evaluation of dialogue systems and resources used for this purpose, with some focus on FAQ retrieval systems, and on other corpora for QA in Portuguese. In Section 3. we describe the creation of AIA-BDE and show some examples of FAQs and variations. Before concluding, in Section 4. we report on the results of two experiments using AIA-BDE, aiming to provide additional insights. Fir"
2020.lrec-1.669,S17-2001,0,0.0162684,"mulated questions can be seen as a simulation of user queries. Their manual creation was also a time-consuming process but, once it has been created, the corpus can be used several times for measuring progress and comparing different approaches for matching user interactions with FAQs. Furthermore, once available, it may also be used by other researchers, in other projects. Despite its original goal, AIA-BDE may further be seen as a benchmark for assessing how other systems of this kind adapt to this domain, or even as the starting point for related tasks, such as Semantic Textual Similarity (Cer et al., 2017) or Natural Language Inference (Bowman et al., 2015) in a QA scenario. To the best of our knowledge, there is no corpus with domain questions and their paraphrases in Portuguese, so this is also a gap we aim to fill. In Section 2. we overview some related work on the evaluation of dialogue systems and resources used for this purpose, with some focus on FAQ retrieval systems, and on other corpora for QA in Portuguese. In Section 3. we describe the creation of AIA-BDE and show some examples of FAQs and variations. Before concluding, in Section 4. we report on the results of two experiments using"
2020.lrec-1.669,D18-1241,0,0.0167125,"in their threads. Relevance was annotated for related questions to the original question, and for comments, to related questions and to the original question, though different annotations were used in different subtasks. Queries were then generated from the subject of each original question and Google used for collecting up to 200 question-comment threads in the forum site. Results with ten or more comments and questions with less than 2,000 characters were considered to be related questions. Recently, datasets for question-answering within a dialogue context have also been developed, such as Choi et al. (2018) or Reddy et al. (2019), created both in a task where one person asks questions on a subject and the other answers, as naturally as possible, based on a text on the target subject. Specifically concerning Portuguese, corpora of subtitles have been used for conversational agents, to better deal with out-of-vocabulary interactions (Magarreiro et al., 2014); there are collections of factoid questions (Santos and Rocha, 2004; Magnini et al., 2004) and topics (Mota et al., 2012) with their answers, previously used in IR and QA shared tasks, and dense domain questions (Criscuolo et al., 2017), also"
2020.lrec-1.669,N19-1423,0,0.0614489,"tching variations with their original question. The second experiment targeted the automatic identification of the source of a variation. 4.1. Matching Variations to Original Questions To better understand the challenge underlying the matching of variations with their original questions, we applied the following unsupervised approaches for this purpose: 2 https://cloud.google.com/translate/docs/ – FastText (Bojanowski et al., 2017) word embeddings, pre-trained for Portuguese5 ; – ELMo (Peters et al., 2018) contextual word embeddings, pre-trained for Portuguese6 (Gardner et al., 2018); – BERT (Devlin et al., 2019) contextual word embeddings, pre-trained for multiple languages7 . The application of Chatterbot followed two simple steps: (i) training with all the original questions and answers; (ii) checking the responses given for each variation. Since the default implementation only provides an answer for a given interaction, we only compute its accuracy for the first hit. For the remaining approaches, we also compute accuracy for the presence of the correct answer in the top-3 or top-5 best ranked candidates. This has in mind that, in many scenarios, it is better to return a smaller set of answers that"
2020.lrec-1.669,W18-2501,0,0.0118064,"sed approaches were used for matching variations with their original question. The second experiment targeted the automatic identification of the source of a variation. 4.1. Matching Variations to Original Questions To better understand the challenge underlying the matching of variations with their original questions, we applied the following unsupervised approaches for this purpose: 2 https://cloud.google.com/translate/docs/ – FastText (Bojanowski et al., 2017) word embeddings, pre-trained for Portuguese5 ; – ELMo (Peters et al., 2018) contextual word embeddings, pre-trained for Portuguese6 (Gardner et al., 2018); – BERT (Devlin et al., 2019) contextual word embeddings, pre-trained for multiple languages7 . The application of Chatterbot followed two simple steps: (i) training with all the original questions and answers; (ii) checking the responses given for each variation. Since the default implementation only provides an answer for a given interaction, we only compute its accuracy for the first hit. For the remaining approaches, we also compute accuracy for the presence of the correct answer in the top-3 or top-5 best ranked candidates. This has in mind that, in many scenarios, it is better to return"
2020.lrec-1.669,W13-2405,0,0.191605,"significant. Keywords: FAQ retrieval, corpora creation, paraphrases detection, textual entailment, dialogue systems, Portuguese Language Processing 1. Introduction Question-answering (QA) dialogue systems should be able to handle different ways of formulating the same information need. Therefore, besides measuring performance on giving the right answers for a given question, their ability to match a given interaction with suitable questions in their knowledge base is often key, and should also be assessed. This is the case, for instance, of Frequently Asked Questions (FAQs) retrieval systems (Karan et al., 2013; Caputo et al., 2016), which retrieve relevant FAQs for interactions in natural language. This paper presents AIA-BDE, a corpus of FAQs, in Portuguese, and variations of their questions, which are reformulations that paraphrase or are entailed by the original questions, created automatically, with Google Translate, or manually, by volunteers. AIA-BDE was developed in the scope of a project that aims to develop more intelligent systems for supporting automatic assistance to entrepreneurs, using natural language (AIA: Apoio Inteligente a Empreendedores). In this project, different information r"
2020.lrec-1.669,P16-1094,0,0.0259364,"cantly different for different types. 2. Related Work Data-driven dialogue systems can learn from corpora like the Ubuntu Dialogue corpus (Lowe et al., 2015), Open Subtitles (Tiedemann, 2009) dialogues, or Twitter conversations (Ritter et al., 2011). Such systems (e.g., Vinyals and Le (2015)) are challenging to assess because they do not have clearly-defined goals. Towards their automatic evaluation, word-overlap metrics borrowed from machine translation (e.g., BLEU, METEOR) have been used to compare responses by the system with ground-truth responses produced by humans (Sordoni et al., 2015; Li et al., 2016). However, it is easy to understand that, due to the variety of valid responses in an open-domain conversation, such measures are not adequate, and it has been confirmed that they correlate very weakly with human judgements (Liu et al., 2016). An expensive alternative is to ask users to interact with the system and leave their feedback on how human and natural their conversation was. When it comes to task-oriented dialogue systems, users may also answer to what extent their task was successfully accomplished (Wen et al., 2017). In any case, subjects can be recruited via crowd-sourcing, but, in"
2020.lrec-1.669,D16-1230,0,0.0146723,"., 2011). Such systems (e.g., Vinyals and Le (2015)) are challenging to assess because they do not have clearly-defined goals. Towards their automatic evaluation, word-overlap metrics borrowed from machine translation (e.g., BLEU, METEOR) have been used to compare responses by the system with ground-truth responses produced by humans (Sordoni et al., 2015; Li et al., 2016). However, it is easy to understand that, due to the variety of valid responses in an open-domain conversation, such measures are not adequate, and it has been confirmed that they correlate very weakly with human judgements (Liu et al., 2016). An expensive alternative is to ask users to interact with the system and leave their feedback on how human and natural their conversation was. When it comes to task-oriented dialogue systems, users may also answer to what extent their task was successfully accomplished (Wen et al., 2017). In any case, subjects can be recruited via crowd-sourcing, but, in order to measure progress, this would have to be done for each update. Single-turn question-answering (QA) systems are a specific kind of task-oriented dialogue systems that allow users to search for information using natural language. Such"
2020.lrec-1.669,W15-4640,0,0.0692476,"Missing"
2020.lrec-1.669,mota-etal-2012-pagico,0,0.0332545,"related questions. Recently, datasets for question-answering within a dialogue context have also been developed, such as Choi et al. (2018) or Reddy et al. (2019), created both in a task where one person asks questions on a subject and the other answers, as naturally as possible, based on a text on the target subject. Specifically concerning Portuguese, corpora of subtitles have been used for conversational agents, to better deal with out-of-vocabulary interactions (Magarreiro et al., 2014); there are collections of factoid questions (Santos and Rocha, 2004; Magnini et al., 2004) and topics (Mota et al., 2012) with their answers, previously used in IR and QA shared tasks, and dense domain questions (Criscuolo et al., 2017), also with their answers; as well as collections with pairs of sentences with their semantic similarity and entailment label (Fonseca et al., 2016; Real et al., 2020). Yet, as far as we know, there is no corpus with Portuguese domainoriented questions and their variations, ready to be used in the evaluation of IR / QA dialogue systems. 3. Corpus Creation The starting point of the AIA-BDE corpus were several groups of FAQs associated to services of the former Balcão do Empreendedo"
2020.lrec-1.669,S15-2047,0,0.0314682,"ll as their paraphrases. Finally, for each query, the binary relevance was manually set for each FAQ retrieved by a set of standard retrieval models (keyword search, phrase search, TF.IDF, language modelling). QA4FAQ (Caputo et al., 2016) is a shared task on Question Answering for FAQs in Italian, organised in the scope of EVALITA 2016, with the goal of retrieving relevant FAQs for a given user query. It relied on 406 FAQs (question, answer, tags), 1,132 user queries collected from the logs of an IR system, and a set of mappings between queries and relevant FAQs. Community Question Answering (Nakov et al., 2015; Nakov et al., 2016; Nakov et al., 2017) is a related task, with the goal of ranking question-comment and question-question similarity in web forums. As data source, its editions have used questions from the Qatar Living forum. Starting with a list of original questions, related questions were obtained for each, together with the first comments in their threads. Relevance was annotated for related questions to the original question, and for comments, to related questions and to the original question, though different annotations were used in different subtasks. Queries were then generated fro"
2020.lrec-1.669,S16-1083,0,0.0575867,"Missing"
2020.lrec-1.669,N18-1202,0,0.00942989,"ossible applications of the corpus. In the first experiment, several unsupervised approaches were used for matching variations with their original question. The second experiment targeted the automatic identification of the source of a variation. 4.1. Matching Variations to Original Questions To better understand the challenge underlying the matching of variations with their original questions, we applied the following unsupervised approaches for this purpose: 2 https://cloud.google.com/translate/docs/ – FastText (Bojanowski et al., 2017) word embeddings, pre-trained for Portuguese5 ; – ELMo (Peters et al., 2018) contextual word embeddings, pre-trained for Portuguese6 (Gardner et al., 2018); – BERT (Devlin et al., 2019) contextual word embeddings, pre-trained for multiple languages7 . The application of Chatterbot followed two simple steps: (i) training with all the original questions and answers; (ii) checking the responses given for each variation. Since the default implementation only provides an answer for a given interaction, we only compute its accuracy for the first hit. For the remaining approaches, we also compute accuracy for the presence of the correct answer in the top-3 or top-5 best rank"
2020.lrec-1.669,Q19-1016,0,0.0143939,"vance was annotated for related questions to the original question, and for comments, to related questions and to the original question, though different annotations were used in different subtasks. Queries were then generated from the subject of each original question and Google used for collecting up to 200 question-comment threads in the forum site. Results with ten or more comments and questions with less than 2,000 characters were considered to be related questions. Recently, datasets for question-answering within a dialogue context have also been developed, such as Choi et al. (2018) or Reddy et al. (2019), created both in a task where one person asks questions on a subject and the other answers, as naturally as possible, based on a text on the target subject. Specifically concerning Portuguese, corpora of subtitles have been used for conversational agents, to better deal with out-of-vocabulary interactions (Magarreiro et al., 2014); there are collections of factoid questions (Santos and Rocha, 2004; Magnini et al., 2004) and topics (Mota et al., 2012) with their answers, previously used in IR and QA shared tasks, and dense domain questions (Criscuolo et al., 2017), also with their answers; as"
2020.lrec-1.669,D11-1054,0,0.0553091,"Missing"
2020.lrec-1.669,L18-1513,0,0.0439355,"Missing"
2020.lrec-1.669,N15-1020,0,0.0429713,"Missing"
2020.lrec-1.669,E17-1042,0,0.0122667,"ith ground-truth responses produced by humans (Sordoni et al., 2015; Li et al., 2016). However, it is easy to understand that, due to the variety of valid responses in an open-domain conversation, such measures are not adequate, and it has been confirmed that they correlate very weakly with human judgements (Liu et al., 2016). An expensive alternative is to ask users to interact with the system and leave their feedback on how human and natural their conversation was. When it comes to task-oriented dialogue systems, users may also answer to what extent their task was successfully accomplished (Wen et al., 2017). In any case, subjects can be recruited via crowd-sourcing, but, in order to measure progress, this would have to be done for each update. Single-turn question-answering (QA) systems are a specific kind of task-oriented dialogue systems that allow users to search for information using natural language. Such systems typically target factoid questions (Voorhees, 2008) and rely on Information Retrieval (IR) techniques (Kolomiyets and Moens, 2011), which means that they can also be assessed with IR-based measures, namely the precision and the recall of the given answers, according to a ground-tru"
2021.acl-demo.9,2020.eamt-1.29,0,0.0159885,"0.5154 on the isolated sub-sample. In Table 1 we illustrate an example of a translation in which the Online-G system produces critical errors as a consequence of translating named entities incorrectly, specifically isolated by the DCF feature. 3.2 DCF: Terminology Similarly to named entities, enforcing that MT systems use specific terminology during translation is a challenging task with particular relevance in commercial use cases. Measuring terminology adherence typically involves relying on automated metrics for MT as well as measuring the accuracy of terminology output (Dinu et al., 2019; Exel et al., 2020). This approach presents two concrete problems: a) applying terminology constraints typically results in only minimal variance between translations, which limits the utility of using automated metrics at the corpus level; and b) measuring accuracy in terminology usage typically relies on exact string matching between a translation hypothesis and its respective reference, which implies that properly inflected translated terms often do not receive proper credit. MT-T ELESCOPE offers a DCF Terminology feature which allows a user to optionally upload a glossary by which to isolate a corresponding"
2021.acl-demo.9,aziz-etal-2012-pet,0,0.0337118,"mparative analysis of segment-level errors with highlighting of variant n-grams. The tool also provides some limited aggregate analysis. Both of the above tools also offer statistical significance testing in the form of a bootstrapped t-test. V IZ S EQ (Wang et al., 2019), whilst only tangentially related, is one of the only comparative tools that offers a web-based interface. Moreover, V IZ S EQ has impressive coverage in terms of Natural Language Generation metrics. However, V IZ S EQ was developed for multi-model comparison and is primarily focused at corpus-level. Other tools such as PET (Aziz et al., 2012) and A P PRAISE (Federmann, 2012) are complementary to MT-T ELESCOPE in that they offer features which leverage annotation and post-edition. DCF: Duplication The removal of duplicates can be particularly important in situations where the test corpus sample contains repetition. Repeated segments in a test sample can artificially inflate the corpus-level score, particularly where that score results from an average of segment-level scores. Whilst we acknowledge that removal of duplicate segments is fairly common in public data sets such as that used in the WMT Shared Tasks and consequently our ex"
2021.acl-demo.9,W17-3204,0,0.0148807,"ynamic Corpus Filtering feature (DCF) updates the output evaluation in real-time to allow the user to ‘zoom in’ on relevant data points. Currently, MT-T ELESCOPE supports filtering by named entity, glossary and source segment length, as well as an option to remove duplicates. Whenever any of these options is selected, the interface will output the size of the sub-sample as a percentage of the original test corpus. 3.1 DCF: Named Entities Successful rendering of named entities is a known challenge for even modern MT systems and can lead to distortion of locations, organization and other names (Koehn and Knowles, 2017; Modrzejewski et al., 2020). Recently, several methods have been proposed to improve the translation 76 Table 1: Example of named entity errors produced Online-G system in comparison to the PROMT system from the WMT20 shared task. C OMET Source Online-G PROMT Reference Маругов врезался на мотоцикле в такси, которым управлял Акбаров. Murugov crashed into a motorcycle taxi, which was ruled by Akbar. Marugov crashed into a taxi driven by Akbarov on a motorcycle. Marugov crashed on a motorcyle into the taxi Akbarov was driving. of named entities in Neural Machine Translation (NMT) (Sennrich and H"
2021.acl-demo.9,2020.eamt-1.24,0,0.0270101,"visualizations are dynamic, interactive and highly customizable. The tools have been built specifically with ease of use in mind, in the hope of expanding access to high quality MT evaluation. There is tremendous scope in the adaptation of the DCF framework to target many other phenomena and future work will be focused primarily in this area. We envisage for example adding filters for specific discourse phenomenon such as pronoun translation. Ideally such filter would allow researchers to measure context usage in NMT without having to rely only on contrastive evaluation (M¨uller et al., 2018; Lopes et al., 2020) and/or human evaluation. 78 We also plan to extend MT-T ELESCOPE to handle a (possibly empty) set of references. This will bring more flexibility to the tool allowing more informed decision when multiple references are available while also supporting Quality Estimation (Specia et al., 2018) when references are not available. Finally we hope to implement exporting functionality to allow saving of analysis output in commonly used formats (e.g. json and PDF). Given that MTT ELESCOPE is an open source platform, we are excited to encourage other users to contribute to its growth with suggestions a"
2021.acl-demo.9,W05-0909,1,0.364415,"stical significance as a means of evaluating the rigor of the resulting system ranking. MT-T ELESCOPE is open source1 , written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality. 1 Introduction When developing MT systems or comparing experiments across papers, it has been common practice for researchers and developers to rely on automated metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) as a means of quantifying the relative performance difference between two models. Commercial deployment of systems and the establishment of state-of-theart in academia is often driven by these metrics alone. Automated metrics have long been an essential means for assessing quality improvements 1 Code available at: https://github.com/ Unbabel/MT-Telescope and Demo video at: https://youtu.be/MZOe1yX8mII 73 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Sys"
2021.acl-demo.9,2020.wmt-1.77,0,0.0173901,"ely accessible framework that requires little technical skill to operate and exposes information about the critical differences between MT outputs that is interactive, informative and highly customizable. 2 MT-T ELESCOPE is opened in a web browser and takes four text (.txt) files as input; source and reference segments and one set of MT outputs for each of the compared systems. Users drag and drop these files directly onto the interface to begin evaluation. C OMET (Rei et al., 2020a) is provided as a default metric given its proven value in the WMT Metrics Shared Task 2020 (Rei et al., 2020b; Mathur et al., 2020). Optionally the user can choose an alternate metric using a selection box. Currently available metrics include B LEU, M ETEOR and CHR F, and a selection of more recently proposed metrics such as P RISM, B LEURT, and B ERT S CORE. 2.2 Visualizations High-level results of the analysis are output in table format with the corresponding system scores. MT-T ELESCOPE then exposes segment-level comparison in three primary visualizations: First, a bubble plot (Figure 1) where the position of bubbles show how scores between the two systems differ for each segment, notable differences being highlighted"
2021.acl-demo.9,2020.eamt-1.6,0,0.0327428,"Missing"
2021.acl-demo.9,P19-1294,0,0.0157087,"erformance -0.1799 0.5154 on the isolated sub-sample. In Table 1 we illustrate an example of a translation in which the Online-G system produces critical errors as a consequence of translating named entities incorrectly, specifically isolated by the DCF feature. 3.2 DCF: Terminology Similarly to named entities, enforcing that MT systems use specific terminology during translation is a challenging task with particular relevance in commercial use cases. Measuring terminology adherence typically involves relying on automated metrics for MT as well as measuring the accuracy of terminology output (Dinu et al., 2019; Exel et al., 2020). This approach presents two concrete problems: a) applying terminology constraints typically results in only minimal variance between translations, which limits the utility of using automated metrics at the corpus level; and b) measuring accuracy in terminology usage typically relies on exact string matching between a translation hypothesis and its respective reference, which implies that properly inflected translated terms often do not receive proper credit. MT-T ELESCOPE offers a DCF Terminology feature which allows a user to optionally upload a glossary by which to isol"
2021.acl-demo.9,2020.emnlp-main.8,0,0.0328337,"Missing"
2021.acl-demo.9,W18-6307,0,0.0607191,"Missing"
2021.acl-demo.9,C18-1274,0,0.0342668,"Missing"
2021.acl-demo.9,N19-4007,0,0.0921556,"inition of ‘improvement’ as an increase in a relevant corpus-level score is insufficient, especially when the relative difference between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of MT systems, such as C OMPARE -MT (Neubig et al., 2019) and MTC OMPAR E VAL (Klejch et al., 2015) and other more general comparative tools such as V IZ S EQ (Wang et al., 2019). Despite the intention of such tools in addressing the above problem, none have been widely adopted as a standard method of evaluating MT. MT-T ELESCOPE was specifically developed to leverage the best of existing approaches in a manner that is as user friendly as possible, with features specifically tailored to the MT use case. The platform supports fine-grained segment-level analysis and interactive visualisations that provide relevant and informative quality intelligence."
2021.acl-demo.9,D19-3043,0,0.0823159,"erence between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of MT systems, such as C OMPARE -MT (Neubig et al., 2019) and MTC OMPAR E VAL (Klejch et al., 2015) and other more general comparative tools such as V IZ S EQ (Wang et al., 2019). Despite the intention of such tools in addressing the above problem, none have been widely adopted as a standard method of evaluating MT. MT-T ELESCOPE was specifically developed to leverage the best of existing approaches in a manner that is as user friendly as possible, with features specifically tailored to the MT use case. The platform supports fine-grained segment-level analysis and interactive visualisations that provide relevant and informative quality intelligence. In particular, the platform also supports focused analWe present MT-T ELESCOPE, a visualization platform designed to fac"
2021.acl-demo.9,P02-1040,0,0.110593,"ides a bootstrapped t-test for statistical significance as a means of evaluating the rigor of the resulting system ranking. MT-T ELESCOPE is open source1 , written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality. 1 Introduction When developing MT systems or comparing experiments across papers, it has been common practice for researchers and developers to rely on automated metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) as a means of quantifying the relative performance difference between two models. Commercial deployment of systems and the establishment of state-of-theart in academia is often driven by these metrics alone. Automated metrics have long been an essential means for assessing quality improvements 1 Code available at: https://github.com/ Unbabel/MT-Telescope and Demo video at: https://youtu.be/MZOe1yX8mII 73 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confere"
2021.acl-demo.9,2020.acl-demos.14,0,0.0211026,"Missing"
2021.acl-demo.9,2020.emnlp-main.213,1,0.903593,"babel.com luisa.coheur@inesc-id.pt Abstract and driving progress in the field of MT. Recent state-of-the-art metrics such as C OMET (Rei et al., 2020a), P RISM (Thompson and Post, 2020), and B LEURT (Sellam et al., 2020), show much higher levels of correlation with human judgement than their predecessors. Notwithstanding the strength of available metrics, when applied and reported at corpus-level, they are only able to provide a general indication of whether one system is superior, based on a single score which in some cases is limited to an arithmetic mean of segment-level score predictions (Rei et al., 2020a). We contend that the broad definition of ‘improvement’ as an increase in a relevant corpus-level score is insufficient, especially when the relative difference between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of M"
2021.acl-demo.9,2020.wmt-1.101,1,0.867216,"babel.com luisa.coheur@inesc-id.pt Abstract and driving progress in the field of MT. Recent state-of-the-art metrics such as C OMET (Rei et al., 2020a), P RISM (Thompson and Post, 2020), and B LEURT (Sellam et al., 2020), show much higher levels of correlation with human judgement than their predecessors. Notwithstanding the strength of available metrics, when applied and reported at corpus-level, they are only able to provide a general indication of whether one system is superior, based on a single score which in some cases is limited to an arithmetic mean of segment-level score predictions (Rei et al., 2020a). We contend that the broad definition of ‘improvement’ as an increase in a relevant corpus-level score is insufficient, especially when the relative difference between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of M"
2021.acl-demo.9,2020.acl-main.704,0,0.0231012,"Missing"
2021.acl-demo.9,W16-2209,0,0.026699,"Missing"
2021.acl-long.242,W19-5305,0,0.0592947,"Missing"
2021.acl-long.242,W19-5307,0,0.0950242,"Missing"
2021.acl-long.242,W19-5313,0,0.0626081,"Missing"
2021.acl-long.242,E14-1042,0,0.0791377,"Missing"
2021.acl-long.242,W13-2305,0,0.0345996,"0.0 online-Y.0 online-B.0 NICT.6938 0 100 200 300 CAiRE.6949 online-G.0 400 500 online-A.0 lmu-unsup-nmt-de-cs.6845 600 700 800 900 1000 Iteration # NEU_KingSoft.6766 CUNI-Unsupervised-NER-post.6934 1100 1200 1300 1400 1500 Unsupervised.de-cs.6935 Unsupervised.de-cs.6929 1600 1700 1800 1900 Figure 6: Weight evolution per MT system when using EXP3 and human-zero as the loss function (de-cs), averaged across 10 runs (the error bars represent the weights’ variance across the 10 runs). puts of their systems that are then evaluated by a community of human evaluators using Direct Assessment scores (Graham et al., 2013). Thus, the winner is the system that achieves the highest average score. For WMT’19 (Barrault et al., 2019), most of the competing systems followed a Transformer architecture (Vaswani et al., 2017), with the main differences among them being: (i) whether they considered document-level or only sentencelevel information; (ii) whether they were trained only on the training data provided by the shared task, or on additional sources as well; (iii) whether they consisted of a single model or an ensemble. 3112 6.2 Online learning for Machine Translation There has been a number of online learning app"
2021.acl-long.242,W19-5321,0,0.0533845,"Missing"
2021.acl-long.242,W19-5302,0,0.0247199,"ing advantage of the human feedback available. Our experiments on WMT’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations. 1 Introduction In Machine Translation (MT), measuring the quality of a large amount of automatic translations can be a challenge. Automatic metrics like B LEU (Papineni et al., 2002) remain popular due to their fast and free computations. Yet, in the last few years we have seen that, as MT quality improves, automatic metrics become less reliable (Ma et al., 2019; Mathur et al., 2020). For example, in the Conference on Machine Translation (WMT)’19 News Translation shared task, the winning system according to human annotators was not even in the top-5 according to B LEU (Barrault et al., 2019). On the other hand, using human assessments can be expensive, especially when evaluating multiple systems. In a real world scenario, given an arbitrary number of MT systems, one would need to evaluate them individually to find the best systems for a given language pair. However, that requires a considerable effort and there may not be enough human annotators to e"
2021.acl-long.242,2020.acl-main.448,0,0.0431331,"the human feedback available. Our experiments on WMT’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations. 1 Introduction In Machine Translation (MT), measuring the quality of a large amount of automatic translations can be a challenge. Automatic metrics like B LEU (Papineni et al., 2002) remain popular due to their fast and free computations. Yet, in the last few years we have seen that, as MT quality improves, automatic metrics become less reliable (Ma et al., 2019; Mathur et al., 2020). For example, in the Conference on Machine Translation (WMT)’19 News Translation shared task, the winning system according to human annotators was not even in the top-5 according to B LEU (Barrault et al., 2019). On the other hand, using human assessments can be expensive, especially when evaluating multiple systems. In a real world scenario, given an arbitrary number of MT systems, one would need to evaluate them individually to find the best systems for a given language pair. However, that requires a considerable effort and there may not be enough human annotators to evaluate all the system"
2021.acl-long.242,W19-5335,0,0.0627536,"Missing"
2021.acl-long.242,2020.emnlp-main.213,1,0.903393,"trategies: • human-zero: If there is no human assessment for the current translation, a score of zero is returned (leading to an unchanged weight on that iteration); 3 Although we assume an absolute scale of scores in [0;100] in our experiments, our approach could be applied to any other level of granularity. 3108 • human-avg: If there is no human assessment for the current translation, the average of the previous scores received by the system behind that translation is returned as the current score; • human-comet: If there is no human assessment for the current translation, the C OMET score (Rei et al., 2020a) between the translation and the pair source/reference available in the corpus is returned as the current score. We pre-trained4 this automatic metric on the datasets of previous shared tasks (WMT’17 (Bojar et al., 2017) and WMT’18 (Bojar et al., 2018)). Thus, for most translations, it displays a small difference regarding the existing human scores (see Fig. 2 for the case of en-de). Moreover, this metric correlates better with ratings by professional translators than the WMT scores (Freitag et al., 2021). Probability density 2.5 2.0 1.5 1.0 0.5 0.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1."
2021.acl-long.242,2020.wmt-1.101,1,0.874395,"trategies: • human-zero: If there is no human assessment for the current translation, a score of zero is returned (leading to an unchanged weight on that iteration); 3 Although we assume an absolute scale of scores in [0;100] in our experiments, our approach could be applied to any other level of granularity. 3108 • human-avg: If there is no human assessment for the current translation, the average of the previous scores received by the system behind that translation is returned as the current score; • human-comet: If there is no human assessment for the current translation, the C OMET score (Rei et al., 2020a) between the translation and the pair source/reference available in the corpus is returned as the current score. We pre-trained4 this automatic metric on the datasets of previous shared tasks (WMT’17 (Bojar et al., 2017) and WMT’18 (Bojar et al., 2018)). Thus, for most translations, it displays a small difference regarding the existing human scores (see Fig. 2 for the case of en-de). Moreover, this metric correlates better with ratings by professional translators than the WMT scores (Freitag et al., 2021). Probability density 2.5 2.0 1.5 1.0 0.5 0.0 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1."
2021.acl-long.242,P16-1152,0,0.0120033,"ether they considered document-level or only sentencelevel information; (ii) whether they were trained only on the training data provided by the shared task, or on additional sources as well; (iii) whether they consisted of a single model or an ensemble. 3112 6.2 Online learning for Machine Translation There has been a number of online learning approaches applied to MT in the past, mainly in Interactive MT and/or post-editing MT systems. However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMart´ınez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al., 2017; Karimova et al., 2018; Peris and Casacuberta, 2019). Even in cases where the MT model is composed of several sub-models (e.g., Ortiz-Mart´ınez (2016)), the goal is to online learn each sub-model’s specific parameters (while our learning goal is the weights of each system in an ensemble). Another key difference between these approaches and ours is that most of them use human post-edited translations as a source of feedback. The exceptions to this are the systems competing for W"
2021.acl-long.242,W17-4756,0,0.0575735,"Missing"
costa-etal-2012-english,P02-1040,0,\N,Missing
costa-etal-2012-english,P07-2045,0,\N,Missing
costa-etal-2012-english,J03-1002,0,\N,Missing
costa-etal-2012-english,2005.mtsummit-papers.11,0,\N,Missing
costa-etal-2012-english,vilar-etal-2006-error,0,\N,Missing
costa-etal-2012-english,mendes-etal-2010-named,1,\N,Missing
costa-etal-2014-translation,2005.eamt-1.13,0,\N,Missing
costa-etal-2014-translation,C02-1150,0,\N,Missing
costa-etal-2014-translation,J03-1002,0,\N,Missing
costa-etal-2014-translation,costa-etal-2012-english,1,\N,Missing
costa-etal-2014-translation,vilar-etal-2006-error,0,\N,Missing
curto-etal-2014-just,N07-1051,0,\N,Missing
curto-etal-2014-just,W04-3251,0,\N,Missing
curto-etal-2014-just,C02-1150,0,\N,Missing
curto-etal-2014-just,P01-1037,0,\N,Missing
curto-etal-2014-just,P06-1051,0,\N,Missing
curto-etal-2014-just,P06-1063,0,\N,Missing
fialho-etal-2012-extending,W97-0108,0,\N,Missing
fialho-etal-2012-extending,pazienza-etal-2008-jmwnl,0,\N,Missing
fialho-etal-2012-extending,buscaldi-rosso-2008-geo,0,\N,Missing
fialho-etal-2012-extending,W04-2214,0,\N,Missing
fialho-etal-2012-extending,pazienza-etal-2008-bottom,0,\N,Missing
fialho-etal-2012-extending,J06-1001,0,\N,Missing
graca-etal-2008-building,N04-1035,0,\N,Missing
graca-etal-2008-building,J90-2002,0,\N,Missing
graca-etal-2008-building,P04-1023,0,\N,Missing
graca-etal-2008-building,J04-4002,0,\N,Missing
graca-etal-2008-building,N03-1017,0,\N,Missing
graca-etal-2008-building,2005.mtsummit-papers.11,0,\N,Missing
graca-etal-2008-building,kruijff-korbayova-etal-2006-annotation,0,\N,Missing
graca-etal-2008-building,P00-1056,0,\N,Missing
K19-1054,D08-1035,0,0.503268,"as topic modeling approaches such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), but here the inherent topics are constrained to the linear discourse structure. To model interactions between lexical distributions, we use a dynamic prior, which assumes that the word probabilities change smoothly across topics. To model segment length characteristics, we assign prior variables conditioned on document modality. The linear segmentation constraint has been used to make inference tractable by exhaustively exploring the segmentation space to obtain the exact maximum-likelihood estimation (Eisenstein and Barzilay, 2008). Given a multi-document setting, this is not feasible, as segments can share topics. We address this issue using a beam search algorithm, which allows the inference procedure to recover from early mistakes. In our experiments, we show that BeamSeg is able to perform well when segmenting learning materials, where previously single-document models obtained better results (Mota et al., 2018). We also observe that topic identification is more accurately determined in a joint model, as opposed to a pipeline approach (performing the tasks sequentially), indicating that both problems should be model"
K19-1054,P13-1167,0,0.0189886,"s’ subject with two annotators. A 0.69 Fleiss-kappa (Shrout and Fleiss, 1979) agreement value was obtained, showing that annotators had a similar perception of whether segments share the same topic. Most of the disagreement cases are due to considering textual and plot-based explanations as different topics. N −k X 1 WD = |ref − hyp |= 6 0, N −k where N is the length of the document and k the window size. WD is a penalty score between 0 (the best value) and 1. For consistency, we take the output segmentations from all systems and evaluate it using the same software (the python module segeval (Fournier, 2013)). The WD average results for the baseline are in Table 1. In the Biography dataset, MultiSeg is the best performing model, improving the WD of Bayesseg-MD by 0.05. In the AVL dataset, the best results are obtained by Bayesseg-MD. The difference to the second best result, Bayesseg, is 0.02. For the Physics dataset, the single-document model Bayesseg achieves the best results with a WD difference of 0.01. These results show that the performance of the algorithms varies across the different datasets. This suggests that the different modeling approaches do not generalize well to the different cha"
K19-1054,P03-1071,0,0.358352,"our contributions as follows: 2 Related Work Following the lexical cohesion theory, segmentation algorithms identify spans of text with prominent vocabulary changes. The main difference between algorithms is how lexical cohesion is implemented: some resort to lexical similarity; the remaining follow a probabilistic approach. Lexical approaches rely on a similarity metric between sentences, usually the cosine. A classic method is TextTiling (Hearst, 1997), which assumes that topic boundaries are found in consecutive sentences with a low similarity value; several other works built on this idea (Galley et al., 2003; Balagopalan et al., 2012). C99 (Choi, 2000) is another lexical approach, and uses a similarity matrix in a divisive clustering to obtain segments. MinCut (Malioutov and Barzilay, 2006) casts segmentation in a minimum cut graph partitioning problem. The graph has a node for each sentence; edges are weighted using lexical similarity. Long-distance textual relationships are modeled by connecting all sentences. Affinity Propagation Segmentation (Kazantseva and Szpakowicz, 2011) also models such relationships but uses affinity propagation clustering (Frey and Dueck, 2007). The algorithm creates a"
K19-1054,Q19-1011,0,0.0175104,"ximizes the segment similarity sum function. Alemi and Ginsparg (2015) proposed the Content Vector Segmentation (CVS) sentence vector representation based on segment word embeddings. Using this representation in C99 improves bag-ofwords results. In another line of research, Wang et al. (2017) combined learning to rank and a convolutional neural network to learn a coherence function between text pairs; higher-ranked pairs are likely to be segments. Despite a promising approach, stateof-the-art results were not achieved. Also following an approach using neural networks, is the SECTOR algorithm (Arnold et al., 2019), which uses a topic embedding trained based on utterance topic classification. Following the network architecture from (Koshorek et al., 2018), two stacked LSTM layers are used to decode word embedding representation of utterances. To recover segmentation, a TextTiling approach is applied to the topic embedding layer. The evaluation results show that SECTOR is able to improve a C99 baseline. • A novel joint model for topic segmentation and identification with a dynamic prior. • An inference procedure based on a beam search algorithm. • A study on how different modality-based segment length pr"
K19-1054,J97-1003,0,0.866503,"model, as opposed to a pipeline approach (performing the tasks sequentially), indicating that both problems should be modeled simultaneously. We summarize our contributions as follows: 2 Related Work Following the lexical cohesion theory, segmentation algorithms identify spans of text with prominent vocabulary changes. The main difference between algorithms is how lexical cohesion is implemented: some resort to lexical similarity; the remaining follow a probabilistic approach. Lexical approaches rely on a similarity metric between sentences, usually the cosine. A classic method is TextTiling (Hearst, 1997), which assumes that topic boundaries are found in consecutive sentences with a low similarity value; several other works built on this idea (Galley et al., 2003; Balagopalan et al., 2012). C99 (Choi, 2000) is another lexical approach, and uses a similarity matrix in a divisive clustering to obtain segments. MinCut (Malioutov and Barzilay, 2006) casts segmentation in a minimum cut graph partitioning problem. The graph has a node for each sentence; edges are weighted using lexical similarity. Long-distance textual relationships are modeled by connecting all sentences. Affinity Propagation Segme"
K19-1054,A00-2004,0,0.816428,"g the lexical cohesion theory, segmentation algorithms identify spans of text with prominent vocabulary changes. The main difference between algorithms is how lexical cohesion is implemented: some resort to lexical similarity; the remaining follow a probabilistic approach. Lexical approaches rely on a similarity metric between sentences, usually the cosine. A classic method is TextTiling (Hearst, 1997), which assumes that topic boundaries are found in consecutive sentences with a low similarity value; several other works built on this idea (Galley et al., 2003; Balagopalan et al., 2012). C99 (Choi, 2000) is another lexical approach, and uses a similarity matrix in a divisive clustering to obtain segments. MinCut (Malioutov and Barzilay, 2006) casts segmentation in a minimum cut graph partitioning problem. The graph has a node for each sentence; edges are weighted using lexical similarity. Long-distance textual relationships are modeled by connecting all sentences. Affinity Propagation Segmentation (Kazantseva and Szpakowicz, 2011) also models such relationships but uses affinity propagation clustering (Frey and Dueck, 2007). The algorithm creates a factor graph and maximizes the segment simil"
K19-1054,D11-1026,0,0.0265474,"ssumes that topic boundaries are found in consecutive sentences with a low similarity value; several other works built on this idea (Galley et al., 2003; Balagopalan et al., 2012). C99 (Choi, 2000) is another lexical approach, and uses a similarity matrix in a divisive clustering to obtain segments. MinCut (Malioutov and Barzilay, 2006) casts segmentation in a minimum cut graph partitioning problem. The graph has a node for each sentence; edges are weighted using lexical similarity. Long-distance textual relationships are modeled by connecting all sentences. Affinity Propagation Segmentation (Kazantseva and Szpakowicz, 2011) also models such relationships but uses affinity propagation clustering (Frey and Dueck, 2007). The algorithm creates a factor graph and maximizes the segment similarity sum function. Alemi and Ginsparg (2015) proposed the Content Vector Segmentation (CVS) sentence vector representation based on segment word embeddings. Using this representation in C99 improves bag-ofwords results. In another line of research, Wang et al. (2017) combined learning to rank and a convolutional neural network to learn a coherence function between text pairs; higher-ranked pairs are likely to be segments. Despite"
K19-1054,N13-1019,0,0.509497,"words are assigned to topics such that probability mass is distributed on a small set of topically relevant words. In order to adapt this idea to segmentation, the model needs to be able to determine if sentences belong to the same topic (or mixture of topics). An example of such adaptation is the single-document segmentation model PLDA (Purver et al., 2006), where topic proportions are shared by sentences within the same segment. Segmentation is then determined through a binary topic shift sentence variable. Models such as TopicTiling (Riedl and Biemann, 2012), Structured Topic Model (STM) (Du et al., 2013), and NTSeg (Jameel and Lam, 2013) extend this LDA-based approach to segmentation. In all these approaches, topic identification is not possible since all segments are a mixture of topics. 3 In this paper, we adopt a probabilistic multidocument view on segmentation. Only two other models follow this approach: MultiSeg (Jeong and Titov, 2010) and Bayesseg-MD (Mota et al., 2016). MultiSeg uses a two-level LDA model where documents are generated using local and global topics. Local topics are specific to a document; global topics are shared between documents. Documents are mixtures of topics, but"
K19-1054,N18-2075,0,0.254601,"ntation based on segment word embeddings. Using this representation in C99 improves bag-ofwords results. In another line of research, Wang et al. (2017) combined learning to rank and a convolutional neural network to learn a coherence function between text pairs; higher-ranked pairs are likely to be segments. Despite a promising approach, stateof-the-art results were not achieved. Also following an approach using neural networks, is the SECTOR algorithm (Arnold et al., 2019), which uses a topic embedding trained based on utterance topic classification. Following the network architecture from (Koshorek et al., 2018), two stacked LSTM layers are used to decode word embedding representation of utterances. To recover segmentation, a TextTiling approach is applied to the topic embedding layer. The evaluation results show that SECTOR is able to improve a C99 baseline. • A novel joint model for topic segmentation and identification with a dynamic prior. • An inference procedure based on a beam search algorithm. • A study on how different modality-based segment length priors influence segmentation. 583 Just as we introduced average velocity we will now describe average acceleration. Notice when velocity changes"
K19-1054,N09-1040,0,0.858066,"cceleration need not coincide. Figure 1: Examples of segment excerpts from video, slide presentation, and PDF documents describing the accelaration topic. Words in bold depict shared vocabulary across segments. from the same segment are assigned the same topic. The inference procedure affords an exact maximum-likelihood estimation by exploring the segmentation space with a dynamic programming algorithm. This approach cannot be applied to multi-document segmentation since the hidden topic variables are integrated out; other single-document models following this approach also have this problem (Eisenstein, 2009; Malmasi et al., 2017). Bayesseg-MD sidesteps this problem by using lexically similar sentences from other documents. The word counts of such sentences are added to the segment likelihood estimation to reduce data sparseness. Despite using all documents for segment likelihood estimations, topic identification is not available. In this paper, we address these issues by designing an inference algorithm that explicitly tracks segment topic assignments. Probabilistic approaches to segmentation follow a setup similar to the LDA model: words are assigned to topics such that probability mass is dist"
K19-1054,P06-1004,0,0.182693,"ifference between algorithms is how lexical cohesion is implemented: some resort to lexical similarity; the remaining follow a probabilistic approach. Lexical approaches rely on a similarity metric between sentences, usually the cosine. A classic method is TextTiling (Hearst, 1997), which assumes that topic boundaries are found in consecutive sentences with a low similarity value; several other works built on this idea (Galley et al., 2003; Balagopalan et al., 2012). C99 (Choi, 2000) is another lexical approach, and uses a similarity matrix in a divisive clustering to obtain segments. MinCut (Malioutov and Barzilay, 2006) casts segmentation in a minimum cut graph partitioning problem. The graph has a node for each sentence; edges are weighted using lexical similarity. Long-distance textual relationships are modeled by connecting all sentences. Affinity Propagation Segmentation (Kazantseva and Szpakowicz, 2011) also models such relationships but uses affinity propagation clustering (Frey and Dueck, 2007). The algorithm creates a factor graph and maximizes the segment similarity sum function. Alemi and Ginsparg (2015) proposed the Content Vector Segmentation (CVS) sentence vector representation based on segment"
K19-1054,P17-1134,0,0.248385,"ot coincide. Figure 1: Examples of segment excerpts from video, slide presentation, and PDF documents describing the accelaration topic. Words in bold depict shared vocabulary across segments. from the same segment are assigned the same topic. The inference procedure affords an exact maximum-likelihood estimation by exploring the segmentation space with a dynamic programming algorithm. This approach cannot be applied to multi-document segmentation since the hidden topic variables are integrated out; other single-document models following this approach also have this problem (Eisenstein, 2009; Malmasi et al., 2017). Bayesseg-MD sidesteps this problem by using lexically similar sentences from other documents. The word counts of such sentences are added to the segment likelihood estimation to reduce data sparseness. Despite using all documents for segment likelihood estimations, topic identification is not available. In this paper, we address these issues by designing an inference algorithm that explicitly tracks segment topic assignments. Probabilistic approaches to segmentation follow a setup similar to the LDA model: words are assigned to topics such that probability mass is distributed on a small set"
K19-1054,D17-1139,0,0.0198192,"s are weighted using lexical similarity. Long-distance textual relationships are modeled by connecting all sentences. Affinity Propagation Segmentation (Kazantseva and Szpakowicz, 2011) also models such relationships but uses affinity propagation clustering (Frey and Dueck, 2007). The algorithm creates a factor graph and maximizes the segment similarity sum function. Alemi and Ginsparg (2015) proposed the Content Vector Segmentation (CVS) sentence vector representation based on segment word embeddings. Using this representation in C99 improves bag-ofwords results. In another line of research, Wang et al. (2017) combined learning to rank and a convolutional neural network to learn a coherence function between text pairs; higher-ranked pairs are likely to be segments. Despite a promising approach, stateof-the-art results were not achieved. Also following an approach using neural networks, is the SECTOR algorithm (Arnold et al., 2019), which uses a topic embedding trained based on utterance topic classification. Following the network architecture from (Koshorek et al., 2018), two stacked LSTM layers are used to decode word embedding representation of utterances. To recover segmentation, a TextTiling ap"
K19-1054,W09-3027,0,0.0486669,"Missing"
K19-1054,J02-1002,0,0.588749,"nt prior configurations are in Table 2. In the table, the LMP and SLP columns correspond to the language model and segment length priors. In the Biography dataset, we can see that using the dynamic LMP instead of the independent improves the the Beta-Bernoulli and Gamma-Poisson results by 0.01 and 0.09, respectively. In the AVL dataset, the dynamic LMP improves the best WD results of the independent LMP by 0.02. When comparing the scope results of the dynamic LMP in the AVL dataset, we observe further improvements when To measure performance, we use the standard Window Difference (WD) metric (Pevzner and Hearst, 2002). WD slides a window through a document and penalizes segmentations according to the difference between the number of expected segment boundaries and the predicted ones. This gives partial credit to near-miss situations. The 587 Table 3: Number of exact segment boundary matches between hypothesis and reference segmentations. Table 2: BeamSeg average WD results. The SLP column depicts the Beta-Bernoulli (BB), and GammaPoisson (GP) distributions. The scope indicates if the SLP is modality-based (M) or if there is one variable for the whole dataset (D). The Biography dataset has one modality, and"
K19-1054,P06-1003,0,0.0988903,"Missing"
K19-1054,W12-3307,0,0.0945838,"to segmentation follow a setup similar to the LDA model: words are assigned to topics such that probability mass is distributed on a small set of topically relevant words. In order to adapt this idea to segmentation, the model needs to be able to determine if sentences belong to the same topic (or mixture of topics). An example of such adaptation is the single-document segmentation model PLDA (Purver et al., 2006), where topic proportions are shared by sentences within the same segment. Segmentation is then determined through a binary topic shift sentence variable. Models such as TopicTiling (Riedl and Biemann, 2012), Structured Topic Model (STM) (Du et al., 2013), and NTSeg (Jameel and Lam, 2013) extend this LDA-based approach to segmentation. In all these approaches, topic identification is not possible since all segments are a mixture of topics. 3 In this paper, we adopt a probabilistic multidocument view on segmentation. Only two other models follow this approach: MultiSeg (Jeong and Titov, 2010) and Bayesseg-MD (Mota et al., 2016). MultiSeg uses a two-level LDA model where documents are generated using local and global topics. Local topics are specific to a document; global topics are shared between"
L16-1044,W07-0718,0,0.381259,"ered by learners (e.g., omission, addition, misformation, and misordering). These two approaches are originally presented as alternative taxonomies, although we found work in the literature pointing to the advantages on their combination (James, 1998). In this work, we extend and unify these approaches in a taxonomy shown on Section 3.3.. Regarding error gravity, i.e. how different errors impact translation quality, literature focuses on two different five point scales that judge fluency (native-like performance) and adequacy (how much of the original meaning is expressed in the translation) (Callison-Burch et al., 2007; Koehn and Monz, 2006). When judging fluency, a 5 is at288 tributed to flawless English and 1 to incomprehensible English. The best score for adequacy is 5 and this means that the whole meaning of the reference is also expressed in the translation, while 1 means that none of the original meaning was kept. However, it seems that people have difficulty in evaluating these two aspects of language separately. Also (Callison-Burch et al., 2007) points out to the difficulty of defining objective scoring guidelines, for example, how many grammatical errors (or what sort) distinguish between the diff"
L16-1044,J07-2003,0,0.0598486,"grammatical). Contrastingly, we believe that creating a corpus annotated with errors and translation quality, will allow to find out these weights empirically, observing from data which errors most affect comprehension. 3. 3.2. Translation tools For our experiments we used 4 different MT systems. Two are mainstream online translation systems3 : Google Translate4 and Systran5 (further referred to as G and S, respectively). The other two are in-house MT systems, both trained using Moses. One of them uses a phrase-based model (Koehn et al., 2007) and the other an hierarchical phrase-based model (Chiang, 2007) (further PSMT and HSMT). Both in-house systems share the same training, approximately 2 million sentence pairs from Europarl (Koehn, 2005). 3.3. Error Annotation Having decided on the corpora (see Section 3.1.) and MT systems to use (see Section 3.2.), we generated the corresponding translations. Given that we had 3 different sources of data (each with 25 sentences) and 4 MT systems, the final set of translations is composed of 300 sentences. This material was annotated by a linguist with the errors represented in figure 1. Corpus In this section, we start by describing the selection of the c"
L16-1044,costa-etal-2012-english,1,0.866831,"Missing"
L16-1044,2013.mtsummit-wptp.8,0,0.0300966,"When judging fluency, a 5 is at288 tributed to flawless English and 1 to incomprehensible English. The best score for adequacy is 5 and this means that the whole meaning of the reference is also expressed in the translation, while 1 means that none of the original meaning was kept. However, it seems that people have difficulty in evaluating these two aspects of language separately. Also (Callison-Burch et al., 2007) points out to the difficulty of defining objective scoring guidelines, for example, how many grammatical errors (or what sort) distinguish between the different levels. Finally, (Daems et al., 2013)’s work merges both error classification and error gravity, allowing to assign weights to each type of error. Herein, weights should be hand-tuned by the user so that problems that have a larger impact on comprehension receive a higher weight. For example, for the task of translating terminology texts, the impact of a lexical error can be set up as more severe than other types of errors (such as grammatical). Contrastingly, we believe that creating a corpus annotated with errors and translation quality, will allow to find out these weights empirically, observing from data which errors most aff"
L16-1044,W06-3114,0,0.0437119,"ion, addition, misformation, and misordering). These two approaches are originally presented as alternative taxonomies, although we found work in the literature pointing to the advantages on their combination (James, 1998). In this work, we extend and unify these approaches in a taxonomy shown on Section 3.3.. Regarding error gravity, i.e. how different errors impact translation quality, literature focuses on two different five point scales that judge fluency (native-like performance) and adequacy (how much of the original meaning is expressed in the translation) (Callison-Burch et al., 2007; Koehn and Monz, 2006). When judging fluency, a 5 is at288 tributed to flawless English and 1 to incomprehensible English. The best score for adequacy is 5 and this means that the whole meaning of the reference is also expressed in the translation, while 1 means that none of the original meaning was kept. However, it seems that people have difficulty in evaluating these two aspects of language separately. Also (Callison-Burch et al., 2007) points out to the difficulty of defining objective scoring guidelines, for example, how many grammatical errors (or what sort) distinguish between the different levels. Finally,"
L16-1044,P07-2045,0,0.00418057,"rror can be set up as more severe than other types of errors (such as grammatical). Contrastingly, we believe that creating a corpus annotated with errors and translation quality, will allow to find out these weights empirically, observing from data which errors most affect comprehension. 3. 3.2. Translation tools For our experiments we used 4 different MT systems. Two are mainstream online translation systems3 : Google Translate4 and Systran5 (further referred to as G and S, respectively). The other two are in-house MT systems, both trained using Moses. One of them uses a phrase-based model (Koehn et al., 2007) and the other an hierarchical phrase-based model (Chiang, 2007) (further PSMT and HSMT). Both in-house systems share the same training, approximately 2 million sentence pairs from Europarl (Koehn, 2005). 3.3. Error Annotation Having decided on the corpora (see Section 3.1.) and MT systems to use (see Section 3.2.), we generated the corresponding translations. Given that we had 3 different sources of data (each with 25 sentences) and 4 MT systems, the final set of translations is composed of 300 sentences. This material was annotated by a linguist with the errors represented in figure 1. Corpu"
L16-1044,2005.mtsummit-papers.11,0,0.0203636,"eights empirically, observing from data which errors most affect comprehension. 3. 3.2. Translation tools For our experiments we used 4 different MT systems. Two are mainstream online translation systems3 : Google Translate4 and Systran5 (further referred to as G and S, respectively). The other two are in-house MT systems, both trained using Moses. One of them uses a phrase-based model (Koehn et al., 2007) and the other an hierarchical phrase-based model (Chiang, 2007) (further PSMT and HSMT). Both in-house systems share the same training, approximately 2 million sentence pairs from Europarl (Koehn, 2005). 3.3. Error Annotation Having decided on the corpora (see Section 3.1.) and MT systems to use (see Section 3.2.), we generated the corresponding translations. Given that we had 3 different sources of data (each with 25 sentences) and 4 MT systems, the final set of translations is composed of 300 sentences. This material was annotated by a linguist with the errors represented in figure 1. Corpus In this section, we start by describing the selection of the corpus (Section 3.1.) and MT systems used to translate it (Section 3.2.). In Section 3.3., we describe the task of annotating the errors obt"
L16-1044,C02-1150,0,0.00920843,"on, we start by describing the selection of the corpus (Section 3.1.) and MT systems used to translate it (Section 3.2.). In Section 3.3., we describe the task of annotating the errors obtained after translation. Lastly, in Section 3.4., we outline the quality assessment process. 3.1. Text sources The corpus is composed of 300 sentence pairs (source/translation) evenly distributed (25 pairs each) between (a) TED-talks transcriptions (and respective EP subtitles)1 , (b) texts from the bilingual Portuguese national airline company magazine UP2 , and (c) the translated TREC evaluation questions (Li and Roth, 2002; Costa et al., 2012). The TED corpus is composed of TED talk subtitles and corresponding EP translations. These were created by volunteers and are available at the TED website. As we are dealing with subtitles (and not transcriptions), content is aligned to fit the screen, and, thus, some pre-processing was needed. Therefore, we manually connected the segments in order to obtain parallel sentences. The TAP corpus is constituted of 51 editions of the bilingual Portuguese national airline company magazine, divided into 2 100 files for EN and EP. It has almost 32 000 aligned sentences and a tota"
L16-1044,2005.eamt-1.13,0,0.0124601,"rror annotation and translation quality. Section 3. describes the text sources, the tools and the annotation process (errors and quality). Section 4. addresses error gravity, showing the classification results. Finally, in Section 5., we highlight the main conclusions and point to future work directions. Related work Several studies have been developed with the goal of classifying translation errors and different taxonomies have been suggested. One of the most used in MT is the hierarchical classification proposed by (Vilar et al., 2006), followed up by (Bojar, 2011). They extend the work of (Llitjs et al., 2005) and split errors into five categories: Missing Words (when some of the words are omitted in the translation), Word Order (when the words in the target sentence are wrongly positioned), Incorrect Words (when the system is unable to find the correct translation for a given word), Unknown Words (when an item is simply copied from the input word to the generated sentence, without being translated), and Punctuation. Another approach, this time for human errors, comes from (H. Dulay and Krashen, 1982). They suggest two major descriptive error taxonomies: the Linguistic Category Classification (LCC)"
L16-1044,vilar-etal-2006-error,0,0.103642,"Missing"
mendes-etal-2010-named,U06-1009,0,\N,Missing
mendes-etal-2010-named,C02-1150,0,\N,Missing
mendes-etal-2010-named,W03-0419,0,\N,Missing
P11-2079,P08-1115,0,0.061136,"IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propose two methods for generating th"
P11-2079,P08-1112,1,0.800281,"Missing"
P11-2079,P07-2045,0,0.00398059,"nd d) are classified as discontinuous. given by: P (p, mono) = C(mono) C(mono)+C(swap)+C(disc) (1) Where C(o) is the number of times a phrase is extracted with the orientation o in that group of phrase pairs. Moses also provides many options for this stage, such as types of smoothing. We use the default smoothing configuration which adds the fixed value of 0.5 to all C(o). 3 • The orientation is monotonous if only the previous word in the source is aligned with the previous word in the target, or, more formally, if n−1 an−1 / A. i−1 ∈ A ∧ aj+1 ∈ source phrase b) prev word(t) MSD models Moses (Koehn et al., 2007) allows many configurations for the reordering model to be used. In this work, we will only refer to the default configuration (msd-bidirectional-fe), which uses the MSD model, and calculates the reordering orientation for the previous and the next word, for each phrase pair. Other possible configurations are simpler than the default one. For instance, the monotonicity model only considers monotone and non-monotone orientation types, whereas the MSD model also considers the monotone orientation type, but distinguishes the non-monotone orientation type between swap and discontinuous. The approa"
P11-2079,D09-1106,0,0.150493,"Missing"
P11-2079,P08-1023,0,0.0774305,"matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propo"
P11-2079,2010.iwslt-evaluation.1,0,0.122315,"Missing"
P11-2079,J10-3007,1,0.84267,"Missing"
P11-2079,2006.iwslt-papers.7,0,0.0248528,"istical phrase-based systems (Koehn et al., 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al., 2010). The basic phrase extraction algorithm uses word alignment information to constraint the possible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignThis paper is organized as follows: Section 2 dement quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better scribes the MSD model; Section 3 presents our two alignments in terms of F-measure do not always lead algorithms; in Section 4 we report the results from the experiments conducted using these algorithms, to better translation quality. 450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450–454, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics and comment on the results; we conclude in Section 5. 2 prev word(s) a) • The orientation is swap, if only the next word in the source is aligned w"
P11-2079,N03-1017,0,\N,Missing
P11-2079,2010.iwslt-papers.14,1,\N,Missing
P11-2079,2008.amta-papers.18,0,\N,Missing
P13-4011,robinson-etal-2008-ask,0,0.0378918,"Missing"
P13-4011,2005.sigdial-1.26,0,0.0150458,"g and deploying kiosks. We will provide the hardware and software required to demonstrate E DGAR, both on a computer and on a tablet. This paper is organized as follows: in Section 2 we present E DGAR’s development platform Introduction Several initiatives have been taking place in the last years, targeting the concept of Edutainment, that is, education through entertainment. Following this strategy, virtual characters have animated several museums all over the world: the 3D animated Hans Christian Andersen is capable of establishing multimodal conversations about the writer’s life and tales (Bernsen and Dybkjr, 2005), Max is a virtual character employed as guide in the Heinz Nixdorf Museums Forum (Pfeiffer et al., 2011), and Sergeant Blackwell, installed in the Cooper-Hewitt National Design Museum in New York, is used by the U.S. Army Recruiting Command as a hi-tech attraction and information source (Robinson et al., 61 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 61–66, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Figure 2: E DGAR architecture and describe typical interactions, in Section 3 we show how we move from in"
P13-4011,W06-1303,0,0.0292703,"tions are declared in the knowledge sources of the agent. As shown in Figure 3, they are coordinated with viseme animations. This XML files have multilingual pairs constituted by different paraphrases of the same question and possible answers. The main reason to follow this approach (and contrary to other works where grammars are used), is to ease the process of creating/enriching the knowledge sources of the agent being developed, which is typically done by non experts in linguistics or computer science. Thus, we opted for following a similar approach of the work described, for instance, in (Leuski et al., 2006), where the agents knowledge sources are easy to create and maintain. An example of a questions/answers pair is: <questions&gt; <q en=&quot;How is everything?&quot; es=&quot;Todo bien?&quot;&gt; Tudo bem?</q&gt; </questions&gt; <answers&gt; <a en=&quot;I am ok, thank you.&quot; es=&quot;Estoy bien, gracias.&quot; emotion=&quot;smile_02&quot;&gt; Estou bem, obrigado.</a&gt; </answers&gt; Figure 3: The E DGAR character in a joyful state. 2.3 Interacting with E DGAR In a typical interaction, the user enters a question with a virtual keyboard or says it to the microphone while pressing a button (Figure 4), in the language chosen in the interface (as previously said, Por"
S17-2032,W13-2322,0,0.0682118,"here word embeddings are applied to measure the similarity of word pairs in an unsupervised manner. This work also describes supervised experiments on a macro/sentence view, which emIntroduction In this paper we present two systems that competed in SemEval-2017 tasks “Semantic Textual Similarity” and “Multilingual Word Similarity”, using supervised and unsupervised techniques, respectively. For the first task we used lexical features, as well as a semantic feature, based in the Abstract Meaning Representation (AMR) and in the SMATCH measure. AMR is a semantic formalism, structured as a graph (Banarescu et al., 2013). SMATCH is a metric for comparison of AMRs (Cai and Knight, 2013). To the best of our knowledge, these were not yet applied to Semantic Textual Similarity. In this paper we focus on the contribution of the SMATCH score as a semantic feature for Semantic Textual Similarity, relative to a model based on lexical clues only. 213 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 213–219, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics ploy hand engineered features and the Gradient Boosting algorithm, as in our STS s"
S17-2032,S16-1089,0,0.174291,"Missing"
S17-2032,P13-2131,0,0.0327351,"airs in an unsupervised manner. This work also describes supervised experiments on a macro/sentence view, which emIntroduction In this paper we present two systems that competed in SemEval-2017 tasks “Semantic Textual Similarity” and “Multilingual Word Similarity”, using supervised and unsupervised techniques, respectively. For the first task we used lexical features, as well as a semantic feature, based in the Abstract Meaning Representation (AMR) and in the SMATCH measure. AMR is a semantic formalism, structured as a graph (Banarescu et al., 2013). SMATCH is a metric for comparison of AMRs (Cai and Knight, 2013). To the best of our knowledge, these were not yet applied to Semantic Textual Similarity. In this paper we focus on the contribution of the SMATCH score as a semantic feature for Semantic Textual Similarity, relative to a model based on lexical clues only. 213 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 213–219, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics ploy hand engineered features and the Gradient Boosting algorithm, as in our STS system. Henry and Sands (2016) employ WordNet for their sentence an"
S17-2032,S17-2002,0,0.0287472,"d better results reveal that SMATCH was applied to AMR with less information than in the source sentence. To conclude, we should say that 20 pairs were consistently better predicted without SMATCH, with an average difference to SMATCH based predictions of 0.38. 4 Equivalence functions equivW N (t1 , t2 ) is defined as:   1 if syn(t1 ) = syn(t2 )    x if syn hyp(t ) ⊃ hyp(t ) 1 2  equivW N =  x if hyp(t ) ⊂ syn hyp(t ) 1 2    0 otherwise, Task 2.1 - Multilingual word similarity: English where: In this section we report the experiments conducted for the second task of 2017 SemEval (Camacho-Collados et al., 2017). The task consists of, given a pair of words, automatically measuring their semantic similarity, in a continuous range of [0 − 4], from unrelated to totally similar. The test set was composed of 500 pairs of tokens (which can be words or multiple-word expressions); a small trial of 18 pairs set was also provided by the organizers. For this task we used a family of equivalence functions, from now on equiv(t1 , t2 ), where t1 and t2 are the tokens to be compared. equiv functions return a value in the range [0 − 1]. This value was later scaled into the goal’s range. Then, we • syn(t) gives the s"
S17-2032,S17-2001,0,0.014975,"AMR3 , and then supplied to SMATCH, which returns a numeric value between 0 and 1 denoting their similarity. In SMATCH, an AMR is translated into triples that represent variable instances, their relations, and global attributes such as the start node and literals. The final SMATCH score is the maximum F score of matching triples, according to various variable mappings, obtained by comparing their instance tokens. These are converted into lower case and then matched for exact equality. Task 1 - Semantic textual similarity In this section we describe our participation in Task 1 of SemEval-2017 (Cer et al., 2017), aimed at assessing the ability of a system to quantify the semantic similarity between two sentences, using a continuous value from 0 to 5 where 5 means semantic equivalence. This task is defined for monolingual and cross-lingual pairs. We participated in the monolingual evaluation for English, and we also report results for Spanish, both with test sets composed by 250 pairs. Most of our lexical features are language independent, thus we use the same model. For a pair of sentences, our system collects the numeric output of metrics that assess their similarity relative to lexical or semantic"
S17-2032,S16-1115,0,0.106296,"ly-rich set of features. These move from lexical to semantic features. In particular, we try to take advantage of the recent Abstract Meaning Representation and SMATCH measure. Although without state of the art results, we introduce semantic structures in textual similarity and analyze their impact. Regarding word similarity, we target the English language and combine WordNet information with Word Embeddings. Without matching the best systems, our approach proved to be simple and effective. 1 2 Related work The general architecture of our STS system is similar to that of Brychc´ın and Svoboda (2016), Potash et al. (2016) or Tian and Lan (2016), but we employ more lexical features and AMR semantics. Brychc´ın and Svoboda (2016) model feature dependence in Support Vector Machines by using the product between pairs of features as new features, while we rely on neural networks. In Potash et al. (2016) it is concluded that feature based systems have better performance than structural learning with syntax trees. A fully-connected neural network is employed on hand engineered features and on an ensemble of predictions from feature based and structural based systems. We also employ a similar neu"
S17-2032,P14-1134,0,0.0172311,"ttings (network topology, training data and normalization) of run “l2f NN-2 (+smatch)” resulted in a Pearson score of 0.72374. More recently, Tensorflow released version 1.0, which resulted in a score of 0.70437 for the same setup5 . (1) The top structure (until “vs.”) is the AMR for the first sentence, where “winning” is incorrectly identified as a verb, and the actual verb (“shoot”) and its subject (“player”) are missing. The same subject is also missing in the bottom AMR. For a comprehensive understanding of the AMR notation and the parser we employed please see Banarescu et al. (2013) and Flanigan et al. (2014), respectively. 4 https://keras.io/ https://www.tensorflow.org/install/ migration#numeric_differences 5 The same happened with example B (and C, al215 Algorithm SMATCH no SMATCH Gold A G. boost NN-1 NN-2 2.77 3.31 2.00 2.93 1.82 1.74 2.8 B G. boost NN-1 NN-2 4.13 4.00 4.05 4.15 4.31 4.34 4.0 C G. boost NN-1 NN-2 1.76 1.01 1.32 1.89 1.66 1.89 0.8 Table 2: Predictions for pairs A, B and C where SMATCH excels, grouped by pair. analyzed how to combine them. In the following subsections we detail our approach. though not presented here): (a / and :op1 (p / pose-02 :ARG0 (w / woman) :ARG1 (c / camer"
S17-2032,P15-1150,0,0.0650356,"Missing"
S17-2032,S16-1087,0,0.0312646,"H is a metric for comparison of AMRs (Cai and Knight, 2013). To the best of our knowledge, these were not yet applied to Semantic Textual Similarity. In this paper we focus on the contribution of the SMATCH score as a semantic feature for Semantic Textual Similarity, relative to a model based on lexical clues only. 213 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 213–219, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics ploy hand engineered features and the Gradient Boosting algorithm, as in our STS system. Henry and Sands (2016) employ WordNet for their sentence and chunk similarity metric, as also occurs in our system for word similarity. 3 bytes and tokens of a pair of sentences, except for the Spectrum kernel on bits (as it is not a valid combination), resulting in 62 of our 159 features. The only semantic feature is the SMATCH score (Cai and Knight, 2013) which represents the similarity among two AMR graphs (Banarescu et al., 2013). The AMR for each sentence in a pair is generated with JAMR3 , and then supplied to SMATCH, which returns a numeric value between 0 and 1 denoting their similarity. In SMATCH, an AMR i"
S17-2032,S15-2015,0,0.0668748,"Missing"
S17-2032,J15-4004,0,0.168237,"Missing"
S17-2032,marelli-etal-2014-sick,0,0.0482071,"rt results for Spanish, both with test sets composed by 250 pairs. Most of our lexical features are language independent, thus we use the same model. For a pair of sentences, our system collects the numeric output of metrics that assess their similarity relative to lexical or semantic aspects. Such features are supplied to a machine learning algorithm to: a) build a model, using pairs labeled with an equivalence value (compliant with the task), or b) predict such value, using the model. 3.1 3.2 Experimental setup We applied all metrics to the train, test and trial examples of the SICK corpus (Marelli et al., 2014) and train and test examples from previous Semantic Textual Similarity in SemEval, as compiled by Tan et al. (2015). Thus, our training dataset is comprised of 24623 vectors (with 9841 from SICK) assigned to a continuous value ranging from 0 to 5. Each vector contains our 159 feature values for the similarity among the sentences in an example pair. We standardized the features by removing the mean and scaling to unit variance and norm. Then, machine learning algorithms were applied to the feature sets to train a model of our Semantic Textual Similarity representations. Namely, we employed ense"
S17-2032,S16-1094,0,0.031307,"ove from lexical to semantic features. In particular, we try to take advantage of the recent Abstract Meaning Representation and SMATCH measure. Although without state of the art results, we introduce semantic structures in textual similarity and analyze their impact. Regarding word similarity, we target the English language and combine WordNet information with Word Embeddings. Without matching the best systems, our approach proved to be simple and effective. 1 2 Related work The general architecture of our STS system is similar to that of Brychc´ın and Svoboda (2016), Potash et al. (2016) or Tian and Lan (2016), but we employ more lexical features and AMR semantics. Brychc´ın and Svoboda (2016) model feature dependence in Support Vector Machines by using the product between pairs of features as new features, while we rely on neural networks. In Potash et al. (2016) it is concluded that feature based systems have better performance than structural learning with syntax trees. A fully-connected neural network is employed on hand engineered features and on an ensemble of predictions from feature based and structural based systems. We also employ a similar neural network on hand engineered features, but"
S19-2019,J14-3006,0,0.123365,"tering approach, starting with one cluster per argument. On the other hand, the coupled model takes into consideration a distance-dependent prior shared among different predicates. Arguments from different predicates are then used as vertices of a similarity graph and each argument selects another argument as a member of the same cluster based on that similarity. Overall, the coupled model performs slightly better than the factored one. In both cases, each argument is represented by a set of syntactic features – sentence voice, argument position, syntactic relation, and existing prepositions. Lang and Lapata (2014) proposed a graph partitioning approach over a multilayer graph. Each layer corresponds to a feature, i.e., each pair of vertices (arguments) is connected through multiple edges, each corresponding to their similarity according to that feature. Then, two clustering approaches were considered, achieving similar results. The first is an adaptation of agglomerative clustering to the multilayer setting. Instead of combining the similarity values into a single score, it clusters the arguments in each layer and then combines the obtained scores into a multilayer score. Clusters with greater multilay"
S19-2019,P10-2045,0,0.0134331,"the baselines reported for the task on the test set in terms of Purity F1 , as well as in terms of BCubed F1 in most cases. 1 Introduction The Frame Semantics theory of language (Fillmore, 1976) states that one cannot understand the meaning of a word without knowing the context surrounding it. That is, a word may evoke different semantic frames depending on its context. Considering this relation, sets of frame definitions and annotated datasets that map text into the semantic frames it evokes are important resources for multiple Natural Language Processing (NLP) tasks (Shen and Lapata, 2007; Aharon et al., 2010; Das et al., 2014). The most prominent of such resources is the FrameNet (Baker et al., 1998), which provides a set of more than 1,200 generic semantic frames, as well as over 200,000 annotated sentences in English. However, this kind 130 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 130–136 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics bination of the scores of each layer is based on a set of conditions, in order to avoid having to learn or guess weights for each feature. In contrast to the previous"
S19-2019,J93-2004,0,0.0644774,"word verbs or arguments, we used the annotations provided with the task dataset. (2) Finally, since the arguments are highly dependent on the verb, we also performed experiments in which we combined the contextualized representation of the argument with that of the verb before applying the clustering approach. 4 Experimental Setup In this section we describe our experimental setup in terms of data, implementation details, and evaluation metrics and baselines. 4.1 Dataset In our experiments, we used the dataset provided by the task organization, built with sentences from the Penn Treebank 3.0 (Marcus et al., 1993), and annotated with FrameNet frames (Task A), frame elements or slots (Task B.1) and generic semantic roles (Task B.2). The development set consists of 600 verb-argument instances, 588 sentences and 1,211 arguments. The (blind) test set comprises 4,620 verb-argument instances, 3,346 sentences, 9,466 arguments labeled for semantic role and 9,510 arguments labeled for frame slot. Additionally, morphosyntactic information is provided in the CoNLL-U format (Buchholz and Marsi, 2006). 4.2 4.3 For comparison purposes, in addition to our results, we report the baselines provided by the task scorer."
S19-2019,P98-1013,0,0.851275,"Missing"
S19-2019,N18-1202,0,0.69824,"induction systems for building semantic frame resources for verbs and their arguments (Qasemi Zadeh et al., 2019). It is split into three subtasks. The first, Task A, focuses on clustering instances of verbs according to the semantic frame they evoke while the others focus on clustering the arguments of those verbs, both according to the frame-specific slots they fill, on Task B.1, and their semantic role, on Task B.2. In this paper, we address the three subtasks by following an approach that takes advantage of the recent developments on the generation of contextualized word representations (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Such representations are able to disambiguate different word senses by varying the position of a word in the embedding space according to its context. This ability is important in the context of semantic frame induction, since different wordsenses typically evoke different frames. To identify words that evoke the same frame or have the same role, our approach consists of clustering their representations by applying the Chinese Whispers algorithm (Biemann, 2006) to a similarity-based graph. This way, we do not need to define the number of clusters a"
S19-2019,W06-3812,0,0.264097,"f the recent developments on the generation of contextualized word representations (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Such representations are able to disambiguate different word senses by varying the position of a word in the embedding space according to its context. This ability is important in the context of semantic frame induction, since different wordsenses typically evoke different frames. To identify words that evoke the same frame or have the same role, our approach consists of clustering their representations by applying the Chinese Whispers algorithm (Biemann, 2006) to a similarity-based graph. This way, we do not need to define the number of clusters and there is no bias towards the Building large datasets annotated with semantic information, such as FrameNet, is an expensive process. Consequently, such resources are unavailable for many languages and specific domains. This problem can be alleviated by using unsupervised approaches to induce the frames evoked by a collection of documents. That is the objective of the second task of SemEval 2019, which comprises three subtasks: clustering of verbs that evoke the same frame and clustering of arguments int"
S19-2019,S19-2003,0,0.128703,"Missing"
S19-2019,W06-2920,0,0.169894,"Missing"
S19-2019,J14-1002,0,0.0287683,"ed for the task on the test set in terms of Purity F1 , as well as in terms of BCubed F1 in most cases. 1 Introduction The Frame Semantics theory of language (Fillmore, 1976) states that one cannot understand the meaning of a word without knowing the context surrounding it. That is, a word may evoke different semantic frames depending on its context. Considering this relation, sets of frame definitions and annotated datasets that map text into the semantic frames it evokes are important resources for multiple Natural Language Processing (NLP) tasks (Shen and Lapata, 2007; Aharon et al., 2010; Das et al., 2014). The most prominent of such resources is the FrameNet (Baker et al., 1998), which provides a set of more than 1,200 generic semantic frames, as well as over 200,000 annotated sentences in English. However, this kind 130 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 130–136 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics bination of the scores of each layer is based on a set of conditions, in order to avoid having to learn or guess weights for each feature. In contrast to the previous approaches, Titov a"
S19-2019,D07-1002,0,0.283836,"le to outperform all of the baselines reported for the task on the test set in terms of Purity F1 , as well as in terms of BCubed F1 in most cases. 1 Introduction The Frame Semantics theory of language (Fillmore, 1976) states that one cannot understand the meaning of a word without knowing the context surrounding it. That is, a word may evoke different semantic frames depending on its context. Considering this relation, sets of frame definitions and annotated datasets that map text into the semantic frames it evokes are important resources for multiple Natural Language Processing (NLP) tasks (Shen and Lapata, 2007; Aharon et al., 2010; Das et al., 2014). The most prominent of such resources is the FrameNet (Baker et al., 1998), which provides a set of more than 1,200 generic semantic frames, as well as over 200,000 annotated sentences in English. However, this kind 130 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 130–136 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics bination of the scores of each layer is based on a set of conditions, in order to avoid having to learn or guess weights for each feature. In cont"
S19-2019,N15-1001,0,0.0207748,", 2014). The most prominent of such resources is the FrameNet (Baker et al., 1998), which provides a set of more than 1,200 generic semantic frames, as well as over 200,000 annotated sentences in English. However, this kind 130 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 130–136 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics bination of the scores of each layer is based on a set of conditions, in order to avoid having to learn or guess weights for each feature. In contrast to the previous approaches, Titov and Khoddam (2015) proposed a reconstructionerror maximization framework which comprises two main components: an auto-encoder, responsible for labeling arguments with induced roles, and a reconstruction model, which takes the induced roles and predicts the argument that fills each role, i.e., it tries to reconstruct the input. The learning error is obtained by comparing the reconstructed argument to the original one. This enables the use of a larger feature set and more complex features, similarly to supervised approaches. Concerning frame induction, Ustalov et al. (2018a) proposed a graph-based approach for th"
S19-2019,E12-1003,0,0.0207223,"lated to the task, in Section 2. Then, in Section 3, we describe our approach and explain how it differs from previous approaches. Section 4 describes our experimental setup. The results of our experiments are presented and discussed in Section 5. Finally, Section 6 summarizes the conclusions of our work and provides pointers for future work. 2 Related Work Following the motivation described in the previous section, previous studies have employed unsupervised approaches for the induction of semantic frames and roles. However, most studies have focused on semantic role induction. For instance, Titov and Klementiev (2012) proposed two models based on the Chinese Restaurant Process (Ferguson, 1973). The factored model induces semantic roles for each predicate independently using an iterative clustering approach, starting with one cluster per argument. On the other hand, the coupled model takes into consideration a distance-dependent prior shared among different predicates. Arguments from different predicates are then used as vertices of a similarity graph and each argument selects another argument as a member of the same cluster based on that similarity. Overall, the coupled model performs slightly better than"
S19-2019,P17-1145,0,0.0232275,"iginal one. This enables the use of a larger feature set and more complex features, similarly to supervised approaches. Concerning frame induction, Ustalov et al. (2018a) proposed a graph-based approach for the triclustering of Subject-Verb-Object (SVO) triples extracted using a dependency parser. Each vertex in the graph is the SVO triple, represented by the concatenation of word embeddings for the three elements. Vertices are connected to their k-nearest neighbours (k=10) according to their cosine similarity. The clusters are then generated using the Watset fuzzy graph clustering algorithm (Ustalov et al., 2017), which induces word-sense information in the graph before clustering. For each cluster, the corresponding triframe is generated by aggregating the subjects, verbs, and objects into separate sets and generating a triple using those sets. This approach outperformed hard clustering approaches, as well as topic-based approaches, such as LDA-Frames (Materna, 2012). generation of clusters of similar size. In the remainder of the paper, we start by providing an overview of previous studies related to the task, in Section 2. Then, in Section 3, we describe our approach and explain how it differs from"
S19-2019,P18-2010,0,0.683724,"contrast to the previous approaches, Titov and Khoddam (2015) proposed a reconstructionerror maximization framework which comprises two main components: an auto-encoder, responsible for labeling arguments with induced roles, and a reconstruction model, which takes the induced roles and predicts the argument that fills each role, i.e., it tries to reconstruct the input. The learning error is obtained by comparing the reconstructed argument to the original one. This enables the use of a larger feature set and more complex features, similarly to supervised approaches. Concerning frame induction, Ustalov et al. (2018a) proposed a graph-based approach for the triclustering of Subject-Verb-Object (SVO) triples extracted using a dependency parser. Each vertex in the graph is the SVO triple, represented by the concatenation of word embeddings for the three elements. Vertices are connected to their k-nearest neighbours (k=10) according to their cosine similarity. The clusters are then generated using the Watset fuzzy graph clustering algorithm (Ustalov et al., 2017), which induces word-sense information in the graph before clustering. For each cluster, the corresponding triframe is generated by aggregating the"
S19-2019,S18-2016,0,0.0191214,"ted in Table 2. We only submitted the clusters obtained using ELMo, since it outperformed BERT on the development set. Similarly, we did not consider the combination of verb and argument representation for the argument clustering tasks. However, we assessed the performance of the baseline based on the dependency relation and the POS tag. On Task A, our approach surpasses all the baselines in terms of Purity F1 , but by less than 2 percentage points. In fact, it has a similar perforters as a baseline. Due to space constraints, we do not report the results of the remaining baselines proposed by Kallmeyer et al. (2018). We report the results of an additional baseline for Task B.2 which considers both the argument’s syntactic relation to the head verb and its Part-ofSpeech (POS) tag (Dep + POS). 4.4 Evaluation metrics We report our results using the metrics defined for the task: number of clusters (#C), purity, inversepurity, and their harmonic mean (Purity F1 ), as proposed by Steinbach et al. (2000), and BCubed (B3 ) precision, recall, and F1 , as proposed by Bagga and Baldwin (1998). 5 Results The results obtained on the development set are reported in Table 1. We can see that using ELMo to obtain the con"
silva-etal-2012-dealing,N03-2016,0,\N,Missing
silva-etal-2012-dealing,N01-1020,0,\N,Missing
silva-etal-2012-dealing,W02-0902,0,\N,Missing
silva-etal-2012-dealing,D07-1092,0,\N,Missing
silva-etal-2012-dealing,mulloni-pekar-2006-automatic,0,\N,Missing
W04-1511,W02-1505,1,0.879847,"Missing"
W04-1511,1993.iwpt-1.22,0,0.0515529,"Missing"
W04-2006,P02-1041,0,0.0296692,"re limits to this task, as in some situations information from semantics and pragmatics should also be taken into account to find the correct semantic value. 4.2 Logical forms generation 4.2.1 Minimal Recursion Semantics Linking syntax with semantics is not an easy task. As Allen says in (Allen, 1995) there seems to be a structural inconsistency between syntactic structure and the structure of the logical form. We can ease this process by using an adequate representation language. In fact, although the concept is not new (Hobbs, 1983), state of the art frameworks such as (Moll´a et al., 2003; Baldridge and Kruijff, 2002) are using flat semantic representations, taht is formulas with no embedded structures (see (Moll´a, 2000) for details about flatness), which simplify the syntactic-semantic interface. At the same time, and because it is not reasonable to generate all the possible interpretations of a sentence, many frameworks are using representation languages that leave underspecified semantic interpretations (also an old concept (Woods, 1978)). MRS (Copestake et al., 2001) uses a flat representation with explicit pointers (called handles) to encode scope effects, corresponding to recursive structures in mor"
W04-2006,P83-1009,0,0.421858,"e paraphrases of it), by using its syntactic context. Obviously, there are limits to this task, as in some situations information from semantics and pragmatics should also be taken into account to find the correct semantic value. 4.2 Logical forms generation 4.2.1 Minimal Recursion Semantics Linking syntax with semantics is not an easy task. As Allen says in (Allen, 1995) there seems to be a structural inconsistency between syntactic structure and the structure of the logical form. We can ease this process by using an adequate representation language. In fact, although the concept is not new (Hobbs, 1983), state of the art frameworks such as (Moll´a et al., 2003; Baldridge and Kruijff, 2002) are using flat semantic representations, taht is formulas with no embedded structures (see (Moll´a, 2000) for details about flatness), which simplify the syntactic-semantic interface. At the same time, and because it is not reasonable to generate all the possible interpretations of a sentence, many frameworks are using representation languages that leave underspecified semantic interpretations (also an old concept (Woods, 1978)). MRS (Copestake et al., 2001) uses a flat representation with explicit pointer"
W11-2705,P05-1045,0,0.0418425,"Missing"
W11-2705,P06-1063,0,0.160274,"Missing"
W11-2705,levy-andrew-2006-tregex,0,0.0933226,"Missing"
W11-2705,C02-1150,0,0.180558,"Missing"
W11-2705,N07-1051,0,0.031602,"Missing"
W11-2705,P02-1006,0,0.129042,"Missing"
W15-2815,afonso-etal-2002-floresta,0,0.661573,"uld be corrected at this step. Then, the input string is split into sentences and then into words (tokenization). As an example, the sentence ‘o joão come a sopa’ (‘João eats a soup’), becomes [’o’, ’joão’, ’come’, ’a’, ’sopa’]. A stemmer identifies suffixes and prefixes. Thus, the word ‘coelhinha’ (as previously said, ‘little female rabbit’), is understood, by its suffix (‘inha’), to be a female and small derivation of the root coelh(o). Therefore, ‘coelhinha’ is converted into [MULHER, COELHO, PEQUENO], hinted to be all part of the same gloss. We have used the treebank ‘floresta sintática’ (Afonso et al., 2002) for training our ‘POStagger’. The output of the POS-tagger for the sentence ‘o joão come a sopa’ is now [(’o’, ’art’), (’joão’, ’prop’), (’come’, ’v-fin’), (’a’, ’prp’), (’sopa’, ’n’)]. We have used a Named Entity Recognizer to find proper names of persons. Our system further supports a list of portuguese names and public personalities names with their matching gestural name. For these specific entities, the system uses the known gesture instead of fingerspelling the name. The POS-tags and recognised entities also contribute with hints. These hints are then confirmed (or not) in the next step"
W15-2815,W15-5124,1,0.793661,"Missing"
W15-5124,W15-2815,1,0.821073,"gal name.surname@L2F.inesc-id.pt, t-sacand@microsoft.com Abstract this system, there is a flexible architecture that takes advantage of NLP tools, as these can give an important contribution to the translation process. For instance, if a proper noun is identified, if no sign is associated with it, fingerspelling is the solution. Moreover, as we will see, in some cases, a word can be signed by signs associated with its root and suffixes. Thus, a stemmer or a Part-of-Speech (POS)-tagger can play a fundamental role in these situations. A detailed description of the system can be found in [6] and [7]. The system can be downloaded from http://web.ist.utl.pt/~ist163556/pt2lgp. This paper is organised as follows: in Section 2 we present related work, in Section 3 we describe some basic linguistic phenomena in LGP, in Section 4 we describe our prototype, and, in Section 5, we explain what can be tested in our demo. Finally, in Section 6 we conclude and point to future work. Several efforts have been done towards the development of platforms that allow the translation of specific sign languages to the correspondent spoken language (and vice-versa). In this (demo) paper, we describe a freely av"
W15-5124,afonso-etal-2002-floresta,0,0.179864,"mer: As a form of morphologic parsing, we apply a stemmer that identifies suffixes and prefixes to use as an adjective or classifier to the gloss. This allows, for example, ‘coelhinha’ (‘little female rabbit’), to be understood, by its suffixes (‘inho’ +‘a’) , to be a small (‘inho) and a female (‘a’) derivation of the root ‘coelh(o)’. • POS-Tagger: We make use of NLTK’s n-gram taggers, starting with a bigram tagger, with a backoff technique for an unigram tagger and the default classification of ‘noun’ (the most common class for Portuguese). We used the treebank ‘floresta sintá(c)tica’ corpus [15] for training the taggers. Using the same example, the result would be: [(’o’, ’art’), (’joão’, ’prop’), (’come’, ’v-fin’), (’a’, ’prp’), (’sopa’, ’n’)]. Figure 4: Hand configurations for numbers (0-9) • Named Entity Extraction: We apply Named Entity Recognition (NER) for identifying names of persons, by matching against a list of common Portuguese names. The animation is synthesised by directly accessing and modifying the action and f-curve data. We always start and end a sentence with the rest pose. For concatenating the actions, we blend from one to the other in a given amount of frames by"
W16-6640,de-marneffe-etal-2006-generating,0,0.0355787,"Missing"
W16-6640,J05-1004,0,0.00941446,"ecking if their structure is the same, that is, if the subtrees’ labels are syntactically equivalent and the number of children is the same (as suggested by Wang and Neumann Proceedings of The 9th International Natural Language Generation conference, pages 242–243, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics (2007)). Then, QG A SP checks, for each token pair, if they match. For the lexical match, lemmas are obtained from WordNet. The semantic match is based on the SRL predicted verb and a verb dictionary. This dictionary is the mapping between PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 2003), gathered from SemLink1 . If two verbs belong to the same set in any of the resources, they are considered to match. It is also considered a semantic match if two nonverb tokens belong to the same synset, from WordNet (Miller, 1995), or if two Named Entitiess (NEs) have the same type, according to Stanford Named Entity Recognition (NER). 3 Evaluation We tested QG A SP on the Engarte corpus2 . We used Engarte’s 32 revised triples labeled as true. These triples were then used both for PA and QG, and tested in a leave one out appro"
W16-6640,D14-1045,0,0.0144506,"et al. (2010)), but to the best of our knowledge QG A SP is the first system that relies on the lexical, syntactic and semantic information in both the Pattern Acquisition (PA) and the Question Generation (QG) steps. 2 QG A SP overview Figure 1 illustrates QG A SP architecture. 2.1 Pattern Acquisition Our seeds are triples constituted by a question, its answer (optional), and a snippet that could answer that question. The question and the snippet from each seed are processed by the Stanford syntactic and dependency parsers (de Marneffe et al., 2006), and MatePlus Semantic Role Labeler (SRL) (Roth and Woodsend, 2014). A pattern is a bidirectional 242 Figure 1: QG A SP overview mapping between subtrees of the question and the correspondent snippet. 2.2 Question Generation Given a sentence, QG A SP starts by parsing it, exactly as before; then it matches the previously learned patterns with the obtained structures. 2.3 The Matching Step The matching step is the same, both in the PA and QG stage. Considering that a loose matching strategy will result in many patterns and questions, thus introducing noise, whereas a too restrict approach will end up in too specific patterns and low variability of questions, Q"
W16-6640,W07-1406,0,0.0642699,"Missing"
