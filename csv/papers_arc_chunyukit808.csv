2020.coling-main.567,Multi-choice Relational Reasoning for Machine Reading Comprehension,2020,-1,-1,3,0,21685,wuya chen,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper presents our study of cloze-style reading comprehension by imitating human reading comprehension, which normally involves tactical comparing and reasoning over candidates while choosing the best answer. We propose a multi-choice relational reasoning (McR$^2$) model with an aim to enable relational reasoning on candidates based on fusion representations of document, query and candidates. For the fusion representations, we develop an efficient encoding architecture by integrating the schemes of bidirectional attention flow, self-attention and document-gated query reading. Then, comparing and inferring over candidates are executed by a novel relational reasoning network. We conduct extensive experiments on four datasets derived from two public corpora, Children{'}s Book Test and Who DiD What, to verify the validity and advantages of our model. The results show that it outperforms all baseline models significantly on the four benchmark datasets. The effectiveness of its key components is also validated by an ablation study."
2015.mtsummit-papers.3,Learning bilingual distributed phrase represenations for statistical machine translation,2015,-1,-1,4,0,37931,chaochao wang,Proceedings of Machine Translation Summit XV: Papers,0,None
W13-2517,Finding More Bilingual Webpages with High Credibility via Link Analysis,2013,20,1,3,0,15479,chengzhi zhang,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"This paper presents an efficient approach to finding more bilingual webpage pairs with high credibility via link analysis, using little prior knowledge or heuristics. It extends from a previous algorithm that takes the number of bilingual URL pairs that a key (i.e., a URL pairing pattern) can match as the objective function to search forthe best set of keysyielding the greatest number of webpage pairs within targeted bilingual websites. Enhanced algorithms are proposed to match more bilingual webpages following the credibility based on statistical analysis of the link relationship of the seed websites available. With about 12,800 seed websites as test set, the enhanced algorithms improve precision over baseline by more than 5%, from 94.06% to 99.40%, and hence find above 20% more true bilingual URL pairs, illustrating that significantly more bilingual webpages with high credibility can be mined with the help of the link analysis."
P13-1061,Non-Monotonic Sentence Alignment via Semisupervised Learning,2013,23,3,2,0.833333,6655,xiaojun quan,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data."
W12-6304,Semi-automatic Annotation of {C}hinese Word Structure,2012,30,6,2,0,6735,jianqiang ma,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Chinese word structure annotation is potentially useful for many NLP tasks, especially for Chinese word segmentation. Li and Zhou (2012) have presented an annotation for word structures in the Penn Chinese Treebank. But they only consider words that have productive affixes, which covers 35% of word types in that corpus. In this paper, we propose a linguistically inspired annotation that covers various morphological derivations of Chinese in a more general way, such that almost all multiple-character words can be structurally analyzed. As manual annotation is expensive, we propose a semi-supervised approach to automatic annotation, which combines the maximum entropy learning and the EM iteration for the Gaussian mixture model. The proposed method has achieved an accuracy of 90% on the testing set."
P12-2001,Higher-order Constituent Parsing and Parser Combination,2012,34,4,2,1,8453,xiao chen,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other high-performance parsers."
D12-1097,Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level,2012,34,37,2,1,39849,billy wong,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level. As a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning. A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine. Various ways to apply this feature to evaluate machine-translated documents are presented, including one without reliance on reference translation. Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements."
C12-2116,Entropy-based Training Data Selection for Domain Adaptation,2012,14,6,4,1,3941,yan song,Proceedings of {COLING} 2012: Posters,0,"Training data selection is a common method for domain adaptation, the goal of which is to choose a subset of training data that works well for a given test set. It has been shown to be effective for tasks such as machine translation and parsing. In this paper, we propose several entropy-based measures for training data selection and test their effectiveness on two tasks: Chinese word segmentation and part-of-speech tagging. The experimental results on the Chinese Penn Treebank indicate that some of the measures provide a statistically significant improvement over random selection for both tasks."
I11-1141,Improving Part-of-speech Tagging for Context-free Parsing,2011,31,3,2,1,8453,xiao chen,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In this paper, we propose a factored parsing model consisting of a lexical and a constituent model. The discriminative lexical model allows the parser to utilize rich contextual features beyond those encoded in the context-free grammar (CFG) in use. Experiment results reveal that our parser achieves statistically significant improvement in both parsing and tagging accuracy on both English and Chinese."
2011.mtsummit-papers.61,Comparative Evaluation of Term Informativeness Measures in Machine Translation Evaluation Metrics,2011,-1,-1,2,1,39849,billy wong,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-4109,Bigram {HMM} with Context Distribution Clustering for Unsupervised {C}hinese Part-of-Speech tagging,2010,16,1,3,0,45143,lidan zhang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4112,Automatic Identification of Predicate Heads in {C}hinese Sentences,2010,5,0,3,0,37897,xiaona ren,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4121,Active Learning Based Corpus Annotation,2010,16,3,3,0,45156,hongyan song,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4154,Combine Person Name and Person Identity Recognition and Document Clustering for {C}hinese Person Name Disambiguation,2010,3,1,4,0,1816,ruifeng xu,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-2409,Reranking with Multiple Features for Better Transliteration,2010,5,10,2,1,3941,yan song,Proceedings of the 2010 Named Entities Workshop,0,"Effective transliteration of proper names via grapheme conversion needs to find transliteration patterns in training data, and then generate optimized candidates for testing samples accordingly. However, the top-1 accuracy for the generated candidates cannot be good if the right one is not ranked at the top. To tackle this issue, we propose to rerank the output candidates for a better order using the averaged perceptron with multiple features. This paper describes our recent work in this direction for our participation in NEWS2010 transliteration evaluation. The official results confirm its effectiveness in English-Chinese bidirectional transliteration."
W10-1755,The Parameter-Optimized {ATEC} Metric for {MT} Evaluation,2010,9,7,2,1,39849,billy wong,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the latest version of the ATEC metric for automatic MT evaluation, with parameters optimized for word choice and word order, the two fundamental features of language that the metric relies on. The former is assessed by matching at various linguistic levels and weighting the informativeness of both matched and unmatched words. The latter is quantified in term of word position and information flow. We also discuss those aspects of language not yet covered by other existing evaluation metrics but carefully considered in the formulation of our metric."
S10-1100,"{HITSZ}{\\_}{CITYU}: Combine Collocation, Context Words and Neighboring Sentence Sentiment in Sentiment Adjectives Disambiguation",2010,7,2,3,0,1816,ruifeng xu,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper presents the HIT_CITYU systems in Semeval-2 Task 18, namely, disambiguating sentiment ambiguous adjectives. The baseline system (HITSZ_CITYU_3) incorporates bi-gram and n-gram collocations of sentiment adjectives, and other context words as features in a one-class Support Vector Machine (SVM) classifier. To enhance the baseline system, collocation set expansion and characteristics learning based on word similarity and semisupervised learning are investigated, respectively. The final system (HITSZ_CITYU_1/2) combines collocations, context words and neighboring sentence sentiment in a two-class SVM classifier to determine the polarity of sentiment adjectives. The final systems achieved 0.957 and 0.953 (ranked 1st and 2nd) macro accuracy, and 0.936 and 0.933 (ranked 2nd and 3rd) micro accuracy, respectively."
zhao-etal-2010-large,How Large a Corpus Do We Need: Statistical Method Versus Rule-based Method,2010,11,8,3,1,305,hai zhao,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We investigate the impact of input data scale in corpus-based learning using a study style of ZipfÂs law. In our research, Chinese word segmentation is chosen as the study case and a series of experiments are specially conducted for it, in which two types of segmentation techniques, statistical learning and rule-based methods, are examined. The empirical results show that a linear performance improvement in statistical learning requires an exponential increasing of training corpus size at least. As for the rule-based method, an approximate negative inverse relationship between the performance and the size of the input lexicon can be observed."
W09-3511,Transliteration of Name Entity via Improved Statistical Translation on Character Sequences,2009,11,8,2,1,3941,yan song,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"Transliteration of given parallel name entities can be formulated as a phrase-based statistical machine translation (SMT) process, via its routine procedure comprising training, optimization and decoding. In this paper, we present our approach to transliterating name entities using the loglinear phrase-based SMT on character sequences. Our proposed work improves the translation by using bidirectional models, plus some heuristic guidance integrated in the decoding process. Our evaluated results indicate that this approach performs well in all standard runs in the NEWS2009 Machine Transliteration Shared Task."
W09-1208,Multilingual Dependency Learning: A Huge Feature Engineering Method to Semantic Dependency Parsing,2009,8,42,3,1,305,hai zhao,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes our system about multilingual semantic dependency parsing (SR-Lonly) for our participation in the shared task of CoNLL-2009. We illustrate that semantic dependency parsing can be transformed into a word-pair classification problem and implemented as a single-stage machine learning system. For each input corpus, a large scale feature engineering is conducted to select the best fit feature template set incorporated with a proper argument pruning strategy. The system achieved the top average score in the closed challenge: 80.47% semantic labeled F1 for the average score."
P09-1007,Cross Language Dependency Parsing using a Bilingual Lexicon,2009,27,50,3,1,305,hai zhao,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper proposes an approach to enhance dependency parsing in a language by using a translated treebank from another language. A simple statistical machine translation method, word-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an ensemble method, the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-of-the-art result."
D09-1004,Semantic Dependency Parsing of {N}om{B}ank and {P}rop{B}ank: An Efficient Integrated Approach via a Large-scale Feature Selection,2009,31,18,3,1,305,hai zhao,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"We present an integrated dependency-based semantic role labeling system for English from both NomBank and PropBank. By introducing assistant argument labels and considering much more feature templates, two optimal feature template sets are obtained through an effective feature selection procedure and help construct a high performance single SRL system. From the evaluations on the date set of CoNLL-2008 shared task, the performance of our system is quite close to the state of the art. As to our knowledge, this is the first integrated SRL system that achieves a competitive performance against previous pipeline systems."
Y08-1025,An Improved Corpus Comparison Approach to Domain Specific Term Recognition,2008,30,2,2,0,41137,xiaoyue liu,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"Domain specific terms are words carrying special conceptual meanings in a sub- ject field. Automatic term recognition plays an important role in many natural language processing and knowledge engineering applications such as information retrieval and knowledge mining. This paper explores a novel approach to automatic term extraction based on the basic ideas of corpus comparison and emerging pattern with significant elabo- ration. It measures the termhood of a term candidate in terms of its peculiarity to a given domain via comparison to several background domains. Our experiments confirm its out- performance against other approaches, achieving an average precision of 83% on the top 10% candidates in terms of their termhood."
W08-2127,Parsing Syntactic and Semantic Dependencies with Two Single-Stage Maximum Entropy Models,2008,10,25,2,1,305,hai zhao,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper describes our system to carry out the joint parsing of syntactic and semantic dependencies for our participation in the shared task of CoNLL-2008. We illustrate that both syntactic parsing and semantic parsing can be transformed into a word-pair classification problem and implemented as a single-stage system with the aid of maximum entropy modeling. Our system ranks the fourth in the closed track for the task with the following performance on the WSJBrown test set: 81.44% labeled macro F1 for the overall task, 86.66% labeled attachment for syntactic dependencies, and 76.16% labeled F1 for semantic dependencies."
I08-4017,Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition,2008,23,90,2,1,305,hai zhao,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper describes a novel character tagging approach to Chinese word segmentation and named entity recognition (NER) for our participation in Bakeoff-4.1 It integrates unsupervised segmentation and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success."
I08-1002,An Empirical Comparison of Goodness Measures for Unsupervised {C}hinese Word Segmentation with a Unified Framework,2008,19,41,2,1,305,hai zhao,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores. Experiments show that description length gain outperforms other measures because of its strength for identifying short words. Further performance improvement is also reported, achieved by proper candidate pruning and by assemble segmentation to integrate the strengths of individual measures."
I05-4010,Harvesting the Bitexts of the Laws of {H}ong {K}ong From the Web,2005,24,11,1,1,21686,chunyu kit,Proceedings of the Fifth Workshop on {A}sian Language Resources ({ALR}-05) and First Symposium on {A}sian Language Resources Network ({ALRN}),0,"In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy. Basic methodology and practical techniques are reported in detail. The resultant bilingual corpus, 10.4M English words and 18.3M Chinese characters, is an authoritative and comprehensive text collection covering the specific and special domain of HK laws. It is particularly valuable to empirical MT research. This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web."
I05-3021,An Example-Based {C}hinese Word Segmentation System for {CWSB}-2,2005,8,2,1,1,21686,chunyu kit,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper reports the example-based segmentation system for our participation in the second Chinese Word Segmentation Bakeoff (CWSB-2), presenting its basic ideas, technical details and evaluation. It is a preliminary implementation. CWSB-2 valuation shows that it performs very well in identifying known words. Its unknown word detection module also illustrates great potential. However, proper facilities for identifying time expressions, numbers and other types of unknown words are needed for improvement."
I05-1020,Period Disambiguation with Maxent Model,2005,13,1,1,1,21686,chunyu kit,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper presents our recent work on period disambiguation, the kernel problem in sentence boundary identification, with the maximum entropy (Maxent) model. A number of experiments are conducted on PTB-II WSJ corpus for the investigation of how context window, feature space and lexical information such as abbreviated and sentence-initial words affect the learning performance. Such lexical information can be automatically acquired from a training corpus by a learner. Our experimental results show that extending the feature space to integrate these two kinds of lexical information can eliminate 93.52% of the remaining errors from the baseline Maxent model, achieving an F-score of 99.8227%."
W03-1724,Integrating Ngram Model and Case-based Learning for {C}hinese Word Segmentation,2003,6,6,1,1,21686,chunyu kit,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents our recent work for participation in the First International Chinese Word Segmentation Bake-off (ICWSB-1). It is based on a general-purpose ngram model for word segmentation and a case-based learning approach to disambiguation. This system excels in identifying in-vocabulary (IV) words, achieving a recall of around 96-98%. Here we present our strategies for language model training and disambiguation rule learning, analyze the system's performance, and discuss areas for further improvement, e.g., out-of-vocabulary (OOV) word discovery."
W02-1808,Learning Case-based Knowledge for Disambiguating {C}hinese Word Segmentation: A Preliminary Study,2002,31,13,1,1,21686,chunyu kit,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,"Just like other NLP applications, a serious problem with Chinese word segmentation lies in the ambiguities involved. Disambiguation methods fall into different categories, e.g., rule-based, statistical-based and example-based approaches, each of which may involve a variety of machine learning techniques. In this paper we report our current progress within the example-based approach, including its framework, example representation and collection, example matching and application. Experimental results show that this effective approach resolves more than 90% of ambiguities found. Hence, if it is integrated effectively with a segmentation method of the precision P > 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%."
W99-0701,Unsupervised Learning of Word Boundary with Description Length Gain,1999,15,67,1,1,21686,chunyu kit,{EACL} 1999: {C}o{NLL}-99 Computational Natural Language Learning,0,None
O94-1006,Automatic Terminology Extraction For Thematic Corpus Based On Subterm Co-Occurrence,1994,0,0,1,1,21686,chunyu kit,Proceedings of Rocling {VII} Computational Linguistics Conference {VII},0,None
C92-4173,Tokenization as the Initial Phase in {NLP},1992,6,71,2,0,16676,jonathan webster,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, the authors address the significance and complexity of tokenization, the beginning step of NLP. Notions of word and token are discussed and defined from the viewpoints of lexicography and pragmatic implementation, respectively. Automatic segmentation of Chinese words is presented as an illustration of tokenization. Practical approaches to identification of compound tokens in English, such as idioms, phrasal verbs and fixed expressions, are developed."
O91-1007,Automatic {C}hinese Text Generation Based On Inference Trees,1991,0,8,7,0,55369,hinglung lin,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
