2020.emnlp-main.703,D16-1125,1,0.840058,"space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller,"
2020.emnlp-main.703,2020.acl-main.463,0,0.195512,"s and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current evaluation paradigms for generalization. Yet, this is the structure of generalization in human development: drawing analogies to episodic mem"
2020.emnlp-main.703,J92-4003,0,0.0781903,"Missing"
2020.emnlp-main.703,P12-1015,0,0.0358063,"., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current"
2020.emnlp-main.703,2020.acl-main.407,0,0.0170751,"ng (She et al., 2014) in the real world face challenging, continuous perception and control (Tellex et al., 2020). Consequently, research in this space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et"
2020.emnlp-main.703,P14-1113,0,0.0214947,"Missing"
2020.emnlp-main.703,W17-2810,0,0.0700925,"Missing"
2020.emnlp-main.703,J93-2004,0,0.0733849,"al These World Scopes go beyond text to consider the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and ongoing progression of how contextual information can factor into representations and tasks. We conclude with a discussion of how 8718 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8718–8735, c November 16–20, 2020. 2020 Association for Computational Linguistics 1 WS1: Corpora and Representations The story of data-driven language research begins with the corpus. The Penn Treebank (Marcus et al., 1993) is the canonical example of a clean subset of naturally generated language, processed and annotated for the purpose of studying representations. Such corpora and the model representations built from them exemplify WS1. Community energy was initially directed at finding formal linguistic structure, such as recovering syntax trees. Recent success on downstream tasks has not required such explicitly annotated signal, leaning instead on unstructured fuzzy representations. These representations span from dense word vectors (Mikolov et al., 2013) to contextualized pretrained representations (Peters"
2020.emnlp-main.703,E12-1076,0,0.0176926,"r new research into more challenging world modeling. Mottaghi et al. (2016) predicts the effects of forces on objects in images. Bakhtin et al. (2019) extends this physical reasoning to complex puzzles of cause and effect. Sun et al. (2019b,a) models scripts and actions, and alternative unsupervised training regimes (Bachman et al., 2019) open up research towards automatic concept formation. Advances in computer vision have enabled building semantic representations rich enough to interact with natural language. In the last decade of work descendant from image captioning (Farhadi et al., 2010; Mitchell et al., 2012), a myriad of tasks on visual question answering (Antol et al., 2015; Das et al., 2018; Yagcioglu et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretrain"
2020.emnlp-main.703,1985.tmi-1.17,0,0.223965,"achs et al., 1981; O’Grady, 2005; Vigliocco et al., 2014). Perception includes auditory, tactile, and visual input. Even restricted to purely linguistic signals, sarcasm, stress, and meaning can be implied through prosody. Further, tactile senses lend meaning, both physical (Sinapov et al., 2014; Thomason et al., 2016) and abstract, to concepts like heavy and soft. Visual perception is a rich signal for modeling a vastness of experiences in the world that cannot be documented by text alone (Harnad, 1990). For example, frames and scripts (Schank and Abelson, 1977; Charniak, 1977; Dejong, 1981; Mooney and Dejong, 1985) require understanding often unstated sets of pre- and post-conditions about the world. To borrow from Charniak (1977), how should we learn the meaning, method, and implications of painting? A web crawl of knowledge Eugene Charniak (A Framed PAINTING: The Representation of a Common Sense Knowledge Fragment 1977) from an exponential number of possible how-to, text-only guides and manuals (Bisk et al., 2020) is misdirected without some fundamental referents to which to ground symbols. Models must be able to watch and recognize objects, people, and activities to understand the language describing"
2020.emnlp-main.703,N18-1202,0,0.171411,"1993) is the canonical example of a clean subset of naturally generated language, processed and annotated for the purpose of studying representations. Such corpora and the model representations built from them exemplify WS1. Community energy was initially directed at finding formal linguistic structure, such as recovering syntax trees. Recent success on downstream tasks has not required such explicitly annotated signal, leaning instead on unstructured fuzzy representations. These representations span from dense word vectors (Mikolov et al., 2013) to contextualized pretrained representations (Peters et al., 2018; Devlin et al., 2019). Word representations have a long history predating the recent success of deep learning methods. Outside of NLP, philosophy (Austin, 1975) and linguistics (Lakoff, 1973; Coleman and Kay, 1981) recognized that meaning is flexible yet structured. Early experiments on neural networks trained with sequences of words (Elman, 1990; Bengio et al., 2003) suggested that vector representations could capture both syntax and semantics. Subsequent experiments with larger models, documents, and corpora have demonstrated that representations learned from text capture a great deal of in"
2020.emnlp-main.703,D18-1261,0,0.0266104,"perimentation with language starkly contrasts with the disembodied chat bots that are the focus of the current dialogue community (Roller et al., 2020; Adiwardana et al., 2020; Zhou et al., 2020; Chen et al., 2018; Serban et al., 2017), which often do not learn from individual experiences and whose environments are not persistent enough to learn the effects of actions. Theory of Mind When attempting to get what we want, we confront people who have their own desires and identities. The ability to consider the feelings and knowledge of others is now commonly referred to as the “Theory of Mind” (Nematzadeh et al., 2018). This paradigm has also been described under the “Speaker-Listener” model (Stephens et al., 2010), and a rich theory to describe this computationally is being actively developed under the Rational Speech Act Model (Frank and Goodman, 2012; Bergen et al., 2016). A series of challenges that attempt to address this fundamental aspect of communication have been introduced (Nematzadeh et al., 2018; Sap et al., 2019). These works are a great start towards deeper understanding, but static datasets can be problematic due to the risk of embedding spurious patterns and bias (de Vries et al., 2020; Le e"
2020.emnlp-main.703,P19-1506,0,0.0297579,"s rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world face challenging, continuous perception and cont"
2020.emnlp-main.703,P16-1144,1,0.890919,"Missing"
2020.emnlp-main.703,N15-1082,0,0.0556621,"Missing"
2020.emnlp-main.703,D14-1162,0,0.0857475,"s in deep models. Traditionally, transfer learning relied on our understanding of model classes, such as English grammar. Domain adaptation simply required sufficient data to capture lexical variation, by assuming most higherlevel structure would remain the same. Unsupervised representations today capture deep associations across multiple domains, and can be used successfully transfer knowledge into surprisingly diverse contexts (Brown et al., 2020). These representations require scale in terms of both data and parameters. Concretely, Mikolov et al. (2013) trained on 1.6 billion tokens, while Pennington et al. (2014) scaled up to 840 billion tokens from Common Crawl. Recent approaches 1 A parallel discussion would focus on the hardware required to enable advances to higher World Scopes. Playstations (Pinto et al., 2009) and then GPUs (Krizhevsky et al., 2012) made many WS2 advances possible. Perception, interaction, and robotics leverage other new hardware. have made progress by substantially increasing the number of model parameters to better consume these vast quantities of data. Where Peters et al. (2018) introduced ELMo with ∼108 parameters, Transformer models (Vaswani et al., 2017) have continued to"
2020.emnlp-main.703,P18-2124,0,0.0547463,"Missing"
2020.emnlp-main.703,P19-1534,0,0.0307256,"rate language that does something to the world. Passive creation and evaluation of generated language separates generated utterances from their effects on other people, and while the latter is a rich learning signal it is inherently difficult to annotate. In order to learn the effects language has on the world, an agent must participate in linguistic activity, such as negotiation (Yang et al., 2019a; He et al., 2018; Lewis et al., 2017), collaboration (Chai et al., 2017), visual disambiguation (Anderson et al., 2018; Lazaridou et al., 2017; Liu and Chai, 2015), or providing emotional support (Rashkin et al., 2019). These activities require inferring mental states and social outcomes—a key area of interest in itself (Zadeh et al., 2019). What “lame” means in terms of discriminative information is always at question: it can be defined as “undesirable,” but what it tells one about the processes operating in the environment requires social context to determine (Bloom, 2002). It is the toddler’s social experimentation with “You’re so lame!” that gives the word weight and definite intent (Ornaghi et al., 2011). In other words, the discriminative signal for the most foundational part of a word’s meaning can o"
2020.emnlp-main.703,Q13-1003,0,0.0607394,"fool!” can be hurtful, while for others it may seem playful. Social knowledge is requisite for realistic understanding of sentiment in situated human contexts. 8725 Relevant recent work The move from WS2 to WS3 requires rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces c"
2020.emnlp-main.703,P19-1180,0,0.0412109,"ent work The move from WS2 to WS3 requires rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world fac"
2020.emnlp-main.703,D19-1454,0,0.0599647,"Missing"
2020.emnlp-main.703,P14-1068,1,0.867068,"al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult"
2020.emnlp-main.703,D19-1592,0,0.0554394,"Missing"
2020.emnlp-main.703,P18-1238,0,0.112804,"Missing"
2020.emnlp-main.703,W14-4313,1,0.780432,"re tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world face challenging, continuous perception and control (Tellex et al., 2020). Consequently, research in this space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 202"
2020.emnlp-main.703,J08-1008,0,0.0602314,"time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current evaluation paradigms for generalization. Yet, this is the structure of generalization in human development: drawing analogies to episodic memories and gathering new data through non-independent experiments. As with many who have analyzed the history of NLP, its trends (Church, 2007), its maturation toward a science (Steedman, 2008), and its major challenges (Hirschberg and Manning, 2015; McClelland et al., 2019), we hope to provide momentum for a direction many are already heading. We call for and embrace the incremental, but purposeful, contextualization of language in human experience. With all that we have learned about what words can tell us and what they keep implicit, now is the time to ask: What tasks, representations, and inductive-biases will fill the gaps? Computer vision and speech recognition are mature enough for investigation of broader linguistic contexts (WS3). The robotics industry is rapidly developing"
2020.emnlp-main.703,D19-1218,0,0.0512018,"Missing"
2020.emnlp-main.703,P19-1644,0,0.137634,"easoning to complex puzzles of cause and effect. Sun et al. (2019b,a) models scripts and actions, and alternative unsupervised training regimes (Bachman et al., 2019) open up research towards automatic concept formation. Advances in computer vision have enabled building semantic representations rich enough to interact with natural language. In the last decade of work descendant from image captioning (Farhadi et al., 2010; Mitchell et al., 2012), a myriad of tasks on visual question answering (Antol et al., 2015; Das et al., 2018; Yagcioglu et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretraining (e.g. via conceptual captions (Sharma et al., 2018)) or further broadened to include audio (Tsai et al., 2019). Vision can also help ground speech signals (Srinivasa"
2020.emnlp-main.703,P06-1124,0,0.0482713,"ardware. have made progress by substantially increasing the number of model parameters to better consume these vast quantities of data. Where Peters et al. (2018) introduced ELMo with ∼108 parameters, Transformer models (Vaswani et al., 2017) have continued to scale by orders of magnitude between papers (Devlin et al., 2019; Radford et al., 2019; Zellers et al., 2019b) to ∼1011 (Brown et al., 2020). Current models are the next (impressive) step in language modeling which started with Good (1953), the weights of Kneser and Ney (1995); Chen and Goodman (1996), and the power-law distributions of Teh (2006). Modern approaches to learning dense representations allow us to better estimate these distributions from massive corpora. However, modeling lexical co-occurrence, no matter the scale, is still modeling the written world. Models constructed this way blindly search for symbolic co-occurences void of meaning. How can models yield both “impressive results” and “diminishing returns”? Language modeling— the modern workhorse of neural NLP systems—is a canonical example. Recent pretraining literature has produced results that few could have predicted, crowding leaderboards with “super-human"" accurac"
2020.emnlp-main.703,P19-1452,0,0.0131052,"e, in the limit, to everything humanity has ever written.1 We are no longer constrained to a single author or source, and the temptation for NLP is to believe everything that needs knowing can be learned from the written world. But, a large and noisy text corpus is still a text corpus. This move towards using large scale raw data has led to substantial advances in performance on existing and novel community benchmarks (Devlin et al., 2019; Brown et al., 2020). Scale in data and modeling has demonstrated that a single representation can discover both rich syntax and semantics without our help (Tenney et al., 2019). This change is perhaps best seen in transfer learning enabled by representations in deep models. Traditionally, transfer learning relied on our understanding of model classes, such as English grammar. Domain adaptation simply required sufficient data to capture lexical variation, by assuming most higherlevel structure would remain the same. Unsupervised representations today capture deep associations across multiple domains, and can be used successfully transfer knowledge into surprisingly diverse contexts (Brown et al., 2020). These representations require scale in terms of both data and pa"
2020.emnlp-main.703,N19-1197,1,0.927858,"the basis of action-oriented categories (Thelen and Smith, 1996) as children learn how to manipulate their perception by manipulating their environment. Language grounding enables an agent to connect words to these actionoriented categories for communication (Smith and Gasser, 2005), but requires action to fully discover such connections. Embodiment—situated action taking—is therefore a natural next broader context. An embodied agent, whether in a virtual world, such as a 2D Maze (MacMahon et al., 2006), a grid world (Chevalier-Boisvert et al., 2019), a simulated house (Anderson et al., 2018; Thomason et al., 2019b; Shridhar et al., 2020), or the real world (Tellex et al., 2011; Matuszek, 2018; Thomason et al., 2020; Tellex et al., 2020) must translate from language to action. Control and action taking open several new dimensions to understanding and actively learning about the world. Queries can be resolved via dialog-based exploration with a human interlocutor (Liu and Chai, 2015), even as new object properties, like texture and weight (Thomason et al., 2017), or feedback, like muscle activations (Moro and Kennington, 2018), become available. We see the need for embodied language with complex meaning"
2020.emnlp-main.703,P19-1656,0,0.0233734,"u et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretraining (e.g. via conceptual captions (Sharma et al., 2018)) or further broadened to include audio (Tsai et al., 2019). Vision can also help ground speech signals (Srinivasan et al., 2020; Harwath et al., 2019) to facilitate discovery of linguistic concepts (Harwath et al., 2020). At the same time, NLP resources contributed to the success of these vision backbones. Hierarchical semantic representations emerge from ImageNet classification pretraining partially due to class hypernyms owed to that dataset’s WordNet origins. For example, the person class sub-divides into many professions and hobbies, like firefighter, gymnast, and doctor. To differentiate such sibling classes, learned vectors can also encode lowe"
2020.emnlp-main.703,P10-1040,1,0.164233,"the recent success of deep learning methods. Outside of NLP, philosophy (Austin, 1975) and linguistics (Lakoff, 1973; Coleman and Kay, 1981) recognized that meaning is flexible yet structured. Early experiments on neural networks trained with sequences of words (Elman, 1990; Bengio et al., 2003) suggested that vector representations could capture both syntax and semantics. Subsequent experiments with larger models, documents, and corpora have demonstrated that representations learned from text capture a great deal of information about meaning in and out of context (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; McCann et al., 2017). The intuition of such embedding representations, that context lends meaning, has long been acknowledged (Firth, 1957; Turney and Pantel, 2010). Earlier on, discrete, hierarchical representations, such as agglomerative clustering guided by mutual information (Brown et al., 1992), were constructed with some innate interpretability. A word’s position in such a hierarchy captures semantic and syntactic distinctions. When the Baum–Welch algorithm (Welch, 2003) is applied to unsupervised Hidden Markov Models, it assigns a class distribution to every word"
2020.emnlp-main.703,2020.cl-1.2,0,0.0416453,"Missing"
2020.emnlp-main.703,N19-1364,0,0.123027,"chniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication. Improvements in hardware and data collection have galvanized progress in NLP across many benchmark tasks. Impressive performance has been achieved in language modeling (Radford et al., 2019; Zellers et al., 2019b; Keskar et al., 2019) and span-selection question answering (Devlin et al., 2019; Yang et al., 2019b; Lan et al., 2020) through massive data and massive models. With models exceeding human performance on such tasks, now is an excellent time to reflect on a key question: Where is NLP going? In this paper, we consider how the data and world a language learner is exposed to define and constrains the scope of that learner’s semantics. Meaning does not arise from the statistical distribution of words, but from their use by people to communicate. Many of the assumptions and understandings on which communication relies lie outside of text. We must consider what is missing from models trained solel"
2020.emnlp-main.703,N10-1125,1,\N,Missing
2020.emnlp-main.703,D12-1110,0,\N,Missing
2020.emnlp-main.703,D08-1094,0,\N,Missing
2020.emnlp-main.703,P15-1135,1,\N,Missing
2020.emnlp-main.703,D16-1230,0,\N,Missing
2020.emnlp-main.703,P18-2103,0,\N,Missing
2020.emnlp-main.703,W18-5446,0,\N,Missing
2020.emnlp-main.703,P19-1388,0,\N,Missing
2020.emnlp-main.703,D17-1259,0,\N,Missing
2020.emnlp-main.703,D18-1256,0,\N,Missing
2020.emnlp-main.703,N19-1423,0,\N,Missing
2020.emnlp-main.703,D19-1598,0,\N,Missing
2021.emnlp-main.85,2020.acl-main.232,0,0.0203588,"reason about human mental states in situated environments. Our platform, data, and models are made available‡ and will facilitate future work on physical agents that can effectively collaborate with humans through situated dialogue. including in custom 2D worlds (Liu et al., 2012; Udagawa and Aizawa, 2020, 2019), in the physical world with human-robot interactions (McGuire et al., 2002; Chai et al., 2014, 2018; Thomason et al., 2020), and in various 3D virtual worlds (Bisk et al., 2018; Suhr et al., 2019). Most closely, our environment builds upon recent work by NarayanChen et al. (2019) and Jayannavar et al. (2020), whereby computational models of user dialogue prediction and user next-action prediction are investigated in the setting of a collaborative dialogue task within the 3D virtual blocks world of Minecraft. However, to our knowledge, none of these previous works explicitly model theory of mind for dialogue agents. Theory of mind as a subject, especially in computation (Laird et al., 2017), has gained increased attention in areas including agent-agent reinforcement learning (Rabinowitz et al., 2018), dialogue systems (Qiu et al., 2021), human-computer interaction (Wang et al., 2021), agent-agent"
2021.emnlp-main.85,W12-1621,1,0.737095,"er diversity in modes of collaboration that are more representative of that in the real world. Third, we introduce a set of baseline computational models to infer fellow player mental states in situ, as a collaborative agent would, and highlight some further challenges present in moving towards building fully realistic agents able to reason about human mental states in situated environments. Our platform, data, and models are made available‡ and will facilitate future work on physical agents that can effectively collaborate with humans through situated dialogue. including in custom 2D worlds (Liu et al., 2012; Udagawa and Aizawa, 2020, 2019), in the physical world with human-robot interactions (McGuire et al., 2002; Chai et al., 2014, 2018; Thomason et al., 2020), and in various 3D virtual worlds (Bisk et al., 2018; Suhr et al., 2019). Most closely, our environment builds upon recent work by NarayanChen et al. (2019) and Jayannavar et al. (2020), whereby computational models of user dialogue prediction and user next-action prediction are investigated in the setting of a collaborative dialogue task within the 3D virtual blocks world of Minecraft. However, to our knowledge, none of these previous wo"
2021.emnlp-main.85,W13-4010,1,0.803923,"ds. Our empirical results demonstrate that while language is certainly important in this inference, the shared physical environment and the perceived activities play a greater role in shaping a partner’s understanding of each other in order to come to a common ground. The contributions of this work are threefold. First, we introduce M IND C RAFT, a task in which pairs of users collaboratively work to create novel materials by combining blocks in the 3D virtual world of Minecraft, with the ultimate objective of creating a final, goal material. Unlike prior work in situated collaborative tasks (Liu et al., 2013; Bisk ∗ Equal Contribution. † et al., 2018; Suhr et al., 2019), a key focus of our Work performed while the author was an undergraduate research assistant at the University of Michigan. work is to facilitate theory of mind modeling—the 1112 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1112–1125 c November 7–11, 2021. 2021 Association for Computational Linguistics ability to attribute mental states, both of one’s own and that of others—an important but not yet wellstudied topic in situated collaborative interactions. Within designed collaborativ"
2021.emnlp-main.85,N19-1423,0,0.0386705,"Missing"
2021.emnlp-main.85,L16-1019,0,0.0508289,"Missing"
2021.emnlp-main.85,2020.findings-emnlp.157,0,0.0420543,"Missing"
2021.emnlp-main.85,D19-1218,0,0.0745862,"certainly important in this inference, the shared physical environment and the perceived activities play a greater role in shaping a partner’s understanding of each other in order to come to a common ground. The contributions of this work are threefold. First, we introduce M IND C RAFT, a task in which pairs of users collaboratively work to create novel materials by combining blocks in the 3D virtual world of Minecraft, with the ultimate objective of creating a final, goal material. Unlike prior work in situated collaborative tasks (Liu et al., 2013; Bisk ∗ Equal Contribution. † et al., 2018; Suhr et al., 2019), a key focus of our Work performed while the author was an undergraduate research assistant at the University of Michigan. work is to facilitate theory of mind modeling—the 1112 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1112–1125 c November 7–11, 2021. 2021 Association for Computational Linguistics ability to attribute mental states, both of one’s own and that of others—an important but not yet wellstudied topic in situated collaborative interactions. Within designed collaborative tasks, we have users record their beliefs about the state of"
2021.emnlp-main.85,H89-1033,0,0.580452,"tners’ beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks. 1 Introduction Creating embodied, situated agents able to move in, communicate naturally about, and collaborate on human terms in the physical world has been a persisting goal in artificial intelligence (Winograd, 1972). During communication in such a setting, agents not only need to ground entities in language to that of the physical world; efficient and accurate human-agent collaboration further requires agents to reason about the progress of the task at hand and to plan and execute a series of collaborative steps, whilst maintaining common ground (Clark, 1996) with collaboration partners, in order to achieve a certain goal. Despite recent advances, we are still far away from fully enabling these desired agent behaviors. One key challenge is in an agent’s ability to establish and maintain common ground in"
2021.findings-acl.368,P19-1181,0,0.0220472,"Missing"
2021.findings-acl.368,2020.findings-emnlp.395,0,0.0360552,", sg arg ) where sg type is the type (e.g., Goto) and sg arg is the argument (e.g., Knife). Each an specifies a type (an type ) of action, from {RotateLeft, RotateRight, MoveAhead, LookUp, LookDown}. Each am has also two parts (am type , am arg ) where am type is the action type (e.g., TurnOn); am arg is the action argument (e.g., Faucet). Sub-Goal Planning. Sub-goal planning acquires a sequence of sub-goals sg1 , · · · , sgn to accomplish the high-level goal G. We predict the type sgitype and argument sgiarg separately to avoid the combinatorial expansion of the output space. Previous work (Jansen, 2020) models sub-goal planning merely from high-level goal directives without visual grounding. These plans are fixed and thus not robust to potential failures during execution and variations of the visual environment. To overcome these drawbacks, our sub-goal planning is done on the fly after the previous sub-goal is executed in the environment. More specifically, our sub-goal planning objective is to learn a model (Msg ) that takes the visual observation at the current step (vt ), the high-level goal directive (G), and a complete sub-goal history prior to the current step (sg<i ) to predict the c"
2021.findings-acl.368,D16-1155,1,0.828401,"robotics. One line of work particularly focuses on teaching robots new tasks through demonstration and instruction (Rybski et al., 2007; Mohseni-Kabir et al., 2018). Originated in the robotics community, learning from demonstration (LfD) (Thomaz and Cakmak, 2009; Argall et al., 2009) enables robots to learn a mapping from world states to robots’ manipulations based on human’s demonstration of desired robot behaviors. More recent work has also explored the use of natural language and dialogue together with demonstration to teach robots new actions (Mohan and Laird, 2014; Scheutz et al., 2017; Liu et al., 2016; She and Chai, 2017; Chai et al., 2018; Gluck and Laird, 2018). To facilitate task learning from natural language instructions, several benchmarks using simulated physical environment have been made available (Anderson et al., 2018; Misra et al., 2018; Blukis et al., 2019; Shridhar et al., 2020). In particular, the vision and language navigation (VLN) benchmark (Anderson et al., 2018) has received a lot of attention. Many models have been developed, such as the Speaker-Follower model (Fried et al., 2018), the Self-Monitoring Navigation Agent(Ma et al., 2019a; Ke et al., 2019), the Regretful A"
2021.findings-acl.368,P19-1441,0,0.0261809,"Scene Navigation Object Manipulation ???????? ?????? ?????? , ?????? ??_???????? ???? ??_???????? ??_?????? ???? , ???? , ???? Mask Selection Sum over heads & Softmax K LN Emb K K = ?????????? ???? , ??, ????<?? ?? = ??????????(???? , ???? , ?????? ⊕ ??<?? ) ?? = ??????????(???? , ???? , ?????? ⊕ ??<?? ) Type Arg FC & Softmax Object Detector Q BERT FC & LN FC & LN FC & LN Word Embedding FC position Mask action type and argument, and object masks respectively. As the three sub-problems share the similar input form, we solve them all together using a unified model based on multi-task learning (Liu et al., 2019a). Our model differs from previous works (Shridhar et al., 2020; Singh et al., 2020) in the following aspects. First, we do not apply recurrent state transitions, but feed the prediction history as the input to each subsequent prediction. This may help better capture correlations between predicates and other modalities. Second, we do not use dense visual features from the scene, but rather the object detection results. By doing this, we map different modalities to the word embedding space before feeding them into the transformer encoder, thus taking advantage of the pre-trained language model"
2021.findings-acl.368,2021.ccl-1.108,0,0.0263713,"Missing"
2021.findings-acl.368,2020.intexsempar-1.4,0,0.0999505,"Missing"
2021.findings-acl.368,D17-1106,0,0.0391213,"Missing"
2021.findings-acl.368,D18-1287,0,0.0215914,"Cakmak, 2009; Argall et al., 2009) enables robots to learn a mapping from world states to robots’ manipulations based on human’s demonstration of desired robot behaviors. More recent work has also explored the use of natural language and dialogue together with demonstration to teach robots new actions (Mohan and Laird, 2014; Scheutz et al., 2017; Liu et al., 2016; She and Chai, 2017; Chai et al., 2018; Gluck and Laird, 2018). To facilitate task learning from natural language instructions, several benchmarks using simulated physical environment have been made available (Anderson et al., 2018; Misra et al., 2018; Blukis et al., 2019; Shridhar et al., 2020). In particular, the vision and language navigation (VLN) benchmark (Anderson et al., 2018) has received a lot of attention. Many models have been developed, such as the Speaker-Follower model (Fried et al., 2018), the Self-Monitoring Navigation Agent(Ma et al., 2019a; Ke et al., 2019), the Regretful Agent (Ma et al., 2019b), and the environment drop-out model (Tan et al., 2019). The VLN benchmark is further extended to study the fidelity of instruction following (Jain et al., 2019) and examined 4203 to understand the bias of the benchmark (Zhang et"
2021.findings-acl.368,P17-1150,1,0.850816,"e of work particularly focuses on teaching robots new tasks through demonstration and instruction (Rybski et al., 2007; Mohseni-Kabir et al., 2018). Originated in the robotics community, learning from demonstration (LfD) (Thomaz and Cakmak, 2009; Argall et al., 2009) enables robots to learn a mapping from world states to robots’ manipulations based on human’s demonstration of desired robot behaviors. More recent work has also explored the use of natural language and dialogue together with demonstration to teach robots new actions (Mohan and Laird, 2014; Scheutz et al., 2017; Liu et al., 2016; She and Chai, 2017; Chai et al., 2018; Gluck and Laird, 2018). To facilitate task learning from natural language instructions, several benchmarks using simulated physical environment have been made available (Anderson et al., 2018; Misra et al., 2018; Blukis et al., 2019; Shridhar et al., 2020). In particular, the vision and language navigation (VLN) benchmark (Anderson et al., 2018) has received a lot of attention. Many models have been developed, such as the Speaker-Follower model (Fried et al., 2018), the Self-Monitoring Navigation Agent(Ma et al., 2019a; Ke et al., 2019), the Regretful Agent (Ma et al., 201"
2021.findings-acl.368,N19-1268,0,0.0117446,"8). To facilitate task learning from natural language instructions, several benchmarks using simulated physical environment have been made available (Anderson et al., 2018; Misra et al., 2018; Blukis et al., 2019; Shridhar et al., 2020). In particular, the vision and language navigation (VLN) benchmark (Anderson et al., 2018) has received a lot of attention. Many models have been developed, such as the Speaker-Follower model (Fried et al., 2018), the Self-Monitoring Navigation Agent(Ma et al., 2019a; Ke et al., 2019), the Regretful Agent (Ma et al., 2019b), and the environment drop-out model (Tan et al., 2019). The VLN benchmark is further extended to study the fidelity of instruction following (Jain et al., 2019) and examined 4203 to understand the bias of the benchmark (Zhang et al., 2020). Beyond navigation, there are also benchmarks that additionally incorporate object manipulation to broaden research on vision and language reasoning, such as embodied question answering (Das et al., 2018a; Gordon et al., 2018). The work closest to ours is the Neural Modular Control (NMC) (Das et al., 2018b), which also decomposes high-level tasks into sub-tasks and addresses each sub-task accordingly. However,"
2021.findings-emnlp.272,2020.acl-main.0,0,0.251225,"Missing"
2021.findings-emnlp.272,D15-1075,0,0.121657,"Missing"
2021.findings-emnlp.272,2020.acl-main.499,0,0.0192123,"enney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand model capabilities, from automatically removing words in language inputs and examining model performance as the input becomes malformed or insufficient for prediction (Li et al., 2016; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, this paper introduces an easily-accessed, versatile evaluation of machine coherence from a small amount of additional annotation. 3 Coherent Text Classification Dialog: ✗ ✗ A1: Well, ironically e"
2021.findings-emnlp.272,P03-1069,0,0.0982477,"; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, this paper introduces an easily-accessed, versatile evaluation of machine coherence from a small amount of additional annotation. 3 Coherent Text Classification Dialog: ✗ ✗ A1: Well, ironically enough I’m sitting here with a cast on my leg because I resumed an aerobics class the night before last. B1: Oh, no. A2: I ripped the ligaments in my right ankle. ✓ Hypothesis: Speaker A ripped the ligaments in her ankle at aerobics class. Figure 2: In CE, we label e"
2021.findings-emnlp.272,D19-1405,0,0.0237134,"r et al., 2019; Tenney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand model capabilities, from automatically removing words in language inputs and examining model performance as the input becomes malformed or insufficient for prediction (Li et al., 2016; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, this paper introduces an easily-accessed, versatile evaluation of machine coherence from a small amount of additional annotation. 3 Coherent Text Classification Dialog"
2021.findings-emnlp.272,2021.ccl-1.108,0,0.0573528,"Missing"
2021.findings-emnlp.272,2020.acl-main.212,0,0.0258922,"rge LMs, and show that our framework, although simple in ideas and implementation, is effective as a quick measure to provide insight into the coherence of machines’ predictions. Large-scale, pre-trained contextual language representations (Devlin et al., 2018; Radford et al., 2018; Raffel et al., 2020; Brown et al., 2020) have approached or exceeded human performance on many existing language understanding benchmarks. However, due to increasing complexity and concerns of statistical bias enabling artificially high performance (Schwartz et al., 2017; Poliak et al., 2018b; Niven and Kao, 2019; Min et al., 2020), the coherence of these state-of-the-art systems and their alignment to humans is not well understood. This is perhaps because benchmarks geared toward language understanding only cover the tip of the iceberg, typically focusing on a high-level end task rather than diving deeper into the kind of coherent, robust understanding that takes place in humans. Language understanding in machines is often boiled down to text classification, where a classifier is tasked with recognizing whether 2 Related Work a text contains a particular semantic class, e.g., textual entailment (Dagan et al., 2005; Bow"
2021.findings-emnlp.272,N16-1098,0,0.0197567,"nding only cover the tip of the iceberg, typically focusing on a high-level end task rather than diving deeper into the kind of coherent, robust understanding that takes place in humans. Language understanding in machines is often boiled down to text classification, where a classifier is tasked with recognizing whether 2 Related Work a text contains a particular semantic class, e.g., textual entailment (Dagan et al., 2005; Bowman In the face of data bias and uninterpretability of et al., 2015), commonsense implausibility (Roem- large LMs, past work has proposed methods to romele et al., 2011; Mostafazadeh et al., 2016; Bisk bustly interpret and evaluate them for various tasks 3169 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3169–3177 November 7–11, 2021. ©2021 Association for Computational Linguistics and domains. Some work has sought to probe contextual language representations through various means to better understand what knowledge they hold and their correspondence to syntactic and semantic patterns (Tenney et al., 2018; Hewitt and Manning, 2019; Jawahar et al., 2019; Tenney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand mode"
2021.findings-emnlp.272,P19-1459,0,0.0211876,"iors in fine-tuned large LMs, and show that our framework, although simple in ideas and implementation, is effective as a quick measure to provide insight into the coherence of machines’ predictions. Large-scale, pre-trained contextual language representations (Devlin et al., 2018; Radford et al., 2018; Raffel et al., 2020; Brown et al., 2020) have approached or exceeded human performance on many existing language understanding benchmarks. However, due to increasing complexity and concerns of statistical bias enabling artificially high performance (Schwartz et al., 2017; Poliak et al., 2018b; Niven and Kao, 2019; Min et al., 2020), the coherence of these state-of-the-art systems and their alignment to humans is not well understood. This is perhaps because benchmarks geared toward language understanding only cover the tip of the iceberg, typically focusing on a high-level end task rather than diving deeper into the kind of coherent, robust understanding that takes place in humans. Language understanding in machines is often boiled down to text classification, where a classifier is tasked with recognizing whether 2 Related Work a text contains a particular semantic class, e.g., textual entailment (Daga"
2021.findings-emnlp.272,W18-5441,0,0.0486446,"Missing"
2021.findings-emnlp.272,S18-2023,0,0.0476767,"Missing"
2021.findings-emnlp.272,2020.acl-main.442,0,0.0191577,"erstand what knowledge they hold and their correspondence to syntactic and semantic patterns (Tenney et al., 2018; Hewitt and Manning, 2019; Jawahar et al., 2019; Tenney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand model capabilities, from automatically removing words in language inputs and examining model performance as the input becomes malformed or insufficient for prediction (Li et al., 2016; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, this paper introduces an e"
2021.findings-emnlp.272,K17-1004,0,0.0158231,"lts support recent findings of spurious behaviors in fine-tuned large LMs, and show that our framework, although simple in ideas and implementation, is effective as a quick measure to provide insight into the coherence of machines’ predictions. Large-scale, pre-trained contextual language representations (Devlin et al., 2018; Radford et al., 2018; Raffel et al., 2020; Brown et al., 2020) have approached or exceeded human performance on many existing language understanding benchmarks. However, due to increasing complexity and concerns of statistical bias enabling artificially high performance (Schwartz et al., 2017; Poliak et al., 2018b; Niven and Kao, 2019; Min et al., 2020), the coherence of these state-of-the-art systems and their alignment to humans is not well understood. This is perhaps because benchmarks geared toward language understanding only cover the tip of the iceberg, typically focusing on a high-level end task rather than diving deeper into the kind of coherent, robust understanding that takes place in humans. Language understanding in machines is often boiled down to text classification, where a classifier is tasked with recognizing whether 2 Related Work a text contains a particular sem"
2021.findings-emnlp.272,P19-1452,0,0.039269,"Missing"
2021.findings-emnlp.272,2020.aacl-main.71,0,0.0364307,"; Hewitt and Manning, 2019; Jawahar et al., 2019; Tenney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand model capabilities, from automatically removing words in language inputs and examining model performance as the input becomes malformed or insufficient for prediction (Li et al., 2016; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, this paper introduces an easily-accessed, versatile evaluation of machine coherence from a small amount of additional annotation. 3 Coher"
2021.findings-emnlp.272,W18-5446,0,0.0554907,"Missing"
2021.findings-emnlp.272,P19-1363,0,0.0118416,"s (Tenney et al., 2018; Hewitt and Manning, 2019; Jawahar et al., 2019; Tenney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand model capabilities, from automatically removing words in language inputs and examining model performance as the input becomes malformed or insufficient for prediction (Li et al., 2016; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, this paper introduces an easily-accessed, versatile evaluation of machine coherence from a small amount of additiona"
2021.findings-emnlp.272,D18-1009,0,0.0356092,"Missing"
2021.findings-emnlp.272,D10-1074,1,0.624922,"Missing"
2021.findings-emnlp.272,D19-1332,0,0.0195779,"means to better understand what knowledge they hold and their correspondence to syntactic and semantic patterns (Tenney et al., 2018; Hewitt and Manning, 2019; Jawahar et al., 2019; Tenney et al., 2019). Meanwhile, behavior testing approaches have also been applied to understand model capabilities, from automatically removing words in language inputs and examining model performance as the input becomes malformed or insufficient for prediction (Li et al., 2016; Murdoch et al., 2018; Hewitt and Manning, 2019), to curating finegrained testing data to measure performance on interesting phenomena (Zhou et al., 2019; Ribeiro et al., 2020). Similar work has used specialized natural language inference tasks (Welleck et al., 2019; Uppal et al., 2020), logic rules (Li et al., 2019; Asai and Hajishirzi, 2020), and annotated explanations (DeYoung et al., 2020; Jhamtani and Clark, 2020) to support and evaluate consistency and coherence of inference in these models. Other works have studied coherence of discourse through the proxy task of sentence re-ordering (Lapata, 2003; Logeswaran et al., 2018). Different from these previous works that focus only on specific tasks or methods, or require heavy annotation, thi"
2021.metanlp-1.3,D14-1162,0,0.0879057,"au ⊆ as , ou ⊆ os and Ys ∩ Yu = φ. Moreover, we address the problem in both conventional ZSCL setting and generalized ZSCL setting. In conventional ZSCL, we only consider unseen pairs in the test phase and the target is to learn a mapping function V → Y u . In generalized ZSCL, images with both seen and unseen concepts can appear in the test set, and the mapping function changes to V → Y s ∪ Y u which is a more general and realistic setting. 3.2 3.3 Unimodal Representation Concept Representation. Given a compositonal concept (a, o), we ﬁrst transform attribute and object using 300-D GloVe (Pennington et al., 2014) separately. Then we use one layer BiLSTM (Hochreiter and Schmidhuber, 1997) to obtain contextualized representation for concepts with dk hidden units. Instead of using the ﬁnal state, we maintain the output features for both attribute and object and output feature matrix C ∈ R2×dk for each compoisitonal concept. Image Representation. We extract the visual features using pretrained ResNet (He et al., 2016) from a given image. In order to obtain more detailed visual features for concept recognition, we keep the output from the last convolutional layer of ResNet-18 to represent the image and the"
A97-2004,W97-1001,1,0.877937,"Missing"
A97-2004,H93-1026,0,\N,Missing
C02-1035,J86-3001,0,0.0767914,"y unit. During multimodal understanding, MIND combines semantic meanings of unimodal inputs (i.e., modality units), and uses contexts (e.g., conversation context and domain context) to form an overall understanding of user multimodal inputs. Such an overall understanding is then captured in a representation called conversation unit. Furthermore, MIND also identifies how an input relates to the overall conversation discourse through discourse understanding. In particular, MIND uses a representation called conversation segment to group together inputs that contribute to a same goal or sub-goal (Grosz and Sidner, 1986). The result of discourse understanding is an evolving conversation history that reflects the overall progress of a conversation. Figure 2 shows a conversation fragment between a user and MIND. In the first user input U1, the deictic Other RIA Components Conversation Segment Discourse Interpreter Discourse Understanding Conversation Unit Multimodal Interpreter Modality Unit (Gesture) Gesture Interpreter Multimodal Understanding Modality Unit Unimodal (Speech & Text) Understanding Domain, Visual Contexts Conversation History Inspired by earlier works on multimodal interfaces (e.g., Bolt, 1980;"
C02-1035,H94-1052,0,0.0106603,"ing User Inputs Given the semantic models of intention, attention and constraints, MIND represents those models using a combination of feature structures (Carpenter, 1992). This representation is inspired by the earlier works (Johnston et al., 1997; Johnston, 1998) and offers a flexibility to accommodate complex inputs. Specifically, MIND represents intention, attention and constraints identified from user inputs as a result of both unimodal understanding and multimodal understanding. During unimodal understanding, MIND applies a decision tree based semantic parser on natural language inputs (Jelinek et al., 1994) to identify salient information. For the gesture input, MIND applies a simple geometry-based recognizer. As a result, information from each unimodal input is represented in a modality unit. We have seen several modality units (in Figure 4, Figure 6, and Figure 7), where intention, attention and constraints are represented in feature structures. Note that only features that can be instantiated by information from the user input are included in the feature structure. For example, since the exact object cannot be identified from U1 speech input, the Content feature is not included in its Attenti"
C02-1035,P97-1036,0,0.0517219,"ve Manner: Comparative Aspect: Location Relation: Equals Anchor: “White Plains” (b) Attention structure in the conversation unit for U4 speech input Figure 8. Attention structures for U4 up with an intelligent response. Furthermore, these models are domain independent and can be applied to any information seeking applications (for structured information). 3.1.3 Representing User Inputs Given the semantic models of intention, attention and constraints, MIND represents those models using a combination of feature structures (Carpenter, 1992). This representation is inspired by the earlier works (Johnston et al., 1997; Johnston, 1998) and offers a flexibility to accommodate complex inputs. Specifically, MIND represents intention, attention and constraints identified from user inputs as a result of both unimodal understanding and multimodal understanding. During unimodal understanding, MIND applies a decision tree based semantic parser on natural language inputs (Jelinek et al., 1994) to identify salient information. For the gesture input, MIND applies a simple geometry-based recognizer. As a result, information from each unimodal input is represented in a modality unit. We have seen several modality units"
C02-1035,P98-1102,0,0.0217744,"Aspect: Location Relation: Equals Anchor: “White Plains” (b) Attention structure in the conversation unit for U4 speech input Figure 8. Attention structures for U4 up with an intelligent response. Furthermore, these models are domain independent and can be applied to any information seeking applications (for structured information). 3.1.3 Representing User Inputs Given the semantic models of intention, attention and constraints, MIND represents those models using a combination of feature structures (Carpenter, 1992). This representation is inspired by the earlier works (Johnston et al., 1997; Johnston, 1998) and offers a flexibility to accommodate complex inputs. Specifically, MIND represents intention, attention and constraints identified from user inputs as a result of both unimodal understanding and multimodal understanding. During unimodal understanding, MIND applies a decision tree based semantic parser on natural language inputs (Jelinek et al., 1994) to identify salient information. For the gesture input, MIND applies a simple geometry-based recognizer. As a result, information from each unimodal input is represented in a modality unit. We have seen several modality units (in Figure 4, Fig"
C02-1035,C98-1099,0,\N,Missing
chai-2000-evaluation,J87-3005,0,\N,Missing
chai-2000-evaluation,C92-2070,0,\N,Missing
chai-2000-evaluation,P96-1051,0,\N,Missing
chai-2000-evaluation,W97-0110,1,\N,Missing
chai-2000-evaluation,M91-1013,0,\N,Missing
D08-1026,J93-2003,0,0.0167108,"conversational systems where users interact with a visual scene, we consider the task of word acquisition as associating words with visual entities in the domain. Given the parallel speech and gaze fixated entities {(w, e)}, we formulate word acquisition as a translation problem and use translation models to estimate word-entity association probabilities p(w|e). The words with the highest association probabilities are chosen as acquired words for entity e. (ms) chandelier speech str eam (ms) gaze str eam te [19] [22] [ ] [10] [10] [11] [11] a 4 4.1 Base Model I Using the translation model I (Brown et al., 1993), where each word is equally likely to be aligned with each entity, we have [fixated entity ID] p(w|e) = ( [19] – bed_frame; [22] – door; [10] – bedroom; [11] – chandelier ) Figure 2: Parallel speech and gaze streams Figure 2 shows an excerpt of the collected speech and gaze fixation in one experiment. In the speech stream, each word starts at a particular timestamp. In the gaze stream, each gaze fixation has a starting timestamp ts and an ending timestamp te . Each gaze fixation also has a list of fixated entities (3D objects). An entity e on the graphical display is fixated by gaze fixation"
D08-1026,P07-1047,1,0.755236,"the visual attention foci accompanying speech are indicated by eye gaze. Eye gaze is an implicit and subconscious input, which brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze information and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions. A recent investigation on word acquisition from transcribed speech and eye gaze in human machine conversation was reported in (Liu et al., 2007). In this work, a translation model was developed to associate words with visual objects on a graphical display. Different from these previous works, here we investigate the incorporation of extra knowledge, specifically speech-gaze temporal information and domain knowledge, with eye gaze to facilitate word acquisition. 245 Data Collection We recruited users to interact with a simplified multimodal conversational system to collect speech and eye gaze data. 3.1 Domain We are working on a 3D room decoration domain. Figure 1 shows the 3D room scene that was shown to the user in the experiments. T"
D08-1026,N04-3012,0,0.0129843,"domain concepts linked to WordNet synsets where cie is the i-th property of entity e, s(cie ) is the synset of property cie as designed in domain model, sj (w) is the j-th synset of word w as defined in WordNet, and sim(·, ·) is the similarity score of two synsets. We computed the similarity score of two synsets based on the path length between them. The similarity score is inversely proportional to the number of nodes along the shortest path between the synsets as defined in WordNet. When the two synsets are the same, they have the maximal similarity score of 1. The WordNet-Similarity tool (Pedersen et al., 2004) was used for the synset similarity computation. 6.3 Word Acquisition with Word-Entity Semantic Relatedness We can use the semantic relatedness of word and entity to help the system acquire semantically compatible words for each entity, and therefore improve word acquisition performance. The semantic relatedness can be applied for word acquisition in two ways: post process learned word-entity association probabilities by rescoring them with semantic relatedness, or directly affect the learning of word-entity associations by constraining the alignment of word and entity in the translation model"
D08-1026,N07-1036,1,0.887943,"Missing"
D10-1046,P07-1047,1,0.811402,"ignal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang 472 et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static vis"
D10-1046,P03-1070,0,0.0104852,"sequently used to resolve references. In contrast to this line of research, here we explore the use of human eye gaze during real-time interaction to model attention and facilitate reference resolution. Eye gaze provides a richer medium for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang 472 et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Boc"
D10-1046,N07-1036,1,0.702853,"m for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang 472 et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capab"
D10-1074,H05-1079,0,0.0895607,"Missing"
D10-1074,P04-1085,0,0.0942694,"Missing"
D10-1074,P09-1080,0,0.0403128,"Missing"
D10-1074,W07-1401,0,0.0362908,"ion structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Example 1: Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B’s mother is eighty-three. (2) B is eighty-three."
D10-1074,H05-1049,0,0.0534411,"Missing"
D10-1074,P07-1131,0,0.0624664,"Missing"
D10-1074,P03-1054,0,0.00368965,"ve you seen Sleeping with the Enemy? A: No. I’ve heard that’s really great, though. B: You have to go see that one. Hypothesis: B suggests A to watch Sleeping with the Enemy. 4.1 • Vertices representing speakers or participants (e.g., sA , sB ). One edge is added to connect each utterance to its speaker (e.g., speaker(u1 , sB )). Basic Representation The first representation is based on the syntactic parsing from conversation utterances and we call it a basic representation. Figure 1(a) shows an example of dependency structures for several utterances that are derived from the Stanford parser (Klein and Manning, 2003), and Figure 1(b) shows the corresponding clause representation. In the dependency structure, the vertices represent entities (e.g., x1 ) and actions (e.g., x3 ) within an utterance. They correspond to terms in the clause representation. An edge between vertices captures a dependency relation and is represented as predicates in the clause representation. For example, the edge between x1 and x3 indicates x1 is the subject of x3 , which is represented by the clause representation subj(x3 , x1 ). Similar representation also applies to the hypothesis as shown in Figure 1(c), 1(d). 4.2 Augmented Re"
D10-1074,N06-1006,0,0.0344084,"Missing"
D10-1074,D08-1081,0,0.0688487,"Missing"
D10-1074,J08-2006,0,0.0401521,", where v1 , . . . , vl are the vertices on the path and e1 , . . . , el−1 are the edges. Each vi describes the type of the vertex in the dependency structure, which is either a noun (N ), a verb (V ), or an utterance (U ). Each ei describes whether the edge is forward (→) or backward (←). For example, in Figure 2(a), the path from x11 to x9 is V → V → V ← N. This kind of string representation of paths in syntactic parse is known as a way of modeling “shallow semantics” between any two constituents in a language structure. It is largely used in other NLP tasks such as semantic role labeling (Pradhan et al., 2008). The difference here is our paths are extracted from dependency parses as opposed to traditional constituent parses, and our paths also incorporate the representation of conversation structures (e.g., utterances and speakers). 6 Applications in Entailment Models In this section we describe how different representations and modeling of LDR are used in the alignment and inference models. 6.1 Applications in Alignment Model Although a noun and a verb can potentially be aligned, to simplify the problem, we restrict the problem to the alignment between two nouns or two verbs. We trained an alignme"
D10-1074,N10-1020,0,0.0671577,"Missing"
D10-1074,2007.sigdial-1.5,0,0.0570332,"Missing"
D10-1074,C08-1101,0,0.0243075,"Missing"
D10-1074,D09-1018,0,0.0515395,"Missing"
D10-1074,H05-1047,0,0.032381,"Missing"
D10-1074,P06-1051,0,0.0605192,"Missing"
D10-1074,W09-3930,1,0.840023,"2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Conversation Segment: B: My mother also was very very independent. She had her own, still had her own little house and still driving her own car, A: Yeah. B: at age eighty-three. Hypothesis: (1) B’s mother is eighty-three. (2) B is eighty-three. To address this limitation, our previous work (Zhang and Chai, 2009) has initiated an investigation on the problem of conversation entailment. The problem was formulated as follows: given a conversation discourse D and a hypothesis H concerning its participant, the goal was to identify whether D entails H. For instance, as in Example 1, the first hypothesis can be entailed from the 756 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756–766, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics conversation segment while the second hypothesis cannot. While our previous work ha"
D10-1074,N07-1070,0,\N,Missing
D13-1038,P89-1009,0,0.394088,"Missing"
D13-1038,C94-2182,0,0.367552,"Missing"
D13-1038,C04-1096,0,0.470386,"jects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also"
D13-1038,W06-1411,0,0.395468,"ognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also represent group-based re"
D13-1038,W08-1108,0,0.0428944,"Missing"
D13-1038,W07-2307,0,0.137443,"Missing"
D13-1038,W09-0629,0,0.0680414,"Missing"
D13-1038,E06-1041,0,0.0166618,"he first heuristic is based on perceptual principles, also called the Gestalt Laws of perception (Sternberg, 2003), which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity (Wertheimer, 1938): objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions. Based on these two principles, previous works have developed different algorithms for perceptual grouping (Thrisson, 1994; Gatt, 2006). In our investigation, we adopted Gatt’s algorithm (Gatt, 2006), which has shown to be more accurate for spatial grouping. Given the results from spatial grouping, we only retain hyperarcs that represent spatial relations between two objects, between two perceived groups, between one object and a perceived group, or between one object and the group it belongs to. The second heuristic is based on the observation that, given a certain orientation, people tend to use a relatum that is closer to the referent than more distant relata. In other words, it is less likely to refer to an object relativ"
D13-1038,D10-1040,0,0.309464,"tual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, the perfect knowledge of the environment is not available to the agent ahead of time. The agent needs to automatically make inferences to connect recognized lower-level visual features with 392 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392–402"
D13-1038,J95-3003,0,0.286131,"Missing"
D13-1038,W05-1606,0,0.21954,"r graphics and thus the internal representation of the scene is known. Some other work focuses on the connection between lower-level visual features and symbolic descriptors for REG (Roy, 2002; Mitchell et al., 2013). However, most work assumes no vision recognition errors. It is well established that automated recognition of visual scenes is extremely challenging. This process is error-prone and full of uncertainties. It is not clear whether the existing approaches can be extended to the situation where the agent has imperfect perception of the shared environment. An earlier work by Horacek (Horacek, 2005) has looked into the problem of mismatched knowledge between conversation partners for REG. The approach is a direct extension of the incremental algorithm (Dale, 1995). However, this work only provides a proof of concept example to illustrate the idea. No empirical evaluation was given. All these previous works have motivated our present investigation. We are interested in REG under mismatched perceptual basis between conversation partners, where the agent has imperfect perception and knowledge of the shared environment. In particular, we took a well-studied graph-based approach (Krahmer et a"
D13-1038,P00-1024,0,0.140695,"Missing"
D13-1038,P06-1131,0,0.458798,"tual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can be incorporated. We then describe an empirical study using Amazon Mechanical Turks for evaluating generated referring expressions. Finally we present evaluation results and discuss potential future directions. 2 Related Work Since the Full Brevity algorithm (Dale, 1989), many approaches have been developed and evaluated for REG (Dale, 1995; Krahmer and Deemter, 2012), such as the incremental algorithm (Dale, 1995), the locative algorithm (Kelleher and Kruijff, 2006), and graph-based approaches (Krahmer et al., 2003; Croitoru and Van Deemter, 2007). Most of these ap393 proaches assume the agent has access to a complete symbolic representation of the domain. While these approaches work well for many applications involving user interfaces, the question is whether they can be extended to the situation where the agent has incomplete or incorrect knowledge and needs to make inference about the domain or the world. Recently, there has been increasing interest in REG for visual objects (Roy, 2002; Golland et al., 2010; Mitchell et al., 2013). Some work (Golland"
D13-1038,J12-1006,0,0.185792,"Missing"
D13-1038,J03-1003,0,0.0969041,"Missing"
D13-1038,W08-1138,0,0.123366,"k). Each referring expression received three votes from the crowd. In total, 217 turks participated in our experiment. 4.2 Generation Strategies We applied a set of different strategies to generate referring expressions for each object. The variations lie in two dimensions: (1) different graph representations: using a hypergraph to represent the perceived scene as described in Section 3.1 versus using a regular graph as introduced in (Krahmer et al., 2003); and (2) different cost functions for attributes and relations: cost functions that have been used in previous works (Theune et al., 2007; Krahmer et al., 2008) and cost functions that incorporate uncertainties of perception as described in Section 3.3.2. Cost functions play an important role in graphbased approaches (Krahmer et al., 2003). Previous works have examined different types of cost functions (Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). We adopted some commonly used cost functions from previous work together with the cost functions defined here. In particular, we experimented with the following different cost functions: Simple Cost: The costs for all hyperarcs are set to 1. With this cost function, the graph-based algor"
D13-1038,W12-1621,1,0.844326,"and make inference of the shared environment. Due to its limited perceptual and reasoning capabilities, the robot’s representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner’s. Although physically co-present, a joint perceptual basis between the human and the robot cannot be established (Clark and Brennan, 1991). Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012). In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human pa"
D13-1038,W13-4010,1,0.799738,"tructed to collaborate with each other on some naming games. Through these games, they collected data on how partners with mismatched perceptual capabilities collaborate to ground their referential communication. The setup in (Liu et al., 2012) is intended to simulate situated dialogue between a human (like the director) and a robot (like the matcher). The robot has a significantly lowered ability in perception and reasoning. The robot’s internal representation of the shared world will be much like the impoverished scene which contains many recognition errors. The data from (Liu et al., 2012; Liu et al., 2013) shows that different strategies were used by conversation partners to produce referential descriptions. Besides directly describing attributes or binary relations with a relatum, they often use group-based descriptions 394 (e.g., a cluster of four objects on the right). This is mainly due to the fact that some objects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs t"
D13-1038,W11-2808,0,0.191993,"Missing"
D13-1038,N13-1137,0,0.0996653,"Missing"
D13-1038,W08-0113,0,0.0301709,"riptions, the first step is to assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or 396 spatialRel : above(a, b) = fabove (v~aloc , v~bloc )  −xb | 1 − |xa400 if ya &lt; yb ; = 0 otherwise. Using the above convention, we have defined semantic grounding fun"
D13-1038,W12-1504,0,0.0243256,"he environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, the perfect knowledge of the environment is not available to the agent ahead of time. The agent needs to automatically make inferences to connect recognized lower-level visual features with 392 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392–402, c Seattle, Washington, U"
D13-1038,2007.mtsummit-ucnlg.19,0,0.118595,"ce identification task). Each referring expression received three votes from the crowd. In total, 217 turks participated in our experiment. 4.2 Generation Strategies We applied a set of different strategies to generate referring expressions for each object. The variations lie in two dimensions: (1) different graph representations: using a hypergraph to represent the perceived scene as described in Section 3.1 versus using a regular graph as introduced in (Krahmer et al., 2003); and (2) different cost functions for attributes and relations: cost functions that have been used in previous works (Theune et al., 2007; Krahmer et al., 2008) and cost functions that incorporate uncertainties of perception as described in Section 3.3.2. Cost functions play an important role in graphbased approaches (Krahmer et al., 2003). Previous works have examined different types of cost functions (Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). We adopted some commonly used cost functions from previous work together with the cost functions defined here. In particular, we experimented with the following different cost functions: Simple Cost: The costs for all hyperarcs are set to 1. With this cost function"
D13-1038,P11-2116,0,0.0149543,"corporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work (Krahmer et al., 2003; Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). For example, we did not include the stochastic cost function which is defined based on the frequencies of attribute selection from the training data (Krahmer et al., 2003). On the one hand, we did not have a large set of human descriptions of the impoverished scene to learn the stochastic cost. On the other hand, it is not clear whether human strategies of describing the impoverished scene should be used to represent optimal strategies for the robot. Nevertheless, the above different cost functions will allow us to evaluate whether incorporating perceptual uncertainties will make a differenc"
D13-1038,W08-1109,0,0.334451,"s have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, the perfect knowledge of the environment is not available to the agent ahead of time. The agent needs to automatically make inferences to connect recognized lower-level visual features with 392 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro"
D16-1155,P10-1129,0,0.00982613,"tructure of state changes detected 1483 Figure 1: The setting of our situated task learning where a human teacher teaches the robot how to fold a T-shirt through both task demonstrations and language instructions. from the physical world. As demonstrated in recent work (She et al., 2014a; Misra et al., 2015; She and Chai, 2016), explicitly modeling change of states is an important step towards interacting with robots in the physical world. Additionally, there has also been an increasing amount of work that learns new tasks either using methods like supervised learning on large corpus of data (Branavan et al., 2010; Branavan et al., 2012; Tellex et al., 2014; Misra et al., 2014), or by learning from humans through dialogue (Cantrell et al., 2012; Mohan et al., 2013; Kirk and Laird, 2013; She et al., 2014b; Mohseni-Kabir et al., 2015). In this paper, we focus on jointly learning new tasks through visual demonstration and language instruction. The learned task model is explicitly represented by an AoG, a hierarchical structure consisting of both linguistic labels and corresponding changes of states from the physical world. This rich task model will facilitate not only language-based communication, but als"
D16-1155,P12-1014,0,0.0146212,"es detected 1483 Figure 1: The setting of our situated task learning where a human teacher teaches the robot how to fold a T-shirt through both task demonstrations and language instructions. from the physical world. As demonstrated in recent work (She et al., 2014a; Misra et al., 2015; She and Chai, 2016), explicitly modeling change of states is an important step towards interacting with robots in the physical world. Additionally, there has also been an increasing amount of work that learns new tasks either using methods like supervised learning on large corpus of data (Branavan et al., 2010; Branavan et al., 2012; Tellex et al., 2014; Misra et al., 2014), or by learning from humans through dialogue (Cantrell et al., 2012; Mohan et al., 2013; Kirk and Laird, 2013; She et al., 2014b; Mohseni-Kabir et al., 2015). In this paper, we focus on jointly learning new tasks through visual demonstration and language instruction. The learned task model is explicitly represented by an AoG, a hierarchical structure consisting of both linguistic labels and corresponding changes of states from the physical world. This rich task model will facilitate not only language-based communication, but also lower-level action pl"
D16-1155,P16-1171,1,0.796445,"of acquiring an effective and meaningful task model to compensate the uncertainties in visual processing. Once the AoG for the task is learned, it can be applied by our inference algorithm, for example, to infer on-going actions from new visual demonstration and generate linguistic labels at different levels of granularity to facilitate human-agent communication. 2 Related Work Recent years have seen an increasing amount of work on grounding language to visual perception (Liu et al., 2012; Matuszek et al., 2012; Yu and Siskind, 2013; Kollar et al., 2013; Naim et al., 2015; Yang et al., 2016; Gao et al., 2016). Furthermore, the robotics community made significant efforts to utilize novel grounding techniques to facilitate task execution given natural language instructions (Chen et al., 2010; Kollar et al., 2010; Tellex et al., 2011; Misra et al., 2014) and task learning from demonstration (Saunders et al., 2006; Chernova and Veloso, 2008). Research on Learning from Demonstration (LfD) employed various approaches to model the tasks (Argall et al., 2009), such as state-to-action mapping (Chernova and Veloso, 2009), predicate calculus (Hofmann et al., 2016), and Hierarchical Task Networks (Nejati et a"
D16-1155,W12-1621,1,0.894391,"cs, computer vision, and natural language processing, a new generation of cognitive robots have emerged that aim to collaborate with humans in joint tasks. To facilitate natural and efficient communication with these physical agents, natural language processing will need to go beyond traditional symbolic representations, but rather ground language to sensors (e.g., visual perception) and actuators (e.g., lower-level control systems) of physical agents. The internal task * The first two authors contributed equally to this paper. Different from previous works that ground language to perception (Liu et al., 2012; Matuszek et al., 2012; Kollar et al., 2013; Yu and Siskind, 2013; Yang et al., 2016), a key innovation in our framework is that language is no longer grounded just to perceived objects in the environment, but is further grounded to a hierarchical structure of state changes where the states are perceived from the environment during visual demonstration. The state of environment is an important notion in robotic systems as the change of states drives planning for lower-level robotic actions. Thus, connecting language concepts to state changes, our learned AoG provides a unified representation"
D16-1155,P15-1096,0,0.0452335,"on mapping (Chernova and Veloso, 2009), predicate calculus (Hofmann et al., 2016), and Hierarchical Task Networks (Nejati et al., 2006; Hogg et al., 2009). However, aspiring to enable human robot communication, the framework developed in this paper focuses on task representation using language grounded to a structure of state changes detected 1483 Figure 1: The setting of our situated task learning where a human teacher teaches the robot how to fold a T-shirt through both task demonstrations and language instructions. from the physical world. As demonstrated in recent work (She et al., 2014a; Misra et al., 2015; She and Chai, 2016), explicitly modeling change of states is an important step towards interacting with robots in the physical world. Additionally, there has also been an increasing amount of work that learns new tasks either using methods like supervised learning on large corpus of data (Branavan et al., 2010; Branavan et al., 2012; Tellex et al., 2014; Misra et al., 2014), or by learning from humans through dialogue (Cantrell et al., 2012; Mohan et al., 2013; Kirk and Laird, 2013; She et al., 2014b; Mohseni-Kabir et al., 2015). In this paper, we focus on jointly learning new tasks through"
D16-1155,N15-1017,0,0.0163075,"ghtly incorporates language is capable of acquiring an effective and meaningful task model to compensate the uncertainties in visual processing. Once the AoG for the task is learned, it can be applied by our inference algorithm, for example, to infer on-going actions from new visual demonstration and generate linguistic labels at different levels of granularity to facilitate human-agent communication. 2 Related Work Recent years have seen an increasing amount of work on grounding language to visual perception (Liu et al., 2012; Matuszek et al., 2012; Yu and Siskind, 2013; Kollar et al., 2013; Naim et al., 2015; Yang et al., 2016; Gao et al., 2016). Furthermore, the robotics community made significant efforts to utilize novel grounding techniques to facilitate task execution given natural language instructions (Chen et al., 2010; Kollar et al., 2010; Tellex et al., 2011; Misra et al., 2014) and task learning from demonstration (Saunders et al., 2006; Chernova and Veloso, 2008). Research on Learning from Demonstration (LfD) employed various approaches to model the tasks (Argall et al., 2009), such as state-to-action mapping (Chernova and Veloso, 2009), predicate calculus (Hofmann et al., 2016), and H"
D16-1155,P02-1014,0,0.0249931,"n the AoG represents a primitive action, which causes the object to directly change from one state to another. Thus we represent a terminal node as a 2-tuple of states (or a “change of state”). Since the visual states detected by computer vision are numeric vectors with continuous values, we first apply a clustering algorithm to form a finite set of discrete state representations. Each cluster then represents a unique situation that one can encounter in a task. Since when learning a new task we usually do not know how many unique situations exist, here we employ a greedy clustering algorithm (Ng and Cardie, 2002), which does not assume a fixed number of clusters. As the greedy clustering algorithm relies on the pairwise similarities of all the visual states, we also train an SVM classifier on a separate dataset of 22 Tshirt folding videos and use its classification output to measure the similarity between two visual states. The SVM classifier takes two numeric vectors as an input, and predicts whether these two vectors represent the same status of a T-shirt. We then apply 1486 = arg max P (N, R|X , Σ) P (Θ|X , Σ, N, R). N,R,Θ Following the iterative grammar induction paradigm (Tu et al., 2013; Xiong e"
D16-1155,P16-1011,1,0.838731,"and Veloso, 2009), predicate calculus (Hofmann et al., 2016), and Hierarchical Task Networks (Nejati et al., 2006; Hogg et al., 2009). However, aspiring to enable human robot communication, the framework developed in this paper focuses on task representation using language grounded to a structure of state changes detected 1483 Figure 1: The setting of our situated task learning where a human teacher teaches the robot how to fold a T-shirt through both task demonstrations and language instructions. from the physical world. As demonstrated in recent work (She et al., 2014a; Misra et al., 2015; She and Chai, 2016), explicitly modeling change of states is an important step towards interacting with robots in the physical world. Additionally, there has also been an increasing amount of work that learns new tasks either using methods like supervised learning on large corpus of data (Branavan et al., 2010; Branavan et al., 2012; Tellex et al., 2014; Misra et al., 2014), or by learning from humans through dialogue (Cantrell et al., 2012; Mohan et al., 2013; Kirk and Laird, 2013; She et al., 2014b; Mohseni-Kabir et al., 2015). In this paper, we focus on jointly learning new tasks through visual demonstration"
D16-1155,W14-4313,1,0.884456,"ch as state-to-action mapping (Chernova and Veloso, 2009), predicate calculus (Hofmann et al., 2016), and Hierarchical Task Networks (Nejati et al., 2006; Hogg et al., 2009). However, aspiring to enable human robot communication, the framework developed in this paper focuses on task representation using language grounded to a structure of state changes detected 1483 Figure 1: The setting of our situated task learning where a human teacher teaches the robot how to fold a T-shirt through both task demonstrations and language instructions. from the physical world. As demonstrated in recent work (She et al., 2014a; Misra et al., 2015; She and Chai, 2016), explicitly modeling change of states is an important step towards interacting with robots in the physical world. Additionally, there has also been an increasing amount of work that learns new tasks either using methods like supervised learning on large corpus of data (Branavan et al., 2010; Branavan et al., 2012; Tellex et al., 2014; Misra et al., 2014), or by learning from humans through dialogue (Cantrell et al., 2012; Mohan et al., 2013; Kirk and Laird, 2013; She et al., 2014b; Mohseni-Kabir et al., 2015). In this paper, we focus on jointly learni"
D16-1155,N16-1019,1,0.771632,"robots have emerged that aim to collaborate with humans in joint tasks. To facilitate natural and efficient communication with these physical agents, natural language processing will need to go beyond traditional symbolic representations, but rather ground language to sensors (e.g., visual perception) and actuators (e.g., lower-level control systems) of physical agents. The internal task * The first two authors contributed equally to this paper. Different from previous works that ground language to perception (Liu et al., 2012; Matuszek et al., 2012; Kollar et al., 2013; Yu and Siskind, 2013; Yang et al., 2016), a key innovation in our framework is that language is no longer grounded just to perceived objects in the environment, but is further grounded to a hierarchical structure of state changes where the states are perceived from the environment during visual demonstration. The state of environment is an important notion in robotic systems as the change of states drives planning for lower-level robotic actions. Thus, connecting language concepts to state changes, our learned AoG provides a unified representation that integrates language and vision to not only support language-based communication b"
D16-1155,P13-1006,0,0.0427321,"neration of cognitive robots have emerged that aim to collaborate with humans in joint tasks. To facilitate natural and efficient communication with these physical agents, natural language processing will need to go beyond traditional symbolic representations, but rather ground language to sensors (e.g., visual perception) and actuators (e.g., lower-level control systems) of physical agents. The internal task * The first two authors contributed equally to this paper. Different from previous works that ground language to perception (Liu et al., 2012; Matuszek et al., 2012; Kollar et al., 2013; Yu and Siskind, 2013; Yang et al., 2016), a key innovation in our framework is that language is no longer grounded just to perceived objects in the environment, but is further grounded to a hierarchical structure of state changes where the states are perceived from the environment during visual demonstration. The state of environment is an important notion in robotic systems as the change of states drives planning for lower-level robotic actions. Thus, connecting language concepts to state changes, our learned AoG provides a unified representation that integrates language and vision to not only support language-b"
D18-1283,P17-1025,0,0.0201257,"plain model behaviors by mining semantic meanings of filters (Zhang et al., 2017a,b) or by generating language explanations (Hendricks et al., 2016; Park et al., 2018). An increasing amount of work on the Visual Question Answering (VQA) task (Antol et al., 2015; Lu et al., 2016) has also looked into more interpretable approaches, for example, by utilizing attention-based models (Fukui et al., 2016) or reasoning based on explicit evidence (Wang et al., 2017). Specifically for action understanding, recent work explicitly models commonsense knowledge including causal relations (Gao et al., 2016; Forbes and Choi, 2017; Zellers and Choi, 2017; Gao et al., 2018) related to concrete actions, which can facilitate action explanation. Commonsense knowledge can be acquired from image annotations (Yatskar et al., 2016) or learned from visual abstraction (Vedantam et al., 2015). Different from the above work, our work here focuses on learning to acquire commonsense evidence for action justification. 3 A Study on Justification Explanation While there is a rich literature on explanations in Psychology, Philosophy, and Linguistics, particularly for higher-level events and decision making (Thagard, 2000; Lombrozo, 2012"
D18-1283,D16-1044,0,0.0381503,"terest in Explainable AI. For example, previous work has applied high-precision rules to explain classifiers’ decisions (Ribeiro et al., 2016, 2018). For Convolutional Neural Networks (CNNs), recent work attempts to explain model behaviors by mining semantic meanings of filters (Zhang et al., 2017a,b) or by generating language explanations (Hendricks et al., 2016; Park et al., 2018). An increasing amount of work on the Visual Question Answering (VQA) task (Antol et al., 2015; Lu et al., 2016) has also looked into more interpretable approaches, for example, by utilizing attention-based models (Fukui et al., 2016) or reasoning based on explicit evidence (Wang et al., 2017). Specifically for action understanding, recent work explicitly models commonsense knowledge including causal relations (Gao et al., 2016; Forbes and Choi, 2017; Zellers and Choi, 2017; Gao et al., 2018) related to concrete actions, which can facilitate action explanation. Commonsense knowledge can be acquired from image annotations (Yatskar et al., 2016) or learned from visual abstraction (Vedantam et al., 2015). Different from the above work, our work here focuses on learning to acquire commonsense evidence for action justification."
D18-1283,P16-1171,1,0.831077,"ork attempts to explain model behaviors by mining semantic meanings of filters (Zhang et al., 2017a,b) or by generating language explanations (Hendricks et al., 2016; Park et al., 2018). An increasing amount of work on the Visual Question Answering (VQA) task (Antol et al., 2015; Lu et al., 2016) has also looked into more interpretable approaches, for example, by utilizing attention-based models (Fukui et al., 2016) or reasoning based on explicit evidence (Wang et al., 2017). Specifically for action understanding, recent work explicitly models commonsense knowledge including causal relations (Gao et al., 2016; Forbes and Choi, 2017; Zellers and Choi, 2017; Gao et al., 2018) related to concrete actions, which can facilitate action explanation. Commonsense knowledge can be acquired from image annotations (Yatskar et al., 2016) or learned from visual abstraction (Vedantam et al., 2015). Different from the above work, our work here focuses on learning to acquire commonsense evidence for action justification. 3 A Study on Justification Explanation While there is a rich literature on explanations in Psychology, Philosophy, and Linguistics, particularly for higher-level events and decision making (Thagar"
D18-1283,P18-1086,1,0.888617,"gs of filters (Zhang et al., 2017a,b) or by generating language explanations (Hendricks et al., 2016; Park et al., 2018). An increasing amount of work on the Visual Question Answering (VQA) task (Antol et al., 2015; Lu et al., 2016) has also looked into more interpretable approaches, for example, by utilizing attention-based models (Fukui et al., 2016) or reasoning based on explicit evidence (Wang et al., 2017). Specifically for action understanding, recent work explicitly models commonsense knowledge including causal relations (Gao et al., 2016; Forbes and Choi, 2017; Zellers and Choi, 2017; Gao et al., 2018) related to concrete actions, which can facilitate action explanation. Commonsense knowledge can be acquired from image annotations (Yatskar et al., 2016) or learned from visual abstraction (Vedantam et al., 2015). Different from the above work, our work here focuses on learning to acquire commonsense evidence for action justification. 3 A Study on Justification Explanation While there is a rich literature on explanations in Psychology, Philosophy, and Linguistics, particularly for higher-level events and decision making (Thagard, 2000; Lombrozo, 2012; Dennett, 1987), explanations for recognit"
D18-1283,D14-1162,0,0.0839234,"ure 2. From an image, we first extract a candidate relation set R and an attribute set E. Every relation r and attribute e are embedded using a Gated Recurrent Neural Network (Chung et al., 2014). remb = GRU([rp , rs , ro ]) pθ (a|z, R, E)p(z|R, E) eemb = GRU([eo , ep ]) z Directly optimizing this conditional probability is not feasible. Usually the Evidence Lower Bound (ELBO) (Sohn et al., 2015) is optimized, which can be derived as the following: ELBO(a, R, E; θ, φ) = − KL(qφ (z|a, R, E)||pθ (z|R, E)) + Eqφ (z|a,R,E) [log pθ (a|z, R, E)] (1) The action a is represented by a GloVe embedding (Pennington et al., 2014), followed by another non-linear layer: aemb = ReLU(Wi aglove + bi ) where aglove ∈ Rk is the pre-trained GloVe embedding. Then the latent variable z can be calculated as: qφ (z|a, R, E) = softmax(Wz [U; aemb ] + bz ) ≤ log p(a|R, E) 2631 emb emb emb where U = [remb 1 , ..., rm , e1 , ..., en ] and emb [U, a ] means the concatenation of U and aemb . and Wz ∈ R2×2k as we assume each zi belongs to one of the two classes {0, 1}. The prior distribution can be calculated as: Relation Embedding r1: (hold, hand, knife) …… rm: (on, knife, cutting-board) Context Vector v Attention Score α GRU Image Att"
D18-1283,N16-3020,0,0.00983974,"ble to the community1 . It can serve as a benchmark for future work on this topic. 1 The dataset is available at https://github.com/ yangshao/Commonsense4Action 2 Related Work Advanced machine learning such as deep learning approaches have shown effectiveness in many applications, however, they often lack transparency and interpretability. This makes it difficult for humans to understand the agent’s capabilities and limitations. To address this problem, there is a growing interest in Explainable AI. For example, previous work has applied high-precision rules to explain classifiers’ decisions (Ribeiro et al., 2016, 2018). For Convolutional Neural Networks (CNNs), recent work attempts to explain model behaviors by mining semantic meanings of filters (Zhang et al., 2017a,b) or by generating language explanations (Hendricks et al., 2016; Park et al., 2018). An increasing amount of work on the Visual Question Answering (VQA) task (Antol et al., 2015; Lu et al., 2016) has also looked into more interpretable approaches, for example, by utilizing attention-based models (Fukui et al., 2016) or reasoning based on explicit evidence (Wang et al., 2017). Specifically for action understanding, recent work explicitl"
D18-1283,N16-1174,0,0.0715834,"Missing"
D18-1283,N16-1023,0,0.0699781,"Missing"
D18-1283,D17-1099,0,0.0224037,"y mining semantic meanings of filters (Zhang et al., 2017a,b) or by generating language explanations (Hendricks et al., 2016; Park et al., 2018). An increasing amount of work on the Visual Question Answering (VQA) task (Antol et al., 2015; Lu et al., 2016) has also looked into more interpretable approaches, for example, by utilizing attention-based models (Fukui et al., 2016) or reasoning based on explicit evidence (Wang et al., 2017). Specifically for action understanding, recent work explicitly models commonsense knowledge including causal relations (Gao et al., 2016; Forbes and Choi, 2017; Zellers and Choi, 2017; Gao et al., 2018) related to concrete actions, which can facilitate action explanation. Commonsense knowledge can be acquired from image annotations (Yatskar et al., 2016) or learned from visual abstraction (Vedantam et al., 2015). Different from the above work, our work here focuses on learning to acquire commonsense evidence for action justification. 3 A Study on Justification Explanation While there is a rich literature on explanations in Psychology, Philosophy, and Linguistics, particularly for higher-level events and decision making (Thagard, 2000; Lombrozo, 2012; Dennett, 1987), explan"
D18-1283,P17-1061,0,0.0168133,"ow) …… an: (banana, sliced) softmax chop: 0.6 drink: 0.1 …… pull: 0.1 feed: 0.1 qφ(z|a, R, E) Action Embedding chop Glove … Linear +Relu Posterior Embedding Figure 2: System architecture for the CVAE model. The dotted region is only used during the model training process. or discrete graphical models and neural network with latent variables. The VAE models the generative process of a random variable x as following: first the latent variable z is generated from a prior probability distribution p(z), then a data sample x is generated from a conditional probability distribution p(x|z). The CVAE (Zhao et al., 2017) is a natural extension of VAE: Both the prior distribution and conditional distribution now are conditioned on an additional context c: p(z|c) and p(x|z, c). In our task, we decompose the inference problem p(a, z|R, E) into two smaller problems. The first sub-problem is to infer p(a|R, E), which is a performer. The second problem is to infer p(z|a, R, E) which is an explainer. These two problems are closely coupled, hence we model them jointly. The probability distribution p(a|R, E) can be written as : p(a|R, E) = X The first KL divergence term is to minimize the distance between the posterio"
H01-1012,P98-2219,0,0.0252883,"k queries directly in their own words. Thus, users do not have to understand the terminology used by system designers identifies concepts (e.g., MULTIMEDIA) and constraints on to label hyperlinks on a website or internalize the hierarchical product attributes (e.g., hard disk size more than 20GB) from the menus of a telephone system [3] or websites. textual user input. The concepts mediate the mapping between user Recently, conversational interfaces for executing simple transactions input and available products through product specifications. They and for finding information are proliferating [7,6]. In this paper, we implement the business logic. present a conversational dialog system, Natural Language Assistant The Dialog Manager uses the current requirements and formulates (or NLA), that helps users shop for notebook computers and discuss action plans for the Action Manager to perform back-end operations the results of user studies that we conducted with this system. (e.g., database access1). The Dialog Manager constructs a response to the user based on the results from the Action Manager and the discourse history and sends the system response to the Presentation 2. NATURAL LANGUAGE A"
H01-1012,H01-1013,1,\N,Missing
H01-1012,C98-2214,0,\N,Missing
H01-1013,P98-2219,0,0.224182,"Missing"
H01-1013,C98-2214,0,\N,Missing
H05-1028,J92-4003,0,0.00867414,"2003) and language modeling using visual context (Roy and Mukherjee 2005). Our salience driven approach is inspired by this earlier work. Here, we do not address the acoustic model of speech recognition, but rather incorporate the salience distribution for language modeling. In particular, our focus is on investigating the effect of incorporating additional information from other modalities (e.g., gesture) with traditional language models. Primed Language Model The calculated salience distribution is used to prime the language model. More specifically, we use a class-based bigram model from (Brown et al, 1992): P ( wi |wi −1 ) = P ( wi |c i ) P (c i |ci −1 ) (3) In Equation (3), ci is the class of the word wi, which could be a syntactic class or a semantic class. P (c i |ci −1 ) is the class transition probability, which reflects the grammatical formation of utterances. P( wi |ci ) is the word class probability which measures the probability of seeing a word wi given a class ci. The class-based N-gram model can make better use of limited training data by clustering words into classes. A number of researchers have shown that the class-based N-gram model can successfully improve the performance of sp"
H05-1028,P04-1001,1,0.902548,"practical for real world applications. Previous studies have shown that, in multimodal conversation, multiple modalities tend to complement each other (Cassell et al. 1994). Fusing two or more modalities can be an effective means of reducing recognition uncertainties, for example, through mutual disambiguation (Oviatt 1999). For semantically-rich modalities such as speech and penbased gesture, mutual disambiguation usually happens at the fusion stage where partial semantic representations from individual modalities are disambiguated and combined into an overall interpretation (Johnston 1998, Chai et al., 2004a). One problem is that some critical but low probability information from individual modalities (e.g., recognized alternatives with low probabilities) may never reach the fusion stage. Therefore, this paper addresses how to use information from one modality (e.g., deictic gesture) to directly influence the semantic processing of another modality (e.g., spoken language understanding) even before the fusion stage. In particular we present a new salience driven approach that uses gesture to influence spoken language understanding. This approach is based on the observation that, during multimodal"
H05-1028,N04-1004,0,0.151547,"Missing"
H05-1028,J95-2003,0,0.0874012,"Work on Salience Modeling We first give a brief overview on the notion of salience and how salience modeling is applied in earlier work on natural language and multimodal language processing. Linguistic salience describes the accessibility of entities in a speaker/hearer’s memory and its implication in language production and interpretation. Many theories on linguistic salience have been developed, including how the salience of entities affects the form of referring expressions as in the Givenness Hierarchy (Gundel et al., 1993) and the local coherence of discourse as in the Centering Theory (Grosz et al., 1995). Salience modeling is used for both language generation and language interpretation; the latter is more relevant to our work. Most salience-based interpretation has focused on reference resolution for both linguistic referring expressions (e.g., pronouns) (Lappin and Leass 1995) and multimodal expressions (Hul et al. 1995; Eisenstein and Christoudias 2004). Visual salience considers an object salient when it attracts a user’s visual attention more than others. The cause of such attention depends on many factors including user intention, familiarity, and physical characteristics of objects. Fo"
H05-1028,A00-2014,0,0.0121635,"ability of observing the acoustic features given hypothesized word sequences and the language model provides the probability of a sequence of words. The language model is computed as follows: P ( w1n ) = P( w1 ) P ( w2 |w1 ) P ( w3 |w1 w2 )...P( wn |w1n −1 ) Using the Markov assumption, the language model can be approximated by a bigram model as in: P(w1n ) = n ∏P(w |w i i −1 ) i =1 To improve the speech understanding results for spoken language interfaces, many systems have 221 applied a loosely-integrated approach which decouples the language model from the acoustic model (Zue et al., 1991, Harper et al., 2000). This allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (Gildea and Hofmann 1999), syntactic or semantic labels (Heeman 1999), and linguistic structures (Chelba and Jelinek 2000, Wang and Harper 2002). Recently, we have seen work on language understanding based on environment (Schuler 2003) and language modeling using visual context (Roy and Mukherjee 2005). Our salience driven approach is inspired by this earlier work. Here, we do not address the acoustic model of speech recognition, but rather incorporate th"
H05-1028,W99-0617,0,0.0344865,"Missing"
H05-1028,J95-1003,0,0.761565,"Missing"
H05-1028,P98-1102,0,0.0373653,"e effective and practical for real world applications. Previous studies have shown that, in multimodal conversation, multiple modalities tend to complement each other (Cassell et al. 1994). Fusing two or more modalities can be an effective means of reducing recognition uncertainties, for example, through mutual disambiguation (Oviatt 1999). For semantically-rich modalities such as speech and penbased gesture, mutual disambiguation usually happens at the fusion stage where partial semantic representations from individual modalities are disambiguated and combined into an overall interpretation (Johnston 1998, Chai et al., 2004a). One problem is that some critical but low probability information from individual modalities (e.g., recognized alternatives with low probabilities) may never reach the fusion stage. Therefore, this paper addresses how to use information from one modality (e.g., deictic gesture) to directly influence the semantic processing of another modality (e.g., spoken language understanding) even before the fusion stage. In particular we present a new salience driven approach that uses gesture to influence spoken language understanding. This approach is based on the observation that"
H05-1028,J94-4002,0,0.0814528,"Missing"
H05-1028,P03-1067,0,0.0230672,"=1 To improve the speech understanding results for spoken language interfaces, many systems have 221 applied a loosely-integrated approach which decouples the language model from the acoustic model (Zue et al., 1991, Harper et al., 2000). This allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (Gildea and Hofmann 1999), syntactic or semantic labels (Heeman 1999), and linguistic structures (Chelba and Jelinek 2000, Wang and Harper 2002). Recently, we have seen work on language understanding based on environment (Schuler 2003) and language modeling using visual context (Roy and Mukherjee 2005). Our salience driven approach is inspired by this earlier work. Here, we do not address the acoustic model of speech recognition, but rather incorporate the salience distribution for language modeling. In particular, our focus is on investigating the effect of incorporating additional information from other modalities (e.g., gesture) with traditional language models. Primed Language Model The calculated salience distribution is used to prime the language model. More specifically, we use a class-based bigram model from (Brown"
H05-1028,W02-1031,0,0.0325927,"Missing"
H05-1028,C98-1099,0,\N,Missing
H05-1028,P02-1048,0,\N,Missing
I11-1169,E09-1004,0,0.0194459,"form is functionally equivalent to the non-standard form found in the text; none address the potential existence of extra information in the non-standard form. Affective text classification attempts to identify the type or polarity of emotion that is expressed by the text, without the aid of extra linguistic cues such as gesture or prosody. Kao et. al. (2009) survey the field and divide approaches into 3 categories: 1) keyword based approaches (Bracewell, 2008), 2) learning-based approaches (Alm et al., 2005; Yang et al., 2007; Binali et al., 2010), and 3) hybrid approaches (Wu et al., 2006; Agarwal et al., 2009). Although there has been some recognition of the effect that non-standard word forms play in emotion detection (Zhang et al., 2006), the primary feature sources for emotion detection systems has been at the word and sentence level (Quan and Ren, 2010). To our knowledge, no previous work has focused on the role non-standard word form plays in conveying emotional and other pragmatic information in text messages. Figure 1: Example dialogue from our corpus 3 Empirical Analysis 3.1 Data Set In order to access whether non-standard word forms have additional pragmatic information, it is necessary to"
I11-1169,H05-1073,0,0.0204591,"Cook and Stevenson, 2009) have also been investigated. All of these methods assume that the normalized form is functionally equivalent to the non-standard form found in the text; none address the potential existence of extra information in the non-standard form. Affective text classification attempts to identify the type or polarity of emotion that is expressed by the text, without the aid of extra linguistic cues such as gesture or prosody. Kao et. al. (2009) survey the field and divide approaches into 3 categories: 1) keyword based approaches (Bracewell, 2008), 2) learning-based approaches (Alm et al., 2005; Yang et al., 2007; Binali et al., 2010), and 3) hybrid approaches (Wu et al., 2006; Agarwal et al., 2009). Although there has been some recognition of the effect that non-standard word forms play in emotion detection (Zhang et al., 2006), the primary feature sources for emotion detection systems has been at the word and sentence level (Quan and Ren, 2010). To our knowledge, no previous work has focused on the role non-standard word form plays in conveying emotional and other pragmatic information in text messages. Figure 1: Example dialogue from our corpus 3 Empirical Analysis 3.1 Data Set I"
I11-1169,P06-2005,0,0.552315,"tion. Our empirical results show that character level features can provide important cues for such detection. 1 A: They won the game! B: Yesssss Introduction Text message conversations are often filled with non-standard word spellings. While some of these are unintentional misspellings, many of them are purposely produced. One commonly acknowledged reason that text message authors intentionally use non-standard word forms is to reduce the amount of time it takes to type the message, or the amount of space the message occupies. This phenomenon has motivated the text message normalization task (Aw et al., 2006), which attempts to replace non-standard spelling and symbols by their standard forms. The normalization task is potentially critical for applications involving text messages, such as text-to-speech synthesis. The intent of the utterance by person B seems clear: he wishes to show that he is happy about the event described by person A. If the non-standard form Yesssss was normalized to the standard form yes, the intent conveyed by the utterance would be ambiguous; it could suggest that person B is happy about this turn of events, or he is indifferent, or he could simply be acknowledging that he"
I11-1169,P10-1079,0,0.10686,"lds in an attempt to handle word normalization simultaneously with several related problems such as detecting sentence and paragraph boundaries. Several different approaches have been proposed for normalization of text messages specifically, including those motivated by machine translation (Aw et al., 2006) and spell-checking (Choudhury et al., 2007). Most recently, Pennell and Liu (2010) use handcrafted rules as classification features to normalize SMS terms that contain character deletion, with a focus on normalization for text-to-speech systems. A few hybrid approaches (Kobus et al., 2008; Beaufort et al., 2010) and an unsupervised approach (Cook and Stevenson, 2009) have also been investigated. All of these methods assume that the normalized form is functionally equivalent to the non-standard form found in the text; none address the potential existence of extra information in the non-standard form. Affective text classification attempts to identify the type or polarity of emotion that is expressed by the text, without the aid of extra linguistic cues such as gesture or prosody. Kao et. al. (2009) survey the field and divide approaches into 3 categories: 1) keyword based approaches (Bracewell, 2008),"
I11-1169,W09-2010,0,0.0930633,"aneously with several related problems such as detecting sentence and paragraph boundaries. Several different approaches have been proposed for normalization of text messages specifically, including those motivated by machine translation (Aw et al., 2006) and spell-checking (Choudhury et al., 2007). Most recently, Pennell and Liu (2010) use handcrafted rules as classification features to normalize SMS terms that contain character deletion, with a focus on normalization for text-to-speech systems. A few hybrid approaches (Kobus et al., 2008; Beaufort et al., 2010) and an unsupervised approach (Cook and Stevenson, 2009) have also been investigated. All of these methods assume that the normalized form is functionally equivalent to the non-standard form found in the text; none address the potential existence of extra information in the non-standard form. Affective text classification attempts to identify the type or polarity of emotion that is expressed by the text, without the aid of extra linguistic cues such as gesture or prosody. Kao et. al. (2009) survey the field and divide approaches into 3 categories: 1) keyword based approaches (Bracewell, 2008), 2) learning-based approaches (Alm et al., 2005; Yang et"
I11-1169,C08-1056,0,0.294807,"Missing"
I11-1169,C10-1104,0,0.0210505,"pressed by the text, without the aid of extra linguistic cues such as gesture or prosody. Kao et. al. (2009) survey the field and divide approaches into 3 categories: 1) keyword based approaches (Bracewell, 2008), 2) learning-based approaches (Alm et al., 2005; Yang et al., 2007; Binali et al., 2010), and 3) hybrid approaches (Wu et al., 2006; Agarwal et al., 2009). Although there has been some recognition of the effect that non-standard word forms play in emotion detection (Zhang et al., 2006), the primary feature sources for emotion detection systems has been at the word and sentence level (Quan and Ren, 2010). To our knowledge, no previous work has focused on the role non-standard word form plays in conveying emotional and other pragmatic information in text messages. Figure 1: Example dialogue from our corpus 3 Empirical Analysis 3.1 Data Set In order to access whether non-standard word forms have additional pragmatic information, it is necessary to study these forms in their original dialogue context. Because no currently available text message dataset contains messages in context, we collected our own. The website “Damn You Autocorrect”1 posts screenshots of short text message conversations tha"
I11-1169,E06-2030,0,0.0294666,"in the non-standard form. Affective text classification attempts to identify the type or polarity of emotion that is expressed by the text, without the aid of extra linguistic cues such as gesture or prosody. Kao et. al. (2009) survey the field and divide approaches into 3 categories: 1) keyword based approaches (Bracewell, 2008), 2) learning-based approaches (Alm et al., 2005; Yang et al., 2007; Binali et al., 2010), and 3) hybrid approaches (Wu et al., 2006; Agarwal et al., 2009). Although there has been some recognition of the effect that non-standard word forms play in emotion detection (Zhang et al., 2006), the primary feature sources for emotion detection systems has been at the word and sentence level (Quan and Ren, 2010). To our knowledge, no previous work has focused on the role non-standard word form plays in conveying emotional and other pragmatic information in text messages. Figure 1: Example dialogue from our corpus 3 Empirical Analysis 3.1 Data Set In order to access whether non-standard word forms have additional pragmatic information, it is necessary to study these forms in their original dialogue context. Because no currently available text message dataset contains messages in cont"
I11-1169,P07-1087,0,0.0217824,"l Language Processing, pages 1437–1441, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work There are two main areas of related work: text normalization and affective text classification. Because it may be unclear how non-standard forms should be read aloud, the field of text-to-speech synthesis has long been interested in text normalization. Sproat et. al. (2001) study several different corpora and identify several types of nonstandard word, including several seen frequently in text message data, such as misspelling, abbreviation, and “funny spellings”. More recent work (Zhu et al., 2007) has employed conditional random fields in an attempt to handle word normalization simultaneously with several related problems such as detecting sentence and paragraph boundaries. Several different approaches have been proposed for normalization of text messages specifically, including those motivated by machine translation (Aw et al., 2006) and spell-checking (Choudhury et al., 2007). Most recently, Pennell and Liu (2010) use handcrafted rules as classification features to normalize SMS terms that contain character deletion, with a focus on normalization for text-to-speech systems. A few hyb"
J12-4003,P98-1013,0,0.648096,"Missing"
J12-4003,D07-1017,0,0.0604353,"Missing"
J12-4003,W11-0106,0,0.0250051,"Missing"
J12-4003,W05-0620,0,0.345211,"Missing"
J12-4003,P08-1090,0,0.0945925,"Missing"
J12-4003,P05-1022,0,0.0613223,"Missing"
J12-4003,S10-1059,0,0.443709,"nts for Nominal Predicates Most recently, Ruppenhofer et al. (2009) proposed SemEval Task 10, “Linking Events and Their Participants in Discourse,” which evaluated implicit argument identiﬁcation systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. As described by Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identiﬁcation part of the task. Chen et al. (2010) extended a standard SRL system by widening the candidate window to include constituents from other sentences. A small number of features based on the FrameNet frame deﬁnitions were extracted for these candidates, and prediction was performed using a log-linear model. Tonelli and Delmonte (2010) also extended a standard SRL system. Both of these systems achieved an implicit argument F1 score of less than 0.02. The organizers and participants appear to agree that training data sparseness was a signiﬁcant problem. This is likely the result of the annotation methodology: Entire documents were ann"
J12-4003,W09-3208,0,0.020724,"anufacturing carpet since 1967. Non-trivial instances of coreference (e.g., Carpet King and the company) allow the author to repeatedly mention the same entity without introducing redundancy into the discourse. Pronominal anaphora is a subset of coreference in which one of the referring expressions is a pronoun. For example, he in Example (11) refers to the same entity as Richard Rippe in Example (10). These examples demonstrate noun phrase coreference. Events, indicated by either verbal or nominal predicates, can also be coreferential when mentioned multiple times in a document (Wilson 1974; Chen and Ji 2009). For many years, the Automatic Content Extraction (ACE) series of large-scale evaluations (NIST 2008) has provided a test environment for systems designed to identify these and other coreference relations. Systems based on the ACE data sets typically take a supervised learning approach to coreference resolution in general (Versley et al. 2008) and pronominal anaphor in particular (Yang, Su, and Tan 2008). A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expressio"
J12-4003,J93-3001,0,0.405169,"Missing"
J12-4003,1993.eamt-1.1,0,0.221631,"nds, other investment funds), Dice(investment funds, a stock . . . money–market fund)} = max{0.8, 0} = 0.8 Precision, recall, and F1 for the example predicate are calculated as follows: Precision = 1.8 = 0.9 2 Recall = 1.8 = 0.6 3 F1 = 2 ∗ Precision ∗ Recall = 0.72 Precision + Recall We calculated the F1 score for the entire testing fold by aggregating the counts used in the above precision and recall calculations. Similarly, we aggregated the counts across all folds to arrive at a single F1 score for the evaluated system. We used a bootstrap resampling technique similar to those developed by Efron and Tibshirani (1993) to test the signiﬁcance of the performance difference between various systems. Given a test pool comprising M missing argument positions iargn along with 11 Our evaluation methodology differs slightly from that of Ruppenhofer et al. (2010) in that we use the Dice metric to compute precision and recall, whereas Ruppenhofer et al. reported the Dice metric separately from exact-match precision and recall. 781 Computational Linguistics Volume 38, Number 4 the predictions by systems A and B for each iargn , we calculated the exact p-value of the performance difference as follows: 1. Create r rando"
J12-4003,P10-1160,1,0.828842,"Missing"
J12-4003,W11-0909,1,0.860831,"Missing"
J12-4003,N09-1017,1,0.918677,"Missing"
J12-4003,J95-2003,0,0.365226,"Missing"
J12-4003,W07-1522,0,0.0133115,"ment for systems designed to identify these and other coreference relations. Systems based on the ACE data sets typically take a supervised learning approach to coreference resolution in general (Versley et al. 2008) and pronominal anaphor in particular (Yang, Su, and Tan 2008). A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting 760 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates in systems that are able to identify missing case–marked expressions in the surrounding discourse (Imamura, Saito, and Izumi 2009). Sasano, Kawahara, and Kurohashi (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is"
J12-4003,P09-2022,0,0.0594636,"Missing"
J12-4003,W08-2123,0,0.0566639,"Missing"
J12-4003,P07-1027,0,0.0365079,"Missing"
J12-4003,J93-2004,0,0.0468236,"Missing"
J12-4003,C04-1157,0,0.0315313,"series of recent investigations that have led to a surge of interest in statistical implicit argument identiﬁcation. Fillmore and Baker (2001) provided a detailed case study of FrameNet frames as a basis for understanding written text. In their case study, Fillmore and Baker manually build up a semantic discourse structure by hooking together frames from the various sentences. In doing so, the authors resolve some implicit arguments found in the discourse. This process is an interesting step forward; the authors did not provide concrete methods to perform the analysis automatically, however. Nielsen (2004) developed a system that is able to detect the occurrence of verb phrase ellipsis. Consider the following sentences: (16) John kicked the ball. (17) Bill [did], too. The bracketed text in Example (17) is a placeholder for the verb phrase kicked the ball in Example (16), which has been elided (i.e., left out). Thus, in Example (17), Bill can be thought of as an implicit argument to some kicking event that is not mentioned. If one resolved the verb phrase ellipsis, then the implicit agent (Bill) would be recovered.4 Nielsen (2004) created a system able to detect the presence of ellipses, produci"
J12-4003,P86-1004,0,0.909099,"ntering Theory (Grosz, Joshi, and Weinstein 1995) focuses on the ways in which referring expressions maintain (or break) coherence in a discourse. These so-called “centering shifts” result from a lack of coreference between salient noun phrases in adjacent sentences. Discourse Representation Theory (DRT) (Kamp and Reyle 1993) is another prominent treatment of referring expressions. DRT embeds a theory of coreference into a ﬁrst-order, compositional semantics of discourse. 2.4 Identifying Implicit Arguments Past research on the actual task of implicit argument identiﬁcation tends to be sparse. Palmer et al. (1986) describe what appears to be the ﬁrst computational treatment of implicit arguments. In that work, Palmer et al. manually created a repository of knowledge concerning entities in the domain of electronic device failures. This knowledge, along with hand-coded syntactic and semantic processing rules, allowed the system to identify implicit arguments across sentence boundaries. As a simple example, consider the following two sentences (borrowed from Palmer et al. [1986]): (12) Disk drive was down at 11/16-2305. (13) Has select lock. Example (13) does not specify precisely which entity has select"
J12-4003,N07-1071,0,0.0297095,"s − → X plays are often entailed by constructions involving agentive, nominalized verbs as the logical subjects of the main verb. In Example (9), the agentive nominal 759 Computational Linguistics Volume 38, Number 4 “player” is logical subject to “won”, the combination of which entails the asymmetric relationship of interest. Thus, to validate such an asymmetric relationship, Zanzotto, Pennacchiotti, and Pazienza (2006) examined the frequency of the “player win” collocation using Google hit counts as a proxy for actual corpus statistics. A number of other studies (e.g., Szpektor et al. 2004, Pantel et al. 2007) have been conducted that are similar to that work. In general, such work focuses on the automatic acquisition of entailment relationships between verbs. Although this work has often been motivated by the need for lexical–semantic information in tasks such as automatic question answering, it is also relevant to the task of implicit argument identiﬁcation because the derived relationships implicitly encode a participant role mapping between two predicates. For example, given a missing arg0 for a like predicate and an explicit arg0 = John for an eat predicate in the preceding discourse, inferenc"
J12-4003,N04-1041,0,0.0133945,"vant to the predicates considered in Equation (2). Assuming the resulting data have N coreferential pairs of arguments, the numerator in Equation (2) is deﬁned as follows: Pcoref   #coref (p1 , argi  , p2 , argj ) pmi (p1 , argi  , p2 , argj ) = N   (3) In Equation (3), #coref returns the number of times the given argument positions are found to be coreferential. In order to penalize low-frequency observations with 9 http://lucene.apache.org. 774 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates artiﬁcially high scores, we used the simple discounting method described by Pantel and Ravichandran (2004) resulting in the following modiﬁcation of Equation (3): Pcoref   x = #coref (p1 , argi  , p2 , argj )   x ∗ x pmi (p1 , argi  , p2 , argj ) = N x+1 (4) x will Thus, if two argument positions are rarely observed as coreferent, the value x+ 1 be small, reducing the PMI score. The denominator in Equation (2) is computed with a similar discount factor: Pcoref x1 = #coref (p1 , argi  , ∗ )   x2 = #coref ( p2 , argj , ∗ )   pmi (p1 , argi  , ∗ )Pcoref pmi ( p2 , argj , ∗ ) = x1 x2 min(x1 ,x2 ) 2 (N ) min(x 1 ,x2 )+1 (5) Thus, if either of the argument positions is rarely observed as"
J12-4003,W08-1810,0,0.0261757,"Missing"
J12-4003,W05-0625,0,0.08951,"Missing"
J12-4003,J08-2005,0,0.0875283,"Missing"
J12-4003,P10-1044,0,0.00970724,"Missing"
J12-4003,W09-2417,0,0.277596,"s. In our previous work, we demonstrated the importance of ﬁltering out nominal predicates that take no local arguments (Gerber, Chai, and Meyers 2009). This approach leads to appreciable gains for certain nominals. The approach does not attempt to actually recover implicit arguments, however. 4 Identiﬁcation of the implicit patient in Example (17) (the ball) should be sensitive to the phenomenon of sense anaphora. If Example (16) was changed to “a ball,” then we would have no implicit patient in Example (17). 762 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates Most recently, Ruppenhofer et al. (2009) proposed SemEval Task 10, “Linking Events and Their Participants in Discourse,” which evaluated implicit argument identiﬁcation systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. As described by Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identiﬁcation part of the task. Chen et al. (2010) extended a standard SRL system by widening the c"
J12-4003,S10-1008,0,0.315833,"henomenon of sense anaphora. If Example (16) was changed to “a ball,” then we would have no implicit patient in Example (17). 762 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates Most recently, Ruppenhofer et al. (2009) proposed SemEval Task 10, “Linking Events and Their Participants in Discourse,” which evaluated implicit argument identiﬁcation systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. As described by Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identiﬁcation part of the task. Chen et al. (2010) extended a standard SRL system by widening the candidate window to include constituents from other sentences. A small number of features based on the FrameNet frame deﬁnitions were extracted for these candidates, and prediction was performed using a log-linear model. Tonelli and Delmonte (2010) also extended a standard SRL system. Both of these systems achieved an implicit argument F1 score of less than 0.02. The organizers and partic"
J12-4003,W09-3813,0,0.0438373,"Missing"
J12-4003,C04-1174,0,0.0606005,"Missing"
J12-4003,W04-3206,0,0.0104255,"ionships such as X wins − → X plays are often entailed by constructions involving agentive, nominalized verbs as the logical subjects of the main verb. In Example (9), the agentive nominal 759 Computational Linguistics Volume 38, Number 4 “player” is logical subject to “won”, the combination of which entails the asymmetric relationship of interest. Thus, to validate such an asymmetric relationship, Zanzotto, Pennacchiotti, and Pazienza (2006) examined the frequency of the “player win” collocation using Google hit counts as a proxy for actual corpus statistics. A number of other studies (e.g., Szpektor et al. 2004, Pantel et al. 2007) have been conducted that are similar to that work. In general, such work focuses on the automatic acquisition of entailment relationships between verbs. Although this work has often been motivated by the need for lexical–semantic information in tasks such as automatic question answering, it is also relevant to the task of implicit argument identiﬁcation because the derived relationships implicitly encode a participant role mapping between two predicates. For example, given a missing arg0 for a like predicate and an explicit arg0 = John for an eat predicate in the precedin"
J12-4003,S10-1065,0,0.372318,"ire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. As described by Ruppenhofer et al. (2010), three submissions were made to the competition, with two of the submissions attempting the implicit argument identiﬁcation part of the task. Chen et al. (2010) extended a standard SRL system by widening the candidate window to include constituents from other sentences. A small number of features based on the FrameNet frame deﬁnitions were extracted for these candidates, and prediction was performed using a log-linear model. Tonelli and Delmonte (2010) also extended a standard SRL system. Both of these systems achieved an implicit argument F1 score of less than 0.02. The organizers and participants appear to agree that training data sparseness was a signiﬁcant problem. This is likely the result of the annotation methodology: Entire documents were annotated, causing each predicate to receive a very small number of annotated examples. In contrast to the evaluation described by Ruppenhofer et al. (2010), the study presented in this article focused on a select group of nominal predicates. To help prevent data sparseness, the size of the group w"
J12-4003,P08-4003,0,0.0196431,"1) refers to the same entity as Richard Rippe in Example (10). These examples demonstrate noun phrase coreference. Events, indicated by either verbal or nominal predicates, can also be coreferential when mentioned multiple times in a document (Wilson 1974; Chen and Ji 2009). For many years, the Automatic Content Extraction (ACE) series of large-scale evaluations (NIST 2008) has provided a test environment for systems designed to identify these and other coreference relations. Systems based on the ACE data sets typically take a supervised learning approach to coreference resolution in general (Versley et al. 2008) and pronominal anaphor in particular (Yang, Su, and Tan 2008). A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting 760 Gerber and Chai SRL of Implicit Arguments for Nominal Predicates in systems that are able to identify missing cas"
J12-4003,P91-1003,0,0.90892,"Missing"
J12-4003,P94-1019,0,0.0817778,"Missing"
J12-4003,J08-3002,0,0.0606366,"Missing"
J12-4003,P06-1107,0,0.0605099,"Missing"
J12-4003,N07-1069,0,\N,Missing
J12-4003,D10-1104,0,\N,Missing
J12-4003,copestake-flickinger-2000-open,0,\N,Missing
J12-4003,W98-0604,0,\N,Missing
J12-4003,N10-1137,0,\N,Missing
J12-4003,W08-2121,0,\N,Missing
J12-4003,W07-1206,0,\N,Missing
J12-4003,J08-2003,0,\N,Missing
J12-4003,H86-1011,0,\N,Missing
J12-4003,S07-1014,0,\N,Missing
J12-4003,W06-1617,0,\N,Missing
J12-4003,W05-1513,0,\N,Missing
J12-4003,W04-3212,0,\N,Missing
J12-4003,N07-1070,0,\N,Missing
J12-4003,D09-1002,0,\N,Missing
J12-4003,C08-1084,0,\N,Missing
J12-4003,J96-1002,0,\N,Missing
J12-4003,N06-1010,0,\N,Missing
J12-4003,P87-1019,0,\N,Missing
J12-4003,P07-1025,0,\N,Missing
J12-4003,C98-1013,0,\N,Missing
J12-4003,D07-1038,0,\N,Missing
J12-4003,P08-1004,0,\N,Missing
J12-4003,P05-1073,0,\N,Missing
J12-4003,P09-1004,0,\N,Missing
J12-4003,W09-1201,0,\N,Missing
J12-4003,N09-2061,0,\N,Missing
J12-4003,J02-3001,0,\N,Missing
J12-4003,S07-1003,0,\N,Missing
J12-4003,P02-1031,0,\N,Missing
J12-4003,D09-1003,0,\N,Missing
J12-4003,J08-2002,0,\N,Missing
J12-4003,S10-1006,0,\N,Missing
J12-4003,D10-1085,0,\N,Missing
J12-4003,W04-2412,0,\N,Missing
J12-4003,W03-3017,0,\N,Missing
N04-4011,J95-1003,0,0.0150042,"gesture, and gaze (Bolt 1980; Cassell et al., 1999; Cohen et al., 1996; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is for the system to understand the meanings of multimodal user inputs. A key element of this understanding process is reference resolution. Reference resolution is a process that finds the most proper referents to referring expressions. To resolve multimodal references, many approaches have been developed, from the use of a focus space model (Neal et al., 1998), a centering framework (Zancanaro et al, 1997), contextual factors (Huls et al., 1995); to recent approaches using unification (Johnston, 1998), finite state machines (Johnston and Bangalore 2000), and contextbased rules (Kehler 2000). Given the substantial work in this area; it is important to evaluate the state of the art, understand the limitations, * This work was supported by grant IIS-0347548 from the National Science Foundation and grant IRGP-03-42111 from Michigan State University. and identify directions for future improvement. We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system. In particular, th"
N04-4011,P98-1102,0,0.58708,"al., 1996; Chai et al., 2002; Johnston et al., 2002). One important aspect of building multimodal systems is for the system to understand the meanings of multimodal user inputs. A key element of this understanding process is reference resolution. Reference resolution is a process that finds the most proper referents to referring expressions. To resolve multimodal references, many approaches have been developed, from the use of a focus space model (Neal et al., 1998), a centering framework (Zancanaro et al, 1997), contextual factors (Huls et al., 1995); to recent approaches using unification (Johnston, 1998), finite state machines (Johnston and Bangalore 2000), and contextbased rules (Kehler 2000). Given the substantial work in this area; it is important to evaluate the state of the art, understand the limitations, * This work was supported by grant IIS-0347548 from the National Science Foundation and grant IRGP-03-42111 from Michigan State University. and identify directions for future improvement. We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system. In particular, this paper examines two important aspects: (1) algorithm re"
N04-4011,C00-1054,0,0.57449,"et al., 2002). One important aspect of building multimodal systems is for the system to understand the meanings of multimodal user inputs. A key element of this understanding process is reference resolution. Reference resolution is a process that finds the most proper referents to referring expressions. To resolve multimodal references, many approaches have been developed, from the use of a focus space model (Neal et al., 1998), a centering framework (Zancanaro et al, 1997), contextual factors (Huls et al., 1995); to recent approaches using unification (Johnston, 1998), finite state machines (Johnston and Bangalore 2000), and contextbased rules (Kehler 2000). Given the substantial work in this area; it is important to evaluate the state of the art, understand the limitations, * This work was supported by grant IIS-0347548 from the National Science Foundation and grant IRGP-03-42111 from Michigan State University. and identify directions for future improvement. We conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system. In particular, this paper examines two important aspects: (1) algorithm requirements for handling a variety of references, and"
N04-4011,W97-1401,0,0.772087,"Missing"
N04-4011,H89-2054,0,\N,Missing
N04-4011,P02-1048,0,\N,Missing
N04-4011,C98-1099,0,\N,Missing
N07-1036,H05-1028,1,0.845623,"hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questions one at a time about the bedroom and the user answered the questions by speaking to the system. Fig.1 shows the 14 questions in the experiments. The user’s speech was recorded through an open microphone and the user’s eye gaze was captured by an Eye Link II eye tracker. From 7 users’ experiments, we collected 554 utterances with a vocabulary of 489 words. Each utterance was"
N07-1036,N04-1004,0,0.0259685,"hat eye gaze has a potential to improve reference resolution in a spoken dialog system (Campana et al., 2001). Furthermore, eye gaze also plays an important role in managing dialog in conversational systems (Qvarfordt and Zhai, 2005). Salience modeling has been used in both natural language and multimodal language processing. Linguistic salience describes entities with their accessibility in a hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questi"
N07-1036,J95-1003,0,0.057841,"rk has also shown that eye gaze has a potential to improve reference resolution in a spoken dialog system (Campana et al., 2001). Furthermore, eye gaze also plays an important role in managing dialog in conversational systems (Qvarfordt and Zhai, 2005). Salience modeling has been used in both natural language and multimodal language processing. Linguistic salience describes entities with their accessibility in a hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system ve"
N09-1017,P98-1013,0,0.034894,"Missing"
N09-1017,W05-0620,0,0.0474654,"Missing"
N09-1017,P05-1022,0,0.0642812,"Missing"
N09-1017,J02-3001,0,0.0901764,"licable to nominal argument structure. Early work in identifying the argument structure of deverbal nominalizations was primarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Sy"
N09-1017,S07-1003,0,0.0937681,"Missing"
N09-1017,P07-1025,0,0.0647072,"ifier with additional new features. Second, we show that this model suffers a substantial performance degradation when evaluated over nominals with implicit arguments. Finally, we identify a set of features - many of them new - that can be used to reliably detect nominals with explicit arguments, thus significantly increasing the performance of the nominal SRL system. Our results also suggest interesting directions for future work. As described in section 5.2, many nominals do not have enough labeled training data to produce accurate argument models. The generalization procedures developed by Gordon and Swanson (2007) for PropBank SRL and Pad´o et al. (2008) for NomBank SRL might alleviate this problem. Additionally, instead of ignoring nominals with implicit arguments, we would prefer to identify the implicit arguments using information contained in the surrounding discourse. Such inferences would help connect entities and events across sentences, providing a fuller interpretation of the text. Acknowledgments The authors would like to thank the anonymous reviewers for their helpful suggestions. The first two authors were supported by NSF grants IIS-0535112 and IIS-0347548, and the third author was support"
N09-1017,W06-1617,0,0.444286,"imarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntacti"
N09-1017,P07-1027,0,0.452477,"rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntactic dependencies, verbal"
N09-1017,C08-1084,0,0.111469,"Missing"
N09-1017,J05-1004,0,0.0548633,"Missing"
N09-1017,W08-2121,1,\N,Missing
N09-1017,N07-1070,0,\N,Missing
N09-1017,C98-1013,0,\N,Missing
N12-1089,P06-2005,0,0.0391014,"Missing"
N12-1089,P00-1037,0,0.0808944,"Missing"
N12-1089,N01-1027,0,0.037338,"dies have looked at the effects SMS writing style has on predictive text performance. How and Kan (2005) analyze a corpus of 10,000 text messages and conclude that changing the standard mapping of letters to keys on 12 key keyboards could improve input performance on SMS data. Although never examined in the context of autocorrection systems, system self-assessment has been studied in other domains. One of the most com1 http://aspell.net/ Figure 1: Example text message dialogue from our corpus with an automatic correction mistake mon application domains is spoken dialogue systems (Levow, 1998; Hirschberg et al., 2001; Litman et al., 2006), where detecting problematic situations can help the system better adapt to user behavior. These systems often make use of prosody and task specific dialogue acts, two feature sources unavailable in general text message dialogues. In summary, while a large body of work addresses similar problems, to our knowledge no previous work has looked into the aspect of self-assessment of autocorrection based on dialogues between text message users. The work presented in this paper represents a first step in this direction. 3 Data Set To support our investigation, we collected a co"
N12-1089,C90-2036,0,0.0604844,"Missing"
N12-1089,C08-1056,0,0.0307613,"Missing"
N12-1089,P98-1122,0,0.0851708,"007). Few studies have looked at the effects SMS writing style has on predictive text performance. How and Kan (2005) analyze a corpus of 10,000 text messages and conclude that changing the standard mapping of letters to keys on 12 key keyboards could improve input performance on SMS data. Although never examined in the context of autocorrection systems, system self-assessment has been studied in other domains. One of the most com1 http://aspell.net/ Figure 1: Example text message dialogue from our corpus with an automatic correction mistake mon application domains is spoken dialogue systems (Levow, 1998; Hirschberg et al., 2001; Litman et al., 2006), where detecting problematic situations can help the system better adapt to user behavior. These systems often make use of prosody and task specific dialogue acts, two feature sources unavailable in general text message dialogues. In summary, while a large body of work addresses similar problems, to our knowledge no previous work has looked into the aspect of self-assessment of autocorrection based on dialogues between text message users. The work presented in this paper represents a first step in this direction. 3 Data Set To support our investi"
N12-1089,J06-3004,0,0.0177916,"ffects SMS writing style has on predictive text performance. How and Kan (2005) analyze a corpus of 10,000 text messages and conclude that changing the standard mapping of letters to keys on 12 key keyboards could improve input performance on SMS data. Although never examined in the context of autocorrection systems, system self-assessment has been studied in other domains. One of the most com1 http://aspell.net/ Figure 1: Example text message dialogue from our corpus with an automatic correction mistake mon application domains is spoken dialogue systems (Levow, 1998; Hirschberg et al., 2001; Litman et al., 2006), where detecting problematic situations can help the system better adapt to user behavior. These systems often make use of prosody and task specific dialogue acts, two feature sources unavailable in general text message dialogues. In summary, while a large body of work addresses similar problems, to our knowledge no previous work has looked into the aspect of self-assessment of autocorrection based on dialogues between text message users. The work presented in this paper represents a first step in this direction. 3 Data Set To support our investigation, we collected a corpus of data containin"
N12-1089,P02-1019,0,0.0665634,"Missing"
N12-1089,D09-1093,0,0.0272577,"Missing"
N12-1089,C98-1117,0,\N,Missing
N16-1019,Q13-1005,0,0.0599451,"15), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different approaches and emphases have been explored. For example, linear programming has been applied to mediate perceptual differences between humans and robots for referential grounding (Liu and Chai, 2015). Approaches to semantic parsing have been applied to ground language to internal world representations (Chen and Mooney, 2008; Artzi and Zettlemoyer, 2013). Logical Semantics with Perception (LSP) (Krishnamurthy and Kollar, 2013) was applied to ground natural language queries to visual referents through jointly parsing natural language (combinatory categorical grammar (CCG)) and visual"
N16-1019,P15-1006,0,0.0175925,"from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different ap"
N16-1019,P15-2017,0,0.0193807,"to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 201"
N16-1019,P15-1005,0,0.0339368,"Missing"
N16-1019,W13-3820,0,0.0195411,"entify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to represent verb semantics that are grounded to the physical world so that artificial agents can truly understand the ongoing activities and (learn to) perform the specified actions. To address this issue, we propose a new task on grounded semantic role labeling. Figure 1 shows an example of grounded SRL. 149 Proceedings of NAACL-HLT 2016, pages 149–159, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics The sentence the woman takes out a cucumber from the refri"
N16-1019,J02-3001,0,0.00957276,"implicit role (destination) is grounded to a track id. Introduction Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments) (Levin, 1993). For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc. Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to repre"
N16-1019,D14-1086,0,0.0532011,"rbs and in a much more complex domain where object recognition and tracking are notably more difficult. Third, our work results in additional layers of annotation to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to"
N16-1019,Q13-1016,0,0.0770443,"description) is grounded to the cutting board (track 5). To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information. In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain (Regneri et al., 2013) in our investigation. We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms. Compared to previous works on language grounding (Tellex et al., 2011; Yu and Siskind, 2013; Krishnamurthy and Kollar, 2013), our work presents several contributions. First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world. By incorporating semantic role information, our approach has led to better grounding performance. Second, most previous works only focused on a small number of verbs with limited activities. We base our investigation on a wider range of verbs and in a much more complex domain where object recognition and tracking are notably m"
N16-1019,P13-2138,0,0.00947237,"tracking are notably more difficult. Third, our work results in additional layers of annotation to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al"
N16-1019,N15-1016,0,0.0104544,"re work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different approaches and emphases have been explored. For example, linear programming has been applied to mediate per"
N16-1019,W12-1621,1,0.857349,"al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Artzi and Zettlemoyer, 2013). Different approaches and emphases have been explored. For example, linear programming has been applied to mediate perceptual differences between humans and robots for referential grounding (Liu and Chai, 2015). Approaches to semantic parsing have been applied to ground language to internal world representations (Chen and Mooney, 2008; Artzi and Zettlemoyer, 2013). Logical Semantics with Perception (LSP) (Krishnamurth"
N16-1019,N15-1015,0,0.0235219,"re of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (Tellex et al., 2014; Art"
N16-1019,P14-5010,0,0.00278094,"rred from the video. For the verb cut, the location and the tool are also rarely specified by linguistic expressions. Nevertheless, these implicit roles contribute to the overall understanding of actions and should also be grounded too. 4.2 Automated Processing To build the structure of the CRF as shown in Figure 2 and extract features for learning and inference, we have applied the following approaches to process language and vision. Language Processing. Language processing consists of three steps to build a structure containing syntactic and semantic information. First, the Stanford Parser (Manning et al., 2014) is applied to create a dependency parsing tree for each sentence. Second, Senna (Collobert et al., 2011) is applied to identify semantic role labels for the key verb in the sentence. The linguistic entities with semantic roles are matched against the dependency nodes in the tree and the corresponding semantic role labels are added to the tree. Third, for each verb, the Propbank (Palmer et al., 2005) entries are searched to extract all relevant semantic roles. The implicit roles (i.e., not specified linguistically) are added as direct children of verb nodes in the tree. Through these three ste"
N16-1019,N15-1017,0,0.0764911,"aptures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 2015), to physical landmarks (Tellex et al., 2011; Tellex et al., 2014), and to perceived actions or activities (T"
N16-1019,N15-1174,0,0.0339839,"Missing"
N16-1019,J05-1004,0,0.190181,"ch role including the implicit role (destination) is grounded to a track id. Introduction Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments) (Levin, 1993). For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc. Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional S"
N16-1019,Q13-1003,0,0.223327,"ctually does the take-out action in the visual scene (track 1) ; the patient is grounded to the cucumber taken out (track 3); and the source is grounded to the refrigerator (track 4). The implicit role of destination (which is not explicitly mentioned in the language description) is grounded to the cutting board (track 5). To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information. In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain (Regneri et al., 2013) in our investigation. We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms. Compared to previous works on language grounding (Tellex et al., 2011; Yu and Siskind, 2013; Krishnamurthy and Kollar, 2013), our work presents several contributions. First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world. By incorporating semantic role information, our app"
N16-1019,D07-1002,0,0.0416949,"erve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to represent verb semantics that are grounded to the physical world so that artificial agents can truly understand the ongoing activities and (learn to) perform the specified actions. To address this issue, we propose a new task on grounded semantic role labeling. Figure 1 shows an example of grounded SRL. 149 Proceedings of NAACL-HLT 2016, pages 149–159, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics The sentence the woman takes out a cucumber from the refrigerator describes an activity in a visual scene"
N16-1019,N15-1173,0,0.0370204,"difficult. Third, our work results in additional layers of annotation to part of the TACoS dataset. This annotation captures the structure of actions informed by semantic roles from the video. The annotated data is available for download 1 . It will provide a benchmark for future work on grounded SRL. 1 http://lair.cse.msu.edu/gsrl.html 150 2 Related Work Recent years have witnessed an increasing amount of work in integrating language and vision, from earlier image annotation (Ramanathan et al., 2013; Kazemzadeh et al., 2014) to recent image/video caption generation (Kuznetsova et al., 2013; Venugopalan et al., 2015; Ortiz et al., ; Elliott and de Vries, 2015; Devlin et al., 2015), video sentence alignment (Naim et al., 2015; Malmaud et al., 2015), scene generation (Chang et al., 2015), and multimodel embedding incorporating language and vision (Bruni et al., 2014; Lazaridou et al., 2015). What is more relevant to our work here is recent progress on grounded language understanding, which involves learning meanings of words through connections to machine perception (Roy, 2005) and grounding language expressions to the shared visual world, for example, to visual objects (Liu et al., 2012; Liu and Chai, 201"
N16-1019,P13-1006,0,0.123839,"tioned in the language description) is grounded to the cutting board (track 5). To tackle this problem, we have developed an approach to jointly process language and vision by incorporating semantic role information. In particular, we use a benchmark dataset (TACoS) which consists of parallel video and language descriptions in a complex cooking domain (Regneri et al., 2013) in our investigation. We have further annotated several layers of information for developing and evaluating grounded semantic role labeling algorithms. Compared to previous works on language grounding (Tellex et al., 2011; Yu and Siskind, 2013; Krishnamurthy and Kollar, 2013), our work presents several contributions. First, beyond arguments explicitly mentioned in language descriptions, our work simultaneously grounds explicit and implicit roles with an attempt to better connect verb semantics with actions from the underlying physical world. By incorporating semantic role information, our approach has led to better grounding performance. Second, most previous works only focused on a small number of verbs with limited activities. We base our investigation on a wider range of verbs and in a much more complex domain where object recog"
N16-1019,P15-1109,0,0.0121407,"id. Introduction Linguistic studies capture semantics of verbs by their frames of thematic roles (also referred to as semantic roles or verb arguments) (Levin, 1993). For example, a verb can be characterized by agent (i.e., the animator of the action) and patient (i.e., the object on which the action is acted upon), and other roles such as instrument, source, destination, etc. Given a verb frame, the goal of Semantic Role Labeling (SRL) is to identify linguistic entities from the text that serve different thematic roles (Palmer et al., 2005; Gildea and Jurafsky, 2002; Collobert et al., 2011; Zhou and Xu, 2015). For example, given the sentence the woman takes out a cucumber from the refrigerator., takes out is the main verb (also called predicate); the noun phrase the woman is the agent of this action; a cucumber is the patient; and the refrigerator is the source. SRL captures important semantic representations for actions associated with verbs, which have shown beneficial for a variety of applications such as information extraction (Emanuele et al., 2013) and question answering (Shen and Lapata, 2007). However, the traditional SRL is not targeted to represent verb semantics that are grounded to the"
P04-1001,N04-4011,1,0.684584,"d those inside the second dashed rectangle correspond to the second pointing gesture. Each node also contains a probability that indicates the likelihood of its corresponding object being selected by the gesture. Furthermore, the salient objects from the prior conversation are also included in the referent graph since they could also be the potential referents (e.g., the rightmost dashed rectangle in Figure 32). To create these graphs, we apply a grammarbased natural language parser to process speech inputs and a gesture recognition component to process gestures. The details are described in (Chai et al. 2004a). Each node from the conversation context is linked to every node corresponding to the first pointing and the second pointing. m n m x x m m x y n m xy (1) mn ) In Equation (1), Q(Gr,Gs) measures the degree of the overall match between the referent graph and the referring graph. P(ax,αm) is the matching probability between a node ax in the referent graph and a node αm in the referring graph. The overall compatibility depends on the similarities between nodes (NodeSim) and the similarities between edges (EdgeSim). The function NodeSim(ax,αm) measures the similarity between a referent node ax"
P04-1001,P97-1040,0,0.0255709,"d grammar, developed by Alan Prince and Paul Smolensky (Prince and Smolensky, 1993). In Optimality Theory, a grammar consists of a set of well-formed constraints. These constraints are applied simultaneously to identify linguistic SemType(rxy, γmn) γmn rxy Same Different Unknown Same 1 0 0.5 Different 0 1 0.5 Unknown 0.5 0.5 0.5 Table 2: Definition of SemType(rxy, γmn) Temp(rxy, γmn) γmn rxy Preceding Concurrent Non-concurrent Unknown Precede 1 0.5 0.7 0.5 Concurrent 0 1 0 0.5 Table 3: Definition of Temp(rxy, γmn) structures. Optimality Theory does not restrict the content of the constraints (Eisner 1997). An innovation of Optimality Theory is the conception of these constraints as soft, which means violable and conflicting. The interpretation that arises for an utterance within a certain context maximizes the degree of constraint satisfaction and is consequently the best alternative (hence, optimal interpretation) among the set of possible interpretations. The key principles or components of Optimality Theory can be summarized as the following three components (Blutner 1998): 1) Given a set of input, Generator creates a set of possible outputs for each input. 2) From the set of candidate outp"
P04-1001,P97-1036,0,0.0533883,"Missing"
P04-1001,P98-1102,0,0.332759,"ints from various aspects (e.g., semantic, temporal, and contextual). A correct interpretation can only be attained by simultaneously considering these constraints. In this process, two issues are important: first, a mechanism to combine information from various sources to form an overall interpretation given a set of constraints; and second, a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints. The first issue focuses on the fusion aspect, which has been well studied in earlier work, for example, through unificationbased approaches (Johnston 1998) or finite state approaches (Johnston and Bangalore, 2000). This paper focuses on the second issue of optimization. As in natural language interpretation, there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing. We have developed a graph-based optimization approach for interpreting multimodal references. This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints. A preliminary evaluation indicates the effectiveness of this approach, p"
P04-1001,C00-1054,0,0.0156874,"temporal, and contextual). A correct interpretation can only be attained by simultaneously considering these constraints. In this process, two issues are important: first, a mechanism to combine information from various sources to form an overall interpretation given a set of constraints; and second, a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints. The first issue focuses on the fusion aspect, which has been well studied in earlier work, for example, through unificationbased approaches (Johnston 1998) or finite state approaches (Johnston and Bangalore, 2000). This paper focuses on the second issue of optimization. As in natural language interpretation, there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing. We have developed a graph-based optimization approach for interpreting multimodal references. This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints. A preliminary evaluation indicates the effectiveness of this approach, particularly for complex user inputs that involve multiple"
P04-1001,W97-1401,0,0.122035,"Missing"
P04-1001,P99-1024,0,\N,Missing
P04-1001,C98-1099,0,\N,Missing
P04-1001,P02-1048,0,\N,Missing
P06-2008,P98-1122,0,0.019436,"ddresses a different aspect of interactive question answering. Both issues raised earlier (Section 1) are inspired by earlier work on intelligent conversational systems. Automated identification of user intent has played an important role in conversational systems. Tremendous amounts of work has focused on this aspect (Stolcke et al., 2000). To improve dialog performance, much effort has also been put on techniques to automatically detect errors during interaction. It has shown that during human machine dialog, there are sufficient cues for machines to automatically identify error conditions (Levow, 1998; Litman et al., 1999; Hirschberg et al., 2001; Walker et al., 2002). The awareness of erroneous situations can help systems make intelligent decisions about how to best guide human partners through the conversation and accomplish the tasks. Motivated by these earlier studies, the goal of this paper is to investigate whether these two issues can be applied in question answering to facilitate intelligent conversational QA. follow-up questions, which will further influence system performance. Both the awareness of problematic situations and understanding of user intent will allow QA systems to a"
P06-2008,P99-1040,0,0.0356387,"Missing"
P06-2008,W03-1206,0,0.0962342,"error free situations). Once users are motivated to find specific information related to their information goals, the interaction context can provide useful cues for the system to automatically identify problematic situations and user intent. 1 Introduction Interactive question answering (QA) has been identified as one of the important directions in QA research (Burger et al., 2001). One ultimate goal is to support intelligent conversation between a user and a QA system to better facilitate user information needs. However, except for a few systems that use dialog to address complex questions (Small et al., 2003; Harabagiu et al., 2005), the general dialog capabilities have been lacking in most ques∗ This work was partially supported by IIS-0347548 from the National Science Foundation. 57 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 57–64, c Sydney, July 2006. 2006 Association for Computational Linguistics as in an interactive setting. Interactive QA has been applied to process complex questions. For analytical and non-factual questions, it is hard to anticipate answers. Clarification dialogues can be applied to negotiate with users about the intent of their questions (Sm"
P06-2008,J00-3003,0,0.0922074,"Missing"
P06-2008,P05-1026,0,0.156457,"s). Once users are motivated to find specific information related to their information goals, the interaction context can provide useful cues for the system to automatically identify problematic situations and user intent. 1 Introduction Interactive question answering (QA) has been identified as one of the important directions in QA research (Burger et al., 2001). One ultimate goal is to support intelligent conversation between a user and a QA system to better facilitate user information needs. However, except for a few systems that use dialog to address complex questions (Small et al., 2003; Harabagiu et al., 2005), the general dialog capabilities have been lacking in most ques∗ This work was partially supported by IIS-0347548 from the National Science Foundation. 57 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 57–64, c Sydney, July 2006. 2006 Association for Computational Linguistics as in an interactive setting. Interactive QA has been applied to process complex questions. For analytical and non-factual questions, it is hard to anticipate answers. Clarification dialogues can be applied to negotiate with users about the intent of their questions (Small et al., 2003). Recent"
P06-2008,N01-1027,0,0.019502,"eractive question answering. Both issues raised earlier (Section 1) are inspired by earlier work on intelligent conversational systems. Automated identification of user intent has played an important role in conversational systems. Tremendous amounts of work has focused on this aspect (Stolcke et al., 2000). To improve dialog performance, much effort has also been put on techniques to automatically detect errors during interaction. It has shown that during human machine dialog, there are sufficient cues for machines to automatically identify error conditions (Levow, 1998; Litman et al., 1999; Hirschberg et al., 2001; Walker et al., 2002). The awareness of erroneous situations can help systems make intelligent decisions about how to best guide human partners through the conversation and accomplish the tasks. Motivated by these earlier studies, the goal of this paper is to investigate whether these two issues can be applied in question answering to facilitate intelligent conversational QA. follow-up questions, which will further influence system performance. Both the awareness of problematic situations and understanding of user intent will allow QA systems to adapt better strategies during interaction and"
P06-2008,C98-1117,0,\N,Missing
P06-2008,N03-1007,0,\N,Missing
P07-1047,J93-2003,0,0.0149696,"the complexity in the gazespeech pattern, we propose to use statistical translation models. Given a time window of enough length, a speech input that contains a list of spoken references (e.g., definite noun phrases) is always accompanied by a list of naturally occurred eye fixations and therefore a list of objects receiving those fixations. All those pairs of speech references and corresponding fixated objects could be viewed as parallel, i.e. they co-occur within the time window. This situation is very similar to the training process of translation models in statistical machine translation (Brown et al., 1993), where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns. The same idea can be borrowed here: by exploring the co-occurrence statistics, we hope to uncover the exact mapping between those eye fixated objects and spoken references. The intuition is that, the more often a fixation is found to exclusively co-occur with a spoken reference, the more likely a mapping should 371 be established between them. 5 Translation Models for Vocabulary Acquisition and Interpretation Formally, we denote the set of observations by D ="
P07-1047,N07-1036,1,0.342124,"ects on the graphical display. The main advantages of this approach include: 1) It is an unsupervised approach with minimum human inference; 2) It does not need any prior knowledge to train a statistical translation model; 3) It yields probabilities that indicate the reliability of the mappings. Certainly, our current approach is built upon simplified assumptions. It is quite challenging to incorporate eye gaze information since it is extremely noisy with large variances. Recent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users (Qu and Chai, 2007). In addition, visual properties of the interface also affect user gaze behavior and thus influence the predication of attention (Prasov et al., 2007) based on eye gaze. Our future work will develop models to address these variations. Nevertheless, the results from our current work have several important implications in building robust conversational interfaces. First of all, most conversational systems are built with static knowledge space (e.g., vocabularies) and can only be updated by the system developers. Our approach can potentially allow the system to automatically acquire knowledge and"
P10-1160,W05-0620,0,0.125261,"Missing"
P10-1160,P08-1090,0,0.0156768,"mention, The two companies, which is the arg0 of produce. As described in Table 2, feature 1 is instantiated with a value of create.agentsend.agent, where create and send are the VerbNet classes that contain produce and ship, respectively. In the conversion to LibLinear’s instance representation, this instantiation is converted into a single binary feature create.agent-send.agent whose value is one. Features 1 and 11 are instantiated once for each mention in c0 , allowing the model to consider information from multiple mentions of the same entity. Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We used a similar PMI score, but defined it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5 http://verbs.colorado.edu/semlink the caption for Table 2): pmi(hp, iargn i , hpf , argf i) = Pcoref (hp, iargn i , hpf , argf i) log Pcoref (hp, iargn i , ∗)Pcoref (hpf , argf i , ∗) (6) To compute Equation 6, we first labeled a subset of the Gigaword"
P10-1160,1993.eamt-1.1,0,0.36284,"Missing"
P10-1160,N09-1017,1,0.939632,"onducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that take no local arguments (Gerber et al., 2009); however, this work did not address the identification of implicit arguments. Burchardt et al. (2005) suggested approaches to implicit argument identification based on observed coreference patterns; however, the authors did not implement and evaluate such methods. We draw insights from all three of these studies. We show that the identification of implicit arguments for nominal predicates leads to fuller semantic interpretations when compared to traditional SRL methods. Furthermore, motivated by Burchardt et al., our model uses a quantitative analysis of naturally occurring coreference patter"
P10-1160,W07-1522,0,0.0253828,"onclude in Section 7. 2 Related work Palmer et al. (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Their approach used a fine-grained domain model to assess the compatibility of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments"
P10-1160,P09-2022,0,0.0598522,"Missing"
P10-1160,C04-1174,0,0.0516293,"of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that take no local arguments"
P10-1160,J93-2004,0,0.0396289,"Missing"
P10-1160,P86-1004,0,0.88351,"led. 1583 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics In the following section, we review related research, which is historically sparse but recently gaining traction. We present our annotation effort in Section 3, and follow with our implicit argument identification model in Section 4. In Section 5, we describe the evaluation setting and present our experimental results. We analyze these results in Section 6 and conclude in Section 7. 2 Related work Palmer et al. (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Their approach used a fine-grained domain model to assess the compatibility of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard"
P10-1160,N04-1041,0,0.0822068,"Missing"
P10-1160,J08-2005,0,0.0574081,"event sequences using pointwise mutual information (PMI) between syntactic positions. We used a similar PMI score, but defined it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5 http://verbs.colorado.edu/semlink the caption for Table 2): pmi(hp, iargn i , hpf , argf i) = Pcoref (hp, iargn i , hpf , argf i) log Pcoref (hp, iargn i , ∗)Pcoref (hpf , argf i , ∗) (6) To compute Equation 6, we first labeled a subset of the Gigaword corpus (Graff, 2003) using the verbal SRL system of Punyakanok et al. (2008) and the nominal SRL system of Gerber et al. (2009). We then identified coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise hp, argn i and hpf , argf i. The numerator in Equation 6 is defined as M N . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs comprising an argument position (e.g., hp, argn i) and any other argument position. Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel"
P10-1160,W09-2417,0,0.515748,"Missing"
P10-1160,W08-2121,0,\N,Missing
P10-1160,H86-1011,0,\N,Missing
P10-1160,S10-1008,0,\N,Missing
P10-1160,W09-1201,0,\N,Missing
P14-2003,W08-0113,0,0.0294934,"is a yellow pepper (5) M: ok, I see apple but orangish yellow (6) D: ok, so that yellow pepper is named Brittany (7) M: uh, the bottom left of those four? Because I do see a yellow pepper in the upper right (8) D: the upper right of the four of them? (9) M: yes (10) D: ok, so that is basically the one to the right of the blue cup (11) M: yeah (12) D: that is actually an apple (13) Previous works on situated referential grounding have mainly focused on computational models that connect linguistic referring expressions to the perceived environment (Gorniak and Roy, 2004; Gorniak and Roy, 2007; Siebert and Schlangen, 2008; Matuszek et al., 2012; Jayant and Thomas, 2013). These works have provided valuable insights on how to manually and/or automatically build key components (e.g., semantic parsing, grounding functions between visual features and words, mapping procedures) for a situated referential grounding system. However, most of these works only dealt with the interpretation of single referring expressions, rather than interrelated expressions in collaborative dialogue. Some earlier work (Edmonds, 1994; Heeman and Hirst, 1995) proposed a symbolic reasoning (i.e. planning) based approach to incorporate coll"
P14-2003,E09-1022,0,0.0298627,"c parsing, grounding functions between visual features and words, mapping procedures) for a situated referential grounding system. However, most of these works only dealt with the interpretation of single referring expressions, rather than interrelated expressions in collaborative dialogue. Some earlier work (Edmonds, 1994; Heeman and Hirst, 1995) proposed a symbolic reasoning (i.e. planning) based approach to incorporate collaborative dialogue. However, in situated settings pure symbolic approaches will not be sufficient and new approaches that are robust to uncertainties need to be pursued. DeVault and Stone (2009) proposed a hybrid approach which combined symbolic reasoning and machine learning for interpreting referential grounding dialogue. But their “environment” was a simplistic block world and the issue of mismatched perceptions was not addressed. 3 As we can see from this example, both the director and the matcher make extra efforts to overcome the mismatched perceptions through collaborative dialogue. Our ultimate goal is to develop computational approaches that can ground interrelated referring expressions to the physical world, and enable collaborative actions of the dialogue agent (similar to"
P14-2003,C94-2182,0,0.324488,"ing expressions to the perceived environment (Gorniak and Roy, 2004; Gorniak and Roy, 2007; Siebert and Schlangen, 2008; Matuszek et al., 2012; Jayant and Thomas, 2013). These works have provided valuable insights on how to manually and/or automatically build key components (e.g., semantic parsing, grounding functions between visual features and words, mapping procedures) for a situated referential grounding system. However, most of these works only dealt with the interpretation of single referring expressions, rather than interrelated expressions in collaborative dialogue. Some earlier work (Edmonds, 1994; Heeman and Hirst, 1995) proposed a symbolic reasoning (i.e. planning) based approach to incorporate collaborative dialogue. However, in situated settings pure symbolic approaches will not be sufficient and new approaches that are robust to uncertainties need to be pursued. DeVault and Stone (2009) proposed a hybrid approach which combined symbolic reasoning and machine learning for interpreting referential grounding dialogue. But their “environment” was a simplistic block world and the issue of mismatched perceptions was not addressed. 3 As we can see from this example, both the director and"
P14-2003,J95-3003,0,0.447164,"to the perceived environment (Gorniak and Roy, 2004; Gorniak and Roy, 2007; Siebert and Schlangen, 2008; Matuszek et al., 2012; Jayant and Thomas, 2013). These works have provided valuable insights on how to manually and/or automatically build key components (e.g., semantic parsing, grounding functions between visual features and words, mapping procedures) for a situated referential grounding system. However, most of these works only dealt with the interpretation of single referring expressions, rather than interrelated expressions in collaborative dialogue. Some earlier work (Edmonds, 1994; Heeman and Hirst, 1995) proposed a symbolic reasoning (i.e. planning) based approach to incorporate collaborative dialogue. However, in situated settings pure symbolic approaches will not be sufficient and new approaches that are robust to uncertainties need to be pursued. DeVault and Stone (2009) proposed a hybrid approach which combined symbolic reasoning and machine learning for interpreting referential grounding dialogue. But their “environment” was a simplistic block world and the issue of mismatched perceptions was not addressed. 3 As we can see from this example, both the director and the matcher make extra e"
P14-2003,Q13-1016,0,0.0629241,"ish yellow (6) D: ok, so that yellow pepper is named Brittany (7) M: uh, the bottom left of those four? Because I do see a yellow pepper in the upper right (8) D: the upper right of the four of them? (9) M: yes (10) D: ok, so that is basically the one to the right of the blue cup (11) M: yeah (12) D: that is actually an apple (13) Previous works on situated referential grounding have mainly focused on computational models that connect linguistic referring expressions to the perceived environment (Gorniak and Roy, 2004; Gorniak and Roy, 2007; Siebert and Schlangen, 2008; Matuszek et al., 2012; Jayant and Thomas, 2013). These works have provided valuable insights on how to manually and/or automatically build key components (e.g., semantic parsing, grounding functions between visual features and words, mapping procedures) for a situated referential grounding system. However, most of these works only dealt with the interpretation of single referring expressions, rather than interrelated expressions in collaborative dialogue. Some earlier work (Edmonds, 1994; Heeman and Hirst, 1995) proposed a symbolic reasoning (i.e. planning) based approach to incorporate collaborative dialogue. However, in situated settings"
P14-2003,W12-1621,1,0.873215,"ences, this paper presents a new approach using probabilistic labeling for referential grounding. This approach aims to integrate different types of evidence from the collaborative referential discourse into a unified scheme. Its probabilistic labeling procedure can generate multiple grounding hypotheses to facilitate follow-up dialogue. Our empirical results have shown the probabilistic labeling approach significantly outperforms a previous graphmatching approach for referential grounding. 1 Based on this data, as a first step we developed a graph-matching approach for referential grounding (Liu et al., 2012; Liu et al., 2013). This approach uses Attributed Relational Graph to capture collaborative discourse and employs a statespace search algorithm to find proper grounding results. Although it has made meaningful progress in addressing collaborative referential grounding under mismatched perceptions, the state-space search based approach has two major limitations. First, it is neither flexible to obtain multiple grounding hypotheses, nor flexible to incorporate different hypotheses incrementally for follow-up grounding. Second, the search algorithm tends to have a high time complexity for optima"
P14-2003,W13-4010,1,0.906851,"presents a new approach using probabilistic labeling for referential grounding. This approach aims to integrate different types of evidence from the collaborative referential discourse into a unified scheme. Its probabilistic labeling procedure can generate multiple grounding hypotheses to facilitate follow-up dialogue. Our empirical results have shown the probabilistic labeling approach significantly outperforms a previous graphmatching approach for referential grounding. 1 Based on this data, as a first step we developed a graph-matching approach for referential grounding (Liu et al., 2012; Liu et al., 2013). This approach uses Attributed Relational Graph to capture collaborative discourse and employs a statespace search algorithm to find proper grounding results. Although it has made meaningful progress in addressing collaborative referential grounding under mismatched perceptions, the state-space search based approach has two major limitations. First, it is neither flexible to obtain multiple grounding hypotheses, nor flexible to incorporate different hypotheses incrementally for follow-up grounding. Second, the search algorithm tends to have a high time complexity for optimal solutions. Thus,"
P14-2003,N03-5008,0,0.0443146,"Missing"
P16-1011,D12-1040,0,0.0506166,"long learning from humans in the future. 1 Introduction As a new generation of cognitive robots start to enter our lives, it is important to enable robots to follow human commands (Tellex et al., 2014; Thomason et al., 2015) and to learn new actions from human language instructions (Cantrell et al., 2012; Mohan et al., 2013). To achieve such a capability, one of the fundamental challenges is to link higher-level concepts expressed by human language to lower-level primitive actions the robot is familiar with. While grounding language to perception (Gorniak and Roy, 2007; Chen and Mooney, 2011; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013; Tellex et al., 2014; Liu et al., 2014; Liu and Chai, 2015) has received much attention in recent years, less work has addressed 108 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 108–117, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics cent work has shown that explicitly modeling resulting change of state for action verbs can improve grounded language understanding (Gao et al., 2016). Motivated by these studies, this paper focuses on result verbs and uses hypothesis spaces to exp"
P16-1011,Q13-1005,0,0.0191266,"ans in the future. 1 Introduction As a new generation of cognitive robots start to enter our lives, it is important to enable robots to follow human commands (Tellex et al., 2014; Thomason et al., 2015) and to learn new actions from human language instructions (Cantrell et al., 2012; Mohan et al., 2013). To achieve such a capability, one of the fundamental challenges is to link higher-level concepts expressed by human language to lower-level primitive actions the robot is familiar with. While grounding language to perception (Gorniak and Roy, 2007; Chen and Mooney, 2011; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013; Tellex et al., 2014; Liu et al., 2014; Liu and Chai, 2015) has received much attention in recent years, less work has addressed 108 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 108–117, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics cent work has shown that explicitly modeling resulting change of state for action verbs can improve grounded language understanding (Gao et al., 2016). Motivated by these studies, this paper focuses on result verbs and uses hypothesis spaces to explicitly represent the result"
P16-1011,W14-4313,1,0.57575,"al., 2007) and from interactions (Gil, 1994), our goal here is to explore the representation of verb semantics and its acquisition through language and action. There has been some work in the robotics community to translate natural language to robotic operations (Kress-Gazit et al., 2007; Jia et al., 2014; Sung et al., 2014; Spangenberg and Henrich, 2015), but not for the purpose of learning new actions. To support action learning, previously we have developed a system where the robot can acquire the meaning of a new verb (e.g., stack) by following human’s step-by-step language instructions (She et al., 2014a; She et al., 2014b). By performing the actions at each step, the robot is able to acquire the desired goal state associated with the new verb. Our empirical results have shown that representing acquired verbs by resulting states allow the robot to plan for primitive actions in novel situations. Moreover, recent work (Misra et al., 2014; Misra et al., 2015) has presented an algorithm for grounding higher-level commands such as “microwave the cup” to lowerlevel robot operations, where each verb lexicon is represented as the desired resulting states. Their empirical evaluations once again have"
P16-1011,P14-2003,1,0.675605,"ion of cognitive robots start to enter our lives, it is important to enable robots to follow human commands (Tellex et al., 2014; Thomason et al., 2015) and to learn new actions from human language instructions (Cantrell et al., 2012; Mohan et al., 2013). To achieve such a capability, one of the fundamental challenges is to link higher-level concepts expressed by human language to lower-level primitive actions the robot is familiar with. While grounding language to perception (Gorniak and Roy, 2007; Chen and Mooney, 2011; Kim and Mooney, 2012; Artzi and Zettlemoyer, 2013; Tellex et al., 2014; Liu et al., 2014; Liu and Chai, 2015) has received much attention in recent years, less work has addressed 108 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 108–117, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics cent work has shown that explicitly modeling resulting change of state for action verbs can improve grounded language understanding (Gao et al., 2016). Motivated by these studies, this paper focuses on result verbs and uses hypothesis spaces to explicitly represent the result states associated with these verbs. In"
P16-1011,P15-1096,0,0.186012,"), but not for the purpose of learning new actions. To support action learning, previously we have developed a system where the robot can acquire the meaning of a new verb (e.g., stack) by following human’s step-by-step language instructions (She et al., 2014a; She et al., 2014b). By performing the actions at each step, the robot is able to acquire the desired goal state associated with the new verb. Our empirical results have shown that representing acquired verbs by resulting states allow the robot to plan for primitive actions in novel situations. Moreover, recent work (Misra et al., 2014; Misra et al., 2015) has presented an algorithm for grounding higher-level commands such as “microwave the cup” to lowerlevel robot operations, where each verb lexicon is represented as the desired resulting states. Their empirical evaluations once again have shown the advantage of representing verbs as desired states in robotic systems. Different from these previous works, we represent verb semantics through a hypothesis space of fluents (rather than a single hypothesis). In addition, we present an incremental learning approach for inducing the hypothesis space and selecting the best hypothesis. corresponding ve"
P16-1011,N16-1019,1,0.489918,"Missing"
P16-1011,P16-1171,1,\N,Missing
P16-1171,P98-1013,0,0.166075,"., 2013; Liu et al., 2014; Naim et al., 2015; Liu and Chai, 2015), no previous work has investigated the link between physical causality denoted by action verbs and the change of state visually perceived. Our work here intends to address this limitation and examine whether the causality denoted by action verbs can provide top-down information to guide visual processing and improve grounded language understanding. 3 3.1 Modeling Physical Causality for Action Verbs Linguistics Background on Action Verbs Verb semantics have been studied extensively in linguistics (Pustejovsky, 1991; Levin, 1993; Baker et al., 1998; Kingsbury and Palmer, 2002). Particularly, for action verbs (such as run, throw, cook), Hovav and Levin (Hovav and Levin, 2010) propose that they can be divided into two types: manner verbs that “specify as part of their meaning a manner of carrying out an action” (e.g., nibble, rub, scribble, sweep, flutter, laugh, run, swim), and result verbs that “specify the coming about of a result state” (e.g., clean, cover, empty, fill, chop, cut, melt, open, enter). Result verbs can be further classified into three categories: Change of State verbs, which denote a change of state for a property of th"
P16-1171,J10-4006,0,0.0166413,"eces” and “halves” may indicate the CoS attribute “NumberOfPieces” for the verb “cut”. We simplify the problem by assuming that the causality vector c0 takes binary values, and also assuming the independence between different causality attributes. Thus, we can formulate this task as a group of binary classification problems: predicting whether a particular causality attribute is positive or negative given the DSM vector of a verb. We apply logistic regression to train a separate classifier for each attribute. Specifically, for the features of a verb, we use the Distributional Memory (typeDM) (Baroni and Lenci, 2010) vector. The class label indicates whether the corresponding attribute is associated with the verb. In our experiment we chose six attributes to study: Attachment, NumberOfPieces, Presence, Visibility, Location, and Size. For each one of the eleven verbs in the grounding task, we predict its causality knowledge using classifiers trained on all other verbs (i.e., 177 verbs in training set). To evaluate the predicted causality vectors, we applied them in the knowledge-driven approach (PVC-Knowledge). Grounding results were compared with the same method using the causality knowledge collected via"
P16-1171,blanco-etal-2008-causal,0,0.0204079,"er performance in grounding language to perception compared to previous approaches (Yang et al., 2016). 2 Related Work The notion of causality or causation has been explored in psychology, linguistics, and computational linguistics from a wide range of perspectives. For example, different types of causal relations such as causing, enabling, and preventing (Goldvarg and Johnson-Laird, 2001; Wolff and Song, 2003) have been studied extensively as well as their linguistic expressions (Wolff, 2003; Song and Wolff, 2003; Neeleman et al., 2012) and automated extraction of causal relations from text (Blanco et al., 2008; Mulkar-Mehta et al., 2011; Radinsky et al., 2012; Riaz and Girju, 2014). Different from these previous works, this paper focuses on the physical causality of action verbs, in other words, change of state in the physical world caused by action verbs as described in (Hovav and Levin, 2010). This is motivated by recent advances in computer vision, robotics, and grounding language to perception and action. A recent trend in computer vision has started looking into intermediate representations beyond lower-level visual features for action recognition, for example, by incorporating object affordan"
P16-1171,kingsbury-palmer-2002-treebank,0,0.106141,"2014; Naim et al., 2015; Liu and Chai, 2015), no previous work has investigated the link between physical causality denoted by action verbs and the change of state visually perceived. Our work here intends to address this limitation and examine whether the causality denoted by action verbs can provide top-down information to guide visual processing and improve grounded language understanding. 3 3.1 Modeling Physical Causality for Action Verbs Linguistics Background on Action Verbs Verb semantics have been studied extensively in linguistics (Pustejovsky, 1991; Levin, 1993; Baker et al., 1998; Kingsbury and Palmer, 2002). Particularly, for action verbs (such as run, throw, cook), Hovav and Levin (Hovav and Levin, 2010) propose that they can be divided into two types: manner verbs that “specify as part of their meaning a manner of carrying out an action” (e.g., nibble, rub, scribble, sweep, flutter, laugh, run, swim), and result verbs that “specify the coming about of a result state” (e.g., clean, cover, empty, fill, chop, cut, melt, open, enter). Result verbs can be further classified into three categories: Change of State verbs, which denote a change of state for a property of the verb’s object (e.g. “to war"
P16-1171,P14-2003,1,0.733734,"evious works (She et al., 2014a; She et al., 2014b; Misra et al., 2015; She and Chai, 2016) explicitly model verbs with predicates describing the resulting states of actions. Their empirical evaluations have demonstrated how incorporating resulting states into verb representations can link language with underlying planning modules for robotic systems. These results have motivated a systematic investigation on modeling physical causality for action verbs. Although recent years have seen an increasing amount of work on grounding language to perception (Yu and Siskind, 2013; Walter et al., 2013; Liu et al., 2014; Naim et al., 2015; Liu and Chai, 2015), no previous work has investigated the link between physical causality denoted by action verbs and the change of state visually perceived. Our work here intends to address this limitation and examine whether the causality denoted by action verbs can provide top-down information to guide visual processing and improve grounded language understanding. 3 3.1 Modeling Physical Causality for Action Verbs Linguistics Background on Action Verbs Verb semantics have been studied extensively in linguistics (Pustejovsky, 1991; Levin, 1993; Baker et al., 1998; Kings"
P16-1171,P15-1096,0,0.391494,"hanges (or, in their terms, consequences of actions) for action recognition. More recently, Fire and Zhu (2015) have developed a framework to learn perceptual causal structures between actions and object statuses in videos. In the robotics community, as robots’ low-level control systems are often pre-programmed to handle (and thus execute) only primitive actions, a high-level language command will need to be translated to a sequence of primitive actions in order for the corresponding action to take place. To make such translation possible, previous works (She et al., 2014a; She et al., 2014b; Misra et al., 2015; She and Chai, 2016) explicitly model verbs with predicates describing the resulting states of actions. Their empirical evaluations have demonstrated how incorporating resulting states into verb representations can link language with underlying planning modules for robotic systems. These results have motivated a systematic investigation on modeling physical causality for action verbs. Although recent years have seen an increasing amount of work on grounding language to perception (Yu and Siskind, 2013; Walter et al., 2013; Liu et al., 2014; Naim et al., 2015; Liu and Chai, 2015), no previous"
P16-1171,N15-1017,0,0.0301893,"et al., 2014a; She et al., 2014b; Misra et al., 2015; She and Chai, 2016) explicitly model verbs with predicates describing the resulting states of actions. Their empirical evaluations have demonstrated how incorporating resulting states into verb representations can link language with underlying planning modules for robotic systems. These results have motivated a systematic investigation on modeling physical causality for action verbs. Although recent years have seen an increasing amount of work on grounding language to perception (Yu and Siskind, 2013; Walter et al., 2013; Liu et al., 2014; Naim et al., 2015; Liu and Chai, 2015), no previous work has investigated the link between physical causality denoted by action verbs and the change of state visually perceived. Our work here intends to address this limitation and examine whether the causality denoted by action verbs can provide top-down information to guide visual processing and improve grounded language understanding. 3 3.1 Modeling Physical Causality for Action Verbs Linguistics Background on Action Verbs Verb semantics have been studied extensively in linguistics (Pustejovsky, 1991; Levin, 1993; Baker et al., 1998; Kingsbury and Palmer, 20"
P16-1171,Q13-1003,0,0.0557584,"gh scale structure motivates us to use the Dixon typology as a basis to define our categorization of causality for verbs. In summary, previous linguistic literature has provided abundant evidence and discussion on change of state for action verbs. It has also provided extensive knowledge on potential dimensions that can be used to categorize change of state as described in this paper. 3.2 A Crowd-sourcing Study Motivated by the above linguistic insight, we have conducted a pilot study to examine the feasibility of causality modeling using a small set of verbs which appear in the TACoS corpus (Regneri et al., 2013). This corpus is a collection of natural language descriptions of actions that occur in a set of cooking videos. This is an ideal dataset to start with since it contains mainly descriptions of physical actions. Possibly because most actions in the cooking domain are goal-directed, a majority of the verbs in TACoS denote results of actions (changes of state) which can be observed in the world. More specifically, we chose ten verbs (clean, rinse, wipe, cut, chop, mix, stir, add, open, shake)) based on the criteria that they occur relatively frequently in the corpus and take a variety of differen"
P16-1171,W14-4322,0,0.0188856,"approaches (Yang et al., 2016). 2 Related Work The notion of causality or causation has been explored in psychology, linguistics, and computational linguistics from a wide range of perspectives. For example, different types of causal relations such as causing, enabling, and preventing (Goldvarg and Johnson-Laird, 2001; Wolff and Song, 2003) have been studied extensively as well as their linguistic expressions (Wolff, 2003; Song and Wolff, 2003; Neeleman et al., 2012) and automated extraction of causal relations from text (Blanco et al., 2008; Mulkar-Mehta et al., 2011; Radinsky et al., 2012; Riaz and Girju, 2014). Different from these previous works, this paper focuses on the physical causality of action verbs, in other words, change of state in the physical world caused by action verbs as described in (Hovav and Levin, 2010). This is motivated by recent advances in computer vision, robotics, and grounding language to perception and action. A recent trend in computer vision has started looking into intermediate representations beyond lower-level visual features for action recognition, for example, by incorporating object affordances (Koppula et al., 2013) and causality between actions and objects (Fat"
P16-1171,P16-1011,1,0.730663,"terms, consequences of actions) for action recognition. More recently, Fire and Zhu (2015) have developed a framework to learn perceptual causal structures between actions and object statuses in videos. In the robotics community, as robots’ low-level control systems are often pre-programmed to handle (and thus execute) only primitive actions, a high-level language command will need to be translated to a sequence of primitive actions in order for the corresponding action to take place. To make such translation possible, previous works (She et al., 2014a; She et al., 2014b; Misra et al., 2015; She and Chai, 2016) explicitly model verbs with predicates describing the resulting states of actions. Their empirical evaluations have demonstrated how incorporating resulting states into verb representations can link language with underlying planning modules for robotic systems. These results have motivated a systematic investigation on modeling physical causality for action verbs. Although recent years have seen an increasing amount of work on grounding language to perception (Yu and Siskind, 2013; Walter et al., 2013; Liu et al., 2014; Naim et al., 2015; Liu and Chai, 2015), no previous work has investigated"
P16-1171,W14-4313,1,0.601002,"and tracking method to detect state changes (or, in their terms, consequences of actions) for action recognition. More recently, Fire and Zhu (2015) have developed a framework to learn perceptual causal structures between actions and object statuses in videos. In the robotics community, as robots’ low-level control systems are often pre-programmed to handle (and thus execute) only primitive actions, a high-level language command will need to be translated to a sequence of primitive actions in order for the corresponding action to take place. To make such translation possible, previous works (She et al., 2014a; She et al., 2014b; Misra et al., 2015; She and Chai, 2016) explicitly model verbs with predicates describing the resulting states of actions. Their empirical evaluations have demonstrated how incorporating resulting states into verb representations can link language with underlying planning modules for robotic systems. These results have motivated a systematic investigation on modeling physical causality for action verbs. Although recent years have seen an increasing amount of work on grounding language to perception (Yu and Siskind, 2013; Walter et al., 2013; Liu et al., 2014; Naim et al.,"
P16-1171,N16-1019,1,0.50977,"Missing"
P16-1171,P13-1006,0,0.0447257,"lace. To make such translation possible, previous works (She et al., 2014a; She et al., 2014b; Misra et al., 2015; She and Chai, 2016) explicitly model verbs with predicates describing the resulting states of actions. Their empirical evaluations have demonstrated how incorporating resulting states into verb representations can link language with underlying planning modules for robotic systems. These results have motivated a systematic investigation on modeling physical causality for action verbs. Although recent years have seen an increasing amount of work on grounding language to perception (Yu and Siskind, 2013; Walter et al., 2013; Liu et al., 2014; Naim et al., 2015; Liu and Chai, 2015), no previous work has investigated the link between physical causality denoted by action verbs and the change of state visually perceived. Our work here intends to address this limitation and examine whether the causality denoted by action verbs can provide top-down information to guide visual processing and improve grounded language understanding. 3 3.1 Modeling Physical Causality for Action Verbs Linguistics Background on Action Verbs Verb semantics have been studied extensively in linguistics (Pustejovsky, 1991;"
P16-1171,C98-1013,0,\N,Missing
P17-1150,Q13-1005,0,0.012947,"ults further demonstrate that the interaction policy acquired from reinforcement learning leads to the most efficient interaction and the most reliable verb models. 2 Related Work To enable human-robot communication and collaboration, recent years have seen an increasing amount of works which aim to learn semantics of language that are grounded to agents’ perception (Gorniak and Roy, 2007; Tellex et al., 2014; Kim and Mooney, 2012; Matuszek et al., 2012a; Liu et al., 2014; Liu and Chai, 2015; Thomason et al., 2015, 2016; Yang et al., 2016; Gao et al., 2016) and action (Matuszek et al., 2012b; Artzi and Zettlemoyer, 2013; She et al., 2014; Misra et al., 2014, 2015; She and Chai, 2016). Specifically for verb semantics, recent works explored the connection between verbs and action planning (She et al., 2014; Misra et al., 2014, 2015; She and Chai, 2016), for example, by representing grounded verbs semantics as the desired goal state of the physical world that is a result of the corresponding actions. Such representations are learned based on example actions demonstrated by humans. Once acquired, these representations will allow agents to interpret verbs/commands issued by humans in new situations and apply acti"
P17-1150,D12-1040,0,0.028057,"Missing"
P17-1150,P09-1010,0,0.0374278,"n phase, Grd and Goal are initialized with existing knowledge of learned verb models. For the learning phase, a state sd ∈ SD is a four tuple: sd = <l, estart , eend , Grd>. estart and eend stands for the environment before the demonstration and after the demonstration. Markov Decision Process (MDP) and its closely related Reinforcement Learning (RL) have been applied to sequential decision-making problems in dynamic domains with uncertainties, e.g., dialogue/interaction management (Singh et al., 2002; Paek and Pieraccini, 2008; Williams and Zweig, 2016), mapping language commands to actions (Branavan et al., 2009), interactive robot learning (Knox and Stone, 2011), and interactive information retrieval (Li et al., 2017). In this work, we formulate the choice of when to ask what questions during interaction as a sequential decisionmaking problem and apply reinforcement learning to acquire an optimal policy to manage interaction. Specifically, each of the execution and learning phases is governed by one policy (i.e., θE and θD ), which is updated by the reinforcement learning algorithm. The use of RL intends to obtain optimal policies that can lead to the highest long-term reward by balancing the cost of"
P17-1150,P16-1171,1,0.778569,"Missing"
P17-1150,P14-2003,1,0.89176,"Missing"
P17-1150,D16-1155,1,0.75432,"Missing"
P17-1150,P15-1096,0,0.586363,"ognitive robots, one of the challenges is that robots do not have sufficient linguistic or world knowledge as humans do. For example, if a human asks a robot to boil the water but the robot has no knowledge what this verb phrase means and how this verb phrase relates to its own actuator, the robot will not be able to execute this command. Thus it is important for robots to continuously learn the meanings of new verbs and how the verbs are grounded to its underlying action representations from its human partners. To support learning of grounded verb semantics, previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016) rely on multiple instances of human demonstrations of corresponding actions. From these demonstrations, robots capture the state change of the environment caused by the actions and represent verb semantics as the desired goal state. One advantage of such state-based representation is that, when robots encounter the same verbs/commands in a new situation, the desired goal state will trigger the action planner to automatically plan a sequence of primitive actions to execute the command. While the state-based verb semantics provides an important link to connect verbs to the"
P17-1150,P16-1011,1,0.298382,"of the challenges is that robots do not have sufficient linguistic or world knowledge as humans do. For example, if a human asks a robot to boil the water but the robot has no knowledge what this verb phrase means and how this verb phrase relates to its own actuator, the robot will not be able to execute this command. Thus it is important for robots to continuously learn the meanings of new verbs and how the verbs are grounded to its underlying action representations from its human partners. To support learning of grounded verb semantics, previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016) rely on multiple instances of human demonstrations of corresponding actions. From these demonstrations, robots capture the state change of the environment caused by the actions and represent verb semantics as the desired goal state. One advantage of such state-based representation is that, when robots encounter the same verbs/commands in a new situation, the desired goal state will trigger the action planner to automatically plan a sequence of primitive actions to execute the command. While the state-based verb semantics provides an important link to connect verbs to the robot’s actuator, pre"
P17-1150,P16-1230,0,0.0649306,"Missing"
P17-1150,N16-1019,1,0.627642,"Missing"
P17-1150,W14-4313,1,0.871381,"mmunication with cognitive robots, one of the challenges is that robots do not have sufficient linguistic or world knowledge as humans do. For example, if a human asks a robot to boil the water but the robot has no knowledge what this verb phrase means and how this verb phrase relates to its own actuator, the robot will not be able to execute this command. Thus it is important for robots to continuously learn the meanings of new verbs and how the verbs are grounded to its underlying action representations from its human partners. To support learning of grounded verb semantics, previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016) rely on multiple instances of human demonstrations of corresponding actions. From these demonstrations, robots capture the state change of the environment caused by the actions and represent verb semantics as the desired goal state. One advantage of such state-based representation is that, when robots encounter the same verbs/commands in a new situation, the desired goal state will trigger the action planner to automatically plan a sequence of primitive actions to execute the command. While the state-based verb semantics provides an important link to c"
P18-1086,P17-1025,0,0.0316528,"a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction. Action-Effect Data Collection We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately. The idea of modeling object physical state change has also been studied in the computer vision commu"
P18-1086,D16-1044,0,0.0650771,"ter vision community (Fire and Zhu, 2016). Computational models have been developed to infer object states from observations and to further predict future state changes (Zhou and Berg, 2016; Wu et al., 2016, 2017). The action recognition task can be treated as detecting the transformation on object states (Fathi and Rehg, 2013; Yang et al., 2013; Wang et al., 2016). However these previous works only focus on the visual presentation of motion effects. Recent years have seen an increasing amount of work integrating language and vision, for example, visual question answering (Antol et al., 2015; Fukui et al., 2016; Lu et al., 2016), image description generation (Xu et al., 2015; Vinyals et al., 2015), and grounding language to perception (Yang et al., 2016; Roy, 2005; Tellex et al., 2011; Misra et al., 2017). While many approaches require a large amount of training data, recent works have developed zero/few shot learning for language and vision (Mukherjee and Hospedales, 2016; Xu et al., 2016, 2017a,b; Tsai and Salakhutdinov, 2017). Different from these previous works, this paper introduces a new task that connects language with vision for physical action-effect prediction. Actions (verb-noun pairs). W"
P18-1086,P16-1171,1,0.881929,"ly information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction. Action-Effect Data Collection We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately. The idea of modeling object physical s"
P18-1086,S13-1035,0,0.0134355,"shot learning for language and vision (Mukherjee and Hospedales, 2016; Xu et al., 2016, 2017a,b; Tsai and Salakhutdinov, 2017). Different from these previous works, this paper introduces a new task that connects language with vision for physical action-effect prediction. Actions (verb-noun pairs). We selected 40 nouns that represent everyday life objects, most of them are from the COCO dataset (Lin et al., 2014), with a combination of food, kitchen ware, furniture, indoor objects, and outdoor objects. We also identified top 3000 most frequently used verbs from Google Syntactic N-gram dataset (Goldberg and Orwant, 2013) (Verbargs set). And we extracted top frequent verb-noun pairs containing a verb from the top 3000 verbs and a noun in the 40 nouns which hold a dobj (i.e., direct object) dependency relation. This resulted in 6573 candidate verbnoun pairs. As changes to an object can occur at various dimensions (e.g., size, color, location, attachment, etc.), we manually selected a subset of verb-noun pairs based on the following criteria: (1) changes to the objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple"
P18-1086,D11-1027,0,0.154297,"own that, using a simple bootstrapping strategy, our approach can combine the noisy web data with a small number of seed examples to improve action-effect prediction. In addition, for a new verb-noun pair, our approach can infer its effect descriptions and predict action-effect relations only based on 3 image examples. The contributions of this paper are three folds. First, it introduces a new task on physical actioneffect prediction, a first step towards an under2 Related Work In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts fro"
P18-1086,C10-1032,0,0.0280567,"these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; C"
P18-1086,D16-1014,0,0.135356,"ch can infer its effect descriptions and predict action-effect relations only based on 3 image examples. The contributions of this paper are three folds. First, it introduces a new task on physical actioneffect prediction, a first step towards an under2 Related Work In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts a"
P18-1086,D17-1106,0,0.0602605,"Missing"
P18-1086,P16-1011,1,0.851729,"e (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation. In the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment (i.e., from images)"
P18-1086,P15-1096,0,0.03005,"e objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation. In the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment"
P18-1086,P17-1150,1,0.876715,"Missing"
P18-1086,W14-4313,1,0.777622,"(1) changes to the objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation. In the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived"
P18-1086,D16-1089,0,0.0376384,"Missing"
P18-1086,speer-havasi-2012-representing,0,0.0186414,"tions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment (i.e., from images). Effects Described in Language. The basic knowledge about physical action-effect is so fundamental and shared among humans. It is often presupposed in our communication and not explicitly stated. Thus, it is difficult to extract naive action-effect relations from the existing textual data (e.g., web). This kind of knowledge is also not readily available in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012). To overcome this problem, we applied crowd-sourcing (Amazon Mechanical Turk) and collected a dataset of language descriptions describing effects for each of the 140 verb-noun pairs. The workers were shown a verb-noun pair, and were asked to use their own words and imag936 Action ignite paper soak shirt fry potato stain shirt Effect Text The paper is on fire. The shirt is thoroughly wet. The potatoes become crisp and golden. There is a visible mark on the shirt. Table 1: Example action and effect text from our collected data. inations to describe what changes might occur to the corresponding"
P18-1086,D14-1162,0,0.0800801,"Missing"
P18-1086,D16-1201,0,0.0420641,"Missing"
P18-1086,P12-1065,0,0.0768891,"Missing"
P18-1086,N16-1019,1,0.891774,"Missing"
P18-1086,N16-1023,0,0.0872902,"2 Related Work In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/l"
P18-1086,D17-1099,0,0.0677201,"data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction. Action-Effect Data Collection We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately. The idea of modeling object physical state change has also been studied in the computer vision community (Fire and Zhu, 2016"
W00-1011,P98-2129,0,0.0237563,"s adaptive dialog management. Introduction A new generation of dialog systems should be viewed as learning systems rather than static models (Jokinen, 2000). Close-world and static approaches have tremendous limitations and often fail when the task becomes complex and the application environment and knowledge changes. Thus, the learning capability of a dialog system has become an important issue. It has been addressed in many different aspects including dynamic construction of mutual knowledge (Andersen et al, 1999), learning of speech acts (Stolcker et al, 1998), learning optimal strategies (Litman et al, 1998; Litman et al, 1999; Walker et al, 1998), collaborative agent in plan recognition (Lesh et al, 1999), etc. This paper addresses the dynamic user modeling and dialog-goal utility measurement to facilitate adaptive dialog behavior. For any dialog system dealing with a technical domain, such as repair support (Weis, 1997), help-desk support, etc, it is crucial for the system not only to pay attention to the user knowledge and experience level and dialog goals, but more important, to have certain mechanisms that adapt the system behavior in terms of action planning, content selection, and content"
W00-1011,P98-2219,0,\N,Missing
W00-1011,C98-2214,0,\N,Missing
W00-1011,C98-2124,0,\N,Missing
W03-0701,J95-1003,0,0.0474767,"Missing"
W03-0701,H89-2054,0,\N,Missing
W04-2504,P98-1013,0,0.0097976,"in interpreting context questions. 2.3 Discourse Processing Given the above discussion, the goal of discourse modeling for context question answering is to automatically identify the discourse roles of a question and discourse relations between questions as the QA session proceeds. This may be a difficult task that requires rich knowledge and deep semantic processing. However, the recent advancement in semantic processing and discourse parsing has provided an excellent foundation for this task. The discourse roles are higher-level abstracts of the semantic roles as those provided in FrameNet (Baker et al., 1998) and Propbank (Kingsbury and Palmer 2002). Recent corpus-based approaches to identify semantic roles (Roth et al 2002, Gildea and Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et al., 2003) have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure. Furthermore, recent work also provides discourse annotated corpora with rhetorical relations (Carlson, et al., 2003) and techniques for discourse paring for texts (Soricut and Marcu, 2003). All these recent advances make the semantic-rich discourse modeling possible. For example, a c"
W04-2504,J86-3001,0,0.594046,"Missing"
W04-2504,J02-3001,0,0.165689,"and Focus. Target indicates the expected answer type such as whether it should be a proposition (e.g., for why and how questions), or a specific type of entity (e.g., TIME and PLACE). Topic indicates the “aboutness” or the scope related to a question. Focus indicates the current focus of attention given a particular topic. Focus always refers to a particular aspect of Topic. Since the informational perspective of discourse should capture the semantics of what has been conveyed, Topic and Focus are linked with the semantic information of a question, for example, semantic roles as described in (Gildea and Jurafsky 2002). Semantic roles concern with the roles of constitutes in a question in terms of its predicate-argument structure. The discourse roles link the semantic roles of individual questions together with respect to the discourse progress through Topic and Focus. For example, Topic can be of type Activity or Entity. Activity can be further categorized by ActType, Participant, and Peripheral. ActType indicates the type of the activity; Participant indicates entities that are participating in the activity with different semantic roles. Peripheral captures auxiliary information such Intent Act: Request M"
W04-2504,P02-1031,0,0.0138008,"cally identify the discourse roles of a question and discourse relations between questions as the QA session proceeds. This may be a difficult task that requires rich knowledge and deep semantic processing. However, the recent advancement in semantic processing and discourse parsing has provided an excellent foundation for this task. The discourse roles are higher-level abstracts of the semantic roles as those provided in FrameNet (Baker et al., 1998) and Propbank (Kingsbury and Palmer 2002). Recent corpus-based approaches to identify semantic roles (Roth et al 2002, Gildea and Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et al., 2003) have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure. Furthermore, recent work also provides discourse annotated corpora with rhetorical relations (Carlson, et al., 2003) and techniques for discourse paring for texts (Soricut and Marcu, 2003). All these recent advances make the semantic-rich discourse modeling possible. For example, a collection of context questions (and answers) can be annotated in terms of their discourse roles and relations. Specifically, the following information can be either au"
W04-2504,C00-1043,0,0.0271257,"ther discusses the potential impact of this refined discourse structure on context question answering. Q1: What is the name of the volcano that destroyed the ancient city of Pompeii? Q2: When did it happen? Q3: how many people were killed? Q4: how tall was this volcano? Q5: Any pictures? Q6: Where is Mount Rainier? Figure 1: An example of context questions 2 Semantic-rich Discourse Modeling For processing single questions, an earlier study shows that an impressive improvement can be achieved when more knowledge-intensive NLP techniques are applied at both question and answer processing level (Harabagiu et al., 2000). For context questions, a parallel question would be whether rich contextual knowledge will help interpret subsequent questions and extracting answers. To address this question, we propose a semantic-rich discourse modeling that captures both discourse roles of questions and discourse transitions between questions, and investigate its usefulness in context question answering. 2.1 Discourse Roles In context question answering, each question is situated in a context. In addition to the semantic information carried by important syntactic entities (such as noun phrase, verb phrase, preposition ph"
W04-2504,P03-1002,0,0.0155973,"urse roles of a question and discourse relations between questions as the QA session proceeds. This may be a difficult task that requires rich knowledge and deep semantic processing. However, the recent advancement in semantic processing and discourse parsing has provided an excellent foundation for this task. The discourse roles are higher-level abstracts of the semantic roles as those provided in FrameNet (Baker et al., 1998) and Propbank (Kingsbury and Palmer 2002). Recent corpus-based approaches to identify semantic roles (Roth et al 2002, Gildea and Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et al., 2003) have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure. Furthermore, recent work also provides discourse annotated corpora with rhetorical relations (Carlson, et al., 2003) and techniques for discourse paring for texts (Soricut and Marcu, 2003). All these recent advances make the semantic-rich discourse modeling possible. For example, a collection of context questions (and answers) can be annotated in terms of their discourse roles and relations. Specifically, the following information can be either automatically identified o"
W04-2504,N03-1030,0,0.0410252,"The discourse roles are higher-level abstracts of the semantic roles as those provided in FrameNet (Baker et al., 1998) and Propbank (Kingsbury and Palmer 2002). Recent corpus-based approaches to identify semantic roles (Roth et al 2002, Gildea and Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et al., 2003) have been successful in identifying domain independent semantic relations with respect to the predicate-argument structure. Furthermore, recent work also provides discourse annotated corpora with rhetorical relations (Carlson, et al., 2003) and techniques for discourse paring for texts (Soricut and Marcu, 2003). All these recent advances make the semantic-rich discourse modeling possible. For example, a collection of context questions (and answers) can be annotated in terms of their discourse roles and relations. Specifically, the following information can be either automatically identified or manually annotated: • Syntactic structures automatically identified from a parser (Collins, 1997); • Semantic roles of entities in the question (Gildea and Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et al., 2003); • Discourse roles either manually annotated or identified by rules that map directly from se"
W09-3928,W02-0216,0,0.0321165,"me gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (B"
W09-3928,P07-1047,1,0.88387,"nings of spoken words by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while interacting with a graphic display, a user could be talking about objects that were previously seen on the display or something completely unrelated to any object th"
W09-3928,E03-2001,0,0.02564,"utterances and thus can be potentially detrimental to word acquisition. To address this problem, this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition. Our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance. 1 Introduction Spoken conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon et al., 2002), intelligent space station control (Aist et al., 2003), and automated training and education (Razzaq and Heffernan, 2004). As in any conversational system, one major bottleneck in conversational interfaces is robust language interpretation. To address this problem, previous multimodal conversational systems have utilized penbased or deictic gestures (Bangalore and Johnston, 2004; Qu and Chai, 2006) to improve interpretation. Besides gestures, eye movements that naturally occur during interaction provide another important channel for language understanding, for example, reference resolution (Byron et al., 2005; Prasov and Chai, 2008). Recent work"
W09-3928,D08-1026,1,0.673133,"rds by grounding them to visual entities (Liu et al., 2007) or domain concepts (Qu and Chai, 2008). Psycholinguistic studies have shown that eye gaze indicates a person’s attention (Just and Carpenter, 1976), and eye movement can facilitate spoken language comprehension (Tanenhaus et al., 1995; Eberhard et al., 1995). It has been found that users’ eyes move to the mentioned object directly before speaking a word (Meyer et al., 1998; Rayner, 1998; Griffin and Bock, 2000). This parallel behavior of eye gaze and speech production motivates our previous work on word acquisition (Liu et al., 2007; Qu and Chai, 2008). However, in interactive conversation, human gaze behavior is much more complex than in the typical controlled settings used in psycholinguistic studies. There are different types of eye movements (Kahneman, 1973). The naturally occurring eye gaze during speech production may serve different functions, for example, to engage in the conversation or to manage turn taking (Nakano et al., 2003). Furthermore, while interacting with a graphic display, a user could be talking about objects that were previously seen on the display or something completely unrelated to any object the user is looking at"
W09-3928,P03-1070,0,\N,Missing
W09-3930,P07-1131,0,0.452681,"rovides tremendous information about conversation participants. Given the increasing amount of available conversation data (e.g., conversation scripts such as meeting scripts, court records, and online chatting), an important question is what do we know about conversation participants? The capability to automatically acquire such information can benefit many applications, for example, development of social networks and discovery of social dynamics. Related to this question, previous work has developed techniques to extract profiling information about participants from conversation interviews (Jing et al., 2007) and to automatically identify dynamics between conversation participants Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 206–215, c Queen Mary University of London, September 2009. 2009 Association for Computational Linguistics 206 uation. the textual entailment task due to unique characteristics about conversation and conversational implicature. To predict entailment, we developed a probabilisitic framework that incorporates semantic representation of conversation context. Our preliminary experimental results have shown that"
W09-3930,H05-1079,0,0.098096,"Missing"
W09-3930,N06-1006,0,0.0967463,"Missing"
W09-3930,doddington-etal-2004-automatic,0,0.0178222,"a diconversation entailment, as we will see later. alogue segment D, we estimate the probability More specifically, a clause is made up by two components: Term and Predicate. P (D  H|D, H) Term: A term can be an entity or an event. An Suppose we have a representation of a diaentity refers to a person, a place, an organization, logue segment D in m clauses d1 , . . . , dm and a or other real world entities. This follows the conrepresentation of the hypothesis H in n clauses cept of mention in the Automatic Content Extrach1 , . . . , hn . Since a hypothesis is the conjunction (ACE) evaluation (Doddington et al., 2004). tion of the decomposed clauses, whether it can be An event refers to an action or an activity. For inferred from a segment is equivalent to whether example, from the sentence “John married Eva in all of its clauses can be inferred from the seg1940” we can identify an event of marriage. Folment. We further simplify the problem by assumlowing the neo-Davidsonian representation (Paring that whether a clause is entailed from a diasons, 1990), all the events are reified as terms in logue segment is conditionally independent from our representation. other clauses. Note that this conditional indepe"
W09-3930,D08-1027,0,0.0150673,"Missing"
W09-3930,P04-1085,0,0.309295,"ties, corporate culture, etc.). To focus our work on the entailment problem, we use the transcribed scripts of the dialogues in our experiments. We also make use of available annotations such as syntactic structures, disfluency markers, and dialogue acts. Related Work Recent work has applied different approaches to acquire information about conversation participants based on human-human conversation scripts, for example, to extract profiling information from conversation interviews (Jing et al., 2007) and to identify agreement/disagreement between participants from multiparty meeting scripts (Galley et al., 2004). In human-machine conversation, inference about conversation participants has been studied as a part of user modeling. For example, earlier work has investigated inference of user intention from utterances to control clarification dialogue (Horvitz and Paek, 2001) and recognition of user emotion and attitude from utterances for intelligent tutoring systems (Litman and Forbes-Riley, 2006). In contrast to previous work, we propose a new angle to address information acquisition about conversation participants, namely, through conversation entailment. This work is inspired by a large body of rece"
W09-3930,H05-1047,0,0.157235,"Missing"
W09-3930,W07-1401,0,0.406948,"This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction. Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment. 1 Example 1: Dialogue Segment: A: And where about were you born? B: Up in Person Country. Hypothesis: (1) B was born in Person Country. (2) B lives in Person Country. Inspired by textual entailment (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), conversation entailment provides an intermediate step towards acquiring information about conversation participants. What we should know or would like to know about a participant can be rather open. The type of information needed about participants is also application-dependent and difficult to generalize. In conversation entailment, we will not face this problem since hypotheses can be used to express any type of information about a participant one might be interested in. Although hypotheses are currently given in our investigation, they can potentially be automatically generated based on i"
W09-3930,H05-1049,0,\N,Missing
W10-4357,W97-1401,0,0.107306,"Missing"
W10-4357,P07-2024,0,0.0283948,"Missing"
W10-4357,W09-3944,0,0.029513,"Missing"
W10-4357,N06-2010,0,0.0292664,"ential type distinguishes singular and plural usages; (2) a three way classification between generic, singular, or plural types. Our empirical results have shown that incorporation of gesture improves performance on differentiating between the generic and the referential type. Incorporation of gesture can also compensate for limitations in automated language processing (e.g., reliable recognition of dialogue acts) and achieve comparable results. These findings have important implications for machine translation of you expressions from multiparty meetings. 2 Gestures in Coreference Resolution. Eisenstein and Davis (2006; 2007) examined coreference resolution on a corpus of speaker-listener pairs in which the speaker had to describe the workings of a mechanical device to the listener, with the help of visual aids. In this gesture heavy dataset, they found gesture data to be helpful in resolving references. In our previous work (2009), we examined gestures for the identification of coreference on multparty meeting data. We found that gestures only provided limited help in the coreference identification task. Given the nature of the meetings under investigation, although gestures have not been shown to be effec"
W10-4357,P07-1045,0,0.0635699,"Missing"
W10-4357,E09-1032,0,0.0324627,"Missing"
W10-4357,2007.sigdial-1.40,0,0.261074,"ng schon gefallen ist. • Singular you: EN: Do you want an extra piece of paper? DE: M¨ochtest du noch ein Blatt Papier? • Plural you: EN: Hope you are all happy! DE: Ich hoffe, ihr seid alle zufrieden! These examples show that correctly identifying different types of You plays an important role in the correct translation of you in different context. To address this issue, this paper investigates the role of hand gestures in disambiguating different usages of you in multiparty meetings. Although identification of you type has been investigated before in the context of addressee identification (Gupta et al., 2007b; Gupta et al., 2007a; Frampton et al., 2009; Purver et al., 2009), our work here focuses on two new angles. First, because of our different application on machine translation, rather than processing you at an utterance level to identify addressee, our work here concerns each occurrence of you within each utterance. Second and more importantly, our work investigates the role of corresponding hand gestures in the disambiguation process. This aspect has not been examined in previous work. When several speakers are conversing in a situated environment, they often overtly gesture at one another t"
W10-4357,P07-2027,0,0.399617,"ng schon gefallen ist. • Singular you: EN: Do you want an extra piece of paper? DE: M¨ochtest du noch ein Blatt Papier? • Plural you: EN: Hope you are all happy! DE: Ich hoffe, ihr seid alle zufrieden! These examples show that correctly identifying different types of You plays an important role in the correct translation of you in different context. To address this issue, this paper investigates the role of hand gestures in disambiguating different usages of you in multiparty meetings. Although identification of you type has been investigated before in the context of addressee identification (Gupta et al., 2007b; Gupta et al., 2007a; Frampton et al., 2009; Purver et al., 2009), our work here focuses on two new angles. First, because of our different application on machine translation, rather than processing you at an utterance level to identify addressee, our work here concerns each occurrence of you within each utterance. Second and more importantly, our work investigates the role of corresponding hand gestures in the disambiguation process. This aspect has not been examined in previous work. When several speakers are conversing in a situated environment, they often overtly gesture at one another t"
W11-0909,P08-1004,0,0.0746925,"acted by the TextRunner system (Banko et al., 2007). The TextRunner system extracts tuples from Internet webpages in an unsupervised fashion. One key difference between TextRunner and other information extraction systems is that TextRunner does not use a closed set of relations (compare to the work described by ACE (2008)). Instead, the relation set is left open, leading to the notion of Open Information Extraction (OIE). Although OIE often has lower precision than traditional information extraction, it is able to extract a wider variety of relations at precision levels that are often useful (Banko and Etzioni, 2008). 3 Using TextRunner to assess joint argument assignments Returning again to Examples 1 and 2, one can query TextRunner in the following way: arg0 : ? Predicate : lose2 arg1 : election In the TextRunner system, arg0 typically indicates the Agent and arg1 typically indicates the Theme. TextRunner provides many tuples in response to this query, two of which are shown below: (5) Usually, [arg0 the president’s party] [Predicate loses] [arg1 seats in the mid-term election]. (6) [arg0 The president] [Predicate lost] [arg1 the election]. The tuples present in these sentences give strong indicators ab"
W11-0909,P10-1160,1,0.928735,"r nominal predicates. The joint model uses propositional knowledge extracted from millions of Internet webpages to help guide prediction. (2) If he cannot get it under control, [p loss] of [arg1 the next election] might result. 1 Introduction Much recent work on semantic role labeling has focused on joint models of arguments. This work is motivated by the fact that one argument can either promote or inhibit the presence of another argument. Because most of this work has been done for verbal SRL, nominal SRL has lagged behind somewhat. In particular, the “implicit” nominal SRL model created by Gerber and Chai (2010) does not address joint argument structures. Implicit arguments are similar to standard SRL arguments, a primary difference being their ability to cross sentence boundaries. In the model created by Gerber and Chai, implicit argument candidates are classified independently and a heuristic post-processing method is applied to derive the final structure. This paper presents a preliminary joint implicit argument model. Consider the following sentences:1 1 Robert Bart Computer Science and Engineering University of Washington Seattle, Washington, USA rbart@cs.washington.edu We will use the notation"
W11-0909,J08-2005,0,0.0902657,"examples demonstrate the importance of inter-argument dependencies. The change from day in Example 3 to meal in Example 4 affects more than just the Temporal label: additionally, the arg1 changes to Beneficiary, even though the underlying text (the children) does not change. To capture this dependency, Toutanova el al. first generate an nbest list of argument labels for a predicate instance. They then re-rank this list using joint features that describe multiple arguments simultaneously. The features help prevent globally invalid argument configurations (e.g., ones with multiple arg0 labels). Punyakanok et al. (2008) formulate a variety of constraints on argument configurations. For example, arguments are not allowed to overlap the predicate, nor are they allowed to overlap each other. The authors treat these constraints as binary variables within an integer linear program, which is optimized to produce the final labeling. Ritter et al. (2010) investigated joint selectional preferences. Traditionally, a selectional preference model provides the strength of association between a predicate-argument position and a specific textual expression. Returning to Examples 1 and 2, one sees that the selectional prefe"
W11-0909,P10-1044,0,0.0679506,"n nbest list of argument labels for a predicate instance. They then re-rank this list using joint features that describe multiple arguments simultaneously. The features help prevent globally invalid argument configurations (e.g., ones with multiple arg0 labels). Punyakanok et al. (2008) formulate a variety of constraints on argument configurations. For example, arguments are not allowed to overlap the predicate, nor are they allowed to overlap each other. The authors treat these constraints as binary variables within an integer linear program, which is optimized to produce the final labeling. Ritter et al. (2010) investigated joint selectional preferences. Traditionally, a selectional preference model provides the strength of association between a predicate-argument position and a specific textual expression. Returning to Examples 1 and 2, one sees that the selectional preference for president and economy in the iarg0 position of loss should be high. Ritter et al. extended this single-argument model using a joint formulation of Latent Dirichlet Allocation (LDA) (Blei et al., 2003). In the generative 64 version of joint LDA, text for the argument positions is generated from a common hidden variable. Th"
W11-0909,W09-2417,0,0.249976,"indicate is-a relationships. Although the link distance from (a) to (b) equals the distance from (c) to (d), the latter are more similar due to their lower depth within the WordNet hierarchy. argument data created by Gerber and Chai (2010). This dataset contains full-text implicit argument annotations for approximately 1,200 predicate instances within the Penn TreeBank. As mentioned in Section 4, all experiments were conducted using predicate instances that take an iarg0 and iarg1 in the ground-truth annotations. We used a tenfold cross-validation setup and the evaluation metrics proposed by Ruppenhofer et al. (2009), which were also used by Gerber and Chai. For each evaluation fold, features were selected using only the corresponding training data and the greedy selection algorithm proposed by Pudil et al. (1994), which starts with an empty feature set and incrementally adds features that provide the highest gains. For comparison with Gerber and Chai’s model, we also evaluated the local prediction model on the evaluation data. Because this model predicted implicit arguments independently, it continued to use the heuristic post-processing algorithm to arrive at the final labeling. However, the prediction"
W11-0909,J08-2002,0,0.0933791,"Missing"
W11-0909,P94-1019,0,0.1897,"ion Intuitively, if synsetcj is connected to a highly ranked synset in A by a short path, then one has evidence that synsetcj answers the original question. The evidence is weaker if the path is long, as in the following example: is-a open primary −−→ direct primary is-a is-a −−→ primary election −−→ election Additionally, a path between more specific synsets (i.e., those lower in the hierarchy) indicates a stronger relationship than a path between more general synsets (i.e., those higher in the hierarchy). These two situations are depicted in Figure 1. The synset similarity metric defined by Wu and Palmer (1994) combines the path length and synset depth intuitions into a single numeric score that is defined as follows: 2 ∗ depth(lca(synset1 , synset2 )) depth(synset1 ) + depth(synset2 ) (12) In Equation 12, lca returns the lowest common ancestor of the two synsets within the WordNet is-a hierarchy. To summarize, Equation 12 indicates the strength of association between synsetcj (e.g., primary election) and a ranked synset synseta from A that answers a question such as “What might a president lose?”. If the association between synsetcj and synseta is small, then the assignment of cj to iarg1 is unlike"
W12-1621,P04-1001,1,0.779215,"h a vector of attributes that represents local features of a single node or the topological features between two nodes. Based on the ARG representations, an inexact graph matching is to find a graph or a subgraph whose error-transformation cost with the already given graph is minimum (Eshera and Fu, 1984). Motivated by the representation power of ARG and the error-correcting capability of inexact graph matching, we developed a graph-based approach to address the referential grounding problem. ARG and probabilistic graph matching have been previously applied in multimodal reference resolution (Chai et al., 2004a; Chai et al., 2004b) by integrating speech and gestures. Here, although we use similar ARG representations, our algorithm is based on inexact graph matching and our focus is on mediating shared perceptual basis. 4.1 Graph Representations Figure 2 illustrates the key elements and the process of our graph-based method. The key elements of our method are two ARG representations, one of which is called the discourse graph and the other called the vision graph. The discourse graph captures the information extracted from the linguistic discourse.2 To create the discourse graph, the linguistic disc"
W12-1621,P06-2051,0,0.0297772,", c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics tion to mediate shared perceptual basis for referential grounding in situated interaction. In the following sections, we first describe an empirical study based on a virtual environment to examine how partners mediate their mismatched visual perceptual basis. We then provide details about our graph matching based approach and its evaluation. 2 Related Work There has been an increasing number of published works on situated language understanding(Scheutz et al., 2007a; Foster et al., 2008; Skubic et al., 2004; Huwel and Wrede, 2006), focusing on interpretation of referents in a shared environment. Different approaches have been developed to resolve visual referents. Gorniak and Roy present an approach that grounds referring expressions to visual objects through semantic decomposition, using context free grammar that connect linguistic structures with underlying visual properties (Gorniak and Roy, 2004a). Recently, they have extended this work by including action-affordances (Gorniak and Roy, 2007). This line of work has mainly focused on grounding words to low-level visual properties. To incorporate situational awareness"
W12-1621,D10-1046,1,0.257837,"Missing"
W13-4010,P05-3001,0,0.371545,"Missing"
W13-4010,J95-3003,0,0.853384,"pplying standard computer vision algorithms and thus may contain different types of processing errors (e.g., mis-segmentation and/or mis-recognition). Using this setup, we have collected a set of dialogues. The following shows an example dialogue segment (collected using the images shown in Figure 1): Related Work In an early work, Mellish (Mellish, 1985) used a constraint satisfaction approach to identify referents that could be only partially specified. This work illustrated the theoretical idea of how to resolve referring expressions based on an internal model of a world. Heeman and Hirst (Heeman and Hirst, 1995) presented a planning-based approach to cast Clark’s collaborative referring idea into a computational model. They used plan construction and plan inference to capture the processes of building referring expressions and identi79 These observations indicate that, the approach to referential grounding in situated dialogue should capture not only binary relations but also group-based relations. Furthermore, it should go beyond traditional approaches that purely rely on semantic constraints from single utterances. It should incorporate the step-by-step collaborative dynamics from the discourse as"
W13-4010,W12-1621,1,0.574473,"iebert and Schlangen, 2008) mainly focused on developing/learning computational models that map words to visual features of objects in the environment. These “visual semantics” of words were then integrated into semantic composition procedures to resolve referring expressions. These previous work has provided valuable insights in computational approaches for reference resolution. However, they mostly dealt with a single expression or a single referent. In this paper, our goal is to resolve complex referring dialogues that involve multiple objects in a shared environment. In our previous work (Liu et al., 2012), we developed a graph-matching based approach to address this problem. However, the previous approach can not handle group-based relations among multiple objects. Furthermore, it did not look into incorporating collaborative behaviors, which is a particularly important characteristic in situated dialogue. This paper aims to address these limitations. collaborative referring to mediate a shared perceptual basis. Motivated by these observations, we have developed an approach that explicitly incorporates collaborative referring into a graph-matching algorithm for referential grounding. As the co"
W13-4010,W08-0113,0,0.375716,"Missing"
W14-4313,P12-1014,0,0.0145432,"arning in a physical blocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level continuous sensorimotor representations of the robot; and on the other hand, support the use of the planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learnFigure 2: System Architecture We developed a dialogue system to support learning new actions. An example setup is shown in Figure 1, in which a SCHUNK arm is used to manipulate blocks placed on a surface. In H1 , the human starts to ask the robot to stack the blue block (i.e., B1 ) on top of the red block (i.e., R1 ). The robot does not understand the action “stack”, so it asks the human for instructions. Then the hu90 Perception Modules: Besides interpreting human language, the robot also continuously perceives the shared environment with its came"
W14-4313,W12-1621,1,0.375644,"senting the acquired knowledge as specific steps as illustrated by the human, the acquired action is represented by the expected final state, which represents the changes of environment as a result of the action. The new action can be directly applied to novel situations by applying planning algorithms. Figure 2 shows the system structure. Next we explain main system modules in detail. Referential Grounding: To make the semantic representation meaningful, it must be grounded to the robot’s representation of perception. We use the graph-based approach for referential grounding as described in (Liu et al., 2012)(Liu et al., 2013). Once the references are grounded, the semantic representation becomes a Grounded Action Frame. For example, as shown in Figure 3, “the blue block” refers to B1 and “the red block on your right” refers to R1. Dialogue Manager: The Dialogue Manager is used to decide what dialog acts the system should perform give a situation. It is composed by: a representation of dialogue state, a space of system activity and a dialogue policy. The dialogue status is computed based on the human intention a dialogue state captures (from semantic representation) and the Grounded Action Frame."
W14-4313,W13-4010,1,0.390297,"ed knowledge as specific steps as illustrated by the human, the acquired action is represented by the expected final state, which represents the changes of environment as a result of the action. The new action can be directly applied to novel situations by applying planning algorithms. Figure 2 shows the system structure. Next we explain main system modules in detail. Referential Grounding: To make the semantic representation meaningful, it must be grounded to the robot’s representation of perception. We use the graph-based approach for referential grounding as described in (Liu et al., 2012)(Liu et al., 2013). Once the references are grounded, the semantic representation becomes a Grounded Action Frame. For example, as shown in Figure 3, “the blue block” refers to B1 and “the red block on your right” refers to R1. Dialogue Manager: The Dialogue Manager is used to decide what dialog acts the system should perform give a situation. It is composed by: a representation of dialogue state, a space of system activity and a dialogue policy. The dialogue status is computed based on the human intention a dialogue state captures (from semantic representation) and the Grounded Action Frame. The current space"
W14-4313,P10-1129,0,\N,Missing
W97-0110,M95-1006,0,0.12868,"Missing"
W97-0110,M92-1002,0,0.115862,"Missing"
W97-0110,A88-1019,0,0.0513983,"ies in the specific rules. The degree of generalization is adjusted to fit the user's needs by use of the statistical Generalization Tree model FinaUy, the optimally generalized rules are applied to scan new information. The results of experiments demonstrate the applicability of our Generalization Tree method. Introduction Research on corpus-based natural language learning and processing is rapidly accelerating following the introduction of large on-line corpora, faster computers, and cheap storage devices. Recent work involves novel ways to employ annotated corpus in part of speech tagging (Church 1988) (Derose 1988) and the application of mutual information statistics on the corpora to uncover lexical information (Church 1989). The goal of the research is the construction of robust and portable natural language processing systems. The wide range of topics available on the Internet calls for an easily adaptable information extraction system for different domains. Adapting an extraction systeem to a new domain is a tedious process. In the traditional customization process, the given corpus must be studied carefully in order to get all the possible ways to express target information. Many rese"
W97-0110,M95-1018,0,0.0150082,"t and portable natural language processing systems. The wide range of topics available on the Internet calls for an easily adaptable information extraction system for different domains. Adapting an extraction systeem to a new domain is a tedious process. In the traditional customization process, the given corpus must be studied carefully in order to get all the possible ways to express target information. Many research groups are implementing the efficient customization of information extraction systems, such as BBN (Weischedel 1995), NYU (Grishman 1995), SRI (Appelt, Hobbs, et al 1995), SRA (Krupka 1995), MITRE (Aberdeen, Burger, et al 1995), and UMass (Fisher, Soderland, et System Overview al 1995). The system cont~i~.~ three major subsystems which, respectively~ address training, rule optlmi~ation, and the scanning of new information. The overall structure of the system is shown in Figure 1. First, each article is partially parsed and segmented into Noun Phrases, Verb Phrases and Prepositional Phrases. An IBM LanguageWare English Dictionary and Computing Term Dictionary, a Partial Parser I, a Tokenizer and a Preprocessor are used in the parsing process. The Tokenizer and the Preprocessor ar"
W97-0110,M95-1014,0,\N,Missing
W97-0110,W89-0240,0,\N,Missing
W97-0110,H89-2012,0,\N,Missing
W97-0110,M92-1024,0,\N,Missing
W97-0809,J87-3003,0,0.0376158,"numerating all the possible ways of expressing the target information. The HASTEN system developed at 2 Lexical Acquisition One way to achieve lexical acquisition is to use the existing repositories of lexical knowledge, such as knowledge base, dictionaries and thesauruses. The key issue is whether those repositories can be effectively applied for the computational purpose. Many researchers have taken steps toward successful extraction of computationally useful lexical information from machine readable dictionaries and convert it into formal representation (Montemagnia and Vanderwende, 1993) (Byrd et al., 1987) (Jensen and Binot, 1987). Sparck Jones&apos;s pioneering re~This work has been supported by a Fellowship from IBM Corporation. 61 search (Jones, 1985), done in early 1960, proposed a lexical representation by synonym list. Very close to that proposal, George Miller and colleagues at Princeton University constructed a large-scale resource for lexical information-WordNet. The most useful feature of WordNet to Natural Language Processing community is the organization of lexical information in terms of word meanings, rather than word forms. It is organized by parts of speech-nouns, verbs, adjectives,"
W97-0809,M95-1011,0,0.0655927,"Missing"
W97-0809,M95-1014,0,0.0232976,"y can apply to a broad class of text instead of only the training articles. Finally, the generalized rules are used to scan large numbers of articles to extract the particular information targeted by the user. This paper concentrates on our design of the generalization mechanism which self modifies to precisely match the user&apos;s specification. 1 Introduction Customizing information extraction systems across different domains has become an important issue in Natural Language Processing. Many research groups are making progress toward efficient customization, such as BBN (Weischedel, 1995), NYU (Grishman, 1995), SRI (Appelt et al., 1995), SRA (Krupka, 1995), MITRE (Aberdeen et al., 1995), UMass (Fisher et al., 1995)...etc. SRI developed a specification language called FASTSPEC that automatically translates regular productions written by the developer into finite state machines (Appelt et al., 1995). FASTSPEC makes the customization easier by avoiding the effort in enumerating all the possible ways of expressing the target information. The HASTEN system developed at 2 Lexical Acquisition One way to achieve lexical acquisition is to use the existing repositories of lexical knowledge, such as knowledge"
W97-0809,M95-1006,0,0.104247,"these rules so that they can apply to a broad class of text instead of only the training articles. Finally, the generalized rules are used to scan large numbers of articles to extract the particular information targeted by the user. This paper concentrates on our design of the generalization mechanism which self modifies to precisely match the user&apos;s specification. 1 Introduction Customizing information extraction systems across different domains has become an important issue in Natural Language Processing. Many research groups are making progress toward efficient customization, such as BBN (Weischedel, 1995), NYU (Grishman, 1995), SRI (Appelt et al., 1995), SRA (Krupka, 1995), MITRE (Aberdeen et al., 1995), UMass (Fisher et al., 1995)...etc. SRI developed a specification language called FASTSPEC that automatically translates regular productions written by the developer into finite state machines (Appelt et al., 1995). FASTSPEC makes the customization easier by avoiding the effort in enumerating all the possible ways of expressing the target information. The HASTEN system developed at 2 Lexical Acquisition One way to achieve lexical acquisition is to use the existing repositories of lexical knowle"
W97-1001,M95-1012,0,0.038152,"Missing"
W97-1001,M95-1011,0,0.0599363,"Missing"
W97-1001,M95-1018,0,0.0670543,"Missing"
W97-1001,M93-1010,0,0.0504791,"Missing"
W97-1001,M95-1006,0,0.144024,"Missing"
W97-1001,M92-1024,0,\N,Missing
W97-1001,H93-1026,0,\N,Missing
W97-1001,M92-1036,0,\N,Missing
