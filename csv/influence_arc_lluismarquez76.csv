2010.eamt-1.15,2007.mtsummit-papers.3,0,0.217612,"s there exist domain adaptation methods to improve results when these data sets are available, we devote most of the work to the first case, but we also check that the method does not hurt the performance in the second case. For this purpose, we complement the standard minimisation methods with an averaged perceptron-based re-estimation of parameters. Perceptrons have been used before with the aim of adding a large amount of new features to statistical systems avoiding the problem of the numerical minimisation of such a large vector of parameters (Liang et al., 2006; Tillmann and Zhang, 2006; Arun and Koehn, 2007). Other algorithms such as MIRA have been used for the same purpose (Arun and Koehn, 2007; Chiang et al., 2008). Here, the philosophy is different. We do not intend to include new information, but to profit better the available data as we will argue in the following. Even using the same data sets, the combination of MERT and the perceptron training can improve more than 2 points of BLEU the result of MERT alone. When including specialised data for development, the difference between MERT and the combined training is not so spectacular, but, still, the perceptron stage attains the leading resul"
2010.eamt-1.15,W05-0909,0,0.0382136,"e original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The last metric, ULC (Gim´enez and Amig´o, 2006), performs a linear combination of a set of 33 lexical metrics, most of them variants of the ones appearing in the table (see Gim´enez (2007) for details). As a general trend, the same conclusions seen with BLEU can be extracted here. For the outof-domain test sets, the addition of the perceptron stage improves for all the metrics but WER the results with respect to MERT alone. For indomain test sets, the answer is not unique and the behaviour depends on the metric, so, there is no a clear effect of the second stage as the similarity of the ULC"
2010.eamt-1.15,W08-0304,0,0.0134784,"optimising the translation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested in finding the optimal parameters on development but on test. In this study, we see how parameters estimated with MERT can generalise quite bad on test sets that depart substantially from the training and development sets. Our goal is to find a more robust − → vector of weights λ that, even without being optimal on development or test when the domain is akin, better generalise on the other cases. We show empirically that this can be don"
2010.eamt-1.15,D08-1024,0,0.0663832,"the work to the first case, but we also check that the method does not hurt the performance in the second case. For this purpose, we complement the standard minimisation methods with an averaged perceptron-based re-estimation of parameters. Perceptrons have been used before with the aim of adding a large amount of new features to statistical systems avoiding the problem of the numerical minimisation of such a large vector of parameters (Liang et al., 2006; Tillmann and Zhang, 2006; Arun and Koehn, 2007). Other algorithms such as MIRA have been used for the same purpose (Arun and Koehn, 2007; Chiang et al., 2008). Here, the philosophy is different. We do not intend to include new information, but to profit better the available data as we will argue in the following. Even using the same data sets, the combination of MERT and the perceptron training can improve more than 2 points of BLEU the result of MERT alone. When including specialised data for development, the difference between MERT and the combined training is not so spectacular, but, still, the perceptron stage attains the leading results. The outline of the paper is as follows. Section 2 introduces the perceptron training, details the algorithm"
2010.eamt-1.15,W02-1001,0,0.0226998,"the choice of a gold standard, two key aspects of using this algorithm for machine translation. Section 3 describes and classifies the data used in the analysis. Afterwards, in Section 4, we detail our experiments. The first one, Crossdomain testing, is devoted to demonstrate how one can enhance his system for an out-of-domain test set by appending a perceptron training. The second experiment focuses on using an out-of-domain development set for tuning the system into the new domain. Finally, we draw our conclusions in Section 5. 2 Perceptron-based training The averaged structured perceptron (Collins, 2002) is an online mistake driven algorithm that determines the weights of a linear feature function by correcting their values according to the distance to the true solution. The score function that quantifies the quality of a translation in SMT, Eq. 1, is a linear function of the hm components. Therefore, the corresponding weights can be learned with the perceptron. Figure 1 details the perceptron algorithm. Given the training data set {f i ,ei }, an initial value for − → the weights λ 0 , the learning rate , and the number of epochs N , the perceptron translates (decodes) every sentence in the"
2010.eamt-1.15,W09-0439,0,0.0161484,"evelopment set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested in finding the optimal parameters on development but on test. In this study, we see how parameters estimated with MERT can generalise quite bad on test sets that depart substantially from the training and development sets. Our goal is to find a more robust − → vector of weights λ that, even without being optimal on development or test when the domain is akin, better generalise on the other cases. We show empirically that this can be done by including ma− → chine learning techiques to es"
2010.eamt-1.15,gimenez-amigo-2006-iqmt,0,0.0283452,"Missing"
2010.eamt-1.15,P07-2045,0,0.010094,"Missing"
2010.eamt-1.15,W04-3250,0,0.051125,"stopping of MERT does not have the same effect. We checked that the almost monotonous increment of BLEU throught MERT iterations on development sometimes translates into erratic BLEU results on test, especially when the domain of the data sets differs. The variance of BLEU scores on test can be large. There are values quite better than the last one but also quite worse, and there is no way to know when to obtain the best one from the training. In order to find out whether the results are statistically significant, we generated 1,000 sets by pair bootstrap resampling of the original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The las"
2010.eamt-1.15,P06-1096,0,0.0583171,"Missing"
2010.eamt-1.15,P04-1077,0,0.0118277,"pair bootstrap resampling of the original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The last metric, ULC (Gim´enez and Amig´o, 2006), performs a linear combination of a set of 33 lexical metrics, most of them variants of the ones appearing in the table (see Gim´enez (2007) for details). As a general trend, the same conclusions seen with BLEU can be extracted here. For the outof-domain test sets, the addition of the perceptron stage improves for all the metrics but WER the results with respect to MERT alone. For indomain test sets, the answer is not unique and the behaviour depends on the metric, so, there is no a clear effect of the sec"
2010.eamt-1.15,C08-1074,0,0.0162189,"nslation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested in finding the optimal parameters on development but on test. In this study, we see how parameters estimated with MERT can generalise quite bad on test sets that depart substantially from the training and development sets. Our goal is to find a more robust − → vector of weights λ that, even without being optimal on development or test when the domain is akin, better generalise on the other cases. We show empirically that this can be done by including ma− → chi"
2010.eamt-1.15,niessen-etal-2000-evaluation,0,0.0340119,"d out whether the results are statistically significant, we generated 1,000 sets by pair bootstrap resampling of the original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The last metric, ULC (Gim´enez and Amig´o, 2006), performs a linear combination of a set of 33 lexical metrics, most of them variants of the ones appearing in the table (see Gim´enez (2007) for details). As a general trend, the same conclusions seen with BLEU can be extracted here. For the outof-domain test sets, the addition of the perceptron stage improves for all the metrics but WER the results with respect to MERT alone. For indomain test sets, the answer is not unique"
2010.eamt-1.15,P02-1038,0,0.0604686,"ptimum value on a development set with the expectation that these optimal weights generalise well to other test sets. However, this is not always the case when domains differ. This work uses a perceptron algorithm to learn more robust weights to be used on out-of-domain corpora without the need for specialised data. For an Arabic-to-English translation system, the generalisation of weights represents an improvement of more than 2 points of BLEU with respect to the MERT baseline using the same information. 1 Introduction In Statistical Machine Translation (SMT) and within the log-linear model (Och and Ney, 2002), the best translation eˆ for a given source sentence f is the most probable one, and the probability is expressed as a weighted sum of different elements: T (f ) = eˆ = argmaxe X λm hm (f, e) . (1) m In the standard most simple form, one considers 8 components being hm (f, e) log-probabilities of: the language model P (e), the generative and discriminative lexical translation probabilities lex(f |e) and lex(e|f ) respectively, the generative and discriminative translation models P (f |e) and P (e|f ), the distortion model Pd (e, f ), and the phrase and word penalties, ph(e) and w(e). c 2010 E"
2010.eamt-1.15,J03-1002,0,0.00375623,"Missing"
2010.eamt-1.15,P03-1021,0,0.0116114,"el P (e), the generative and discriminative lexical translation probabilities lex(f |e) and lex(e|f ) respectively, the generative and discriminative translation models P (f |e) and P (e|f ), the distortion model Pd (e, f ), and the phrase and word penalties, ph(e) and w(e). c 2010 European Association for Machine Translation. ° The λ weights, which account for the relative importance of each feature in the log-linear probabilistic model, are commonly estimated by optimising the translation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this"
2010.eamt-1.15,P02-1040,0,0.0871189,"e and discriminative lexical translation probabilities lex(f |e) and lex(e|f ) respectively, the generative and discriminative translation models P (f |e) and P (e|f ), the distortion model Pd (e, f ), and the phrase and word penalties, ph(e) and w(e). c 2010 European Association for Machine Translation. ° The λ weights, which account for the relative importance of each feature in the log-linear probabilistic model, are commonly estimated by optimising the translation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested"
2010.eamt-1.15,P06-1091,0,0.209153,"s can be easier adapted. As there exist domain adaptation methods to improve results when these data sets are available, we devote most of the work to the first case, but we also check that the method does not hurt the performance in the second case. For this purpose, we complement the standard minimisation methods with an averaged perceptron-based re-estimation of parameters. Perceptrons have been used before with the aim of adding a large amount of new features to statistical systems avoiding the problem of the numerical minimisation of such a large vector of parameters (Liang et al., 2006; Tillmann and Zhang, 2006; Arun and Koehn, 2007). Other algorithms such as MIRA have been used for the same purpose (Arun and Koehn, 2007; Chiang et al., 2008). Here, the philosophy is different. We do not intend to include new information, but to profit better the available data as we will argue in the following. Even using the same data sets, the combination of MERT and the perceptron training can improve more than 2 points of BLEU the result of MERT alone. When including specialised data for development, the difference between MERT and the combined training is not so spectacular, but, still, the perceptron stage at"
2011.mtsummit-papers.63,2010.eamt-1.33,0,0.129673,"p towards hybridization. Although it has been shown to help in improving translation quality, the combination does not represent a real hybridization since systems do not interact among them (see Thurmair 555 (2009) for a classiﬁcation of HMT architectures). In the case of actual interdependences, one of the systems in action leads the translation process and the other ones strengthen it. Much work has been done in building systems where the statistical component is in charge of the translation and the companion system provides complementary information. For instance, Eisele et al. (2008) and Chen and Eisele (2010) introduce lexical information coming from a rule-based translator into an SMT system, in the form of new phrase pairs for the translation table. In both cases results are positive on out-of-domain tests. The opposite direction, that is, where the RBMT system leads the translation and the SMT system provides complementary information, has been less explored. Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally,"
2011.mtsummit-papers.63,W08-0328,0,0.0945801,"s’ outputs, is a ﬁrst step towards hybridization. Although it has been shown to help in improving translation quality, the combination does not represent a real hybridization since systems do not interact among them (see Thurmair 555 (2009) for a classiﬁcation of HMT architectures). In the case of actual interdependences, one of the systems in action leads the translation process and the other ones strengthen it. Much work has been done in building systems where the statistical component is in charge of the translation and the companion system provides complementary information. For instance, Eisele et al. (2008) and Chen and Eisele (2010) introduce lexical information coming from a rule-based translator into an SMT system, in the form of new phrase pairs for the translation table. In both cases results are positive on out-of-domain tests. The opposite direction, that is, where the RBMT system leads the translation and the SMT system provides complementary information, has been less explored. Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their"
2011.mtsummit-papers.63,W10-1708,0,0.424766,"arge of the translation and the companion system provides complementary information. For instance, Eisele et al. (2008) and Chen and Eisele (2010) introduce lexical information coming from a rule-based translator into an SMT system, in the form of new phrase pairs for the translation table. In both cases results are positive on out-of-domain tests. The opposite direction, that is, where the RBMT system leads the translation and the SMT system provides complementary information, has been less explored. Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Similar in spirit to Federmann et al. (2010), translations given by SMatxinT are controlled by the RBMT system in a way that will be clariﬁed in the following sections, but SMatxinT is enriched with a wider variety of SMT translation options. 3 3.1 A Hybrid MT Model Guided by RBMT Individual MT Systems Our hybrid model builds"
2011.mtsummit-papers.63,P07-2045,0,0.00881348,"way that will be clariﬁed in the following sections, but SMatxinT is enriched with a wider variety of SMT translation options. 3 3.1 A Hybrid MT Model Guided by RBMT Individual MT Systems Our hybrid model builds on three individual machine translation systems, a rule-based SpanishBasque system and two variants of regular phrase based statistical MT systems. These three subsystems are described below. SMT basic system (SMTb) The development of the baseline system was carried out using available state-of-the-art tools: GIZA++ toolkit (Och, 2003), SRILM toolkit (Stolcke, 2002) and Moses Decoder (Koehn et al., 2007). More particularly, we used a log-linear combination of several common feature functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3-gram language model Figure 1: General architecture of SMatxinT. The RBMT modules which guide the MT process are the grey boxes with modiﬁed Kneser-Ney smoothing. We also used a lexical reordering model (‘msd-bidirectional-fe’ training option). Parameter optimization was done following the us"
2011.mtsummit-papers.63,W04-3250,0,0.076376,"Missing"
2011.mtsummit-papers.63,P03-1021,0,0.0159102,"ons given by SMatxinT are controlled by the RBMT system in a way that will be clariﬁed in the following sections, but SMatxinT is enriched with a wider variety of SMT translation options. 3 3.1 A Hybrid MT Model Guided by RBMT Individual MT Systems Our hybrid model builds on three individual machine translation systems, a rule-based SpanishBasque system and two variants of regular phrase based statistical MT systems. These three subsystems are described below. SMT basic system (SMTb) The development of the baseline system was carried out using available state-of-the-art tools: GIZA++ toolkit (Och, 2003), SRILM toolkit (Stolcke, 2002) and Moses Decoder (Koehn et al., 2007). More particularly, we used a log-linear combination of several common feature functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3-gram language model Figure 1: General architecture of SMatxinT. The RBMT modules which guide the MT process are the grey boxes with modiﬁed Kneser-Ney smoothing. We also used a lexical reordering model (‘msd-bidirectional-"
2011.mtsummit-papers.63,P02-1040,0,0.0860558,"ieces of the source language corresponding to the tree constituents. The ﬁnal decoding accounts also for ﬂuency by using language models, and can be monotonic (and so, fast) because the structure has been already decided by the RBMT component. As a proof of concept we have instantiated and applied the SMatxinT architecture to a pair of structurally and morphologically distant languages, Spanish and Basque. The results obtained on several benchmark corpora show that the hybrid approach is able to signiﬁcantly improve the out-of-domain results of the best individual SMT system in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. A manual evaluation has been performed on a set of 100 samples from the test set verifying the signiﬁcant advantage of the SMatxinT hybrid system. More detailed analyses reveal that all the components of the hybrid system play an important role in the system (i.e., RBMT structural translation, SMT translation candidates and RBMT original translation). We think that the improvement obtained is remarkable given the simple statistical decoding process implemented so far. Indeed, the upper bound performance for the hybrid method calculated with the current se"
2011.mtsummit-papers.63,2006.amta-papers.25,0,0.0157519,"responding to the tree constituents. The ﬁnal decoding accounts also for ﬂuency by using language models, and can be monotonic (and so, fast) because the structure has been already decided by the RBMT component. As a proof of concept we have instantiated and applied the SMatxinT architecture to a pair of structurally and morphologically distant languages, Spanish and Basque. The results obtained on several benchmark corpora show that the hybrid approach is able to signiﬁcantly improve the out-of-domain results of the best individual SMT system in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. A manual evaluation has been performed on a set of 100 samples from the test set verifying the signiﬁcant advantage of the SMatxinT hybrid system. More detailed analyses reveal that all the components of the hybrid system play an important role in the system (i.e., RBMT structural translation, SMT translation candidates and RBMT original translation). We think that the improvement obtained is remarkable given the simple statistical decoding process implemented so far. Indeed, the upper bound performance for the hybrid method calculated with the current setting reveals that there is st"
2011.mtsummit-papers.63,2009.mtsummit-posters.21,0,0.336101,"Missing"
2011.mtsummit-wpt.7,P07-2045,0,0.0412064,"tistical translations the parts not covered by GF. However, the grammar must be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47"
2011.mtsummit-wpt.7,P03-1021,0,0.0589389,"st be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47 0.29 6.72 0.31 0.24 0.45 0.03 0.52 0.56 0.43 8.21 0.38 0.30 0.56 0.22"
2011.mtsummit-wpt.7,J03-1002,0,0.00441486,"h ambiguities, i.e., multiple translation options, and can complete with statistical translations the parts not covered by GF. However, the grammar must be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST RO"
2011.mtsummit-wpt.7,P02-1040,0,0.0882648,"t the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47 0.29 6.72 0.31 0.24 0.45 0.03 0.52 0.56 0.43 8.21 0.38 0.30 0.56 0.22 0.76 0.68 0.56 9.10 0"
2012.amta-papers.13,P11-2019,0,0.0624166,"e main factors that lead to noise and contradictions in the annotation process (Takagi, 2001; Zaenen, 2006). These aspects are especially evident in fields such as Interactive Evolutionary Computation (IEC), where unknown fitness functions or subject-centered evaluations make high-repetitive, labor-intensive evaluation tasks extremely common. Researchers in these areas are devoting considerable effort towards the definition of novel annotation protocols that can reduce the fatigue of the annotators and result in more accurate and reliable resources (Llor`a et al., 2005b; Formiga et al., 2010; Alm, 2011). In this paper, we select one of these annotation schemes, developed and successfully tested in the context of Active Interactive Genetic Algorithms lluism@lsi.upc.edu (aiGAs) (Llor`a et al., 2005b; Formiga et al., 2010), and employ it to manually annotate a corpus of more than 11,000 machine-translated texts with relative (rankings) and absolute (adequate/non adequate) quality assessments. The method is based on a explicit decomposition of the traditional approach for ranking annotation (i.e., a many-to-many comparison problem) into a set of pairwise ranking decisions, from which a full rank"
2012.amta-papers.13,P11-2027,0,0.0133983,"red partial ordering model for a complex and linguistically dense problem like automatic translation ranking. The annotation of machine-translated texts with quality assessments is typically carried out as part of MT evaluation campaigns (Callison-Burch et al., 2010). In this context, human assessments are typically used to rank the competing systems or to measure the correlation between reference-based metrics (Papineni et al., 2002; Gim´enez and M`arquez, 2010) with human quality assessments. More recently, a renewed interest in confidence and quality estimation for MT (Specia et al., 2009; Banchs and Li, 2011) has triggered the development of ad-hoc corpora to be used for training supervised models of translation quality (Specia et al., 2010). 3 Annotation methodology Our objective is to build a corpus of rankings and absolute quality annotations for alternative translations of the same source sentences. These two layers of annotation are complementary and useful in different ways, and they can be exploited to learn models of quality with different applications, i.e., to select among alternative translations or to discard unsatisfactory outputs. We considered 1,882 real-world translation requests i"
2012.amta-papers.13,2005.mtsummit-papers.11,0,0.0064544,"to learn models of quality with different applications, i.e., to select among alternative translations or to discard unsatisfactory outputs. We considered 1,882 real-world translation requests in English submitted to an online translation service. A professional translator corrected the most obvious typos, slang or chat abbreviations and provided reference translations into Spanish for all of them. We automatically translated the corrected sentences into Spanish with five different systems: one of them is a state-of-the-art phrase-based MT system based on Moses that we trained using Europarl (Koehn, 2005), newswire (Callison-Burch et al., 2010) and UN (Rafalovitch and Dale, 2009) parallel corpora; the remaining four systems are online commercial systems that we queried via their web APIs, namely SDL/LanguageWeaver1 , Google Translate2 , Bing Translator3 and Systran4 . As a quality criterion for the assessments we selected adequacy, i.e., the amount of information that is correctly conveyed by a translation. This choice is motivated by the results of a preliminary annotation in which we compared five different quality criteria (namely: adequacy, fluency, a combination of adequacy and fluency, p"
2012.amta-papers.13,P02-1040,0,0.0905546,") or music applications (Yang and Chen, 2009). In this paper, we set up a large-scale annotation activity with the purpose of demonstrating the practicality of employing the aiGA-inspired partial ordering model for a complex and linguistically dense problem like automatic translation ranking. The annotation of machine-translated texts with quality assessments is typically carried out as part of MT evaluation campaigns (Callison-Burch et al., 2010). In this context, human assessments are typically used to rank the competing systems or to measure the correlation between reference-based metrics (Papineni et al., 2002; Gim´enez and M`arquez, 2010) with human quality assessments. More recently, a renewed interest in confidence and quality estimation for MT (Specia et al., 2009; Banchs and Li, 2011) has triggered the development of ad-hoc corpora to be used for training supervised models of translation quality (Specia et al., 2010). 3 Annotation methodology Our objective is to build a corpus of rankings and absolute quality annotations for alternative translations of the same source sentences. These two layers of annotation are complementary and useful in different ways, and they can be exploited to learn mo"
2012.amta-papers.13,pighin-etal-2012-faust,1,0.833398,"Missing"
2012.amta-papers.13,2009.mtsummit-posters.15,0,0.0116128,"to select among alternative translations or to discard unsatisfactory outputs. We considered 1,882 real-world translation requests in English submitted to an online translation service. A professional translator corrected the most obvious typos, slang or chat abbreviations and provided reference translations into Spanish for all of them. We automatically translated the corrected sentences into Spanish with five different systems: one of them is a state-of-the-art phrase-based MT system based on Moses that we trained using Europarl (Koehn, 2005), newswire (Callison-Burch et al., 2010) and UN (Rafalovitch and Dale, 2009) parallel corpora; the remaining four systems are online commercial systems that we queried via their web APIs, namely SDL/LanguageWeaver1 , Google Translate2 , Bing Translator3 and Systran4 . As a quality criterion for the assessments we selected adequacy, i.e., the amount of information that is correctly conveyed by a translation. This choice is motivated by the results of a preliminary annotation in which we compared five different quality criteria (namely: adequacy, fluency, a combination of adequacy and fluency, post-editing effort and a subjective measure of translation “goodness”) and s"
2012.amta-papers.13,2009.mtsummit-papers.16,0,0.0178317,"loying the aiGA-inspired partial ordering model for a complex and linguistically dense problem like automatic translation ranking. The annotation of machine-translated texts with quality assessments is typically carried out as part of MT evaluation campaigns (Callison-Burch et al., 2010). In this context, human assessments are typically used to rank the competing systems or to measure the correlation between reference-based metrics (Papineni et al., 2002; Gim´enez and M`arquez, 2010) with human quality assessments. More recently, a renewed interest in confidence and quality estimation for MT (Specia et al., 2009; Banchs and Li, 2011) has triggered the development of ad-hoc corpora to be used for training supervised models of translation quality (Specia et al., 2010). 3 Annotation methodology Our objective is to build a corpus of rankings and absolute quality annotations for alternative translations of the same source sentences. These two layers of annotation are complementary and useful in different ways, and they can be exploited to learn models of quality with different applications, i.e., to select among alternative translations or to discard unsatisfactory outputs. We considered 1,882 real-world"
2012.amta-papers.13,specia-etal-2010-dataset,0,0.0141496,"-translated texts with quality assessments is typically carried out as part of MT evaluation campaigns (Callison-Burch et al., 2010). In this context, human assessments are typically used to rank the competing systems or to measure the correlation between reference-based metrics (Papineni et al., 2002; Gim´enez and M`arquez, 2010) with human quality assessments. More recently, a renewed interest in confidence and quality estimation for MT (Specia et al., 2009; Banchs and Li, 2011) has triggered the development of ad-hoc corpora to be used for training supervised models of translation quality (Specia et al., 2010). 3 Annotation methodology Our objective is to build a corpus of rankings and absolute quality annotations for alternative translations of the same source sentences. These two layers of annotation are complementary and useful in different ways, and they can be exploited to learn models of quality with different applications, i.e., to select among alternative translations or to discard unsatisfactory outputs. We considered 1,882 real-world translation requests in English submitted to an online translation service. A professional translator corrected the most obvious typos, slang or chat abbrevi"
2012.amta-papers.13,J06-4012,0,\N,Missing
2012.eamt-1.15,2011.eamt-1.9,0,0.0850637,"Missing"
2012.eamt-1.15,D07-1091,0,0.142983,"Missing"
2012.eamt-1.15,N03-1017,0,0.0074291,"Missing"
2012.eamt-1.15,2010.amta-commercial.12,0,0.0636708,"Missing"
2012.eamt-1.15,P05-1033,0,\N,Missing
2012.eamt-1.61,2010.eamt-1.33,0,0.0341778,"at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu/ mainly used for multil"
2012.eamt-1.61,2007.mtsummit-wpt.4,0,0.085674,"tion 5 summarises the work and outlines possible lines to follow. 2 Related work This work tackles two topics which are lately attracting the attention of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a"
2012.eamt-1.61,W08-0328,0,0.0588025,"OLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu"
2012.eamt-1.61,2011.mtsummit-wpt.7,1,0.890558,"Missing"
2012.eamt-1.61,P02-1040,0,0.0863732,"e using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜na-Bonet et al. (2011a). 3.3 GF system As explained in Section 2, the extension of GF to a new domain implies the construction of a specialised grammar that expands the general resource grammar. Since in our case of applica4 Figure 1: Architecture of the GF translation system. http://www.epo.org/ 271 tion we are far from a close and limited domain, some probabilistic components are also necessary. The general arch"
2012.eamt-1.61,2011.mtsummit-papers.63,1,0.780941,"Missing"
2012.eamt-1.61,W10-1708,0,0.0126031,"s not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu/ mainly used for multilingual natural language applications. Grammars in GF are represented as a pair of an abstract syntax –an interlingua that captures the semantics of"
2012.eamt-1.61,P07-2045,0,0.0204122,"patents. The language of patents follows a formal style adequate to be analysed with a grammar, but at the same time uses a rich and particular vocabulary adequate to be gathered statistically. We focus on the English-French language pair so that the effects of translating into a morphologically rich language can be studied. With respect to the engine, a grammar-based translator is developed to assure grammatically correct translations. We extend GF (Grammatical Framework, Ranta (2011)) and write a new grammar for patent translation. The SMT system that complements the RBMT is based on Moses (Koehn et al., 2007). This system works on two different levels. First, it is used to build the parallel lexicon of the GF translator on the fly. Second, it is the top level decoder that takes the final decision about which phrases should be used. In the following Section 2 describes recent work both in patent translation and hybrid systems. Section 3 explains our hybrid system and Section 4 evaluates its performance. Finally, Section 5 summarises the work and outlines possible lines to follow. 2 Related work This work tackles two topics which are lately attracting the attention of researchers, patent translation"
2012.eamt-1.61,J03-1002,0,0.00340697,"rk-up within the patent such as paragraph tags for example. Two small sets for development and test purposes have also been selected with the same restrictions: 993 fragments for development and 1008 for test. 3.2 In-domain SMT system The first component is a standard state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 3.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜"
2012.eamt-1.61,P03-1021,0,0.0115529,"Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜na-Bonet et al. (2011a). 3.3 GF system As explained in Section 2, the extension of GF to a new domain implies the construction of a specialised grammar that expands the general resource grammar. Since in our case of applica4 Figure 1: Architecture of the GF translation system. http://www.epo.org/ 271 tion we are far from a close and limited domain, some probabilistic compon"
2012.eamt-1.61,W04-2104,0,0.0234676,"Missing"
2012.eamt-1.61,W03-2008,0,0.0281322,"acting the attention of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not"
2012.eamt-1.61,2005.mtsummit-wpt.6,0,0.0495801,"of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this"
2012.eamt-1.61,2009.eamt-1.28,0,0.0457617,"ent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being"
2012.eamt-1.61,2009.mtsummit-posters.21,0,0.0427205,"sted, the technology evolved towards rule-based systems (RBMT). Later in the 90s the everyday more powerful computers allowed to develop empirical translation systems. Recently a type of empirical system, the statistical one (SMT), has become a widely used standard for translation. At this point the two main paradigms, RBMT and SMT, coexist with their strengths and weaknesses. Luckily these strengths and weaknesses are complementary and current efforts are being made to hybridise both of them and develop new technologies. A classification and description of hybrid translation can be found in (Thurmair, 2009). In general RBMT provides high precision, due to an analysis of the text, but has limited coverage c 2012 European Association for Machine Translation. 269 and a considerable amount of effort and linguistic knowledge is required in order to build such a system. On the other hand, SMT can achieve a huge coverage and is good at lexical selection and fluency but has problems in building structurally and grammatically correct translations. Hybrid MT (HMT) is an emerging and challenging area of machine translation, which aims at combining the known techniques into systems that retain the best feat"
2012.eamt-1.61,2011.eamt-1.5,0,\N,Missing
2012.eamt-1.61,N09-2055,0,\N,Missing
2012.freeopmt-1.7,E06-1032,0,0.0360443,"h et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been"
2012.freeopmt-1.7,P08-1007,0,0.0201128,"have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been tested. Due to its simplicity, we decided to use the idea presented by Gim´enez and M`arquez (2008), where a set of simple metrics are combined by means of the arithmetic mean. 66 This work presents a deep evaluation experiment of a hybrid architecture that tries to get the best of both worlds, rule-based and statistical. The results obtained corroborated the known doubts about BLEU. And suggests that the further development of the hybrid system should be guided by a linguistically more informed metric that should be able to capture the s"
2012.freeopmt-1.7,2011.mtsummit-papers.63,1,0.814633,"Missing"
2012.freeopmt-1.7,W10-1708,0,0.0154751,"n applied to corpora different from those used for training (out-of-domain evaluation). Because of these complementary virtues and drawbacks several works are being devoted to build hybrid systems with components of both approaches. A classification and a summary of hybrid architectures can be seen in Thurmair (2009). The case we present here is within the philosophy of those systems where the RBMT system leads the translation and the SMT system provides complementary information. Following this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to i"
2012.freeopmt-1.7,W08-0332,1,0.87217,"Missing"
2012.freeopmt-1.7,P07-2045,0,0.00584996,"cing the sparseness produced by the agglutinative nature of Basque and the small amount of parallel corpora. Adapting the baseline system to work at the morpheme level mainly consists of training the decoder on the segmented text. The SMT system trained on segmented words generates a sequence of morphemes. So, in order to obtain the final Basque text from the segmented output, a word-generation post-process is applied. State-of-the-art tools are used in this case. GIZA++ toolkit (Och, 2003) is used for the alignments, SRILM toolkit (Stolcke, 2002) for the language model and the Moses Decoder (Koehn et al., 2007). We used a log-linear functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3gram language model with modified Kneser-Ney smoothing. We also used a lexical reordering 1 http://www.opentrad.com 67 model (‘msd-bidirectional-fe’ training option). Parameter optimization was done following the usual practice, i.e., Minimum-Error-Rate Training (Och, 2003), however, the metric used for the optimization is not only BLEU, but it dep"
2012.freeopmt-1.7,W06-3114,0,0.0279294,"ctionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been tested. Due to its sim"
2012.freeopmt-1.7,W05-0904,0,0.0204949,"the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been tested. Due to its simplicity, we decided to use the idea presented by Gim´enez and M`arquez (2008), where a set of simple metrics are combined by means of the arithmetic mean. 66 This work presents a deep evaluation experiment of a hybrid architecture that tries to get the best of both worlds, rule-based and statistical. The results obtained corroborated the known doubts about BLEU. And suggests that the further development of the hybrid system should be guided by a linguistically more informed"
2012.freeopmt-1.7,N03-2021,0,0.0335585,"owing this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of"
2012.freeopmt-1.7,P03-1021,0,0.00972803,"system, words are split into several morphemes by using a Basque morphological analyzer/lemmatizer, aiming at reducing the sparseness produced by the agglutinative nature of Basque and the small amount of parallel corpora. Adapting the baseline system to work at the morpheme level mainly consists of training the decoder on the segmented text. The SMT system trained on segmented words generates a sequence of morphemes. So, in order to obtain the final Basque text from the segmented output, a word-generation post-process is applied. State-of-the-art tools are used in this case. GIZA++ toolkit (Och, 2003) is used for the alignments, SRILM toolkit (Stolcke, 2002) for the language model and the Moses Decoder (Koehn et al., 2007). We used a log-linear functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3gram language model with modified Kneser-Ney smoothing. We also used a lexical reordering 1 http://www.opentrad.com 67 model (‘msd-bidirectional-fe’ training option). Parameter optimization was done following the usual practic"
2012.freeopmt-1.7,P02-1040,0,0.0992093,"he RBMT system leads the translation and the SMT system provides complementary information. Following this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic informati"
2012.freeopmt-1.7,W07-0707,0,0.0606548,"Missing"
2012.freeopmt-1.7,2009.mtsummit-posters.21,0,0.0266657,"nslation more locally and have problems with long distance reordering. They also tend to produce very obvious errors, which are annoying for regular users, e.g., lack of gender and number agreement, bad punctuation, etc. Moreover, SMT systems can experience a severe degradation of performance when applied to corpora different from those used for training (out-of-domain evaluation). Because of these complementary virtues and drawbacks several works are being devoted to build hybrid systems with components of both approaches. A classification and a summary of hybrid architectures can be seen in Thurmair (2009). The case we present here is within the philosophy of those systems where the RBMT system leads the translation and the SMT system provides complementary information. Following this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluati"
2013.mtsummit-papers.9,P07-1038,0,0.0258696,"Baseline (Callison-Burch et al., 2012): subset of 17 baseline features from Specia et al. (2010), containing token counts and their ratio, LM probabilities, n-grams filtered by quartiles, punctuation marks and fertility ratios. 2. A SIYA QE based features (Gim´enez and M`arquez, 2010): 26 A SIYA QE features, comprising bilingual dictionary ambiguity and overlap; ratios concerning chunks, named-entities and PoS; source and candidate language model perplexities and inverse perplexities over lexical forms, chunks and PoS and out-of-vocabulary word indicators. 2.2 Pseudo-reference based features. Albrecht and Hwa (2007) introduced the concept of Pseudo-Reference (PR) based features for translation regression estimation, later extended to ranking (Soricut and Echihabi, 2010). Their hypothesis was based on previous findings showing that, in the absence of human-produced references, automatically produced ones were still good in differentiating good and bad translations. These features require one or more secondary MT systems, used to generate translations starting from the same input. It is also crucial to have a goodquality MT system among the candidates, as the pseudo-reference becomes a more solid reference"
2013.mtsummit-papers.9,C12-1008,0,0.230788,"ithout comparison (Specia et al., 2010). In this paper, we will use the term Quality Estimation (QE) to refer to the latter case, that is, predicting the quality of the translated text avoiding the need of human correct translations. QE has recently evolved towards two separate subtasks (Callison-Burch et al., 2012) consisting in scoring itself (Specia et al., 2010) and ranking, where different MT outputs for a given source sentence have to be ranked according to their comparative quality. Results obtained so far on QE have been more satisfactory for the ranking approach (Specia et al., 2010; Avramidis, 2012; Callison-Burch et al., 2012). System ranking based on human quality annotations has been established as a common practice for MT evaluation in shared tasks (Callison-Burch et al., 2012). Therefore, training corpora are available for researchers to train ranking functions with supervised machine learning methods to perform automatic ranking mimicking human annotations. Learned models can be reusable, provided they are system independent and based on a generic analysis (i.e., no system dependent features can be used for training), and applicable to other sets containing any input and multiple"
2013.mtsummit-papers.9,W12-3102,0,0.140924,"Missing"
2013.mtsummit-papers.9,W11-2107,0,0.311773,"useful in a system combination scenario, obtaining better results than any individual translation system. 1 Introduction Automatic evaluation of machine translation (MT) quality is a crucial task for system development, combination and tuning, which has received increasing attention from the MT community in the recent years. Translation quality estimation has classically been addressed as a scoring task (Specia et al., 2010), where some scoring function predicts the absolute quality of the automatic translation of a source text compared to human references (Papineni et al., 2002; NIST, 2002; Denkowski and Lavie, 2011) or without comparison (Specia et al., 2010). In this paper, we will use the term Quality Estimation (QE) to refer to the latter case, that is, predicting the quality of the translated text avoiding the need of human correct translations. QE has recently evolved towards two separate subtasks (Callison-Burch et al., 2012) consisting in scoring itself (Specia et al., 2010) and ranking, where different MT outputs for a given source sentence have to be ranked according to their comparative quality. Results obtained so far on QE have been more satisfactory for the ranking approach (Specia et al., 2"
2013.mtsummit-papers.9,P02-1040,0,0.0979224,"d standard rankings and prove to be useful in a system combination scenario, obtaining better results than any individual translation system. 1 Introduction Automatic evaluation of machine translation (MT) quality is a crucial task for system development, combination and tuning, which has received increasing attention from the MT community in the recent years. Translation quality estimation has classically been addressed as a scoring task (Specia et al., 2010), where some scoring function predicts the absolute quality of the automatic translation of a source text compared to human references (Papineni et al., 2002; NIST, 2002; Denkowski and Lavie, 2011) or without comparison (Specia et al., 2010). In this paper, we will use the term Quality Estimation (QE) to refer to the latter case, that is, predicting the quality of the translated text avoiding the need of human correct translations. QE has recently evolved towards two separate subtasks (Callison-Burch et al., 2012) consisting in scoring itself (Specia et al., 2010) and ranking, where different MT outputs for a given source sentence have to be ranked according to their comparative quality. Results obtained so far on QE have been more satisfactory fo"
2013.mtsummit-papers.9,2012.amta-papers.13,1,0.424781,"Missing"
2013.mtsummit-papers.9,P10-1063,0,0.613992,"(Denkowski and Lavie, 2011) scores computed at sentence level on the training set and learned from them as the gold standard. Concerning human rankings, we computed the average position of the QE predicted translations within the real test human rankings. More details are given in Section 3. We used a large set of features to characterize examples and perform learning. They are grouped in three different sets and described in the following subsections. Some of them are inspired by well-established features from the literature (baseline, Section 2.1), others apply the pseudoreference idea from Soricut and Echihabi (2010) 70 to a larger set of MT evaluation measures (Section 2.2) and, finally, others are language model–based features developed for the particular corpora of application (Section 2.3). 2.1 Baseline Features Specia et al. (2010) defined a broad set of features covering important aspects for QE learning. Later, Callison-Burch et al. (2012) selected a subset of 17 features for the WMT shared task on QE. Furthermore, other evaluation suites exist which define several QE basic metrics. An example of those is the A SIYA toolkit (Gim´enez and M`arquez, 2010). In our work, we will take the union of both"
2013.mtsummit-posters.10,W13-2715,1,0.877354,"Missing"
2013.mtsummit-posters.10,2011.mtsummit-wpt.7,1,0.89823,"Missing"
2013.mtsummit-posters.10,2010.amta-commercial.14,0,0.114865,"Missing"
2015.eamt-1.9,D13-1176,0,\N,Missing
2015.eamt-1.9,W14-4015,1,\N,Missing
2015.eamt-1.9,P02-1040,0,\N,Missing
2015.eamt-1.9,P12-3024,1,\N,Missing
2015.eamt-1.9,P07-2045,0,\N,Missing
2015.eamt-1.9,D14-1003,0,\N,Missing
2015.eamt-1.9,D11-1084,0,\N,Missing
2015.eamt-1.9,N13-1090,0,\N,Missing
2015.eamt-1.9,P13-4033,0,\N,Missing
2015.eamt-1.9,J03-1002,0,\N,Missing
2015.eamt-1.9,2010.iwslt-papers.10,0,\N,Missing
2015.eamt-1.9,P13-4014,0,\N,Missing
2015.eamt-1.9,W04-3250,0,\N,Missing
2015.eamt-1.9,Y14-1004,0,\N,Missing
2015.eamt-1.9,D12-1108,0,\N,Missing
2015.eamt-1.9,tiedemann-2012-parallel,0,\N,Missing
2015.eamt-1.9,C14-1017,0,\N,Missing
2015.eamt-1.9,P03-1021,0,\N,Missing
2015.eamt-1.9,P14-1129,0,\N,Missing
C02-1112,S01-1028,1,0.901898,"follows a two-step process: 1. Choosing the representation as a set of features for the context of occurrence of the target word senses. 2. Applying a Machine Learning (ML) algorithm to train on the extracted features and tag the target word in the test examples. Current WSD systems attain high performances for coarse word sense differences (two or three senses) if enough training material is available. In contrast, the performance for finer-grained sense differences (e.g. WordNet senses as used in Senseval 2 (Preiss & Yarowsky, 2001)) is far from application needs. Nevertheless, recent work (Agirre and Martinez, 2001a) shows that it is possible to exploit the precision-coverage trade-off and build a high precision WSD system Lluís Màrquez TALP Research Center Polytechnical University of Catalonia Barcelona, Spain lluism@lsi.upc.es that tags a limited number of target words with a predefined precision. This paper explores the contribution of a broad set of syntactically motivated features that ranges from the presence of complements and adjuncts, and the detection of subcategorization frames, up to grammatical relations instantiated with specific words. The performance of the syntactic features is measured"
C02-1112,A00-1031,0,0.0122473,"al features correspond to open-class lemmas that appear in windows of different sizes around the target word. In this experiment, we used two different window-sizes: 4 lemmas around the target (coded as win_lem_4w), and the lemmas in the sentence plus the 2 previous and 2 following sentences (win_lem_2s). Local features include bigrams and trigrams (coded as big_, trig_ respectively) that contain the target word. An index (+1, -1, 0) is used to indicate the position of the target in the bigram or trigram, which can be formed by part of speech, lemmas or word forms (wf, lem, pos). We used TnT (Brants, 2000) for PoS tagging. For instance, we could extract the following features for the target word known from the sample sentence below: word form “whole” occurring in a 2 sentence window (win_wf_2s), the bigram “known widely” where target is the last word (big_wf_+1) and the trigram “RB RB N” formed by the two PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements:"
C02-1112,W01-1808,0,0.0287421,"target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First, we distinguish between direct"
C02-1112,P96-1025,0,0.0275141,"the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic information, in the Japanese tasks, several groups relied on dependency trees to extract features that were used by different models (SVM, Bayes, or vector space models). For the English tasks, the team from the University"
C02-1112,J94-4003,0,0.0603174,"Missing"
C02-1112,P93-1016,0,0.0270377,"PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First,"
C02-1112,P96-1006,0,0.149392,"Missing"
C02-1112,W98-0701,0,0.126818,"-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic informatio"
C02-1112,P94-1013,0,0.288745,"n WSD system based on the precision-coverage trade-off is also investigated. The paper is structured as follows. Section 2 reviews the features previously used in the literature. Section 3 defines a basic feature set based on the preceding review. Section 4 presents the syntactic features as defined in our work, alongside the parser used. In section 5 the two ML algorithms are presented, as well as the strategies for the precision-coverage trade-off. Section 6 shows the experimental setting and the results. Finally section 7 draws the conclusions and summarizes further work. 2. Previous work. Yarowsky (1994) defined a basic set of features that has been widely used (with some variations) by other WSD systems. It consisted on words appearing in a window of ±k positions around the target and bigrams and trigrams constructed with the target word. He used words, lemmas, coarse part-of-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a"
C02-1112,C02-1013,0,\N,Missing
C14-1020,P02-1034,0,0.423722,"itional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using"
C14-1020,P06-2034,0,0.0263295,"derable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a"
C14-1020,J02-3001,0,0.163552,"cognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of conc"
C14-1020,D12-1083,1,0.768831,"low trees. A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example, the first clause “along my route tell me the next steak house” is elaborated by the second clause “that is within a mile”. The relations by which clauses in a text are linked are called coherence relations (e.g., Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide additional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson, 1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is. 3.3 New features In order to compare to the structured representation, we also devoted significant effort towards engineering a set of features to be used in a flat feature-vector representation; they can be used in isolation or in combination with the kernel-based a"
C14-1020,P06-1115,0,0.033166,"eranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a local model, where the hyp"
C14-1020,H05-1064,0,0.0355336,"ated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly bas"
C14-1020,P05-1024,0,0.0308755,"o be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures f"
C14-1020,J08-2001,1,0.787599,"Missing"
C14-1020,H94-1053,0,0.881717,"{price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be fo"
C14-1020,W06-2909,1,0.869136,"of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-gr"
C14-1020,H91-1020,0,0.794759,"Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 193 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 193–202, Dublin, Ireland, August 23-29 2014. Finally, a database query is formed from the list of labels and values, and is then executed against the database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed. {$and [{cuisine:&quot;lebanese&quot;}, {city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied"
C14-1020,H89-1026,0,0.078377,"city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of"
C16-2001,P15-2113,1,0.887768,"Missing"
C16-2001,S16-1138,1,0.853194,"Missing"
C16-2001,D15-1068,1,0.89046,"Missing"
C16-2001,P03-1054,0,0.0264421,"larity value using a similarity matrix. The similarity and the embeddings along with other additional similarity features are then passed through a hidden layer and next to the output layer for classification. The qe and ce are learned by backpropagating the (cross entropy) errors from the output layer. qe and ce vectors are finally concatenated and used as features in our SVM model. Tree kernels We use tree kernels to measure the syntactic similarity between the question and the comment. First, we produce shallow syntactic trees for the question and for the comment using the Stanford parser (Klein and Manning, 2003). Following Severyn and Moschitti (2012), we link the two trees by connecting nodes such as NP, PP, VP, when there is at least one lexical overlap between the corresponding phrases of the trees, and we mark those links using a specific tag. The kernel function K is defined as: K((t1 , t2 ), (c1 , c2 )) = T K(t1 , c1 )+T K(t2 , c2 ), where T K(t, c) is a tree kernel function operating over a pair of question (t) and comment (c) trees.3 Classification Performance We evaluated our comment classifier on the SemEval-2016 Task 3 test set with the official scorer, obtaining the following results: MAP"
C16-2001,S15-2047,1,0.903863,"Missing"
C16-2001,S15-2036,1,0.910076,"Missing"
C98-2159,P97-1032,0,0.0306717,"uracy, since correct answers are computed as wrong and vice-versa. In following sections we will show how this uncertainty in the evaluation may be, in some cases, larger than the reported improvements from one system to another, so invalidating the conclusions of the comparison. 3 Model since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to s t u d y the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant- absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. W h a t we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can find when evaluating the performance of a certain tagger are presented in table 1. OK/~OK stand for a right/wrong tag (respect to the absolute correct disambiguation). When both the tagger and the test corpus have the correct tag, the tag is correctly evaluated a.s right. When the te"
C98-2159,W96-0208,0,\N,Missing
C98-2159,P97-1010,0,\N,Missing
C98-2159,P97-1031,1,\N,Missing
D14-1027,W14-3352,1,0.726645,"Missing"
D14-1027,W07-0718,0,0.384691,"Missing"
D14-1027,W11-2103,0,0.0593209,"ourse parser can be downloaded from http://alt.qcri.org/tools/ 216 In particular, let r and r0 be the references for the pairs ht1 , t2 i and ht01 , t02 i, we can redefine all the members of Eq. 1, e.g., K(t1 , t01 ) becomes K(ht1 , ri, ht01 , r0 i) = PTK(φM (t1 , r), φM (t01 , r0 )) + PTK(φM (r, t1 ), φM (r0 , t01 )), In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (τ ), which was official at WMT12. Table 1 presents the τ scores for all metric variants introduced in this paper: for the individual language pairs"
D14-1027,W05-0904,0,0.114217,"ndation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans hav"
D14-1027,W12-3102,0,0.167189,"Missing"
D14-1027,W12-3129,0,0.108627,".org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translatio"
D14-1027,W10-1750,1,0.877508,"Missing"
D14-1027,P07-1098,1,0.765065,"ework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are"
D14-1027,W08-0331,0,0.225781,"y, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the fe"
D14-1027,W07-0738,1,0.935253,"Missing"
D14-1027,P14-1065,1,0.878614,"Missing"
D14-1027,P02-1040,0,0.0928706,"ranslations Francisco Guzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges"
D14-1027,D12-1083,1,0.785191,"relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feat"
D14-1027,W07-0707,0,0.511133,"Missing"
D14-1027,P13-1048,1,0.81161,"L "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feature space. Consideri"
D14-1027,C14-1020,1,0.886934,"Missing"
D14-1027,D14-1050,1,0.886979,"Missing"
D14-1027,W13-3509,1,0.930152,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,P13-2125,1,0.924292,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,2006.amta-papers.25,0,0.625255,"uzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some ev"
D14-1027,W11-2113,0,0.0478224,"re widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically."
D14-1027,N03-1033,0,0.00879476,"o-REL .-REL &apos;&apos;-REL to think . "" to think . "" relation propagation direction DIS:ELABORATION Bag-of-words relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0"
D14-1027,D12-1097,0,0.0991997,"elopment in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy"
D14-1027,P06-1051,1,\N,Missing
D14-1049,W05-0620,1,0.767449,"Missing"
D14-1049,J08-2001,1,0.858282,"Missing"
D14-1049,P04-1043,0,0.0488302,"ling Xavier Llu´ıs TALP Research Center Universitat Polit`ecnica de Catalunya Xavier Carreras Xerox Research Centre Europe xavier.carreras@xrce.xerox.com lmarquez@qf.org.qa xlluis@cs.upc.edu Abstract In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the semantic role. The main contribution of this paper is a formulation of SRL parsing in terms of efficient shortest-path inference, under the assumption that the SRL model is restricted to arc-factored features of the syntactic path linking the argument with the predicate. We introduce a Semantic Role Labeling (SRL) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments. Our ma"
D14-1049,J02-3001,0,0.348468,"Missing"
D14-1049,P02-1031,0,0.0870273,"Missing"
D14-1049,W04-3212,0,0.122752,"Missing"
D14-1049,W09-1209,0,0.0260059,"Missing"
D14-1049,W08-2122,0,0.0707503,"Missing"
D14-1049,W08-2123,0,0.090991,"Missing"
D14-1049,D09-1059,0,0.0880912,"s TALP Research Center Universitat Polit`ecnica de Catalunya Xavier Carreras Xerox Research Centre Europe xavier.carreras@xrce.xerox.com lmarquez@qf.org.qa xlluis@cs.upc.edu Abstract In this paper we take a different approach. In our scenario SRL is the end goal, and we assume that syntactic parsing is only an intermediate step to extract features to support SRL predictions. In this setting we define a model that, given a predicate, identifies each of the semantic roles together with the syntactic path that links the predicate with the argument. Thus, following previous work (Moschitti, 2004; Johansson, 2009), we take the syntactic path as the main source of syntactic features, but instead of just conditioning on it, we predict it together with the semantic role. The main contribution of this paper is a formulation of SRL parsing in terms of efficient shortest-path inference, under the assumption that the SRL model is restricted to arc-factored features of the syntactic path linking the argument with the predicate. We introduce a Semantic Role Labeling (SRL) parser that finds semantic roles for a predicate together with the syntactic paths linking predicates and arguments. Our main contribution is"
D14-1049,D07-1015,1,0.87509,"Missing"
D14-1049,Q13-1018,1,0.806529,"Missing"
D14-1049,W02-1001,0,\N,Missing
D14-1049,W09-1201,1,\N,Missing
D14-1050,N09-1003,0,0.0327503,"Missing"
D14-1050,J92-4003,0,0.375881,"reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic similarity, while also combining them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the rerankin"
D14-1050,S13-2060,0,0.012357,"a baseline, we picked the best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100."
D14-1050,W10-2802,0,0.024579,"best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100. In our experiments, we will"
D14-1050,H94-1053,0,0.260291,"e list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with respect to the gold standard"
D14-1050,D11-1096,1,0.95073,"SKS) (b) SKS with Brown Clusters Figure 1: CSL structures: standard and with Brown Clusters. Another relevant line of research are the semantic kernels, i.e., kernels that use lexical similarity between features. One of the first that applyed LSA was (Cristianini et al., 2002), whereas (Bloehdorn et al., 2006; Basili et al., 2006) used WordNet. Semantic structural kernels of the type we use in this paper were first introduced in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b). The most advanced model based on tree kernels, which we also use in this paper, is the Smoothed PTK (Croce et al., 2011). We further apply a semantic kernel (SK), namely the Smoothed Partial Tree Kernel (Croce et al., 2011), which uses the lexical similarity between the tree nodes, while computing the substructure space. This is the first time that SKs are applied to reranking hypotheses. This (i) makes the global sentence structure along with concepts available to the learning algorithm, and (ii) enables computing the similarity between lexicals in syntactic patterns that are enriched by concepts. We tested our models on the Restaurant domain. Our results show that: (i) The basic CRF parser, which uses semi-Ma"
D14-1050,W06-2909,1,0.683466,"d Brown clusters, and another one using semantic similarity among the words composing the structure. The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers. 1 {$and [{cuisine:&quot;lebanese&quot;},{city:&quot;doha&quot;}, {price:&quot;low&quot;},{amenity:&quot;carry out&quot;}]} The state-of-the-art of CSL is represented by conditional models for sequence labeling such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) trained with simple morphological and lexical features. The basic CRF model was improved by means of reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic"
D14-1050,C10-5001,1,0.846723,"ng them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word se"
D14-1050,P12-4002,1,0.857346,"vative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word sequences, often re"
D14-1050,H89-1026,0,0.0214012,"hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with resp"
D14-1050,P10-1023,0,0.0382136,"Missing"
D14-1050,J07-2002,0,0.0577582,"Missing"
D14-1050,N03-1033,0,0.00778762,"the hypothesis. 3.3 Semantic structures Tree kernels allow us to compute structural similarities between two trees; thus, we engineered a special structure for the CSL task. In order to capture the structural dependencies between the semantic tags,1 we use a basic tree (see for example Figure 1a), where the words of a sentence are tagged with their semantic tags. 4 Experiments The experiments aim at investigating the role of feature vectors, PTK, SK and BCs in reranking. We first describe the experimental setting and then we move into the analysis of the results. 2 We use the Stanford tagger (Toutanova et al., 2003). For instance, if the output sequence is Other-RatingOther-Amenity the 3-gram patterns would be: S-OtherRating, Other-Rating-Other, Rating-Other-Amenity, and Other-Amenity-E. 3 1 They are associated with the following IDs: 0-Other, 1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6Hours, 7-Location, and 8-Price. 438 semi-CRF Reranker Train 6,922 7,000 Devel. 739 3,695 Test 1,521 7,605 Total 9,182 39,782 N F1 2 87.76 5 92.63 10 95.23 100 98.72 Table 2: Oracle F1 score for N -best lists. Table 1: Number of instances and pairs used to train the semi-CRF and rerankers, respectively. 4.1 1 83"
D14-1050,N04-3012,0,0.201946,"Missing"
D14-1050,H91-1020,0,0.482894,"probability to be globally correct as estimated using local classifiers or global classifiers that only use local features. Then, a reranker, typically a meta-classifier, tries to select the best hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whethe"
D14-1050,C14-1020,1,0.793271,"Missing"
D14-1050,C10-5000,0,\N,Missing
D15-1068,S15-2047,1,0.877035,"Missing"
D15-1068,S12-1059,0,0.0177823,"Missing"
D15-1068,S15-2036,1,0.541482,"Missing"
D15-1068,P15-2113,1,0.19975,"Missing"
D15-1068,P04-1035,0,0.00602109,"og sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The mixing parameter λ ∈ [0, 1] determines the relative strength of the two components. Our approach is inspired by Pang and Lee (2004), where they model the proximity relation between sentences for finding subjective sentences in product reviews, whereas we are interested in global inference based on local classifiers. The optimization problem can be efficiently solved by finding a minimum cut of a weighted undirected graph G = (V, E). The set of nodes V = {v1 , v2 , · · · , vn , s, t} represent the n comments in a thread, the source and the sink. We connect each comment node vi to the source node s by adding an edge w(vi , s) with capacity siG , and to the sink node t by adding an edge w(vi , t) with capacity siB . Finally,"
D15-1068,I11-1164,0,0.0214203,"ssification in Community Question Answering ˜ Giovanni Da San Martino, Simone Filice, Shafiq Joty, Alberto Barr´on-Cedeno, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov, Qatar Computing Research Institute, HBKU {sjoty,albarron,gmartino,sfilice, lmarquez,amoschitti,pnakov}@qf.org.qa Abstract As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming. This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant. One early work going in this direction is that of Qu and Liu (2011), who tried to determine whether a question is “solved” or not, given its associated thread of comments. As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local feat"
D15-1068,W04-2401,0,0.398636,"he CQA-QL dataset: after merging Bad and Potential into Bad. The Task 1 Train 2,600 16,541 8,069 8,472 3 http://www.qatarliving.com/moving-qatar/posts/can-iobtain-driving-license-my-qid-written-employee http://www.qatarliving.com/forum http://alt.qcri.org/semeval2015/task3/ 574 3.1 ci and cj have the same label; assigning 0 to xijS means that ci and cj do not have the same label. The same interpretation holds for the other possible classes (in this case only Different).4 Let ciG be the cost of classifying ci as Good, cijS be the cost of assigning the same labels to ci and cj , etc. Following (Roth and Yih, 2004), these costs are obtained from local classifiers by taking log probabilities, i.e., ciG = − log siG , cijS = − log sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The"
D15-1068,S15-2035,0,0.084801,"ment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in"
D15-1068,W03-0402,0,0.128215,"Missing"
D15-1068,P15-2117,0,0.0999102,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,W01-0515,0,0.0287716,"he comment-pair variables are consistent: xijD = xiG ⊕ xjG , ∀i, j 1 ≤ i < j ≤ n. λ ∈ [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); Integer Linear Programming Approach Here we follow the inference with clas"
D15-1068,S15-2037,0,0.0777175,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,D07-1002,0,\N,Missing
D15-1068,N10-1145,0,\N,Missing
D15-1068,P07-1098,1,\N,Missing
D15-1068,C10-1131,0,\N,Missing
D15-1068,N13-1106,0,\N,Missing
D15-1068,S15-2038,0,\N,Missing
D15-1068,P08-1082,0,\N,Missing
D15-1068,W13-3509,1,\N,Missing
D15-1068,D13-1044,1,\N,Missing
D16-1165,S16-1138,0,0.0817602,"Missing"
D16-1165,J93-2003,0,0.046238,"ceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, which takes as input the original question and two comments together with thei"
D16-1165,P15-2114,0,0.286591,"h q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subtask B, respectively. In this paper, we mainly"
D16-1165,P08-1019,0,0.141574,"mple, q and q 0 are indeed related, and c is a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presente"
D16-1165,P03-1003,0,0.125068,"input components. It does so in a modular kernel function, including stacking from independent subtask A and B classifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of s"
D16-1165,S16-1172,0,0.37884,"provide labeled examples for the so called “subtask A” (q 0 c; appropriateness) and “subtask B” (qq 0 ; relatedness), one could use this supervision to help train the neural network for the primary cQA task. We observed that relatedness has proven quite informative. However, the improvements observed from using appropriateness were more modest. 10 As measured by the relative drop in MAP performance. System MAP AvgRec MRR System MAP AvgRec MRR Full Network Full + appr. preds. 54.51 55.82 60.93 61.63 62.94 62.39 Full Network + subtask A preds. * 1st (Mihaylova et al., 2016) Full Network * 2nd (Filice et al., 2016) * 3rd (Mihaylov and Nakov, 2016b) ... SemEval Average ... SemEval Worst 55.82 55.41 54.51 52.95 51.68 ... 49.30 ... 43.20 Baseline 2 (IR+chron.) 40.36 45.97 45.83 Table 3: Using appropriateness predictions. We present here a stacked experiment in which an additional neural network trained to predict appropriateness is used to inform the full network model. More concretely, we train a feed-forward pairwise neural network for subtask A, which is a simplification of the architecture from Figure 2. The input is reduced to three elements (q 0 , c1 , c2 ), where q 0 is the thread question and c1 an"
D16-1165,P15-1078,1,0.879885,"Missing"
D16-1165,P16-2075,1,0.638784,"Missing"
D16-1165,S16-1137,1,0.835795,"Missing"
D16-1165,S16-1131,0,0.0212185,"od vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, wh"
D16-1165,P11-1143,0,0.142772,"indeed related, and c is a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subta"
D16-1165,P16-2065,1,0.915419,"t the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way. This search can be h"
D16-1165,S16-1136,1,0.898572,"t the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way. This search can be h"
D16-1165,K15-1032,1,0.799997,"types are relevant, but the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different"
D16-1165,S16-1129,1,0.836098,"Missing"
D16-1165,N13-1090,0,0.0460853,"ion, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply averaging over the embeddings of all words in the text. Features We experiment with three kinds of features: (i) lexical features that measure similarity at a word, word n-gram, and paraphrase level, (ii) distributed representations that measure similarity at a syntactic and semantic level, (iii) domain-specific knowledge features, which capture similarity using thread-level information and other features that have proven valuable to solve similar tasks (Nicosia et al., 2015). 4.1 Lexical similarity features These types of features mea"
D16-1165,S16-1128,0,0.0167536,"of AvgRec and MRR. Note that, even without the Subtask A predictions, our pairwise neural network still produces results that are on par with the state of the art (with improvements slightly over one point in both cases). 6 Related Work Recently, a variety of neural network models have been applied to community question answering tasks such as question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2015) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Feng et al., 2015; Tan et al., 2015; Filice et al., 2016; Barr´on-Cede˜no et al., 2016; Mohtarami et al., 2016). Most of these papers concentrate on constructing advanced neural network architectures in order to model the problem at hand better. For instance, dos Santos et al. (2015) propose a neural network approach combining a convolutional neural network and a bag-of-words representation for modeling question-question similarity. Similarly, Tan et al. (2015) adopt a neural attention mechanism over bidirectional long short-term memory (LSTM) neural network to generate better answer representations given the questions. Similarly, Lei et al. (2015) use a combination of recurrent and convolutional neura"
D16-1165,S15-2047,1,0.902818,"Missing"
D16-1165,S15-2036,1,0.912003,"Missing"
D16-1165,P02-1040,0,0.104411,"ivation at the output is f (q, q10 , c1 , q20 , c2 ) = sig(wvT [φ(q, q10 , c1 , q20 , c2 ), ψ(q, q10 ), ψ(q, q20 ), ψ(q10 , c1 ), ψ(q20 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). We use these feature vectors to encode machine translation evaluation measures, components thereof, cQA task-specific features, etc. The next section gives more detail about these features. MT FEATS We use (as pairwise features) the following six machine translation evaluation features: (i) B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply"
D16-1165,P07-1059,0,0.107973,"stacking from independent subtask A and B classifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, a"
D16-1165,2006.amta-papers.25,0,0.0717448,"ponents thereof, cQA task-specific features, etc. The next section gives more detail about these features. MT FEATS We use (as pairwise features) the following six machine translation evaluation features: (i) B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply averaging over the embeddings of all words in the text. Features We experiment with three kinds of features: (i) lexical features that measure similarity at a word, word n-gram, and paraphrase level, (ii) distributed representations that measure similari"
D16-1165,P13-1045,0,0.0380397,"of the reference, length ratio between them, and BLEU’s brevity penalty. Again, these are computed over the same six pairs of vectors as before. 4.2 Distributed representations We use the following vector-based embeddings of all input components: q, c1 , c2 , q10 , and q20 . QL VEC We train in-domain word embeddings using WORD 2 VEC on all available QatarLiving data. Again, we use these embeddings to compute 100dimensional vector representations for all input components by averaging over all words in the texts. S YNTAX VEC We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25dimensional vector that is produced internally as a by-product of parsing. Moreover, we use the above vectors to calculate pairwise similarity features, i.e., the cosine between the following six vector pairs: (q, c1 ), (q, c2 ), (q10 , c1 ), (q20 , c2 ), (q, q10 ) and (q, q20 ). 4.3 Domain-specific features We extract various domain-specific features that use thread-level and other useful information known to capture relatedness and appropriateness. S AME AUTHOR We have a thread-level metafeature, which we apply to the pairs (q10 , c1 ), (q20 , c2 ). It checks whether"
D16-1165,J11-2003,0,0.0785057,"sifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise f"
D16-1165,S15-2038,0,0.0219871,"SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural"
D16-1165,P15-2116,0,0.0859259,"Missing"
D16-1165,S16-1132,0,0.025907,"ilice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, which takes as input th"
D16-1165,P15-1025,0,0.0858496,"s a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subtask B, respectively."
D18-1452,S16-1130,1,0.8612,"Missing"
D18-1452,P15-2113,1,0.615975,"al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddings, and we defer the joint learning with global inference to the structured model. From the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a"
D18-1452,E17-2115,0,0.0635121,"in answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features. In work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model. Moreover, they focus on one task only, while we use multitask learning. Bonadiman et al. (2017) proposed a multitask neural architecture where the three tasks are trained together with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do. The general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic"
D18-1452,I17-2075,0,0.03107,"een subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate ta"
D18-1452,S16-1172,0,0.0498933,"erent cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the 4197 datasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SU PER TEAM (Mihaylova et al., 2016) and K ELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features. In work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model."
D18-1452,P15-1078,1,0.900381,"Missing"
D18-1452,P16-2075,1,0.897508,"Missing"
D18-1452,S16-1137,1,0.897878,"Missing"
D18-1452,D15-1068,1,0.814627,"Missing"
D18-1452,P16-1165,1,0.636808,"al inference over arbitrary graph structures accounting for the dependencies between subtasks to provide globally good solutions. The experimental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results. In future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction. Acknowledgments The first aut"
D18-1452,N16-1084,1,0.905394,"Missing"
D18-1452,K17-1024,1,0.89338,"Missing"
D18-1452,C18-1181,0,0.0548546,"tion and Motivation Question answering web forums such as StackOverflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question–comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often results in noisy and redundant content. Within community Question Answering (cQA) forums, two subtasks are of special relevance when a user poses a new question to the website (Hoogeveen et al., 2018; Lai et al., 2018): (i) finding similar questions (question-question relatedness), and (ii) finding relevant answers to the new question, if they already exist (answer selection). ∗ Work conducted while this author was at QCRI, HBKU. Both subtasks have been the focus of recent research as they result in end-user applications. The former is interesting for a user who wants to explore the space of similar questions in the forum and to decide whether to post a new question. It can also be relevant for the forum owners as it can help detect redundancy, eliminate question duplicates, and improve the overall forum st"
D18-1452,P16-1101,0,0.0474628,"er with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do. The general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. Huang et al. (2015) proposed an LSTM-CRF framework for such tasks. Ma and Hovy (2016) included a CNN in the framework to compute word representations from character-level embeddings. While these studies consider tasks related to constituents in a sentence, e.g., words and phrases, we focus on methods to represent comments and to model dependencies between comment-level tags. We also experiment with arbitrary graph structures in our CRF model to model dependencies at different levels. 3 Learning Approach cim Let q be a newly-posed question, and denote the m-th comment (m ∈ {1, 2, . . . , M }) in the answer thread for the i-th potentially related question qi (i ∈ {1, 2, . . . ,"
D18-1452,K15-1032,1,0.833942,"uctures accounting for the dependencies between subtasks to provide globally good solutions. The experimental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results. In future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction. Acknowledgments The first author would like to thank the funding su"
D18-1452,S16-1136,1,0.878923,"Missing"
D18-1452,N13-1090,0,0.119305,"Missing"
D18-1452,D16-1165,1,0.935991,"he relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these models, we use DNNs to induce taskspecific embeddings, and, more importantly, we perform multitask learning of three different cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the 4197 datasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SU PER TEAM (Mihaylova et al., 2016) and K ELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features."
D18-1452,S16-1083,1,0.914124,"Missing"
D18-1452,P02-1040,0,0.100744,"Missing"
D18-1452,P15-2114,0,0.0698502,"cially answer-goodness and question-question-relatedness influence answerselection significantly; (iii) the CRFs exploit the dependencies between subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN model"
D18-1452,D18-1131,1,0.84905,"Missing"
D18-1452,2006.amta-papers.25,0,0.0322229,"Missing"
D18-1452,P18-1162,0,0.0171294,"sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddi"
D18-1452,P15-2117,0,0.0300461,"taskspecific embeddings, and we defer the joint learning with global inference to the structured model. From the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these mo"
D18-1452,P13-1045,0,0.0436668,"Missing"
D18-1452,P15-2116,0,0.0365402,"Missing"
D19-5811,D18-1054,0,0.0661608,"Missing"
D19-5811,N16-1180,0,0.0530225,"Missing"
D19-5811,P14-1035,0,0.0256844,"retraining the memory network with artificially created question–answer pairs. Our key contributions are: i) this is the first systematic exploration of the challenges in fulltext BookQA, ii) we present a full pipeline framework for the task, iii) we publish a dataset of Who questions which expect book characters as an answer, and iv) we include a critical discussion on the shortcomings of the current QA approach, and we discuss potential avenues for future research. 2 BookQA Framework 3.1 Book & Question Preprocessing Books and questions are preprocessed in advance using the book-nlp parser (Bamman et al., 2014), a system for character detection and shallow parsing in books (Iyyer et al., 2016; Frermann and Szarvas, 2017) which provides, among others: sentence segmentation, POS tagging, dependency parsing, named entity recognition, and coreference resolution. The parser identifies and clusters character mentions, so that all coreferent (direct or pronominal) character mentions are associated with the same unique character identifier. 3.2 Context Selection In order to make inference over book text tractable and give our model a better chance at predicting the correct answer, we must restrict the conte"
D19-5811,Q18-1023,0,0.243451,"n and Computation, School of Informatics, University of Edinburgh 2 School of Computing and Information Systems, The University of Melbourne 3 Amazon Research s.angelidis@ed.ac.uk lea.frermann@unimelb.edu.au {marchegg,roiblan,lluismv}@amazon.com Abstract Wikipedia articles) used in neural QA architectures; (b) many facts about a book story are never made explicit, and require external or commonsense knowledge to infer them; (c) the QA system cannot rely on pre-existing KBs; (d) traditional retrieval techniques are less effective in selecting relevant passages from self-contained book stories (Kocisky et al., 2018); (e) collecting humanannotated BookQA data is a significant challenge; (f) stylistic disparities in the language used among different books may hinder generalization. Additionally, the style of book questions may vary significantly, with different approaches being potentially useful for different question types: from queries about story facts that have entities as answers (e.g., Who and Where questions); to open-ended questions that require the extraction or generation of longer answers (e.g., Why or How questions). The difference in reasoning required for different question types can make it"
D19-5811,D18-1454,0,0.0456618,"Missing"
D19-5811,D14-1067,0,0.0184585,"e same time, we confirm that NarrativeQA is a highly challenging data set, and that there is need for novel research in order to achieve high-precision BookQA results. We analyze some of the bottlenecks of the current approach, and we argue that more research is needed on text representation, retrieval of relevant passages, and reasoning, including commonsense knowledge. 1 Introduction Considerable volume of research work has looked into various Question Answering (QA) settings, ranging from retrieval-based QA (Voorhees, 2001) to recent neural approaches that reason over Knowledge Bases (KB) (Bordes et al., 2014), or raw text (Shen et al., 2017; Deng and Tam, 2018; Min et al., 2018). In this paper we use the NarrativeQA corpus (Kocisky et al., 2018) as a starting point and focus on the task of answering questions from the full text of books, which we call BookQA. BookQA has unique characteristics which prohibit the direct application of current QA methods. For instance, (a) books are usually orders of magnitude longer than the short texts (e.g., ∗ Work done while first author was interning at Amazon. 78 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 78–85 c Hong Ko"
D19-5811,D16-1147,0,0.0274171,"likelihood. if a sentence is relevant to a question, using positive (questions, summary sentence) training pairs which have been heuristically matched. Randomly sampled negative pairs were also used. At retrieval time, a question is used to retrieve relevant passages from the full text of a book. 3.3 Neural Inference Having replaced character mentions in questions and books with character identifiers, we first pretrain word2vec embeddings (Mikolov et al., 2013) for all words and book characters in our corpus.2 Our neural inference model is a variant of the KeyValue Memory Network (KV-MemNet) (Miller et al., 2016), which has been previously applied to QA tasks over KBs and short texts. The original model was designed to handle a fixed set of potential answers across all QA examples, as do most neural QA architectures. This comes in contrast with our task, where the pool of candidate characters is different for each book. Our KV-MemNet variant, illustrated in Figure 1, uses a dynamic output layer where different candidate answers are made available for different books, while the remaining model parameters are shared. A question is initially represented as q0 , i.e., the average of its word embeddings3 ("
D19-5811,N18-4012,0,0.0229619,"hallenging data set, and that there is need for novel research in order to achieve high-precision BookQA results. We analyze some of the bottlenecks of the current approach, and we argue that more research is needed on text representation, retrieval of relevant passages, and reasoning, including commonsense knowledge. 1 Introduction Considerable volume of research work has looked into various Question Answering (QA) settings, ranging from retrieval-based QA (Voorhees, 2001) to recent neural approaches that reason over Knowledge Bases (KB) (Bordes et al., 2014), or raw text (Shen et al., 2017; Deng and Tam, 2018; Min et al., 2018). In this paper we use the NarrativeQA corpus (Kocisky et al., 2018) as a starting point and focus on the task of answering questions from the full text of books, which we call BookQA. BookQA has unique characteristics which prohibit the direct application of current QA methods. For instance, (a) books are usually orders of magnitude longer than the short texts (e.g., ∗ Work done while first author was interning at Amazon. 78 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 78–85 c Hong Kong, China, November 4, 2019. 2019 Association for Co"
D19-5811,P18-1160,0,0.0244032,"and that there is need for novel research in order to achieve high-precision BookQA results. We analyze some of the bottlenecks of the current approach, and we argue that more research is needed on text representation, retrieval of relevant passages, and reasoning, including commonsense knowledge. 1 Introduction Considerable volume of research work has looked into various Question Answering (QA) settings, ranging from retrieval-based QA (Voorhees, 2001) to recent neural approaches that reason over Knowledge Bases (KB) (Bordes et al., 2014), or raw text (Shen et al., 2017; Deng and Tam, 2018; Min et al., 2018). In this paper we use the NarrativeQA corpus (Kocisky et al., 2018) as a starting point and focus on the task of answering questions from the full text of books, which we call BookQA. BookQA has unique characteristics which prohibit the direct application of current QA methods. For instance, (a) books are usually orders of magnitude longer than the short texts (e.g., ∗ Work done while first author was interning at Amazon. 78 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 78–85 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguis"
D19-5811,P19-1220,0,0.0124784,"ven when considering only the subset of Who questions that expect characters as answers. 3 The length of books and limited annotated data prohibit the application of end-to-end neural QA models that reason over the full text of a book. Instead, we opted for a pipeline approach, whose components are described below. by Tay et al. (2019), who proposed a curriculum learning-based two-phase approach (context selection and neural inference). More papers have looked into answering NarrativeQA’s questions from only book/movie summaries (Indurthi et al., 2018; Bauer et al., 2018; Tay et al., 2018a,b; Nishida et al., 2019). This is a fundamentally simpler task, because: i) the systems need to reason over a much shorter context, i.e., the summary; and ii) there is the certainty that the answer can be found in the summary. This paper is another step in the exploration of the full NarrativeQA task, and embraces the goal of finding an answer in the complete book text. We propose a system that first selects a small subset of relevant book passages, and then uses a memory network to reason and extract the answer from them. The network is specifically adapted for generalization across books. We analyze different optio"
D19-5811,D18-1238,0,0.0544729,"Missing"
D19-5811,P19-1486,0,0.0124345,"combination of automatic and crowdsourced efforts, we obtained a total of 3,427 QA pairs, spanning 614 books.1 Table 1: Who questions from NarrativeQA for the book The Mysteries of Udolpho, by Ann Radcliffe. The diversity and complexity of questions in the corpus remains high, even when considering only the subset of Who questions that expect characters as answers. 3 The length of books and limited annotated data prohibit the application of end-to-end neural QA models that reason over the full text of a book. Instead, we opted for a pipeline approach, whose components are described below. by Tay et al. (2019), who proposed a curriculum learning-based two-phase approach (context selection and neural inference). More papers have looked into answering NarrativeQA’s questions from only book/movie summaries (Indurthi et al., 2018; Bauer et al., 2018; Tay et al., 2018a,b; Nishida et al., 2019). This is a fundamentally simpler task, because: i) the systems need to reason over a much shorter context, i.e., the summary; and ii) there is the certainty that the answer can be found in the summary. This paper is another step in the exploration of the full NarrativeQA task, and embraces the goal of finding an a"
D19-5811,D17-1200,1,\N,Missing
D19-5811,N19-1423,0,\N,Missing
E03-1038,M95-1012,0,0.010507,"consensus about that Named Entity Recognition and Classification (NERC) are Natural Language Processing tasks which may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikhe"
E03-1038,A97-1029,0,0.13099,"slated into Catalan, including several entities. There is a wide consensus about that Named Entity Recognition and Classification (NERC) are Natural Language Processing tasks which may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–wr"
E03-1038,W02-2002,0,0.0199984,"L&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advant"
E03-1038,M98-1014,0,0.0543784,"performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El president"
E03-1038,W02-2004,1,0.764051,"ng data was left as unlabelled data. As evaluation method we use the common measures for recognition tasks: precision, recall and F1 . Precision is the percentage of NEs predicted by a system which are correct. Recall is the percentage of NEs in the data that a system correctly recognizes. Finally, the F1 measure computes the harmonic mean of precision (p) and recall (r) as 2 p • Op + r). 3 The Spanish NER System The Spanish NER system is based on the best system at CoNLL&apos;02, which makes use of a set of AdaBoost–based binary classifiers for recognizing the Named Entities in running text. See (Carreras et al., 2002) for details. The NE recognition task is performed as a sequence tagging problem through the well–known BIO labelling scheme. Here, the input sentence is treated as a word sequence and the output tagging codifies the NEs in the sentence. In particular, each word is tagged as either the beginning of a NE (B tag), a word inside a NE (I tag), or a word outside a NE (0 tag). In our case, a NER model is composed by: (a) a representation function, which maps a word and its context into a set of features, and (b) three binary classifiers (one corresponding to each tag) which, operating on the feature"
E03-1038,W99-0613,0,0.379481,"Missing"
E03-1038,M98-1015,0,0.0203914,"ssifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El pr"
E03-1038,W02-2019,0,0.0233796,"ly, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advantage of the facts that Spanish and Catalan are two Romance languages with similar syntact"
E03-1038,W02-2020,0,0.0246992,"s between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our"
E03-1038,M98-1021,0,0.0255916,"1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER="
E03-1038,W02-2024,0,0.0701502,"ente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos,"
E03-1038,W02-2025,0,0.0420667,"ente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos,"
E03-1038,W02-2031,0,0.0471146,"Missing"
E03-1038,M95-1006,0,0.149935,"etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]"
E03-1038,M98-1016,0,0.0312668,"on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justic"
gencheva-etal-2017-context,J15-3002,0,\N,Missing
gencheva-etal-2017-context,nakov-etal-2017-trust,1,\N,Missing
gencheva-etal-2017-context,W02-0109,0,\N,Missing
gimenez-marquez-2004-svmtool,N03-1033,0,\N,Missing
gimenez-marquez-2004-svmtool,W02-1001,0,\N,Missing
gimenez-marquez-2008-towards,E06-1032,0,\N,Missing
gimenez-marquez-2008-towards,niessen-etal-2000-evaluation,0,\N,Missing
gimenez-marquez-2008-towards,2004.tmi-1.8,0,\N,Missing
gimenez-marquez-2008-towards,W07-0411,0,\N,Missing
gimenez-marquez-2008-towards,N03-2021,0,\N,Missing
gimenez-marquez-2008-towards,P02-1040,0,\N,Missing
gimenez-marquez-2008-towards,W06-3112,0,\N,Missing
gimenez-marquez-2008-towards,P04-1077,0,\N,Missing
gimenez-marquez-2008-towards,W06-1610,0,\N,Missing
gimenez-marquez-2008-towards,2005.eamt-1.15,0,\N,Missing
gimenez-marquez-2008-towards,W05-0909,0,\N,Missing
gimenez-marquez-2008-towards,N06-1058,0,\N,Missing
gimenez-marquez-2008-towards,C04-1072,0,\N,Missing
gimenez-marquez-2008-towards,P06-2003,1,\N,Missing
gimenez-marquez-2008-towards,P01-1020,0,\N,Missing
gimenez-marquez-2008-towards,W05-0904,0,\N,Missing
H05-1081,W04-2412,1,0.881998,"Missing"
H05-1081,W05-0620,1,0.859115,"Missing"
H05-1081,W04-2415,1,0.848646,"with clause nor chunk boundaries, discard ARG0-5 arguments not present in PropBank frames for a certain verb, etc. 1 Features extracted from partial parsing and Named Entities are common to Model 1 and 2, while features coming from full parse trees only apply to Model 2. Relative position, distance in words and chunks, and level of embedding (in #clause-levels) with respect to the constituent. Constituent path as described in (Gildea and Jurafsky, 2002) and all 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent. Partial parsing path as described in (Carreras et al., 2004) and all 3/4/5-grams of path elements beginning at the verb predicate or ending at the constituent. Syntactic frame as described by Xue and Palmer (2004) Table 3: Predicate–constituent features: Models 1/2 The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last"
H05-1081,J02-3001,0,0.704534,"cantly boost results of individual systems. This combination scheme is also very flexible since the individual systems are not required to provide any information other than their solution. Extensive experimental evaluation in the CoNLL2005 shared task framework supports our previous claims. The proposed architecture outperforms the best results reported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because t"
H05-1081,W05-0623,0,0.094927,"didates is performed in an elegant global inference procedure as constraint satisfaction, which, formulated as Integer Linear Programming, can be solved efficiently. Interestingly, the generalized inference layer allows to include in the objective function, jointly with the candidate argument scores, a number of linguistically-motivated constraints to obtain a coherent solution. Differing from the strategy presented in this paper, their inference layer does not include learning. Also, they require confidence values from individual classifiers. This is the best performing system at CoNLL-2005. Haghighi et al. (2005) implemented a double reranking model on top of the base SRL models to select the most probable solution among a set of candidates. The re-ranking is performed, first, on a set of n-best solutions obtained by the base system run on a single parse tree, and, then, on the set of bestcandidates coming from the n-best parse trees. The re-ranking approach allows to define global complex features applying to complete candidate solutions to train the rankers. The main drawback, compared to our approach, is that re-ranking does not permit to combine different solutions since it is forced to select a c"
H05-1081,W05-0625,0,0.249114,"ses only global attributes extracted from the solutions provided by the individual systems, e.g., the sequence of role labels generated by each system for the current predicate. We do not use any attributes specific to the individual models, not even the confidence assigned by the individual classifiers. Besides simplicity, the consequence of this decision is that our approach does not impose any restrictions on the individual SRL strategies, as long as one solution is provided for each predicate. On the other hand, probabilistic inference processes, which have been successfully used for SRL (Koomen et al., 2005), mandate that each individual candidate argument be associated with its raw activation, or confidence, in the given model. However, this information is not directly available in two out of three of our individual models, which classify argument chunks and not entire arguments. Despite its simplicity, our approach obtains encouraging results: the combined system outperforms any of the individual systems and, using exactly the same data, it is also competitive with the best SRL systems that participated in the latest CoNLL shared task evaluation (Carreras and M`arquez, 2005). 2 Semantic Corpora"
H05-1081,W05-0628,1,0.868909,"Missing"
H05-1081,C04-1100,0,0.0556393,"eported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis. Under such real-world conditions, the difference between the two SRL approaches (with full or partial syntax) is not that high. More interestingly, the two SRL strategies perform better for different semantic roles. For example, mod"
H05-1081,W05-0634,0,0.105345,"lso very flexible since the individual systems are not required to provide any information other than their solution. Extensive experimental evaluation in the CoNLL2005 shared task framework supports our previous claims. The proposed architecture outperforms the best results reported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs wor"
H05-1081,W05-0635,1,0.778897,"Missing"
H05-1081,P03-1002,1,0.917855,"Missing"
H05-1081,W04-3212,0,0.373931,"ombination scheme is also very flexible since the individual systems are not required to provide any information other than their solution. Extensive experimental evaluation in the CoNLL2005 shared task framework supports our previous claims. The proposed architecture outperforms the best results reported in that evaluation exercise. 1 Introduction The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005a; Carreras and M`arquez, 2005). It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al., 2003) and Question Answering (Narayanan and Harabagiu, 2004). Most current SRL approaches can be classified in one of two classes: approaches that take adOn the other hand, when only automaticallygenerated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less r"
I08-1042,P07-1111,0,0.889987,"eason is that SMT systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations. We argue that, in order to perform more robust, i.e., less biased, automatic MT evaluations, different quality dimensions should be jointly taken into account. A natural solution to this challenge consists in combining the scores conferred by different metrics, ideally covering a heterogeneous set of quality aspects. In the last few years, several approaches to metric combination have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a). In spite of working on a limited set of quality aspects, mostly lexical features, these approaches have provided effective means of combining different metrics into a single measure of quality. All these methods implement a parametric combination scheme. Their models involve a number of parameters whose weight must be adjusted (see further details in Section 2). As an alternative path towards heterogeneous MT evaluation, in this work, we explore the possibility of relying on non-parametric combination schemes, in which metrics are combined without having to adjust their relative importance"
I08-1042,P07-1038,0,0.724429,"eason is that SMT systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations. We argue that, in order to perform more robust, i.e., less biased, automatic MT evaluations, different quality dimensions should be jointly taken into account. A natural solution to this challenge consists in combining the scores conferred by different metrics, ideally covering a heterogeneous set of quality aspects. In the last few years, several approaches to metric combination have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a). In spite of working on a limited set of quality aspects, mostly lexical features, these approaches have provided effective means of combining different metrics into a single measure of quality. All these methods implement a parametric combination scheme. Their models involve a number of parameters whose weight must be adjusted (see further details in Section 2). As an alternative path towards heterogeneous MT evaluation, in this work, we explore the possibility of relying on non-parametric combination schemes, in which metrics are combined without having to adjust their relative importance"
I08-1042,P06-2003,1,0.842042,"Missing"
I08-1042,W05-0909,0,0.0518654,"etric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation. 1 Introduction Automatic evaluation metrics have notably accelerated the development cycle of MT systems in the last decade. There exist a large number of metrics based on different similarity criteria. By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full"
I08-1042,E06-1032,0,0.215031,"Missing"
I08-1042,P01-1020,0,0.0716507,"ption is that ‘good’ translations should resemble human translations. Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amig´o et al., 2005). In the following, we describe the most relevant approaches to metric combination suggested in recent literature. All are parametric, and most of them are based on machine learning techniques. We distinguish between approaches relying on human likeness and approaches relying on human acceptability. 2.1 Approaches based on Human Likeness The first approach to metric combination based on human likeness was that by Corston-Oliver et al. (2001) who used decision trees to distinguish between human-generated (‘good’) and machinegenerated (‘bad’) translations. They focused on evaluating only the well-formedness of automatic translations (i.e., subaspects of fluency), obtaining high levels of classification accuracy. Kulesza and Shieber (2004) extended the approach by Corston-Oliver et al. (2001) to take into account other aspects of quality further than fluency alone. Instead of decision trees, they trained Support Vector Machine (SVM) classifiers. They used features inspired by well-known metrics such as BLEU, NIST , WER, and PER. Met"
I08-1042,2003.mtsummit-papers.10,0,0.847655,"st a large number of metrics based on different similarity criteria. By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full constituent parsing (Liu and Gildea, 2005), and on dependency parsing (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arquez, 2"
I08-1042,2005.eamt-1.15,0,0.0656737,"cy. Kulesza and Shieber (2004) extended the approach by Corston-Oliver et al. (2001) to take into account other aspects of quality further than fluency alone. Instead of decision trees, they trained Support Vector Machine (SVM) classifiers. They used features inspired by well-known metrics such as BLEU, NIST , WER, and PER. Metric quality was evaluated both in terms of classification accuracy and correlation with human assessments at the sentence level. 2 Usually adequacy, fluency, or a combination of the two. A significant improvement with respect to standard individual metrics was reported. Gamon et al. (2005) presented a similar approach which, in addition, had the interesting property that the set of human and automatic translations could be independent, i.e., human translations were not required to correspond, as references, to the set of automatic translations. 2.2 a given automatic output a is addressed through a set of independent probabilistic tests (one per metric) in which the goal is to falsify the hypothesis that a is a human reference. The input for QARLA is a set of test cases A (i.e., automatic translations), a set of similarity metrics X, and a set of models R (i.e., human references"
I08-1042,W07-0738,1,0.887688,"Missing"
I08-1042,N06-1058,0,0.0261497,"). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full constituent parsing (Liu and Gildea, 2005), and on dependency parsing (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arquez, 2007), and at the properly semantic level, e.g., over discourse representations (Gim´enez, 2007). However, none of current metrics provides, in isolation, a g"
I08-1042,W06-3114,0,0.0532549,"y and Brew, 2007; Owczarzak et al., 2007). We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arquez, 2007), and at the properly semantic level, e.g., over discourse representations (Gim´enez, 2007). However, none of current metrics provides, in isolation, a global measure of quality. Indeed, all metrics focus on partial aspects of quality. The main problem of relying on partial metrics is that we may obtain biased evaluations, which may lead us to derive inaccurate conclusions. For instance, CallisonBurch et al. (2006) and Koehn and Monz (2006) have recently reported several problematic cases related to the automatic evaluation of systems oriented towards maximizing different quality aspects. Corroborating the findings by Culy and Riehemann (2003), they showed that BLEU overrates SMT systems with respect to other types of systems, such 1 ROUGE and METEOR may consider morphological variations. METEOR may also look up for synonyms in WordNet. as rule-based, or human-aided. The reason is that SMT systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations. We argue tha"
I08-1042,2004.tmi-1.8,0,0.562006,"in WordNet. as rule-based, or human-aided. The reason is that SMT systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations. We argue that, in order to perform more robust, i.e., less biased, automatic MT evaluations, different quality dimensions should be jointly taken into account. A natural solution to this challenge consists in combining the scores conferred by different metrics, ideally covering a heterogeneous set of quality aspects. In the last few years, several approaches to metric combination have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a). In spite of working on a limited set of quality aspects, mostly lexical features, these approaches have provided effective means of combining different metrics into a single measure of quality. All these methods implement a parametric combination scheme. Their models involve a number of parameters whose weight must be adjusted (see further details in Section 2). As an alternative path towards heterogeneous MT evaluation, in this work, we explore the possibility of relying on non-parametric combination schemes, in which metrics are combined with"
I08-1042,C04-1072,0,0.30149,", i.e., their ability to emulate human assessors. The underlying assumption is that ‘good’ translations should be acceptable to human evaluators. Human acceptability is usually measured on the basis of correlation between automatic metric scores and human assessments of translation quality2 . • Human Likeness: Metrics are evaluated in terms of their ability to capture the features which distinguish human from automatic translations. The underlying assumption is that ‘good’ translations should resemble human translations. Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amig´o et al., 2005). In the following, we describe the most relevant approaches to metric combination suggested in recent literature. All are parametric, and most of them are based on machine learning techniques. We distinguish between approaches relying on human likeness and approaches relying on human acceptability. 2.1 Approaches based on Human Likeness The first approach to metric combination based on human likeness was that by Corston-Oliver et al. (2001) who used decision trees to distinguish between human-generated (‘good’) and machinegenerated (‘bad’) translations. They focused on"
I08-1042,W05-0904,0,0.156743,"l., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full constituent parsing (Liu and Gildea, 2005), and on dependency parsing (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arquez, 2007), and at the properly semantic level, e.g., over discourse representations (Gim´enez, 2007). However, none of current metrics provides, in isolation, a global measure of quality. Indeed, all metrics focus on partial aspects of quality. The main problem of relying on partial metrics is that we may obtain biased evaluations, which may lead us to d"
I08-1042,N07-1006,0,0.544129,"or human-aided. The reason is that SMT systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations. We argue that, in order to perform more robust, i.e., less biased, automatic MT evaluations, different quality dimensions should be jointly taken into account. A natural solution to this challenge consists in combining the scores conferred by different metrics, ideally covering a heterogeneous set of quality aspects. In the last few years, several approaches to metric combination have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a). In spite of working on a limited set of quality aspects, mostly lexical features, these approaches have provided effective means of combining different metrics into a single measure of quality. All these methods implement a parametric combination scheme. Their models involve a number of parameters whose weight must be adjusted (see further details in Section 2). As an alternative path towards heterogeneous MT evaluation, in this work, we explore the possibility of relying on non-parametric combination schemes, in which metrics are combined without having to adjust t"
I08-1042,2007.tmi-papers.15,0,0.813422,"level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full constituent parsing (Liu and Gildea, 2005), and on dependency parsing (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arquez, 2007), and at the properly semantic level, e.g., over discourse representations (Gim´enez, 2007). However, none of current metrics provides, in isolation, a global measure of quality. Indeed, all metrics focus on partial aspects of quality. The main problem of relying on partial metrics is that we may obtain biased evaluations, which may lead us to derive inaccurate conclusions. For instance, CallisonBurch et al. (2006) and Koehn and Monz ("
I08-1042,N03-2021,0,0.0860435,"ntactic and semantic). Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation. 1 Introduction Automatic evaluation metrics have notably accelerated the development cycle of MT systems in the last decade. There exist a large number of metrics based on different similarity criteria. By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper"
I08-1042,niessen-etal-2000-evaluation,0,0.190804,"ng at different linguistic levels (e.g., lexical, syntactic and semantic). Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation. 1 Introduction Automatic evaluation metrics have notably accelerated the development cycle of MT systems in the last decade. There exist a large number of metrics based on different similarity criteria. By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Othe"
I08-1042,W06-3112,0,0.20249,"Missing"
I08-1042,W07-0411,0,0.0622154,"Missing"
I08-1042,2001.mtsummit-papers.68,0,0.048129,"exical dimension, we work on a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic). Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation. 1 Introduction Automatic evaluation metrics have notably accelerated the development cycle of MT systems in the last decade. There exist a large number of metrics based on different similarity criteria. By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; K"
I08-1042,quirk-2004-training,0,0.167112,"translations were not required to correspond, as references, to the set of automatic translations. 2.2 a given automatic output a is addressed through a set of independent probabilistic tests (one per metric) in which the goal is to falsify the hypothesis that a is a human reference. The input for QARLA is a set of test cases A (i.e., automatic translations), a set of similarity metrics X, and a set of models R (i.e., human references) for each test case. With such a testbed, QARLA provides the two essential ingredients required for metric combination: Approaches based on Human Acceptability Quirk (2004) applied supervised machine learning algorithms (e.g., perceptrons, SVMs, decision trees, and linear regression) to approximate human quality judgements instead of distinguishing between human and automatic translations. Similarly to the work by Gamon et al. (2005) their approach does not require human references. More recently, Albrecht and Hwa (2007a; 2007b) re-examined the SVM classification approach by Kulesza and Shieber (2004) and, inspired by the work of Quirk (2004), suggested a regression-based learning approach to metric combination, with and without human references. The regression"
I08-1042,2006.amta-papers.25,0,0.0610805,"utting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation. 1 Introduction Automatic evaluation metrics have notably accelerated the development cycle of MT systems in the last decade. There exist a large number of metrics based on different similarity criteria. By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full constituent parsing (Liu and G"
I08-1042,P04-1077,0,0.515995,", i.e., their ability to emulate human assessors. The underlying assumption is that ‘good’ translations should be acceptable to human evaluators. Human acceptability is usually measured on the basis of correlation between automatic metric scores and human assessments of translation quality2 . • Human Likeness: Metrics are evaluated in terms of their ability to capture the features which distinguish human from automatic translations. The underlying assumption is that ‘good’ translations should resemble human translations. Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amig´o et al., 2005). In the following, we describe the most relevant approaches to metric combination suggested in recent literature. All are parametric, and most of them are based on machine learning techniques. We distinguish between approaches relying on human likeness and approaches relying on human acceptability. 2.1 Approaches based on Human Likeness The first approach to metric combination based on human likeness was that by Corston-Oliver et al. (2001) who used decision trees to distinguish between human-generated (‘good’) and machinegenerated (‘bad’) translations. They focused on"
I08-1042,W06-1610,0,0.0271388,"pineni et al., 2001). Other well-known metrics are WER (Nießen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin 319 and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few. All these metrics take into account information at the lexical level1 , and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003). In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). Other authors have tried to exploit information at deeper linguistic levels. For instance, we may find metrics based on full constituent parsing (Liu and Gildea, 2005), and on dependency parsing (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arquez, 2007), and at the properly semantic level, e.g., over discourse representations (Gim´enez, 2007). However, none of current metrics"
I17-1001,P17-1080,1,0.491617,"ing Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Yonatan Belinkov1 Llu´ıs M`arquez2 Hassan Sajjad2 Nadir Durrani2 Fahim Dalvi2 James Glass1 1 MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA {belinkov, glass}@mit.edu 2 Qatar Computing Research Institute, HBKU, Doha, Qatar {lmarquez, hsajjad, ndurrani, faimaduddin}@qf.org.qa Abstract One observation that has been made is that lower layers in the neural MT network learn different kinds of information than higher layers. For example, Shi et al. (2016) and Belinkov et al. (2017) found that representations from lower layers of the NMT encoder are more predictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for imp"
I17-1001,D17-1151,0,0.0125781,"al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et al. (2017) found that encoders with 2 to 4 layers performed the best. For completeness, we report here results using features extracted from models trained originally with 2 and 3 layers, in addition to our basic setting of 4 layers. Table 6 shows consistent trends with our previous observations: POS tagging does not benefit from upper layers, while SEM tagging does, although the improvement is rather small in the shallower models. 5 0 Related Work Techniques for analyzing neural network models include visualization of hidden units (Elman, 1991; Karpathy et al., 2015; K´ad´ar et al., 2016; Qian et al.,"
I17-1001,E17-2039,0,0.0170778,"Missing"
I17-1001,P07-1005,0,0.0220979,"coder are more predictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), an"
I17-1001,I17-1015,1,0.421519,"semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on e"
I17-1001,K17-1037,0,0.00554294,"ve similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations. 0 1 2 3 4 4 POS SEM 87.9 81.8 92.0 87.8 91.7 87.4 91.8 87.6 91.9 88.2 3 POS SEM 87.9 81.9 92.5 88.2 92.3 88.0 92.4 88.4 – – 2 POS SEM 87.9 82.0 92.7 88.5 92.7 88.7 – – – – Table 6: POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages. tain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wu and King, 2016; Alishahi et al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et"
I17-1001,W11-1012,0,0.0281742,"inguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are assigned different SEM t"
I17-1001,P82-1020,0,0.755314,"Missing"
I17-1001,P13-2074,0,0.0196012,"gical tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are assigned different SEM tags depending on their type (e.g., geopolitical entity, organizati"
I17-1001,C12-1083,0,0.0154324,"Missing"
I17-1001,P16-1140,0,0.0367,"Missing"
I17-1001,E17-2060,0,0.0165727,"peech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on either SEM or POS tagging using features from different l"
I17-1001,D16-1159,0,0.531034,"tter for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on either SEM or POS tagging using features"
I17-1001,D15-1246,0,0.0767367,"Missing"
I17-1001,W17-4115,0,0.0393383,"Missing"
I17-1001,Q16-1037,0,0.0323719,"0 1 2 3 4 4 POS SEM 87.9 81.8 92.0 87.8 91.7 87.4 91.8 87.6 91.9 88.2 3 POS SEM 87.9 81.9 92.5 88.2 92.3 88.0 92.4 88.4 – – 2 POS SEM 87.9 82.0 92.7 88.5 92.7 88.7 – – – – Table 6: POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages. tain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wu and King, 2016; Alishahi et al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et al. (2017) found that encoders with 2 to 4 layers performed the best. For completeness, we report here results using features extracted from models t"
I17-1001,C10-1081,0,0.031874,"ictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are ass"
I17-1001,L16-1561,0,0.00604061,"OS and SEM tags using the features hkj that are obtained from the English encoder and evaluate their accuracies. Figure 1 illustrates the process. • Consistent with previous work, we find that lower layer representations are usually better for POS tagging. However, we also find that representations from higher layers are better at capturing semantics, even though these are word-level labels. This is especially true with tags that are more semantic in nature such as discourse functions or noun concepts. 2 3 3.1 Data and Experimental Setup Data MT We use the fully-aligned United Nations corpus (Ziemski et al., 2016) for training NMT models, which includes 11 million multi-parallel sentences in six languages: Arabic (Ar), Chinese (Zh), English (En), French (Fr), Spanish (Es), and Russian (Ru). We train En-to-* models on the first 2 million sentences of the train set, using the official train/dev/test split. This dataset has the benefit of multiple alignment of the six languages, which allows for comparable cross-linguistic analysis. Note that the parallel dataset is only used for training the NMT model. The classifier is then trained on the supervised data (described next) and all accuracies are reported"
I17-1001,D16-1079,0,0.0193389,"Missing"
J08-2001,W04-2412,1,0.733851,"Missing"
J08-2001,W05-0620,1,0.919702,"Missing"
J08-2001,W04-2415,1,0.828554,"Missing"
J08-2001,W05-0622,0,0.0206004,"Missing"
J08-2001,copestake-flickinger-2000-open,0,0.00738653,"]Pred to [the boy beside her]Recipient Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities participating in an event, and Temporal and Manner for the characterization of other aspects of the event or participant relations. This type of role labeling thus yields a ﬁrstlevel semantic representation of the text that indicates the basic event properties and relations among relevant entities that are expressed in the sentence. Research has proceeded for decades on manually created lexicons, grammars, and other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000) in support of deep semantic analysis of language input, but such approaches have been labor-intensive and often restricted to narrow domains. The 1990s saw a growth in the development of statistical machine learning methods across the ﬁeld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe an"
J08-2001,P07-1028,0,0.0144889,"Missing"
J08-2001,S07-1048,0,0.143442,"Missing"
J08-2001,W05-0625,0,0.0189955,"Missing"
J08-2001,W04-0803,0,0.00817493,"Missing"
J08-2001,S07-1005,1,0.808742,"Missing"
J08-2001,W05-0628,1,0.603848,"Missing"
J08-2001,S07-1008,1,0.800462,"Missing"
J08-2001,J01-3003,1,0.757374,"been labor-intensive and often restricted to narrow domains. The 1990s saw a growth in the development of statistical machine learning methods across the ﬁeld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 an"
J08-2001,W04-2705,0,0.0796831,"ather than requiring manual encoding. These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments—for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and"
J08-2001,W04-2609,0,0.0198714,"evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for acquiring and exploiting semantic relations among the different components of a text. This special issue of Computational Linguistics presents several articles representing the state-of-the-art in SRL, and this overview is intended to provide a broader c"
J08-2001,N06-2026,0,0.0821305,"Missing"
J08-2001,C04-1100,0,0.0222676,"s et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for acquiring and exploiting semantic relations among th"
J08-2001,W03-0411,0,0.0160444,"Missing"
J08-2001,J05-1004,0,0.471683,"Missing"
J08-2001,W05-0634,0,0.0213716,"Missing"
J08-2001,J06-2001,0,0.0246464,"Missing"
J08-2001,P03-1002,0,0.288914,"ea, and Kingsbury 2005), and NomBank (Meyers et al. 2004), enabling the development of statistical approaches speciﬁcally for SRL. With the advent of supporting resources, SRL has become a well-deﬁned task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al. [2003], Xue and Palmer [2004], Pradhan et al. [2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The identiﬁcation of event frames may potentially beneﬁt many natural language processing (NLP) applications, such as information extraction (Surdeanu et al. 2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al. 2005), and machine translation (Boas 2002). Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al. 2004; Rosario and Hearst 2004). Although the use of SRL systems in real-world applications has thus far been limited, the outlook is promising for extending this type of analysis to many applications requiring some level of semantic interpretation. SRL represents an excellent framework with which to perform research on computational techniques for a"
J08-2001,H05-1111,1,0.811574,"Missing"
J08-2001,P05-1073,0,0.0404859,"Missing"
J08-2001,W04-3212,0,0.06057,"Missing"
J08-2001,S07-1077,1,0.814405,"Missing"
J08-2001,N07-1069,0,\N,Missing
J08-2001,W04-3213,1,\N,Missing
J08-2001,boas-2002-bilingual,0,\N,Missing
J08-2001,S07-1018,0,\N,Missing
J08-2001,S07-1016,0,\N,Missing
J08-2001,J13-3006,1,\N,Missing
J08-2001,A97-1052,0,\N,Missing
J08-2001,P04-1055,0,\N,Missing
J08-2001,J02-3001,0,\N,Missing
J08-2001,W06-2303,0,\N,Missing
J13-3006,P08-1037,1,0.925043,"Missing"
J13-3006,P11-2123,1,0.853466,"hen test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective ta"
J13-3006,W01-0703,1,0.790514,"Missing"
J13-3006,C96-1005,1,0.358301,"are usually more useful for characterizing selectional preferences, as in the <tool> class for the instrument role of break. The priority of using specific synsets over more general ones is, thus, justified in the sense that they may better represent the most relevant semantic characteristics of the selectional preferences. The alternative method (SPwn ) is based on the depth of the concepts in the WordNet hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model the specificity of concepts (the deeper the more specific) is not new (Rada et al. 1989; Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect to generalization: When we check which SP is a better fit for a given target head, we always prefer the SP that contains the most specific generalization for the target head (the lowest synset which is a hypernym of the target word). 641 Computational Linguistics Volume 39, Number 3 Table 4 Excerpt from the selectional preferences for write-Arg0 according to SPwn , showing from deeper to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists the depth of synsets in WordNet. Description includes the words and"
J13-3006,J10-4006,0,0.243113,"on of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resnik’s selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and Weir 2002). Despite its earlier publication, Resnik’s method is still the most popular ´ Pado, ´ and Erk 2007; Erk, Pado, ´ representative among WordNet-based methods (Pado, and Pado´ 2010; Baroni and Lenci 2010). We also chose to use Resnik’s model in this paper. One of the disadvantages of the WordNet-based models, compared with the distributional similarity models, is that they require that the heads are present in WordNet. This limitation can negatively influence the coverage of the model, and also its generalization ability. 2.2 Distributional Similarity Models Distributional similarity models assume that a word is characterized by the words it co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size context window. Each word w would be represented by the set of words"
J13-3006,D08-1007,0,0.0621817,"Missing"
J13-3006,boas-2002-bilingual,0,0.013958,"e predicates. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential"
J13-3006,E03-1034,0,0.0636171,"P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resnik’s selectional preference scored best among WordNet-based methods (Li and Abe 1998; Clark and Weir 2002). Despite i"
J13-3006,W04-2412,1,0.876656,"Missing"
J13-3006,W05-0620,1,0.896889,"Missing"
J13-3006,N10-1058,1,0.848114,"esnik’s model tends to always predict the most frequent roles whereas our model covers a wider role selection. Resnik’s tendency to overgeneralize makes more frequent roles cover all the vocabulary, and the weighting system penalizes roles with fewer occurrences. 12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment. 650 Zapirain et al. Selectional Preferences for Semantic Role Classification The results for distributional models indicate that the SPs using Lin’s ready-made pre thesaurus (simLin ) outperforms Pado´ and Lapata’s distributional similarity model (Pado´ and Lapata 2007) calculated over the BNC (simLin ) in both Tables 1"
J13-3006,A00-2018,0,0.119058,"state-of-the-art semantic role labeling system (Surdeanu et al. 2007). SwiRL ranked second among the systems that did not implement model combination at the CoNLL-2005 shared task and fifth overall (Carreras and M`arquez 2005). Because the focus of this section is on role classification, we modified the SRC component of SwiRL to use gold argument boundaries, that is, we assume that semantic role identification works perfectly. Nevertheless, for a realistic evaluation, all the features in the role classification model are generated using actual syntactic trees generated by the Charniak parser (Charniak 2000). The key idea behind our approach is model combination: We generate a battery of base models using all resources available and we combine their outputs using multiple strategies. Our pool of base models contains 13 different models: The first is the 13 The data sets used for the experiments reported in this section are exactly the ones described in Section 4.1. 651 Computational Linguistics Volume 39, Number 3 unmodified SwiRL SRC, the next six are the selected SP models from the previous section, and the last six are variants of SwiRL SRC. In each variant, the feature set of the unmodified S"
J13-3006,J02-2003,0,0.0199537,"s SPRes (p, r, w0 ), is formulated as follows:3 SPRes (p, r, w0 ) = max c0 ∈hyp(w0 ) P(c0 |p, r)log P(c0 |p,r) P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun) triples, and correlation with human plausibility judgment is used for evaluation. Resn"
J13-3006,P97-1067,0,0.0336605,"ntains John loves Mary, then the pair (ncsubj, love) would be in the set T for John. The measure uses information-theoretic principles, and I(w, d, v) represents the information content of the triple (Lin 1998). Although the use of co-occurrence vectors for words to compute similarity has been ¨ standard practice, some authors have argued for more complex uses. Schutze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical cooccurrence network, and applied it to a lexical choice task. Chakraborti et al. (2007) used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method. In this paper, we will also use second order to refer to a new method which goes beyond the usual co-occurrence vectors (cf. Section 3.3). A full review of distributional models is out of the scope of this paper, as we are interested in showing that some of those models can be used successfully to improve SR"
J13-3006,P07-1028,0,0.76274,"tion could be more fine-grained, as defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy and Carroll 2003), and other lexical resources could be used as well. Other authors have used automatically induced hierarchical word classes, clustered according to occurrence information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009). On the other extreme, each word would be its own semantic class, as in the lexical model, but one could also model selectional preference using distributional similarity (Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel 2008). In this paper we will focus on WordNet-based models that use the whole hierarchy and on distributional similarity models, and we will use the lexical model as baseline. 2.1 WordNet-Based Models Resnik (1993b) proposed the modeling of selectional preferences using semantic classes from WordNet and applied the model to tackle some ambiguity issues in syntax, such as noun-compounds, coordination, and prepositional phrase attachment. Given two alternative structures, Resnik used selectional preferences to choose the attachment maximizing the fitness of the head to"
J13-3006,J10-4007,0,0.0471056,"Missing"
J13-3006,J02-3001,0,0.690118,"yntactic ambiguity. For instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the distributional similarity measure defined by Lin (1998). Distributional similarity was used to overcome sparsity problems: Alongside the counts in the training data of the target words, the counts of words similar to the target ones were also used. Although not made explicit, Lin was actually using a distributional similarity model of selectional preferences. The application of distributional selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles rely on the evaluation of artificial ta"
J13-3006,P92-1052,0,0.05076,"oth training examples for Temporal (i.e., November and winter), and <geographical area> covers the examples for Location. When test words Texas and December occur in Examples (6) and (7), the semantic classes to which they belong can be used to tag the first as Location and the second as Temporal. As an alternative to the use of WordNet, one can also apply automatically acquired distributional similarity thesauri. Distributional similarity methods analyze the cooccurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 Zapirain et al. Selectional Preferences for Semantic Role Classification but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to"
J13-3006,I08-1055,0,0.0292023,"end have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification. Whereas the former is mostly a syntactic recognition task, the"
J13-3006,P90-1034,0,0.513268,"every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences"
J13-3006,P08-1068,0,0.105046,"Missing"
J13-3006,P99-1004,0,0.246709,"at co-occur with it, T(w). In a more elaborate model, each word w would be represented as a vector   i (w) corresponds to the weight of the ith word in with weights, where T of words T(w) the vector. The weights can be calculated following a simple frequency of co-occurrence, or using some other formula. Then, given two words w and w0 , their similarity can be computed using any similarity measure between their co-occurrence sets or vectors. For instance, early work by Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0 ) (cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,   0 ) (cf. Equation (5) and T(w including Jaccard and the cosine between two vectors T(w) in Figure 1). In the context of lexical semantics, the similarity measure defined by Lin (1998) has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of cooccurrences of w contains pairs (d,v) of dependencies and words, representing the fact simJac (w, w0 ) = simcos (w, w0 ) =   simLin (w, w0 ) =  |T(w) ∩ T(w0 )| |T(w) ∪ T(w0 )| n"
J13-3006,J98-2002,0,0.0537239,"erence of a predicate p and role r for a head w0 of any potential argument, noted as SPRes (p, r, w0 ), is formulated as follows:3 SPRes (p, r, w0 ) = max c0 ∈hyp(w0 ) P(c0 |p, r)log P(c0 |p,r) P(c0 ) R(p, r) (3) The numerator formalizes the goodness of fit for the best semantic class c0 that contains w0 . The hypernym (i.e., superclass) of w0 yielding the maximum value is chosen. The denominator models how restrictive the selectional preference is for p and r, as modeled in Equation (1). Variations of Resnik’s idea to find a suitable level of generalization have been explored in later years. Li and Abe (1998) applied the minimum-description length principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a class should be preferred rather than its children. Brockmann and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgment task for German. 3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented in this paper. 636 Zapirain et al. Selectional Preferences for Semantic Role Classification The models return weights for (verb, syntactic function, noun)"
J13-3006,P98-2127,0,0.198549,"ional similarity thesauri. Distributional similarity methods analyze the cooccurrence patterns of words and are able to capture, for instance, that December is more closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is typically used on-line (i.e., given a pair of words, their similarity is computed on the go), 634 Zapirain et al. Selectional Preferences for Semantic Role Classification but, in order to speed up its use, it has also been used to produce off-line a full thesauri, storing, for every word, the weighted list of all outstanding similar words (Lin 1998). In the Distributional similarity model, when test item Texas in Example (6) is to be labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Marti"
J13-3006,S07-1005,0,0.346404,"Missing"
J13-3006,W06-2106,0,0.177585,"Missing"
J13-3006,H94-1020,0,0.407013,"Missing"
J13-3006,J08-2001,1,0.891986,"Missing"
J13-3006,J03-4004,0,0.257012,"New York, in contrast to the lower similarity to November and winter, would be used to label the argument with the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a"
J13-3006,P07-1098,0,0.0545287,"s of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classification. Whereas the former is mostly a sy"
J13-3006,C04-1100,0,0.0643825,"der the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identification and argument classif"
J13-3006,P10-1045,0,0.0304033,"Missing"
J13-3006,J07-2002,0,0.165313,"Missing"
J13-3006,D07-1042,0,0.235467,"Missing"
J13-3006,J05-1004,0,0.62839,"Missing"
J13-3006,N07-1071,0,0.014092,"erences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called “semantic fields” used to classify all nouns in WordNet (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011). The classification could be more fine-grained, as defined"
J13-3006,P00-1014,0,0.249246,"sparseness as one of the main reasons. In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results. Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days (Wilks 1975). Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models. The contributions of this work to the field of SRL are the following: 1. We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs. 2. We show that the selectional preference models are able to generalize lexical features and improve role classification performance in a controlled"
J13-3006,J08-2006,0,0.0842412,"Missing"
J13-3006,W09-1119,0,0.00868279,"th the Location role. The automatic acquisition of selectional preferences is a well-studied topic in NLP. Many methods using semantic classes and selectional preferences have been proposed and applied to a variety of syntactic–semantic ambiguity problems, including syntactic parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez 2008; Koo, Carreras, and Collins 2008; Agirre et al. 2011), word sense disambiguation (Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun resolution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and Roth 2009). In addition, selectional preferences have been shown to be effective to improve the quality of inference and information extraction rules (Pantel et al. 2007; Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not mention selectional preferences, but all of them use some notion of preferring certain semantic types over others in order to accomplish their respective task. In fact, one could use different notions of semantic types. In one extreme, we would have a small set of coarse semantic classes. For instance, some authors have used the 26 so-called “semantic fi"
J13-3006,H93-1054,0,0.155446,"rgument classification subtask, and suggested the lexical data sparseness as one of the main reasons. In this work, we will focus on Semantic Role Classification (SRC), and we will show that selectional preferences (SP) are useful for generalizing lexical features, helping fight sparseness and domain shifts, and improving SRC results. Selectional preferences try to model the kind of words that can fill a specific argument of a predicate, and have been widely used in computational linguistics since the early days (Wilks 1975). Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and distributional similarity based on corpora (Pantel and Lin 2000) have been successfully used for acquiring selectional preferences, and in this work we have used several of those models. The contributions of this work to the field of SRL are the following: 1. We formalize and implement a method that applies several selectional preference models to Semantic Role Classification, introducing for the first time the use of selectional preferences for prepositions, in addition to selectional preferences for verbs. 2. We show that the selectional preference models are able to generalize lexica"
J13-3006,P10-1044,0,0.130344,"Missing"
J13-3006,J98-1004,0,0.108058,"s co-occurring with w, and I(w, d, v) is the mutual information between w and d, v. 637 Computational Linguistics Volume 39, Number 3 that the corpus contains an occurrence of w having dependency d with v. For instance, if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set T for John. The measure uses information-theoretic principles, and I(w, d, v) represents the information content of the triple (Lin 1998). Although the use of co-occurrence vectors for words to compute similarity has been ¨ standard practice, some authors have argued for more complex uses. Schutze (1998) builds vectors for each context of occurrence of a word, combining the co-occurrence vectors for each word in the context. The vectors for contexts were used to induce senses and to improve information retrieval results. Edmonds (1997) built a lexical cooccurrence network, and applied it to a lexical choice task. Chakraborti et al. (2007) used transitivity over co-occurrence relations, with good results on several classification tasks. Note that all these works use second order and higher order to refer to their method. In this paper, we will also use second order to refer to a new method whi"
J13-3006,D11-1012,0,0.155516,"Missing"
J13-3006,P03-1002,1,0.820408,"s. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal . Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can benefit from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In M`arquez et al. (2008) the reader can find a broad introduction to SRL, covering several historical and definitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument ident"
J13-3006,J11-2003,1,0.89747,"Missing"
J13-3006,P09-2019,1,0.908931,"Missing"
J13-3006,N07-1070,0,\N,Missing
J13-3006,P10-1046,0,\N,Missing
J13-3006,P92-1053,0,\N,Missing
J13-3006,C98-2122,0,\N,Missing
J17-4001,D14-1188,0,0.0242496,"et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, a"
J17-4001,P13-2068,0,0.126108,"ot until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gr"
J17-4001,W16-2302,0,0.0340023,"Missing"
J17-4001,J93-2003,0,0.0452676,"Missing"
J17-4001,W07-0718,0,0.047654,"d cohesion. In Section 4, we have suggested some simple ways to create such metrics, and we have also shown that they yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metri"
J17-4001,W08-0309,0,0.0358878,"y yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez 2007; Popovic"
J17-4001,W10-1703,0,0.0860047,"Missing"
J17-4001,W12-3102,0,0.454393,"tion on the training data set.5 Note that our approach to learn the interpolation weights is similar to the one used by PRO for tuning the relative weights of the components of a log-linear SMT model (Hopkins and May 2011). Unlike PRO, (i) we used human judgments, not automatic scores, and (ii) we trained on all pairs, not on a subsample. 3.3 Correlation Measures In our experiments, we only considered translation into English (as we had a discourse parser for English only), and we used the data described in Table 1. For evaluation, we followed the standard set-up of the Metrics task of WMT12 (Callison-Burch et al. 2012). For segment-level evaluation, we used Kendall’s τ (Kendall 1938), which can be 5 When fitting the model, we did not include a bias term, as this was harmful. 692 Joty et al. Discourse Structure in Machine Translation Evaluation calculated directly from the human pairwise judgments. For system-level evaluation, we used Spearman’s rank correlation (Spearman 1904) and, in some cases, also Pearson correlation (Pearson 1895), which are appropriate correlation measures as here we have vectors of scores. We measured the correlation of the evaluation metrics with the human judgments provided by the"
J17-4001,W09-0401,0,0.0707378,"Missing"
J17-4001,W11-2103,0,0.0547186,"Missing"
J17-4001,E06-1032,0,0.0857549,"Missing"
J17-4001,W09-2404,0,0.0325022,"Discourse in Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2"
J17-4001,W12-3156,0,0.0221975,"Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational L"
J17-4001,D07-1007,0,0.126996,"Missing"
J17-4001,W11-1211,0,0.0252185,"010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despit"
J17-4001,P07-1005,0,0.124229,"Missing"
J17-4001,P05-1033,0,0.0907183,"systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-senten"
J17-4001,D08-1024,0,0.100033,"Missing"
J17-4001,2003.mtsummit-papers.9,0,0.0284114,"mmunity. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm"
J17-4001,2003.mtsummit-papers.10,0,0.151421,"iciently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b;"
J17-4001,W11-2107,0,0.0349766,"ally significant improvements are marked with ∗∗ for p-value < 0.01. System D ISCO TKparty Best at WMT 14 FR - EN DE - EN HI - EN CS - EN RU - EN Overall 0.433∗∗ 0.417 +0.016 0.380∗∗ 0.345 +0.035 0.434 0.438 −0.004 0.328∗∗ 0.284 +0.044 0.355∗∗ 0.336 +0.019 0.386∗∗ 0.364 +0.024 the best performing metric both at the system level and at the segment level at the WMT08 and WMT09 metrics tasks. From the original ULC, we replaced M ETEOR by the four newer variants M ETEOR-ex (exact match), M ETEOR-st (+stemming), M ETEOR-sy (+synonymy lookup), and M ETEOR-pa (+paraphrasing) in A SIYA’s terminology (Denkowski and Lavie 2011). We also added to the mix TERp-A (a variant of TER with paraphrasing), BLEU, NIST, and R OUGE-W, for a total of 18 individual metrics. The metrics in this set use diverse linguistic information, including lexical-, syntactic-, and semantic-oriented individual metrics. Regarding the discourse metrics, we used five variants, including DR and DR-LEX described in Section 2, and three more constrained variants oriented to match words between trees only if they occur under the same substructure types (e.g., the same nuclearity type). These variants are designed by introducing structural modificatio"
J17-4001,2012.amta-papers.6,0,0.0150667,"a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovi"
J17-4001,N04-1035,0,0.0647715,"nformation (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual informa"
J17-4001,W07-0738,1,0.707228,"Missing"
J17-4001,W09-0440,1,0.876974,"Missing"
J17-4001,H93-1040,0,0.672723,"Missing"
J17-4001,W10-1750,1,0.808317,"Missing"
J17-4001,D11-1084,0,0.0630017,"Missing"
J17-4001,P12-3024,1,0.890983,"Missing"
J17-4001,E12-3001,0,0.0251663,"istency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. th"
J17-4001,W13-3302,0,0.0210552,"; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling"
J17-4001,W16-2345,1,0.883757,"Missing"
J17-4001,D14-1027,1,0.899067,"Missing"
J17-4001,P14-1065,1,0.865351,"Missing"
J17-4001,P15-1078,1,0.877827,"Missing"
J17-4001,W13-2252,0,0.0241376,"ch is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 2013). Thus, specialized evaluation measures have been proposed, for example, for the translation of discourse connectives (Hajlaoui and Popescu-Belis 2012; Meyer et al. 2012; Hajlaoui 2013) and for pronominal anaphora (Hardmeier and Federico 2010), among others. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few previous attempts to incorporate discourse information. One example includes the semantics-aware metrics of Gim´enez and M`arquez (2009) and Gim´enez et al. (2010), which used the Discourse Representation Theory (Kamp and Reyle 1993) and tree-based discourse representation structures (DRS) produced by a semantic parser. They calculated the similarity between the MT output and the references based on DRS subtree matching as defi"
J17-4001,2012.amta-caas14.1,0,0.0723287,"gt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT"
J17-4001,2010.iwslt-papers.10,0,0.146919,"esearch problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Har"
J17-4001,W15-2501,1,0.905816,"Missing"
J17-4001,D12-1108,0,0.0554427,"Missing"
J17-4001,D11-1125,0,0.347504,"es 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently tha"
J17-4001,D12-1083,1,0.888624,"Missing"
J17-4001,J15-3002,1,0.897197,"Missing"
J17-4001,W14-3352,1,0.889983,"Missing"
J17-4001,N03-1017,0,0.0235221,"Missing"
J17-4001,W07-0734,0,0.0592291,"the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of li"
J17-4001,W10-1737,0,0.0859654,"Missing"
J17-4001,D14-1220,0,0.0691207,"Missing"
J17-4001,P14-2047,0,0.0404142,"Missing"
J17-4001,W04-1013,0,0.011816,"ation groups: Group I contains our discourse-based evaluation metrics, DR, and DR-LEX. Group II includes the publicly available MT evaluation metrics that participated in the WMT12 metrics task, excluding those that did not have results for all language pairs (Callison-Burch et al. 2012). More precisely, they are SPEDE 07 P P, AMBER, M ETEOR, T ERROR C AT, SIMPBLEU, XE N E RR C ATS, W ORD B LOCK EC, B LOCK E RR C ATS, and POS F. Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (Papineni et al. 2002), NIST (Doddington 2002), R OUGE (Lin 2004), and TER (Snover et al. 2006). We calculated the metrics in this group using Asiya. In particular, we used the following Asiya versions of TER and R OUGE: TER P -A and ROUGE- W.8 For each metric in groups II and III, we present the system-level and segment-level results for the original metric as well as for the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results y"
J17-4001,W05-0904,0,0.34905,"post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sen"
J17-4001,W12-3129,0,0.0531251,"Missing"
J17-4001,P11-1023,0,0.0237776,"between constituency trees (Liu and Gildea 2005). In the semantic case, there are metrics that 16 A notable exception is the work of Tu, Zhou, and Zong (2013), who report up to 2.3 BLEU points of improvement for Chinese-to-English translation using an RST-based MT framework. 17 http://www.isi.edu/natural-language/mteval/. 18 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/. 712 Joty et al. Discourse Structure in Machine Translation Evaluation exploit the similarity over named entities, predicate–argument structures (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), or semantic frames (Lo and Wu 2011). Finally, there are metrics that combine several lexico-semantic aspects (Gim´enez and M`arquez 2010b). As we mentioned earlier, one problem with discourse-related MT research is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 20"
J17-4001,E14-1017,0,0.0578767,"that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the"
J17-4001,W13-2202,0,0.0551203,"Missing"
J17-4001,W14-3336,0,0.0496545,"Missing"
J17-4001,A00-2002,0,0.288346,"Missing"
J17-4001,P11-3009,0,0.0273654,"ang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the resear"
J17-4001,W13-3306,0,0.032096,"Missing"
J17-4001,W12-0117,0,0.023287,"Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate"
J17-4001,2012.amta-papers.20,0,0.112933,"u 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far"
J17-4001,W13-3303,0,0.0173794,"Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that c"
J17-4001,W13-2221,0,0.0481696,"Missing"
J17-4001,W13-3307,0,0.04636,"Missing"
J17-4001,P03-1021,0,0.159662,"Missing"
J17-4001,P02-1040,0,0.0997549,"been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has"
J17-4001,P09-2004,0,0.0245932,"al cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Mac"
J17-4001,popescu-belis-etal-2012-discourse,0,0.0540549,"Missing"
J17-4001,W07-0707,0,0.0583959,"2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sentence and between sentences in a text have not been"
J17-4001,prasad-etal-2008-penn,0,0.0680612,"man assessments; (ii) Different levels of discourse structure and relations provide different information, which shows smooth accumulative contribution to the final correlation score; (iii) Both discourse relations and nuclearity labels have sizeable impact on the evaluation metric, the latter being more important than the former. The last point emphasizes the appropriateness of the RST theory as a formalism for the discourse structure of texts. Contrary to other discourse theories (e.g., the Discourse Lexicalized Tree Adjoining Grammar [Webber 2004] used to build the Penn Discourse Treebank [Prasad et al. 2008]), RST accounts for the nuclearity as an important element of the discourse structure. 5.3 Qualitative Analysis of Good and Bad Translations In the previous two sections we provided a quantitative analysis of which discourse information has the biggest impact on the performance of our discourse-based measure (DR-LEX) and also which parts of the discourse trees help in distinguishing good from bad translations. In this section, we present some qualitative analysis by inspecting a 707 Computational Linguistics Volume 43, Number 4 WMT_2011 WMT_2012 WMT_2013 F1 measure 0.825 0.800 good bad 0.775"
J17-4001,P05-1034,0,0.192756,"Missing"
J17-4001,W03-0402,0,0.0123992,"urces of information in a more direct way. In that paper, we proposed a pairwise setting for learning MT evaluation metrics with preference tree kernels. The setting can incorporate syntactic and discourse information encapsulated in tree-based structures and the objective is to learn to differentiate better from worse translations by using all subtree structures as implicit features. The discourse parser we used is the same used in this article. The syntactic tree is mainly constructed using the Illinois chunker (Punyakanok and Roth 2001). The kernel used for learning is a preference kernel (Shen and Joshi 2003; Moschitti 2008), which decomposes into Partial Tree Kernel (Moschitti 2006) applications between pairs of enriched tree structures. Word unigram matching is also included in the kernel computation, thus being quite similar to DR-LEX. Table 8 shows the results obtained on the same WMT12 data set by using only discourse structures, only syntactic structures or both structures together. As we can see, the τ scores of the syntactic and the discourse variants are not very different (with a general advantage for syntax), but when put together there is a sizeable improvement in correlation for all"
J17-4001,2006.amta-papers.25,0,0.422386,"it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are u"
J17-4001,W15-3031,0,0.0560843,"Missing"
J17-4001,N15-2015,0,0.019606,"d Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that current automatic evaluation metrics such as B"
J17-4001,W12-4213,0,0.0584238,"Missing"
J17-4001,W10-2602,0,0.0143272,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,W10-1728,0,0.0292312,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,P13-2066,0,0.0534853,"Missing"
J17-4001,P14-1080,0,0.0440631,"Missing"
J17-4001,N12-1046,0,0.0614062,"Missing"
J17-4001,H05-1097,0,0.0561496,"creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and"
J17-4001,W12-2503,0,0.0257687,"ch had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu"
J17-4001,D07-1080,0,0.245176,"Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan"
J17-4001,D12-1097,0,0.0277845,"Missing"
J17-4001,N09-2004,0,0.0308979,"ive log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion an"
J17-4001,D13-1163,0,0.01503,"which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´a"
J17-4001,C00-2137,0,0.0572188,"r the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results yield statistically significant improvement over the original metric. Note that testing statistical significance is not trivial in our case because we have a complex correlation score for which the assumptions that standard tests make are not met. We thus resorted to a non-parametric randomization framework (Yeh 2000), which is commonly used in NLP research.9 4.1 System-Level Results Table 2 shows the system-level experimental results for WMT12. We can see that DR is already competitive by itself: On average, it has a correlation of 0.807, which is very close to the BLEU and the TER scores from group II (0.810 and 0.812, respectively). Moreover, DR yields improvements when combined with 13 of the 15 metrics, with a resulting correlation higher than those of the two individual metrics being combined. This fact suggests that DR contains information that is complementary to that used by most of the other metr"
J17-4001,W14-3302,0,\N,Missing
K17-1024,abdelali-etal-2014-amara,0,0.035599,"Missing"
K17-1024,S16-1081,0,0.0323627,"els. More relevant to our work is the work of Ganin et al. (2016), who proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target. Here, we use adversarial training to learn task-specific representations in a cross-language setting, which is novel for this task, to the best of our knowledge. Question-question similarity was part of Task 3 on cQA at SemEval-2016/2017 (Nakov et al., 2016b, 2017); there was also a similar subtask as part of SemEval-2016 Task 1 on Semantic Textual Similarity (Agirre et al., 2016). Question-question similarity is an important problem with application to question recommendation, question duplicate detection, community question answering, and question answering in general. Typically, it has been addressed using a variety of textual similarity measures. Some work has paid attention to modeling the question topic, which can be done explicitly, e.g., using a graph of topic terms (Cao et al., 2008), or implicitly, e.g., using LDA-based topic language model that matches the questions not only at the term level but also at the topic level (Zhang et al., 2014). Another importan"
K17-1024,P15-2114,0,0.0838445,"rk has paid attention to modeling the question topic, which can be done explicitly, e.g., using a graph of topic terms (Cao et al., 2008), or implicitly, e.g., using LDA-based topic language model that matches the questions not only at the term level but also at the topic level (Zhang et al., 2014). Another important aspect is syntactic structure, e.g., Wang et al. (2009) proposed a retrieval model for finding similar questions based on the similarity of syntactic trees, and Da San Martino et al. (2016) used syntactic kernels. Yet another emerging approach is to use neural networks, e.g., dos Santos et al. (2015) used convolutional neural networks (CNNs), Romeo et al. (2016) used long short-term memory (LSTMs) networks with neural attention to select the important part when comparing two questions, and Lei et al. (2016) used a combined recurrent–convolutional model to map questions to continuous semantic representations. Finally, translation models have been popular for question-question similarity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) us"
K17-1024,P03-1003,0,0.017696,"ity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English)"
K17-1024,P15-1078,1,0.559126,"Missing"
K17-1024,P16-2075,1,0.644004,"Missing"
K17-1024,D15-1068,1,0.173955,"Missing"
K17-1024,N16-1084,1,0.911664,"Missing"
K17-1024,N16-1153,1,0.473014,"Missing"
K17-1024,W15-1521,0,0.0266254,"of related information is only in English. Here, we adapt the idea for adversarial training for domain adaptation as proposed by Ganin et al. (2016). Figure 2 shows the architecture of our crosslanguage adversarial neural network (CLANN) model. The input to the network is a pair (q, q 0 ), which is first mapped to fixed-length vectors (zq , zq0 ). To generate these word embeddings, one can use existing tools such as word2vec (Mikolov et al., 2013) and monolingual data from the respective languages. Alternatively, one can use crosslanguage word embeddings, e.g., trained using the bivec model (Luong et al., 2015). The latter can yield better initialization, which could be potentially crucial when the labeled data is too small to train the input representations with the end-to-end system. 228 ! ! (new;&ar/en)& ! q ! zq ! ! zq’ ! ! (related;&en)& ! q’ BP& h f ! ! ! ! ! ! c hl gradient& reversal& /λ(grad(Ll)& φ""( q ,q’ ) ! ! input& interaction&& embeddings& layer& features& class&label& ! ! l ! language&label& λ(grad(Ll)& loss$Ll& Figure 2: Architecture of CLANN for the question to question similarity problem in cQA. The network then models the interactions between the input embeddings by passing them th"
K17-1024,S16-1136,1,0.400568,"dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained on the TED and on the OPUS bi-texts; for language modeling, it also used the English Gigaword corpus. We further used as features the cosine similarity between question embeddings. In particular, we used (i) 300-dimensional pre-trained Google News embeddings from (Mikolov et al., 2013), (ii) 100dimensional embeddings trained on the entire Qatar Living forum (Mihaylov and Nakov, 2016), and (iii) 25-dimensional Stanford neural parser embeddings (Socher et al., 2013). The latter are produced by the parser internally, as a by-product. Furthermore, we computed various task-specific features, most of them introduced in the 2015 edition of the SemEval task by (Nicosia et al., 2015; Joty et al., 2015). This includes some question-level features: (1) number of URLs/images/emails/phone numbers; (2) number of tokens/sentences; (3) average number of tokens; (4) type/token ratio; (5) number of nouns/verbs/adjectives/adverbs/ pronouns; (6) number of positive/negative smileys; (7) numbe"
K17-1024,N13-1090,0,0.139962,"ify any test example {qn , qn,k n in Arabic. This scenario is of practical importance, e.g., when an Arabic speaker wants to query the system in Arabic, and the database of related information is only in English. Here, we adapt the idea for adversarial training for domain adaptation as proposed by Ganin et al. (2016). Figure 2 shows the architecture of our crosslanguage adversarial neural network (CLANN) model. The input to the network is a pair (q, q 0 ), which is first mapped to fixed-length vectors (zq , zq0 ). To generate these word embeddings, one can use existing tools such as word2vec (Mikolov et al., 2013) and monolingual data from the respective languages. Alternatively, one can use crosslanguage word embeddings, e.g., trained using the bivec model (Luong et al., 2015). The latter can yield better initialization, which could be potentially crucial when the labeled data is too small to train the input representations with the end-to-end system. 228 ! ! (new;&ar/en)& ! q ! zq ! ! zq’ ! ! (related;&en)& ! q’ BP& h f ! ! ! ! ! ! c hl gradient& reversal& /λ(grad(Ll)& φ""( q ,q’ ) ! ! input& interaction&& embeddings& layer& features& class&label& ! ! l ! language&label& λ(grad(Ll)& loss$Ll& Figure 2:"
K17-1024,S17-2003,1,0.046657,"Missing"
K17-1024,D16-1165,1,0.895205,"Missing"
K17-1024,S16-1083,1,0.894404,"Missing"
K17-1024,S15-2036,1,0.775796,"Missing"
K17-1024,P02-1040,0,0.117239,"to English potentially related questions qi0 . 4.3 Pairwise Features In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation between the input question and the potentially related questions.3 These features were proposed in the previous literature for the question– question similarity problem, and they are necessary to obtain state-of-the-art results. In particular, we calculated the similarity between the two questions using machine translation evaluation metrics, as suggested in (Guzm´an et al., 2016). In particular, we used B LEU (Papineni et al., 2002); NIST (Doddington, 2002); TER v0.7.25 (Snover et al., 2006); M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; Unigram P RECISION; Unigram R ECALL. We also used features that model various components of B LEU, as proposed in (Guzm´an et al., 2015): n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), hypothesis and reference length, length ratio, and brevity penalty. 2 Our cross-language dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English p"
K17-1024,W16-1403,0,0.021532,"etup is useful for many real-world applications. One expensive solution is to annotate data for each input language and then to train a separate system for each one. Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language (Hartrumpf et al., 2008; Lin and Kuo, 2010; Ture and Boschee, 2016). However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (Mohammad et al., 2016), or may not be really needed (Bouma et al., 2008; Pouran Ben Veyseh, 2016). Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing. In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA), when the input question can be either in English or in Arabic, and the questions it is compared to are always in English. We start with a simple language-independent representation based on cross-language word embedd"
K17-1024,P07-1059,0,0.0110691,"that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English). The goal is to rerank a set of K retrieved qu"
K17-1024,P13-1045,0,0.0193858,"nslating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained on the TED and on the OPUS bi-texts; for language modeling, it also used the English Gigaword corpus. We further used as features the cosine similarity between question embeddings. In particular, we used (i) 300-dimensional pre-trained Google News embeddings from (Mikolov et al., 2013), (ii) 100dimensional embeddings trained on the entire Qatar Living forum (Mihaylov and Nakov, 2016), and (iii) 25-dimensional Stanford neural parser embeddings (Socher et al., 2013). The latter are produced by the parser internally, as a by-product. Furthermore, we computed various task-specific features, most of them introduced in the 2015 edition of the SemEval task by (Nicosia et al., 2015; Joty et al., 2015). This includes some question-level features: (1) number of URLs/images/emails/phone numbers; (2) number of tokens/sentences; (3) average number of tokens; (4) type/token ratio; (5) number of nouns/verbs/adjectives/adverbs/ pronouns; (6) number of positive/negative smileys; (7) number of single/double/ triple exclamation/interrogation symbols; (8) number of interr"
K17-1024,J11-2003,0,0.0117402,"r question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English). The goal is to rerank a set of K retrieved questions {qk0 }K k=1 written in a source language (e.g., English) a"
K17-1024,tiedemann-2012-parallel,0,0.00637833,"ith the goal to rerank the ten related English questions. As an example, this is the Arabic translation of the original English question from Figure 1:    K Éë ; à AË@ @ YîE. àñÊª®K @ XAÓ ? HAJ Ó@Q»B@ àñ¢ª . Jj . K AÓ èñ¯ ñë AÓ , ÑªK éK . Ag. B@ I KA¿ @ X@ ? éKñJ . ú¯ QK , AKXCK . éJÖ Ï AK. 20 úÍ@ 15 áÓ We further collected 221 additional original questions and 1,863 related questions as unlabeled data, and we got the 221 English questions translated to Arabic.2 4.2 Cross-language Embeddings We used the TED (Abdelali et al., 2014) and the OPUS parallel Arabic–English bi-texts (Tiedemann, 2012) to extract a bilingual dictionary, and to learn cross-language embeddings. We chose these bi-texts as they are conversational (TED talks and movie subtitles, respectively), and thus informal, which is close to the style of our community question answering forum. We trained Arabic-English cross-language word embeddings from the concatenation of these bitexts using bivec (Luong et al., 2015), a bilingual extension of word2vec, which has achieved excellent results on semantic tasks close to ours (Upadhyay et al., 2016). In particular, we trained 200dimensional vectors using the parameters descri"
K17-1024,D16-1055,0,0.100723,"neural network (CLANN) model over a strong nonadversarial system. 1 Introduction Developing natural language processing (NLP) systems that can work indistinctly with different input languages is a challenging task; yet, such a setup is useful for many real-world applications. One expensive solution is to annotate data for each input language and then to train a separate system for each one. Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language (Hartrumpf et al., 2008; Lin and Kuo, 2010; Ture and Boschee, 2016). However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (Mohammad et al., 2016), or may not be really needed (Bouma et al., 2008; Pouran Ben Veyseh, 2016). Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing. In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA"
K17-1024,P16-1157,0,0.018411,"used the TED (Abdelali et al., 2014) and the OPUS parallel Arabic–English bi-texts (Tiedemann, 2012) to extract a bilingual dictionary, and to learn cross-language embeddings. We chose these bi-texts as they are conversational (TED talks and movie subtitles, respectively), and thus informal, which is close to the style of our community question answering forum. We trained Arabic-English cross-language word embeddings from the concatenation of these bitexts using bivec (Luong et al., 2015), a bilingual extension of word2vec, which has achieved excellent results on semantic tasks close to ours (Upadhyay et al., 2016). In particular, we trained 200dimensional vectors using the parameters described in (Upadhyay et al., 2016), with a context window of size 5 and iterating for 5 epochs. We then compute the representation for a question by averaging the embedding vectors of the words it contains. Using these cross-language embeddings allows us to compare directly representations of an Arabic or an English input question q to English potentially related questions qi0 . 4.3 Pairwise Features In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation betwee"
K17-1024,2006.amta-papers.25,0,0.0426642,"atures In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation between the input question and the potentially related questions.3 These features were proposed in the previous literature for the question– question similarity problem, and they are necessary to obtain state-of-the-art results. In particular, we calculated the similarity between the two questions using machine translation evaluation metrics, as suggested in (Guzm´an et al., 2016). In particular, we used B LEU (Papineni et al., 2002); NIST (Doddington, 2002); TER v0.7.25 (Snover et al., 2006); M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; Unigram P RECISION; Unigram R ECALL. We also used features that model various components of B LEU, as proposed in (Guzm´an et al., 2015): n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), hypothesis and reference length, length ratio, and brevity penalty. 2 Our cross-language dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained o"
K17-1024,P11-1066,0,0.013245,"sed on the similarity of syntactic trees, and Da San Martino et al. (2016) used syntactic kernels. Yet another emerging approach is to use neural networks, e.g., dos Santos et al. (2015) used convolutional neural networks (CNNs), Romeo et al. (2016) used long short-term memory (LSTMs) networks with neural attention to select the important part when comparing two questions, and Lei et al. (2016) used a combined recurrent–convolutional model to map questions to continuous semantic representations. Finally, translation models have been popular for question-question similarity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Bril"
karadzhov-etal-2017-fully,W01-0515,0,\N,Missing
karadzhov-etal-2017-fully,N09-2040,0,\N,Missing
karadzhov-etal-2017-fully,D14-1162,0,\N,Missing
karadzhov-etal-2017-fully,P16-2065,1,\N,Missing
karadzhov-etal-2017-fully,Y10-1062,0,\N,Missing
moreno-etal-2006-generation,cucchiarini-dhalleweyn-2004-new,0,\N,Missing
N10-1058,W05-0620,1,0.899021,"Missing"
N10-1058,A00-2018,0,0.0292806,"Missing"
N10-1058,P07-1028,0,0.14412,"t parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1 http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between"
N10-1058,J02-3001,0,0.663127,"Classification SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1 http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, dependi"
N10-1058,P98-2127,0,0.0232366,"two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words. Co-occurrence vectors where constructed using freely available software (Pad´o and Lapata, 2007) run over the British National Corpus. We used the optimal parameters (Pad´o and Lapata, 2007, p. 179). We will refer to these similarities as simcos and simJac , respectively. In contrast, second order similarity uses vectors of similar words, i.e., the similarity of two words was computed using the cosine (or Jaccard measure) between the thesaurus entries of those words in Lin’s thesaurus (Lin, 1998). We refer to these as sim2cos and sim2Jac . Given a target sentence with a verb and its arguments, the task of SR classification is to assign the correct role to each of the arguments. When using SPs alone, we only use the headwords of the ar374 guments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SPsim (v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selec"
N10-1058,J07-2002,0,0.0496725,"Missing"
N10-1058,N07-1070,0,0.0906038,"that all systems showed a significant performance degradation (∼10 F1 points) when applied to test data from a different genre of that of the training Mihai Surdeanu Stanford NLP Group Stanford Univ. mihais@stanford.edu set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (Zapirain et al., 2009) in two directions: (1) We learn separate SPs for prepo"
N10-1058,J08-2006,0,0.183777,"count. Semantic information is usually captured through lexicalized features on the predicate and the head–word of the argument to be classified. Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora. Indeed, the SRL evaluation exercises at CoNLL2004 and 2005 (Carreras and M`arquez, 2005) observed that all systems showed a significant performance degradation (∼10 F1 points) when applied to test data from a different genre of that of the training Mihai Surdeanu Stanford NLP Group Stanford Univ. mihais@stanford.edu set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected fr"
N10-1058,H93-1054,0,0.563865,"Missing"
N10-1058,P09-2019,1,0.279698,"Missing"
N10-1058,C98-2122,0,\N,Missing
N15-1121,C10-3009,0,0.640623,"Missing"
N15-1121,W09-1207,0,0.306018,"function by combining a traditional feature scoring function with a tensor-based scoring function. 2 https://github.com/taolei87/RBGParser 1152 Predicate word Predicate POS Argument word Argument POS Pred. + arg. words Pred. + arg. POS Voice + pred. word Path Path + arg. POS Path + pred. POS Path + arg. word Path + pred. word Voice + pred. + arg. POS Voice + pred. POS Table 1: Templates for first-order semantic features. These features are also (optionally) combined with role labels. 3.1 Traditional Scoring Using Manually-designed Features In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure. The score Ssem (x, ysyn , zsem ) is then defined as the inner product between the parameter vector and the feature vector. In the first-order arc-factored case, Ssem (x, ysyn , zsem ) = w · φ(x, ysyn , zsem ) X = w · φ(p, a, r), (p,a,r)∈zsem where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn ). We also experiment with second order features, i.e., considering two arguments associated with the same predicat"
N15-1121,D09-1003,0,0.317743,"Missing"
N15-1121,W09-1205,0,0.0722058,"main test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full model utilizes 4-way tensor"
N15-1121,J02-3001,0,0.656237,"Zhao et al., 2009a). On three out of five languages, the tensor-based model outperforms this system. These results are particularly notable because the system of Zhao et al. (2009a) employs a rich set of language-specific features carefully engineered for this task. Finally, we demonstrate that using four-way tensor yields better performance than its three-way counterpart, highlighting the importance of modeling the relation between role labels and properties of the path. 2 Related Work A great deal of SRL research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among othe"
N15-1121,D09-1059,0,0.335342,"uild our scoring function by combining a traditional feature scoring function with a tensor-based scoring function. 2 https://github.com/taolei87/RBGParser 1152 Predicate word Predicate POS Argument word Argument POS Pred. + arg. words Pred. + arg. POS Voice + pred. word Path Path + arg. POS Path + pred. POS Path + arg. word Path + pred. word Voice + pred. + arg. POS Voice + pred. POS Table 1: Templates for first-order semantic features. These features are also (optionally) combined with role labels. 3.1 Traditional Scoring Using Manually-designed Features In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure. The score Ssem (x, ysyn , zsem ) is then defined as the inner product between the parameter vector and the feature vector. In the first-order arc-factored case, Ssem (x, ysyn , zsem ) = w · φ(x, ysyn , zsem ) X = w · φ(p, a, r), (p,a,r)∈zsem where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn ). We also experiment with second order features, i.e., considering two arguments associated wit"
N15-1121,P14-1130,1,0.649857,"re vectors that capture distinct facets of semantic dependence: predicate, argument, syntactic path and role label. By compressing this sparse representation into lower dimensions, we obtain dense representations for words (predicate, argument) and their connecting paths, uncovering meaningful interactions. The associated parameters are maintained as a four-way low-rank tensor, and optimized for SRL performance. Tensor modularity enables us to employ standard online algorithms for training. Our approach to SRL is inspired by recent success of our tensor-based approaches in dependency parsing (Lei et al., 2014). Applying analogous techniques to SRL brings about new challenges, however. The scoring function needs to reflect the highorder interactions between the predicate, argument, 1150 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1150–1160, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics their syntactic path and the corresponding role label. Therefore, we parametrize the scoring function as a four-way tensor. Generalization to high-order tensors also requires new initialization and update procedures"
N15-1121,Q13-1018,1,0.900111,"Missing"
N15-1121,S14-2082,0,0.0478503,"s, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computation"
N15-1121,J08-2003,1,0.868522,"dhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering. Finally, system combination approaches such as reranking typically outperform individual systems (Bj¨orkelund et al., 2010). Our method can be easily integrated as a component in one of those systems. In technical terms, our work builds on our recent tensor-based approach for dependency parsin"
N15-1121,D14-1045,0,0.684785,"tic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering. Finally, system combination approaches such as reranking typically outperform individual systems (Bj¨orkelund et al., 2010). Our method can be easily integrated as a component in one of those systems. In techn"
N15-1121,P03-1002,0,0.0928285,"Work A great deal of SRL research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature en"
N15-1121,W08-2121,1,0.852627,"Missing"
N15-1121,J08-2002,0,0.148737,"eady identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach c"
N15-1121,W04-3212,0,0.296176,"L research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in"
N15-1121,D14-1041,0,0.0571193,"re widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to p"
N15-1121,C00-2137,0,0.0458716,"Missing"
N15-1121,D14-1109,1,0.838822,"is a very simple iterative algorithm and is used to find the largest eigenvalues and eigenvectors (or singular values and vectors in SVD case) of a matrix. Its generalization directly applies to our high-order tensor case. 5 Implementation Details Decoding Following Llu´ıs et al. (2013), the decoding of SRL is formulated as a bipartite maximum assignment problem, where we assign arguments to semantic roles for each predicate. We use the maximum weighted assignment algorithm (Kuhn, 1955). For syntactic dependency parsing, we employ the randomized hill-climbing algorithm from our previous work (Zhang et al., 2014). 1155 Input: sparse tensor T , rank number i and fixed rank-1 components P (j), Q(j), R(j) and S(j) for j = 1..(i − 1) Output: new component P (i), Q(i), R(i) and S(i). 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: Randomly initialize four unit vectors p, q, r and s P T 0 = T − j P (j) ⊗ Q(j) ⊗ R(j) ⊗ S(j) repeat p = hT 0 , −, q, r, si and normalize it q = hT 0 , p, −, r, si and normalize it r = hT 0 , p, q, −, si and normalize it s = hT 0 , p, q, r, −i norm = ksk22 until norm converges P (i) = p and Q(i) = q R(i) = r and S(i) = s Figure 2: The iterative power method for highorder tensor initialization."
N15-1121,W09-1209,0,0.434698,"secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full mo"
N15-1121,W09-1208,0,0.304639,"secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full mo"
N15-1121,J13-3006,1,\N,Missing
N15-1121,W09-1201,1,\N,Missing
N16-1084,P11-1151,0,0.145112,"ize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored"
N16-1084,W02-1001,0,0.414237,"inference is intractable for general graphs, i.e., graphs with loops. Despite this, it has been advocated by Pearl (1988) to use BP in loopy graphs as an approximation scheme; see also (Murphy, 2012), page 768. The algorithm is then called “loopy” BP, or LBP. Although LBP gives approximate solutions for general graphs, it often works well in practice (Murphy et al., 1999), outperforming other methods such as mean field (Weiss, 2001) and graph-cut (Burfoot et al., 2011). It is important to mention that the approach presented above (i.e., subsection 3.1) is similar in spirit to the approach of Collins (2002), Carreras and M`arquez (2003) and Punyakanok et al. (2005). The main difference is that they use a Perceptron-like online algorithm, where the updates are done based on the best label configuration (i.e., argmaxy p(y|x, θ)) rather than the marginals. One can use graph-cut (applicable only for binary output variables) or max-product LBP for the decoding task. However, this yields a discontinuous estimate (even with averaged perceptron) for the gradient (see Section 5). For the same reason, we use sum-product LBP rather than max-product LBP. 707 3.2 A Joint Model with Global Normalization Altho"
N16-1084,P15-2114,0,0.0916561,"hibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 )."
N16-1084,P08-1019,0,0.0307759,", in your talking baout airsoft i think its prohibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) fo"
N16-1084,S15-2035,0,0.0824082,"of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classification in cQA, we find some work exploiting threadlevel information. Hou et al. (2015) used features about the position of the comment in the thread. Barr´on-Cede˜no et al. (2015) developed more elaborated global features to model thread structure and the interaction among users. Other work exploited global inference algorithms at the thread-level. For instance, (Zhou et al., 2015c; Zhou et al., 2015b; Barr´on-Cede˜no et al., 2015) treated the task as sequential classification, using a variety of machine learning algorithms to label the sequence of timesorted comments: LSTMs, CRFs, SVMhmm , etc. Finally, Joty et al. (2015) showed that exploiting the pairwise relations between c"
N16-1084,D15-1068,1,0.636014,"Missing"
N16-1084,P11-1143,0,0.118538,"aout airsoft i think its prohibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments"
N16-1084,S15-2047,1,0.844443,"Missing"
N16-1084,S15-2036,1,0.602545,"Missing"
N16-1084,P04-1035,0,0.0140909,"CRF model improves results significantly over all rivaling models, yielding the best results on the task to date. In the remainder of this paper, after discussing related work in Section 2, we introduce our joint models in Section 3. We then describe our experimental settings in Section 4. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to"
N16-1084,W00-0721,0,0.0961383,"ify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classi"
N16-1084,C04-1197,0,0.0586523,"a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classification in cQA, we find some work exploiting thre"
N16-1084,W04-2401,0,0.0624102,"-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a percept"
N16-1084,W06-1639,0,0.0569981,"Section 2, we introduce our joint models in Section 3. We then describe our experimental settings in Section 4. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction prob"
N16-1084,P15-1025,0,0.200763,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1084,P15-2117,0,0.206056,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1084,S15-2037,0,0.113962,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1153,P13-4021,0,0.0197328,"Missing"
N16-1153,D15-1075,0,0.0326755,"summary of the noisy body, and the encoder-decoder model is trained to act as a denoising auto-encoder. Moreover, training a decoder for the title (rather than the body) is also much faster since titles tend to be short (around 10 words). The encoders pre-trained in this manner are subsequently fine-tuned according to the discriminative criterion described already in Section 3. 5 LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2015; Bowman et al., 2015; Rockt¨aschel et al., 2016). Their success can be attributed to neural gates that adaptively read or discard information to/from internal memory states. Specifically, a LSTM network successively reads the input token xt , internal state ct−1 , as well as the visible state ht−1 , and generates the new states c t , ht : it = σ(Wi xt + Ui ht−1 + bi ) ft = σ(Wf xt + Uf ht−1 + bf ) ot = σ(Wo xt + Uo ht−1 + bo ) zt = tanh(Wz xt + Uz ht−1 + bz ) ct = it zt + ft ct−1 ht = ot tanh(ct ) where i, f and o are input, forget and output gates, respectively. Given the visible state sequence {hi }li=1 , we ca"
N16-1153,W14-4012,0,0.00502985,"Missing"
N16-1153,D14-1179,0,0.0156321,"Missing"
N16-1153,P15-2114,0,0.281941,"al., 2016). Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata information for questions. They define each question as a distribution which generates each word (embedding) independently, and subsequently use a Fisher kernel to assess question similarities. Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bagof-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations. Further, we propose a training paradigm that utilizes the entire corpus of unannotated questions in a semi-supervised manner. Recent work on answer selection on community QA forums, simila"
N16-1153,P08-1019,0,0.0102075,"Missing"
N16-1153,D14-1002,0,0.00488573,"mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exchange AskUbuntu dataset. and associated filters map local chunks (win"
N16-1153,D13-1176,0,0.0134402,"r to focus temporal averaging in these models on key pieces of the questions. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost. 3 http://askubuntu.com/ 1280 We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for development and"
N16-1153,P14-1062,0,0.00406558,"e also train three alternative benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exch"
N16-1153,D14-1181,0,0.0115394,"ive benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exchange AskUbu"
N16-1153,D15-1180,1,0.218935,"Missing"
N16-1153,P11-1143,0,0.02256,"Missing"
N16-1153,S15-2047,1,0.46169,"Missing"
N16-1153,S16-1083,1,0.108575,"Missing"
N16-1153,D15-1044,0,0.015101,"s. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost. 3 http://askubuntu.com/ 1280 We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for development and hyper parameter tuning and another for testing. We evaluate our"
N16-1153,P15-2116,0,0.0315029,"Missing"
N16-1153,P15-1025,0,0.0509211,"Missing"
N18-1070,N18-2004,1,0.808385,"ores into a single snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 20"
N18-1070,D17-1317,0,0.0837272,"its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on t"
N18-1070,gencheva-etal-2017-context,1,0.834798,"aluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to so"
N18-1070,N18-5006,1,0.775642,"and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition,"
N18-1070,P16-1044,0,0.326197,"n. T imeDistributed(LST M ) (X, W, E) −−−−−−−−−−−−−−−−→ {m1 , ..., mn } (2) 768 Figure 2: The architecture of our memory network model for stance detection. Furthermore, we convert each input claim s into its representation using the corresponding LSTM and CNN networks as follows: where mj is the LSTM representation of xj , and TimeDistributed() indicates a wrapper that enables training the LSTM over all pieces of evidence by applying the same LSTM model to each time-step of a 3D input tensor, i.e., (X, W, E). While LSTM networks were designed to effectively capture and memorize their inputs (Tan et al., 2016), Convolutional Neural Networks (CNNs) emphasize the local interaction between the individual words in the input word sequence, which is important for obtaining an effective representation. Here, we use a CNN in order to encode each xj into its representation cj as shown below (see line 13 in Table 1). LST M,CN N s −−−−−−−−→ slstm , scnn (4) where slstm and scnn are the representations of s computed using LST M and CN N networks, respectively. Note that these are separate networks with different parameters from those used to encode the pieces of evidence. Lines 10–14 of Table 1 describe the ab"
N18-1070,karadzhov-etal-2017-fully,1,0.797992,"be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model"
N18-1070,N18-1074,0,0.0508096,"snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers t"
N18-1070,K15-1032,1,0.705898,"xtension of the general architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stan"
N18-1070,W14-2508,0,0.13385,"s we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have d"
N18-1070,P16-2065,1,0.613587,"architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stand-alone task. The task aims"
N18-1070,S16-1074,0,0.0172524,"ylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the"
N18-1070,S16-1003,0,0.260879,"Missing"
N18-1070,C16-1230,0,0.034862,"7; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the stance detection problem (Baird et a"
N18-1070,D14-1162,0,0.0795601,"Missing"
N18-1070,N09-2040,0,\N,Missing
N18-2004,karadzhov-etal-2017-built,1,0.798906,"st of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised"
N18-2004,N09-2040,0,0.0986266,"news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Po"
N18-2004,karadzhov-etal-2017-fully,1,0.855688,"Missing"
N18-2004,D16-1011,0,0.191672,"c.). Despite the interdependency between fact checking and stance detection, research on these two problems has not been previously supported by an integrated corpus. This is a gap we aim to bridge by retrieving documents for each claim and annotating them for stance, thus ensuring a natural distribution of the stance labels. Moreover, in order to be trusted by users, a factchecking system should be able to explain the reasoning that led to its decisions. This is best supported by showing extracts (such as sentences or phrases) from the retrieved documents that illustrate the detected stance (Lei et al., 2016). Unfortunately, existing datasets do not offer manual annotation of sentence- or phrase-level supporting evidence. While deep neural networks with attention mechanisms can infer and extract such evidence automatically in an unsupervised way (Parikh et al., 2016), potentially better results can be achieved when having the target sentence provided in advance, which enables supervised or semi-supervised training of the attention. This would allow not only more reliable evidence extraction, but also better stance prediction, and ultimately better factuality prediction. Following this idea, our co"
N18-2004,gencheva-etal-2017-context,1,0.869644,"Missing"
N18-2004,P15-2085,0,0.0248319,"unifies stance detection, stance rationale, relevant document retrieval, and fact checking. This is the first corpus to offer such a combination, not only for Arabic but in general. We further demonstrated experimentally that these unified annotations, and the gold rationales in particular, are beneficial both for stance detection and for fact checking. In future work, we plan to extend the annotations to cover other important aspects of fact checking such as source reliability, language style, and temporal information, which have been shown useful in previous research (Castillo et al., 2011; Lukasik et al., 2015; Ma et al., 2016; Mukherjee and Weikum, 2015; Popat et al., 2017). Overall, the above experiments demonstrate that having a gold rationale can enable better learning. However, the results should be considered as a kind of upper bound on the expected performance improvement, since here we used gold rationales at test time, which would not be available in a real-world scenario. Still, we believe that sizable improvements would still be possible when using the gold rationales for training only. Acknowledgment This research was carried out in collaboration between the MIT Computer Science and Art"
N18-2004,W01-0515,0,0.0674273,"as in (Karadzhov et al., 2017b), we transformed each claim into sub-queries by selecting named entities, adjectives, nouns and verbs with the highest TF.DF score, calculated on a collection of documents from the claims’ sources. Then, we used these sub-queries with the claim itself as input to the search API and retrieved the first 20 returned links, from which we excluded those directing to V ERIFY and R EUTERS, and social media websites that are mostly opinionated. Finally, we calculated two similarity measures between the links’ content (documents) and the claims: the tri-gram containment (Lyon et al., 2001) and the cosine distance between average word embeddings of both texts.6 . We only kept documents with non-zero values for both measures, yielding 3,042 documents: 1,239 for false claims and 1,803 for true claims. (2a) (original false claim) FIFA intends to investigate the game between Syria and Australia.  JË@ Ð Q ª K A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j AJË@Q @ð AK Pñ (2b) (corrected claim in V ERIFY) FIFA does not intend to investigate the game between Syria and Australia, as pro-regime pages claim.  JË@ Ð Q ª K B A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j    j® ú«Y K A"
N18-2004,K15-1032,1,0.76343,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,N18-5006,1,0.869083,"Missing"
N18-2004,R15-1058,1,0.833945,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,P16-2065,1,0.789602,"ed by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 -"
N18-2004,D16-1264,0,0.0505697,"traction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detection or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels"
N18-2004,S16-1003,0,0.170265,"Missing"
N18-2004,D17-1317,0,0.384567,"t checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detect"
N18-2004,N18-1070,1,0.860201,"Missing"
N18-2004,D16-1244,0,0.101792,"Missing"
N18-2004,N18-1074,0,0.0626421,"tion or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection betwe"
N18-2004,W14-2508,0,0.248156,"ality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in pract"
N18-2004,pasha-etal-2014-madamira,0,0.0837142,"Missing"
N18-2004,P17-2067,0,0.173053,"ext analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Popat et al., 2016).1 As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from English, and focusing on US politics.2 In contrast, we start with claims that are not only relevant to the Arab world, but that were also originally made in Arabic, thus producing the first publicly available Arabic fact-checking dataset. Stance detection has been studied so far disjointly from fact checking. While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-"
N18-5006,N16-3003,0,0.160717,"opriate sentence tokenizer for each language. For English, NLTK’s (Loper and Bird, 2002) sent_tokenize handles splitting the text into sentences. However, for Arabic it can only split text based on the presence of the period (.) character. This is because other sentence endings — such as question marks— are different characters (e.g., the Arabic question mark is ‘?’, and not ‘?’). Hence, we used our custom regular expressions to split the Arabic text into sentences. Next comes tokenization. For English, we used NLTK’s tokenizer (Bird et al., 2009), while for Arabic we used Farasa’s segmenter (Abdelali et al., 2016). For Arabic, tokenization is not enough; we also need word segmentation since conjunctions and clitics are commonly attached to the main word, e.g., Â ¢þ + Âþtya + Á¤ (‘and his house’, lit. “and house his”). This causes explosion in the vocabulary size and data sparseness. Features Here we do not propose new features, but rather reuse features that have been previously shown to work well for check-worthiness (Hassan et al., 2015; Gencheva et al., 2017). From (Hassan et al., 2015), we include TF.IDFweighted bag of words, part-of-speech tags, named entities as recognized by Alchemy API, sentim"
N18-5006,P17-1042,0,0.0134211,"n (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic. One of the most important components of the system that we had to port across languages were the word embeddings. We experimented with the following cross-language embeddings: – VecMap: we used a parallel English-Arabic corpus of TED talks1 (Cettolo et al., 2012) to generate monolingual embeddings (Arabic and English) using word2vec (Mikolov et al., 2013). Then we projected these embeddings into a joint vector space using VecMap (Artetxe et al., 2017). 2 Note that these results are not comparable to those in (Gencheva et al., 2017) as we use a different evaluation setup: train/test split vs. cross-validation, debates that involve not only Hillary Clinton and Donald Trump, and we also disable the metadata and the discourse parse features. 1 We used TED talks as they are conversational large corpora, which is somewhat close to the debates we train on. 28 Figure 3: Screenshot of ClaimRank’s output for an Arabic news article, sorted by score. System word2vec VecMap MUSE Attract-Repel Random MAP 0.323 0.298 0.319 0.342 0.161 English R-Pr P@5 P@"
N18-5006,W02-0109,0,0.043495,"eir corresponding color codes. Scores are also stored in the session object along with the sentence list as parallel arrays. In case the user wants the sentences sorted by their scores, or wants to mimic one of the annotation sources strategy in sentence selection, the server gets the text from the session, and re-scores/orders it and sends it back to the client. 3.2 Model 3.4 Adaptation to Arabic To handle Arabic along with English, we integrated some new tools. First, we had to add a language detector in order to use the appropriate sentence tokenizer for each language. For English, NLTK’s (Loper and Bird, 2002) sent_tokenize handles splitting the text into sentences. However, for Arabic it can only split text based on the presence of the period (.) character. This is because other sentence endings — such as question marks— are different characters (e.g., the Arabic question mark is ‘?’, and not ‘?’). Hence, we used our custom regular expressions to split the Arabic text into sentences. Next comes tokenization. For English, we used NLTK’s tokenizer (Bird et al., 2009), while for Arabic we used Farasa’s segmenter (Abdelali et al., 2016). For Arabic, tokenization is not enough; we also need word segmen"
N18-5006,J93-2004,0,0.0701683,"se (Vuli´c et al., 2017), thus yielding better vectors, even for English. The overall MAP results for Arabic are competitive, compared to English. The best model is MUSE, while Attract-Repel is way behind, probably because, unlike VecMap and MUSE, its word embeddings are trained on unsegmented Arabic, which causes severe data sparseness issues. We further needed a part-of-speech (POS) tagger for Arabic, for which we used Farasa (Abdelali et al., 2016), while we used NLTK’s POS tagger for English (Bird et al., 2009). This yields different tagsets: for English, this is the Penn Treebank tagset (Marcus et al., 1993), while for Arabic this the Farasa tagset. Thus, we had to further map all POS tags to the same tagset: the Universal tagset (Petrov et al., 2012). 3.5 Evaluation We train the system on five English political debates, and we test on two debates: either English or their Arabic translations. Note that, compared to our original model (Gencheva et al., 2017), here we use more debates: seven instead of four. Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse"
N18-5006,2012.eamt-1.60,0,0.018724,"compared to our original model (Gencheva et al., 2017), here we use more debates: seven instead of four. Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic. One of the most important components of the system that we had to port across languages were the word embeddings. We experimented with the following cross-language embeddings: – VecMap: we used a parallel English-Arabic corpus of TED talks1 (Cettolo et al., 2012) to generate monolingual embeddings (Arabic and English) using word2vec (Mikolov et al., 2013). Then we projected these embeddings into a joint vector space using VecMap (Artetxe et al., 2017). 2 Note that these results are not comparable to those in (Gencheva et al., 2017) as we use a different evaluation setup: train/test split vs. cross-validation, debates that involve not only Hillary Clinton and Donald Trump, and we also disable the metadata and the discourse parse features. 1 We used TED talks as they are conversational large corpora, which is somewhat close to the debates we train on. 2"
N18-5006,Q17-1022,0,0.0427226,"Missing"
N18-5006,gencheva-etal-2017-context,1,0.907961,"n et al., 2015). It is trained on data annotated by students, professors, and journalists, and uses features such as sentiment, TF.IDF-weighted words, part-of-speech tags, and named entities. In contrast, (i) we have much richer features, (ii) we support English and Arabic, (iii) we learn from choices made by nine reputable fact-checking organizations, and (iv) we can mimic the selection strategy of each of them. In our previous work, we focused on debates from the US 2016 Presidential Campaign and we used pre-existing annotations from online fact-checking reports by professional journalists (Gencheva et al., 2017). Here we use roughly the same features, with some differences (see below). However, (i) we train on more debates (seven instead of four for English, and also Arabic translations for two debates), (ii) we add support for Arabic, and (iii) we deploy a working system. Patwari et al. (2017) focused on the 2016 US Election campaign as well and independently obtained their data in a similar way. However, they used less features, they did not mimic any specific website, nor did they deploy a working system. Introduction The proliferation of fake news demands the attention of both investigative journ"
N18-5006,petrov-etal-2012-universal,0,0.0725666,"Missing"
N18-5006,P13-1162,0,0.0541594,"ttached to the main word, e.g., Â ¢þ + Âþtya + Á¤ (‘and his house’, lit. “and house his”). This causes explosion in the vocabulary size and data sparseness. Features Here we do not propose new features, but rather reuse features that have been previously shown to work well for check-worthiness (Hassan et al., 2015; Gencheva et al., 2017). From (Hassan et al., 2015), we include TF.IDFweighted bag of words, part-of-speech tags, named entities as recognized by Alchemy API, sentiment scores, and sentence length (in tokens). From (Gencheva et al., 2017), we adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and also for subjectivity. 27 Figure 2: Screenshot of ClaimRank’s output for an English presidential debate, in natural order. – MUSE embeddings: In a similar fashion, we generated cross-language embeddings from the same TED talks using Facebook’s supervised MUSE model (Lample et al., 2017) to project the Arabic and the English monolingual embeddings into a joint vector space. – Attract-Repel embeddings: we used the pretrained English-Arabic embeddings from AttractRepel (Mrkši´c et al., 2017). Table 1 shows the system perfor"
N18-5006,J15-3002,0,0.0693925,"Missing"
N18-5006,P17-1006,0,0.0251397,"Missing"
P06-2003,niessen-etal-2000-evaluation,0,\N,Missing
P06-2003,W05-0826,1,\N,Missing
P06-2003,N03-2021,0,\N,Missing
P06-2003,P02-1040,0,\N,Missing
P06-2003,P04-1077,0,\N,Missing
P06-2003,2005.iwslt-1.1,0,\N,Missing
P06-2003,2004.iwslt-evaluation.1,0,\N,Missing
P06-2003,C04-1072,0,\N,Missing
P06-2003,P05-1035,1,\N,Missing
P06-2003,W05-0904,0,\N,Missing
P06-2003,2005.iwslt-1.23,0,\N,Missing
P06-2003,gimenez-amigo-2006-iqmt,1,\N,Missing
P06-2037,W05-0909,0,0.0120797,"d on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Mart´ı, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5 4.2 Combining Sources: Language Models In order to improve results, in first place we turned our eyes to language modeling. In addition to 6 http://www.statmt.org/wpt05/. 289 http://www.systransoft.com/. system BLEU.n"
P06-2037,N03-2021,0,0.0164086,"0) and test (500) sets. Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Mart´ı, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5 4.2 Combining Sources: Language Models In order to improve results, in first place we turned our eyes to language modeling. In addition to 6 http://www.statmt.org/wpt"
P06-2037,C88-1016,0,0.132829,"Missing"
P06-2037,J03-1002,0,0.00240283,"System Construction Fortunately, there is a number of freely available tools to build a phrase-based SMT system. We used only standard components and techniques for our basic system, which are all described below. The SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) supports creation and evaluation of a variety of language models. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. In order to build phrase-based translation models, a phrase extraction must be performed on a word-aligned parallel corpus. We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) to generate word alignments We applied the phraseextract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++. We work with the union of source-to-target and targetto-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase was more than three times longer than the target/source phrase are ignored. Finally, phrase pairs are scored by relative frequency. Note that no smoothing is performed. Regarding the arg-max search, we used"
P06-2037,2001.mtsummit-papers.68,0,0.0193724,",843 glosses was 8.25 words for English and 8.13 for Spanish. Finally, gloss pairs were randomly split into training (4,843), development (500) and test (500) sets. Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (‘D1’) (Mart´ı, 1996) and 168,779 definitons (1,553,674 tokens) (‘D2’) (Vox, 1990), respectively. Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al., 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1, 2) (Melamed et al., 2003), and the METEOR measure (Banerjee and Lavie, 2005). These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references. Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup. The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b). 5 4.2 Combining Sources: Lan"
P06-2037,koen-2004-pharaoh,0,0.213019,"t out from parallel corpora, and decoders usually perform approximate search, e.g., by using dynamic programming and beam search. However, in word-based models the modeling of the context in which the words occur is very weak. This problem is significantly alleviated by phrase-based models (Och, 2002), which represent nowadays the state-of-the-art in SMT. Models. A substantial increase in performance is achieved, according to several standard MT evaluation metrics. Although moderate, this boost in performance is statistically significant according to the bootstrap resampling test described by Koehn (2004b) and applied to the BLEU metric. The main reason behind this improvement is that the large out-of-domain corpus contributes mainly with coverage and recall and the in-domain corpus provides more precise translations. We present a qualitative error analysis to support these claims. Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results. Apart from the experimental findings, our study has generated a very valuable resource. Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from"
P06-2037,W04-3250,0,0.363001,"t out from parallel corpora, and decoders usually perform approximate search, e.g., by using dynamic programming and beam search. However, in word-based models the modeling of the context in which the words occur is very weak. This problem is significantly alleviated by phrase-based models (Och, 2002), which represent nowadays the state-of-the-art in SMT. Models. A substantial increase in performance is achieved, according to several standard MT evaluation metrics. Although moderate, this boost in performance is statistically significant according to the bootstrap resampling test described by Koehn (2004b) and applied to the BLEU metric. The main reason behind this improvement is that the large out-of-domain corpus contributes mainly with coverage and recall and the in-domain corpus provides more precise translations. We present a qualitative error analysis to support these claims. Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results. Apart from the experimental findings, our study has generated a very valuable resource. Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from"
P06-2037,P02-1040,0,\N,Missing
P08-1063,W05-0620,1,0.882424,"Missing"
P08-1063,H94-1020,0,0.239361,"Missing"
P08-1063,J05-1004,0,0.670746,"lysis allows to determine “who” did “what” to “whom”, “when” and “where”, and, thus, characterize the participants and properties of the events established by the predicates. This kind of semantic analysis is very interesting for a broad spectrum of NLP applications (information extraction, summarization, question answering, machine translation, etc.), since it opens the door to exploit the semantic relations among linguistic constituents. The properties of the semantically annotated corpora available have conditioned the type of research and systems that have been developed so far. PropBank (Palmer et al., 2005) is the most widely used corpus for training SRL systems, probably because it contains running text from the Penn Treebank corpus with annotations on all verbal predicates. Also, a few evaluation exercises on SRL have been conducted on this corpus in the CoNLL-2004 and 2005 conferences. However, a serious criticisms to the PropBank corpus refers to the role set it uses, which consists of a set of numbered core arguments, whose semantic translation is verb-dependent. While Arg0 and Arg1 are intended to indicate the general roles of Agent and Theme, other argument numbers do not generalize acros"
P08-1063,S07-1016,0,0.0984302,"uistic theory (e.g., Agent, Theme, Patient, Recipient, Cause, etc.). We foresee two advantages of using such thematic roles. On the one hand, statistical SRL systems trained from them could generalize better and, therefore, be more robust and portable, as suggested in (Yi et al., 2007). On the other hand, roles in a paradigm like VerbNet would allow for inferences over the assigned roles, which is only possible in a more limited way with PropBank. In a previous paper (Zapirain et al., 2008), we presented a first comparison between the two previous role sets on the SemEval-2007 Task 17 corpus (Pradhan et al., 2007). The SemEval-2007 corpus only 550 Proceedings of ACL-08: HLT, pages 550–558, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics comprised examples about 50 different verbs. The results of that paper were, thus, considered preliminary, as they could depend on the small amount of data (both in training data and number of verbs) or the specific set of verbs being used. Now, we extend those experiments to the entire PropBank corpus, and we include two extra experiments on domain shifts (using the Brown corpus as test set) and on grouping VerbNet labels. More concrete"
P08-1063,N07-1069,0,0.468698,"Missing"
P08-1063,S07-1077,1,0.89617,"Missing"
P09-2019,E03-1034,0,0.404843,"Missing"
P09-2019,W05-0620,1,0.840172,"Missing"
P09-2019,P07-1028,0,0.614192,"l preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles, rely on the evaluation on pseudo tasks or human plausibility judgments. In (Erk, 2007) a distributional similarity–based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). The results over 100 frame-specific roles showed that distributional similarities get smaller error rates than Resnik and EM, with Lin’s formula having the smallest error rate. Moreover, coverage of distributional similarities and Resnik are rather low. Our distributional model for selectional preferences follows her formalization. Currently, there are several models of distributional similarity that could be used for selectional preferences. More recently, Pad´o an"
P09-2019,J02-3001,0,0.342338,", we tried the optimal parameters as described in (Pad´o and Lapata, 2007, p. 179): word-based space, medium context, loglikelihood association, and 2,000 basis elements. We tested Jaccard, cosine and Lin’s measure (Lin, 1998) for similarity, yielding simjac , simcos and simlin , respectively. Distributional similarity has also been used to tackle syntactic ambiguity. Pantel and Lin (2000) obtained very good results using the distributional similarity measure defined by Lin (1998). The application of selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and annotated corpus (PropBank), a wider range of selectional preference models, and the analysis of out-of-domain results. Other papers applying semantic preferences in the context of semantic roles, rely on the evaluation on pseudo tasks o"
P09-2019,P98-2127,0,0.719365,"evaluation. Resnik’s selectional preference scored best among classbased methods, but it performed equal to a simple, purely lexical, conditional probability model. JFK was assassinated (in Dallas)Location JFK was assassinated (in November)T emporal 73 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 73–76, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP WordNet-based SP models: we use Resnik’s selectional preference model. Distributional SP models: Given the availability of publicly available resources for distributional similarity, we used 1) a ready-made thesaurus (Lin, 1998), and 2) software (Pad´o and Lapata, 2007) which we run on the British National Corpus (BNC). In the first case, Lin constructed his thesaurus based on his own similarity formula run over a large parsed corpus comprising journalism texts. The thesaurus lists, for each word, the most similar words, with their weight. In order to get the similarity for two words, we could check the entry in the thesaurus for either word. But given that the thesaurus is not symmetric, we take the average of both similarities. We will refer to this similarity measure as simth lin . Another option is to use second-"
P09-2019,J07-2002,0,0.0769415,"Missing"
P09-2019,P00-1014,0,0.0602223,"here we compute the similarity of two words using the entries in the thesaurus, either using the cosine or Jaccard measures. We will refer to these similarity measures th2 as simth2 jac and simcos hereinafter. For the second case, we tried the optimal parameters as described in (Pad´o and Lapata, 2007, p. 179): word-based space, medium context, loglikelihood association, and 2,000 basis elements. We tested Jaccard, cosine and Lin’s measure (Lin, 1998) for similarity, yielding simjac , simcos and simlin , respectively. Distributional similarity has also been used to tackle syntactic ambiguity. Pantel and Lin (2000) obtained very good results using the distributional similarity measure defined by Lin (1998). The application of selectional preferences to semantic roles (as opposed to syntactic functions) is more recent. Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. They used distributional clustering and WordNet-based techniques on a SRL task on FrameNet roles. They report a very small improvement of the overall performance when using distributional clustering techniques. In this paper we present complementary experiments, with a different role set and ann"
P09-2019,J08-2006,0,0.0691101,"WordNet based models, which have a lower word coverage compared to distributional similarity–based models. 5 prec. .779 .589 .573 .607 .580 .635 .657 .654 co-occurrences extracted from the BNC (simJac , simcos simLin ), and the results obtained when using Lin’s thesaurus directly (simth Lin ) and as a th2 ). second-order vector (simth2 and sim cos Jac As expected, the lexical baseline attains very high precision in all datasets, which underscores the importance of the lexical head word features in argument classification. The recall is quite low, specially in Brown, confirming and extending (Pradhan et al., 2008), which also reports similar performance drops when doing argument classification on out-of-domain data. One of the main goals of our experiments is to overcome the data sparseness of lexical features both on in-domain and out-of-domain data. All our selectional preference models improve over the lexical matching baseline in recall, up to 30 absolute percentage points in the WSJ test dataset and 44 absolute percentage points in the Brown corpus. This comes at the cost of reduced precision, but the overall F-score shows that all selectional preference models improve over the baseline, with up t"
P09-2019,H93-1054,0,0.300645,"nd generalize poorly to new corpora. This work explores the usefulness of selectional preferences to alleviate the lexical dependence of SRL systems. Selectional preferences introduce semantic generalizations on the type of arguments preferred by the predicates. Therefore, they are expected to improve generalization on infrequent and unknown words, and increase the discriminative power of the argument classifiers. For instance, consider these two sentences: 2 Related Work Automatic acquisition of selectional preferences is a relatively old topic, and will mention the most relevant references. Resnik (1993) proposed to model selectional preferences using semantic classes from WordNet in order to tackle ambiguity issues in syntax (noun-compounds, coordination, PP-attachment). Brockman and Lapata (2003) compared several class-based models (including Resnik’s selectional preferences) on a syntactic plausibility judgement task for German. The models return weights for (verb, syntactic function, noun) triples, and the correlation with human plausibility judgement is used for evaluation. Resnik’s selectional preference scored best among classbased methods, but it performed equal to a simple, purely le"
P09-2019,N07-1070,0,\N,Missing
P09-2019,C98-2122,0,\N,Missing
P09-5003,W04-0803,0,\N,Missing
P09-5003,W04-2705,0,\N,Missing
P09-5003,copestake-flickinger-2000-open,0,\N,Missing
P09-5003,A00-2018,0,\N,Missing
P09-5003,W04-2401,0,\N,Missing
P09-5003,W04-2415,1,\N,Missing
P09-5003,boas-2002-bilingual,0,\N,Missing
P09-5003,burchardt-etal-2006-salsa,0,\N,Missing
P09-5003,W08-2121,1,\N,Missing
P09-5003,C04-1100,0,\N,Missing
P09-5003,W09-1212,1,\N,Missing
P09-5003,S07-1018,0,\N,Missing
P09-5003,W05-0628,1,\N,Missing
P09-5003,J08-2003,0,\N,Missing
P09-5003,S07-1008,1,\N,Missing
P09-5003,W07-1402,0,\N,Missing
P09-5003,W09-1204,0,\N,Missing
P09-5003,S07-1005,0,\N,Missing
P09-5003,S07-1048,0,\N,Missing
P09-5003,H05-1111,0,\N,Missing
P09-5003,W08-2116,0,\N,Missing
P09-5003,W03-1008,0,\N,Missing
P09-5003,W07-0738,1,\N,Missing
P09-5003,S07-1016,0,\N,Missing
P09-5003,W08-2124,1,\N,Missing
P09-5003,W08-2126,0,\N,Missing
P09-5003,J08-2005,0,\N,Missing
P09-5003,C00-2108,0,\N,Missing
P09-5003,W04-3212,0,\N,Missing
P09-5003,W05-0639,0,\N,Missing
P09-5003,N07-1070,0,\N,Missing
P09-5003,W04-2609,0,\N,Missing
P09-5003,N03-1031,0,\N,Missing
P09-5003,J06-2001,0,\N,Missing
P09-5003,W08-2125,0,\N,Missing
P09-5003,H05-1112,0,\N,Missing
P09-5003,W09-1112,0,\N,Missing
P09-5003,W08-0332,1,\N,Missing
P09-5003,C04-1134,0,\N,Missing
P09-5003,H05-1047,0,\N,Missing
P09-5003,W08-2111,0,\N,Missing
P09-5003,A97-1052,0,\N,Missing
P09-5003,W05-0625,0,\N,Missing
P09-5003,N06-5006,0,\N,Missing
P09-5003,J03-4003,0,\N,Missing
P09-5003,W06-1601,0,\N,Missing
P09-5003,P07-1028,0,\N,Missing
P09-5003,W09-2423,0,\N,Missing
P09-5003,W09-1205,0,\N,Missing
P09-5003,J08-2004,0,\N,Missing
P09-5003,P04-1055,0,\N,Missing
P09-5003,W08-2101,0,\N,Missing
P09-5003,P09-1003,0,\N,Missing
P09-5003,N06-2026,0,\N,Missing
P09-5003,W05-0620,1,\N,Missing
P09-5003,P09-1033,0,\N,Missing
P09-5003,W08-2123,0,\N,Missing
P09-5003,P09-2019,1,\N,Missing
P09-5003,W09-1213,0,\N,Missing
P09-5003,P05-1073,0,\N,Missing
P09-5003,W05-0622,0,\N,Missing
P09-5003,P09-1004,0,\N,Missing
P09-5003,W09-1201,1,\N,Missing
P09-5003,W08-2122,0,\N,Missing
P09-5003,P03-1002,0,\N,Missing
P09-5003,N06-1025,0,\N,Missing
P09-5003,P07-1080,0,\N,Missing
P09-5003,J02-3001,0,\N,Missing
P09-5003,P08-1063,1,\N,Missing
P09-5003,W05-0634,0,\N,Missing
P09-5003,J05-1004,0,\N,Missing
P09-5003,P09-1005,0,\N,Missing
P09-5003,W03-0411,0,\N,Missing
P09-5003,J01-3003,0,\N,Missing
P09-5003,W09-1203,0,\N,Missing
P09-5003,J08-2002,0,\N,Missing
P09-5003,2007.tmi-papers.10,0,\N,Missing
P09-5003,N09-2004,0,\N,Missing
P09-5003,2009.eamt-1.30,0,\N,Missing
P09-5003,W04-2412,1,\N,Missing
P09-5003,D07-1101,0,\N,Missing
P09-5003,W09-1106,0,\N,Missing
P09-5003,W06-2303,0,\N,Missing
P09-5003,P06-1146,0,\N,Missing
P12-3024,D11-1042,1,0.868053,"Missing"
P12-3024,gimenez-marquez-2008-towards,1,0.746858,"Missing"
P12-3024,2011.mtsummit-papers.24,0,0.0343056,"Missing"
P12-3024,2007.mtsummit-papers.39,0,0.255256,"Research Center Universitat Polit`ecnica de Catalunya {mgonzalez,jgimenez,lluism}@lsi.upc.edu Abstract Automatic detection and classification of the errors produced by MT systems is a challenging problem. The cause of such errors may depend not only on the translation paradigm adopted, but also on the language pairs, the availability of enough linguistic resources and the performance of the linguistic processors, among others. Several past research works studied and defined fine-grained typologies of translation errors according to various criteria (Vilar et al., 2006; Popovi´c et al., 2006; Kirchhoff et al., 2007), which helped manual annotation and human analysis of the systems during the MT development cycle. Recently, the task has received increasing attention towards the automatic detection, classification and analysis of these errors, and new tools have been made available to the community. Examples of such tools are AMEANA (Kholy and Habash, 2011), which focuses on morphologically rich languages, and Hjerson (Popovi´c, 2011), which addresses automatic error classification at lexical level. Error analysis in machine translation is a necessary step in order to investigate the strengths and weakness"
P12-3024,J11-4002,0,0.0547947,"Missing"
P12-3024,W06-3101,0,0.151963,"Missing"
P12-3024,P11-4010,0,0.02621,"uent parsing trees, discourse structures and semantic roles. Also, there exist very few tools devoted to visualize the errors produced by the MT systems. Here, instead of dealing with the automatic classification of errors, we deal with the automatic selection and visualization of the information used by the evaluation measures. 6 5 Related Work In the literature, we can find detailed typologies of the errors produced by MT systems (Vilar et al., 2006; Farr´us et al., 2011; Kirchhoff et al., 2007) and graphical interfaces for human classification and annotation of these errors, such as BLAST (Stymne, 2011). They represent a framework to study the performance of MT systems and develop further refinements. However, they are defined for a specific pair of languages or domain and they are difficult to generalize. For instance, the study described in (Kirchhoff et al., 2007) focus on measures relying on the characterization of the input documents (source, 143 Conclusions and Future Work The main goal of the A SIYA toolkit is to cover the evaluation needs of researchers during the development cycle of their systems. A SIYA generates a number of linguistic analyses over both the candidate and the refe"
P12-3024,vilar-etal-2006-error,0,0.145419,"Missing"
P13-4031,berka-etal-2012-automatic,0,0.0354975,"Missing"
P13-4031,P05-1022,0,0.0174047,"explanation of its most important features is given in the demonstrative video. In the following, Section 2 gives an overview of the A SIYA toolkit and the information gathered from the evaluation output. Section 3 and Section 4 describe in depth the tS EARCH application and the on-line interface, respectively. Finally, Section 5 reviews similar applications in comparison to the functionalities addressed by tS EARCH. 2 and other analyzers. In those cases, A SIYA uses the SVMTool (Gim´enez and M`arquez, 2004), BIOS (Surdeanu et al., 2005), the CharniakJohnson and Berkeley constituent parsers (Charniak and Johnson, 2005; Petrov and Klein, 2007), and the MALT dependency parser (Nivre et al., 2007), among others. In the tS EARCH platform, the system manages the communication with an instance of the A SIYA toolkit running on the server. For every test suite, the system maintains a synchronized representation of the input data, the evaluation results and the linguistic information generated. Then, the system updates a database where the test suites are stored for further analysis using the tS EARCH tool, as described next. 3 MT Evaluation with the A SIYA Toolkit The tS EARCH Tool http://asiya.lsi.upc.edu/demo A"
P13-4031,gimenez-marquez-2004-svmtool,1,0.683691,"Missing"
P13-4031,P12-3024,1,0.810313,"Missing"
P13-4031,N07-1051,0,0.017932,"ortant features is given in the demonstrative video. In the following, Section 2 gives an overview of the A SIYA toolkit and the information gathered from the evaluation output. Section 3 and Section 4 describe in depth the tS EARCH application and the on-line interface, respectively. Finally, Section 5 reviews similar applications in comparison to the functionalities addressed by tS EARCH. 2 and other analyzers. In those cases, A SIYA uses the SVMTool (Gim´enez and M`arquez, 2004), BIOS (Surdeanu et al., 2005), the CharniakJohnson and Berkeley constituent parsers (Charniak and Johnson, 2005; Petrov and Klein, 2007), and the MALT dependency parser (Nivre et al., 2007), among others. In the tS EARCH platform, the system manages the communication with an instance of the A SIYA toolkit running on the server. For every test suite, the system maintains a synchronized representation of the input data, the evaluation results and the linguistic information generated. Then, the system updates a database where the test suites are stored for further analysis using the tS EARCH tool, as described next. 3 MT Evaluation with the A SIYA Toolkit The tS EARCH Tool http://asiya.lsi.upc.edu/demo A complete list of external"
P13-4031,vilar-etal-2006-error,0,0.100302,"Missing"
P13-4031,C10-2013,0,\N,Missing
P14-1065,W11-2103,0,0.0360688,"herence relations in the source language when generating target-language translations. In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (Joty et al., 2012); then, we show that they can help improve a number of MT evaluation metrics at the segment- and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanis"
P14-1065,N04-1035,0,0.0377299,"ve process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, an"
P14-1065,W12-3102,0,0.0260244,"Missing"
P14-1065,W07-0738,1,0.784963,"Missing"
P14-1065,W09-0440,1,0.927179,"Missing"
P14-1065,W11-1211,0,0.0329553,"sed discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a diffe"
P14-1065,D08-1024,0,0.0729502,"Missing"
P14-1065,P05-1033,0,0.102259,"over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this"
P14-1065,2010.iwslt-papers.10,0,0.0454846,"ial of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR"
P14-1065,D12-1108,0,0.0251193,"ramework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relations in the source la"
P14-1065,W10-1750,1,0.883167,"Missing"
P14-1065,W11-2107,0,0.0510915,"the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judges 8 20 15 18 498 924 570 708 171 303 207 249 20 31 18 32"
P14-1065,D11-1125,0,0.120375,"et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield be"
P14-1065,D12-1083,1,0.558373,"is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus sho"
P14-1065,P02-1040,0,0.0916457,"html SPAN NUC EDU Attribution SPAN Satellite NUC Nucleus SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear"
P14-1065,P13-1048,1,0.222411,"the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into ac"
P14-1065,W07-0707,0,0.0299954,"g proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse info"
P14-1065,P05-1034,0,0.0192094,"tence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield bett"
P14-1065,W04-1013,0,0.0173282,"NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judge"
P14-1065,2006.amta-papers.25,0,0.149639,"us SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 sy"
P14-1065,W05-0904,0,0.154446,"n campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware m"
P14-1065,W12-3129,0,0.298188,"e of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is"
P14-1065,D07-1080,0,0.0300512,"ks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judg"
P14-1065,2012.amta-papers.20,0,0.0666078,"ion for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For"
P14-1065,P11-3009,0,0.0426777,"tation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse"
P14-1065,moschitti-basili-2006-tree,0,0.00987264,"of the relation while satellites are supportive ones. Note that the nuclearity and relation labels in the reference translation are also realized in the system translation in (b), but not in (c), which makes (b) a better translation compared to (c), according to our hypothesis. We argue that existing metrics that only use lexical and syntactic information cannot distinguish well between (b) and (c). 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitations on the type of substructures that can be compared. 2 The discourse"
P14-1065,D12-1097,0,0.146948,"o develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion score. Unlike their work, which measures lexical cohesion at the document-level, here we are concerned with coherence (rhetorical) structure, primarily at the sentence-level. 3 3.1 Gener"
P14-1065,P07-1098,0,0.0569476,"ive partial credit to subtrees that differ in labels but match in their skeletons. More specifically, it uses the tags SPAN and EDU to build the skeleton of the tree, and considers the nuclearity and/or the relation labels as properties, added as children, of these tags. For example, a SPAN has two properties (its nuclearity and its relation), and an EDU has one property (its nuclearity). The words of an EDU are placed under the predefined children NGRAM. In order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in (Moschitti et al., 2007); not shown in Figure 2, for simplicity. Experimental Setup In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English.3 This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English (CS EN ), French-English ( FR - EN), German-English (DE - EN), and Spanish-English (ES - EN); as well as a dataset with the English references. We measured the correlation of the metrics with the human judgments pro"
P14-1065,N09-2004,0,\N,Missing
P14-1065,W13-3300,0,\N,Missing
P15-1078,W10-1750,1,0.923317,"Missing"
P15-1078,P14-1023,0,0.016965,"q. (3), can be rewritten as follows: 4.1 Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX 25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as W IKI -GW25. Furthermore, we experiment with W IKI GW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. Network Training The negative log likelihood of the training data for the model parameters θ = (W12 , W1r , W2r , wv , b12 , b1r , b2r , bv ) can be written"
P15-1078,W11-2107,0,0.0115221,"100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as will be discussed below, their"
P15-1078,P14-1129,0,0.0611925,"Missing"
P15-1078,W08-0331,0,0.313473,"that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce"
P15-1078,W07-0718,0,0.0607111,"(SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development. The quality of automatic MT evaluation metrics is usually assessed by computing their correlation with human judgments. To that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In"
P15-1078,W07-0738,1,0.52667,"Missing"
P15-1078,W11-2103,0,0.0331433,"r improvements: +1.5 and +2.0 points absolute when adding SYNTAX 25 and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at"
P15-1078,D14-1027,1,0.819258,"Missing"
P15-1078,W12-3102,0,0.0577153,"Missing"
P15-1078,P14-1065,1,0.735686,"Missing"
P15-1078,W14-3352,1,0.565452,"Missing"
P15-1078,P02-1040,0,0.100737,"ans that rivals the state of the art. 1 Introduction Automatic machine translation (MT) evaluation is a necessary step when developing or comparing MT systems. Reference-based MT evaluation, i.e., comparing the system output to one or more human reference translations, is the most common approach. Existing MT evaluation measures typically output an absolute quality score by computing the similarity between the machine and the human translations. In the simplest case, the similarity is computed by counting word n-gram matches between the translation and the reference. This is the case of BLEU (Papineni et al., 2002), which has been the standard for MT evaluation for years. Nonetheless, more recent evaluation measures take into account various aspects of linguistic similarity, and achieve better correlation with human judgments. 1 We do not argue that the pairwise approach is better than the direct estimation of human quality scores. Both approaches have pros and cons; we see them as complementary. 805 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 805–814, c Beijing, China, July 26-31"
P15-1078,2004.tmi-1.8,0,0.0676924,"ased approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as features. Secondly, we use information about both the reference and the two alternative translations simultaneously in a neural-based learning framework capable of modeling complex interactions between the features. Another related work is that of Kulesza and Shieber (2004), in which lexical and syntactic features, together with other metrics, e.g., BLEU and NIST, are used in an SVM classifier to discriminate good from bad translations. However, their setting is not pairwise comparison, but a classification task to distinguish human- from machineproduced translations. Moreover, in their work, using syntactic features decreased the correlation with human judgments dramatically (although classification accuracy improved), while in our case the effect is positive. In our previous work (Guzm´an et al., 2014a), we introduced a learning framework for the pairwise sett"
P15-1078,D14-1162,0,0.0794828,"h application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3 Learning Task Given two translation h"
P15-1078,W07-0707,0,0.0221202,"Missing"
P15-1078,W05-0904,0,0.150588,"ecause the use of kernels requires that the SVM operate in the much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to repr"
P15-1078,2006.amta-papers.25,0,0.187957,"2 VEC 300, trained on 100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as wi"
P15-1078,W12-3129,0,0.0269611,"much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our se"
P15-1078,P13-1045,0,0.14241,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W13-2202,0,0.0122239,"and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and B"
P15-1078,D13-1170,0,0.00357626,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W14-3336,0,0.0290128,"r, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and Bojar, 2014) for a discussion. Here we use the strict version used at WMT11 and WMT12. 4.4 Experiments and Results Experimental Settings Datasets: We train our neural models on WMT11 and we evaluate them on WMT12. We further use a random subset of 5,000 examples from WMT13 as a validation set to implement early stopping. Early stopping: We train on WMT11 for up to 10,000 epochs, and we calculate Kendall’s τ on the development set after each epoch. We then select the model that achieves the highest τ on the validation set; in case of ties for the best τ , we select the latest epoch that achieved the highes"
P15-1078,W11-2113,0,0.253095,"quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce absolute quality scor"
P15-1078,D12-1097,0,0.0127587,"o make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it bot"
P15-1078,N13-1090,0,0.0630498,"nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3"
P15-1078,W11-0329,0,0.0126675,"Missing"
P15-1078,W14-3302,0,\N,Missing
P15-2113,P07-1098,1,0.451732,"al problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer s"
P15-2113,N10-1145,0,0.0528465,"answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve high"
P15-2113,S15-2047,1,0.84994,"Missing"
P15-2113,S15-2035,0,0.0819635,"1.45 65.57±1.54 76.23±0.45 76.43±0.92 75.05±0.70 75.61±0.63 75.71±0.71 Table 3: Precision, Recall, F1 , Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1 , F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup As in the competition, the results are macroaveraged at class level. The results of the top 3 Our local classifiers are support vector machines systems are reported for comparison: JAIST (Tran (SVM) with C = 1 (Joachims, 1999), logistic et al., 2015), HITSZ (Hou et al., 2015) and regression with a Gaussian prior with variance 10, QCRI (Nicosia et al., 2015), where the latter refers and logistic ordinal regression (McCullagh, 1980). to our old system that we used for the competition. In order to capture long-range sequential depenThe two main observations are (i) using threaddencies, we use a second-order SVMhmm (Yu level features helps significantly; and (ii) the ordiand Joachims, 2008) (with C = 500 and nal regression model, which captures the idea that epsilon = 0.01) and a second-order linear-chain potential lies between good and bad, achieves at CRF, which con"
P15-2113,S15-2036,1,0.620958,"Missing"
P15-2113,D13-1044,1,0.903656,"Missing"
P15-2113,W01-0515,0,0.0237286,"can affect the label of the current answer, this dependency is too loose to have impact on the selection accuracy. In other words, labels should be used together with answers’ content to account for stronger and more effective dependencies. 2 Basic and Thread-Level Features 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the for"
P15-2113,W13-3509,1,0.800933,"g the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used"
P15-2113,D07-1002,0,0.0974152,"a Abstract This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and"
P15-2113,P08-1082,0,0.0417245,"n can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should b"
P15-2113,S15-2038,0,0.16032,"Missing"
P15-2113,C10-1131,0,0.223512,"Missing"
P15-2113,N13-1106,0,0.0861461,"Missing"
P16-2075,S16-1172,0,0.151144,"Missing"
P16-2075,S16-1138,0,0.225785,"Missing"
P16-2075,P15-1078,1,0.661142,"Missing"
P16-2075,S16-1137,1,0.558982,"Missing"
P16-2075,N16-1153,1,0.859816,"Missing"
P16-2075,P15-2114,0,0.110685,". Furthermore, in MTE we can expect shorter texts, which are typically much more similar. In contrast, in cQA, the question and the intended answers might differ significantly both in terms of length and in lexical content. Thus, it is not clear a priori whether the MTE network can work well to address the cQA problem. Here, we show that the analogy is not only convenient, but also that using it can yield state-of-the-art results for the cQA task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and"
P16-2075,P11-1143,0,0.168069,", 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely abo"
P16-2075,S16-1136,1,0.71134,"Missing"
P16-2075,P03-1003,0,0.0648629,"plied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the"
P16-2075,N13-1090,0,0.12755,"feature sets as ψ(q, c1 ) and ψ(q, c2 ). When including the external features, the activation at the output is f (q, c1 , c2 ) = sig(wvT [φ(q, c1 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). 4 Features We experiment with three kinds of features: (i) input embeddings, (ii) features from MTE (Guzm´an et al., 2015) and (iii) task-specific features from SemEval-2015 Task 3 (Nicosia et al., 2015). A. Embedding Features We used two types of vector-based embeddings to encode the input texts q, c1 and c2 : (1) G OOGLE VECTORS: 300dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The encoding of the full text is just the average of the word embeddings. (2) S YNTAX: We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. Also, we compute cosine similarity features with the above vectors: cos(q, c1 ) and cos(q, c2 ). B. MTE features We use the following MTE metrics (MT FEATS), which compare the similarity between the question and a candidate answer: (1) B LEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover"
P16-2075,J11-2003,0,0.0747787,") and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely above the task baselines."
P16-2075,S16-1083,1,0.652512,"Missing"
P16-2075,S15-2036,1,0.343646,"Missing"
P16-2075,S15-2038,0,0.165347,"(Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely above the task baselines. Furthermore, by adap"
P16-2075,P15-2116,0,0.0936834,"Missing"
P16-2075,P02-1040,0,0.0977685,"in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better, which is appropriate for a ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships between them; (iii) it uses a number of machine translation evaluation measures that have not been explored for the cQA task before, e.g., T ER (Snover et al., 2006), M ETEOR (Lavie and Denkowski, 2009), and B LEU (Papineni et al., 2002). The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c1 and c2 from the question thread—which play the role of the two competing translation hypotheses—we have to decide whether c1 is a better answer than c2 to question q—which plays the role of the translation reference. If we have a function f (q, c1 , c2 ) to make this decision, then we can rank the finite list of comments in the thread by comparing all possible pairs and by accumulating for each comment the scores for it given by f . From a general point of view, MTE and the cQ"
P16-2075,P15-1025,0,0.0694515,"answer to the question. Furthermore, in MTE we can expect shorter texts, which are typically much more similar. In contrast, in cQA, the question and the intended answers might differ significantly both in terms of length and in lexical content. Thus, it is not clear a priori whether the MTE network can work well to address the cQA problem. Here, we show that the analogy is not only convenient, but also that using it can yield state-of-the-art results for the cQA task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Rie"
P16-2075,P07-1059,0,0.0901484,"015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields resu"
P16-2075,2006.amta-papers.25,0,0.555241,"ural network is interesting for the cQA problem because: (i) it works in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better, which is appropriate for a ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships between them; (iii) it uses a number of machine translation evaluation measures that have not been explored for the cQA task before, e.g., T ER (Snover et al., 2006), M ETEOR (Lavie and Denkowski, 2009), and B LEU (Papineni et al., 2002). The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c1 and c2 from the question thread—which play the role of the two competing translation hypotheses—we have to decide whether c1 is a better answer than c2 to question q—which plays the role of the translation reference. If we have a function f (q, c1 , c2 ) to make this decision, then we can rank the finite list of comments in the thread by comparing all possible pairs and by accumulating for each comment the"
P16-2075,P13-1045,0,0.0205596,"). 4 Features We experiment with three kinds of features: (i) input embeddings, (ii) features from MTE (Guzm´an et al., 2015) and (iii) task-specific features from SemEval-2015 Task 3 (Nicosia et al., 2015). A. Embedding Features We used two types of vector-based embeddings to encode the input texts q, c1 and c2 : (1) G OOGLE VECTORS: 300dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The encoding of the full text is just the average of the word embeddings. (2) S YNTAX: We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. Also, we compute cosine similarity features with the above vectors: cos(q, c1 ) and cos(q, c2 ). B. MTE features We use the following MTE metrics (MT FEATS), which compare the similarity between the question and a candidate answer: (1) B LEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover et al., 2006). (4) M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; (5) Unigram P RECISION; (6) Unigram R ECALL. BLEU COMP. We further use as features various components"
P97-1031,H92-1022,0,0.0453582,"Missing"
P97-1031,W95-0101,0,0.0508296,"Missing"
P97-1031,A92-1018,0,0.116965,"Missing"
P97-1031,W96-0102,0,0.0262351,"es or constraints (Voutilainen and Jgrvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al., 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; . I~:.i:;:;~: I / i.wcous le~ed |t wri.e. |... l Corpus Figure h Tagger architecture. We also present a constraint-acquisition algorithm that uses statistical decision trees to learn context constraints from annotated corpora and we use the acquired constraints to feed the POS tagger. The paper is organized as follows. In section 2 we describe our language model, in section 3 we describe the constraint acquisition algorithm, and in se"
P97-1031,P89-1015,0,0.0375431,"tested and evaluated on the WSJ corpus. 1 Introduction Language Model In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jgrvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al., 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; . I~:.i:;:;~: I / i.wcous le~ed |t wri.e. |... l Corpus Figure h Tagger arch"
P97-1031,C90-3030,0,0.0516042,"Missing"
P97-1031,C96-2148,1,0.915537,"Missing"
P97-1031,C94-1027,0,0.0530995,"J corpus. 1 Introduction Language Model In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jgrvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al., 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; . I~:.i:;:;~: I / i.wcous le~ed |t wri.e. |... l Corpus Figure h Tagger architecture. We also present a co"
P97-1031,E95-1029,0,0.0215161,"Missing"
P97-1031,A97-1013,1,0.822392,"Missing"
P98-2164,P97-1010,0,0.026221,"test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus- but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort -space/time complexity- are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance. For these reasons, this paper calls for a discussion on POS taggers evaluation, aiming to establish a more rigorous test experimentation setting/designing, indispensable to extract reliable conclusions. As a starting point, we will focus only o"
P98-2164,P97-1031,1,0.87924,"Missing"
P98-2164,W96-0208,0,0.0310376,"ing tagger performances against a reference test corpus, and to make some criticism about common practices followed by the NLP researchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus- but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort -space/time complexity- are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a mult"
P98-2164,P97-1032,0,0.0293534,"nce correct answers are c o m p u t e d as wrong and vice-versa. In following sections we will show how this uncertainty in the evaluation m a y be, in some cases, larger than the reported improvements from one system to another, so invalidating the conclusions of the comparison. 3 Model since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to s t u d y the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant- absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. W h a t we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can find when evaluating the performance of a certain tagger are presented in table 1. OK/--aOK stand for a r i g h t / w r o n g tag (respect to the absolute correct disambiguation). When both the tagger and the test corpus have the correct tag, the tag is correctly evaluated as right."
pighin-etal-2012-analysis,H05-1098,0,\N,Missing
pighin-etal-2012-analysis,P07-2045,0,\N,Missing
pighin-etal-2012-faust,specia-etal-2010-dataset,0,\N,Missing
pighin-etal-2012-faust,P11-2027,0,\N,Missing
pighin-etal-2012-faust,2005.mtsummit-papers.11,0,\N,Missing
Q13-1018,W05-0620,1,0.913586,"Missing"
Q13-1018,D07-1101,1,0.741319,"Missing"
Q13-1018,W02-1001,0,0.0930078,"Missing"
Q13-1018,S12-1029,0,0.0736045,"ergences between the two layers. 219 Transactions of the Association for Computational Linguistics, 1 (2013) 219–230. Action Editor: Brian Roark. c Submitted 1/2013; Revised 3/2013; Published 5/2013. 2013 Association for Computational Linguistics. � SBJ Mary ARG 0 main contributions of this paper are: features in the semantic component (Gildea and JuP OPRD loves ARG 1 IM to OBJ play guitar . ARG 1 ARG 0 Figure 1: A sentence with (top) Figure 1: Ansyntactic exampledependencies ... and semantic dependencies for the predicates “loves” and “play” (bottom). The thick arcs illustrate a structural diDas et al. (2012) ... vergence where the argument “Mary” is linked to “play” with a path involving three syntactic Riedel and McCallum (2011) . dependencies. .. 3 A Syntactic-Semantic Dependency This is clearly seen in dependency-based representaModel tions of syntax and semantic roles (Surdeanu et al., rafsky, 2002;SRL Xueas and Palmer, 2004; Punyakanok • We frame a weighted assignment prob- et al.,lem 2008). However, without further assumptions, in a bipartite graph. Under this framework thiswe property makes the optimization problem can control assignment constraints betweencomputationally One simple roles"
Q13-1018,W09-1205,0,0.139785,"Missing"
Q13-1018,J02-3001,0,0.702903,"which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results. 1 Introduction Semantic role labeling (SRL) is the task of identifying the arguments of lexical predicates in a sentence and labeling them with semantic roles (Gildea and Jurafsky, 2002; M`arquez et al., 2008). SRL is an important shallow semantic task in NLP since predicate-argument relations directly represent semantic properties of the type “who” did “what” to “whom”, “how”, and “why” for events expressed by predicates (typically verbs and nouns). Predicate-argument relations are strongly related to the syntactic structure of the sentence: the majority of predicate arguments correspond to some syntactic constituent, and the syntactic structure that connects an argument with the predicate is a strong indicator of its semantic role. Actually, semantic Consequently, since th"
Q13-1018,P02-1031,0,0.0561981,"syntactic structure that connects an argument with the predicate is a strong indicator of its semantic role. Actually, semantic Consequently, since the first works, SRL systems have assumed access to the syntactic structure of the sentence (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). A simple approach is to obtain the parse trees as a pre-process to the SRL system, which allows the use of unrestricted features of the syntax. However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (Gildea and Palmer, 2002). A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (Toutanova et al., 2008; Punyakanok et al., 2008). As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (Surdeanu et al., 2008; Hajiˇc et al., 2009; Johansson, 2009; Titov et al., 2009; Llu´ıs et al., 2009), which is the focus of this paper. These joint models should favor the syntactic structure that is most consistent with the semanti"
Q13-1018,D09-1059,0,0.42298,"yntax. However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (Gildea and Palmer, 2002). A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (Toutanova et al., 2008; Punyakanok et al., 2008). As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (Surdeanu et al., 2008; Hajiˇc et al., 2009; Johansson, 2009; Titov et al., 2009; Llu´ıs et al., 2009), which is the focus of this paper. These joint models should favor the syntactic structure that is most consistent with the semantic predicate-argument structures of a sentence. In principle, these models can exploit syntactic and semantic features simultaneously, and could potentially improve the accuracy for both syntactic and semantic relations. One difficulty in the design of joint syntacticsemantic parsing models is that there exist important structural divergences between the two layers. 219 Transactions of the Association for Computational Ling"
Q13-1018,D07-1015,1,0.40731,"Missing"
Q13-1018,D10-1125,0,0.425569,"control assignment constraints betweencomputationally One simple roles and hard. arguments. Key to approximation our method, weis to usecan a pipeline model: compute the optimal efficiently searchfirst over a large space of syn-syntactic realizations semanticfor arguments. tactic tree, and then of optimize the best semantic structure given the syntactic tree. In the rest of the • We solve joint inference of syntactic and sepaper we describe a method that searches over synmantic dependencies with a dual decompositactic and semantic dependency structures jointly. tion method, similar to that of Koo et al. (2010). We impose the assumption syntactic Ourfirst system produces consistent that syntactic and features of the semantic component are restricted to the predicate-argument structures while searching syntactic a predicate and an argument, over a path large between space of syntactic configurations. following previous work (Johansson, 2009). ForIn the for experimental we compare jointr we mally, a predicatesection p, argument a and role and pipeline models. The final results of our joint will define a vector of dependency indicators π p,a,r syntactic-semantic system are competitive with the p,a,r si"
Q13-1018,W09-1212,1,0.909451,"Missing"
Q13-1018,J08-2001,1,0.470243,"Missing"
Q13-1018,P09-1039,0,0.0362687,"together with its syntactic path π p,a,r . As in the syntactic component, this function is typically defined as a linear function over a set of features of the semantic dependency and its path. The inference problem of our joint model is: argmax s syn(x, y) + s srl(x, z, π) y,z,π (4) subject to cTree : cRole : y is a valid dependency tree X ∀p, r : zp,a,r ≤ 1 a cArg : cPath : ∀p, a : X r zp,a,r ≤ 1 ∀p, a, r : if zp,a,r = 1 then π p,a,r is a path from p to a, otherwise π p,a,r = 0 cSubtree : ∀p, a, r : π p,a,r is a subtree of y Constraint cTree dictates that y is a valid dependency tree; see (Martins et al., 2009) for a detailed specification. The next two sets of constraints concern the semantic structure only. cRole imposes that each semantic role is realized at most once.2 Conversely, cArg dictates that an argument can realize at most one semantic role in a predicate. The final two sets of constraints model the syntactic-semantic interdependencies. cPath imposes that each π p,a,r represents a syntactic path between p and a whenever there exists a semantic relation. Finally, cSubtree imposes that the paths in π are consistent with the full syntactic structure, i.e. they are subtrees. 1 In this paper"
Q13-1018,P05-1012,0,0.158834,"A joint model for syntactic and semantic dependency parsing could be defined as: argmax s syn(x, y) + s srl(x, z, y) y,z . (1) In the equation, s syn(x, y) gives a score for the syntactic tree y. In the literature, it is standard to use arc-factored models defined as X s syn(x, y) = s syn(x, h, m, l) , (2) yh,m,l =1 where we overload s syn to be a function that computes scores for individual syntactic dependencies. In linear discriminative models one has s syn(x, h, m, l) = wsyn · fsyn (x, h, m, l), where fsyn is a feature vector for a syntactic dependency and wsyn is a vector of parameters (McDonald et al., 2005). In Section 6 we describe how we trained score functions with discriminative methods. The other term in Eq. 1, s srl(x, z, y), gives a score for a semantic dependency structure z using features of the syntactic structure y. Previous work has empirically proved the importance of exploiting syntactic features in the semantic component (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Punyakanok et al., 2008). However, without further assumptions, this property makes the optimization problem computationally hard. One simple approximation is to use a pipeline model: first compute the optimal synt"
Q13-1018,W04-2705,0,0.0805238,"Missing"
Q13-1018,P04-1043,0,0.0526438,"lem computationally hard. One simple approximation is to use a pipeline model: first compute the optimal syntactic tree y, and then optimize for the best semantic structure z given y. In the rest of the paper we describe a method that searches over syntactic and semantic dependency structures jointly. We first note that for a fixed semantic dependency, the semantic component will typically restrict the syntactic features representing the dependency to a specific subtree of y. For example, previous work has restricted such features to the syntactic path that links a predicate with an argument (Moschitti, 2004; Johansson, 2009), and in this paper we employ this restriction. Figure 1 gives an example of a subtree, where we highlight the syntactic path that connects the semantic dependency between “play” and “Mary” with role ARG 0. Formally, for a predicate p, argument a and role r we define a local syntactic subtree π p,a,r reprep,a,r sented as a vector: πh,m,l indicates if a dependency 221 hh, m, li is part of the syntactic path that links predicate p with token a and role r.1 Given full syntactic and semantic structures y and z it is trivial to construct a vector π that concatenates vectors π p,a,"
Q13-1018,P05-1013,0,0.095953,"Missing"
Q13-1018,J05-1004,0,0.127514,"ion algorithm seeks agreement at the level of individual dependencies. One dif225 ference is that our semantic process predicts partial syntax (restricted to syntactic paths connecting predicates and arguments), while in their case each of the two processes predicts the full set of dependencies. 6 Experiments We present experiments using our syntacticsemantic parser on the CoNLL-2009 Shared Task English benchmark (Hajiˇc et al., 2009). It consists of the usual WSJ training/development/test sections mapped to dependency trees, augmented with semantic predicate-argument relations from PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) also represented as dependencies. It also contains a PropBanked portion of the Brown corpus as an out-of-domain test set. Our goal was to evaluate the contributions of parsing algorithms in the following configurations: Base Pipeline Runs a syntactic parser and then runs an SRL parser constrained to paths of the best syntactic tree. In the SRL it only enforces constraint cArg, by simply classifying the candidate argument in each path into one of the possible semantic roles or as NULL. Pipeline with Assignment Runs the assignment algorithm for SRL, enforcing c"
Q13-1018,J08-2005,0,0.78721,"structure of the sentence (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). A simple approach is to obtain the parse trees as a pre-process to the SRL system, which allows the use of unrestricted features of the syntax. However, as in other pipeline approaches in NLP, it has been shown that the errors of the syntactic parser severely degrade the predictions of the SRL model (Gildea and Palmer, 2002). A common approach to alleviate this problem is to work with multiple alternative syntactic trees and let the SRL system optimize over any input tree or part of it (Toutanova et al., 2008; Punyakanok et al., 2008). As a step further, more recent work has proposed parsing models that predict syntactic structure augmented with semantic predicate-argument relations (Surdeanu et al., 2008; Hajiˇc et al., 2009; Johansson, 2009; Titov et al., 2009; Llu´ıs et al., 2009), which is the focus of this paper. These joint models should favor the syntactic structure that is most consistent with the semantic predicate-argument structures of a sentence. In principle, these models can exploit syntactic and semantic features simultaneously, and could potentially improve the accuracy for both syntactic and semantic relat"
Q13-1018,D11-1001,0,0.133702,"0. Action Editor: Brian Roark. c Submitted 1/2013; Revised 3/2013; Published 5/2013. 2013 Association for Computational Linguistics. � SBJ Mary ARG 0 main contributions of this paper are: features in the semantic component (Gildea and JuP OPRD loves ARG 1 IM to OBJ play guitar . ARG 1 ARG 0 Figure 1: A sentence with (top) Figure 1: Ansyntactic exampledependencies ... and semantic dependencies for the predicates “loves” and “play” (bottom). The thick arcs illustrate a structural diDas et al. (2012) ... vergence where the argument “Mary” is linked to “play” with a path involving three syntactic Riedel and McCallum (2011) . dependencies. .. 3 A Syntactic-Semantic Dependency This is clearly seen in dependency-based representaModel tions of syntax and semantic roles (Surdeanu et al., rafsky, 2002;SRL Xueas and Palmer, 2004; Punyakanok • We frame a weighted assignment prob- et al.,lem 2008). However, without further assumptions, in a bipartite graph. Under this framework thiswe property makes the optimization problem can control assignment constraints betweencomputationally One simple roles and hard. arguments. Key to approximation our method, weis to usecan a pipeline model: compute the optimal efficiently searc"
Q13-1018,D10-1001,0,0.0786099,"s srl(x, z, π) (2) yh,m,l =1 cate with the argument. We show how efficient predictions these models be made using aswhere wewith overload s syn can to be a function that signment algorithms in bipartite graphs. Simultacomputes scores for individual labeled syntactic neously, we use In a standard arc-factored dependency dependencies. discriminative models one has model that predicts the full syntactic tree of the sens syn(x, h, m, l) = wsyn · fsyn (x, h, m, l), where tence. Finally, we employ dual decomposition techfsyn is a feature vector for the syntactic dependency niques (Koo et al., 2010; Rush et al., 2010; Sontag and wsyn is a vector of parameters (McDonald et al., et al., 2010) to find agreement between the full de2005). pendency tree and the partial syntactic trees linking Thepredicate other term, z, y), gives a scorethe for each withsitssrl(x, arguments. In summary, a semantic dependency structure using the syntactic structure y as features. Previous work has empiri220 cally proved the importance of exploiting syntactic y,z,π predicate token p and argument token a labeled with semantic roletor. We will represent a semantic role subject structurecTree as a vector semantic depen: yzisindexed"
Q13-1018,W08-2121,1,0.616614,"Missing"
Q13-1018,J08-2002,0,0.383434,"Missing"
Q13-1018,W04-3212,0,0.0622858,"ndividual syntactic dependencies. In linear discriminative models one has s syn(x, h, m, l) = wsyn · fsyn (x, h, m, l), where fsyn is a feature vector for a syntactic dependency and wsyn is a vector of parameters (McDonald et al., 2005). In Section 6 we describe how we trained score functions with discriminative methods. The other term in Eq. 1, s srl(x, z, y), gives a score for a semantic dependency structure z using features of the syntactic structure y. Previous work has empirically proved the importance of exploiting syntactic features in the semantic component (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Punyakanok et al., 2008). However, without further assumptions, this property makes the optimization problem computationally hard. One simple approximation is to use a pipeline model: first compute the optimal syntactic tree y, and then optimize for the best semantic structure z given y. In the rest of the paper we describe a method that searches over syntactic and semantic dependency structures jointly. We first note that for a fixed semantic dependency, the semantic component will typically restrict the syntactic features representing the dependency to a specific subtree of y. For example,"
Q13-1018,C00-2137,0,0.106408,"Missing"
Q13-1018,N07-1070,0,\N,Missing
Q13-1018,J13-3006,1,\N,Missing
Q13-1018,W09-1201,1,\N,Missing
R19-1141,D18-1389,1,0.867559,"d dataset. Section 4 describes our method and features. Section 5 presents the experiments and the evaluation results. Finally, Section 7 concludes and points to some possible directions for future work. 1229 Proceedings of Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 20"
R19-1141,N19-1216,1,0.885016,"Missing"
R19-1141,D19-1565,1,0.88107,"78-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answer"
R19-1141,P08-1118,0,0.0399865,"Missing"
R19-1141,S17-2006,0,0.0513316,"Missing"
R19-1141,P15-2139,0,0.0249286,"tweets and temporal information (Ma et al., 2016). We also want to explore other multi-task learning options, e.g., as described in (Ruder, 2017). Figure 2: Ablation experiment with the multi model. Each row is an experiment removing one target. Each column is the MAP difference with respect to the multi model for the corresponding target. It would be interesting to investigate the reasons why the NYT source does not benefit from the multi-task architecture. In order to adapt to this situation with a single model, we plan to experiment with a network with soft parameter sharing, e.g., as in (Duong et al., 2015). For example, we could create a chain of layers that back-propagate to the input using only single task targets and then add an auxiliary layer that is shared between the tasks on the side. In this way, the model would be able to turn off the multi-task learning completely for some of the sources. However, training such kind of model might require significantly more training data; semi-supervised training might be a possible solution. Acknowledgments We would like to thank the anonymous reviewer, whose constructive feedback has helped us improve the quality of this paper. This work is part of"
R19-1141,W09-0439,0,0.0126282,"CW-USPD-2016 corpus contains four debates, we perform 4-fold cross-validation, where each time we leave one debate out for testing, and we train on the remaining three debates. Moreover, in order to stabilize the results, we repeat each experiment three times with different random seeds and we report the average over these three reruns of the system.4 4 Having multiple reruns is a standard procedure to stabilize an optimization algorithm that is sensitive to the random seed, e.g., this strategy has been argued for when using MERT for tuning hyper-parameters in Statistical Machine Translation (Foster and Kuhn, 2009). In our neural model, we used ReLU units and a shared layer of size 300. For training, we used Stochastic Gradient Descent with Nesterov momentum,5 iterating for 100 epochs. Recall that our main objective is to prioritize the claims that should be selected for manual factchecking, which is best achieved by proposing a ranked list of claims. Thus, we have a ranking task, for which we use suitable information retrieval evaluation measures. In particular, we adopt Mean Average Precision (MAP) as our primary evaluation measure. We further report RPrecision, or R-Pr, and precision at k, or P@k,6 f"
R19-1141,gencheva-etal-2017-context,1,0.605665,"Missing"
R19-1141,S19-2147,0,0.0306601,"news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data min"
R19-1141,N18-5006,1,0.895733,"Missing"
R19-1141,J15-3002,0,0.0278902,"IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden layers. During training, in the process of backpropagation, each task modifies the weights of its own task-specific layer and also of the shared layer. 1231 Figure 1: The architecture of our neural multi-task learning model, predicting whether each of the nine individual fact-checking organizations (tasks) would consider this sentence check-worthy and one cumulati"
R19-1141,D18-1388,0,0.0308285,"tural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERifi"
R19-1141,W16-2117,0,0.0319884,"been collected. Thus, the task can be reduced to recognizing textual entailment (Dagan et al., 2009). 1230 de Marneffe et al. (2008) also looked for contradictions in text. They tried to classify the contradictions that can be found in a piece of text in two categories —those occurring via antonymy, negation, and date/number mismatch, and those arising from different world knowledge and lexical contrasts. The features that are selected for the task of contradiction detection include polarity, numbers, dates and time, antonymy, factivity, modality, structural, and relational features. Finally, Le et al. (2016) used deep learning. They argued that the top terms in claim vs. nonclaim sentences are highly overlapping in content, which is a problem for bag-of-words approaches. Thus, they used a Convolutional Neural Network, where each word is represented by its embedding and each named entity is replaced by its tag, e.g., person, organization, location. Unlike the above work, we mimic the selection strategy of one specific fact-checking organization by learning to jointly predict the selection choices by multiple such organizations. 3 Data In our experiments, we used the CW-USPD2016 dataset from our pr"
R19-1141,K15-1032,1,0.805395,"f Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fac"
R19-1141,S19-2149,1,0.839641,"l., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a factchecking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation o"
R19-1141,N13-1090,0,0.0171527,"sk of checkworthiness prediction. In particular, from (Hassan et al., 2015b), we adopt TF.IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden layers. During training, in the process of backpropagation, each task modifies the weights of its own task-specific layer and also of the shared layer. 1231 Figure 1: The architecture of our neural multi-task learning model, predicting whether each of the nine individual fact-ch"
R19-1141,P18-1022,0,0.0483181,"related work. Section 3 describes the used dataset. Section 4 describes our method and features. Section 5 presents the experiments and the evaluation results. Finally, Section 7 concludes and points to some possible directions for future work. 1229 Proceedings of Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Na"
R19-1141,P13-1162,0,0.0835321,"hether each of the nine individual sources (tasks) would have selected it, and whether at least one of them would, which is the special task ANY. The input to our neural network consists of various domain-specific features that have been previously shown to work well for the task of checkworthiness prediction. In particular, from (Hassan et al., 2015b), we adopt TF.IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden la"
R19-1141,C18-1283,0,0.0892411,"d et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a factchecking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation of true and false news online. The first work to target check-worthiness estimation, i.e., predicting which sentences in a given input text should be prioritized for factchecking, was the ClaimBuster system (Hassan et al., 2015b)"
R19-1141,N18-1074,0,0.0902678,"Missing"
R19-1141,W14-2508,0,0.076833,"oint that attracted wide public attention to the problem. By then, a number of organizations, e.g., FactCheck1 and Snopes2 among many others, launched factchecking initiatives. Yet, this proved to be a very demanding manual effort, and only a relatively small number of claims could be fact-checked. Thus, it is important to prioritize what to check. 1 2 http://www.factcheck.org/ http://www.snopes.com/ Llu´ıs M`arquez Amazon Core ML lluismv@amazon.com The task of detecting check-worthy claims has been recognized as an important stage in the process of fully automatic fact-checking. According to Vlachos and Riedel (2014) this is a multistep process that (i) extracts statements to be fact-checked, (ii) constructs appropriate questions, (iii) obtains the answers from relevant sources, and (iv) reaches a verdict using these answers. Hassan et al. (2015a) presented a similar vision, and in a follow up work they made check-worthiness an integral part of an end-to-end fact-checking system Hassan et al. (2017). Here, we approach the problem of mimicking the selection strategy of several renowned fact-checking organizations such as PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and The Wash"
R19-1141,D19-3038,1,0.810868,"Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova"
S01-1017,W99-0606,0,0.0129816,"G. Rigau TALP Research Center Universitat Politecnica de Catalunya Jordi Girona Salgado, 1-3 Barcelona, Catalonia, Spain {escudero,lluism,g.rigau}@lsi.upc.es Abstract The particular algorithm used in our system to perform the classification of senses is the generalized AdaBoost.MH with confidence-rated predictions (Schapire and Singer, 1999). This algorithm is able to deal straightforwardly with multiclass multi-label problems, and has been previously applied, with significant success, to a number of NLP disambiguation tasks, including, among others: Part-of-speech tagging and PP-attachment (Abney et al., 1999), text categorization (Schapire and Singer, 2000), and shallow parsing (Carreras and Marquez, 2001). The weak hypotheses used in this work are decision stumps, which can be seen as extremely simple decision trees with one internal node testing the value of a single binary feature (e.g. &quot;the word dark appears in the context of the word to be disambiguated?&quot;) and two leaves that give the prediction of the senses based on the feature value. The &quot;Lazy&quot; Boosting, is a simple modification of the AdaBoost.MH algorithm, which consists of reducing the feature space that is explored when learning each w"
S01-1017,W01-0726,1,0.809627,"-3 Barcelona, Catalonia, Spain {escudero,lluism,g.rigau}@lsi.upc.es Abstract The particular algorithm used in our system to perform the classification of senses is the generalized AdaBoost.MH with confidence-rated predictions (Schapire and Singer, 1999). This algorithm is able to deal straightforwardly with multiclass multi-label problems, and has been previously applied, with significant success, to a number of NLP disambiguation tasks, including, among others: Part-of-speech tagging and PP-attachment (Abney et al., 1999), text categorization (Schapire and Singer, 2000), and shallow parsing (Carreras and Marquez, 2001). The weak hypotheses used in this work are decision stumps, which can be seen as extremely simple decision trees with one internal node testing the value of a single binary feature (e.g. &quot;the word dark appears in the context of the word to be disambiguated?&quot;) and two leaves that give the prediction of the senses based on the feature value. The &quot;Lazy&quot; Boosting, is a simple modification of the AdaBoost.MH algorithm, which consists of reducing the feature space that is explored when learning each weak classifier. More specifically, a small proportion of attributes are randomly selected and the b"
S01-1017,magnini-cavaglia-2000-integrating,0,0.0680963,"where the last three correspond to collocations of two consecutive words. The topical context is formed by c1, ... , Cm, which stand for the unordered set of open class words appearing in a medium-size 21-word window centered around the target word. The more innovative use of semantic domain information is detailed in the next section. 1.2.1 Domain Information We have enriched the basic set of features by adding semantic information in the form of domain labels. These domain labels are computed during a preprocessing step using the 164 domain labels linked to the nominal part of WordNet 1.6 (Magnini and Cavaglia, 2000). For each training example, a program gathers, from its context, all nouns and their synsets with the attached domain labels, and scores them according to a certain scoring function. The weights assigned by this function depend on the number of domain labels assigned to each noun and their relative frequencies in the whole WordNet. The result of this procedure is the set of domain labels that achieve a score higher than a certain experimentally set threshold, which are incorporated as regular features for describing the example. Two different simplifications have been carried out. Firstly, mu"
S01-1017,H93-1052,0,0.0707811,"Missing"
S07-1077,W01-0703,1,0.803584,"ematic roles are allowed. 3 Including Selectional Preferences Selectional Preferences (SP) try to capture the fact that linguistic elements prefer arguments of a certain semantic class, e.g. a verb like ‘eat’ prefers as subject edible things, and as subject animate entities, as in “She was eating an apple” They can be learned from corpora, generalizing from the observed argument heads (e.g. ‘apple’, ‘biscuit’, etc.) into abstract classes (e.g. edible things). In our case we 1 http://mallet.cs.umass.edu Restriction 5 applies to PropBank output. Restriction 6 applies to VerbNet output 2 follow (Agirre and Martinez, 2001) and use WordNet (Fellbaum, 1998) as the generalization classes (the concept &lt;food,nutrient>). The aim of using Selectional Preferences (SP) in SRL is to generalize from the argument heads in the training instances into general word classes. In theory, using word classes might overcome the data sparseness problem for the head-based features, but at the cost of introducing some noise. More specifically, given a verb, we study the occurrences of the target verb in a training corpus (e.g. the PropBank corpus), and learn a set of SPs for each argument and adjunct of that verb. For instance, given"
S07-1077,W04-2415,1,0.897212,"Missing"
S07-1077,J02-3001,0,0.595616,"selected by exploring the sentence spans or regions defined by the clause boundaries, and they are labeled with BIO tags depending on the location of the token: at the beginning, inside, or outside of a verb argument. After this data pre-processing step, we obtain a more compact and easier to process data representation, making also impossible overlapping and embedded argument predictions. 2.2 Feature Representation Apart from Selectional Preferences (cf. Section 3) and those extracted from provided semantic information, most of the features we used are borrowed from the existing literature (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., forthcoming). 354 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357, c Prague, June 2007. 2007 Association for Computational Linguistics On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase; Verb voice; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. • VerbNet class of the verb (in the ”close” track only). On the focus constituent: • Type; Head; • First and last words and POS t"
S07-1077,J05-1004,0,0.0932139,"set and it ranks first in the SRL subtask of the Semeval-2007 task 17. 1 We participated in both the “close” and the “open” tracks of Semeval2007 with the same system, making use, in the second case, of the larger CoNLL2005 training set. 2 System Description 2.1 Data Representation Introduction In Semantic Role Labeling (SRL) the goal is to identify word sequences or arguments accompanying the predicate and assign them labels depending on their semantic relation. In this task we disambiguate argument structures in two ways: predicting VerbNet (Kipper et al., 2000) thematic roles and PropBank (Palmer et al., 2005) numbered arguments, as well as adjunct arguments. In this paper we describe our system for the SRL subtask of the Semeval2007 task 17. It is based on the architecture and features of the system named ‘model 2’ of (Surdeanu et al., forthcoming), but it introduces two changes: we use Maximum Entropy for learning instead of AdaBoost and we enlarge the feature set with combined features and other semantic features. Traditionally, most of the features used in SRL are extracted from automatically generated syntactic and lexical annotations. In this task, we also experiment with provided hand labele"
S07-1077,W04-3212,0,0.413048,"sentence spans or regions defined by the clause boundaries, and they are labeled with BIO tags depending on the location of the token: at the beginning, inside, or outside of a verb argument. After this data pre-processing step, we obtain a more compact and easier to process data representation, making also impossible overlapping and embedded argument predictions. 2.2 Feature Representation Apart from Selectional Preferences (cf. Section 3) and those extracted from provided semantic information, most of the features we used are borrowed from the existing literature (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., forthcoming). 354 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357, c Prague, June 2007. 2007 Association for Computational Linguistics On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase; Verb voice; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. • VerbNet class of the verb (in the ”close” track only). On the focus constituent: • Type; Head; • First and last words and POS tags of the constituent"
S07-1095,atserias-etal-2006-freeling,0,0.0251929,"Missing"
S07-1095,W03-0421,1,0.890417,"Missing"
S07-1095,P02-1034,0,0.0541836,"Missing"
S07-1095,H05-1081,1,0.897279,"Missing"
S07-1095,S07-1008,1,0.816864,"Missing"
S07-1095,P05-1073,0,0.0449545,"to the features used, we focus only on global features that can be extracted independently of the local models. We show in Section 6 that this approach performs better on the small SemEval corpora than approaches that include features from the local models. We group the features into two sets: (a) features that extract information from the whole candidate set, and (b) features that model the structure of each candidate frame: Features from the whole candidate set: (1) Position of the current candidate in the whole set. Frame candidates are generated using the dynamic programming algorithm of Toutanova et al. (2005), and then sorted in descending order of the log probability of the whole frame (i.e., the sum of all argument log probabilities as reported by the local model). Hence, smaller positions indicate candidates that the local model considers better. (2) For each argument in the current frame, we store its number of repetitions in the whole candidate set. The intuition is that an argument that appears in many candidate frames is most likely correct. Features from each candidate frame: (3) The complete sequence of argument labels, extended with the predicate lemma and voice, similar to Toutanova et"
S10-1001,W99-0212,0,0.0429338,"cation of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sw"
S10-1001,S10-1022,0,0.0889833,"Missing"
S10-1001,orasan-etal-2008-anaphora,0,0.0341483,"o to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Catalan Dutch English German Italian Spanish #docs Training #sents #tokens #docs 829 145 229 900 80 875 8,709 2,544 3,648 19,233 2,951 9,022 2"
S10-1001,S10-1021,1,0.673549,"Missing"
S10-1001,W99-0707,0,0.0438335,"6 English The OntoNotes Release 2.0 corpus (Pradhan et al., 2007) covers newswire and broadcast news data: 300k words from The Wall Street Journal, and 200k words from the TDT-4 collection, respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license."
S10-1001,doddington-etal-2004-automatic,0,0.0804398,"sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Catalan Dutch English German Italian Spanish #docs Training #sents #tokens #docs 829 145 229 900 80 875 8,709 2"
S10-1001,rodriguez-etal-2010-anaphoric,1,0.142463,"Missing"
S10-1001,S10-1017,1,0.853056,"Missing"
S10-1001,W08-1007,0,0.0168985,"annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license.2 Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3 The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March"
S10-1001,C08-1098,0,0.00615627,"respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license.2 Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3 The German and Dutch training datasets were not completely stable during the competition per"
S10-1001,W05-0303,0,0.0585819,"Missing"
S10-1001,hoste-de-pauw-2006-knack,1,0.856981,"Missing"
S10-1001,S10-1020,0,0.0536547,"Missing"
S10-1001,van-noord-etal-2006-syntactic,0,0.0179959,"Missing"
S10-1001,S10-1018,0,0.147949,"Missing"
S10-1001,M95-1005,0,0.744599,"d). Results are presented sequentially by language and setting, and participating systems are ordered alphabetically. The participation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm concluEvaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1 . Mentions are rewarded with 1 point if their boundaries coincide with those 11 4 Only 45 entries in Table 5 from 192 potential cases. BART (Broscheit et al., 2010) Corry (Uryupina, 2010) RelaxCor (Sapena et al., 2010) SUCRE (Kobdani and Sch¨utze, 2010) TANL-1 (Attardi et al., 2010) UBIU (Zhekova and K¨ubler, 2010) System Architecture ML Methods External Resources Closest"
S10-1001,H05-1004,0,0.516482,"st scores in each setting are highlighted in bold). Results are presented sequentially by language and setting, and participating systems are ordered alphabetically. The participation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm concluEvaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1 . Mentions are rewarded with 1 point if their boundaries coincide with those 11 4 Only 45 entries in Table 5 from 192 potential cases. BART (Broscheit et al., 2010) Corry (Uryupina, 2010) RelaxCor (Sapena et al., 2010) SUCRE (Kobdani and Sch¨utze, 2010) TANL-1 (Attardi et al., 2010) UBIU (Zhekova and K¨ubler, 2010)"
S10-1001,S10-1019,0,0.0843106,"Missing"
S10-1001,C10-2125,1,\N,Missing
S10-1001,M98-1029,0,\N,Missing
S13-1020,W05-0909,0,0.388265,"Missing"
S13-1020,P04-1077,0,0.118587,"Missing"
S13-1020,W05-0904,0,0.032249,"omatic translation, or vice versa. Some of the metrics are not symmetric so we compute similarity between s1 and s2 in both directions and average the resulting scores. The measures are computed with the Asiya Toolkit for Automatic MT Evaluation (Gim´enez and M`arquez, 2010b). The only pre-processing carried out was tokenization (Asiya performs additional inbox pre-processing operations, though). We consid2 We also tried with linear kernels, but RBF always obtained better results. 144 Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (Liu and Gildea, 2005): DP-HWCMic-4 for grammatical categories chains, DP-HWCMir-4 over grammatical relations, and DP-Or (⋆) over words ruled by non-terminal nodes. Also, one measure that estimates the similarity over constituent parse trees: CP-STM4 (Liu and Gildea, 2005). Semantic Similarity Three measures that estimate the similarities over semantic roles (i.e. arguments and adjuncts): SR-Or , SR-Mr (⋆), and SR-Or (⋆). Additionally, two metrics that estimate similarities over discourse representations: DR-Or (⋆) and DR-Orp(⋆). 3 Asiya is available at http://asiya.lsi.upc.edu. Full descriptions of the metrics are"
S13-1020,P02-1040,0,0.0951332,"Missing"
S13-1020,2006.amta-papers.25,0,0.13064,"Missing"
S13-1020,S12-1060,0,0.109397,"Missing"
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
S15-2047,S15-2048,0,0.127647,"Missing"
S15-2047,N10-1145,0,0.0124059,"A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment,"
S15-2047,S15-2039,0,0.0505919,"Missing"
S15-2047,S15-2035,0,0.132649,"Missing"
S15-2047,S15-2040,0,0.0642528,"Missing"
S15-2047,P07-1098,1,0.123047,"are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with featu"
S15-2047,S15-2036,1,0.483036,"Missing"
S15-2047,D14-1162,0,0.0890449,"tures above can be binary, integer, or real-valued, e.g., can be calculated using various weighting schemes such as TF.IDF for words/lemmata/stems. Although most participants focused on engineering features to be used with a standard classifier such as SVM or a decision tree, some also used more advanced techniques. For example, some teams used sequence or partial tree kernels (Moschitti, 2006). Another popular technique was to use word embeddings, e.g., modeled using convolution or recurrent neural networks, or with latent semantic analysis, and also vectors trained using word2vec and GloVe (Pennington et al., 2014), as pre-trained on Google News or Wikipedia, or trained on the provided Qatar Living data. Less popular techniques included dialog modeling for the list of comments for a given question, e.g., using conditional random fields to model the sequence of comment labels (Good, Bad, Potential, Dialog), mapping the question and the comment to a graph structure and performing graph traversal, using word alignments between the question and the comment, time modeling, and sentiment analysis. Finally, for Arabic, some participants translated the Arabic data to English, and then extracted features from bo"
S15-2047,S15-2044,0,0.0450275,"Missing"
S15-2047,D13-1044,1,0.0589439,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. For Arabic, we also made use of a real cQA portal, the Fatwa website,3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars. For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic. Overall for the task, we needed manual annotations in two different languages an"
S15-2047,D07-1002,0,0.0113579,"focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied line"
S15-2047,P08-1082,0,0.294043,"Eval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to"
S15-2047,S15-2038,0,0.257423,"Missing"
S15-2047,S15-2041,0,0.062348,"Missing"
S15-2047,C10-1131,0,0.028049,"Missing"
S15-2047,D07-1003,0,0.0108685,"hop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; t"
S15-2047,N13-1106,0,0.0203816,"Missing"
S15-2047,S15-2042,0,0.0485548,"Missing"
S15-2047,S15-2043,0,0.0966631,"Missing"
S15-2047,S15-2037,0,0.0693424,"Missing"
S16-1083,S16-1128,1,0.911614,"2 MAP points over the IR baseline). They use distributed representations of words, knowledge graphs generated with BabelNet, and frames from FrameNet. Their contrastive2 run is even better, with MAP of 77.33. The second best system is that of ConvKN (Barr´on-Cede˜no et al., 2016) with MAP of 76.02; they are also first on MRR, second on AvgRec and F1 , and third on Accuracy. The third best system is KeLP (Filice et al., 2016) with MAP of 75.83; they are also first on AvgRec, F1 , and Accuracy. They have a contrastive run with MAP of 76.28, which would have ranked second. The fourth best, SLS (Mohtarami et al., 2016) is very close, with MAP of 75.55; it is also first on MRR and Accuracy, and third on AvgRec. It uses a bag-of-vectors approach with various vector- and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers. 6.3 Subtask C, English (Question-External Comment Similarity) The results for subtask C, English are shown in Table 5. This subtask attracted 10 teams, and 28 runs: 10 primary and 18 contrastive. Here the teams performed much better than they did for subtask B. The first three baselines were al"
S16-1083,P07-1098,1,0.303795,"cess of their creation. Section 5 explains the evaluation measures. Section 6 presents the results for all subtasks and for all participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree"
S16-1083,S15-2047,1,0.907588,"Missing"
S16-1083,S15-2036,1,0.813812,"Missing"
S16-1083,D13-1044,1,0.636695,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. Using information about the thread is another important direction. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, whether the answer is first, whether the answer is last (Hou et al., 2015). Similarly, the third-best team, QCRI,"
S16-1083,P08-1082,0,0.0152363,"participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to a"
S16-1083,D07-1002,0,\N,Missing
S16-1083,N10-1145,0,\N,Missing
S16-1083,S15-2035,0,\N,Missing
S16-1083,C10-1131,0,\N,Missing
S16-1083,N13-1106,0,\N,Missing
S16-1083,P15-1078,1,\N,Missing
S16-1083,P15-2113,1,\N,Missing
S16-1083,R15-1058,1,\N,Missing
S16-1083,S16-1130,1,\N,Missing
S16-1083,S16-1138,1,\N,Missing
S16-1083,S16-1126,0,\N,Missing
S16-1083,S16-1132,0,\N,Missing
S16-1083,S16-1134,0,\N,Missing
S16-1083,S16-1136,1,\N,Missing
S16-1083,S16-1133,0,\N,Missing
S16-1083,S16-1172,1,\N,Missing
S16-1083,S16-1137,1,\N,Missing
S16-1083,S16-1131,0,\N,Missing
S16-1083,N16-1084,1,\N,Missing
S16-1083,S16-1135,0,\N,Missing
S16-1083,N16-1152,1,\N,Missing
S16-1083,P16-2075,1,\N,Missing
S16-1083,P16-2065,1,\N,Missing
S16-1083,S15-2037,0,\N,Missing
S16-1083,D15-1068,1,\N,Missing
S16-1083,K15-1032,1,\N,Missing
S16-1137,S16-1130,1,0.829487,"uld work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar Foundation. It is part of the Interactive sYstems for Answer Search (Iyas) project, which is developed in collaboration with M"
S16-1137,P15-2113,1,0.3401,"Missing"
S16-1137,2012.eamt-1.60,0,0.0222671,"of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty. We will refer to the set of these features as BLEU COMP. 4.3 Task-specific features QL VEC (in MTE-NN-improved only). Similarly to the G OOGLE VEC, but on task-specific data, we train word vectors using WORD 2 VEC on all available cQA training data (Qatar Living) and use them as input to the NN. QL+IWSLT VEC (in MTE-NN-{primary, contrastive1/2} only). We also use trained word vectors on the concatenation of the cQA training data and the English portion of the IWSLT data, which consists of TED talks (Cettolo et al., 2012) and is thus informal and somewhat similar to cQA data. TASK FEAT. We further extract various taskspecific skip-arc features, most of them proposed for the 2015 edition of the task (Nakov et al., 2015). This includes some comment-specific features: • number of URLs/images/emails/phone numbers; • number of occurrences of the string thank;3 • number of tokens/sentences; • average number of tokens; • type/token ratio; • number of nouns/verbs/adjectives/adverbs/pronouns; • number of positive/negative smileys; • number of single/double/triple exclamation/interrogation symbols; • number of interroga"
S16-1137,P15-2114,0,0.102811,"we used our system for subtask A to solve subtask C, which asks to find good answers to a new question that was not asked before in the forum by reranking the answers to related questions. For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google). Without any subtask C specific addition, we achieved the fourth best result in the task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al."
S16-1137,P03-1003,0,0.0981864,"asks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who app"
S16-1137,P15-1078,1,0.448296,"Missing"
S16-1137,P16-2075,1,0.558646,"Missing"
S16-1137,D15-1068,1,0.265429,"Missing"
S16-1137,N16-1084,1,0.557796,"Missing"
S16-1137,P11-1143,0,0.386837,"al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a differ"
S16-1137,N12-1019,0,0.0191253,"2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreover, instead of using MTE metrics as features, we port an entire MTE framework to the cQA problem. 3 Neural Model for Answer Ranking The NN model we use for answer ranking is depicted in Figure 1. It is a direct adaptation of the feed-forward NN for MTE described in (Guzm´an et al., 2015). Technically, we have a binary classification task with input (q, c1 , c2 ), which should output 1 if c1 is a better answer to q than c2 , and 0 otherwise.2 The network computes a sigmoid"
S16-1137,P16-2065,1,0.435928,"we have adopted a pairwise neural network architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings of the input texts that are non-linearly combined in the hidden layer. 892 Our post-competition improvements have shown state-of-the-art performance (Guzm´an et al., 2016), with sizeable contribution from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions a"
S16-1137,S16-1136,1,0.763699,"we have adopted a pairwise neural network architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings of the input texts that are non-linearly combined in the hidden layer. 892 Our post-competition improvements have shown state-of-the-art performance (Guzm´an et al., 2016), with sizeable contribution from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions a"
S16-1137,K15-1032,1,0.168613,"from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HB"
S16-1137,R15-1058,1,0.13714,"from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HB"
S16-1137,N13-1090,0,0.0509505,"(q, c1 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). 4 Learning Features We experiment with three kinds of features: (i) input embeddings, (ii) features motivated by previous work on Machine Translation Evaluation (MTE) (Guzm´an et al., 2015) and (iii) task-specific features, mostly proposed by participants in the 2015 edition of the task (Nakov et al., 2015). 4.1 Embedding Features We use the following vector-based embeddings of (q, c1 , c2 ) as input to the NN: • G OOGLE VEC: We use the pre-trained, 300dimensional embedding vectors, which Tomas Mikolov trained on 100 billion words from Google News (Mikolov et al., 2013). • S YNTAX VEC: We parse the entire question/comment text using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. 889 Moreover, we use the above vectors to calculate pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evalu"
S16-1137,S15-2047,1,0.355744,"Missing"
S16-1137,S15-2036,1,0.557554,"Missing"
S16-1137,P02-1040,0,0.113293,"late pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candidate answer: • B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). • NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). • TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). • M ETEOR: A measure that matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). • P RECISION: measure, originating in information retrieval. • R ECALL: another measure coming from information retrieval. BLEU COMP. Following (Guzm´an et al., 2015), we further use as features various components that are"
S16-1137,P07-1059,0,0.170705,"s et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. Howeve"
S16-1137,2006.amta-papers.25,0,0.118347,"only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candidate answer: • B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). • NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). • TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). • M ETEOR: A measure that matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). • P RECISION: measure, originating in information retrieval. • R ECALL: another measure coming from information retrieval. BLEU COMP. Following (Guzm´an et al., 2015), we further use as features various components that are involved in the computation of B LEU: n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), lengths of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty. We will refer to the set of these f"
S16-1137,P13-1045,0,0.067615,"nput embeddings, (ii) features motivated by previous work on Machine Translation Evaluation (MTE) (Guzm´an et al., 2015) and (iii) task-specific features, mostly proposed by participants in the 2015 edition of the task (Nakov et al., 2015). 4.1 Embedding Features We use the following vector-based embeddings of (q, c1 , c2 ) as input to the NN: • G OOGLE VEC: We use the pre-trained, 300dimensional embedding vectors, which Tomas Mikolov trained on 100 billion words from Google News (Mikolov et al., 2013). • S YNTAX VEC: We parse the entire question/comment text using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. 889 Moreover, we use the above vectors to calculate pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candid"
S16-1137,J11-2003,0,0.0693038,"election (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreo"
S16-1137,S15-2038,0,0.178473,"oschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreover, instead of usin"
S16-1137,P15-2116,0,0.0572579,"Missing"
S16-1137,P15-1025,0,0.0622365,"the network. Finally, we used our system for subtask A to solve subtask C, which asks to find good answers to a new question that was not asked before in the forum by reranking the answers to related questions. For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google). Without any subtask C specific addition, we achieved the fourth best result in the task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Br"
S17-2003,S17-2044,0,0.0542572,"Missing"
S17-2003,N10-1145,0,0.016031,"es used by these systems and provides further discussion. Finally, Section 6 presents the main conclusions. 2 Question-answer similarity has been a subtask (subtask A) of our task in its two previous editions (Nakov et al., 2015, 2016b). This is a wellresearched problem in the context of general question answering. One research direction has been to try to match the syntactic structure of the question to that of the candidate answer. For example, Wang et al. (2007) proposed a probabilistic quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers. Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs. Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees. Yao et al. (2013) applied linear chain conditional random fields (CRFs) with features derived from TED to learn associations between questions and candidate answers. Moreover, syntactic structure was central for some of the top systems that participated in SemEval-2016 Task 3 (Filice et al., 2016; Barr´on-Cede˜no et al., 2016). Related Work The first step to automatically answer questions on"
S17-2003,C16-2001,1,0.881977,"Missing"
S17-2003,S15-2035,0,0.0226369,"andidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, such as whether the answer is first or last (Hou et al., 2015). Similarly, the third-best team, QCRI, used features to model a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolutional neural networks to recognize good comments (Zhou et al., 2015b). In follow-up work, Zhou et al. (2015a) included long-short term memory (LSTM) units in their convolutional neural network to model the classification sequence for the thread, and Barr´on-Cede˜no et al. (2015) exploited the"
S17-2003,K15-1032,1,0.0248911,"to make more consistent global decisions about the goodness of the answers in the thread. They modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In follow up work, Joty et al. (2016) proposed joint learning models that integrate inference within the learning process using global normalization and an Ising-like edge potential. 5 https://github.com/tbmihailov/ semeval2016-task3-cqa 6 Using a heuristic that if several users call somebody a troll, then s/he should be one (Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016a; Mihaylov et al., 2017b). 30 Category Original Questions Train+Dev+Test from SemEval-2015 – Train(1,2)+Dev+Test from SemEval-2016 (200+67)+50+70 2,480+291+319 – – – (1,999+670)+500+700 (181+54)+59+81 (606+242)+155+152 (1,212+374)+286+467 880 24 139 717 – (19,990+6,700)+5,000+7,000 8,800 – – – (1,988+849)+345+654 (16,319+5,154)+4,061+5,943 (1,683+697)+594+403 246 8,291 263 (14,110+3,790)+2,440+3,270 2,930 (5,287+1,364)+818+1,329 (6,362+1,777)+1,209+1,485 (2,461+649)+413+456 1,523 1,407 0 Related Questions – Perfect Match – Relevant – Irrelevant Related Comments (w"
S17-2003,S17-2009,0,0.0610799,"Missing"
S17-2003,S15-2036,1,0.824235,"Missing"
S17-2003,J11-2003,0,0.0485113,"ting systems across all three subtasks. This includes fine-tuned word embeddings5 (Mihaylov and Nakov, 2016b); features modeling text complexity, veracity, and user trollness6 (Mihaylova et al., 2016); sentiment polarity features (Nicosia et al., 2015); and PMI-based goodness polarity lexicons (Balchev et al., 2016; Mihaylov et al., 2017a). Yet another research direction has been on using machine translation models as features for question-answer similarity (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016a; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features tha"
S17-2003,D16-1244,0,0.0146703,"Missing"
S17-2003,S17-2059,0,0.0505879,"Missing"
W00-0706,J98-1001,0,0.252615,"Missing"
W00-0706,kilgarriff-rosenzweig-2000-english,0,0.0194361,"ure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense. Thus, only the single most reliable piece of evidence is used to perform disambiguation. Regarding the details of implementation (smoothing, pruning of the decision list, etc.) we have followed (Agirre and Martinez, 2000). Decision Lists were one of the most successful systems on the 1st Senseval competition for WSD (Kilgarriff and Rosenzweig, 2000). E x e m p l a r - b a s e d Classifier (EB). In exemplar, instance, or memory-based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are simply stored in memory and the classification of new examples is based on the most similar stored exemplars. In our implementation, all examples are kept in memory and the classification is based on a k-NN (Nearest-Neighbours) algorithm using Hamming distance to measure closeness. For k's greater than 1, the resulting sense is the weighted majority sense of the k nearest neighbours --where each example"
W00-0706,J98-1006,0,0.0308183,"31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state-of-the-art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). are &quot;relevant&quot; for their class. When classifying a new example, SNo W is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNo W for WSD is explained in (Escudero et al., 2000c). S N o W is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context-sensitive spelling correction, POS tagging, P P - a t t a c h m e n t disambiguation, etc. D e c i s i o n L i s t s (DL). In this setting"
W00-0706,W96-0208,0,0.15912,"{escudero, lluism, g.rigau}@Isi.upc.es Abstract form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD - - i n the style of other studies devote"
W00-0706,P96-1006,0,0.0522569,"Missing"
W00-0706,W99-0502,0,0.0411649,"Missing"
W00-0706,W97-0323,0,0.175474,"a (UPC) Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia {escudero, lluism, g.rigau}@Isi.upc.es Abstract form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of th"
W00-0706,A97-1015,0,0.0261339,"revious methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997; Ratnaparkhi, 1999)-- is needed to assure the validity of the supervised approach, and to determine to which extent a tuning pre-process is necessary to make real WSD systems portable. In this direction, this work compares five different M L algorithms and explores their portability and tuning ability by training and testing them on different corpora. This paper describes a set of comparative experiments, including cross-corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNOW, Decision Lists, and"
W00-0706,P94-1013,0,0.231419,"a r d E s c u d e r o and L l u i s M h r q u e z and G e r m a n R i g a u TALP Research Center. LSI Department. Universitat Polit~cnica de C a t a l u n y a (UPC) Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia {escudero, lluism, g.rigau}@Isi.upc.es Abstract form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard M L algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WS"
W00-0706,J99-2002,0,\N,Missing
W00-0706,J98-1005,0,\N,Missing
W00-0706,J98-4002,0,\N,Missing
W00-0706,J96-2004,0,\N,Missing
W00-0706,W00-1702,0,\N,Missing
W00-1322,J98-4002,0,0.0999223,"Missing"
W00-1322,P92-1032,0,0.0581055,"o the opinion of other authors (Ng, 1997b)): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to whi"
W00-1322,J98-1001,0,0.0568154,"Missing"
W00-1322,kilgarriff-rosenzweig-2000-english,0,0.0348502,"Missing"
W00-1322,J98-1006,0,0.218572,"ent corpora. Additionally, supervised methods suffer from the &quot;knowledge acquisition bottleneck&quot; (Gale et al., 1992a). (Ng, 1997b) estimates that the manual annotation effort necessary to build a broad coverage semantically annotated English corpus is about 16 personyears. This overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain. Due to this fact, recent works have focused on reducing the acquisition cost as well as the need for supervision in corpus-based methods. It is our belief that the research by (Leacock et al., 1998; Mihalcea and Moldovan, 1999) 2 provide enough evidence towards the &quot;opening&quot; of the bottleneck in the near future. For that reason, it is worth further investigating the robustness and portability of existing supervised ML methods to better resolve the WSD problem. It is important to note that the focus of this work will be on the empirical crosscorpus evaluation of several M L supervised algorithms. Other important issues, such as: selecting the best attribute set, discussing an appropriate definition of senses for the task, etc., are not addressed in this paper. 2 2.1 Learning Algorithms T"
W00-1322,W96-0208,0,0.0162853,"sical setting (Duda and Hart, 1973). T h a t is, assuming independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during training process using relative frequencies. To avoid the effect of zero counts when estimating probabilities, a very simple smoothing technique has been used, which was proposed in (Ng, 1997a). Despite its simplicity, Naive Bayes is claimed to obtain state-of-theart accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997a; Leacock et al., 1998). 2.2 E x e m p l a r - b a s e d Classifier (EB) In Exemplar-based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are stored in memory and the classification of new examples is based on the classes of the most similar stored examples. In our implementation, all examples are kept in memory and the classification of a new example is based on a k-NN (Nearest-Neighbours) algorithm using Hamming distance 3 to measure closeness (in doing so, all examples are examined). For k's greater t h a n 1, the resulting"
W00-1322,P96-1006,0,0.610856,"Ng, 1997b)): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is n"
W00-1322,W97-0323,0,0.188713,"e aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary t"
W00-1322,W97-0201,0,0.712724,"e aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. 172 This paper is organized as follows: Section 2 presents the four ML algorithms compared. In section 3 the setting is presented in detail, including the corpora and the experimental methodology used. Section 4 reports the experiments carried out and the results obtained. Finally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary t"
W00-1322,A97-1015,0,0.0120727,"nally, section 5 concludes and outlines some lines for further research. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a), in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover &quot;all&quot; potential types of examples. To date, a thorough study of the domain dependence of WSD - - i n the style of other studies devoted to parsing (Sekine, 1997)-has not been carried out. We think that such an study is needed to assess the validity of the supervised approach, and to determine to which extent a tuning process is necessary to make real WSD systems portable. In order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ML algorithms (previously applied to WSD) by training and testing them on different corpora. Additionally, supervised methods suffer from the &quot;knowledge acquisition bottleneck&quot; (Gale et al., 1992a). (Ng, 1997b) estimates that the manual annotation effort necessary to buil"
W00-1322,J98-1005,0,0.0162043,"Missing"
W00-1322,P94-1013,0,0.0919327,"Missing"
W00-1322,J99-2002,0,\N,Missing
W00-1322,W00-1702,0,\N,Missing
W02-2004,W01-0726,1,\N,Missing
W03-0421,W01-0726,1,0.870152,"Missing"
W03-0421,W02-2004,1,0.561477,"Missing"
W03-0422,W02-1001,0,0.0206526,"ng strategy works online at sentence level. When visiting a sentence, the functions being learned are first used to recognize the NE phrases, and then updated according to the correctness of their solution. We analyze the dependencies among the involved perceptrons and a global solution in order to design a global update rule based on the recognition of namedentities, which reflects to each individual perceptron its committed errors from a global perspective. The learning approach presented here is closely related to –and inspired by– some recent works in the area of NLP and Machine Learning. Collins (2002) adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback. Crammer and Singer (2003) presented an online topic-ranking algorithm involving several perceptrons and ranking-based update rules for training them. 2 Named-Entity Phrase Chunking In this section we describe our NERC approach as a phrase chunking problem. First we formalize the problem of NERC, then we propose a NE-Chunker. 2.1 Problem Formalization Let x be a sentence belonging to the sentence space X , formed by n words xi with i ranging from 0 to n − 1. Let K be the set of NE categories, which"
W03-1504,W02-2004,1,0.882393,"Missing"
W03-1504,E03-1038,1,0.880696,"Missing"
W03-1504,W99-0613,0,0.348162,"for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– independent NERC systems, which may be trained from small training sets by taking advantage of unlabelled examples (Collins and Singer, 1999; Abney, 2002), and which are easy to adapt to changing domains (being all these aspects closely related). This work focuses on exploring the construction of a low–cost Named Entity classification (NEC) module for Catalan without making use of large/expensive resources of the language. In doing so, the paper first explores the training of classification models by using only Catalan resources and then proposes a training scheme, in which a Catalan/Spanish bilingual classifier is trained directly from a training set including examples of the two languages. In both cases, the bootstrapping of the"
W03-1504,W03-0419,0,0.0206414,"Missing"
W03-1504,W02-2024,0,0.0121427,"ch may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifying those units in a text has kept on growing during the last years. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC competition task. More recent approaches can be found in the proceedings of the shared task at the 2002 and 2003 editions of the Conference on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), where several machine–learning (ML) systems were compared at the NERC task for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– in"
W03-1504,P02-1046,0,\N,Missing
W03-2903,P96-1043,0,\N,Missing
W03-2903,J01-2001,0,\N,Missing
W03-2903,C96-2130,0,\N,Missing
W04-0806,taule-etal-2004-minicors,0,0.0331544,"Missing"
W04-0806,J99-4008,0,0.0963522,"Missing"
W04-0861,W04-0828,1,0.681474,"Missing"
W04-0861,W04-0837,1,0.823382,"Missing"
W04-0861,P94-1013,0,0.0414439,"Catalonia. The integration was carried out by the TALP group.   Naive Bayes (NB) is the well–known Bayesian algorithm that classifies an example by choosing the class that maximizes the product, over all features, of the conditional probability of the class given the feature. The provider of this module is IXA. Conditional probabilities were smoothed by Laplace correction.  Decision List (DL) are lists of weighted classification rules involving the evaluation of one single feature. At classification time, the algorithm applies the rule with the highest weight that matches the test example (Yarowsky, 1994). The provider is IXA and they also applied smoothing to generate more robust decision lists.  In the Vector Space Model method (cosVSM), each example is treated as a binary-valued feature vector. For each sense, one centroid vector is obtained from training. Centroids are compared with the vectors representing test examples, using the cosine similarity function, and the closest centroid is used to classify the example. No smoothing is required for this method provided by IXA. 2 The WSD Modules Support Vector Machines (SVM) find the hyperplane (in a high dimensional feature space) that separa"
W04-2412,W04-2413,0,0.038009,"Missing"
W04-2412,W04-2415,1,0.477006,"Missing"
W04-2412,W03-1006,0,0.0146244,"raining classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et"
W04-2412,W03-0423,0,0.0978149,"Missing"
W04-2412,Y01-1001,0,0.032398,"Missing"
W04-2412,W03-1007,0,0.00623602,"actic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with sema"
W04-2412,W03-1008,0,0.0372572,"c semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, the"
W04-2412,J02-3001,0,0.495339,"nner, Cause, etc. Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic"
W04-2412,P02-1031,0,0.0266753,"ting systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level"
W04-2412,N03-2009,0,0.0138529,"ls. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentr"
W04-2412,W04-2416,0,0.64655,"e development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL str"
W04-2412,W04-2417,0,0.020755,"ng components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature developme"
W04-2412,W04-2421,0,0.307179,"teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL strategies and into the development of features (see sections 4.2 and 4.3). As an exception, van den Bosch et al. (2004) applied a 3 Arguments in data do not embed, though format allows so. The San Francis"
W04-2412,P03-1002,0,0.882784,"evant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. 1 CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles . Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) a"
W04-2412,W04-2418,0,0.0168107,"system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different"
W04-2412,W04-2419,0,0.23673,"the CoNLL-2004 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the te"
W04-2412,W00-0726,0,0.168458,"Missing"
W04-2412,J93-2004,0,0.0426925,"n of the predicate of the proposition. Most of the time, the verb corresponds to the target verb of the proposition, which is provided as input, and only in few cases the verb participant spans more words than the target verb. Except for non-trivial cases, this situation makes the verb fairly easy to identify and, since there is one verb with each proposition, evaluating its recognition overestimates the overall performance of a system. For this reason, the verb argument is excluded from evaluation. 3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993), and follows the setting of past editions of the CoNLL shared task: training set (sections 15-18), development set (section 20) and test set (section 21). We first describe annotations related to argument structure. Then, we describe the preprocessing of input data. Finally, we describe the format of the data sets. 3.1 PropBank The Proposition Bank (PropBank) (Palmer et al., 2004) annotates the Penn Treebank with verb argument structure. The semantic roles covered by PropBank are the following: • Numbered arguments (A0–A5, AA): Arguments defining verb-specific roles. Their semantics depends o"
W04-2412,W01-0708,0,0.0512608,"Missing"
W04-2412,W04-2420,0,0.0341188,"Missing"
W04-2412,W04-2422,0,0.316071,"nd labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As"
W04-2412,W03-0419,0,\N,Missing
W04-2412,J05-1004,0,\N,Missing
W04-2412,W04-2414,0,\N,Missing
W04-2415,W04-2412,1,0.635591,"Missing"
W04-2415,W02-1001,0,0.108027,"We describe a system for the CoNLL-2004 Shared Task on Semantic Role Labeling (Carreras and M`arquez, 2004a). The system implements a two-layer learning architecture to recognize arguments in a sentence and predict the role they play in the propositions. The exploration strategy visits possible arguments bottom-up, navigating through the clause hierarchy. The learning components in the architecture are implemented as Perceptrons, and are trained simultaneously online, adapting their behavior to the global target of the system. The learning algorithm follows the global strategy introduced in (Collins, 2002) and adapted in (Carreras and M`arquez, 2004b) for partial parsing tasks. 2 Semantic Role Labeling Strategy The strategy for recognizing propositional arguments in sentences is based on two main observations about argument structure in the data. The first observation is the relation of the arguments of a proposition with the chunk and clause hierarchy: a proposition places its arguments in the clause directly containing the verb (local clause), or in one of the ancestor clauses. Given a clause, we define the sequence of top-most syntactic elements as the words, chunks or clauses which are dire"
W05-0620,W04-2412,1,0.350852,"Missing"
W05-0620,A00-2018,0,0.317238,"fers a substantial loss in the Brown test set. Noticeably, the parser of Collins (1999) seems to be the more robust when moving from WSJ to Brown. 4 A Review of Participant Systems Nineteen systems participated in the CoNLL-2005 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques • Full parser of Charniak (2000). Jointly predicts PoS tags and full parses. • Named Entities predicted with the MaximumEntropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. However, we allowed its use because there is no available named entity recognizer developed with WSJ data. The reported performance on the CoNLL-2003 test is F1 = 88.31, with Prec/Rec. at 88.12/88.51. Tables 2 and 3 summarize the performance of the syntactic processors on the development and test sets. The performance of full parsers on t"
W05-0620,W05-0627,0,0.0745744,"ee, using a script that is available at the shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, whic"
W05-0620,W03-0423,0,0.105348,"from WSJ to Brown. 4 A Review of Participant Systems Nineteen systems participated in the CoNLL-2005 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques • Full parser of Charniak (2000). Jointly predicts PoS tags and full parses. • Named Entities predicted with the MaximumEntropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. However, we allowed its use because there is no available named entity recognizer developed with WSJ data. The reported performance on the CoNLL-2003 test is F1 = 88.31, with Prec/Rec. at 88.12/88.51. Tables 2 and 3 summarize the performance of the syntactic processors on the development and test sets. The performance of full parsers on the WSJ test is lower than that reported in the corresponding papers. The reason is that our evaluation figures have been computed i"
W05-0620,W05-0622,0,0.185711,"strict evaluation basis with respect to punctuation. by Ponzetto and Strube (2005), who used C4.5. Ensembles of decision trees learned through the AdaBoost algorithm (AB) were applied by M`arquez et al. (2005) and Surdeanu and Turmo (2005). Tjong Kim Sang et al. (2005) applied, among others, Memory-Based Learning (MBL). Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel–based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson and Nugues, 2005) and Tree Conditional Random Fields (T-CRF) (Cohn and Blunsom, 2005), that extend the sequential CRF model to tree structures. Finally, Lin and Smith (2005) presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification. From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of increasing the robustness and coverage of the systems, which are quite d"
W05-0620,Y01-1001,0,0.0131567,"Missing"
W05-0620,J02-3001,0,0.591475,"the base SRL models. Actually, Haghighi et al. (2005) performed a double selection step: an inner re-ranking of n-best solutions coming from the base system on a single tree; and an outer selection of the final solution among the candidate solutions coming from n-best parse trees. The reranking approach allows to define global complex features applying to complete candidate solutions to train the rankers. 4.3 Features Looking at the description of the different systems, it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005a; Xue and Palmer, 2004). With no exception, all systems have made intensive use of syntax to extract features. While most systems work only on the output of a parser —Charniak’s being the most preferred— some systems depend on many syntactic parsers. In the latter situation, either a system is a combination of many individual systems (each working with a different parser), or a system extracts features from many different parse trees while exploring the nodes of only one parse tree. Most systems have also considered named entities for extracting fe"
W05-0620,W05-0623,0,0.902367,"that is available at the shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased netw"
W05-0620,W05-0624,0,0.00904432,"test sets. Unlike in full parsing, the figures have been computed on a strict evaluation basis with respect to punctuation. by Ponzetto and Strube (2005), who used C4.5. Ensembles of decision trees learned through the AdaBoost algorithm (AB) were applied by M`arquez et al. (2005) and Surdeanu and Turmo (2005). Tjong Kim Sang et al. (2005) applied, among others, Memory-Based Learning (MBL). Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel–based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson and Nugues, 2005) and Tree Conditional Random Fields (T-CRF) (Cohn and Blunsom, 2005), that extend the sequential CRF model to tree structures. Finally, Lin and Smith (2005) presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification. From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of incr"
W05-0620,W05-0626,0,0.0430337,"d C4.5. Ensembles of decision trees learned through the AdaBoost algorithm (AB) were applied by M`arquez et al. (2005) and Surdeanu and Turmo (2005). Tjong Kim Sang et al. (2005) applied, among others, Memory-Based Learning (MBL). Regarding novel learning paradigms not applied in previous shared tasks, we find Relevant Vector Machine (RVM), which is a kernel–based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson and Nugues, 2005) and Tree Conditional Random Fields (T-CRF) (Cohn and Blunsom, 2005), that extend the sequential CRF model to tree structures. Finally, Lin and Smith (2005) presented a proposal radically different from the rest, with very light learning components. Their approach (Consensus in Pattern Matching, CPM) contains some elements of Memory-based Learning and ensemble classification. From the Machine Learning perspective, system combination is another interesting component observed in many of the proposals. This fact, which is a difference from last year shared task, is explained as an attempt of increasing the robustness and coverage of the systems, which are quite dependent on input parsing errors. The different outputs to combine are obtained by varyi"
W05-0620,W04-0803,0,0.418567,"syntactic information. In (Carreras and M`arquez, 2004) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004. Ten systems contributed to the task, which was evaluated using the PropBank corpus (Palmer et al., 2005). The best results were around 70 in F1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming"
W05-0620,J93-2004,0,0.0477049,"tated sentence, in columns. Input consists of words (1st column), PoS tags (2nd), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th). The 7th column marks target verbs, and their propositions are found in remaining columns. According to the PropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; for intersperse (9th), A0 is the arranger, and A1 the entity interspersed. 2.2 Closed Challenge Setting 3 The organization provided training, development and test sets derived from the standard sections of the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005) corpora. In the closed challenge, systems have to be built strictly with information contained in the training sections of the TreeBank and PropBank. Since this collection contains the gold reference annotations of both syntactic and predicate-argument structures, the closed challenge allows: (1) to make use of any preprocessing system strictly developed within this setting, and (2) to learn from scratch any annotation that is contained in the data. To support the former, the organization provided the output of state-of-theart syntactic preprocessors, descri"
W05-0620,W05-0628,1,0.618126,"Missing"
W05-0620,W05-0629,0,0.0642718,"ists of n-best parsings generated by available tools (“n-cha” by Charniak parser; “nbikel” by Bikel’s implementation of Collins parser). Interestingly, Yi and Palmer (2005) retrained Ratnaparkhi’s parser using the WSJ training sections enriched with semantic information coming from PropBank annotations. These are referred to as AN and AM parses. As it can be seen, Charniak parses were used by most of the systems. Collins parses were used also in some of the best performing systems based on combination. The exceptions to the hierarchical processing are the systems by Pradhan et al. (2005b) and Mitsumori et al. (2005), which perform a chunking-based sequential tokenization. As for the former, the system is the same than the one presented in the 2004 edition. The system by M`arquez et al. (2005) explores hierarchical syntactic structures but selects, in a preprocess, a sequence of tokens to perform a sequential tagging afterwards. punyakanok haghighi marquez pradhan surdeanu tsai che moschitti tjongkimsang yi ozgencil johansson cohn park mitsumori venkatapathy ponzetto lin sutton ML-method SNoW ME AB SVM AB ME,SVM ME SVM ME,SVM,TBL ME SVM RVM T-CRF ME SVM ME DT CPM ME synt n-cha,col n-cha cha,upc cha,col/ch"
W05-0620,W05-0634,0,0.120346,"figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words, PoS tags, base chunks,"
W05-0620,C04-1197,0,0.342398,"around 70 in F1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions"
W05-0620,W05-0625,0,0.784912,"lly, the alternative outputs to combine can be generated by different input syntactic structures or nbest parse candidates, or by applying different learning algorithms to the same input information. The type of combination is reported in the last column. M`arquez et al. (2005) and Tjong Kim Sang et al. (2005) performed a greedy merging of the arguments of base complete solutions (“s-join”). Yi 159 and Palmer (2005) did also a greedy merging of arguments but taking into account not complete solutions but all candidate arguments labeled by base systems (“ac-join”). In a more sophisticated way, Punyakanok et al. (2005) and Tsai et al. (2005) performed global inference as constraint satisfaction using Integer Linear Programming, also taking into account all candidate arguments (“ac-ILP”). It is worth noting that the generalized inference applied in those papers allows to include, jointly with the combination of outputs, a number of linguisticallymotivated constraints to obtain a coherent solution. Pradhan et al. (2005b) followed a stacking approach by learning a chunk-based SRL system including as features the outputs of two syntax-based systems. Finally, Haghighi et al. (2005) and Sutton and McCallum (2005)"
W05-0620,W05-0635,0,0.119021,".92 78.34 75.78 77.04 78.44 74.83 76.59 80.93 71.69 76.03 79.35 71.17 75.04 81.55 69.37 74.97 79.30 71.08 74.97 75.19 73.45 74.31 77.94 70.44 74.00 76.31 71.10 73.61 73.48 72.70 73.09 74.13 71.50 72.79 74.76 69.17 71.86 73.35 69.37 71.31 72.77 66.37 69.43 72.66 64.21 68.17 74.02 63.12 68.13 70.80 63.09 66.72 67.86 63.63 65.68 52.58 29.69 37.95 Table 6: Overall precision, recall and F1 rates obtained by the 19 participating systems in the CoNLL-2005 shared task on the development and test sets. Systems sorted by F 1 score on the WSJ+Brown test set. best individual system on the task is that of Surdeanu and Turmo (2005), which obtained F1 =75.04 on the combined test set, about 3 points below than the best performing combined system. On the development set, that system achieved a performace of 75.17 (slightly below than the 75.27 reported by Che et al. (2005) on the same dataset). According to the description papers, we find that other individual systems, from which the combined systems are constructed, performed also very well. For instance, Tsai et al. (2005) report F1 =75.76 for a base system on the development set, M`arquez et al. (2005) report F1 =75.75, Punyakanok et al. (2005) report F1 =74.76, and Hag"
W05-0620,P03-1002,0,0.906092,"lly, Haghighi et al. (2005) performed a double selection step: an inner re-ranking of n-best solutions coming from the base system on a single tree; and an outer selection of the final solution among the candidate solutions coming from n-best parse trees. The reranking approach allows to define global complex features applying to complete candidate solutions to train the rankers. 4.3 Features Looking at the description of the different systems, it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005a; Xue and Palmer, 2004). With no exception, all systems have made intensive use of syntax to extract features. While most systems work only on the output of a parser —Charniak’s being the most preferred— some systems depend on many syntactic parsers. In the latter situation, either a system is a combination of many individual systems (each working with a different parser), or a system extracts features from many different parse trees while exploring the nodes of only one parse tree. Most systems have also considered named entities for extracting features. The main types"
W05-0620,W05-0636,0,0.441296,"e would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning ("
W05-0620,W05-0630,0,0.0403621,"redicates including argument boundaries. The second stage, reflected in column “label” of Table 4, is the proper labeling of selected candidates. Most of the systems used a two-step procedure consisting of first identifying arguments (e.g., 158 with a binary “null” vs. “non-null” classifier) and then classifying them. This is referred to as “i+c” in the table. Some systems address this phase in a single classification step by adding a “null” category to the multiclass problem (referred to as “c’). The methods performing a sequential tagging use a BIO tagging scheme (“bio”). As a special case, Moschitti et al. (2005) subdivide the “i+c” strategy into four phases: after identification, heuristics are applied to assure compatibility of identified arguments; and, before classifying arguments into roles, a preclassification into core vs. adjunct arguments is performed. Venkatapathy et al. (2005) use three labels instead of two in the identification phase : “null”, “mandatory”, and “optional”. Since arguments in a solution do not embed and most systems identify arguments as nodes in a hierarchical structure, non-embedding constraints must be resolved in order to generate a coherent argument labeling. The “embe"
W05-0620,W05-0637,0,0.0358171,"Missing"
W05-0620,W05-0631,0,0.0211991,"Missing"
W05-0620,W05-0638,0,0.106509,"igures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning (DT) was also repres"
W05-0620,J05-1004,0,0.932473,"ach target verb all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, Cause, etc. Last year, the CoNLL-2004 shared task aimed at evaluating machine learning SRL systems based only on partial syntactic information. In (Carreras and M`arquez, 2004) one may find a detailed review of the task and also a brief state-of-the-art on SRL previous to 2004. Ten systems contributed to the task, which was evaluated using the PropBank corpus (Palmer et al., 2005). The best results were around 70 in F1 measure. Though not directly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the f"
W05-0620,W05-0632,0,0.0658557,"he shared task webpage. Otherwise, the performance would have Prec./Recall figures below 37. 156 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separa"
W05-0620,W05-0633,0,0.0218973,"Missing"
W05-0620,W05-0621,0,0.0322955,"earning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning (DT) was also represented UPC Chunker UPC Clauser Collins (1999) Char"
W05-0620,W04-3212,0,0.801657,"tly comparable, these figures are substantially lower than the best results published up to date using full parsing as input information (F1 slightly over 79). In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Eight systems relying on full parsing information were evaluated in that event using the FrameNet corpus (Fillmore et al., 2001). From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al., 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al., 2005a). Following last year’s initiative, the CoNLL-2005 shared task1 will concern again the recognition of semantic roles for the English language. Compared to the shared task of CoNLL-2004, the novelties introduced in the 2005 edition are: • Aiming at evaluating the contribution of full parsing in SRL, the complete syntactic trees given by two alternative parsers have been provided as input information for the task. The rest of input information does not vary and corresponds to the levels of processing treated in the previous editions of the CoNLL shared task, i.e., words,"
W05-0620,W05-0639,0,0.251594,"6 Up to 8 different learning algorithms have been applied to train the learning components of participant systems. See the “ML-method” column of table 4 for a summary of the following information. Log–linear models and vector-based linear classifiers dominated over the rest. Probably, this is due to the versatility of the approaches and the availability of very good software toolkits. In particular, 8 teams used the Maximum Entropy (ME) statistical framework (Che et al., 2005; Haghighi et al., 2005; Park and Rim, 2005; Tjong Kim Sang et al., 2005; Sutton and McCallum, 2005; Tsai et al., 2005; Yi and Palmer, 2005; Venkatapathy et al., 2005). Support Vector Machines (SVM) were used by 6 teams. Four of them with the standard polynomial kernels (Mitsumori et al., 2005; Tjong Kim Sang et al., 2005; Tsai et al., 2005; Pradhan et al., 2005b), another one using Gaussian kernels (Ozgencil and McCracken, 2005), and a last group using tree-based kernels specifically designed for the task (Moschitti et al., 2005). Another team used also a related learning approach, SNoW, which is a Winnowbased network of linear separators (Punyakanok et al., 2005). Decision Tree learning (DT) was also represented UPC Chunker UPC"
W05-0620,J03-4003,0,\N,Missing
W05-0620,P04-1043,0,\N,Missing
W05-0620,W03-0419,0,\N,Missing
W05-0628,W04-2415,1,0.347059,"Missing"
W05-0628,J02-3001,0,0.64197,"ase chunk boundaries, A0-A5 arguments not present in PropBank frames for a certain verb are not allowed, etc. We also tried beam search on top of the classifiers’ predictions to find the sequence of labels with highest sentence-level probability (as a summation of individual predictions). But the results did not improve the basic greedy tagging. Regarding feature representation, we used all input information sources, with the exception of verb senses and Collins’ parser. We did not contribute with significantly original features. Instead, we borrowed most of them from the existing literature (Gildea and Jurafsky, 2002; Carreras et al., 2004; Xue and Palmer, 2004). Broadly speaking, we considered features belonging to four categories3 : (1) On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase in which verb is included: single-word or multi-word; Verb voice: active, passive, copulative, infinitive, or progressive; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. (2) On the focus constituent: • Type; Head: extracted using common head-word rules; if the first element is a PP chunk, th"
W05-0628,W04-3212,0,0.468055,"in PropBank frames for a certain verb are not allowed, etc. We also tried beam search on top of the classifiers’ predictions to find the sequence of labels with highest sentence-level probability (as a summation of individual predictions). But the results did not improve the basic greedy tagging. Regarding feature representation, we used all input information sources, with the exception of verb senses and Collins’ parser. We did not contribute with significantly original features. Instead, we borrowed most of them from the existing literature (Gildea and Jurafsky, 2002; Carreras et al., 2004; Xue and Palmer, 2004). Broadly speaking, we considered features belonging to four categories3 : (1) On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase in which verb is included: single-word or multi-word; Verb voice: active, passive, copulative, infinitive, or progressive; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. (2) On the focus constituent: • Type; Head: extracted using common head-word rules; if the first element is a PP chunk, then the head of the first NP is extracted; • Fi"
W05-0826,J93-2003,0,0.00539859,"y of them in isolation. Moreover, we have built a wordbased translation model based on WordNet which is used for unknown words. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000), (Yamada and Knight, 2001), (Koehn and Knight, 2002), (Koehn et al., 2003), (Schafer and Yarowsky, 2003) and (Gildea, 2003). Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view, which is defined as any possible representation of the information contained in a bitext. We enrich data view tokens with features further than lexical such as PoS, lemma, and chu"
W05-0826,carreras-etal-2004-freeling,0,0.075818,"Missing"
W05-0826,P03-1011,0,0.0339277,"Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on WordNet which is used for unknown words. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000), (Yamada and Knight, 2001), (Koehn and Knight, 2002), (Koehn et al., 2003), (Schafer and Yarowsky, 2003) and (Gildea, 2003). Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view, which is defined as any possible representation"
W05-0826,gimenez-marquez-2004-svmtool,1,0.552809,"Missing"
W05-0826,N03-1017,0,0.036456,"tion, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on WordNet which is used for unknown words. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000), (Yamada and Knight, 2001), (Koehn and Knight, 2002), (Koehn et al., 2003), (Schafer and Yarowsky, 2003) and (Gildea, 2003). Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view"
W05-0826,P00-1056,0,0.0455492,"the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on WordNet which is used for unknown words. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000), (Yamada and Knight, 2001), (Koehn and Knight, 2002), (Koehn et al., 2003), (Schafer and Yarowsky, 2003) and (Gildea, 2003). Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degr"
W05-0826,W03-1002,0,0.14031,"degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on WordNet which is used for unknown words. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000), (Yamada and Knight, 2001), (Koehn and Knight, 2002), (Koehn et al., 2003), (Schafer and Yarowsky, 2003) and (Gildea, 2003). Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view, which is defined as any poss"
W05-0826,P01-1067,0,0.0689639,"orking with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on WordNet which is used for unknown words. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000), (Yamada and Knight, 2001), (Koehn and Knight, 2002), (Koehn et al., 2003), (Schafer and Yarowsky, 2003) and (Gildea, 2003). Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotatio"
W06-2925,W06-2920,0,0.0741801,"Eisner (2000). We experiment with a large feature set that models: the tokens involved in dependencies and their immediate context, the surfacetext distance between tokens, and the syntactic context dominated by each dependency. In experiments, the treatment of multilingual information was totally blind. 1 2 Parsing and Learning Algorithms This section describes the three main components of the dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information"
W06-2925,W02-1001,0,0.184514,", the scoring function is used to select the dependency label that maximizes the score. We take advantage of this two-step processing to introduce features for the scoring function that represent some of the internal dependencies of the span (see Section 3 for details). It has to be noted that the parsing algorithm we use does not score dependencies on top of every possible internal structure. Thus, by conditioning on features extracted from y we are making the search approximative. 2.3 Perceptron Learning As learning algorithm, we use Perceptron tailored for structured scenarios, proposed by Collins (2002). In recent years, Perceptron has been used in a number of Natural Language Learning works, such as in 182 partial parsing (Carreras et al., 2005) or even dependency parsing (McDonald et al., 2005). Perceptron is an online learning algorithm that learns by correcting mistakes made by the parser when visiting training sentences. The algorithm is extremely simple, and its cost in time and memory is independent from the size of the training corpora. In terms of efficiency, though, the parsing algorithm must be run at every training sentence. Our system uses the regular Perceptron working in prima"
W06-2925,C96-1058,0,0.052651,"ext distance between tokens, and the syntactic context dominated by each dependency. In experiments, the treatment of multilingual information was totally blind. 1 2 Parsing and Learning Algorithms This section describes the three main components of the dependency parsing: the parsing model, the parsing algorithm, and the learning algorithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. Regarding experimentation, the treatment of multilingual data has been totally blind, with no spec"
W06-2925,P05-1012,0,0.831221,"rithm. 2.1 Introduction We describe a learning system for the CoNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its parameters based on the parsing mistakes. The features used to score, while based on the previous work in dependency parsing (McDonald et al., 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. Regarding experimentation, the treatment of multilingual data has been totally blind, with no special processing or features that depend on the language. Considering its simplicity, our system Model Let 1, . . . , L be the dependency labels, defined beforehand. Let x be a sentence of n words, x1 . . . xn . Finally, let Y(x) be the space of well-formed dependency trees for x. A dependency tree y ∈ Y(x) is a set of n dependencies of the form ["
W06-2925,W03-2403,0,\N,Missing
W06-2925,J03-4003,0,\N,Missing
W06-2925,dzeroski-etal-2006-towards,0,\N,Missing
W06-2925,W03-2405,0,\N,Missing
W06-2925,afonso-etal-2002-floresta,0,\N,Missing
W06-3126,2003.mtsummit-papers.32,0,0.0270451,"order to remove the additional linguistic annotation. We did not observe significant differences in either alternative. Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W). 3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al., 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1 -measure (e = 1, 2) (Melamed et al., 2003), 1-WER (Nießen et al., 2000), 1-PER (Leusch et al., 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Optimization of the decoding parameters (λtm , λlm , λw ) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric. 3 For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup. However, for English-to-Spanish we were forced to use the exact module alone. System Baseline LDV- COMBO System Baseline LDV- COMBO 1-PER 0.5514 0.5478 1-PER 0.5158 0.5382 1-WER 0.3741 0.3657 Spanish-to-Englis"
W06-3126,gimenez-marquez-2004-svmtool,1,0.810326,"Missing"
W06-3126,koen-2004-pharaoh,0,0.0421964,"ment of the linguistic tools utilized. Our system is further described in Section 2. Results are reported in Section 3. Conclusions and further work are briefly outlined in Section 4. 166 Proceedings of the Workshop on Statistical Machine Translation, pages 166–169, c New York City, June 2006. 2006 Association for Computational Linguistics Figure 1: A case of word alignment possibilities on top of lexical units (a) and linguistic data views (b). 2 System Description The LDV- COMBO system follows the SMT architecture suggested by the workshop organizers. We use the Pharaoh beam-search decoder (Koehn, 2004). First, training data are linguistically annotated. In order to achieve robustness the same tools have been used to linguistically annotate both languages. The SVMTool1 has been used for PoS-tagging (Gim´enez and M`arquez, 2004). The Freeling2 package (Carreras et al., 2004) has been used for lemmatizing. Finally, the Phreco software (Carreras et al., 2005) has been used for shallow parsing. In this paper we focus on data views at the word level. 6 different data views have been built: (W) word, (L) lemma, (WP) word and PoS, (WC) word and chunk IOB label, (WPC) word, PoS and chunk IOB label,"
W06-3126,P04-1077,0,0.0159517,"c annotation. We did not observe significant differences in either alternative. Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W). 3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al., 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1 -measure (e = 1, 2) (Melamed et al., 2003), 1-WER (Nießen et al., 2000), 1-PER (Leusch et al., 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Optimization of the decoding parameters (λtm , λlm , λw ) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric. 3 For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup. However, for English-to-Spanish we were forced to use the exact module alone. System Baseline LDV- COMBO System Baseline LDV- COMBO 1-PER 0.5514 0.5478 1-PER 0.5158 0.5382 1-WER 0.3741 0.3657 Spanish-to-English BLEU-4 GTM-1 GTM-2 0.2709 0.6159 0.2"
W06-3126,W05-0909,0,0.0238466,"significant differences in either alternative. Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W). 3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al., 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1 -measure (e = 1, 2) (Melamed et al., 2003), 1-WER (Nießen et al., 2000), 1-PER (Leusch et al., 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Optimization of the decoding parameters (λtm , λlm , λw ) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric. 3 For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup. However, for English-to-Spanish we were forced to use the exact module alone. System Baseline LDV- COMBO System Baseline LDV- COMBO 1-PER 0.5514 0.5478 1-PER 0.5158 0.5382 1-WER 0.3741 0.3657 Spanish-to-English BLEU-4 GTM-1 GTM-2 0.2709 0.6159 0.2579 0.2708 0.6202 0.2585 METEOR 0.5836"
W06-3126,J93-2003,0,0.0109185,"Missing"
W06-3126,carreras-etal-2004-freeling,0,0.0678753,"Missing"
W06-3126,2003.mtsummit-papers.6,0,0.0691466,"Missing"
W06-3126,P05-1066,0,0.130416,"Missing"
W06-3126,P03-1011,0,0.0210984,"grees of linguistic analysis from the lexical to the shallow syntactic level. Translation models are built on top of combinations of these alignments. We present results for the Spanish-to-English and English-toSpanish tasks. We show that liniguistic information may be helpful, specially when the target language has a rich morphology. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. In the last years, many efforts have been devoted to this matter (Yamada and Knight, 2001; Gildea, 2003). Following our previous work (Gim´enez and M`arquez, 2005), we use shallow syntactic information to generate more precise alignments. Far from full syntactic complexity, we suggest going back to the simpler alignment methods first described by IBM (1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). Apart from redefining the scope of the alignment unit, we may use different linguistic data views. We enrich tokens with features further For instance, suppose the case illustrated in Figure 1"
W06-3126,gimenez-amigo-2006-iqmt,1,0.810199,"Missing"
W06-3126,N03-2021,0,0.0262909,"annotation or translation models must be post-processed in order to remove the additional linguistic annotation. We did not observe significant differences in either alternative. Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W). 3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al., 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1 -measure (e = 1, 2) (Melamed et al., 2003), 1-WER (Nießen et al., 2000), 1-PER (Leusch et al., 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Optimization of the decoding parameters (λtm , λlm , λw ) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric. 3 For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup. However, for English-to-Spanish we were forced to use the exact module alone. System Baseline LDV- COMBO System Baseline LDV- COMBO 1-PER 0.5514 0.5478"
W06-3126,niessen-etal-2000-evaluation,0,0.0257404,"ls must be post-processed in order to remove the additional linguistic annotation. We did not observe significant differences in either alternative. Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W). 3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al., 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1 -measure (e = 1, 2) (Melamed et al., 2003), 1-WER (Nießen et al., 2000), 1-PER (Leusch et al., 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Optimization of the decoding parameters (λtm , λlm , λw ) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric. 3 For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup. However, for English-to-Spanish we were forced to use the exact module alone. System Baseline LDV- COMBO System Baseline LDV- COMBO 1-PER 0.5514 0.5478 1-PER 0.5158 0.5382 1-WER 0."
W06-3126,J03-1002,0,0.00482855,"n order to achieve robustness the same tools have been used to linguistically annotate both languages. The SVMTool1 has been used for PoS-tagging (Gim´enez and M`arquez, 2004). The Freeling2 package (Carreras et al., 2004) has been used for lemmatizing. Finally, the Phreco software (Carreras et al., 2005) has been used for shallow parsing. In this paper we focus on data views at the word level. 6 different data views have been built: (W) word, (L) lemma, (WP) word and PoS, (WC) word and chunk IOB label, (WPC) word, PoS and chunk IOB label, (LC) lemma and chunk IOB label. Then, running GIZA++ (Och and Ney, 2003), we obtain token alignments for each of the data views. Combined phrase-based translation models are built on top of the Viterbi alignments output by GIZA++. Phrase extraction is performed following the phraseextract algorithm depicted by Och (2002). We do not apply any heuristic refinement. We work with phrases up to 5 tokens. Phrase pairs appearing only once have been discarded. Scoring is performed by relative frequency. No smoothing is applied. In this paper we focus on the global phrase extraction (GPHEX) method described by Gim´enez 1 The SVMTool may be freely downloaded at http://www.l"
W06-3126,2001.mtsummit-papers.68,0,0.014074,"be the translation model. Therefore either the input must be enriched with linguistic annotation or translation models must be post-processed in order to remove the additional linguistic annotation. We did not observe significant differences in either alternative. Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W). 3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al., 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1 -measure (e = 1, 2) (Melamed et al., 2003), 1-WER (Nießen et al., 2000), 1-PER (Leusch et al., 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Optimization of the decoding parameters (λtm , λlm , λw ) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric. 3 For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup. However, for English-to-Spanish we were forced to use the exact m"
W06-3126,W03-1002,0,0.019042,"would allow us to distinguish between the two cases. Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses. Therefore, it would be possible to distinguish for instance between two realizations of ‘plays’ with different meanings: ‘heP RP playsV BG guitarN N ’ and ‘heP RP playsV BG footballN N ’. Of course, there is a natural trade-off between the use of linguistic data views and data sparsity. Fortunately, we hava data enough so that statistical parameter estimation remains reliable. The approach which is closest to ours is that by Schafer and Yarowsky (2003) who suggested a combination of models based on shallow syntactic analysis (part-of-speech tagging and phrase chunking). They followed a backoff strategy in the application of their models. Decoding was based on Finite State Automata. Although no significant improvement in MT quality was reported, results were promising taking into account the short time spent in the development of the linguistic tools utilized. Our system is further described in Section 2. Results are reported in Section 3. Conclusions and further work are briefly outlined in Section 4. 166 Proceedings of the Workshop on Stat"
W06-3126,P01-1067,0,0.0543441,"action using different degrees of linguistic analysis from the lexical to the shallow syntactic level. Translation models are built on top of combinations of these alignments. We present results for the Spanish-to-English and English-toSpanish tasks. We show that liniguistic information may be helpful, specially when the target language has a rich morphology. 1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. In the last years, many efforts have been devoted to this matter (Yamada and Knight, 2001; Gildea, 2003). Following our previous work (Gim´enez and M`arquez, 2005), we use shallow syntactic information to generate more precise alignments. Far from full syntactic complexity, we suggest going back to the simpler alignment methods first described by IBM (1993). Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). Apart from redefining the scope of the alignment unit, we may use different linguistic data views. We enrich tokens with features further For instance, suppose the case illustra"
W06-3126,W05-0826,1,\N,Missing
W07-0719,W05-0909,0,0.0520496,"Missing"
W07-0719,I05-2021,0,0.0414795,"stem is better than (>), equal to (=), or worse than (<) the ‘DPT’ system, with respect to adequacy, fluency, and overall MT quality, are presented. ‘cuesti´on’ as ‘matter’, although acceptable, is breaking the phrase ‘cuesti´on de orden’ of high cohesion, which is commonly translated as ‘point of order’. The cause underlying these problems is that DPT predictions are available only for a subset of phrases. Thus, during decoding, for these cases our DPT models may be in disadvantage. 5 Related Work Recently, there is a growing interest in the application of WSD technology to MT. For instance, Carpuat and Wu (2005b) suggested integrating WSD predictions into a SMT system in a ‘hard’ manner, either for decoding, by constraining the set of acceptable translation candidates for each given source word, or for post-processing the SMT system output, by directly replacing the translation of each selected word with the WSD system prediction. They did not manage to improve MT quality. They encountered several problems inherent to the SMT architecture. In particular, they described what they called the “language model effect” in SMT: “The lexical choices are made in a way that heavily prefers phrasal cohesion in"
W07-0719,P05-1048,0,0.242623,"stem is better than (>), equal to (=), or worse than (<) the ‘DPT’ system, with respect to adequacy, fluency, and overall MT quality, are presented. ‘cuesti´on’ as ‘matter’, although acceptable, is breaking the phrase ‘cuesti´on de orden’ of high cohesion, which is commonly translated as ‘point of order’. The cause underlying these problems is that DPT predictions are available only for a subset of phrases. Thus, during decoding, for these cases our DPT models may be in disadvantage. 5 Related Work Recently, there is a growing interest in the application of WSD technology to MT. For instance, Carpuat and Wu (2005b) suggested integrating WSD predictions into a SMT system in a ‘hard’ manner, either for decoding, by constraining the set of acceptable translation candidates for each given source word, or for post-processing the SMT system output, by directly replacing the translation of each selected word with the WSD system prediction. They did not manage to improve MT quality. They encountered several problems inherent to the SMT architecture. In particular, they described what they called the “language model effect” in SMT: “The lexical choices are made in a way that heavily prefers phrasal cohesion in"
W07-0719,carreras-etal-2004-freeling,0,0.0136921,"Missing"
W07-0719,gimenez-amigo-2006-iqmt,1,0.854024,"Missing"
W07-0719,gimenez-marquez-2004-svmtool,1,0.741691,"Missing"
W07-0719,W05-0826,1,0.885054,"Missing"
W07-0719,N03-1017,0,0.0259866,"rom word translation to phrase translation. Second, we move from the ‘blank-filling’ task to the ‘full translation’ task. We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation. 1 Introduction Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al., 2003). In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context. Following the approach by Vickrey et al. (2005), we deal with the ‘phrase translation’ problem as a classification problem. We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence. We extend the work by Vickrey et al. (2005) in two main aspects. First, we move from ‘word translation’ to ‘phrase translation’. Second, we move from the ‘blank"
W07-0719,koen-2004-pharaoh,0,0.0650806,"phrase translation table is built on top of the union of alignments corresponding to different linguistic data views). We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase is more than three times longer than the target/source phrase are ignored. Phrase pairs are scored on the basis of unsmoothed relative frequency (i.e., MLE). Regarding the argmax search, we used the Pharaoh beam search decoder (Koehn, 2004), which naturally fits with the previous tools. 2.2 DPT for SMT Instead of relying on MLE estimation to score the phrase pairs (fi , ej ) in the translation table, we suggest considering the translation of every source phrase fi as a multi-class classification problem, where every possible translation of fi is a class. We use local linear SVMs 2 . Since SVMs are binary classifiers, the problem must be binarized. We 1 http://www.fjoch.com/GIZA++.html We use the SVMlight package, which is freely available at http://svmlight.joachims.org (Joachims, 1999). 2 160 have applied a simple one-vs-all bi"
W07-0719,P06-1096,0,0.097782,"Missing"
W07-0719,P04-1077,0,0.0241816,"Missing"
W07-0719,J03-1002,0,0.00218625,"se the SRI Language Modeling Toolkit (Stolcke, 2002) for language modeling. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e., lemma, part-of-speech, and base phrase chunks) as described by Gim´enez and M`arquez (2005). Text is automatically annotated, using the SVMTool (Gim´enez and M`arquez, 2004), Freeling (Carreras et al., 2004), and Phreco (Carreras et al., 2005) packages. We used the GIZA++ SMT Toolkit1 (Och and Ney, 2003) to generate word alignments. We apply the phrase-extract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++ following the ‘global phrase extraction’ strategy described by Gim´enez and M`arquez (2005) (i.e., a single phrase translation table is built on top of the union of alignments corresponding to different linguistic data views). We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the s"
W07-0719,2001.mtsummit-papers.68,0,0.031968,"he impact of using DPT models for the isolated ‘phrase translation’ task. In spite of working on a very specific domain, a large room for improvement, coherent with WSD performance, and results by Vickrey et al. (2005), is predicted. Then, in Section 4, we tackle the full translation task. DPT models are integrated in a ‘soft’ manner, by making them available to the decoder so they can fully interact with other models. Results using a reduced set of highly frequent source phrases show a significant improvement, according to several automatic evaluation metrics. Interestingly, the BLEU metric (Papineni et al., 2001) is not able to reflect this improvement. Through a rigorous process of manual evaluation we have verified the gain. We have also observed that it is mainly related to adequacy. These results confirm that better phrase translation probabilities may be helpful for the full translation task. However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study. 2 Discriminative Phrase Translation In this section we describe the phrase-based SMT baseline system and how DPT models are built and integrated i"
W07-0719,P06-1091,0,0.0449571,"Missing"
W07-0719,H05-1097,0,0.627196,"adequacy, according to a rigorous process of manual evaluation. 1 Introduction Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al., 2003). In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context. Following the approach by Vickrey et al. (2005), we deal with the ‘phrase translation’ problem as a classification problem. We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence. We extend the work by Vickrey et al. (2005) in two main aspects. First, we move from ‘word translation’ to ‘phrase translation’. Second, we move from the ‘blank-filling’ task to the ‘full translation’ task. Our approach is fully described in Section 2. We apply it to the Spanish-to-English translation of European Parliament Proceedings. In Section 3, prior to considering the ‘full translation’ task, we ana"
W07-0719,vilar-etal-2006-error,0,0.0195577,"Missing"
W07-0719,S01-1040,0,0.0295431,"extracted from the same training data as in the case of MLE models, i.e., an aligned parallel corpus, obtained as described in Section 2.1. We use each sentence pair in which the source phrase fi occurs to generate a positive example for the classifier corresponding to the actual translation of fi in that sentence, according to the automatic alignment. This will be as well a negative example for the classifiers corresponding to the rest of possible translations of fi . 2.2.1 Feature Set We consider different kinds of information, always from the source sentence, based on standard WSD methods (Yarowsky et al., 2001). As to the local context, inside the source phrase to disambiguate, and 5 tokens to the left and to the right, we use n-grams (n ∈ {1, 2, 3}) of: words, partsof-speech, lemmas and base phrase chunking IOB labels. As to the global context, we collect topical information by considering the source sentence as a bag of lemmas. 2.2.2 Decoding. A Trick. At translation time, we consider every instance of fi as a separate case. In each case, for all possible translations of fi , we collect the SVM score, according to the SVM classification rule. We are in fact modeling P (ej |fi ). However, these sco"
W07-0719,P05-1035,0,\N,Missing
W07-0738,P04-1077,0,0.0635476,"fferent granularity levels, and from different viewpoints. For instance, we might compare the semantic structure of two sentences (i.e., which actions, semantic arguments and adjuncts exist) or we might compare lexical units according to the semantic role they play inside the sentence. For that purpose, we use two very simple kinds of similarity measures over LEs: ‘Overlapping’ and ‘Matching’. We provide a general definition: Overlapping between items inside LEs, according to their type. Formally: X ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). Overlapping(t) = Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partially seen as a ‘bag’ of LEs. Possible kinds of LEs are: word forms, parts-of-s"
W07-0738,C04-1072,0,0.200272,"fferent granularity levels, and from different viewpoints. For instance, we might compare the semantic structure of two sentences (i.e., which actions, semantic arguments and adjuncts exist) or we might compare lexical units according to the semantic role they play inside the sentence. For that purpose, we use two very simple kinds of similarity measures over LEs: ‘Overlapping’ and ‘Matching’. We provide a general definition: Overlapping between items inside LEs, according to their type. Formally: X ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length). Stemming is enabled (Lin and Och, 2004a). Overlapping(t) = Let us note that ROUGE and METEOR may consider stemming (i.e., morphological variations). Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998). Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures. We have defined what we call ‘linguistic elements’ (LEs). 2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partially seen as a ‘bag’ of LEs. Possible kinds of LEs are: word forms, parts-of-s"
W07-0738,P06-2003,1,0.894349,"Missing"
W07-0738,W05-0909,0,0.0413037,"e level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). LE may consist, in its turn, of one or more LEs, which we call ‘items’ inside the LE. For instance, a ‘phrase’ LE may consist of ‘phrase’ items, ‘part-ofspeech’ (PoS) items, ‘word form’ items, etc. Items may be also combinations of LEs. For instance, a ‘phrase’ LE may be seen as a sequence of ‘wordform:PoS’ items. 2.2.2 Similarity Measures We are interested in comparing linguistic structures, and linguistic units. LEs allow for comparisons at different granularity levels, and from different viewpoints. For instance, we might compare the semantic structure of two sentences (i.e., which action"
W07-0738,E06-1032,0,0.78632,"034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001). In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.) BLEU may not be a reliable MT quality indicator. The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one. Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon. The reason is that, while MT quality aspects are diverse, BLEU limits its scope to the lexical dimension. In this work, we suggest using metrics which take into account linguistic features at more abstract levels. We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) ar"
W07-0738,P05-1022,0,0.00658471,"ts lexical overlapping between terminal nodes of type ‘A’ (Adjective/Adverbs). ‘DP-Ol -4’ reflects lexical overlapping between nodes hanging at level 4 or deeper. Additionally, we consider three coarser metrics (‘DP-Ol *’, ‘DP-Oc -*’ and ‘DP-Or -*’) which correspond to the uniformly averaged values over all levels, categories, and relationships, respectively. 2.4.2 On Constituency Parsing (CP) ‘CP’ metrics capture similarities between constituency parse trees associated to automatic and reference translations. Constituency trees are provided by the Charniak-Johnson’s Max-Ent reranking parser (Charniak and Johnson, 2005). CP-STM(i)-l This metric corresponds to the STM metric presented by Liu and Gildea (2005). All syntactic subpaths in the candidate and the reference trees are retrieved. The fraction of matching subpaths of a given length, ‘l’, is computed. For instance, ‘CP-STMi-5’ retrieves the proportion of length-5 matching subpaths. Average accumulated scores may be computed as well. For instance, ‘CP-STM-9’ retrieves average accumulated proportion of matching subpaths up to length-9. 259 2.5 Shallow-Semantic Similarity We have designed two new families of metrics, ‘NE’ and ‘SR’, which are intended to ca"
W07-0738,gimenez-amigo-2006-iqmt,1,0.817374,"Missing"
W07-0738,gimenez-marquez-2004-svmtool,1,0.183461,"Missing"
W07-0738,W06-3114,0,0.290763,"lsi.upc.edu Abstract disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al., 2001). In particular, they noted that when the systems under evaluation are of a different nature (e.g., rule-based vs. statistical, human-aided vs. fully automatical, etc.) BLEU may not be a reliable MT quality indicator. The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g., statistical systems), and penalizes those which use a different one. Evaluation results recently reported by Callison-Burch et al. (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon. The reason is that, while MT quality aspects are diverse, BLEU limits its scope to the lexical dimension. In this work, we suggest using metrics which take into account linguistic features at more abstract levels. We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more rel"
W07-0738,W05-0904,0,0.616692,"tric, ‘SP-Oc -*’ which considers the average overlapping over all chunk types. At a more abstract level, we use the NIST metric (Doddington, 2002) to compute accumulated/individual scores over sequences of: Lemmas – SP-NIST(i)l -n Parts-of-speech – SP-NIST(i)p -n Base phrase chunks – SP-NIST(i)c -n For instance, ‘SP-NISTl -5’ corresponds to the accumulated NIST score for lemma n-grams up to length 5, whereas ‘SP-NISTip-5’ corresponds to the individual NIST score for PoS 5-grams. 2.4 Syntactic Similarity We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea (2005) and Amig´o et al. (2006) based on dependency and constituency parsing. 2.4.1 On Dependency Parsing (DP) ‘DP’ metrics capture similarities between dependency trees associated to automatic and reference translations. Dependency trees are provided by the MINIPAR dependency parser (Lin, 1998). Similarities are captured from different viewpoints: DP-HWC(i)-l This metric corresponds to the HWC metric presented by Liu and Gildea (2005). All head-word chains are retrieved. The fraction of matching head-word chains of a given length, ‘l’, is computed. We have slightly modified this metric in order to"
W07-0738,J93-2004,0,0.030725,"ass of linguistic elements and items to be used. Below, we instantiate these measures over several particular cases. 2.3 Shallow Syntactic Similarity Metrics based on shallow parsing (‘SP’) analyze similarities at the level of PoS-tagging, lemmatization, and base phrase chunking. Outputs and references are automatically annotated using stateof-the-art tools. PoS-tagging and lemmatization are provided by the SVMTool package (Gim´enez and M`arquez, 2004), and base phrase chunking is provided by the Phreco software (Carreras et al., 2005). Tag sets for English are derived from the Penn Treebank (Marcus et al., 1993). We instantiate ‘Overlapping’ over parts-of-speech and chunk types. The goal is to capture the proportion of lexical items correctly translated, according to their shallow syntactic realization: SP-Op-t Lexical overlapping according to the partof-speech ‘t’. For instance, ‘SP-Op -NN’ roughly reflects the proportion of correctly translated singular nouns. We also introduce a coarser metric, ‘SP-Op -*’ which computes average overlapping over all parts-of-speech. SP-Oc-t Lexical overlapping according to the chunk type ‘t’. For instance, ‘SP-Oc -NP’ roughly 258 reflects the successfully translate"
W07-0738,N03-2021,0,0.235231,"have resorted to several existing metrics, and we have also developed new ones. Below, we group them according to the level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). LE may consist, in its turn, of one or more LEs, which we call ‘items’ inside the LE. For instance, a ‘phrase’ LE may consist of ‘phrase’ items, ‘part-ofspeech’ (PoS) items, ‘word form’ items, etc. Items may be also combinations of LEs. For instance, a ‘phrase’ LE may be seen as a sequence of ‘wordform:PoS’ items. 2.2.2 Similarity Measures We are interested in comparing linguistic structures, and linguistic units. LEs allow for comparisons at different granularity levels,"
W07-0738,H05-1081,1,0.853513,"Missing"
W07-0738,niessen-etal-2000-evaluation,0,0.684882,"Missing"
W07-0738,2001.mtsummit-papers.68,0,0.0346849,"omputational Linguistics 2 A Heterogeneous Metric Set For our experiments, we have compiled a representative set of metrics1 at different linguistic levels. We have resorted to several existing metrics, and we have also developed new ones. Below, we group them according to the level at which they operate. 2.1 Lexical Similarity Most of the current metrics operate at the lexical level. We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al., 2001). NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002). GTM We set to 1 the value of the e parameter (Melamed et al., 2003). METEOR We run all modules: ‘exact’, ‘porterstem’, ‘wn stem’ and ‘wn synonymy’, in that order (Banerjee and Lavie, 2005). LE may consist, in its turn, of one or more LEs, which we call ‘items’ inside the LE. For instance, a ‘phrase’ LE may consist of ‘phrase’ items, ‘part-ofspeech’ (PoS) items, ‘word form’ items, etc. Items may be also combinations of LEs. For instance, a ‘phrase’ LE may be seen as a sequence of ‘wordform:PoS’ items. 2.2"
W07-0738,2001.mtsummit-eval.8,0,0.102644,"ical overlapping over all NE types. This metric includes the NE type ‘O’ (i.e., Not-a-NE). We introduce another variant, ‘NE-Oe -**’, which considers only actual NEs. NE-Me-t Lexical matching between NEs according to their type t. For instance, ‘NE-Me -LOC’ reflects the proportion of fully translated NEs of type ‘LOC’ (i.e., location). The ‘NE-Me -*’ metric considers the average lexical matching over all NE types, this time excluding type ‘O’. Other authors have measured MT quality over NEs in the recent literature. In particular, the ‘NEMe -*’ metric is similar to the ‘NEE’ metric defined by Reeder et al. (2001). 2.5.2 On Semantic Roles (SR) ‘SR’ metrics analyze similarities between automatic and reference translations by comparing the SRs (i.e., arguments and adjuncts) which occur in them. Sentences are automatically annotated using the SwiRL package (M`arquez et al., 2005). This package requires at the input shallow parsed text enriched with NEs, which is obtained as described in Section 2.5.1. See the list of SR types in Table 2. Type A0 A1 A2 A3 A4 A5 AA AM-ADV AM-CAU AM-DIR AM-DIS AM-EXT AM-LOC AM-MNR AM-MOD AM-NEG AM-PNC AM-PRD AM-REC AM-TMP Description arguments associated with a verb predicat"
W07-0738,P05-1035,0,\N,Missing
W08-0332,W07-0718,0,0.192175,"ore is not adjusted. Optimal metric sets are determined by maximizing the correlation with human assessments, either at the document or sentence level. However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search. First, metrics are ranked according to their individual quality. Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases. 3 Experimental Work We use all into-English test beds from the 2006 and 2007 editions of the SMT workshop (Koehn and Monz, 2006; Callison-Burch et al., 2007). These include the translation of three different language-pairs: German-to-English (de-en), Spanish-to-English (es-en), and French-to-English (fr-en), over two different scenarios: in-domain (European Parliament Proceedings) and out-of-domain (News Commentary Corpus)1 . In all cases, a single reference translation is available. In addition, human assessments on adequacy and fluency are available for a subset of systems and sentences. Each sentence has been evaluated at least by two different judges. A brief numerical description of these test beds is available in Table 1. 1 We have not used"
W08-0332,P04-1014,0,0.0111666,"Missing"
W08-0332,W07-0738,1,0.49558,"Missing"
W08-0332,I08-1042,1,0.844564,"Missing"
W08-0332,W06-3114,0,0.0637375,"tric to the overall score is not adjusted. Optimal metric sets are determined by maximizing the correlation with human assessments, either at the document or sentence level. However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search. First, metrics are ranked according to their individual quality. Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases. 3 Experimental Work We use all into-English test beds from the 2006 and 2007 editions of the SMT workshop (Koehn and Monz, 2006; Callison-Burch et al., 2007). These include the translation of three different language-pairs: German-to-English (de-en), Spanish-to-English (es-en), and French-to-English (fr-en), over two different scenarios: in-domain (European Parliament Proceedings) and out-of-domain (News Commentary Corpus)1 . In all cases, a single reference translation is available. In addition, human assessments on adequacy and fluency are available for a subset of systems and sentences. Each sentence has been evaluated at least by two different judges. A brief numerical description of these test beds is available i"
W08-0332,2004.tmi-1.8,0,0.0460446,"Missing"
W08-0332,W05-0904,0,0.235876,"nslations against human references. Extensive details on the metric set may be found in the IQMT technical manual (Gim´enez, 2007). Apart from individual metrics, we have also applied a simple integration scheme based on uniformly-averaged linear metric combinations (Gim´enez and M`arquez, 2008a). 2 What is new? The main novelty, with respect to the set of metrics presented last year (Gim´enez and M`arquez, 2007), is the incorporation of a novel family of metrics at the properly semantic level. DR metrics analyze similarities between automatic and reference DR-STM Semantic Tree Matching, a la Liu and Gildea (2005), but over DRS instead of over constituency trees. DR-Or -⋆ Lexical overlapping over DRS. DR-Orp-⋆ Morphosyntactic overlapping on DRS. Further details on DR metrics can be found in (Gim´enez and M`arquez, 2008b). 2.1 Improved Sentence Level Behavior Metrics based on deep linguistic analysis rely on automatic processors trained on out-domain data, which may be, thus, prone to error. Indeed, we found out that in many cases, metrics are unable to produce a result due to the lack of linguistic analysis. For instance, in our experiments, for SR metrics, we found that the semantic role labeler was u"
W08-0332,N07-1006,0,0.0352971,"7/7 them by the average x score attained over all other test cases for which the parser succeeded. • xi → by linearly interpolating x and Ol scores for all test cases, via the arithmetic mean. In both cases, system scores are calculated by averaging over all sentence scores. Currently, these variants are applied only to SR and DR metrics. 2.2 Uniform Linear Metric Combinations We have simulated a non-parametric combination scheme based on human acceptability by working on uniformly averaged linear combinations (ULC) of metrics (Gim´enez and M`arquez, 2008a). Our approach is similar to that of Liu and Gildea (2007) except that in our case the contribution of each metric to the overall score is not adjusted. Optimal metric sets are determined by maximizing the correlation with human assessments, either at the document or sentence level. However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search. First, metrics are ranked according to their individual quality. Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases. 3 Experimental Work We use all into-English test beds f"
W08-2121,D07-1101,0,0.331336,"sults, but the improvement is not large. These initial efforts indicate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended"
W08-2121,W08-2134,0,0.108513,"movements, split clauses, and split noun phrases. 6.3 Normalized SRL Performance Table 6.3 lists the scores for the semantic subtask measured as the ratio of the labeled F1 score and LAS. As previously mentioned, this score estimates the performance of the SRL component independent of the performance of the syntactic parser. This analysis is not a substitute for the actual experiment where the SRL components are evaluated using correct syntactic information but, nevertheless, it indicates several interesting facts. First, the ranking of the top three systems in Table 10 changes: the system of Che et al. (2008) is now ranked first, and the system of Johansson and Nugues (2008) is second. This shows that Che et al. have a relatively stronger SRL component, whereas Johansson and Nugues developed a better parser. Second, several other systems improved their ranking compared to Table 10: e.g., chen from position thirteenth to ninth and choi from sixteenth to eighth. This indicates that these systems were penalized in the official ranking mainly due to the relative poor performance of their parsers. Note that this experiment is relevant only for systems that implemented pipeline architectures, where the"
W08-2121,W08-2139,0,0.0101528,"MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learning/Opt. column in the table. The system of Riedel and MezaRuiz (2008) deserves a special mention: even though Riedel and Meza-Ruiz did not implement a syntactic parser, they are the only group that performed the complete SRL subtask – i.e., predicate identification and classification, argument identification and classification – jointly, simultaneously for all the predicates in a sentence. They"
W08-2121,M98-1028,0,0.0358658,"and other phenomena, based on a theoretical perspective similar to that of Government and Binding Theory (Chomsky, 1981). 3.1.2 BBN Pronoun Coreference and Entity Type Corpus BBN’s NE annotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall"
W08-2121,W08-2138,1,0.51476,"Missing"
W08-2121,W06-1670,0,0.0107243,"haracters (“ ”) are used in columns 2–5 to ensure the same number of rows for all columns corresponding to one sentence. All syntactic and semantic dependencies are annotated relative to the split word forms (columns 6–8). Table 2 shows the columns available to the systems participating in the open challenge: namedentity labels as in the CoNLL-2003 Shared Task (Tjong Kim San and De Meulder, 2003) and from the BBN Wall Street Journal Entity Corpus,2 WordNet supersense tags, and the output of an offthe-shelf dependency parser (Nivre et al., 2007b). Columns 1–3 were predicted using the tagger of Ciaramita and Altun (2006). Because the BBN corpus shares lexical content with the Penn Treebank, we generated the BBN tags using a 2-fold cross-validation procedure. 2.2 Evaluation Measures We separate the evaluation measures into two groups: (i) official measures, which were used for the ranking of participating systems, and (ii) additional unofficial measures, which provide further insight into the performance of the participating systems. 2.2.1 Official Evaluation Measures The official evaluation measures consist of three different scores: (i) syntactic dependencies are scored using the labeled attachment score (LA"
W08-2121,W05-0620,1,0.81831,"Missing"
W08-2121,gimenez-marquez-2004-svmtool,1,0.278194,"Missing"
W08-2121,C04-1186,0,0.0984296,"formalism, but also extends them significantly: this year’s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems. 1 • SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies. While SRL on top of a dependency treebank has been addressed before (Hacioglu, 2004), our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates. Introduction In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 la"
W08-2121,W08-2122,0,0.366479,"Missing"
W08-2121,W08-2136,0,0.0382094,"Missing"
W08-2121,W08-2123,1,0.339084,"-TMP AM-MNR AM-LOC A3 AM-MOD AM-ADV AM-DIS R-A0 AM-NEG A4 C-A1 R-A1 AM-PNC AM-EXT AM-CAU AM-DIR R-AM-TMP R-A2 R-AM-LOC R-AM-MNR A5 AM-PRD C-A0 C-A2 R-AM-CAU C-A3 R-A3 C-AM-MNR C-AM-ADV AM-REC AA R-AM-PNC C-AM-EXT C-AM-TMP C-A4 Frequency &lt; 10 Frequency 161409 109437 51197 25913 13080 11409 10269 9986 9496 5369 4432 4097 3281 3118 2565 2445 1428 1346 1318 797 307 246 155 91 78 70 65 50 37 29 24 20 16 14 12 11 11 11 70 watanabe). Remarkably, the top-scoring system (johansson) is in a class of its own, with scores 2–3 points higher than the next system. This is most likely caused by the fact that Johansson and Nugues (2008) implemented a thorough system that addressed all facets of the task with state-ofthe-art methods: second-order parsing model, argument identification/classification models separately tuned for PropBank and NomBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of t"
W08-2121,W08-2135,0,0.0101844,"sks: Henderson et al. (2008), who implemented a generative history-based model (Incremental Sigmoid Belief Networks with vectors of latent variables) where syntactic and semantic structures are separately 170 generated but using a synchronized derivation (sequence of actions); Samuelsson et al. (2008), who, within an ensemble-based architecture, implemented a joint syntactic-semantic model using MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learn"
W08-2121,W07-2416,1,0.71052,"oNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages. The CoNLL-2008 shared task1 proposes a unified dependency-based c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 http://www.yr-bcn.es/conll2008 159 • Based on the observation that a richer set of syntactic dependencies improves semantic processing (Johansson and Nugues, 2007), the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks. For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations. • A practical framework is provided for the joint learning of syntactic and semantic dependencies. CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159–177 Manchester, August 2008 Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly set"
W08-2121,W03-0419,0,0.0172595,"Missing"
W08-2121,W08-2124,1,0.554724,"Missing"
W08-2121,W08-2140,0,0.0210024,"a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summarize the architectures and inference strategies used for the identification and classification of predicates and arguments. The columns indicate that most systems modeled the SRL problem as a token-by-token classification problem (“class” in the table) with a corresponding greedy inference strategy. Some systems (e.g., yuret, samuelsson, henderson, lluis) incorporate SRL within parsing, in which case we report the corresponding parsing architecture and inference approach. Vickrey and Koller (2008) simplify the sentences to be labeled using a set of hand-crafted rules before deploying a classification model on top of a constituent-based representation. Unlike in the case of parsing, few systems (yuret, samuelssson, and morante) combine several PA models and the combination is limited to simple voting strategies (see the PA Comb. column). Finally, the ML Methods column lists the Machine Learning (ML) methods used. The column indicates that maximum entropy (ME) was the most popular method (12 distinct systems relied on it). Support Vector Machines (SVM) (eight systems) and the Perceptron"
W08-2121,J93-2004,0,0.0476215,"of the constituent-to-dependency conversion process. The section concludes with an overview of the shared task corpora. 3.1 Input Corpora Input to our merging procedures includes the Penn Treebank, BBN’s named entity corpus, PropBank and NomBank. In this section, we will provide brief descriptions of these annotations in terms of both form and content. All annotations are currently being distributed by the Linguistic Data Consortium, with the exception of NomBank, which is freely downloadable.6 6 http://nlp.cs.nyu.edu/meyers/NomBank. html 162 3.1.1 Penn Treebank 3 The Penn Treebank 3 corpus (Marcus et al., 1993) consists of hand-coded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kuˆcera, 1964) (test only). These hand parses are notated in-line and sometimes involve changing the strings of the input data. For example, in file wsj 0309, the token fearlast in the text corresponds to the two tokens fear and last in the annotated data. In a similar way, cannot is regularly split to can and not. It is significant that the other annotations assume the tokenization of the Penn Treebank, as this makes it easier for us to merge"
W08-2121,W03-3023,0,0.13388,"s(T ) return create-dependency-tree(T ) 3.2 Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al., 1993). Since dependency syntax represents grammatical structure by means of labeled binary head–dependent relations rather than phrases, the task of the conversion procedure is to identify and label the head–dependent pairs. The idea underpinning constituent-to-dependency conversion algorithms (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003) is that head–dependent pairs are created from constituents by selecting one word in each phrase as the head and setting all other as its dependents. The dependency labels are then inferred from the phrase–subphrase or phrase–word relations. Our conversion procedure (Johansson and Nugues, 2007) differs from this basic approach by exploiting the rich structure of the constituent format used in Penn Treebank 3: procedure import-glarf(T ) Import a GLARF surface dependency graph G for each multi-word name N in G for each token d in N Set the function tag of d to NAME for each dependency link h →L"
W08-2121,H05-1066,0,0.305206,"ate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summari"
W08-2121,W08-2126,0,0.0266466,"mBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of the top five groups in the closed challenge submitted results in the open challenge. Only one of the systems that participated in both challenges (zhang) improved the results submitted in the closed challenge. Zhang et al. (2008) achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus. The improvements measured were relatively small for the in-domain WSJ corpus (0.2 labeled macro F1 points) but larger for the out-of-domain Brown corpus (approximately 1 labeled macro F1 point). Table 9: Statistics for semantic roles. 4 Submissions and Results Nineteen groups submitted test runs in the closed challenge and five groups participated in the open challenge. Three of the latter groups participated on"
W08-2121,W01-1511,0,0.0379316,"Missing"
W08-2121,W04-2705,1,0.723818,"Missing"
W08-2121,W06-2933,1,0.512001,"Missing"
W08-2121,J05-1004,0,0.584805,"compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types 7 http://projects.ldc.upenn.edu/ace/ of ARGM (TMP, ADV, etc.).8 Rather than using PropBank directly, we used the version created for the CoNLL-2005 shared task (Carreras and M`arquez, 2005). PropBank’s pointers to subtrees are converted into th"
W08-2121,W08-2125,0,0.116607,"g paper in the proceedings. Results are sorted in descending order of the labeled F1 score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding task. 5 Approaches Table 5 summarizes the properties of the systems that participated in the closed the open challenges. The second column of the table highlights the overall architectures. We used + to indicate that the components are sequentially connected. The lack of a + sign indicates that the corresponding tasks are performed jointly. For example, Riedel and Meza-Ruiz (2008) perform predicate and argument identification and classification jointly, whereas Ciaramita et al. (2008) implemented a pipeline architecture of three components. We use the ||to indicate that several different architectures that span multiple subtasks were deployed in parallel. This summary of system architectures indicates that it is common that systems combine several components in the semantic or syntactic subtasks – e.g., nine systems jointly performed predicate/argument identification and classification – but only four systems combined components between the syntactic and semantic subta"
W08-2121,J03-4003,0,\N,Missing
W08-2121,D07-1096,1,\N,Missing
W08-2121,W04-2412,1,\N,Missing
W08-2124,W06-2925,1,0.909248,"Missing"
W08-2124,D07-1101,0,0.0833103,"ng architecture for syntactic-semantic parsing and to test whether the syntactic and semantic layers can benefit each other from the global training and inference. All the components of our system were built from scratch for this shared task. Due to strong time limitations, our design decisions were biased towards constructing a simple and feasible system. Our proposal is a first order linear model that relies on an online averaged Perceptron for learning (Collins, 2002) and an extended Eisner algorithm for the joint parsing inference. Systems based on Eisner algorithm (Carreras et al., 2006; Carreras, 2007) showed a competitive performance in the syntactic parsing of the English language in some past CoNLL shared tasks. Also, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. we believe that extending the Eisner algorithm to jointly parse syntactic and semantic dependencies it is a natural step to follow. Note that syntactic and semantic tasks are related but not identical. Semantic dependencies can take place between words loosely related by the syntactic structure. Ano"
W08-2124,W02-1001,0,0.016267,"were significantly lower due to bugs present at submission time. 1 Introduction The main goal of this work was to construct a joint learning architecture for syntactic-semantic parsing and to test whether the syntactic and semantic layers can benefit each other from the global training and inference. All the components of our system were built from scratch for this shared task. Due to strong time limitations, our design decisions were biased towards constructing a simple and feasible system. Our proposal is a first order linear model that relies on an online averaged Perceptron for learning (Collins, 2002) and an extended Eisner algorithm for the joint parsing inference. Systems based on Eisner algorithm (Carreras et al., 2006; Carreras, 2007) showed a competitive performance in the syntactic parsing of the English language in some past CoNLL shared tasks. Also, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. we believe that extending the Eisner algorithm to jointly parse syntactic and semantic dependencies it is a natural step to follow. Note that syntactic and sema"
W08-2124,C96-1058,0,0.190134,"Missing"
W08-2124,P05-1012,0,0.0617991,"ght (see www.joachims.org for details). CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 188–192 Manchester, August 2008 is the core module of this work. It simultaneously derives the syntactic and semantic dependencies by using a first order Eisner model, extended with semantic labels and trained with averaged Perceptron. Finally, postprocessing simply selects the most frequent sense for each predicate. 2.1 Preprocessing and feature extraction All features in our system are calculated in the preprocessing phase. We use the features described in McDonald et al. (2005) and Carreras et al. (2006) as input for the syntactic parsing phase, except for the dynamic features from Carreras et al. (2006). The joint syntactic-semantic parser uses all the previous features and also specific features for semantic parsing from Xue and Palmer (2004) and Surdeanu et al. (2007). The features have been straightforwardly adapted to the dependency structure used in this shared task, by substituting any reference to a syntactic constituent by the head of that constituent. About 5M features were extracted from the training corpus. The number of features was reduced to ∼222K usi"
W08-2124,C04-1197,0,0.0331908,"filtered by the POS of the head and modifier (discarding all labels not previously seen in the training corpus between words with the same POS). Another filter removes the core arguments not present in the frame file of each predicate. This strategy allowed us to significantly improve efficiency without any loss in accuracy. 2.5 Postprocess A simple postprocess assigns the most frequent sense to each identified predicate. Frequencies were computed from the training corpus. Experiments performed combining the best and second output of the joint parser and enforcing domain constraints via ILP (Punyakanok et al., 2004) showed no significant improvements. 3 Experiments and Results All the experiments reported here were done using the full training corpus, and results are presented on the development set. The number of features used by the syntactic parser is ∼177K. The joint parser uses ∼45K additional features for recognizing semantic dependencies. Figure 2 shows the learning curves from epoch 1 to 17 for several subsystems and variants. More specifically, it includes LAS performance on syntactic parsing, both for the individual parser and for the syntactic annotation coming from the joint syntactic-semanti"
W08-2124,W08-2121,1,0.799924,"Missing"
W08-2124,W04-3212,0,0.0852457,"ing a first order Eisner model, extended with semantic labels and trained with averaged Perceptron. Finally, postprocessing simply selects the most frequent sense for each predicate. 2.1 Preprocessing and feature extraction All features in our system are calculated in the preprocessing phase. We use the features described in McDonald et al. (2005) and Carreras et al. (2006) as input for the syntactic parsing phase, except for the dynamic features from Carreras et al. (2006). The joint syntactic-semantic parser uses all the previous features and also specific features for semantic parsing from Xue and Palmer (2004) and Surdeanu et al. (2007). The features have been straightforwardly adapted to the dependency structure used in this shared task, by substituting any reference to a syntactic constituent by the head of that constituent. About 5M features were extracted from the training corpus. The number of features was reduced to ∼222K using a frequency threshold filter. A detailed description of the feature set can be found at Llu´ıs (Forthcoming 2008). 2.2 Syntactic parsing Our system uses the Eisner algorithm combined with an online averaged Pereceptron. We define the basic model, which is also the star"
W09-0440,P06-2003,1,0.919826,"Missing"
W09-0440,W05-0909,0,0.0572761,"Missing"
W09-0440,C04-1180,0,0.023841,"nts associated to predicates (i.e., one-place properties), whereas ‘DR-Or -imp’ reflects lexical overlapping between referents associated to implication conditions. We also introduce the ‘DROr -⋆’ metric, which computes average lexical overlapping over all DRS types. Exploiting Semantic Similarity for Automatic MT Evaluation ‘DR’ metrics analyze similarities between automatic and reference translations by comparing their respective DRSs. These are automatically obtained using the C&C Tools (Clark and Curran, 2004)2 . Sentences are first parsed on the basis of a combinatory categorial grammar (Bos et al., 2004). Then, the BOXER component (Bos, 2005) extracts DRSs. As an illustration, Figure 1 shows the DRS representation for the sentence “Every man loves Mary.”. The reader may find the output of the BOXER component (top) together with the equivalent first-order formula (bottom). DRS may be viewed as semantic trees, which are built through the application of two types of DRS conditions: DR-Orp-t These metrics compute morphosyntactic overlapping (i.e., between parts of speech associated to lexical items) between discourse representation structures of the same type t. We also define the ‘DR-Orp -⋆’ met"
W09-0440,C04-1072,0,0.0234522,"Word Recognition Accuracy Sentence Recognition Accuracy Average Adequacy Average Fluency CRR 7 14 500 6 400 — — 1.40 1.16 ASR read 7 14 500 6 400 0.74 0.23 1.02 0.98 ASR spont 7 13 500 6 400 0.68 0.17 0.93 0.98 Table 1: IWSLT 2006 MT Evaluation Campaign. Chinese-to-English test bed description • Human Likeness: Metrics are evaluated in terms of their ability to capture the features which distinguish human from automatic translations. The underlying assumption is that good translations should resemble human translations. Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amig´o et al., 2005). System-level Behavior At the system level (Rsys , columns 7-9), the highest quality is in general attained by metrics based on deep linguistic analysis, either syntactic or semantic. Among lexical metrics, the highest correlation is attained by BLEU and the variant of GTM rewarding longer matchings (e = 2). As to the impact of sentence ill-formedness, while most metrics at the lexical level suffer a significant variation across the three subscenarios, the performance of metrics at deeper linguistic levels is in general quite stable. However, in the case of the translat"
W09-0440,W05-0904,0,0.55788,"set of metrics over several evaluation scenarios of decreasing translation quality. In particular, we have studied the case of Chinese-to-English speech translation, which is a paradigmatic example of low quality and heavily ill-formed output. 1 Introduction Recently, there is a growing interest in the development of automatic evaluation metrics which exploit linguistic knowledge at the syntactic and semantic levels. For instance, we may find metrics which compute similarities over shallow syntactic structures/sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007), constituency trees (Liu and Gildea, 2005) and dependency trees (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may also find metrics operating over shallow semantic structures, such as named entities and semantic roles (Gim´enez and M`arquez, 2007). Linguistic metrics have been proven to produce more reliable system rankings than metrics limitThe rest of the paper is organized as follows. In Section 2, prior to presenting experimental work, we describe the set of metrics employed in our experiments. We also introduce a novel family of metrics which operate at the properly semantic level b"
W09-0440,2007.tmi-papers.15,0,0.248882,"particular, we have studied the case of Chinese-to-English speech translation, which is a paradigmatic example of low quality and heavily ill-formed output. 1 Introduction Recently, there is a growing interest in the development of automatic evaluation metrics which exploit linguistic knowledge at the syntactic and semantic levels. For instance, we may find metrics which compute similarities over shallow syntactic structures/sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007), constituency trees (Liu and Gildea, 2005) and dependency trees (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may also find metrics operating over shallow semantic structures, such as named entities and semantic roles (Gim´enez and M`arquez, 2007). Linguistic metrics have been proven to produce more reliable system rankings than metrics limitThe rest of the paper is organized as follows. In Section 2, prior to presenting experimental work, we describe the set of metrics employed in our experiments. We also introduce a novel family of metrics which operate at the properly semantic level by analyzing similarities over discourse representations. Experimental work is then pre"
W09-0440,N03-2021,0,0.300449,"Missing"
W09-0440,niessen-etal-2000-evaluation,0,0.0961229,"Missing"
W09-0440,P04-1014,0,0.0188338,"g to their type ‘t’. For instance, ‘DR-Or -pred’ roughly reflects lexical overlapping between the referents associated to predicates (i.e., one-place properties), whereas ‘DR-Or -imp’ reflects lexical overlapping between referents associated to implication conditions. We also introduce the ‘DROr -⋆’ metric, which computes average lexical overlapping over all DRS types. Exploiting Semantic Similarity for Automatic MT Evaluation ‘DR’ metrics analyze similarities between automatic and reference translations by comparing their respective DRSs. These are automatically obtained using the C&C Tools (Clark and Curran, 2004)2 . Sentences are first parsed on the basis of a combinatory categorial grammar (Bos et al., 2004). Then, the BOXER component (Bos, 2005) extracts DRSs. As an illustration, Figure 1 shows the DRS representation for the sentence “Every man loves Mary.”. The reader may find the output of the BOXER component (top) together with the equivalent first-order formula (bottom). DRS may be viewed as semantic trees, which are built through the application of two types of DRS conditions: DR-Orp-t These metrics compute morphosyntactic overlapping (i.e., between parts of speech associated to lexical items)"
W09-0440,W07-0411,0,0.048663,"Missing"
W09-0440,2001.mtsummit-papers.68,0,0.106397,"Missing"
W09-0440,W07-0738,1,0.887344,"Missing"
W09-0440,P04-1077,0,0.0468013,"Word Recognition Accuracy Sentence Recognition Accuracy Average Adequacy Average Fluency CRR 7 14 500 6 400 — — 1.40 1.16 ASR read 7 14 500 6 400 0.74 0.23 1.02 0.98 ASR spont 7 13 500 6 400 0.68 0.17 0.93 0.98 Table 1: IWSLT 2006 MT Evaluation Campaign. Chinese-to-English test bed description • Human Likeness: Metrics are evaluated in terms of their ability to capture the features which distinguish human from automatic translations. The underlying assumption is that good translations should resemble human translations. Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amig´o et al., 2005). System-level Behavior At the system level (Rsys , columns 7-9), the highest quality is in general attained by metrics based on deep linguistic analysis, either syntactic or semantic. Among lexical metrics, the highest correlation is attained by BLEU and the variant of GTM rewarding longer matchings (e = 2). As to the impact of sentence ill-formedness, while most metrics at the lexical level suffer a significant variation across the three subscenarios, the performance of metrics at deeper linguistic levels is in general quite stable. However, in the case of the translat"
W09-0440,W07-0707,0,0.270468,"al study on the behavior of a heterogeneous set of metrics over several evaluation scenarios of decreasing translation quality. In particular, we have studied the case of Chinese-to-English speech translation, which is a paradigmatic example of low quality and heavily ill-formed output. 1 Introduction Recently, there is a growing interest in the development of automatic evaluation metrics which exploit linguistic knowledge at the syntactic and semantic levels. For instance, we may find metrics which compute similarities over shallow syntactic structures/sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007), constituency trees (Liu and Gildea, 2005) and dependency trees (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007). We may also find metrics operating over shallow semantic structures, such as named entities and semantic roles (Gim´enez and M`arquez, 2007). Linguistic metrics have been proven to produce more reliable system rankings than metrics limitThe rest of the paper is organized as follows. In Section 2, prior to presenting experimental work, we describe the set of metrics employed in our experiments. We also introduce a novel family of metrics whi"
W09-0440,2006.amta-papers.25,0,0.0606614,"Missing"
W09-0440,P05-1035,0,\N,Missing
W09-0440,2006.iwslt-evaluation.1,0,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-1212,burchardt-etal-2006-salsa,0,0.0522326,"Missing"
W09-1212,W06-2925,1,0.898848,"Missing"
W09-1212,D07-1101,0,0.277768,"with each language presenting their own particularities. An interesting particularity is the direct correspondence between syntactic and semantic dependencies provided in Catalan, Spanish and Chinese. We believe that these correspondences can be captured by a joint system. We specially look at the syntactic-semantic alignment of the Catalan and Spanish datasets. Our system is an extension of the Llu´ıs and M`arquez (2008) CoNLL-2008 Shared Task system. We introduce these two following novelties: We present a system developed for the CoNLL-2009 Shared Task (Hajiˇc et al., 2009). We extend the Carreras (2007) parser to jointly annotate syntactic and semantic dependencies. This state-of-the-art parser factorizes the built tree in second-order factors. We include semantic dependencies in the factors and extend their score function to combine syntactic and semantic scores. The parser is coupled with an on-line averaged perceptron (Collins, 2002) as the learning method. Our averaged results for all seven languages are 71.49 macro F1 , 79.11 LAS and 63.06 semantic F1 . 1 • An extension of the second-order Carreras (2007) algorithm to annotate semantic dependencies. Introduction Systems that jointly ann"
W09-1212,W02-1001,0,0.0315067,"an and Spanish datasets. Our system is an extension of the Llu´ıs and M`arquez (2008) CoNLL-2008 Shared Task system. We introduce these two following novelties: We present a system developed for the CoNLL-2009 Shared Task (Hajiˇc et al., 2009). We extend the Carreras (2007) parser to jointly annotate syntactic and semantic dependencies. This state-of-the-art parser factorizes the built tree in second-order factors. We include semantic dependencies in the factors and extend their score function to combine syntactic and semantic scores. The parser is coupled with an on-line averaged perceptron (Collins, 2002) as the learning method. Our averaged results for all seven languages are 71.49 macro F1 , 79.11 LAS and 63.06 semantic F1 . 1 • An extension of the second-order Carreras (2007) algorithm to annotate semantic dependencies. Introduction Systems that jointly annotate syntactic and semantic dependencies were introduced in the past CoNLL2008 Shared Task (Surdeanu et al., 2008). These systems showed promising results and proved the feasibility of a joint syntactic and semantic parsing (Henderson et al., 2008; Llu´ıs and M`arquez, 2008). The Eisner (1996) algorithm and its variants are commonly used"
W09-1212,C96-1058,0,0.0168552,"oupled with an on-line averaged perceptron (Collins, 2002) as the learning method. Our averaged results for all seven languages are 71.49 macro F1 , 79.11 LAS and 63.06 semantic F1 . 1 • An extension of the second-order Carreras (2007) algorithm to annotate semantic dependencies. Introduction Systems that jointly annotate syntactic and semantic dependencies were introduced in the past CoNLL2008 Shared Task (Surdeanu et al., 2008). These systems showed promising results and proved the feasibility of a joint syntactic and semantic parsing (Henderson et al., 2008; Llu´ıs and M`arquez, 2008). The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing. Improvements of this algorithm presented by McDonald et al. (2006) and Carreras (2007) achieved state-of-the-art performance for English in the CoNLL-2007 Shared Task (Nivre et al., 2007). Johansson and Nugues (2008) presented a system based on the Carreras’ extension of the Eisner algorithm that ranked first in the past CoNLL2008 Shared Task. We decided to extend the Car79 • A combined syntactic-semantic scoring for Catalan and Spanish to exploit the syntacticsemantic mappings. The following section outlines the"
W09-1212,W08-2122,0,0.0217183,"o combine syntactic and semantic scores. The parser is coupled with an on-line averaged perceptron (Collins, 2002) as the learning method. Our averaged results for all seven languages are 71.49 macro F1 , 79.11 LAS and 63.06 semantic F1 . 1 • An extension of the second-order Carreras (2007) algorithm to annotate semantic dependencies. Introduction Systems that jointly annotate syntactic and semantic dependencies were introduced in the past CoNLL2008 Shared Task (Surdeanu et al., 2008). These systems showed promising results and proved the feasibility of a joint syntactic and semantic parsing (Henderson et al., 2008; Llu´ıs and M`arquez, 2008). The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing. Improvements of this algorithm presented by McDonald et al. (2006) and Carreras (2007) achieved state-of-the-art performance for English in the CoNLL-2007 Shared Task (Nivre et al., 2007). Johansson and Nugues (2008) presented a system based on the Carreras’ extension of the Eisner algorithm that ranked first in the past CoNLL2008 Shared Task. We decided to extend the Car79 • A combined syntactic-semantic scoring for Catalan and Spanish to exploit the syntacticsemanti"
W09-1212,W08-2123,0,0.107434,"Missing"
W09-1212,kawahara-etal-2002-construction,0,0.0355757,"Missing"
W09-1212,W08-2124,1,0.743443,"Missing"
W09-1212,E06-1011,0,0.0689132,"Missing"
W09-1212,P05-1012,0,0.0812802,"components: 1) Preprocessing and feature extraction. 2) Syntactic preparsing. 3) Joint syntactic-semantic parsing. 4) Predicate classification. The preprocessing and feature extraction is intended to ease and improve the performance of the parser precomputing a binary representation of Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 79–84, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics each sentence features. These features are borrowed from existing and widely-known systems (Xue and Palmer, 2004; McDonald et al., 2005; Carreras et al., 2006; Surdeanu et al., 2007). The following step is a syntactic pre-parse. It is only required to pre-compute additional features (e.g., syntactic path, syntactic frame) from the syntax. These new features will be used for the semantic role component of the following joint parser. The joint parser is the core of the system. This single algorithm computes the complete parse that optimizes a score according to a function that depends on both syntax and semantics. Some of the required features that could be unavailable or expensive to compute at that time are provided by the pr"
W09-1212,W08-2121,1,0.913236,"Missing"
W09-1212,taule-etal-2008-ancora,0,0.0621579,"Missing"
W09-1212,W04-3212,0,0.0810005,"consists on four main components: 1) Preprocessing and feature extraction. 2) Syntactic preparsing. 3) Joint syntactic-semantic parsing. 4) Predicate classification. The preprocessing and feature extraction is intended to ease and improve the performance of the parser precomputing a binary representation of Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 79–84, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics each sentence features. These features are borrowed from existing and widely-known systems (Xue and Palmer, 2004; McDonald et al., 2005; Carreras et al., 2006; Surdeanu et al., 2007). The following step is a syntactic pre-parse. It is only required to pre-compute additional features (e.g., syntactic path, syntactic frame) from the syntax. These new features will be used for the semantic role component of the following joint parser. The joint parser is the core of the system. This single algorithm computes the complete parse that optimizes a score according to a function that depends on both syntax and semantics. Some of the required features that could be unavailable or expensive to compute at that time"
W09-1212,D07-1096,0,\N,Missing
W09-2411,D08-1069,0,0.0606217,"Missing"
W09-2411,H05-1004,0,0.822315,"means of two different evaluation metrics. By setting up a multilingual scenario, we can explore to what extent it is possible to implement a general system that is portable to the three languages, how much language-specific tuning is necessary, and the significant differences between Romance languages and English, as well as those between two closely related languages such as Spanish and Catalan. Besides, we expect to gain some useful insight into the development of multilingual NLP applications. As far as the evaluation is concerned, by employing B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) algorithms we can consider both the advantages and drawbacks of using one or the other scoring metric. For comparison purposes, the MUC score will also be reported. Among others, we are interested in the following questions: Which evaluation metric provides a more accurate picture of the accuracy of the system performance? Is there a strong correlation between them? Can 1 Corpora annotated with coreference are scarce, especially for languages other than English. 70 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70–75, c Boulder,"
W09-2411,P98-2143,0,0.0998874,"nce information has been shown to be beneficial in many NLP applications such as Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 2000), and Machine Translation. In these systems, there is a need to identify the different pieces of information that refer to the same discourse entity in order to produce coherent and fluent summaries, disambiguate the references to an entity, and solve anaphoric pronouns. Coreference is an inherently complex phenomenon. Some of the limitations of the traditional rulebased approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow This task will promote the development of linguistic resources –annotated corpora1– and machine-learning techniques oriented to coreference resolution. In particular, we aim to evaluate and compare coreference resolution systems in a multilingual context, including Catalan, English, and Spanish languages, and by means of two different evaluation metrics. By setting up a multilingual scenario, we can explore to what extent it is possible to implement a general system that is portable to the three languages, how much language-specifi"
W09-2411,D08-1067,0,0.338689,"Missing"
W09-2411,J00-4005,0,0.144123,"Missing"
W09-2411,M95-1005,0,0.926369,"Missing"
W10-1750,W07-0738,1,0.893535,"Missing"
W10-1750,W08-0332,1,0.880192,"Missing"
W10-1750,W09-0440,1,0.81187,"Missing"
W10-1750,W08-2222,0,0.0149095,"new sweater is blue.”, where the possessive feminine adjective “her” refers to the proper noun “Maria”. In this work, as a first proposal, instead of elaborating on novel similarity measures, we have borrowed and extended the Discourse Representation (DR) metrics defined by Gim´enez and M`arquez (2009). These metrics analyze similarities between automatic and reference translations by comparing their respective discourse representations over individual sentences. For the discursive analysis of texts, DR metrics rely on the C&C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008). This software is based on the Discourse Representation Theory (DRT) by Kamp and Reyle (1993). DRT is a theoretical framework offering a representation language for the examination of contextually dependent meaning in discourse. A discourse is represented in a discourse representation structure (DRS), which is essentially a variation of first-order predicate calculus —its forms are pairs • the link between a demonstrative pronoun and its referent, which is exemplified in the sentences “He developed a new theory on grammar. However, this is not the only theory he developed”. In the second sent"
W10-1750,W08-0309,0,0.0401479,"his case DRdoc variants seem to obtain higher correlation than their DR counterparts. The improvement is especially substantial in terms of Spearman and Kendall coefficients, which do not consider absolute values but ranking positions. However, it could be the case that it was just an average ef336 quoted question, the verb “invite” works as an object control verb because its patient “Chechen representatives” is also the agent of the verb visit. as described in (Gim´enez and M`arquez, 2008). This strategy has proven as an effective means of combining the scores conferred by different metrics (Callison-Burch et al., 2008; Callison-Burch et al., 2009). Metrics submitted are: Example 1: The minister went on to say, “What would Moscow say if we were to invite Chechen representatives to visit Jerusalem?” DRdoc an arithmetic mean over a heuristicallydefined set of DRdoc metric variants, respectively computing lexical overlap, morphosyntactic overlap, and semantic tree matching (M = {‘DRdoc -Or (⋆)’, ‘DRdoc -Orp (⋆)’, ‘DRdoc STM4 ’}). Since DRdoc metrics do not operate over individual segments, we have assigned each segment the score of the document in which it is contained. Anaphora and pronoun resolution. Wheneve"
W10-1750,W05-0904,0,0.183533,"). Since the cost of exhaustive resampling was prohibitive, we have limited to 1,000 resamplings. Confidence intervals, not shown in the tables, are in all cases lower than 10−3 . complex conditions: disjunction, implication, negation, question, and propositional attitude operations. For instance, the DRS representation for the sentence “Every man loves Mary.” is as follows: ∃y named(y, mary, per) ∧ (∀x man(x) → ∃z love(z) ∧ event(z) ∧ agent(z, x) ∧ patient(z, y)). DR integrates three different kinds of metrics: DR-STM These metrics are similar to the Syntactic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSs instead of constituent trees. All semantic subpaths in the candidate and reference trees are retrieved. The fraction of matching subpaths of a given length (l=4 in our experiments) is computed. DR-Or (⋆) Average lexical overlap between discourse representation structures of the same type. Overlap is measured according to the formulae and definitions by Gim´enez and M`arquez (2007). DR-Orp (⋆) Average morphosyntactic overlap, i.e., between grammatical categories –partsof-speech– associated to lexical items, between discourse representation structures of the same t"
W10-1750,W09-0401,0,\N,Missing
W10-1750,P07-2009,0,\N,Missing
W10-1750,W04-3250,0,\N,Missing
W11-1001,C04-1046,0,0.512338,"cessor in terms of robustness to noisy data, making it possible to employ a wider range of linguistic processors. Within this framework, in this paper we describe our attempt to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for pairwise translation ranking. The extent to which this kind of annotations are transferred from source to target has indeed a very high correlation with respect to human quality assessments (Lo and Wu, 2010a; 2010b). The measure that we propose is then an ideal addition to already established CE measures, e.g., (Specia et al., 2009; Blatz et al., 2004), as it attempts to explicitly model the adequacy of translation hypotheses as a function of predicateargument structure coverage. While we are aware of the fact that the current definition of the model can be improved in many different ways, our preliminary investigation, on five English to Spanish translation 2 benchmarks, shows promising accuracy on the difficult task of pairwise translation ranking, even for translations with very few distinguishing features. To capture different aspects of the projection of SRL annotations we employ two instances of the abstract architecture shown in Figu"
W11-1001,W05-0620,1,0.854319,"Missing"
W11-1001,2011.eamt-1.13,0,0.030712,"Missing"
W11-1001,lo-wu-2010-evaluating,0,0.149947,"on re-ranking and user feedback evaluation. Preliminary experiments in pairwise hypothesis ranking on five confidence estimation benchmarks show that the model has the potential to capture salient aspects of translation quality. 1 Introduction The ability to automatically assess the quality of translation hypotheses is a key requirement towards the development of accurate and dependable translation models. While it is largely agreed that proper transfer of predicate-argument structures from source to target is a very strong indicator of translation quality, especially in relation to adequacy (Lo and Wu, 2010a; 2010b), the incorporation of this kind of information in the Statistical Machine Translation (SMT) evaluation pipeline is still limited to few and isolated cases, e.g., (Gim´enez and M`arquez, 2010). In this paper, we propose a general model for the incorporation of predicate-level semantic annotations in the framework of Confidence Estimation 1 (CE) for machine translation, with a specific focus on the sub-problem of pairwise hypothesis ranking. The model is based on the following underlying assumption: by observing how automatic alignments project semantic annotations from source to targe"
W11-1001,W10-3807,0,0.195784,"on re-ranking and user feedback evaluation. Preliminary experiments in pairwise hypothesis ranking on five confidence estimation benchmarks show that the model has the potential to capture salient aspects of translation quality. 1 Introduction The ability to automatically assess the quality of translation hypotheses is a key requirement towards the development of accurate and dependable translation models. While it is largely agreed that proper transfer of predicate-argument structures from source to target is a very strong indicator of translation quality, especially in relation to adequacy (Lo and Wu, 2010a; 2010b), the incorporation of this kind of information in the Statistical Machine Translation (SMT) evaluation pipeline is still limited to few and isolated cases, e.g., (Gim´enez and M`arquez, 2010). In this paper, we propose a general model for the incorporation of predicate-level semantic annotations in the framework of Confidence Estimation 1 (CE) for machine translation, with a specific focus on the sub-problem of pairwise hypothesis ranking. The model is based on the following underlying assumption: by observing how automatic alignments project semantic annotations from source to targe"
W11-1001,J05-1004,0,0.0460126,"framework for the inclusion of linguistic processors in CE that has the advantage of requiring resources and software tools only on the source side of the translation, where well-formed input can reasonably be expected. 3 Model The task of semantic role labeling (SRL) consists in recognizing and automatically annotating semantic relations between a predicate word (not necessarily a verb) and its arguments in natural language texts. The resulting predicate-argument structures are commonly referred to as propositions, even though we will also use the more general term annotations. In PropBank (Palmer et al., 2005) style annotations, which our model is based on, predicates are generally verbs and roles are divided into two classes: core roles (labeled A0, A1, . . . A5), whose semantic value is defined by the predicate syntactic frame, and adjunct roles (labeled AM-*, e.g., AMTMP or AM-LOC) 1 which are a closed set of verbindependent semantic labels accounting for predicate aspects such as temporal, locative, manner or purpose. For instance, in the sentence “The commission met to discuss the problem” we can identify two predicates, met and discuss. The corresponding annotations are “[A0 The commission] ["
W11-1001,P02-1040,0,0.0837072,"between “good” and “bad” translations. Even though the authors show that a small set of shallow features and some supervision can produce good results on a specific benchmark, we are convinced that more linguistic features are needed for these methods to perform better across a wider spectrum of domains and applications. Concerning the usage of SRL for SMT, Wu and Fung (2009) reported a first successful application of semantic role labels to improve translation quality. They note that improvements in translation quality are not reflected by traditional MT evaluation metrics (Doddington, 2002; Papineni et al., 2002) based on n-gram overlaps. To further investigate the topic, Lo and Wu (2010a; 2010b) involved human annotators to demonstrate that the quality of semantic role projection on translated sentences is very highly correlated with human assessments. Gim´enez and M`arquez (2010) describe a framework for MT evaluation and meta-evaluation combining a rich set of n-gram-based and linguistic metrics, including several variants of a metric based on SRL. Automatic and reference translations are annotated independently, and the lexical overlap between corresponding arguments is employed as an indicator of"
W11-1001,2009.mtsummit-papers.16,0,0.210699,"on the linguistic processor in terms of robustness to noisy data, making it possible to employ a wider range of linguistic processors. Within this framework, in this paper we describe our attempt to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for pairwise translation ranking. The extent to which this kind of annotations are transferred from source to target has indeed a very high correlation with respect to human quality assessments (Lo and Wu, 2010a; 2010b). The measure that we propose is then an ideal addition to already established CE measures, e.g., (Specia et al., 2009; Blatz et al., 2004), as it attempts to explicitly model the adequacy of translation hypotheses as a function of predicateargument structure coverage. While we are aware of the fact that the current definition of the model can be improved in many different ways, our preliminary investigation, on five English to Spanish translation 2 benchmarks, shows promising accuracy on the difficult task of pairwise translation ranking, even for translations with very few distinguishing features. To capture different aspects of the projection of SRL annotations we employ two instances of the abstract archi"
W11-1001,specia-etal-2010-dataset,0,0.0686989,"sentence. In all cases in which multiple assessments are available, we used the average of the assessments. The wmt07 dataset would be the most interesting of all, in that it provides separate assessments for the two main dimensions of translation quality, adequacy and fluency, as well as system rankings. Unluckily, the number of annotations in this dataset is very small, and after eliminating the ties the numbers are even smaller. As results on such small numbers would not be very representative, we decided not to include them in our evaluation. We also evaluated on the dataset described in (Specia et al., 2010), which we will refer to as specia. As the system is based on Europarl data, it is to be considered an in-domain benchmark. The dataset includes results produced by four different systems, each translation being annotated by only one judge. Given the size of the corpus (the output of each system has been annotated on the same set of 4,000 sentences), this dataset is the most representative among those that we considered. It is also especially interesting for two other reasons: 1) systems are assigned a score ranging from 1 (bad) to 4 (good as it is) based on the number of edits required to pro"
W11-1001,W05-0635,0,0.0127553,"ly to generate quite different translations for the same input sentences. 4.2 Setup Our model consists of four main components: an automatic semantic role labeler (to annotate source sentences); a lexical translation model (to gener6 ate the alignments required to map the annotations onto a translation hypothesis); a translation model for predicate-argument structures, to assign a score to projected annotations; and a translation model for role fillers, to assign a score to the projection of each argument. To automatically label our training data with semantic roles we used the Swirl system2 (Surdeanu and Turmo, 2005) with the bundled English models for syntactic and semantic parsing. On the CoNLL-2005 benchmark (Carreras and M`arquez, 2005), Swirl sports an F1-measure of 76.46. This figure drops to 75 for mixed data, and to 65.42 on out-of-domain data, which we can regard as a conservative estimate of the accuracy of the labeler on wmt benchmarks. For all the translation tasks we employed the Moses phrase-based decoder3 in a single-factor configuration. The -constraint command line parameter is used to force Moses to output the desired translation. For the English to Spanish lexical translation model, we"
W11-1001,N09-2004,0,0.0153762,"ercentage of uni-grams are introduced. Features are selected via Partial Least Squares (PLS) regression (Wold et al., 1984). Inductive Confidence Machines (Papadopoulos et al., 2002) are used to estimate an optimal threshold to distinguish between “good” and “bad” translations. Even though the authors show that a small set of shallow features and some supervision can produce good results on a specific benchmark, we are convinced that more linguistic features are needed for these methods to perform better across a wider spectrum of domains and applications. Concerning the usage of SRL for SMT, Wu and Fung (2009) reported a first successful application of semantic role labels to improve translation quality. They note that improvements in translation quality are not reflected by traditional MT evaluation metrics (Doddington, 2002; Papineni et al., 2002) based on n-gram overlaps. To further investigate the topic, Lo and Wu (2010a; 2010b) involved human annotators to demonstrate that the quality of semantic role projection on translated sentences is very highly correlated with human assessments. Gim´enez and M`arquez (2010) describe a framework for MT evaluation and meta-evaluation combining a rich set o"
W11-1001,W09-0401,0,\N,Missing
W12-3115,W12-3102,0,0.0373135,"Missing"
W12-3115,gimenez-marquez-2004-svmtool,1,0.710027,"Missing"
W12-3115,P07-2045,0,0.00341633,"Missing"
W12-3115,N06-1014,0,0.0444623,"ores, and C − in the bottom (Q1) and bottom two (Q12) fourths. As an example, the feature DEP/C+ /Q4/R encodes the value of C+ within the top fourth of the ranked list of projected dependencies when only considering word roots, while DEP/C− /W is the value of C− on the whole edge set estimated using word forms. 3 Experiment setup To extract the extended feature set we use an alignment model, a POS tagger and a dependency parser. Concerning the former, we trained an unsupervised model with the Berkeley aligner4 , an implementation of the symmetric word-alignment model described by Liang et al. (2006). The model is trained on Europarl and newswire data released as part of WMT 2011 (Callison-Burch et al., 2011) training data. For POS tagging and semantic role annotation we use SVMTool5 (Jes´us Gim´enez and Llu´ıs M`arquez, 2004) and Swirl6 (Surdeanu and Turmo, 2005), respectively, with default configurations. To estimate the SEQ and DEP features we use reference and automatic translations of the newswire section of WMT 2011 training data. The automatic translations are generated by the same configuration generating the data for the quality estimation task. The n-gram models are estimated wi"
W12-3115,W05-0635,0,0.0201636,"value of C− on the whole edge set estimated using word forms. 3 Experiment setup To extract the extended feature set we use an alignment model, a POS tagger and a dependency parser. Concerning the former, we trained an unsupervised model with the Berkeley aligner4 , an implementation of the symmetric word-alignment model described by Liang et al. (2006). The model is trained on Europarl and newswire data released as part of WMT 2011 (Callison-Burch et al., 2011) training data. For POS tagging and semantic role annotation we use SVMTool5 (Jes´us Gim´enez and Llu´ıs M`arquez, 2004) and Swirl6 (Surdeanu and Turmo, 2005), respectively, with default configurations. To estimate the SEQ and DEP features we use reference and automatic translations of the newswire section of WMT 2011 training data. The automatic translations are generated by the same configuration generating the data for the quality estimation task. The n-gram models are estimated with the DeltaAvg MAE 0.4664 0.4694 0.6346 0.6248 Table 2: Comparison of the baseline and extended feature set on development data. SRILM toolkit 7 , with order equal to 3 and KneserNey (Kneser and Ney, 1995) smoothing. As a learning framework we resort to Support Vector"
W12-3115,W07-0700,0,\N,Missing
W12-3115,W11-2100,0,\N,Missing
W13-2215,W11-2107,0,0.0192041,"E models we used the data from the WMT13 shared task on quality estimation (System Selection Quality Estimation at Sentence Level task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the follow"
W13-2215,W12-3133,1,0.860966,"Missing"
W13-2215,2012.amta-monomt.1,1,0.796291,"Missing"
W13-2215,2013.mtsummit-papers.9,1,0.774503,"Missing"
W13-2215,padro-etal-2010-freeling,0,0.0202206,"Missing"
W13-2215,W06-1607,0,0.0198766,"s with additional information, such as POS tags or lemmas. In that case, factors other than surface (e.g. POS) are usually less sparse, allowing the construction of factor-specific language models with higher-order n-grams. Such language models can help to obtain syntactically more correct outputs. We used the standard models available in Moses as feature functions: relative frequencies, lexical weights, word and phrase penalties, wbe-msdbidirectional-fe reordering models, and two language models (one for surface and one for POS tags). Phrase scoring was computed using GoodTuring discounting (Foster et al., 2006). As aforementioned, we developed five factored Moses-based independent systems with different 134 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the"
W13-2215,P02-1040,0,0.0862502,"on Statistical Machine Translation, pages 134–140, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation"
W13-2215,D08-1090,0,0.0227644,"namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and with a cache size of 100 for kernel evaluations. The trade-off parameter was empirically set to 0.001. Table 2 sh"
W13-2215,2011.eamt-1.18,1,0.897081,"Missing"
W13-2215,P10-1063,0,0.0150842,"task5 ), which contains the test sets from other WMT campaigns with human assessments. We used five groups of features, namely: i) QuestQE: 17 QE features provided by the Quest toolkit6 ; ii) AsiyaQE: 26 QE features provided by the Asiya toolkit for MT evaluation (Gim´enez and M`arquez, 2010a); iii) LM (and LM-PoS) perplexities trained with monolingual data; iv) PR: Classical lexical-based measures -BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Denkowski and Lavie, 2011)- computed with a pseudo-reference approach, that is, using the other system candidates as references (Soricut and Echihabi, 2010); and v) PROTHER: Reference based metrics provided by Asiya, including GTM, ROUGE, PER, TER (Snover et al., 2008), and syntax-based evaluation measures also with a pseudo-reference approach. We trained a Support Vector Machine ranker by means of pairwise comparison using the SVMlight toolkit (Joachims, 1999), but with the “-z p” parameter, which can provide system rankings for all the members of different groups. The learner algorithm was run according to the following parameters: linear kernel, expanding the working set by 9 variables at each iteration, for a maximum of 50,000 iterations and"
W13-2215,D11-1125,0,0.0166218,"nal Linguistics approaches. We explain them in Section 3. As a final decision, we applied a system selection scheme (Formiga et al., 2013; Specia et al., 2010) to consider the best candidate for each sentence, according to human trained quality estimation (QE) models. We set monotone reordering of the punctuation signs for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every cor"
W13-2215,P07-2045,0,0.10399,"such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. 1 Introduction 2 The TALP-UPC center (Center for Language and Speech Technologies and Applications at Universitat Polit`ecnica de Catalunya) focused on the English to Spanish translation of the WMT13 shared task. Our primary (contrastive) run is an internal system selection comprised of different training approaches (without CommonCrawl, unless stated): (a) Moses Baseline (Koehn et al., 2007b), (b) Moses Baseline + Morphology Generation (Formiga et al., 2012b), (c) Moses Baseline + News Adaptation (Henr´ıquez Q. et al., 2011), (d) Moses Baseline + News Adaptation + Morphology Generation , and (e) Moses Baseline + News Adaptation + Filtered CommonCrawl Adaptation (Barr´on-Cede˜no et al., 2013). Our secondary run includes is the full training strategy marked as (e) in the previous description. The main differences with respect to our last year’s participation (Formiga et al., 2012a) are: i) the inclusion of the CommonCrawl corpus, using Baseline system: Phrase-Based SMT Our contrib"
W13-2215,2005.mtsummit-papers.11,0,0.0437825,"s for the decoding using the Moses wall feature. We tuned the systems using the Moses MERT (Och, 2003) implementation. Our focus was on minimizing the BLEU score (Papineni et al., 2002) of the development set. Still, for exploratory purposes, we tuned configuration (c) using PRO (Hopkins and May, 2011) to set the initial weights at every iteration of the MERT algorithm. However, it showed no significant differences compared to the original MERT implementation. We trained the baseline system using all the available parallel corpora, except for common-crawl. That is, European Parliament (EPPS) (Koehn, 2005), News Commentary, and United Nations. Regarding the monolingual data, there were more News corpora organized by years for Spanish. The data is available at the Translation Task’s website1 . We used all the News corpora to busld the language model (LM). Firstly, a LM was built for every corpus independently. Afterwards, they were combined to produce de final LM. For internal testing we used the News 2011 and News 2012 data and concatenated the remaining three years of News data as a single parallel corpus for development. We processed the corpora as in our participation to WMT12 (Formiga et al"
W13-2215,P03-1021,0,\N,Missing
W13-2215,2011.eamt-1.20,1,\N,Missing
W13-2244,2012.amta-papers.13,1,0.318131,"Missing"
W13-2244,2013.mtsummit-papers.9,1,0.683064,"Missing"
W13-2244,P10-1063,0,0.0136334,"indicators. Secondly, we used Random Forests (Breiman, 2001), the rationale was the same as ranking-topairwise implementation from SVMlight . However, SVMlight considers two different data preprocessing methods depending on the kernel of the classifier: LINEAR and RBF-Kernel. We used the same data-preprocessing algorithm from SVMlight in order to train a Random Forest classifier with ties (three classes: {0,-1,1}) based upon the pairwise relations. We used the Random Forests implementation of scikit-learn toolkit (Pedregosa et al., 2011) with 50 estimators. 3.2 Pseudo-Reference-based Features Soricut and Echihabi (2010) introduced the concept of pseudo-reference-based features (PR) for translation ranking estimation. The principle is that, in the lack of human-produced references, automatic ones are still good for differentiating good from bad translations. One or more secondary MT systems are required to generate translations starting from the same input, which are Once the classes are given by the Random For360 Note that a considerable amount of the features described in the baseline group (QQE and AQE) fall in this category. In this subsection we include some extra features we devised to capture source– t"
W13-2244,P12-3024,1,0.541573,"Missing"
W13-2244,W05-0635,0,0.0232696,"measure for “pseudo-prefixes” (considering only up to four initial characters for every token). 3.4 We interpolated different language models comprising the WMT’12 Monolingual corpora (EPPS, News, UN and Gigafrench for English). The interpolation weights were computed as to minimize the perplexity according to the WMT Translation Task test data (2008-2010)4 . The features are as follow: 5. Based on semantic information (SEM) Twelve features calculated with named entity- and semantic role-based evaluation measures (again, provided by A SIYA). Sentences are automatically annotated using SwiRL (Surdeanu and Turmo, 2005) and BIOS (Surdeanu et al., 2005). We used these features in the de-en subtask only. 9. Language Model Features (LM) Two log-probabilities of the translation candidate with respect to the above described interpolated language models over word forms and PoS labels. 6. Explicit semantic analysis (ESA) Two versions of explicit semantic analysis (Gabrilovich and Markovitch, 2007), a semantic similarity measure, built on top of Wikipedia (we used the opening paragraphs of 100k Wikipedia articles as in 2010). 3.3 Adapted Language-Model Features 4 Experiments and Results In this section we describe t"
W13-2244,P04-1077,0,0.0383445,"Missing"
W13-2244,N03-2021,0,0.0179653,"be calculated with any evaluation measure or text similarity function, which gives us all feature variants in this group. We consider the following PR-based features: 7. Alignment-based features (ALG / ALGPR) One measure calculated over the aligned words between a candidate translation and the source (ALG); and two measures based on the comparison between these alignments for two different translations (e.g., candidate and pseudo-reference) and the source (ALGPR).3 3. Derived from A SIYA’s metrics (APR) Twenty-three PR features, including GTM-l (l∈{1,2,3}) to reward different length matching (Melamed et al., 2003), four variants of ROUGE (-L, -S*, -SU* and -W) (Lin and Och, 2004), WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER, and TERbase (i.e., without stemming, synonymy look-up, nor paraphrase support) (Snover et al., 2009), and all the shallow and full parsing measures (i.e., constituency and dependency parsing, PoS, chunking and lemmas) that A SIYA provides either for Spanish or English as target languages. 8. Length model (LeM) A measure to estimate the quality likelihood of a candidate sentence by considering the “expected length” of a proper translation from the source. The measure"
W13-2244,niessen-etal-2000-evaluation,0,0.0675224,"re variants in this group. We consider the following PR-based features: 7. Alignment-based features (ALG / ALGPR) One measure calculated over the aligned words between a candidate translation and the source (ALG); and two measures based on the comparison between these alignments for two different translations (e.g., candidate and pseudo-reference) and the source (ALGPR).3 3. Derived from A SIYA’s metrics (APR) Twenty-three PR features, including GTM-l (l∈{1,2,3}) to reward different length matching (Melamed et al., 2003), four variants of ROUGE (-L, -S*, -SU* and -W) (Lin and Och, 2004), WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER, and TERbase (i.e., without stemming, synonymy look-up, nor paraphrase support) (Snover et al., 2009), and all the shallow and full parsing measures (i.e., constituency and dependency parsing, PoS, chunking and lemmas) that A SIYA provides either for Spanish or English as target languages. 8. Length model (LeM) A measure to estimate the quality likelihood of a candidate sentence by considering the “expected length” of a proper translation from the source. The measure was introduced by (Pouliquen et al., 2003) to identify document translations. We estimated it"
W14-3351,W07-0738,1,0.886748,"Missing"
W14-3351,P91-1022,0,0.304658,"text against the translations. The metrics can be divided into two subsets: those that do not require any external resources (Section 3.1) and those that depend on a parallel corpus (Section 3.2). 3.1 The length factor (LeM) is rooted in the fact that the length of a text and its translation tend to preserve a certain length correlation. For instance, translations from English into Spanish or French tend to be longer than their source. Similar measures were proposed during the statistical machine translation early days, both considering characterand word-level lengths (Gale and Church, 1993; Brown et al., 1991). Pouliquen et al. (2003) defines the length factor as: Language-Independent Resource-Free Metrics We opted for two characterisations that allow for the comparison of texts across languages without external resources nor language-related knowledge —as far as the languages use the same writing system.4 The first characterisation is character n-grams; proposed by McNamee and Mayfield (2004) for cross-language information retrieval between European languages. Texts are broken down into overlapping character sequences of length n, with 1-character shifting. We opt for case-folded bigrams (NGRAM-co"
W14-3351,candito-etal-2010-statistical,0,0.0198857,"may make unfair comparisons when they are not able to reflect 2 Reference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gi"
W14-3351,P12-3024,1,0.897396,"Missing"
W14-3351,C10-2013,0,0.0192448,"may make unfair comparisons when they are not able to reflect 2 Reference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gi"
W14-3351,P13-4031,1,0.877708,"Missing"
W14-3351,P05-1022,0,0.0280345,"for each part of speech. We also use NIST (Doddington, 2002) to compute accumulated scores over sequences of n = 1..5 parts of speech (SP-pNIST). Similarly, CP metrics analyse similarities between constituent parse trees associated to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2.2 Explicit-Semantics Metric Additionally, we borrowed a metric originally proposed in the field of Information Retrieval: explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007). ESA is a similarity metric that relies on a large corpus of general knowledge to represent texts. Our knowledge corpora are composed of ∼ 100K Wikipedia articles from 2010 for the following target languages: English, French and German. In this case, ref and cand translations are both mapped onto the Wikipedia collection W . The similarities between each text and every article a ∈ W are computed on the"
W14-3351,W11-2107,0,0.0387306,"that transliteration is a good short-cut when dealing with different writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light version"
W14-3351,W05-0904,0,0.179329,"al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion of matching category-chains of length 3. Parsing-based Metrics Our initial set of parsing-based metrics is a followup of the proposal by Gim´enez and M`arquez (2010b): it leverages the structural information provided by linguistic processors to compute several similarity cues between two analyzed sentences. A SIYA includes plenty of metrics that capture syntactic and semantic aspects of a translation. New metrics based on linguistic structural information for French and German and upgraded versions of the parsers for English and Spanis"
W14-3351,W13-2202,0,0.0317518,"the alignments src–cand and src–ref (ALGNr); and (iii) the ratio of shared alignments between src– cand and src–ref (ALGNp). Parallel-Corpus Metrics We consider two metrics that make use of parallel corpora: length factor and alignment. 4 Experimental Results The tuning and selection of the different metrics to build UPC-IPA and UPC-STOUT was 4 Previous research showed that transliteration is a good short-cut when dealing with different writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those result"
W14-3351,W14-3336,0,0.026961,"ain over the baseline, the SEM ones do not. Finally, it merits some attention the good results achieved by the baseline for translations into English. We may remark here that our baseline included also the best performing state-of-the-art metrics, including all the variants of METEOR, that reported good results in the WMT13 challenge. Tables 6 and 7 show the official results obtained by UPC-IPA and UPC-STOUT in WMT14.8 The best and worst figures for each language pair are included for comparison —the worst performing submission at segment level is neglected as it seems to be a dummy (Mach´acˇ ek and Bojar, 2014 to appear). Both UPC-IPA and UPC-STOUT configurations resulted in different performances depending on the language pair. UPC-STOUT scored above the average for all the language pairs except for en–cs at both system and segment level, and en–ru at system level. Although the evaluation results are not directly comparable to the WMT13 ones, one can note that the results were notably better for pairs that involved Czech and Russian, and worse for those that involved French and German at system level. Analysing the impact of the evaluation methods and building comparable results in order to addres"
W14-3351,N03-2021,0,0.0556188,"ort-cut when dealing with different writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. The"
W14-3351,niessen-etal-2000-evaluation,0,0.0602984,"e.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. They were composed following different criteria, depending on the translation direction. Parsing-based"
W14-3351,N07-1051,0,0.00964749,"ference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion"
W14-3351,P06-1055,0,0.00957742,"able to reflect 2 Reference-based Metrics We recently added a new set of metrics to A SIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met1 http://asiya.lsi.upc.edu 394 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 2.1 Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-"
W14-3351,1992.tmi-1.7,0,0.718175,"Missing"
W14-3351,W09-0441,0,0.105035,"ent writing systems (Barr´on-Cede˜no et al., 2014). 5 396 https://code.google.com/p/berkeleyaligner conducted considering the WMT13 Metrics Task dataset (Mach´acˇ ek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the A SIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. They were composed following diffe"
W14-3351,J93-1004,0,\N,Missing
W14-3351,W09-0440,1,\N,Missing
W14-3351,W13-2201,0,\N,Missing
W14-3352,E06-1032,0,0.104635,"translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to the combination other metrics fr"
W14-3352,W07-0734,0,0.111099,"Missing"
W14-3352,D08-1024,0,0.0117943,"se years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous"
W14-3352,W14-3336,0,0.123967,"Missing"
W14-3352,2003.mtsummit-papers.9,0,0.0555949,"part raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to th"
W14-3352,P07-1098,0,0.0527917,"words in an Elementary Discourse Unit (EDU) are grouped under a predefined tag EDU, to which the nuclearity status of the EDU is attached: nucleus vs. satellite. Coherence relations, such as Attribution, Elaboration, and Enablement, between adjacent text spans constitute the internal nodes of the tree. Like the EDUs, the nuclearity statuses of the larger discourse units are attached to the relation labels. Notice that with this representation the tree kernel can easily be extended to find subtree matches at the word level, i.e., by including an additional layer of dummy leaves as was done in (Moschitti et al., 2007). We applied the same solution in our representations. Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolutio"
W14-3352,2003.mtsummit-papers.10,0,0.311329,"trics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Ne"
W14-3352,2012.amta-papers.6,0,0.0421525,"@qf.org.qa Abstract Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design bette"
W14-3352,P03-1021,0,0.00737295,"orms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Associatio"
W14-3352,P02-1040,0,0.100492,"nally, we add other metrics from the A SIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mea"
W14-3352,2006.amta-papers.25,0,0.163958,"ent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that are based on new representations of the DTs. In the remainder of this section, we will focus on the individual DT representations that"
W14-3352,P14-1065,1,0.494914,"Missing"
W14-3352,D07-1080,0,0.0139657,"hat participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, w"
W14-3352,D11-1125,0,0.0213356,"segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that"
W14-3352,D12-1083,1,\N,Missing
W14-4015,P13-4033,1,0.886979,"on of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between wo"
W14-4015,P07-2045,0,0.00478793,"arding their accuracy when trying to predict related words (Section 3.1) and also regarding its possible effect within a translation system (Section 3.2). In both cases one observes that the quality of the translation and alignments previous to building the semantic models are bottlenecks for the final performance: part of the vocabulary, and therefore translation pairs, are lost in the training process. Future work includes studying different kinds of alignment heuristics. We plan to develop new features based on the semantic models to use them inside state-of-the-art SMT systems like Moses (Koehn et al., 2007) or discourse-oriented decoders like Docent (Hardmeier et al., 2013). 3.2 Cross-Lingual Lexical Substitution Another way to evaluate the semantic models is through the effect they have in translation. We implemented the Cross-Lingual Lexical Substitution task carried out in SemEval-2010 (Task2, 2010) 133 References Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based document-level statistical machine translation. In Proc. of the 2011 Conference on Empirical Methods in NLP, pages 909–919, UK. C. Hardmeier, J. Nivre, and J. Tiedemann. 2012. Document-wide decoding for phrase-based statistical machi"
W14-4015,W09-2412,0,0.0379861,"Missing"
W14-4015,tiedemann-2012-parallel,1,0.865268,"Missing"
W14-4015,2011.mtsummit-papers.13,0,0.179084,"erence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to ha"
W14-4015,D11-1084,0,\N,Missing
W14-4015,D12-1108,1,\N,Missing
W15-3402,W06-2810,0,0.207288,"Missing"
W15-3402,cui-etal-2008-corpus,0,0.201057,"rpus using IR techniques. They use the characteristic vocabulary of the domain (100 terms extracted from an external in-domain corpus) to query a Lucene search engine4 over the whole encyclopædia. Our approach is completely different: we try to get along with Wikipedia’s structure with a strategy to walk through the category graph departing from a root or pseudo-root category, which defines our domain of interest. We empirically set a threshold to stop exploring the graph such that the included categories most likely represent an entire domain (cf. Section 3). This approach is more similar to Cui et al. (2008), who explore the Wiki-Graph and score every category in order to assess its likelihood of belonging to the domain. Other tools are being developed to extract corpora from Wikipedia. Linguatools5 released a comparable corpus extracted from Wikipedias in 253 language pairs. Unfortunately, neither their tool nor the applied methodology description are available. CatScan26 is a tool that allows to explore and search categories recursively. The Accurat toolkit (Pinnis et al., 2012; S¸tef˘anescu, Dan and Ion, Radu and Hunsicker, Sabine, 2012)7 aligns comparable documents and extracts parallel sente"
W15-3402,W04-3208,0,0.0142898,"for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background Sport Comparability in multilingual corpora is a fuzzy concept that has received alternative definitions without reaching an overall consensus (Rapp, 1995; Eagles Document Eag–Tcwg–Ctyp, 1996; Fung, 1998; Fung and Cheung, 2004; Wu and Fung, 2005; McEnery and Xiao, 2007; Sharoff et al., 2013). Ideally, a comparable corpus should contain texts in multiple languages which are similar in terms of form and content. Regarding content, they should observe similar structure, function, and a long list of characteristics: register, field, tenor, mode, time, and dialect (Maia, 2003). Nevertheless, finding these characteristics in real-life data collections is virtually impossible. Therefore, we attach to the following simpler four-class classification (Skadin¸a et al., 2010): (i) Parallel texts are true and accurate translati"
W15-3402,W95-0114,0,0.0936815,"tatistical machine translation engines for specific domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some othe"
W15-3402,P02-1040,0,0.0936058,"rallel corpora used to train the SMT systems (top rows) and of the sets used for development and test. CS Sc Sp Un Comp. Europarl 27.99 34.00 30.02 30.63 – c3g cog monoen ¯ S·len union 38.81 57.32 54.27 56.14 64.65 40.53 56.17 52.96 57.40 62.95 46.94 57.60 55.74 58.39 62.65 43.68 58.14 55.17 58.80 64.47 43.68 54.89 52.45 56.78 – Table 8: BLEU scores obtained on the Wikipedia test sets for the 20 specialised systems described in Section 5. A comparison column (Comp.) where all the systems are trained with corpora of the same size is also included (see text). against the BLEU evaluation metric (Papineni et al., 2002). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering. (i) Training systems with Wikipedia or Europarl for domain-specific translation. Table 8 shows the evaluation results on WPtest. All the specialised systems obtain significant improvements with respect to the Europarl system, regardless of their size. For instance, the worst specialised system (c3g with only 95,715 sentences for CS) outperforms by more than 10 points of BLEU the general Europarl translator. The mos"
W15-3402,W11-1212,0,0.211252,"ction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section we describe our proposal to extract domain-specific comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The terminology used in this description is as follows. Let c be a Wikipedia category and c∗ be the top category of a domain. Let a be a Wikipedia article; a ∈ c if a contains c among its categories. Let G be the Wikipedia category graph. Vocabulary definition. The domain vocabula"
W15-3402,2005.mtsummit-papers.11,0,0.108055,"to be used for training SMT systems. Some standard parallel corpora have the same order of magnitude. For tasks other than MT, where the precision on the extracted pairs can be more important than the recall, one can obtain cleaner corpora by using a threshold that maximises precision instead of F1 . Evaluation: Statistical Machine Translation Task In this section we validate the quality of the obtained corpora by studying its impact on statistical machine translation. There are several parallel corpora for the English–Spanish language pair. We select as a general-purpose corpus Europarl v7 (Koehn, 2005), with 1.97M parallel sentences. The order of magnitude is similar to the largest corpus we have extracted from Wikipedia, so we can compare the results in a size-independent way. If our corpus extracted from Wikipedia was made up with parallel fragments of the desired domain, it should be the most adequate to translate these domains. If the quality of the parallel fragments was acceptable, it should also help when translating out-of-domain texts. In order to test these hypotheses we analyse three settings: (i) train SMT systems only with Wikipedia (WP) or Europarl (EP) to translate domain-spe"
W15-3402,J10-4005,0,0.0131287,"n Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background"
W15-3402,J03-1002,0,0.00628859,"eliminate duplicates from this corpus, the size of the union is close to the sum of the individual corpora. This indicates that every similarity measure selects a different set of parallel fragments. Beside the specialised corpus for each domain, we build a larger corpus with all the data (Un). Again, duplicate fragments coming from articles belonging to more than one domain are removed. SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with Moses (Koehn et al., 2007). We optimise the feature weights of the model with Minimum Error Rate Training (MERT) (Och, 2003) c3g cog monoen ¯ S·len union WPdev WPtest GNOME CS Sc Sp Un 95,715 182,283 210,664 120,835 577,428 723,760 1,213,965 1,367,169 956,346 3,847,381 334,828 451,324 461,237 389,975 1,181,664 883,366 1,430,962 1,638,777 1,160,977 4,948,241 300 500 1000 300 500 – 300 500 – 900 1500 – Table 7: Number of sentences of the Wikipedia parallel corpora used to train the SMT systems (top rows) and of the sets used for development"
W15-3402,P11-1133,0,0.0249799,"nally, the most related tool to ours: CorpusPedia8 extracts non-aligned, softly-aligned, and strongly-aligned comparable corpora from Wikipedia (Otero and L´opez, 2010). The difference with respect to our model is that they only consider the articles associated to one specific category and not to an entire domain. The inter-connection among Wikipedia editions in different languages has been exploited for multiple tasks including lexicon induction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section"
W15-3402,P11-2000,0,0.224925,"Missing"
W15-3402,P95-1050,0,0.107949,"m to train statistical machine translation engines for specific domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available;"
W15-3402,C10-1124,0,0.028803,"Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of"
W15-3402,1992.tmi-1.7,0,0.831818,"Missing"
W15-3402,I05-1023,0,0.0315766,"nguage pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background Sport Comparability in multilingual corpora is a fuzzy concept that has received alternative definitions without reaching an overall consensus (Rapp, 1995; Eagles Document Eag–Tcwg–Ctyp, 1996; Fung, 1998; Fung and Cheung, 2004; Wu and Fung, 2005; McEnery and Xiao, 2007; Sharoff et al., 2013). Ideally, a comparable corpus should contain texts in multiple languages which are similar in terms of form and content. Regarding content, they should observe similar structure, function, and a long list of characteristics: register, field, tenor, mode, time, and dialect (Maia, 2003). Nevertheless, finding these characteristics in real-life data collections is virtually impossible. Therefore, we attach to the following simpler four-class classification (Skadin¸a et al., 2010): (i) Parallel texts are true and accurate translations or approximate"
W15-3402,2009.mtsummit-posters.26,0,0.0424596,"comparable documents and extracts parallel sentences, lexicons, and named entities. Finally, the most related tool to ours: CorpusPedia8 extracts non-aligned, softly-aligned, and strongly-aligned comparable corpora from Wikipedia (Otero and L´opez, 2010). The difference with respect to our model is that they only consider the articles associated to one specific category and not to an entire domain. The inter-connection among Wikipedia editions in different languages has been exploited for multiple tasks including lexicon induction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 200"
W15-3402,N10-1063,0,0.0207021,"ual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section we describe our proposal to extract domain-specific comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The terminology used in this description is as follows. Let c be a Wikipedia category and c∗ be the top category of a domain. Let a be a Wikipedia article; a ∈ c if a contains c among its categories. Let G be the Wikipedia category graph. Vocabulary definition. The domain vocabulary represents the set of terms that better c"
W15-3402,zesch-etal-2008-extracting,0,0.105477,"Missing"
W15-3402,2012.eamt-1.37,0,0.108348,"Missing"
W15-3402,P07-2045,0,\N,Missing
W15-3402,P12-3016,0,\N,Missing
W15-3402,tiedemann-2012-parallel,0,\N,Missing
W15-3402,P03-1021,0,\N,Missing
W15-4908,W14-4012,0,0.0416542,"Missing"
W15-4908,P14-1129,0,0.0180384,"there are some works that try to use vector models trained using recurrent neural networks (RNN) to improve decoder outputs. For instance, in (Sundermeyer et al., 2014) they build two kinds of models at word level, one based on word alignments and other one phrase-based. The authors train RNNs to obtain their models and they use them to rerank n-best lists after decoding. They report improvements in BLEU and TER scores in several language pairs, but they are not worried about context issues of a document although they do take into account both sides of the translation: source and target. In (Devlin et al., 2014) they also present joint models that augment the NNLM with a source context window to introduce a new decoding feature. They ﬁnally present improvements in BLEU score for Arabic-English language pair and show a new technique to introduce this kind of models inside MT systems in a computationally efﬁcient way. These two last works prove the power of applying NN models as features inside MT systems. 3 Training monolingual and bilingual semantic models As we explained before, there are several works that use monolingual WVM as language models, 61 or the composition of monoligual models to build b"
W15-4908,D11-1084,0,0.587373,"ion in order to maintain the characteristics of the discourse. The evolution of the topic through a text is also an important feature to preserve. All these aspects can be used to improve the translation quality by trying to assure coherence throughout a document. Several recent works go on that direction. Some of them present postprocessing approaches making changes into a ﬁrst translation according to document-level information (Mart´ınez-Garcia et al., 2014a; Xiao et al., 2011). Others introduce the information within the decoder, by, for instance, implementing a topicbased cache approach (Gong et al., 2011; Xiong et al., 2015). The decoding methodology itself can be changed. This is the case of a document-oriented decoder, Docent (Hardmeier et al., 2013), which implements a search in the space of translations of a whole document. This framework allows us to consider features that apply at document level. One of the main goals of this paper is to take advantage of this capability to include semantic information at decoding time. We present here the usage of a semantic representation based on word embeddings as a language model within a document-oriented decoder. To do this, we trained a word vec"
W15-4908,P12-3024,1,0.877684,"Missing"
W15-4908,2010.iwslt-papers.10,0,0.051944,"he paper is organized as follows. A brief revision of the related work is done in Section 2. In Section 3, we describe our approach of using a bilingual word vector model as a language model. The model is compared to monolingual models and evaluated. We show and discuss the results of our experiments on the full translation task in Section 5. Finally, we draw the conclusions and deﬁne several lines of future work in Section 6. 2 Related Work In the last years, approaches to document-level translation have started to emerge. The earliest ones deal with pronominal anaphora within an SMT system (Hardmeier and Federico, 2010; Nagard and Koehn, 2010). These authors develop models that, with the help of coreference resolution methods, identify links among words in a text and use them for a better translation of pronouns. More recent approaches focus on topic cohesion. (Gong et al., 2011) tackle the problem by making available to the decoder the previous translations at decoding time using a cache system. In this way, one can bias the system towards the lexicon already used. (Xiong et al., 2015) also present a topic-based coherence improvement for an SMT system by trying to preserve the continuity of sentence topics"
W15-4908,W10-1737,0,0.0719471,"Missing"
W15-4908,D12-1108,0,0.211389,"Missing"
W15-4908,J03-1002,0,0.00952259,"α is the proportion of content words in the training corpus and � is a small ﬁxed probability, as described in (Hardmeier, 2014). The assumption is the same here as before, the better the choice, the closer the context vector will be to the vector representation of the evaluated word. The ﬁnal score for a document translation candidate is an average of the scores of its words. 5.2 Experimental Settings Our SMT baseline system is based on Moses. The translation system has been trained with the Europarl corpus in its version 7 for the Spanish– English language pair. We used the GIZA++ software (Och and Ney, 2003) to do the word 63 alignments. The language model is an interpolation of several 5-gram language models obtained using SRILM (Stolcke, 2002) with interpolated Kneser-Ney discounting on the target side of the Europarl corpus v7; United Nations; NewsCommentary 2007, 2008, 2009 and 2010; AFP, APW and Xinhua corpora as given by (Specia et al., 2013)3 The optimization of the weights is done with MERT (Och, 2003) against the BLEU measure on the NewsCommentary corpus of 2009. As in the previous section, our experiments are carried out over the NewsCommentary-2011 test set. We chose the newswire docum"
W15-4908,P13-4033,0,0.684855,". All these aspects can be used to improve the translation quality by trying to assure coherence throughout a document. Several recent works go on that direction. Some of them present postprocessing approaches making changes into a ﬁrst translation according to document-level information (Mart´ınez-Garcia et al., 2014a; Xiao et al., 2011). Others introduce the information within the decoder, by, for instance, implementing a topicbased cache approach (Gong et al., 2011; Xiong et al., 2015). The decoding methodology itself can be changed. This is the case of a document-oriented decoder, Docent (Hardmeier et al., 2013), which implements a search in the space of translations of a whole document. This framework allows us to consider features that apply at document level. One of the main goals of this paper is to take advantage of this capability to include semantic information at decoding time. We present here the usage of a semantic representation based on word embeddings as a language model within a document-oriented decoder. To do this, we trained a word vector model (WVM) using neural networks. As a ﬁrst approach, a monolingual model is used in analogy with the standard monolingual language models based o"
W15-4908,Y14-1004,0,0.363562,"ord vector models are more appropriate for the purpose of translation. The ﬁnal document-level translator incorporating the semantic model outperforms the basic Docent (without semantics) and also performs slightly over a standard sentencelevel SMT system in terms of ULC (the average of a set of standard automatic evaluation metrics for MT). Finally, we also present some manual analysis of the translations of some concrete documents. 1 Introduction Document-level information is usually lost during the translation process when using Statistical Machine Translation (SMT) sentence-based systems (Hardmeier, 2014; Webber, 2014). Cross-sentence dependencies are totally ignored, as they translate sentence by sentence without taking into account any document context when choosing the best translation. Some simple phenomena like c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 59 coreferent pronouns outside a sentence cannot be properly translated in this way, which is already important because the correct translation of pronouns in a document confers a high level of coherence to the ﬁnal translation. Also, discourse connective"
W15-4908,P15-1001,0,0.0369739,"Missing"
W15-4908,P07-2045,0,0.0140381,"ng to the distribution of the word over the target document; and ﬁnally, generate the new translation tak60 ing into account the results of the ﬁrst two steps. These approaches report improvements in the ﬁnal translations but, in most of them. the improvements can only be seen through a detailed manual evaluation. When using automatic evaluation metrics like BLEU (Papineni et al., 2002), differences are not signiﬁcant. A document-oriented SMT decoder is presented in (Hardmeier et al., 2012; Hardmeier et al., 2013). The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007). The authors present a stochastic local search decoding method for phrase-based SMT systems which allows decoding complete documents. Docent starts from an initial state (translation) given by Moses and this one is modiﬁed by the application of a hill climbing strategy to ﬁnd a (local) maximum of the score function. The score function and some deﬁned change operations are the ones encoding the document-level information. One remarkable characteristic of this decoder, besides the change of perspective in the implementation from sentence-level to document-level, is that it allows the usage of a"
W15-4908,W04-3250,0,0.406098,"Missing"
W15-4908,C14-1017,0,0.033399,"Missing"
W15-4908,W14-4015,1,0.864402,"Missing"
W15-4908,N13-1090,0,0.0309364,"c Space Language Model (SSLM). In this case, the decoder uses the information of the word vector model to evaluate the adequacy of a word inside a translation by calculating the distance among the current word and its context. In the last years, several distributed word representation models have been introduced. Furthermore, distributed models have been successfully applied to several different NLP tasks. These models are able to capture and combine the semantic information of the text. An efﬁcient implementation of the Context Bag of Words (CBOW) and the Skipgram algorithms is presented in (Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013d). Within this implementation WVMs are trained using a neural network. These models proved to be robust and powerful to predict semantic relations between words even across languages. They are implemented inside the word2vec software package. However, they are not able to handle lexical ambiguity as they conﬂate word senses of polysemous words into one common representation. This limitation is already discussed in (Mikolov et al., 2013b) and in (Wolf et al., 2014), in which bilingual extensions of the word2vec architecture are also proposed. These"
W15-4908,P03-1021,0,0.0418603,"line system is based on Moses. The translation system has been trained with the Europarl corpus in its version 7 for the Spanish– English language pair. We used the GIZA++ software (Och and Ney, 2003) to do the word 63 alignments. The language model is an interpolation of several 5-gram language models obtained using SRILM (Stolcke, 2002) with interpolated Kneser-Ney discounting on the target side of the Europarl corpus v7; United Nations; NewsCommentary 2007, 2008, 2009 and 2010; AFP, APW and Xinhua corpora as given by (Specia et al., 2013)3 The optimization of the weights is done with MERT (Och, 2003) against the BLEU measure on the NewsCommentary corpus of 2009. As in the previous section, our experiments are carried out over the NewsCommentary-2011 test set. We chose the newswire documents as test set because typically they are documents with high consistency and coherence. Regarding the document-level decoder, we use Docent. The ﬁrst step in the Docent translation process is the output of our Moses baseline system. We set the initial Docent weights to be the same as the ones obtained with MERT for the Moses baseline. Finally, the word vector models used in the experiments of this sectio"
W15-4908,P02-1040,0,0.0958032,"ays within a same document. The aim is to incorporate document contexts into an existing SMT system following 3 steps. First, they identify the ambiguous words; then, they obtain a set of consistent translations for each word according to the distribution of the word over the target document; and ﬁnally, generate the new translation tak60 ing into account the results of the ﬁrst two steps. These approaches report improvements in the ﬁnal translations but, in most of them. the improvements can only be seen through a detailed manual evaluation. When using automatic evaluation metrics like BLEU (Papineni et al., 2002), differences are not signiﬁcant. A document-oriented SMT decoder is presented in (Hardmeier et al., 2012; Hardmeier et al., 2013). The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007). The authors present a stochastic local search decoding method for phrase-based SMT systems which allows decoding complete documents. Docent starts from an initial state (translation) given by Moses and this one is modiﬁed by the application of a hill climbing strategy to ﬁnd a (local) maximum of the score function. The score function and some deﬁned change operation"
W15-4908,P13-4014,0,0.0225964,"Missing"
W15-4908,D14-1003,0,0.0175693,"s well as in other areas. In short, NMT systems are build over a trained neural network that is able to output a translation given a source text in the input (Sutskever et al., 2014b; Sutskever et al., 2013; Bahdanau et al., 2014; Cho et al., 2014). However, these systems report some problems when translating unknown or rare words. We are aware of only few works that try to address this problem (Sutskever et al., 2014a; Jean et al., 2014). Furthermore, there are some works that try to use vector models trained using recurrent neural networks (RNN) to improve decoder outputs. For instance, in (Sundermeyer et al., 2014) they build two kinds of models at word level, one based on word alignments and other one phrase-based. The authors train RNNs to obtain their models and they use them to rerank n-best lists after decoding. They report improvements in BLEU and TER scores in several language pairs, but they are not worried about context issues of a document although they do take into account both sides of the translation: source and target. In (Devlin et al., 2014) they also present joint models that augment the NNLM with a source context window to introduce a new decoding feature. They ﬁnally present improveme"
W15-4908,D13-1176,0,0.168547,"Missing"
W15-4908,P15-1002,0,0.0355122,"Missing"
W15-4908,tiedemann-2012-parallel,0,0.0147478,"neously the semantic information associated to the source word and the information in the target side of the translation. In this way, we hope to better capture the semantic information that is implicitly given by translating a text. To better characterize ambiguous words for MT, for instance, we expect to be able to distinguish among the different meanings that the word desk can have when translated in Spanish: desk|mesa vs. desk|mostrador vs. desk|escritorio. 3.2 Settings The training set for our models is built from parallel corpora in the English-Spanish language pair available in Opus 1 (Tiedemann, 2012; Tiedemann, 2009). These corpora have been automatically aligned and therefore contain the aligment information necessary to build our bilingual models. We chose the one-to-one alignments to avoid noise and duplicities in the ﬁnal data. Table 1 shows the size of the speciﬁc data used: EuropalV7, United Nations, Multilingual United Nations, and Subtitles-2012. Monolingual models are also build with these corpora and therefore are comparable in size. With this corpus, the ﬁnal training set has 584 million words for English and 759 for Spanish. 1 http://opus.lingﬁl.uu.se/ Training Development Te"
W15-4908,2011.mtsummit-papers.13,0,0.260151,"onnectives are valuable because they mark the ﬂow of the discourse in a text. It is desirable to transfer them to the output translation in order to maintain the characteristics of the discourse. The evolution of the topic through a text is also an important feature to preserve. All these aspects can be used to improve the translation quality by trying to assure coherence throughout a document. Several recent works go on that direction. Some of them present postprocessing approaches making changes into a ﬁrst translation according to document-level information (Mart´ınez-Garcia et al., 2014a; Xiao et al., 2011). Others introduce the information within the decoder, by, for instance, implementing a topicbased cache approach (Gong et al., 2011; Xiong et al., 2015). The decoding methodology itself can be changed. This is the case of a document-oriented decoder, Docent (Hardmeier et al., 2013), which implements a search in the space of translations of a whole document. This framework allows us to consider features that apply at document level. One of the main goals of this paper is to take advantage of this capability to include semantic information at decoding time. We present here the usage of a semant"
W99-0608,P98-1029,0,0.553502,"decision trees, neural networks, rule-induction systems, etc.). Several applications to real tasks have been performed, and, regarding NLP, we find ensembles of classifiers in context-sensitive spelling correction (Golding and Roth, 1999), text categorization (Schapire and Singer, 1998; Blum and Mitchell, 1998), and text filtering (Schapire et al., 1998). Combination of classitiers have also been applied to POS tagging. For instance, van Halteren (1996) combined a number of similar tuggers by way of a straightforward majority vote. More recently, two parallel works (van Halteren et al., 1998; Brill and Wu, 1998) combined, with a remarkable success, the output of a set of four tuggers based on different principles and feature modelling. Finally, in the work by MSxquez et al. (1998) the combination of taggers is used in a bootstrapping algorithm to train a part of speech tagger from a limited amount of training material. The aim of the present work is to improve an existing POS tagger based on decision trees (Mkrquez and Rodriguez, 1997), by using ensembles of classifiers. This tagger treats separately the different types (classes) of ambiguity by considering a different decision tree for each class. T"
W99-0608,J95-4004,0,0.184072,"the same size are extracted by randomly drawing, with replacement, n times. Such new training sets are called bootstrap replicates. In each replicate, some examples appear multiple times, while others do not appear. A classifier is induced from each bootstrap replicate and then they are combined in a voting approach. The technique is called bootstrap aggregation, from which the acronym bagging is derived. In our case, the bagging approach was performed following the description of Breiman (1996), constructing 10 replicates for each data set 5. class. Very similar to Brill's lexical patterns (Brill, 1995), we also have included features to capture collocational information. Such features are obtained by composition of the already described single attributes and they are sequences of contiguous words and/or POS tags (up to three items). The resulting features were grouped according to their specificity to generate ensembles of eight trees 6. The idea here is that specific information (lexical attributes and collocational patterns) would produce classifiers that cover concrete cases (hopefully, with a high precision), while more general information (POS tags) would produce more general (but prob"
W99-0608,A88-1019,0,0.0205079,"All words not present in the training corpus are considered unknown. In principle, we have to assume that they can take any tag corresponding to open categories (i.e., noun, proper noun, verb, adjective, adverb, cardinal, etc.), which sum up to 20 in the Penn Treebank tagset. In this approach, an additional ambiguity class for unknown words is considered, and so, they are treated exactly in the same way as the other ambiguous words, except by the type of information used for acquiring the trees, 2.2 STT: A Statistical T r e e - b a s e d Tagger The aim of statistical or probabilistic tagging (Church, 1988; Cutting et al., 1992) is to assign the most likely sequence of tags given the observed sequence of words. For doing so, two kinds of information are used: the lexical probabilities, i.e, the probability of a particular tag conditional on the particular word, and the contextual probabilities, which describe the probability of a particular tag conditional on the surrounding tags. Contextual (or transition) probabilities are usually reduced to the conditioning of the preceding tag (bigrams), or pair of tags (trigrams), however, the general formulation allows a broader definition of context. In"
W99-0608,A92-1018,0,0.0155164,"present in the training corpus are considered unknown. In principle, we have to assume that they can take any tag corresponding to open categories (i.e., noun, proper noun, verb, adjective, adverb, cardinal, etc.), which sum up to 20 in the Penn Treebank tagset. In this approach, an additional ambiguity class for unknown words is considered, and so, they are treated exactly in the same way as the other ambiguous words, except by the type of information used for acquiring the trees, 2.2 STT: A Statistical T r e e - b a s e d Tagger The aim of statistical or probabilistic tagging (Church, 1988; Cutting et al., 1992) is to assign the most likely sequence of tags given the observed sequence of words. For doing so, two kinds of information are used: the lexical probabilities, i.e, the probability of a particular tag conditional on the particular word, and the contextual probabilities, which describe the probability of a particular tag conditional on the surrounding tags. Contextual (or transition) probabilities are usually reduced to the conditioning of the preceding tag (bigrams), or pair of tags (trigrams), however, the general formulation allows a broader definition of context. In this way, the set of ac"
W99-0608,W96-0102,0,0.0592426,"Missing"
W99-0608,P98-1081,0,0.0754207,"Missing"
W99-0608,P97-1032,0,0.0248129,"Missing"
