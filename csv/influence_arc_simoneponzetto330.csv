2020.lrec-1.728,D18-1523,0,0.0164621,"t al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Induction The majority of word vector models do not discriminate between multiple senses of individual words. However, a polysemous word can be identified via manual analysis of its nearest neighbours—they reflect different senses of the word. Table 1 shows manually sense-labelled most similar terms to the word Ruby according to the pre-trained fastText model (Grave et al., 2018). As it was suggested early by Widdows and Dorow (2002), the distributional properties"
2020.lrec-1.728,P18-1001,0,0.0606807,"Missing"
2020.lrec-1.728,L18-1618,0,0.022236,"the most appropriate sense (labelled by the centroid word of a corresponding cluster). by Ustalov et al. (2018), extending it with a back-end for multiple languages, language detection, and sense browsing capabilities. 5. Evaluation We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task. 5.1. Lexical Similarity and Relatedness Experimental Setup We use the SemR-11 datasets4 (Barzegar et al., 2018), which contain word pairs with manually assigned similarity scores from 0 (words are not related) to 10 (words are fully interchangeable) for 12 languages: English (en), Arabic (ar), German (de), Spanish (es), Farsi (fa), French (fr), Italian (it), Dutch (nl), Portuguese (pt), Russian (ru), Swedish (sv), Chinese (zh). The task is to assign relatedness scores to these pairs so that the ranking of the pairs by this score is close to the ranking defined by the oracle score. The performance is measured with Pearson correlation of the rankings. Since one word can have several different senses in o"
2020.lrec-1.728,W06-3812,1,0.364448,"ess algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Ind"
2020.lrec-1.728,Q17-1010,0,0.301237,"ontexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their contexts (Athiwaratkun et al., 2018; Jain et al., 2019). Unfortunately, many widespread word embedding models, such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017), do not handle polysemous words. Words in these models are represented with single vectors, which were constructed from diverse sets of contexts corresponding to different senses. In such cases, their disam? Currently at Yandex. biguation needs knowledge-rich approaches. We tackle this problem by suggesting a method of posthoc unsupervised WSD. It does not require any external knowledge and can separate different senses of a polysemous word using only the information encoded in pretrained word embeddings. We construct a semantic similarity graph for words and partition it into densely connect"
2020.lrec-1.728,N19-1423,0,0.00893264,"ification problem. Knowledge-based approaches construct sense embeddings, i.e. embeddings that separate various word senses. SupWSD (Papandrea et al., 2017) is a state-of-the-art system for supervised WSD. It makes use of linear classifiers and a number of features such as POS tags, surrounding words, local collocations, word embeddings, and syntactic relations. GlossBERT model (Huang et al., 2019), which is another implementation of supervised WSD, achieves a significant improvement by leveraging gloss information. This model benefits from sentence-pair classification approach, introduced by Devlin et al. (2019) in their BERT contextualized embedding model. The input to the model consists of a context (a sentence which contains an ambiguous word) and a gloss (sense definition) from WordNet. The contextgloss pair is concatenated through a special token ([SEP]) and classified as positive or negative. On the other hand, sense embeddings are an alternative to traditional word vector models such as word2vec, fastText or GloVe, which represent monosemous words well but fail for ambiguous words. Sense embeddings represent individual senses of polysemous words as separate vectors. They can be linked to an ex"
2020.lrec-1.728,L18-1550,0,0.16799,"pment of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al. (2018), enabling WSD in these languages. Models and system are available online. Keywords: word sense induction, word sense disambiguation, word embeddings, sense embeddings, graph clustering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning m"
2020.lrec-1.728,S13-2113,0,0.0199433,"s senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously descr"
2020.lrec-1.728,D19-1355,0,0.0605952,"Missing"
2020.lrec-1.728,P19-1165,0,0.0213523,"(a sentence which contains an ambiguous word) and a gloss (sense definition) from WordNet. The contextgloss pair is concatenated through a special token ([SEP]) and classified as positive or negative. On the other hand, sense embeddings are an alternative to traditional word vector models such as word2vec, fastText or GloVe, which represent monosemous words well but fail for ambiguous words. Sense embeddings represent individual senses of polysemous words as separate vectors. They can be linked to an explicit inventory (Iacobacci et al., 2015) or induce a sense inventory from unlabelled data (Iacobacci and Navigli, 2019). LSTMEmbed (Iacobacci and Navigli, 2019) aims at learning sense embeddings linked to BabelNet (Navigli and Ponzetto, 2012), at the same time handling word ordering, and using pre-trained embeddings as an objective. Although it was tested only on English, the approach can be easily adapted to other languages present in BabelNet. However, manually labelled datasets as well as knowledge bases exist only for a small number of wellresourced languages. Thus, to disambiguate polysemous words in other languages one has to resort to fully unsupervised techniques. The task of Word Sense Induction (WSI)"
2020.lrec-1.728,P15-1010,0,0.0808199,"Missing"
2020.lrec-1.728,W19-7405,0,0.01615,"lysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their contexts (Athiwaratkun et al., 2018; Jain et al., 2019). Unfortunately, many widespread word embedding models, such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017), do not handle polysemous words. Words in these models are represented with single vectors, which were constructed from diverse sets of contexts corresponding to different senses. In such cases, their disam? Currently at Yandex. biguation needs knowledge-rich approaches. We tackle this problem by suggesting a method of posthoc unsupervised WSD. It does not require any external knowledge and can separate different senses of a polyse"
2020.lrec-1.728,S13-2049,0,0.151837,"ge in Pearson correlation score when switching from the baseline fastText embeddings to our sense vectors. The new vectors significantly improve the relatedness detection for German, Farsi, Russian, and Chinese, whereas for Italian, Dutch, and Swedish the score slightly falls behind the baseline. For other languages, the performance of sense vectors is on par with regular fastText. 5.2. Word Sense Disambiguation The purpose of our sense vectors is disambiguation of polysemous words. Therefore, we test the inventories constructed with egvi on the Task 13 of SemEval-2013 — Word Sense Induction (Jurgens and Klapaftis, 2013). The task is to identify the different senses of a target word in context in a fully unsupervised manner. 5947 lar to state-of-the-art word sense disambiguation and word sense induction models. In particular, we can see that it outperforms SenseGram on the majority of metrics. We should note that this comparison is not fully rigorous, because SenseGram induces sense inventories from word2vec as opposed to fastText vectors used in our work. 5.3. Figure 3: Absolute improvement of Pearson correlation scores of our embeddings compared to fastText. This is the averaged difference of the scores for"
2020.lrec-1.728,D15-1200,0,0.0219916,"ems, Rubyist, Rubyists, Rubys, Sadie, Sapphire, Sypro, Violet, jRuby, ruby, rubyists Table 1: Top nearest neighbours of the fastText vector of the word Ruby are clustered according to various senses of this word: programming language, gem, first name, color, but also its spelling variations (typeset in black color). vectors. Here, the definition of context may vary from window-based context to latent topic-alike context. Afterwards, the resulting clusters are either used as senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Ke"
2020.lrec-1.728,W14-0121,0,0.0241505,"= K. arated into two clusters. Interestingly, fastText handles typos, code-switching, and emojis by correctly associating all non-standard variants to the word they refer, and our method is able to cluster them appropriately. Both inventories were produced with K = 200, which ensures stronger connectivity of graph. However, we see that this setting still produces too many clusters. We computed the average numbers of clusters produced by our model with K = 200 for words from the word relatedness datasets and compared these numbers with the number of senses in WordNet for English and RuWordNet (Loukachevitch and Dobrov, 2014) for Russian (see Table 3). We can see that the number of senses extracted by our method is consistently higher than the real number of senses. We also compute the average number of senses per word for all the languages and different values of K (see Figure 5). The average across languages does not change much as we increase K. However, for larger K the average exceed the median value, indicating that more languages have lower number of senses per word. At the same time, while at smaller K the maximum average number of senses per word does not exceed 6, larger values of K produce outliers, e.g"
2020.lrec-1.728,C14-3003,0,0.031943,"ddings, graph clustering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning meaning to a word in context. The majority of approaches to WSD are based on the use of knowledge bases, taxonomies, and other external manually built resources (Moro et al., 2014; Upadhyay et al., 2018). However, different senses of a polysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vecto"
2020.lrec-1.728,D17-2016,1,0.833565,"5 clustered, in the method presented in this section we sparsify the graph by removing 13 nodes which were not in the set of the “anti-edges” i.e. pairs of most dissimilar terms out of these 50 neighbours. Examples of anti-edges i.e. pairs of most dissimilar terms for this graph include: (Haskell, Sapphire), (Garnet, Rails), (Opal, Rubyist), (Hazel, RubyOnRails), and (Coffeescript, Opal). Labelling of Induced Senses We label each word cluster representing a sense to make them and the WSD results interpretable by humans. Prior systems used hypernyms to label the clusters (Ruppert et al., 2015; Panchenko et al., 2017), e.g. “animal” in the “python (animal)”. However, neither hypernyms nor rules for their automatic extraction are available for all 158 languages. Therefore, we use a simpler method to select a keyword which would help to interpret each cluster. For each graph node v ∈ V we count the number of anti-edges it belongs to: count(v) = |{(wi , wi ) : (wi , wi ) ∈ E ∧ (v = wi ∨ v = wi )}|. A graph clustering yields a partition of V into n clusters: V = {V1 , V2 , ..., Vn }. For each cluster Vi we define a keyword wikey as the word with the largest number of antiedges count(·) among words in this clus"
2020.lrec-1.728,D17-2018,0,0.0605054,"Missing"
2020.lrec-1.728,W16-1620,1,0.576845,"rwards, the resulting clusters are either used as senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and G"
2020.lrec-1.728,D14-1162,0,0.0837391,"Missing"
2020.lrec-1.728,L18-1167,1,0.8605,"pairs with manually assigned similarity scores from 0 (words are not related) to 10 (words are fully interchangeable) for 12 languages: English (en), Arabic (ar), German (de), Spanish (es), Farsi (fa), French (fr), Italian (it), Dutch (nl), Portuguese (pt), Russian (ru), Swedish (sv), Chinese (zh). The task is to assign relatedness scores to these pairs so that the ranking of the pairs by this score is close to the ranking defined by the oracle score. The performance is measured with Pearson correlation of the rankings. Since one word can have several different senses in our setup, we follow Remus and Biemann (2018) and define the relatedness score for a pair of words as the maximum cosine similarity between any of their sense vectors. We extract the sense inventories from fastText embedding vectors. We set N = K for all our experiments, i.e. the number of vertices in the graph and the maximum number of vertices’ nearest neighbours match. We conduct experiments with N = K set to 50, 100, and 200. For each cluster Vi we create a sense vector si by averaging vectors that belong to this cluster. We rely on the methodology of (Remus and Biemann, 2018) shifting the generated sense vector to the direction of t"
2020.lrec-1.728,P15-4018,1,0.847544,"e all 50 terms are 5945 clustered, in the method presented in this section we sparsify the graph by removing 13 nodes which were not in the set of the “anti-edges” i.e. pairs of most dissimilar terms out of these 50 neighbours. Examples of anti-edges i.e. pairs of most dissimilar terms for this graph include: (Haskell, Sapphire), (Garnet, Rails), (Opal, Rubyist), (Hazel, RubyOnRails), and (Coffeescript, Opal). Labelling of Induced Senses We label each word cluster representing a sense to make them and the WSD results interpretable by humans. Prior systems used hypernyms to label the clusters (Ruppert et al., 2015; Panchenko et al., 2017), e.g. “animal” in the “python (animal)”. However, neither hypernyms nor rules for their automatic extraction are available for all 158 languages. Therefore, we use a simpler method to select a keyword which would help to interpret each cluster. For each graph node v ∈ V we count the number of anti-edges it belongs to: count(v) = |{(wi , wi ) : (wi , wi ) ∈ E ∧ (v = wi ∨ v = wi )}|. A graph clustering yields a partition of V into n clusters: V = {V1 , V2 , ..., Vn }. For each cluster Vi we define a keyword wikey as the word with the largest number of antiedges count(·)"
2020.lrec-1.728,I05-3027,0,0.0267775,"rs of GPU-accelerated computations. We release the constructed sense inventories for all the available languages. They contain all the necessary information for using them in the proposed WSD system or in other downstream tasks. 4.2. Word Sense Disambiguation System The first text pre-processing step is language identification, for which we use the fastText language identification models by Bojanowski et al. (2017). Then the input is tokenised. For languages which use Latin, Cyrillic, Hebrew, or Greek scripts, we employ the Europarl tokeniser.2 For Chinese, we use the Stanford Word Segmenter (Tseng et al., 2005). For Japanese, we use Mecab (Kudo, 2006). We tokenise Vietnamese with UETsegmenter (Nguyen and Le, 2016). All other languages are processed with the ICU tokeniser, as implemented in the PyICU project.3 After the tokenisation, the system analyses all the input words with pre-extracted sense inventories and defines the most appropriate sense for polysemous words. Figure 2 shows the interface of the system. It has a textual input form. The automatically identified language of text is shown above. A click on any of the words displays a prompt (shown in black) with the most appropriate sense of a"
2020.lrec-1.728,D18-1270,0,0.0263365,"ering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning meaning to a word in context. The majority of approaches to WSD are based on the use of knowledge bases, taxonomies, and other external manually built resources (Moro et al., 2014; Upadhyay et al., 2018). However, different senses of a polysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their co"
2020.lrec-1.728,L18-1164,1,0.90455,"Missing"
2020.lrec-1.728,C02-1114,0,0.018722,"tained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Induction The majority of word vector models do not discriminate between multiple senses of individual words. However, a polysemous word can be identified via manual analysis of its nearest neighbours—they reflect different senses of the word. Table 1 shows manually sense-labelled most similar terms to the word Ruby according to the pre-trained fastText model (Grave et al., 2018). As it was suggested early by Widdows and Dorow (2002), the distributional properties of a word can be used to construct a graph of words that are semantically related to it, and if a word is polysemous, such graph can easily be partitioned into a number of densely connected subgraphs corresponding to different senses of this word. Our algorithm is based on the same principle. 5944 3.1. SenseGram: A Baseline Graph-based Word Sense Induction Algorithm nodes – those which should not be connected: E = {(w1 , w1 ), (w2 , w2 ), ..., (wN , wN )}. To clarify this, consider the target (ego) word w = python, its top similar term w1 = Java and the resultin"
2020.lrec-1.728,J19-3002,1,\N,Missing
2020.semeval-1.2,P18-1073,0,0.0246037,"traints in target languages by translating EN constraints to target languages via Google Translate. A similar approach of automatic constraint translation has already been proven very effective in the context of symmetric similarity-based specialization of embedding spaces for low-resource languages (Ponti et al., 2019). This way, Wang et al. (2020) obtain an LE-specialized embedding space for each language. Following that, in the second step, they learn a linear projection mapping between the LE-specialized monolingual spaces with the VecMap tool for inducing bilingual word embedding spaces (Artetxe et al., 2018). Recent comparative evaluations (Glavaˇs et al., 2019; Vuli´c et al., 2019a) rendered VecMap as one of the most robust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similari"
2020.semeval-1.2,I13-1095,0,0.0123381,"-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is"
2020.semeval-1.2,Q17-1010,0,0.00755369,"bust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similarity between distributional word vectors as a baseline. To this end, we use the 300-dimensional FastText embeddings (Bojanowski et al., 2017) trained on Wikipedias of respective languages.6 For the cross-lingual (sub)tasks we induce the bilingual embedding spaces via the simple Procrustes alignment (Smith et al., 2017), using 5K word translation dictionaries, as described in Glavaˇs et al. (2019). Since LE is an asymmetric relation and cosine similarity is a symmetric measure, we did not expect this baseline to be particularly competitive and expected most participants to outperform it. For the Any track, we used GLEN, our recent neural explicit specialization model for LE (Glavaˇs and Vuli´c, 2019) as a competitive baseline. GLEN"
2020.semeval-1.2,S16-1168,0,0.059972,"lly validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014"
2020.semeval-1.2,D15-1075,0,0.0445674,"ded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degr"
2020.semeval-1.2,P15-2001,0,0.0651471,"Missing"
2020.semeval-1.2,S17-2002,0,0.336887,"we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates a perfect LE relation between the concepts"
2020.semeval-1.2,D18-1269,0,0.0318256,"n annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014;"
2020.semeval-1.2,N13-1073,0,0.0750154,"resume something similar to be the case with LE and the proposed 4lang graphs – it is inherently difficult to create a reliable LE score based on paths and distances in a symbolic representation that is a (directed) graph. Team UAlberta (Hauer et al., 2020). The approach of UAlberta for cross-lingual binary LE detection combines sentence-level translations (i.e., parallel corpora), distributional word vectors (i.e., word embeddings) and multilingual lexical resources. Their base method, dubbed BITEXT, mines candidates for cross-lingual LE from parallel corpora – they simply run the FastAlign (Dyer et al., 2013) word alignment algorithm and assume that the LE relation holds between all aligned pairs of words. As clarified by the authors, this will, in most cases, extract cross-lingual synonyms, which, strictly speaking, do satisfy the LE relation; also, in some cases, the alignments will be established between close (e.g., first order) hyponymy-hypernymy pairs – in this case, however, the bitext alignment of words alone does not suggest the direction of the LE relation. The authors simply declare any pair of words from our cross-lingual datasets to stand in the LE relation if they find this pair in t"
2020.semeval-1.2,ehrmann-etal-2014-representing,0,0.0286213,"re of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online),"
2020.semeval-1.2,E17-1056,1,0.777636,"all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) sugg"
2020.semeval-1.2,N15-1184,0,0.0325169,"Missing"
2020.semeval-1.2,P14-1113,0,0.0244977,"has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 202"
2020.semeval-1.2,P05-1014,0,0.0147749,"Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models an"
2020.semeval-1.2,D16-1235,1,0.900545,"Missing"
2020.semeval-1.2,P18-1004,1,0.899108,"Missing"
2020.semeval-1.2,P19-1476,1,0.837489,"Missing"
2020.semeval-1.2,P19-1070,1,0.894937,"Missing"
2020.semeval-1.2,D17-1185,1,0.900423,"Missing"
2020.semeval-1.2,P16-1193,0,0.0238705,"SQ). We offered two different evaluation tracks. In the distributional (Dist) track we allowed only for fully distributional systems, capturing LE only on the basis of unannotated corpora. In contrast, the Any track invited systems that exploit any kind of additional external resources, including lexico-semantic networks. Overall, we did not observe any empirically confirmed strong systems in the Dist track, further corroborating the findings from prior work that building LE-oriented vectors distributionally is more difficult than for some other relations such as broader semantic relatedness (Henderson and Popa, 2016). However, several runs submitted to the Any track pushed the state of the art both in binary LE detection and graded LE prediction, for most of the languages and language pairs in our evaluation. 2 Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in"
2020.semeval-1.2,J15-4004,1,0.896991,"Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 in"
2020.semeval-1.2,W19-4310,1,0.900241,"Missing"
2020.semeval-1.2,P15-2020,1,0.902688,"Missing"
2020.semeval-1.2,S15-1019,0,0.138539,"in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of concepts (which can be both unigrams and multi-word expressions, i.e., phrases) in order to induce the directed graphs conforming to the 4lang formalism (Kornai et al., 2015). 4lang graphs are directed graphs with concepts as nodes and three types of edges: 0 0 edges of type 0 denote attribution (cat → − four-legged), lexical entailment (cat → − mammal), or 0 unary predication (cat → − meow); edges of type 1 and 2 denote relations between the predicate and its 1 2 subject and object, respectively (e.g., cat ← − catch → − mouse).4 Kov´acs et al. (2020) first extract definitions from Wiktionary using language-specific templates. Each definition is then transformed into a 4lang graph with the help of a language-specific Universal Dependencies (Nivre et al., 2016) par"
2020.semeval-1.2,P19-1313,0,0.0113187,"a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License."
2020.semeval-1.2,Q15-1016,0,0.0406784,"rediction for SQ) and each language-pair in (3) and (4) (e.g., binary LE detection for HR-TR) instantiates one concrete subtask. We allowed participants to submit their predictions for an arbitrary set of subtasks. Moreover, the participants were allowed to tackle only graded LE prediction or only binary LE detection. Evaluation Metrics. For each graded LE prediction subtask, we measured the alignment of predictions and gold LE scores using the Spearman’s Rank Correlation Coefficient (Spearman ρ), which is in line with previous work on similar concept pair scoring datasets (Hill et al., 2015; Levy et al., 2015; Vuli´c et 27 al., 2017, inter alia). For the binary LE detection subtasks we resorted to the standard F1 measure. 4 Participating Systems We now describe in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of"
2020.semeval-1.2,S10-1002,0,0.040472,"egree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluati"
2020.semeval-1.2,W13-0904,0,0.0173071,"c lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the"
2020.semeval-1.2,Q17-1022,1,0.903619,"Missing"
2020.semeval-1.2,S13-2005,0,0.0319147,"”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barc"
2020.semeval-1.2,D18-1026,1,0.886103,"Missing"
2020.semeval-1.2,D19-1226,1,0.869316,"Missing"
2020.semeval-1.2,W16-1622,0,0.0342201,"Missing"
2020.semeval-1.2,P18-2057,0,0.0129808,"died in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the su"
2020.semeval-1.2,E14-4008,0,0.0287191,"Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared ta"
2020.semeval-1.2,P16-1226,0,0.0129442,"ection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reaso"
2020.semeval-1.2,E17-1007,0,0.0156047,"graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical enta"
2020.semeval-1.2,P06-1101,0,0.114198,"em runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vaguen"
2020.semeval-1.2,N18-1056,0,0.334657,".org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE holds between concepts"
2020.semeval-1.2,N18-1103,1,0.907335,"Missing"
2020.semeval-1.2,J17-4004,1,0.90587,"Missing"
2020.semeval-1.2,N18-1048,1,0.882981,"Missing"
2020.semeval-1.2,D19-1449,1,0.894355,"Missing"
2020.semeval-1.2,P19-1490,1,0.629745,"Missing"
2020.semeval-1.2,N16-1142,0,0.0991043,"http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE"
2020.semeval-1.2,2020.semeval-1.31,0,0.482762,"quire a bilingual word embedding space, merely two monolingual word embedding spaces: sets X and Y are obtained by thresholding monolingual word embedding similarities (the threshold value is tuned on the development portions of our LE sets). Finally, all possible pairs (xi , yj ) ∈ X × Y are considered to stand in the LE relation. Finally, in the third run, the authors couple the bitext-based FastText aligner with the BABEL A LIGN algorithm, which aligns concepts across languages based on BabelNet (Navigli and Ponzetto, 2012), a massively multilingual lexico-semantic network. Team SHIKEBLCU (Wang et al., 2020). The approach of Wang et al. (2020) extends the wellestablished line of work based on specializing (i.e., fine-tuning) distributional word vectors for lexical relations, be it symmetric semantic similarity (Faruqui et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2018; Glavaˇs and Vuli´c, 2018; Ponti et al., 2018) or the asymmetric LE relation (Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019; Vuli´c et al., 2019b), using constraints from external lexico-semantic resources like WordNet for supervision. At the core of the approach is the Lexical-Entailment Attract Re"
2020.semeval-1.2,C14-1212,0,0.0213805,"ordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this"
2020.semeval-1.2,N18-1101,0,0.0246649,"Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relat"
2020.wanlp-1.17,2020.acl-main.485,0,0.230454,"Missing"
2020.wanlp-1.17,Q17-1010,0,0.775331,"er programmer as woman is to homemaker”, which is algebraically encoded in the embedding ~ ~ ~ ~ space with the analogical relation man− computer programmer ≈ woman− homemaker (Bolukbasi et al., 2016). The existence of such biases in word embeddings stems from the combination of (1) human biases manifesting themselves in terms of word co-occurrences (e.g., the word woman appearing in a training corpus much more often in the context of homemaker than together with computer programmer) and (2) the distributional nature of the word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017), which induce word vectors precisely by exploiting word co-occurrences, i.e., thus also encoding the human biases as a (negative) side-effect, which represents, expressed according to the taxnomy of harms proposed by Blodgett et al. (2020), a representational harm, more specifically, stereotyping. In order to quantify the amount of bias in word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT), which is based on the associative difference in terms of semantic similarity between two sets of target terms, e.g., male and female terms, towards two sets of attr"
2020.wanlp-1.17,S17-2001,0,0.164749,"Online), December 12, 2020 focus on the multi-dimensional analysis of biases in Arabic word embeddings. The motivation for this work is twofold: (1) Arabic is one of the most widely spoken languages in the world:1 this means that the biases encoded in language technology for Arabic have the potential for affecting more people than for most other languages; (2) language resources for Arabic – large corpora (Goldhahn et al., 2012), pretrained word embeddings (Mohammad et al., 2017; Bojanowski et al., 2017), and datasets for measuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature, has recently been shown to systematically overestimate the bias present in an embedding space (Ethayarajh et al., 2019), in this work, we couple it with several other bias tests, desi"
2020.wanlp-1.17,P17-2072,0,0.0168841,"–199 Barcelona, Spain (Online), December 12, 2020 focus on the multi-dimensional analysis of biases in Arabic word embeddings. The motivation for this work is twofold: (1) Arabic is one of the most widely spoken languages in the world:1 this means that the biases encoded in language technology for Arabic have the potential for affecting more people than for most other languages; (2) language resources for Arabic – large corpora (Goldhahn et al., 2012), pretrained word embeddings (Mohammad et al., 2017; Bojanowski et al., 2017), and datasets for measuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature, has recently been shown to systematically overestimate the bias present in an embedding space (Ethayarajh et al., 2019), in this work, we couple it with several oth"
2020.wanlp-1.17,P19-1166,0,0.0156101,"easuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature, has recently been shown to systematically overestimate the bias present in an embedding space (Ethayarajh et al., 2019), in this work, we couple it with several other bias tests, designed to capture and quantify other aspects of human biases: Embedding Coherence Test (Dev and Phillips, 2019), Bias Analogy Test (Lauscher et al., 2020) and Implicit Bias Tests (Gonen and Goldberg, 2019). Our work, which is to the best of our knowledge the first study on quantifying biases in Arabic distributional word vector spaces, yields some interesting findings: biases seem more prominent in vectors trained on texts written in Egyptian Arabic than those written in Modern Standard Arabic (MSA). Also, the implicit gender bias i"
2020.wanlp-1.17,goldhahn-etal-2012-building,0,0.16885,"ternational License. creativecommons.org/licenses/by/4.0/. License details: http:// 192 Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 192–199 Barcelona, Spain (Online), December 12, 2020 focus on the multi-dimensional analysis of biases in Arabic word embeddings. The motivation for this work is twofold: (1) Arabic is one of the most widely spoken languages in the world:1 this means that the biases encoded in language technology for Arabic have the potential for affecting more people than for most other languages; (2) language resources for Arabic – large corpora (Goldhahn et al., 2012), pretrained word embeddings (Mohammad et al., 2017; Bojanowski et al., 2017), and datasets for measuring semantic quality of Arabic embeddings (Elrazzaz et al., 2017; Cer et al., 2017) – are publicly available, allowing for the analyses of biases that these resources potentially hide. As a first step in the analysis of language technology biases for Arabic, we present A RAWEAT, an Arabic extension to the multilingual XWEAT framework (Lauscher and Glavaˇs, 2019). Because the WEAT test (Caliskan et al., 2017), though it has the notable advantage of drawing inspiration from psychology literature"
2020.wanlp-1.17,W19-3621,0,0.422201,"tional harm, more specifically, stereotyping. In order to quantify the amount of bias in word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT), which is based on the associative difference in terms of semantic similarity between two sets of target terms, e.g., male and female terms, towards two sets of attribute terms, e.g., career and family terms. Most recently, the WEAT test, measuring the degree of explicit bias in the distributional space, has been coupled with other tests, aiming to measure other aspects of bias, such as the amount of implicit bias (Gonen and Goldberg, 2019) or the presence of the analogical bias (Lauscher et al., 2020). While there is evidence that distributional vectors often encode human biases, the amount of biases does not seem to be universal across different languages and corpora, as recently shown by Lauscher and Glavaˇs (2019) in the analysis of distributional biases across seven different languages. In this work, we This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 192 Proceedings of the Fifth Arabic Natural Language Processing Workshop, p"
2020.wanlp-1.17,W19-3822,0,0.0237785,"from a set of attribute terms. They also propose several debiasing methods. Gonen and Goldberg (2019) show that many debiasing methods only mask but do not fully remove biases present in the embedding spaces. They propose to additionally test for 197 implicit biases, by trying to classify or cluster the sets of target terms. Lauscher et al. (2020) unify the different notions of biases into explicit and implicit bias specifications, based on which they propose methods for quantifying and removing biases. While their is some effort to account for gender-awareness in Arabic machine translation (Habash et al., 2019), we are, to the best of our knowledge, the first to measure bias in Arabic Language Technology. 5 Conclusion Language technologies aim to avoid reflecting negative human biases such as racism and sexism. Yet, the ubiquitous word embeddings, used as input for many NLP models, seem to encode many such biases. In this work, we extensively quantify and analyze the biases in different vector spaces built from text in Arabic, a major world language with close to 300M native speakers. To this effect, we translate existing bias specifications from English to Arabic and investigate biases in embedding"
2020.wanlp-1.17,D19-1530,0,0.0194305,"cts (e.g., ant, flea) Weapons (e.g., gun) Arts (e.g., poetry) Arts Mental (e.g., sad) Pleasant (e.g., health) Pleasant Male (e.g., brother, son) Male Long-term (e.g., always) Unpleasant (e.g., abuse) Unpleasant Female (e.g., woman, sister) Female Short-term (e.g., occasional) Table 2: WEAT bias tests. cases; and even in those cases the MSA translation is also in usage in other Arabic dialects.2 We further omitted WEAT tests 3–6 and 10 as they are based on proper names. While it has been shown that names are a good proxy for identifying and removing bias towards specific groups of people (Hall Maudslay et al., 2019), it is difficult to “translate” them.3 As an example of the resulting AraWEAT test, Table 1 list the Arabic translation of WEAT test T 7. An overview on the remaining tests with their respective target and attribute term sets is provided in Table 2. T1 ECT BAT KM T2 W ECT BAT KM T7 ECT BAT KM T8 ECT BAT KM Model Lang FT A RABIC FT E GYPT MSA 0.85 0.69 0.47 0.71 0.51* 0.62 0.43 0.63 -0.15* 0.17* 0.5 0.56 0.05* 0.02* 0.44 0.56 Egyptian 1.17 0.45 0.49 0.95 0.97 0.56 0.51 0.65 0.65* 0.54 0.51 0.6 0.09* 0.63 0.47 0.6 W W W AV SG W IKI MSA AV SG T WITTER Mixed 0.27* 0.82 0.49 0.62 0.98 0.61 0.43 0."
2020.wanlp-1.17,S19-1010,1,0.634103,"Missing"
2020.wanlp-1.17,D14-1162,0,0.0845284,"example “Man is to computer programmer as woman is to homemaker”, which is algebraically encoded in the embedding ~ ~ ~ ~ space with the analogical relation man− computer programmer ≈ woman− homemaker (Bolukbasi et al., 2016). The existence of such biases in word embeddings stems from the combination of (1) human biases manifesting themselves in terms of word co-occurrences (e.g., the word woman appearing in a training corpus much more often in the context of homemaker than together with computer programmer) and (2) the distributional nature of the word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017), which induce word vectors precisely by exploiting word co-occurrences, i.e., thus also encoding the human biases as a (negative) side-effect, which represents, expressed according to the taxnomy of harms proposed by Blodgett et al. (2020), a representational harm, more specifically, stereotyping. In order to quantify the amount of bias in word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT), which is based on the associative difference in terms of semantic similarity between two sets of target terms, e.g., male and female terms"
2020.wanlp-1.17,D19-1531,0,0.0263612,"Missing"
2021.eacl-demos.11,2020.acl-main.485,0,0.0799835,"Missing"
2021.eacl-demos.11,Q17-1010,0,0.10173,"Missing"
2021.eacl-demos.11,N19-3002,0,0.0202981,"onal nature of word representation models (Harris, 1954) it is – depending on the sociotechnical context – an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications. In this work, we address this gap by introducing D EB IE, the first integrated platform offering bias measurement and mitigation for arbitrary static embedding spaces and bias specifications. The D E B IE platform is grounded in the general framework for implicit and explicit debiasing of word embedding spaces (Lauscher e"
2021.eacl-demos.11,2020.emnlp-main.656,0,0.0405917,"Missing"
2021.eacl-demos.11,D14-1162,0,0.0865587,"BAM (GBDD). 3.4 Integrated Data D EB IE is designed as a general tool, which allows user to upload their own embedding spaces and define their own bias specifications for testing and/or debiasing. Nonetheless, we include into the platform a set of commonly used bias specifications and word embedding spaces. Concretely, D EB IE includes the whole WEAT test collection (Caliskan et al., 2017), containing the explicit bias specifications summarized in Table 1. D EB IE also comes with three word embedding spaces, pretrained with different models: (1) fastText (Bojanowski et al., 2017),6 (2) GloVe (Pennington et al., 2014),7 and (3) CBOW (Mikolov et al., 2013).8 All three spaces are 300-dimensional and their vocabularies are limited to 200K most frequent words. Debiasing Methods D EB IE encompasses implementations of two debiasing models from (Lauscher et al., 2020a), for which an implicit bias specification suffices:5 General Bias Direction Debiasing (GBDD). As an extension of the linear projection model of Dev and Phillips (2019), GBDD relies on identifying the bias direction in the distributional space. Let (ti1 , tj2 ) be word pairs with ti1 ∈ T1 , tj2 ∈ T2 , respectively. First, we obtain partial bias dire"
2021.eacl-demos.11,P19-2031,0,0.0145707,"pending on the sociotechnical context – an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications. In this work, we address this gap by introducing D EB IE, the first integrated platform offering bias measurement and mitigation for arbitrary static embedding spaces and bias specifications. The D E B IE platform is grounded in the general framework for implicit and explicit debiasing of word embedding spaces (Lauscher et al., 2020a). Within this framework, an implicit bias consis"
2021.eacl-demos.11,P19-1070,1,0.892893,"Missing"
2021.eacl-demos.11,W19-3621,0,0.206795,"logies, such as the fa−→ − − −−−−−−−→ ≈ mous example of sexism: − man programmer −−−−−−−→ − − − − → woman − homemaker (Bolukbasi et al., 2016). While this is not surprising, given the distributional nature of word representation models (Harris, 1954) it is – depending on the sociotechnical context – an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications. In this work, we address this gap by introducing D EB IE, the first integrated platform offering bias measurement and mitigation f"
2021.eacl-demos.11,J15-4004,0,0.0347694,"h U ΣV > as the singular value decomposition of XT2 XT>1 . In the last step, the original space and its “translation” X = XWX (which is equally biased), are averaged to obtain the debiased embedding space: Semantic Quality Tests (SQ). The debiasing models (3.3) modify the embedding space. While they reduce the bias, they may reduce the general semantic quality of the embedding space, which could be detrimental for model performance in downstream applications. This is why we couple the bias tests with measures of semantic word similarity on two established word-similarity datasets: SimLex-999 (Hill et al., 2015) or WordSim-353 (Finkelstein et al., 2001). We compute the Spearman correlation between the human similarity scores assigned to word pairs and corresponding cosines computed from the embedding space. 3.3 1 BAM(X) = (X + XWX ) . 2 Note that D EB IE can trivially compose the two debiasing models – the resulting space after applying GBDD (BAM) can be the input for BAM (GBDD). 3.4 Integrated Data D EB IE is designed as a general tool, which allows user to upload their own embedding spaces and define their own bias specifications for testing and/or debiasing. Nonetheless, we include into the platfo"
2021.eacl-demos.11,N19-1064,0,0.0362523,"Missing"
2021.eacl-demos.11,S19-1010,1,0.838466,"Missing"
2021.eacl-demos.11,2020.wanlp-1.17,1,0.794946,"Missing"
2021.eacl-main.56,aker-etal-2017-simple,0,0.0119796,"ork, none of these works considers the sequence of affective information in text; instead, they feed the entire news articles as one segment into their models. In contrast, the aim of our work is to evaluate this source of information, using a neural architecture. • We build a novel fake news dataset, called MultiSourceFake, that is collected from a large set of websites and annotated on the basis of the joint agreement of a set of news sources. 2 Related Work Previous work on fake news detection is mainly divided into two main lines, namely with a focus on social media (Zubiaga et al., 2015; Aker et al., 2017; Ghanem et al., 2019) or online news articles (Tausczik and Pennebaker, 2010; Horne and Adali, 2017; Rashkin et al., 2017; Barr´on-Cedeno et al., 2019). In this work we focus on the latter one. Factchecking (Karadzhov et al., 2017; Zlatkova et al., 2019; Shu et al., 2019a) is another closely related research topic. However, fact-checking targets only short texts (that is, claims) and focuses on using external resources (e.g. Web, knowledge sources) to verify the factuality of the news. The focus in previous work on fake news detection is mainly on proposing new feature sets. Horne and Adali ("
2021.eacl-main.56,C18-1244,0,0.0222144,"t statements can unmask the fabrication. On the other hand, in fake news articles the authors exploit the length of the news to conceal their fabricated story. This fact exposes the readers to be emotionally manipulated while reading longer texts that have several imprecise or fabricated plots. The flow of information has been investigated for different tasks: Reagan et al. (2016) studied the emotional arcs in stories in order to understand complex emotional trajectories; Maharjan et al. (2018) model the flow of emotions over a book and quantify its usefulness for predicting success in books; Kar et al. (2018) explore the problem of creating tags for movies from plot synopses using emotions. Unlike previous works (Rashkin et al., 2017; Shu et al., 2018; Castelo et al., 2019; Ghanem et al., 2020) that discarded the chronological order of events in news articles, in this work we propose a model that takes into account the affective changes in texts to detect fake news. We hypothesize that fake news has a different distribution of affective information across the text compared to real news, e.g. more fear emotion in the first part of the article or more overall offensive terms, etc. Therefore, modelin"
2021.eacl-main.56,karadzhov-etal-2017-fully,0,0.110113,"7; Ghanem et al., 2019) or online news articles (Tausczik and Pennebaker, 2010; Horne and Adali, 2017; Rashkin et al., 2017; Barr´on-Cedeno et al., 2019). In this work we focus on the latter one. Factchecking (Karadzhov et al., 2017; Zlatkova et al., 2019; Shu et al., 2019a) is another closely related research topic. However, fact-checking targets only short texts (that is, claims) and focuses on using external resources (e.g. Web, knowledge sources) to verify the factuality of the news. The focus in previous work on fake news detection is mainly on proposing new feature sets. Horne and Adali (2017) present a set of content-based features, including readability (number of unique words, SMOG readability measure, etc.), stylistic (frequency of partof-speech tags, number of stop words, etc.) and psycholinguistic features (i.e., several categories from the LIWC dictionary (Tausczik and Pennebaker, 2010)). When these features are fed into a Support Vector Machine (SVM) classifier and applied, for instance, to the task of distinguishing satire from real news, they obtain high accuracies. Using the same features for the task of fake news detection, however, results in somewhat lower scores. P´e"
2021.eacl-main.56,N18-2042,0,0.03477,"Missing"
2021.eacl-main.56,W10-0204,0,0.0182268,"evance of the affective information with respect to the topics. For this, we concatenate the topic summarized vector vtopic with the representation vector vaffect , aimed at capturing the affective information extracted from each segment (Section 3.2). vconcat = vtopic ⊕ vaffect To merge the different representations and capture their joint interaction in each segment, the model processes the produced concatenated vector vconcat with another fully connected layer: • Emotions: We use emotions as features to detect their change among articles’ segments. For that we use the NRC emotions lexicon (Mohammad and Turney, 2010) that contains ∼14K words labeled using the eight Plutchik’s emotions (8 Features). • Sentiment: We extract the sentiment from the text, positive and negative, again using the NRC lexicon (Mohammad and Turney, 2010) (2 Features). • Morality: We consider cue words from the Moral Foundations Dictionary2 (Graham et al., 2009) where words are assigned to one (or more) of the following categories: care, harm, fairness, unfairness (cheating), loyalty, betrayal, authority, subversion, sanctity and degradation (10 Features). • Imageability: We use a list of words rated by their degree of abstractness"
2021.eacl-main.56,C18-1287,0,0.0551735,"Missing"
2021.eacl-main.56,D17-1317,0,0.0787607,"to conceal their fabricated story. This fact exposes the readers to be emotionally manipulated while reading longer texts that have several imprecise or fabricated plots. The flow of information has been investigated for different tasks: Reagan et al. (2016) studied the emotional arcs in stories in order to understand complex emotional trajectories; Maharjan et al. (2018) model the flow of emotions over a book and quantify its usefulness for predicting success in books; Kar et al. (2018) explore the problem of creating tags for movies from plot synopses using emotions. Unlike previous works (Rashkin et al., 2017; Shu et al., 2018; Castelo et al., 2019; Ghanem et al., 2020) that discarded the chronological order of events in news articles, in this work we propose a model that takes into account the affective changes in texts to detect fake news. We hypothesize that fake news has a different distribution of affective information across the text compared to real news, e.g. more fear emotion in the first part of the article or more overall offensive terms, etc. Therefore, modeling the flow of such information may help discriminating fake from real news. Our model consists of two main sub-modules, topic-b"
2021.eacl-main.56,N16-1174,0,0.056493,"nformation by searching Twitter for users who shared news. Out of the whole collected information, we use only the textual information of news articles, which is the part we are interested in. 683 Baselines. To evaluate the performance of our model, we use a combination of fake news detection models and deep neural network architectures: • CNN, LSTM: We use CNN and LSTM models and validate their performance when treating each document as one fragment. We experiment with different hyper-parameters and report results for the ones that performed best on the validation set. • HAN: The authors of (Yang et al., 2016) proposed a Hierarchical Attention Networks (HAN) model for long document classification. The proposed model consists of two levels of attention mechanisms, i.e., word and sentence attention. The model splits each document into sentences and learns sentence representations from words. • BERT: is a text representation model that showed superior performance on multiple natural language processing (NLP) benchmarks (Devlin et al., 2019). We use the pre-trained bertbase-uncased version which has 12-layers and yields output embeddings with a dimension of size 768. We feed the hidden representation o"
2021.eacl-main.56,N16-1000,0,0.0992272,"might be less harmful than news articles. They may have some eye-catching terms that aim to manipulate the readers’ emotions (Chakraborty et al., 2016). In many cases, the identification of this kind of exaggeration in short statements can unmask the fabrication. On the other hand, in fake news articles the authors exploit the length of the news to conceal their fabricated story. This fact exposes the readers to be emotionally manipulated while reading longer texts that have several imprecise or fabricated plots. The flow of information has been investigated for different tasks: Reagan et al. (2016) studied the emotional arcs in stories in order to understand complex emotional trajectories; Maharjan et al. (2018) model the flow of emotions over a book and quantify its usefulness for predicting success in books; Kar et al. (2018) explore the problem of creating tags for movies from plot synopses using emotions. Unlike previous works (Rashkin et al., 2017; Shu et al., 2018; Castelo et al., 2019; Ghanem et al., 2020) that discarded the chronological order of events in news articles, in this work we propose a model that takes into account the affective changes in texts to detect fake news. W"
2021.eacl-main.56,D19-1216,0,0.0156223,"architecture. • We build a novel fake news dataset, called MultiSourceFake, that is collected from a large set of websites and annotated on the basis of the joint agreement of a set of news sources. 2 Related Work Previous work on fake news detection is mainly divided into two main lines, namely with a focus on social media (Zubiaga et al., 2015; Aker et al., 2017; Ghanem et al., 2019) or online news articles (Tausczik and Pennebaker, 2010; Horne and Adali, 2017; Rashkin et al., 2017; Barr´on-Cedeno et al., 2019). In this work we focus on the latter one. Factchecking (Karadzhov et al., 2017; Zlatkova et al., 2019; Shu et al., 2019a) is another closely related research topic. However, fact-checking targets only short texts (that is, claims) and focuses on using external resources (e.g. Web, knowledge sources) to verify the factuality of the news. The focus in previous work on fake news detection is mainly on proposing new feature sets. Horne and Adali (2017) present a set of content-based features, including readability (number of unique words, SMOG readability measure, etc.), stylistic (frequency of partof-speech tags, number of stop words, etc.) and psycholinguistic features (i.e., several categories"
2021.emnlp-main.615,P18-1017,0,0.0137701,"nd parties) and coalitions into the model. As for NER, we use a sequence tagging setup and create our training data as follows. Based on a number of manually created dictionaries, we augment the German input data with BIO labels on the token level (Figure 1). We then use this data in a MTL setting, in combination with C-type prediction for Ireland, Germany and Austria. Valence classification. To improve results for polarity classification, we experiment with a highly related task from the area of sentiment analysis (Zhang et al., 2018). Specifically, we use the multilingual NRC VAD lexicon of Mohammad (2018) Named Entity Recognition. Directing the mod- which provides valence scores for over 20,000 pivot els’ attention towards named entities such as per- terms, created via Best-Worst Scaling. We extract sons, organisations etc. might provide useful infor- the scores for the English words and their German mation for C-type prediction. We use the English translations and, for each sentence in our German Ontonotes dataset (Weischedel et al., 2013) and the and Irish input data, compute a valence score by German Twitter news headlines data (Ruppenhofer simply adding up the scores for each lemma in et a"
2021.emnlp-main.615,2020.emnlp-demos.7,0,0.0337264,"Missing"
2021.emnlp-main.615,2020.lrec-1.566,1,0.780076,"Missing"
2021.emnlp-main.615,2020.coling-main.426,0,0.0228252,"the capabilities of computational methods for text analysis have drastically evolved and the usage of Natural Language Processing (NLP) methods have increasingly expanded in scope, enabling a wide range of applications for the automatised analysis of political texts (Glavaš et al., 2019). Much work in NLP in the past has focused on making sense of how legislative processes are codified in text – e.g., by analysing debate moIn this paper, we present the task of coalition sigtions (Abercrombie et al., 2019; Abercrombie and nal detection as a new, challenging NLU task. We Batista-Navarro, 2020; Sawhney et al., 2020, inter propose to approach this task as a machine learning alia). However, when analysing electoral dynamics problem and decompose it into two subtasks, (i) the it seems necessary to complement the application identification of text sequences that contain a signal of NLP methods on domain-specific texts such as and (ii) the prediction of the polarity of each potenelectoral manifestos with a broader analysis of sig- tial signal, that is, whether the speaker promotes or nals from news texts (Blokker et al., 2020). opposes the signalled coalition – cf. examples 1.1 Specifically in the case of go"
2021.emnlp-main.615,2020.emnlp-demos.6,0,0.024649,"Missing"
broscheit-etal-2010-extending,M95-1005,0,\N,Missing
broscheit-etal-2010-extending,W05-0406,0,\N,Missing
broscheit-etal-2010-extending,C04-1074,0,\N,Missing
broscheit-etal-2010-extending,W01-0717,0,\N,Missing
broscheit-etal-2010-extending,W05-0303,0,\N,Missing
broscheit-etal-2010-extending,J96-1002,0,\N,Missing
broscheit-etal-2010-extending,H05-1004,0,\N,Missing
broscheit-etal-2010-extending,P05-3002,0,\N,Missing
broscheit-etal-2010-extending,P04-1018,0,\N,Missing
broscheit-etal-2010-extending,P94-1019,0,\N,Missing
broscheit-etal-2010-extending,N06-1025,1,\N,Missing
broscheit-etal-2010-extending,P06-1041,0,\N,Missing
broscheit-etal-2010-extending,P02-1014,0,\N,Missing
broscheit-etal-2010-extending,W02-1040,0,\N,Missing
broscheit-etal-2010-extending,J01-4004,0,\N,Missing
broscheit-etal-2010-extending,P04-2010,0,\N,Missing
broscheit-etal-2010-extending,kunze-lemnitzer-2002-germanet,0,\N,Missing
broscheit-etal-2010-extending,poesio-kabadjov-2004-general,1,\N,Missing
broscheit-etal-2010-extending,finthammer-cramer-2008-exploring,0,\N,Missing
broscheit-etal-2010-extending,P03-1023,0,\N,Missing
D12-1128,W09-2420,0,0.0477125,"Missing"
D12-1128,W11-0104,0,0.147473,"ea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics search on the topic has been done, including the use of statistical translations of sentences into many languages as features for supervised models (Banea and Mihalcea, 2011; Lefever et al., 2011), and the projection of monolingual knowledge onto another language (Khapra et al., 2011). Yet the above two goals, i.e., disambiguating in an arbitrary language and using lexical and semantic knowledge from many languages in a joint way to improve the WSD task, have not hitherto been attained. In this paper, we address both objectives and propose a graph-based approach to multilingual joint Word Sense Disambiguation. Our proposal brings together the lexical knowledge from different languages by exploiting empirical evidence for disambiguation from each of them, and then"
D12-1128,S10-1054,0,0.0151949,"s and discussion. We report our results for CL-LS and CL-WSD in Tables 3 and 4. We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively. The evaluation scheme is based on the SemEval2007 English lexical substitution task (McCarthy and Navigli, 2009), and consists of an adaptation of the metrics of precision and recall for the translation setting. For each task, we compare our monolingual and multilingual approaches against the best performing SemEval systems for these tasks, namely UBA-T (Basile and Semeraro, 2010) and UVT-v (van Gompel, 2010) for CL-LS and CL-WSD, respectively, as well as a recent supervised proposal that exploits automatically generated multilingual features from parallel text and translated contexts (Lefever et al., 2011, Parasense). For each task we also report its official baseline, namely the first translation from an online-dictionary6 for CL-LS, and the most frequent word alignment obtained by 6 www.spanishdict.com 1407 applying GIZA++ to the Europarl data for CL-WSD. Our cross-lingual results confirm all trends of the English monolingual evaluation, namely that: a) our joint mu"
D12-1128,P06-1013,1,0.437409,"d Lapata, 2010; Ponzetto and Navigli, 2010). Inverse path length sum (PLength): We then developed a graph connectivity measure which scores each sense by summing over the inverse length of all paths which connect it to other senses in the graph: scorej = X p ∈ paths(sj ) 1 elength(p)−1 scorej = At the core of our algorithm lies the combination of the scores generated using the different translations of the target word w. For this purpose, we apply socalled ensemble methods, which have been shown to improve the performance of both supervised (Florian et al., 2002) and unsupervised WSD systems (Brody et al., 2006). Given |T |lexicalizations and |S |senses for w, the input to the combination component consists of a |T |× |S |matrix LScore, where each cell lScorei,j quantifies the empirical support for sense sj from a term ti ∈ T (see Section 3.2 for an example). The ensemble method computes from this translation-sense matrix a combined scoring, expressing the joint confidence across terms in different languages over the set of senses S. In this work, we use the ‘Probability Mixture’ (PMixture) method 1404 lScorei,j p(si,j ), p(si,j ) = P|S| . i=1 s=1 lScorei,s banknEN banconES BanknDE PMixture 3.5 Ensem"
D12-1128,P91-1034,0,0.217592,"al effects of exploiting multilingual knowledge in a joint fashion. 2 Related Work Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al., 1992; Chan and Ng, 2005; Zhong and Ng, 2009). Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform WSD using a similarity measure (Diab, 2003). A historical approach (Brown et al., 1991) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features. All the above approaches to multilingual or crosslingual WSD rely on bilingual corpora, including those which exploit existing multilingual WordNetlike resources (Ide et al., 2002), or use automatically induced multilingual co-occurrence graphs (Silberer 1400 and Ponzetto, 2010). However, this requirement is often very hard to satisfy, especially if we need wide coverage. To overcome this limitation, in this work we make use of Babel"
D12-1128,D07-1007,0,0.0154445,"use of BabelNet (Navigli and Ponzetto, 2010), a very large multilingual lexical knowledge base. This resource – complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al. (2010) and Meyer and Gurevych (2012), inter alia – provides a truly multilingual semantic network by combining Wikipedia’s multilinguality with the output of a state-of-the-art machine translation system to achieve high coverage for all languages. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al. (2007), who successfully used sense information to boost state-of-the-art statistical MT. In this work we focus instead on the benefits of using multilingual information for WSD by exploiting the structure of a multilingual semantic network. 3 Multilingual Joint WSD We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and"
D12-1128,P07-1005,0,0.022428,"nd Ponzetto, 2010), a very large multilingual lexical knowledge base. This resource – complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al. (2010) and Meyer and Gurevych (2012), inter alia – provides a truly multilingual semantic network by combining Wikipedia’s multilinguality with the output of a state-of-the-art machine translation system to achieve high coverage for all languages. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al. (2007), who successfully used sense information to boost state-of-the-art statistical MT. In this work we focus instead on the benefits of using multilingual information for WSD by exploiting the structure of a multilingual semantic network. 3 Multilingual Joint WSD We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and 3.5). 3.1 BabelNet Babe"
D12-1128,P91-1017,0,0.101892,"sensetagged corpora like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for"
D12-1128,W02-0811,0,0.017964,"etitive performance on various WSD tasks (Navigli and Lapata, 2010; Ponzetto and Navigli, 2010). Inverse path length sum (PLength): We then developed a graph connectivity measure which scores each sense by summing over the inverse length of all paths which connect it to other senses in the graph: scorej = X p ∈ paths(sj ) 1 elength(p)−1 scorej = At the core of our algorithm lies the combination of the scores generated using the different translations of the target word w. For this purpose, we apply socalled ensemble methods, which have been shown to improve the performance of both supervised (Florian et al., 2002) and unsupervised WSD systems (Brody et al., 2006). Given |T |lexicalizations and |S |senses for w, the input to the combination component consists of a |T |× |S |matrix LScore, where each cell lScorei,j quantifies the empirical support for sense sj from a term ti ∈ T (see Section 3.2 for an example). The ensemble method computes from this translation-sense matrix a combined scoring, expressing the joint confidence across terms in different languages over the set of senses S. In this work, we use the ‘Probability Mixture’ (PMixture) method 1404 lScorei,j p(si,j ), p(si,j ) = P|S| . i=1 s=1 lSc"
D12-1128,1992.tmi-1.9,0,0.779099,"like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Ling"
D12-1128,W02-0808,0,0.0358311,"Missing"
D12-1128,P11-1057,0,0.0458791,"and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics search on the topic has been done, including the use of statistical translations of sentences into many languages as features for supervised models (Banea and Mihalcea, 2011; Lefever et al., 2011), and the projection of monolingual knowledge onto another language (Khapra et al., 2011). Yet the above two goals, i.e., disambiguating in an arbitrary language and using lexical and semantic knowledge from many languages in a joint way to improve the WSD task, have not hitherto been attained. In this paper, we address both objectives and propose a graph-based approach to multilingual joint Word Sense Disambiguation. Our proposal brings together the lexical knowledge from different languages by exploiting empirical evidence for disambiguation from each of them, and then combining this information in a synergistic way: each language provides a piece of sense evidence for the meani"
D12-1128,2005.mtsummit-papers.11,0,0.0112189,"ross-lingual WSD tasks cast disambiguation as a word translation problem: given an English polysemous noun in context as input, the system disambiguates it by providing a translation into another language (translations are deemed correct if they preserve the meaning of the source word in the target language). Their main difference, instead, lies in the range of translations which are assumed to be valid: that is, while CL-LS assumes no predefined sense inventory (i.e., any translation can be potentially correct), CL-WSD makes use of a sense inventory built on the basis of the Europarl corpus (Koehn, 2005). Our approach to lexical disambiguation involves two steps: first, given a target word in context, we disambiguate it as usual to the highest-ranked Babel synset; next, given the translations in the selected synset, we return the most suitable lexicalization in the language of interest. Since the selected synset can contain multiple translations in a target language for the input English word, we explore using an unsupervised strategy to select the most reliable translation from multiple candidates. To this end, we return for each test instance only the Algorithm P/R/F1 Baseline 23.80 Monolin"
D12-1128,S10-1094,0,0.0165637,"eshold. If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual lexical disambiguation Using a multilingual lexical resource makes it possible to perform WSD in any of its languages. Accordingly, we complement our evaluation on English texts with a second set of experiments where we quantify the impact o"
D12-1128,S10-1003,0,0.334273,"umura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). At the same time, new re1399 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1399–1410, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics search on the topic has been done, including the use of statistical translations of sentences into many languages as features for supervised models (Banea and Mihalcea, 2011; Lefever et al., 2011), and the projection of monolingual knowledge onto another language ("
D12-1128,P11-2055,0,0.113447,"Missing"
D12-1128,P11-1033,0,0.039171,"Missing"
D12-1128,W04-0805,0,0.0171319,"rch in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) has always been focused mostly on English. Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sensetagged corpora like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yarowsky, 1999). While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of"
D12-1128,W04-0806,0,0.0760061,"Missing"
D12-1128,P11-1134,0,0.0172269,"Missing"
D12-1128,W09-2412,0,0.0550903,"Missing"
D12-1128,H93-1061,0,0.0223142,"ent languages would improve the quality of text understanding in arbitrary languages. However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) has always been focused mostly on English. Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sensetagged corpora like SemCor (Miller et al., 1993). As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al., 2004; M`arquez et al., 2004; Orhan et al., 2007; Okumura et al., 2010). Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al., 1991; Gale et al., 1992; Resnik and Yar"
D12-1128,nastase-etal-2010-wikinet,0,0.0289083,"Missing"
D12-1128,P10-1023,1,0.932453,"bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features. All the above approaches to multilingual or crosslingual WSD rely on bilingual corpora, including those which exploit existing multilingual WordNetlike resources (Ide et al., 2002), or use automatically induced multilingual co-occurrence graphs (Silberer 1400 and Ponzetto, 2010). However, this requirement is often very hard to satisfy, especially if we need wide coverage. To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010), a very large multilingual lexical knowledge base. This resource – complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al. (2010) and Meyer and Gurevych (2012), inter alia – provides a truly multilingual semantic network by combining Wikipedia’s multilinguality with the output of a state-of-the-art machine translation system to achieve high coverage for all languages. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al. (2007)"
D12-1128,P12-3012,1,0.815104,"on all disambiguation tasks. 5 Conclusions In this paper we presented a multilingual joint approach to WSD. Key to our methodology is the effective use of a wide-coverage multilingual knowledge base, BabelNet, which we exploit to perform graph-based WSD across languages and combine complementary sense evidence from translations in different languages using an ensemble method. This is the first proposal to exploit structured multilingual information within a joint, knowledge-rich framework for WSD. The APIs to perform multilingual WSD using BabelNet are freely available for research purposes (Navigli and Ponzetto, 2012b). Thanks to multilingual joint WSD we achieve state-of-the-art performance on three different gold standards. The good news about these results is that not only can further advances be achieved by using multilingual lexical knowledge, but, more importantly, that combining multilingual sense evidence from different languages at the same time yields consistent improvements over a monolingual apBaseline UvT-v Parasense French P/R/F1 21.25 N/A 24.54 German P/R/F1 13.16 N/A 16.88 Italian P/R/F1 15.18 N/A 18.03 Spanish P/R/F1 19.74 23.39 22.80 Monolingual graph Degree PLength 22.94 23.42 17.15 17."
D12-1128,J03-1002,0,0.00411186,"hm P/R/F1 Baseline 23.80 Monolingual Degree 30.52 graph PLength 30.64 Multilingual ensemble Degree PLength 32.21 32.47 UBA-T 32.17 Table 3: Performance on SemEval-2010 lexical substitution (best results are bolded). most frequent translation found in the Babel synset. Given that the two tasks make different assumptions on the sense inventory (no fixed inventory for CLLS vs. Europarl-based for CL-WSD), the frequency of a translation is calculated as either the number of Babel synsets in which it occurs (CL-LS), or its frequency of alignment with the target word, as obtained by applying GIZA++ (Och and Ney, 2003) to Europarl (CL-WSD). To provide an answer for all instances, we return this most frequent translation even when no sense assignment is attempted – i.e., no sense of the target word is connected to any other sense of the context words – or a tie occurs. Results and discussion. We report our results for CL-LS and CL-WSD in Tables 3 and 4. We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively. The evaluation scheme is based on the SemEval2007 English lexical substitution task (McCarthy an"
D12-1128,S07-1011,0,0.0197176,"Missing"
D12-1128,S01-1005,0,0.0449544,"we follow previous work (e.g., Navigli and Lapata (2010)) and explore a weakly-supervised setting where the system attempts no sense assignment if the highest score among those assigned to the senses of a target word is below a certain threshold. If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual"
D12-1128,S10-1087,0,0.0141433,"der to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual lexical disambiguation Using a multilingual lexical resource makes it possible to perform WSD in any of its languages. Accordingly, we complement our evaluation on English texts with a second set of experiments where we quantify the impact of our approach on a lexical dis"
D12-1128,S10-1027,1,0.851206,"Missing"
D12-1128,W04-0811,0,0.058295,"i and Lapata (2010)) and explore a weakly-supervised setting where the system attempts no sense assignment if the highest score among those assigned to the senses of a target word is below a certain threshold. If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCor’s MFS if no assignment has been attempted. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al., 2010) and IIITH (Reddy et al., 2010). By complementing our multilingual method with the MFS heuristic we achieve a performance comparable with the state of the art on this task. Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without mul1406 Cross-lingual lexical disambiguation Using a multilingu"
D12-1128,S10-1053,0,0.0229438,"Missing"
D12-1128,S10-1013,0,\N,Missing
D12-1128,W09-2413,0,\N,Missing
D12-1128,P10-1154,1,\N,Missing
D12-1128,S10-1002,0,\N,Missing
D12-1128,S10-1012,0,\N,Missing
D17-1185,E12-1004,0,0.598618,"ymy and meronymy. On the other hand, models for embedding KBs (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2015) uniformly model both symmetric and asymmetric relations. They learn a single vector representation (i.e., embedding) for each KB concept, assuming implicitly that the same concept representation is equally useful for predicting symmetric and asymmetric relations alike. Relation-specific learning-based models have, to the largest extent, targeted hypernymy. Distributional models predict the hypernymy relations by combining raw distributional vectors of concepts in a pair (Baroni et al., 2012; Roller et al., 2014; Santus et al., 2014), whereas path-based models base predictions on lexico-syntactic paths from co-occurrence contexts obtained from a large corpus (Snow et al., 2004; Nakashole et al., 2012; Shwartz et al., 2016). Shwartz et al. (2016) combine the path-based and distributional models to 1757 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1757–1767 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics reach state-of-the-art performance in hypernymy detection. Both distributional and path"
D17-1185,W11-2501,0,0.186963,"prediction performance, we evaluate two reduced models variants. 4.1 Datasets We evaluate the Dual Tensor model on the following hypernymy and meronymy detection datasets: HypeNet dataset. Arguing that existing datasets were too small for training their recurrent network, Shwartz et al. (2016) compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts in direct relation (i.e., no transitive closure). Other hypernymy detection datasets. We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset (Baroni and Lenci, 2011) and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of models trained on larger datasets; WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitiv"
D17-1185,P99-1008,0,0.0561512,"edicting all relations, symmetric and asymmetric alike. By directly updating concept embeddings in training, they cannot make relation predictions for concepts outside of the training set. Hypernymy and Meronymy Detection. Hypernymy and meronymy are arguably the two most prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations us"
D17-1185,E17-1056,1,0.792705,"Missing"
D17-1185,N15-1184,0,0.197212,"Missing"
D17-1185,N13-1092,0,0.0797104,"Missing"
D17-1185,J06-1005,0,0.0287255,"c relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Kotlerman et al., 2010) or"
D17-1185,C92-2082,0,0.374207,"e same concept embeddings for predicting all relations, symmetric and asymmetric alike. By directly updating concept embeddings in training, they cannot make relation predictions for concepts outside of the training set. Hypernymy and Meronymy Detection. Hypernymy and meronymy are arguably the two most prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distrib"
D17-1185,D15-1242,0,0.156284,"Missing"
D17-1185,N15-1098,0,0.260129,"Missing"
D17-1185,N16-1018,0,0.00553929,"Missing"
D17-1185,D12-1104,0,0.00462594,"presentation (i.e., embedding) for each KB concept, assuming implicitly that the same concept representation is equally useful for predicting symmetric and asymmetric relations alike. Relation-specific learning-based models have, to the largest extent, targeted hypernymy. Distributional models predict the hypernymy relations by combining raw distributional vectors of concepts in a pair (Baroni et al., 2012; Roller et al., 2014; Santus et al., 2014), whereas path-based models base predictions on lexico-syntactic paths from co-occurrence contexts obtained from a large corpus (Snow et al., 2004; Nakashole et al., 2012; Shwartz et al., 2016). Shwartz et al. (2016) combine the path-based and distributional models to 1757 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1757–1767 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics reach state-of-the-art performance in hypernymy detection. Both distributional and path-based methods, however, model asymmetry only implicitly (e.g., via the order of embeddings in the concatenation). Besides, path-based models are languagedependent since they require syntactically preprocessed dat"
D17-1185,P06-1015,0,0.0215926,"prominent asymmetric lexico-semantic relations. Methods for their detection can roughly be classified as either distributional or path-based. Pathbased methods consider lexico-syntactic paths con1758 necting pairs of words in their co-occurrence contexts in large corpus. Early approaches, e.g., Hearst (1992) for hypernymy and Berland and Charniak (1999) for meronymy, exploited a small set of manually created lexico-syntactic patterns that imply a relation of interest (e.g., a such as b). Subsequent approaches looked at ways to eliminate the need for manual compilation of extraction patterns. Pantel and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Ko"
D17-1185,E17-1007,0,0.387102,"try and relation-specific embedding specialization. Meronymy classification. We next evaluate the meronymy classification performance of the models on the WN-Me dataset. The results are shown in Table 3. Same as in the case of hypernymy classification, D UAL -T significantly outperforms all three baselines, with S INGLE -T outperforming B ILIN -P ROD. All distributional models we evaluate achieve poorer performance on meronymy than hypernymy detection, especially considering that WN-Me is a balanced dataset, whereas HypeNet is heavily skewed towards negative instances. 4.4 Ranking Experiments Shwartz et al. (2017) propose ranking as an alternative evaluation setting for hypernymy detection. The goal is to rank positive relation pairs higher than negative ones. Our D UAL -T model (and associated baselines) rank the concept pairs in decreasing order of assigned relations scores s(c1 , c2 ). Following Shwartz et al. (2017), we report performance in terms of overall average precision (AP) and average precision at rank 100 (AP@100). Hypernymy ranking. We evaluate the ranking performance on four small hypernymy test sets: BLESS, EVALuation, Benotto, and Weeds (cf. Table 1). As these datasets are not big enou"
D17-1185,D14-1162,0,0.107298,"can be considered distributional as it requires only distributional vectors of words as input. Consequently, in contrast to path-based methods, it is language-independent and more widely applicable. Experimental results on hypernymy and meronymy detection show that the Dual Tensor model outperforms both distributional and path-based models. We additionally demonstrate that our approach exhibits stable performance across languages and can, to some extent, diminish the negative effects of polysemy. 2 Related Work Specializing Word Embeddings. Unspecialized word embeddings (Mikolov et al., 2013; Pennington et al., 2014) capture general semantic properties of words, but are unable to differentiate between different types of semantic relations (e.g., vectors of car and driver might be as similar as vectors of car and vehicle). However, we often need embeddings to be similar only if an exact lexico-semantic relation holds between the words. Numerous methods for specializing word embeddings for particular relations have been proposed (Yu and Dredze, 2014; Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2016, inter alia), primarily aiming to differentiate synonymic similarity from other types of semant"
D17-1185,E14-1054,0,0.0603517,"s. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Kotlerman et al., 2010) or that the linguistics contexts of a hyponym are more informative than the contexts of its hypernyms (Rimell, 2014; Santus et al., 2014). Supervised hypernymy classifiers represent the pair of words by combining their distributional vectors in different ways – concatenating them (Baroni et al., 2012) or subtracting them (Roller et al., 2014) – and feeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hy"
D17-1185,C14-1097,0,0.106143,"Missing"
D17-1185,C14-1212,0,0.255308,"uing that existing datasets were too small for training their recurrent network, Shwartz et al. (2016) compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts in direct relation (i.e., no transitive closure). Other hypernymy detection datasets. We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset (Baroni and Lenci, 2011) and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of models trained on larger datasets; WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitive closure of hypernymy (all parts of speech) and meronymy (nouns) relations and couple them with all synonym and antonym relations (all parts of speech), as well as lexical entailment rela"
D17-1185,W03-1011,0,0.0865055,"and Pennacchiotti (2006) and Girju et al. (2006) proposed bootstrapping approaches to meronymy detection, starting from a seed set of part-whole pairs. Snow et al. (2004) provided all dependency paths connecting the concepts in corpus to a logistic regression classifier for hypernymy detection. Distributional methods detect asymmetric relations using only distributional vectors of words as input. Distributional models come in both unsupervised and supervised flavors. Unsupervised metrics for hypernymy detection assume either that the hyponym’s contexts are included in the hypernym’s contexts (Weeds and Weir, 2003; Kotlerman et al., 2010) or that the linguistics contexts of a hyponym are more informative than the contexts of its hypernyms (Rimell, 2014; Santus et al., 2014). Supervised hypernymy classifiers represent the pair of words by combining their distributional vectors in different ways – concatenating them (Baroni et al., 2012) or subtracting them (Roller et al., 2014) – and feeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-"
D17-1185,W16-5309,0,0.162772,"et al., 2014) – and feeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hybrid model which additionally exploits syntactic information. Distributional and path-based models have been used to discriminate between multiple lexicosemantic relations, including hypernymy and meronymy, at once (Santus et al., 2016; Shwartz and Dagan, 2016). However, as pointed out by (Chersoni et al., 2016), distributional vectors and scores based on their comparison fail to discriminate between multiple relation types at once. In this work, we focus on binary classification for a single relation (hypernymy and meronymy) at a time. 3 Dual Tensor Model The following assumptions and desirable properties guided the design of the Dual Tensor model for detection of asymmetric lexico-semantic relations: (1) Unspecialized distributional vectors are not good signals for detecting specific lexico-semantic relations. We thus nee"
D17-1185,E14-4008,0,0.592214,"s for embedding KBs (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2015) uniformly model both symmetric and asymmetric relations. They learn a single vector representation (i.e., embedding) for each KB concept, assuming implicitly that the same concept representation is equally useful for predicting symmetric and asymmetric relations alike. Relation-specific learning-based models have, to the largest extent, targeted hypernymy. Distributional models predict the hypernymy relations by combining raw distributional vectors of concepts in a pair (Baroni et al., 2012; Roller et al., 2014; Santus et al., 2014), whereas path-based models base predictions on lexico-syntactic paths from co-occurrence contexts obtained from a large corpus (Snow et al., 2004; Nakashole et al., 2012; Shwartz et al., 2016). Shwartz et al. (2016) combine the path-based and distributional models to 1757 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1757–1767 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics reach state-of-the-art performance in hypernymy detection. Both distributional and path-based methods, however, model asymmetry on"
D17-1185,W15-4208,0,0.153154,"models variants. 4.1 Datasets We evaluate the Dual Tensor model on the following hypernymy and meronymy detection datasets: HypeNet dataset. Arguing that existing datasets were too small for training their recurrent network, Shwartz et al. (2016) compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts in direct relation (i.e., no transitive closure). Other hypernymy detection datasets. We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset (Baroni and Lenci, 2011) and EVALuation dataset (Santus et al., 2015) contain instances of hypernymy and four other relations. BLESS additionally contains random word pairs; (2) Weeds dataset (Weeds et al., 2014) contains hypernymy and co-hyponymy pairs; (3) Benotto dataset (Benotto, 2015) couples hypernymy pairs with synonymy and antonymy pairs. Because these datasets contain at most several thousand pairs, we only use them to evaluate the performance of models trained on larger datasets; WN-Hy and WN-Me datasets. We create these datasets by taking concept pairs from WordNet. We take all instances from the transitive closure of hypernymy (all parts of speech)"
D17-1185,W16-5310,0,0.194742,"eeding the resulting vector to a supervised classifier like logistic regression. Most recently, Shwartz et al. (2016) coupled path-based and distributional information with a recurrent neural network (RNN), yielding state-of-the-art hypernymy detection performance. Although our Dual Tensor model is purely distributional, we show that it may outperform such a hybrid model which additionally exploits syntactic information. Distributional and path-based models have been used to discriminate between multiple lexicosemantic relations, including hypernymy and meronymy, at once (Santus et al., 2016; Shwartz and Dagan, 2016). However, as pointed out by (Chersoni et al., 2016), distributional vectors and scores based on their comparison fail to discriminate between multiple relation types at once. In this work, we focus on binary classification for a single relation (hypernymy and meronymy) at a time. 3 Dual Tensor Model The following assumptions and desirable properties guided the design of the Dual Tensor model for detection of asymmetric lexico-semantic relations: (1) Unspecialized distributional vectors are not good signals for detecting specific lexico-semantic relations. We thus need to derive specialized re"
D17-1185,Q15-1025,0,0.134167,"alized and specialized word embeddings via a pair of tensors. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on hypernymy and meronymy detection suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the model can account for polysemy and that it exhibits stable performance across languages. 1 Introduction Detection of semantic relations that hold between words is the central task of lexical semantics, tightly coupled with obtaining representations that capture meaning of words (Mikolov et al., 2013; Wieting et al., 2015; Mrkˇsi´c et al., 2016, inter alia). As such, robust detection of lexico-semantic relations may benefit virtually any natural language processing application. Because lexico-semantic knowledge bases (KBs) like WordNet (Fellbaum, 1998) are general and of limited coverage, numerous methods for detecting lexico-semantic relations rely on distributional word representations obtained from large corpora. Although distributional models have evolved over time, from count-based (Landauer et al., 1998) and generative (Blei et al., 2003) to prediction-based (Mikolov et al., 2013), the similarity between"
D17-1185,C00-2137,0,0.236339,"on the HypeNet dataset (Shwartz et al., 2016). We show the performance of the D UAL -T model in Table 2, together with the path-based and hybrid (combination of path-based and distributional signal) variants of the the state-of-the-art RNN model of Shwartz et al. (2016). On the more challenging, lexically-split dataset D UAL -T model significantly3 outperforms the more complex hybrid HypeNet model (Shwartz et al., 2016), an RNN model coupling representations of syntactic paths from a large corpus with 3 All performance differences were tested using the nonparametric stratified shuffling test (Yeh, 2000) with α = 0.05. 1762 Lex. split Model respect to supervised baselines even more clearly than hypernymy classification results. All supervised models outperform the best unsupervised model in terms of AP, but only D UAL -T is consistently better when considering only 100 top-ranked pairs (AP@100). This adds to the conclusion that explicit modeling of asymmetry using dual tensors yields crucial performance boost. Rand. split P R F1 P R F1 C ONCAT-SVM B ILIN -P ROD S INGLE -T 78.6 73.3 77.7 44.6 50.0 55.5 56.9 59.4 64.8 79.9 81.0 85.7 75.9 79.8 82.6 77.9 80.5 84.1 D UAL -T 76.5 61.1 67.9 87.7 85."
D17-1185,P14-2089,0,0.446633,"from count-based (Landauer et al., 1998) and generative (Blei et al., 2003) to prediction-based (Mikolov et al., 2013), the similarity between distributional vectors still indicates only the abstract semantic association and not a precise semantic relation (e.g., vectors of antonyms may be as similar as vectors of synonyms). Consequently, a number of approaches have been proposed for specializing distributional spaces for specific lexico-semantic relations, either by (1) modifying the learning objective or regularization of the original embedding model by incorporating linguistic constraints (Yu and Dredze, 2014; Kiela et al., 2015) or (2) retroactively fitting the pre-trained unspecialized embeddings to linguistic constraints (Faruqui et al., 2015; Mrkˇsi´c et al., 2016). However, these methods specialize distributional vector spaces primarily for detecting the symmetric relation of semantic similarity (i.e., graded synonymy) and not for asymmetric lexico-semantic relations such as hypernymy and meronymy. On the other hand, models for embedding KBs (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2015) uniformly model both symmetric and asymmetric relations. They learn a single vector represe"
D17-1185,P16-1226,0,0.650181,"Missing"
D17-1318,P04-1085,0,0.0314735,"i et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or pairwise agreement detection from political debates (Menini and Tonelli, 2016). Our Contributions. a) New task: Given a collection of political documents such as, e.g., electoral manifestos, we look at ways to perform an automatic, topic-based agreement-disagreement classification. b) New approach: We first segment the texts into coarse-grained domains. Next, coarse domains are used to extract a fine-grained list of topic-based points of view which, in turn, ar"
D17-1318,S16-2016,1,0.880807,"Missing"
D17-1318,E17-2109,1,0.44123,"Missing"
D17-1318,D13-1191,0,0.0161033,"ical proportions (Sim et al., 2013) and the scaling on a left-right spectrum of politicians’ speeches (Slapin and Proksch, 2008). More recently, researchers looked at topic-centered approaches to provide finer-grained analyses, including segmentation methods for topic-labeled manifestos (Glavaˇs et al., 2016), supporting manual coders in identifying coarse-grained political topics (Zirn et al., 2016), as well as topic-based and cross-lingual political scaling (Nanni et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or"
D17-1318,N03-2012,0,0.0715586,"ˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or pairwise agreement detection from political debates (Menini and Tonelli, 2016). Our Contributions. a) New task: Given a collection of political documents such as, e.g., electoral manifestos, we look at ways to perform an automatic, topic-based agreement-disagreement classification. b) New approach: We first segment the texts into coarse-grained domains. Next, coarse domains are used to extract a fine-grained list of topic-based points of view which, in turn, are used to perform class"
D17-1318,S10-1004,0,0.0705934,"Missing"
D17-1318,C16-1232,1,0.914464,"al., 2013) and the scaling on a left-right spectrum of politicians’ speeches (Slapin and Proksch, 2008). More recently, researchers looked at topic-centered approaches to provide finer-grained analyses, including segmentation methods for topic-labeled manifestos (Glavaˇs et al., 2016), supporting manual coders in identifying coarse-grained political topics (Zirn et al., 2016), as well as topic-based and cross-lingual political scaling (Nanni et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or pairwise agreement detecti"
D17-1318,D14-1162,0,0.08393,"Missing"
D17-1318,D13-1010,0,0.021268,"henomena has gained considerable momentum (Grimmer and Stewart, 2013), arguably because of both the availability of parliamentary proceedings (van Aggelen et al., 2017), electoral manifestos (Volkens et al., 2011) and campaign debates (Woolley and Peters, 2008), and the interest of the computational social science (CSS) community in the potential of text mining methods for advancing political science research (Lazer et al., 2009). Previous work focused on the automatic detection of sentiment expressions in political news (Young and Soroka, 2012), the identification of ideological proportions (Sim et al., 2013) and the scaling on a left-right spectrum of politicians’ speeches (Slapin and Proksch, 2008). More recently, researchers looked at topic-centered approaches to provide finer-grained analyses, including segmentation methods for topic-labeled manifestos (Glavaˇs et al., 2016), supporting manual coders in identifying coarse-grained political topics (Zirn et al., 2016), as well as topic-based and cross-lingual political scaling (Nanni et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and"
D17-2016,J13-3008,0,0.00623738,"Missing"
D17-2016,D12-1129,1,0.834113,"o the semantic class as features. Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class. 3.2 an ambiguous word and its context. The output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. By default, only the best matching sense is displayed. The user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API9 using a query composed of the ambiguous word and its hypernym, e.g. “jaguar animal”. The first hit of this query is selected to represent the induced word sense. Interpretability of each sense is further ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. Figure 2). Note that all these elements are obtained without manual intervention. Finally,"
D17-2016,C92-2082,0,0.232587,"ollections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense usage examples. For more details about the model induction process refer to (Panchenko et al., 2017). Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense. Word senses based"
D17-2016,S13-2049,0,0.0191872,"ical context clues. Each of these elements is extracted automatically. The reasons of the predictions are provided in terms of common sparse features of the input sentence and a sense representation (E). The induced senses are linked to BabelNet using the method of Faralli et al. (2016) (F). Figure 3: All words disambiguation mode: results of disambiguation of all nouns in a sentence. 94 # Words # Senses Avg. Polysemy # Contexts 863 2,708 3.13 11,712 WSD Model Inventory Features Table 1: Evaluation dataset based on BabelNet. methods for WSD, including participants of the SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unsupervised knowledge-free WSD systems based on word sense embeddings (Bartunov et al., 2016; Pelevina et al., 2016). These evaluations were based on the “lexical sample” setting, where the system is expected to predict a sense identifier of the ambiguous word. In this section, we perform an extra evaluation that assesses how well hypernyms of ambiguous words are assigned in context by our system. Namely, the task is to assign a correct hypernym of an ambiguous word, e.g. “animal” for the word “Jaguar” in the context “Jaguar is a large spotted predator of tropical America”. This task"
D17-2016,P13-4007,0,0.351132,"Missing"
D17-2016,Q14-1019,0,0.219564,"retable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework. Implementation of the system is open source.1 A live demo featuring several disambiguation models is available online.2 Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the ga"
D17-2016,D14-1113,0,0.0192822,"nces are used as sense usage examples. For more details about the model induction process refer to (Panchenko et al., 2017). Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense. Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores. Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation. AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory. SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD. Other methods to learn sense embeddings were proposed, but these do not feature open implementations for WSD. Among all listed systems, only Babelfy implements a user inte"
D17-2016,E17-1009,1,0.647167,"ioned features enabling interpretability. For instance, systems based on sense embeddings are based on dense uninterpretable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework. Implementation of the system is open source.1 A live demo featuring several disambiguation models is available online.2 Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements repr"
D17-2016,W16-1620,1,0.761499,"parse features that represent the sense. Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores. Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation. AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory. SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD. Other methods to learn sense embeddings were proposed, but these do not feature open implementations for WSD. Among all listed systems, only Babelfy implements a user interface supporting interpretable visualization of the disambiguation results. 3 Unsupervised Knowledge-Free Interpretable WSD This section describes (1) how WSD models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context. 3 Induction"
D17-2016,W06-3812,1,0.134336,"tecture of the WSD system. As one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework4 , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense"
D17-2016,P15-1173,0,0.0206782,"Missing"
D17-2016,biemann-2012-turk,1,0.85795,"Missing"
D17-2016,P17-1145,1,0.786442,"dge-Free Interpretable WSD This section describes (1) how WSD models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context. 3 Induction of the WSD Models 4 https://github.com/alvations/pywsd 92 http://spark.apache.org Super senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm (Biemann, 2006). The edges in this sense graph are established by disambiguation of the related words (Faralli et al., 2016; Ustalov et al., 2017). The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. “animal”. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional “per word” models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features. Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtain"
D17-2016,P10-4014,0,0.042323,"ntations. Introduction The notion of word sense is central to computational lexical semantics. Word senses can be either encoded manually in lexical resources or induced automatically from text. The former knowledgebased sense representations, such as those found in the BabelNet lexical semantic network (Navigli and Ponzetto, 2012), are easily interpretable by humans due to the presence of definitions, usage examples, taxonomic relations, related words, and images. The cost of such interpretability is that every element mentioned above is encoded Knowledge-Based and/or Supervised Systems IMS (Zhong and Ng, 2010) is a supervised allwords WSD system that allows users to integrate additional features and different classifiers. By default, the system relies on the linear support vector machines with multiple features. The AutoExtend (Rothe and Sch¨utze, 2015) approach can be used to learn embeddings for lexemes and synsets 1 2 91 https://github.com/uhh-lt/wsd http://jobimtext.org/wsd Proceedings of the 2017 EMNLP System Demonstrations, pages 91–96 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Induction of the WSD Models (Scala/Spark): §3.1 WSD Model Index RES"
D19-3034,P19-1070,1,0.888836,"Missing"
D19-3034,Q17-1010,0,0.274436,"ments and queries differently lexicalize concepts. Semantic search seeks to overcome such lexical mismatches between document and user queries (Li and Xu, 2014). Early approaches to semantic search relied on external resources like WordNet (Moldovan and Mihalcea, 2000) and Wikipedia (Strube and Ponzetto, 2006), suffering from the resource’s limited coverage. More recent semantic search systems (Vuli´c and Moens, 2015; Litschko et al., 2018; Nogueira and Cho, 2019) remedy for those coverage issues by encoding text using distributional word vectors (i.e., word embeddings) (Mikolov et al., 2013; Bojanowski et al., 2017) and neural text encoders (Devlin et al., 2018). While there is a plethora of semantic text encoding models, there have been few attempts to empirically compare them on IR tasks. In this 2 We first describe S EAGLE’s semantic encoders, belonging to two categories: word embedding aggregators and pre-trained text encoders. 2.1 Word Embedding Aggregators Word embedding aggregators encode the text by aggregating pretrained d-dimensional embeddings of its words. Formally, a document matrix Vd ∈ RN ×d sequentially stacks embeddings ti ∈ Rd corresponding to tokens ti (i ∈ {1, · · · , N }) of document"
D19-3034,D19-1059,0,0.148074,"rs for Information Retrieval Fabian David Schmidt∗, Markus Dietsche∗, Simone Paolo Ponzetto and Goran Glavaˇs Data and Web Science Group University of Mannheim fabian.david.schmidt@hotmail.de dietsche.markus@googlemail.com {simone, goran}@informatik.uni-mannheim.de Abstract work, we aim to allow for such comparative evaluations on arbitrary IR test collections. We introduce S EAGLE, a platform for concurrent execution and comparative evaluation of semantic search models. S EAGLE implements most recently proposed (1) word embedding aggregation models (Arora et al., 2017; R¨uckl´e et al., 2018; Yang et al., 2019; Zhelezniak et al., 2019) as well as (2) two pretraining-based text encoders (Gysel et al., 2017; Devlin et al., 2018) and allows users to evaluate them on arbitrary IR collections. Coupled with pretrained cross-lingual embedding spaces (Glavaˇs et al., 2019), S EAGLE also supports cross-lingual search. The platform’s modular architecture based on micro-services makes it easy to extend it with additional semantic encoding models. S EAGLE is accessible via an easy-to-use web interface. We introduce S EAGLE,1 a platform for comparative evaluation of semantic text encoding models on information"
E06-2015,J96-1002,0,0.0475453,"Missing"
E06-2015,W05-0620,0,0.0826244,"Missing"
E06-2015,J02-3001,0,0.0296174,"brunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp/ Abstract Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance. 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Kehler et al., 2004, inter alia). Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (Gildea & Jurafsky, 2002; Carreras & M`arquez, 2005, SRL henceforth). This paper explores whether coreference resolution can benefit from SRL, more specifically, which phenomena are affected by such information. The motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973). As an example, consider a sentence from the Automatic Co"
E06-2015,H05-1003,0,0.0621186,"Missing"
E06-2015,N04-1037,0,0.270315,"the report predicate, and It being the AGENT of say, could trigger the (semantic parallelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (REs). This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov & Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitch"
E06-2015,P02-1014,0,0.1233,"80 465 420 #coref ch. 904 399 354 #pron. 1037 358 329 NWIRE #comm. nouns 1210 485 484 #prop. names 2023 923 712 Table 1: Partitions of the ACE 2003 training data corpus ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REi and REj the features are computed as follows2 . (a) Lexical features STRING MATCH T if REi and REj have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REi is a pronoun; else F. J PRONOUN T if REj is a pronoun; else F. J DEF T if REj starts with the; else F. J DEM T if REj starts with this, that, these, or those; else F. NUMBER T if both REi and REj agree in number; else F. GENDE"
E06-2015,J05-1004,0,0.0586635,"antic class matching. Unfortunately, a simple WordNet semantic class lookup exhibits problems such as coverage and sense disambiguation3 , which make the WN CLASS feature very noisy. As a consequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only in case the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument label ARGi is always defined with respect to a predicate lemma predi . Given such level of semantic information available at the RE level, we introduce two new features4 . I SEMROLE the semantic predicate pairs of REi . role argument3"
E06-2015,N04-1030,0,0.0220541,"(d) Distance features DISTANCE how many sentences REi and REj are apart. 2.4 Semantic Role Features The baseline system employs only a limited amount of semantic knowledge. In particular, semantic information is limited to WordNet semantic class matching. Unfortunately, a simple WordNet semantic class lookup exhibits problems such as coverage and sense disambiguation3 , which make the WN CLASS feature very noisy. As a consequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only in case the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument l"
E06-2015,J01-4004,0,0.454657,"lud1 We used the training data corpus only, as the availability of the test data was restricted to ACE participants. 143 TRAIN. DEVEL TEST #coref ch. 587 201 228 #pron. 876 315 291 BNEWS #comm. nouns 572 163 238 #prop. names 980 465 420 #coref ch. 904 399 354 #pron. 1037 358 329 NWIRE #comm. nouns 1210 485 484 #prop. names 2023 923 712 Table 1: Partitions of the ACE 2003 training data corpus ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REi and REj the features are computed as follows2 . (a) Lexical features STRING MATCH T if REi and REj have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REi"
E06-2015,M95-1005,0,0.0729664,"ototyping we experimented unpairing the arguments from the predicates, which yielded worse results. This is supported by the PropBank arguments always being defined with respect to a target predicate. Binarizing the features — i.e. do REi and REj have the same argument or predicate label with respect to their closest predicate? — also gave worse results. 144 original Soon et al. duplicated baseline R 58.6 MUC-6 P F1 67.3 62.3 64.9 65.6 65.3 R 56.1 MUC-7 P F1 65.5 60.4 55.1 68.5 61.1 baseline +SRL role 3.1 argumentExperiments Performance Metrics We report in the following tables the MUC score (Vilain et al., 1995). Scores in Table 2 are computed for all noun phrases appearing in either the key or the system response, whereas Tables 3 and 4 refer to scoring only those phrases which appear in both the key and the response. We discard therefore those responses not present in the key, as we are interested here in establishing the upper limit of the improvements given by SRL. We also report the accuracy score for all three types of ACE mentions, namely pronouns, common nouns and proper names. Accuracy is the percentage of REs of a given mention type correctly resolved divided by the total number of REs of t"
E17-1009,D14-1110,0,0.0391811,"e amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of con"
E17-1009,de-marneffe-etal-2006-generating,0,0.0297452,"Missing"
E17-1009,J13-3008,0,0.232296,"Missing"
E17-1009,S13-2050,0,0.0233307,"del Precision Recall F-score Precision Recall F-score Dependencies Dependencies Exp. 0.728 0.687 0.343 0.633 0.466 0.659 0.432 0.414 0.190 0.379 0.263 0.396 Dependencies + LM Dependencies Exp. + LM 0.689 0.684 0.681 0.676 0.685 0.680 0.426 0.412 0.422 0.408 0.424 0.410 Table 2: Effect of the feature expansion: performance on the full (on the left) and the sense-balanced (on the right) TWSI datasets. The models were trained on the Wikipedia corpus using the coarse inventory (1.96 senses per word). The best results overall are underlined. 4.2 generated using word embeddings6 . The AI-KU system (Baskaya et al., 2013) directly clusters test contexts using the k-means algorithm based on lexical substitution features. The Unimelb system (Lau et al., 2013) uses one hierarchical topic model to induce and disambiguate senses of one word. The UoS system (Hope and Keller, 2013) induces senses by building an ego-network of a word using dependency relations, which is subsequently clustered using the MaxMax clustering algorithm. The La Sapienza system (Jurgens and Klapaftis, 2013), relies on WordNet for the sense inventory and disambiguation. In contrast to the TWSI evaluation, the most fine-grained model yields the"
E17-1009,J93-1003,0,0.524247,"syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another word. We extract the list of words that significantly co-occur in a sentence with the target word in the input corpus based on the log-likelihood as word-feature weight (Dunning, 1993). Language Model Feature. This type of features are based on a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995). In particular, a word is represented by (1) right and left context words, e.g. “office • and”, (2) two preceding words “new office •”, and (3) two succeeding words, e.g. “• and chairs”. We use the conditional probabilities of the resulting trigrams as word-feature weights. Our approach was inspired by the knowledgebased system Babelfy (Moro et al., 2014). While the inventory of Babelfy is interpretable as it relies on BabelNet, the system provides no underlying reasons"
E17-1009,D12-1129,1,0.811774,"scores: Pthe hypernym relevance score, calculated as w∈cluster sim(t, w)f req(w, h), and Pthe hypernym coverage score, calculated Here the as w∈cluster min(f req(w, h), 1). sim(t, w) is the relatedness of the cluster word w to the target word t, and the f req(w, h) is the frequency of the hypernymy relation (w, h) as extracted via patterns. Thus, a high-ranked hypernym h has high relevance, but also is confirmed by several cluster words. This stage results in a ranked list of labels that specify the word sense, for which we here show the first one, e.g. “table (furniture)” or “table (data)”. Faralli and Navigli (2012) showed that web search engines can be used to bootstrap senserelated information. To further improve interpretability of induced senses, we assign an image to each word in the cluster (see Figure 2) by queryWord Sense Induction We induce a sense inventory by clustering of egonetwork of similar words. In our case, an inventory represents senses by a word cluster, such as “chair, bed, bench, stool, sofa, desk, cabinet” for the “furniture” sense of the word “table”. The sense induction processes one word t of the distributional thesaurus T per iteration. First, we retrieve nodes of the ego-netwo"
E17-1009,W06-3812,1,0.735982,"s, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller, 2013). Graph edges represent semantic relations between words derived using corpus-bas"
E17-1009,biemann-2012-turk,1,0.869612,"sociated with the senses are retrieved using a search engine:“table data” and “table furniture”. 4 2. Sense feature representation. Each sense in our model is characterized by a list of sparse features ordered by relevance to the sense. Figure 2 (2) shows most salient dependency features to senses of the word “table”. These feature representations are obtained by aggregating features of sense cluster words. Experiments We use two lexical sample collections suitable for evaluation of unsupervised WSD systems. The first one is the Turk Bootstrap Word Sense Inventory (TWSI) dataset introduced by Biemann (2012). It is used for testing different configurations of our approach. The second collection, the SemEval 2013 word sense induction dataset by Jurgens and Klapaftis (2013), is used to compare our approach to existing systems. In both datasets, to measure WSD performance, induced senses are mapped to gold standard senses. In experiments with the TWSI dataset, the models were trained on the Wikipedia corpus4 while in experiments with the SemEval datasets models are trained on the ukWaC corpus (Baroni et al., 2009) for a fair comparison with other participants. In systems based on dense vector repres"
E17-1009,C92-2082,0,0.102793,"y a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions over features, features can vice versa be characterized by a distribution over words. We use this duality to compute feature similarities using the same mechanism and explore their use in disambiguation below. 3.3 3.4 Labeling Induced Senses with Hypernyms and Images Each sense cluster is automatically labeled to improve its interpretability. First, we extract hypernyms from the input corpus using Hearst (1992) patterns. Second, we rank hypernyms relevant to the cluster by a product of two scores: Pthe hypernym relevance score, calculated as w∈cluster sim(t, w)f req(w, h), and Pthe hypernym coverage score, calculated Here the as w∈cluster min(f req(w, h), 1). sim(t, w) is the relatedness of the cluster word w to the target word t, and the f req(w, h) is the frequency of the hypernymy relation (w, h) as extracted via patterns. Thus, a high-ranked hypernym h has high relevance, but also is confirmed by several cluster words. This stage results in a ranked list of labels that specify the word sense, fo"
E17-1009,N15-1059,0,0.0197223,"word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are"
E17-1009,P12-1092,0,0.0637845,"lly from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words"
E17-1009,P15-1010,0,0.0282997,"es rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-p"
E17-1009,P14-2050,0,0.0238238,"rameters and its comparable performance on the WSI task to the state-of-theart (Di Marco and Navigli, 2013). 2013) as it yields comparable performance on semantic similarity to state-of-the-art dense representations (Mikolov et al., 2013) compared on the WordNet as gold standard (Riedl, 2016), but is interpretable as word are represented by sparse interpretable features. Namely we use dependencybased features as, according to prior evaluations, this kind of features provides state-of-the-art semantic relatedness scores (Pad´o and Lapata, 2007; Van de Cruys, 2010; Panchenko and Morozova, 2012; Levy and Goldberg, 2014). First, features of each word are ranked using the LMI metric (Evert, 2005). Second, the word representations are pruned keeping 1000 most salient features per word and 1000 most salient words per feature. The pruning reduces computational complexity and noise. Finally, word similarities are computed as a number of common features for two words. This is again followed by a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions over features, features can"
E17-1009,J98-1001,0,0.0520264,"Missing"
E17-1009,D15-1200,0,0.0789216,"for Word Sense Induction and Disambiguation Alexander Panchenko‡ , Eugen Ruppert‡ , Stefano Faralli† , Simone Paolo Ponzetto† and Chris Biemann‡ ‡ † Language Technology Group, Computer Science Dept., University of Hamburg, Germany Web and Data Science Group, Computer Science Dept., University of Mannheim, Germany {panchenko,ruppert,biemann}@informatik.uni-hamburg.de {faralli,simone}@informatik.uni-mannheim.de Abstract Word sense induction from domain-specific corpora is a supposed to solve this problem. However, most approaches to word sense induction and disambiguation, e.g. (Sch¨utze, 1998; Li and Jurafsky, 2015; Bartunov et al., 2016), rely on clustering methods and dense vector representations that make a WSD model uninterpretable as compared to knowledge-based WSD methods. Interpretability of a statistical model is important as it lets us understand the reasons behind its predictions (Vellido et al., 2011; Freitas, 2014; Li et al., 2016). Interpretability of WSD models (1) lets a user understand why in the given context one observed a given sense (e.g., for educational applications); (2) performs a comprehensive analysis of correct and erroneous predictions, giving rise to improved disambiguation"
E17-1009,S13-2049,0,0.14673,"el is characterized by a list of sparse features ordered by relevance to the sense. Figure 2 (2) shows most salient dependency features to senses of the word “table”. These feature representations are obtained by aggregating features of sense cluster words. Experiments We use two lexical sample collections suitable for evaluation of unsupervised WSD systems. The first one is the Turk Bootstrap Word Sense Inventory (TWSI) dataset introduced by Biemann (2012). It is used for testing different configurations of our approach. The second collection, the SemEval 2013 word sense induction dataset by Jurgens and Klapaftis (2013), is used to compare our approach to existing systems. In both datasets, to measure WSD performance, induced senses are mapped to gold standard senses. In experiments with the TWSI dataset, the models were trained on the Wikipedia corpus4 while in experiments with the SemEval datasets models are trained on the ukWaC corpus (Baroni et al., 2009) for a fair comparison with other participants. In systems based on dense vector representations, there is no straightforward way to get the most salient features of a sense, which makes the analysis of learned representations problematic. 3. Disambiguat"
E17-1009,N16-1082,0,0.0684958,"n}@informatik.uni-hamburg.de {faralli,simone}@informatik.uni-mannheim.de Abstract Word sense induction from domain-specific corpora is a supposed to solve this problem. However, most approaches to word sense induction and disambiguation, e.g. (Sch¨utze, 1998; Li and Jurafsky, 2015; Bartunov et al., 2016), rely on clustering methods and dense vector representations that make a WSD model uninterpretable as compared to knowledge-based WSD methods. Interpretability of a statistical model is important as it lets us understand the reasons behind its predictions (Vellido et al., 2011; Freitas, 2014; Li et al., 2016). Interpretability of WSD models (1) lets a user understand why in the given context one observed a given sense (e.g., for educational applications); (2) performs a comprehensive analysis of correct and erroneous predictions, giving rise to improved disambiguation models. The contribution of this paper is an interpretable unsupervised knowledge-free WSD method. The novelty of our method is in (1) a technique to disambiguation that relies on induced inventories as a pivot for learning sense feature representations, (2) a technique for making induced sense representations interpretable by labeli"
E17-1009,P03-1054,0,0.0189642,"g them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another word. We extract the list of words that significantly co-occur in a sentence with the target word in the input corpus based on the log-likelihood as word-feature weight (Dunning, 1993). Language Model Feature. This type of features are based on a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995). In particular, a word is represented by (1) right and left context words,"
E17-1009,W02-0811,0,0.0372423,"resting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical"
E17-1009,C12-1109,1,0.808148,"ervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e"
E17-1009,S13-2051,0,0.175444,"med by assigning the sense with the highest overlap between the instance’s context words and the words of the sense cluster. V´eronis (2004) compiles a corpus with contexts of polysemous nouns using a search engine. A word graph is built by drawing edges between co-occurring words in the gathered corpus, where edges below a certain similarity threshold were discarded. His HyperLex algorithm detects hubs of this graph, which are interpreted as word senses. Disambiguation is this experiment is performed by computing the distance between context words and hubs in this graph. Di Marco and Navigli (2013) presents a comprehensive study of several graph-based WSI methods including Chinese Whispers, HyperLex, curvature clustering (Dorow et al., 2005). Besides, 87 3 authors propose two novel algorithms: Balanced Maximum Spanning Tree Clustering and Squares (B-MST), Triangles and Diamonds (SquaT++). To construct graphs, authors use first-order and second-order relations extracted from a background corpus as well as keywords from snippets. This research goes beyond intrinsic evaluations of induced senses and measures impact of the WSI in the context of an information retrieval via clustering and di"
E17-1009,S15-2049,0,0.00630219,"re. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other unsupervised systems while offering the possibility to justify its decisions in human-readable form. 1 Introduction A word sense disambiguation (WSD) system takes as input a target word t and its context C. The system returns an identifier of a word sense si from the word sense inventory {s1 , ..., sn } of t, where the senses are typically defined manually in advance. Despite significant progress in methodology during the two last decades (Ide and V´eronis, 1998; Agirre and Edmonds, 2007; Moro and Navigli, 2015), WSD is still not widespread in applications (Navigli, 2009), which indicates the need for further progress. The difficulty of the problem largely stems from the lack of domain-specific training data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017."
E17-1009,W02-1006,0,0.0721617,"later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic"
E17-1009,Q14-1019,0,0.377872,"l, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word"
E17-1009,P10-1023,1,0.451602,"lar, we extract three types of features: Interpretable approaches. The need in methods that interpret results of opaque statistical models is widely recognised (Vellido et al., 2011; Vellido et al., 2012; Freitas, 2014; Li et al., 2016; Park et al., 2016). An interpretable WSD system is expected to provide (1) a human-readable sense inventory, (2) human-readable reasons why in a given context c a given sense si was detected. Lexical resources, such as WordNet, solve the first problem by providing manually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to Babel"
E17-1009,W16-1620,1,0.723784,"difficulty of the problem largely stems from the lack of domain-specific training data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Related Work 2014; Bartunov et al., 2016; Li and Jurafsky, 2015; Pelevina et al., 2016). Comparisons of the AdaGram (Bartunov et al., 2016) to (Neelakantan et al., 2014) on three SemEval word sense induction and disambiguation datasets show the advantage of the former. For this reason, we use AdaGram as a representative of the state-of-the-art word sense embeddings in our experiments. In addition, we compare to SenseGram, an alternative sense embedding based approach by Pelevina et al. (2016). What makes the comparison to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their"
E17-1009,D14-1113,0,0.0333137,"ng data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Related Work 2014; Bartunov et al., 2016; Li and Jurafsky, 2015; Pelevina et al., 2016). Comparisons of the AdaGram (Bartunov et al., 2016) to (Neelakantan et al., 2014) on three SemEval word sense induction and disambiguation datasets show the advantage of the former. For this reason, we use AdaGram as a representative of the state-of-the-art word sense embeddings in our experiments. In addition, we compare to SenseGram, an alternative sense embedding based approach by Pelevina et al. (2016). What makes the comparison to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre"
E17-1009,W97-0323,0,0.288821,"son to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingu"
E17-1009,P15-1173,0,0.186255,"Missing"
E17-1009,W16-1401,0,0.0436871,"Missing"
E17-1009,P15-4018,1,0.84569,"-readable reasons why in a given context c a given sense si was detected. Lexical resources, such as WordNet, solve the first problem by providing manually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert,"
E17-1009,J07-2002,0,0.101588,"Missing"
E17-1009,W12-0502,1,0.771556,"ated by the absence of meta-parameters and its comparable performance on the WSI task to the state-of-theart (Di Marco and Navigli, 2013). 2013) as it yields comparable performance on semantic similarity to state-of-the-art dense representations (Mikolov et al., 2013) compared on the WordNet as gold standard (Riedl, 2016), but is interpretable as word are represented by sparse interpretable features. Namely we use dependencybased features as, according to prior evaluations, this kind of features provides state-of-the-art semantic relatedness scores (Pad´o and Lapata, 2007; Van de Cruys, 2010; Panchenko and Morozova, 2012; Levy and Goldberg, 2014). First, features of each word are ranked using the LMI metric (Evert, 2005). Second, the word representations are pruned keeping 1000 most salient features per word and 1000 most salient words per feature. The pruning reduces computational complexity and noise. Finally, word similarities are computed as a number of common features for two words. This is again followed by a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions ov"
E17-1009,J98-1004,0,0.194444,"Missing"
E17-1009,L16-1421,1,0.84598,"nually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another"
E17-1009,C14-1016,0,0.0529438,"supervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the tar"
E17-1009,C02-1114,0,0.0171122,"text clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller, 2013). Graph edges represent semantic relations between words derived u"
E17-1009,P10-4014,0,0.0897347,"but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and P"
E17-1009,W97-0322,0,\N,Missing
E17-1056,D13-1018,1,0.87749,"ntrastMedium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just a Few Links Stefano Faralli1 , Alexander Panchenko2 , Chris Biemann2 and Simone Paolo Ponzetto1 1 Data and Web Science Group, University of Mannheim, Germany Language Technology Group, University of Hamburg, Germany {stefano,simone}@informatik.uni-mannheim.de {panchenko,biemann}@informatik.uni-hamburg.de 2 Abstract closed information extraction approaches (Dutta et al., 2014). The use of an encyclopedia-centric (e.g., Wikipedia-based) dictionary of entities leads to poor coverage of domain-specific terminologies (Faralli and Navigli, 2013). This can be alleviated by constructing knowledge bases of ever increasing coverage and complexity from the Web (Wu et al., 2012; Gupta et al., 2014; Dong et al., 2014) or by community efforts (Bollacker et al., 2008). However, the focus on large size and wide coverage at entity level has led all these resources to avoid the complementary problem of curating and maintaining a clean taxonomic backbone with as minimal supervision as possible. That is, no resource, to date, integrates structured information from existing wide-coverage knowledge graphs with empirical evidence from text for the ex"
E17-1056,S15-2151,1,0.925043,"Missing"
E17-1056,P99-1016,0,0.48042,"cit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. Co"
E17-1056,N15-1151,0,0.0311339,"Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011; Faruqui and Kumar, 2015, inter alia); In this paper, we focus on a different, yet complementary task, which is a necessary step when inducing novel KBs from scratch, namely extracting clean taxonomies from noisy knowlSome aspects of the proposed approach – namely, the propagation of the nodes’ weights through the graph, which we metaphorically represent as the flow of a contrast medium across nodes (Section 3.3) – are somewhat similar in spirit to spreading activation (Collins and Loftus, 1975) and random walks on graphs (Lov´asz, 1993) approaches. However, in contrast to spreading activation approaches we leverage"
E17-1056,S16-1206,1,0.913086,"oisy edges, the wrongly-acquired relations between unrelated concepts or out-of-domain relations, e.g., Jaguar Cars isa Feline; iii) cycles of hypernymy relations, such as those derived from counts over very large corpora (Seitner et al., 2016), e.g., jaguar (Panthera onca) → feline → animal → jaguar (Panthera onca). We accordingly define the task of extracting a clean taxonomy from a NKG as that of pruning the cycles, as well as the noisy edges and nodes, from the hypernymy subgraph T of G. 3.2 vised methods. To this end, we use the linked disambiguated distributional KBs from Faralli et al. (2016)1 , which are built in three steps: 1) Learning a JoBimText model. Initially, a sense inventory is created from a large text collection using the pipeline of the JoBimText project (Biemann and Riedl, 2013).2 The resulting structure contains disambiguated protoconcepts (i.e., senses), their similar and related terms, as well as aggregated contextual clues per proto-concept. 2) Disambiguation of related terms. Similar terms and hypernyms associated with a protoconcept are fully disambiguated based on the partial disambiguation from step (1). The result is a proto-conceptualization (PCZ), where a"
E17-1056,C92-2082,0,0.315937,"rmation from existing wide-coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present Contr"
E17-1056,L16-1056,1,0.855973,"e been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manual"
E17-1056,P06-1101,0,0.34143,"mapping (Navigli and Ponzetto, 2012; Faralli et al., 2016, inter alia) or by relying on ground truth information from the Linguistic Linked Open Data cloud (Chiarcos et al., 2012). Knowledge Bases (KBs) can be created in many different ways depending on the availability of external resources and specific application needs. Recently, much work in Natural Language Processing focused on Knowledge Base Completion (Nickel et al., 2016a, KBC), the task of enriching and refining existing KBs. Many different methods have been explored for KBC, including exploitation of resources such as text corpora (Snow et al., 2006; Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011;"
E17-1056,D10-1108,0,0.212701,"coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms"
E17-1056,P09-1113,0,0.091549,"d Ponzetto, 2012; Faralli et al., 2016, inter alia) or by relying on ground truth information from the Linguistic Linked Open Data cloud (Chiarcos et al., 2012). Knowledge Bases (KBs) can be created in many different ways depending on the availability of external resources and specific application needs. Recently, much work in Natural Language Processing focused on Knowledge Base Completion (Nickel et al., 2016a, KBC), the task of enriching and refining existing KBs. Many different methods have been explored for KBC, including exploitation of resources such as text corpora (Snow et al., 2006; Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011; Faruqui and Kumar,"
E17-1056,velardi-etal-2012-new,1,0.878279,"Missing"
E17-1056,J13-3007,1,0.916583,"a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure"
E17-1056,D11-1142,0,\N,Missing
E17-2083,N15-1151,0,0.0215897,"observe what textual patterns hold between them. They then associate the recognized patterns to particular KB relations and finally search the corpus for other entity pairs mentioned using the same patterns (Snow et al., 2004; Snow et al., ; Mintz et al., 2009; Aprosio et al., 2013). A slight modification is the approach by (West et al., 2014) where lexicalized KB relations are posed as queries to a search engine and results are parsed to find pairs of entities between which the initially queried relation holds. Complementary to this, open information extraction methods (Etzioni et al., 2011; Faruqui and Kumar, 2015) extract large amounts of facts from text that can then be used for extending KBs (Dutta et al., 2014). Text-centered approaches, however, simply cannot capture knowledge that is rarely made explicit in texts. For example, much of the common-sense knowledge that is obvious to people such as, for instance, that bananas are yellow or that humans breath are rarely (or never) made explicit in textual corpora. A partial solution to this problem is provided by internal approaches that primarily rely on existing information in the KB itself (Bordes et al., 2011; Jenatton et al., 2012; Socher et al.,"
E17-2083,N15-1184,0,0.0558918,"Missing"
E17-2083,D15-1038,0,0.0305965,"and multilingual embedding space. Our results indicate that using cross-lingual links between entity lexicalizations in different languages yields better NTNKBC model. That is, our experiments imply that the cross-lingual signal enabled through the multilingual KB and shared multilingual embedding space provides improved regularization for the neural KBC model. We intend to investigate whether such cross-lingual regularization can yield similar improvements for other neural KBC models and whether it can be combined with other types of regularization, such as that based on augmenting KB paths (Guu et al., 2015). We will also evaluate the cross-lingually extended KB-embedding models on other high-level tasks such as error detection and KB consistency checking. 520 References Alessio Palmero Aprosio, Claudio Giuliano, and Alberto Lavelli. 2013. Extending the coverage of DBpedia properties using distant supervision over Wikipedia. In Proceedings of the 2013 Workshop on Natural Language Processing and DBpedia, pages 20–31, Trento, Italy. Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora"
E17-2083,P09-1113,0,0.0187637,"ethods typically employ a form of a distant supervi516 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 516–522, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics sion. They first recognize mentions of pairs of KB entities in text and observe what textual patterns hold between them. They then associate the recognized patterns to particular KB relations and finally search the corpus for other entity pairs mentioned using the same patterns (Snow et al., 2004; Snow et al., ; Mintz et al., 2009; Aprosio et al., 2013). A slight modification is the approach by (West et al., 2014) where lexicalized KB relations are posed as queries to a search engine and results are parsed to find pairs of entities between which the initially queried relation holds. Complementary to this, open information extraction methods (Etzioni et al., 2011; Faruqui and Kumar, 2015) extract large amounts of facts from text that can then be used for extending KBs (Dutta et al., 2014). Text-centered approaches, however, simply cannot capture knowledge that is rarely made explicit in texts. For example, much of the c"
E17-2083,D12-1128,1,0.816098,"Missing"
E17-2083,P16-1157,0,0.0261236,"KB completion, however, make no use of cross-lingual links between entities, which are readily available in existing multilingual resources like BabelNet (Navigli and Ponzetto, 2012b). Here, we extend the model of Socher et al. (2013) with cross-lingual links from BabelNet and demonstrate how introducing additional (cross-lingual) knowledge through these links improves the reasoning over the KB in terms of better performance on the link prediction task. Our findings are, in turn, different yet complementary to those found by building crosslingual embeddings using parallel or comparable data (Upadhyay et al., 2016) or KB-centric multilingual joint approaches to word understanding like, for instance, that of Navigli and Ponzetto (2012b). Assuming that each monolingual embedding space captures a slightly different aspect of a relation between same concepts, by introducing cross-lingual links over a shared embedding space we believe we provide an additional external regularization mechanism for the NTNKBC model. 3 Cross-Lingual Information for Knowledge Base Completion In Figure 1 we highlight the main steps of our cross-lingual extension of the NTNKBC model. We first use BabelNet to translate KB triples u"
E17-2083,C00-2137,0,0.0118443,".5K triples vs. 33.4K entities and 92K triples for both German and Italian). The Italian monolingual model (Mono-IT) outperforms the German monolingual model (Mono-DE) despite comparable training set sizes, which we credit to the lower quality of the DE→EN translation matrix in comparison with the IT→EN translation matrix (see Table 2). The multilingual model outperforms only one of the three monolingual models. This is not so surprising (although it might seem so at first glance) if 2 All performance differences were tested for significance using the non-parametric stratified shuffling test (Yeh, 2000). we consider that ML-NTN merely combines three disjoint KBs which share semantic information only through shared embedding space and relation tensors. Without the direct, cross-lingual links between entities of different monolingual KBs, these signals seem to be insufficient to compensate for a much larger number of parameters (three times larger number of entities) that the ML-NTN model has to learn compared to monolingual models. The cross-lingual model (CL-NTN), on the other hand, significantly outperforms all monolingual models. We believe that this is because by adding cross-lingual trip"
E17-2109,W16-2102,1,0.397961,"Missing"
E17-2109,D14-1181,0,0.00505458,"ed models (Kellstedt, 2000; Young and Soroka, 2012), supervised classification models (Purpura and Hillard, 2006; Stewart and Zhukov, 2009; Verberne et al., 2014; Karan et al., 2016), and unsupervised scaling models (Slapin and Proksch, 2008; Proksch and Slapin, 2010). All of these models use the discrete, word-based representations of text. Recently, however, continuous semantic text representations (Mikolov et al., 2013b; Le and Mikolov, 2014; Kiros et al., 2015; Mrkˇsi´c et al., 2016) outperformed word-based text representations on a battery of mainstream natural language processing tasks (Kim, 2014; Bordes et al., 2014; Tang et al., 2016). Although the idea of automated estimation of ideological beliefs is old (Abelson and Carroll, 1965), models estimating these beliefs from texts have only appeared in the last fifteen years (Laver and Garry, 2000; Laver et al., 2003; Slapin and Proksch, 2008; Proksch and Slapin, 2010). In the pioneering work on political text scaling, Laver and Garry (2000) used predefined dictionaries of words labeled with position scores. They then scored documents by aggregating the scores of dictionary words they contain. Extending this work, they proposed the mode"
E17-2109,N16-1018,0,0.0230683,"Missing"
E17-2109,D14-1162,0,0.0810828,"Missing"
E17-2109,C00-2137,0,0.225784,"observations from (Proksch and Slapin, 2010). The same holds for our alignment model (A LIGN HFLP). In contrast, the scaling based on the aggregation similarity measure (AGG -HFLP) seems to better correspond to the left-to-right ideological positioning. We hypothesize that this is because the comparison between semantically more imprecise aggregated text embeddings assigns more weight to the most salient dimension of speeches, which we speculate is the ideological position. In contrast, by comparing semantically more precise word em7 According to the non-parametric stratified shuffling test (Yeh, 2000) 692 Conclusion In this work, we presented what is, to the best of our knowledge, the first approach for cross-lingual scaling of political texts. We induce a multilingual embedding space and compute semantic similarities for all pairs of texts using unsupervised measures for semantic textual similarity. We then use a graph-based score propagation algorithm to transform pairwise similarities into position scores. Experimental results from the straightforward quantitative evaluation we propose show that our semantically-informed scaling predicts party positions for two relevant political dimens"
J19-3002,2016.gwc-1.10,1,0.89418,"where nodes are words and edges are synonymy relationships. Synsets represent word senses and are building blocks of thesauri and lexical ontologies, such as WordNet (Fellbaum 1998). These resources are crucial for many NLP applications that require common sense reasoning, such as information retrieval (Gong, Cheang, and Hou U 2005), sentiment analysis (Montejo-R´aez et al. 2014), and question answering (Kwok, Etzioni, and Weld 2001; Zhou et al. 2013). For most languages, no manually constructed resource is available that is comparable to the English WordNet in terms of coverage and quality (Braslavski et al. 2016). For instance, Kiselev, Porshnev, and Mukhin (2015) present a comparative analysis of lexical resources available for the Russian language, concluding that there is no resource compared with WordNet in terms of completeness and availability for Russian. This lack of linguistic resources for many languages strongly motivates the development of new methods for automatic construction of WordNet-like resources. In this section, we apply WATSET for unsupervised synset induction from a synonymy graph and compare it with state-of-the-art graph clustering algorithms run on the same task. 441 Computat"
J19-3002,E17-2036,0,0.0177047,"an observe a Zipfian-like power-law (Zipf 1949) distribution with a few clusters, such as artifact and person, accounting for a large fraction of all nouns in the resource. Overall, in this experiment we decided to focus on nouns, as the input distributional thesauri used in this experiment (as presented in Section 6.2) are most studied for modeling of noun semantics (Panchenko et al. 2016b). The WordNet supersenses were applied later also for word sense disambiguation as a system of broad sense labels (Flekova and Gurevych 2016). For BabelNet, there is a similar data set called BabelDomains (Camacho-Collados and Navigli 2017) produced by automatically labeling BabelNet synsets with 32 different domains based on the topics of Wikipedia featured articles. Despite the larger size, however, BabelDomains provides only a silver standard (being semi-automatically created). We thus opt in the following to use WordNet supersenses only, because they provide instead a gold standard created by human experts. 6.1.2 Flat Cuts of the WordNet Taxonomy. The second type of semantic classes used in our study are more semantically specific and defined as subtrees of WordNet at some fixed 462 Ustalov et al. WATSET: Local-Global Graph"
J19-3002,N13-1104,0,0.067555,"Missing"
J19-3002,W03-1022,0,0.139544,"tly. Examples of concrete semantic classes include sets of animals (dog, cat, . . . ), vehicles (car, motorcycle, . . . ), and fruit trees (apple tree, peach tree, . . . ). In this experiment, we use a gold standard derived from a reference lexicographical database, namely, WordNet (Fellbaum 1998). 35 The examples are from the file triw2v-watset-n30-top-top-triples.txt is available in the “Downloads” section of our GitHub repository at https://github.com/uhh-lt/triframes. 461 Computational Linguistics Volume 45, Number 3 Figure 14 A summary of the noun semantic classes in WordNet supersenses (Ciaramita and Johnson 2003). This allows us to benchmark the ability of WATSET to reconstruct the semantic lexicon of such a reliable reference resource that has been widely used in NLP for many decades. 6.1.1 WordNet Supersenses. The first data set used in our experiments consists of 26 broad semantic classes, also known as supersenses in the literature (Ciaramita and Johnson 2003): person, communication, artifact, act, group, food, cognition, possession, location, substance, state, time, attribute, object, process, tops, phenomenon, event, quantity, motive, animal, body, feeling, shape, plant, and relation. This syste"
J19-3002,N16-1172,0,0.0539531,"Missing"
J19-3002,E17-2028,0,0.0465007,"Missing"
J19-3002,J14-1002,0,0.075833,"Missing"
J19-3002,de-marneffe-etal-2006-generating,0,0.0378146,"Missing"
J19-3002,J13-3008,0,0.072588,"Missing"
J19-3002,E03-1020,0,0.152312,"of networks of linguistic data, such as word co-occurrence graphs, especially those that were used for induction of linguistic structures such as word senses. Markov Clustering (MCL; van Dongen 2000) is a hard clustering algorithm, that is, a method that partitions nodes of the graph in a set of disjoint clusters. This method is based on simulation of stochastic flow in graphs. MCL simulates random walks within a graph by the alternation of two operators, called expansion and inflation, which recompute the class labels. Notably, it has been successfully used for the word sense induction task (Dorow and Widdows 2003). Chinese Whispers (CW; Biemann 2006, 2012) is a hard clustering algorithm for weighted graphs, which can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority of labels among the neighboring nodes. The algorithm has a hyperparameter that controls graph weights, which can be set to three values: (1) CWtop sums 425 Computational Linguistics Volume 45, Number 3 over the neighborhood’s classes; (2) CWlin downgrades the influence of a neighboring node by its degree; or (3) CWlog by the logari"
J19-3002,P18-1128,0,0.152993,"443 Computational Linguistics Volume 45, Number 3 Table 7 Statistics of the gold standard data sets used in our experiments. Resource Language # words # synsets # pairs WordNet BabelNet English 148,730 11,710,137 117,659 6,667,855 152,254 28,822,400 RuWordNet YARN Russian 110,242 9,141 49,492 2,210 278,381 48,291 toolkit (Seabold and Perktold 2010).19 Since the hypothesis tested by the McNemar’s test is whether the results from both algorithms are similar against the alternative that they are not, we use the p-value of this test to assess the significance of the difference between F1 -scores (Dror et al. 2018). We consider the performance of one algorithm to be higher than the performance of another if its F1 -score is larger and the corresponding p-value is smaller than a significance level of 0.01. Gold Standards. We conduct our evaluation on four lexical semantic resources for two different natural languages. Statistics of the gold standard data sets are present in Table 7. We report the number of lexical units (# words), synsets (# synsets), and the generated synonymy pairs (# pairs). We use WordNet,20 a popular English lexical database constructed by expert lexicographers (Fellbaum 1998). Word"
J19-3002,N15-1184,0,0.0489558,"eights based on the cosine of the angle between Skip-Gram word vectors (Mikolov et al. 2013), we should note that such an approach assigns high values of similarity not just to synonymous words, but to antonymous and generally any lexically related words. This is a common problem with lexical embedding spaces, which we tried to evade by explicitly using a synonymy dictionary as an input. For example, “audio play” and “radio play,” or “accusative” and “oblique,” are semantically related expressions, but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -"
J19-3002,P16-1191,0,0.120597,"Missing"
J19-3002,J02-3001,0,0.346615,"pairs over a certain threshold are added to output synsets. We compare this approach to five other state-of-the-art graph clustering algorithms described in Section 2.1 as the baselines. 2.3 Semantic Frame Induction Frame Semantics was originally introduced by Fillmore (1982) and further developed in the FrameNet project (Baker, Fillmore, and Lowe 1998). FrameNet is a lexical resource composed of a collection of semantic frames, relationships between them, and a corpus of frame occurrences in text. This annotated corpus gave rise to the development of frame parsers using supervised learning (Gildea and Jurafsky 2002; Erk and Pado´ 2006; Das et al. 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata 2007) and Textual Entailment (Burchardt et al. 2009; Ben Aharon, Szpektor, and Dagan 2010). However, frame-semantic resources are arguably expensive and time-consuming to build because of difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking do"
J19-3002,goldhahn-etal-2012-building,0,0.0267265,"Missing"
J19-3002,E12-1059,0,0.186249,"Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense induction using only such a paraphrase corpus as Paraphrase Database (Pavlick et al. 2015). Tian et al. (2014) introduced a"
J19-3002,Q16-1015,0,0.0324022,"Missing"
J19-3002,C92-2082,0,0.235054,"Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approa"
J19-3002,S13-2113,0,0.0987206,"mann 2006, 2012) is a hard clustering algorithm for weighted graphs, which can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority of labels among the neighboring nodes. The algorithm has a hyperparameter that controls graph weights, which can be set to three values: (1) CWtop sums 425 Computational Linguistics Volume 45, Number 3 over the neighborhood’s classes; (2) CWlin downgrades the influence of a neighboring node by its degree; or (3) CWlog by the logarithm of its degree. MaxMax (Hope and Keller 2013a) is a fuzzy clustering algorithm particularly designed for the word sense induction task. In a nutshell, pairs of nodes are grouped if they have a maximal mutual affinity. The algorithm starts by converting the undirected input graph into a directed graph by keeping the maximal affinity nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: All transitive children of this root form a cluster and the roots are marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster. Clique Per"
J19-3002,P11-1147,0,0.0258674,"Missing"
J19-3002,P12-1092,0,0.0191533,"arse tf–idf vectors are clustered, using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual wor"
J19-3002,S17-1025,0,0.346463,"Missing"
J19-3002,S13-2049,0,0.285653,"p between the instance’s context words and the words of the sense cluster. V´eronis (2004) compiles a corpus with contexts of polysemous nouns using a search engine. 427 Computational Linguistics Volume 45, Number 3 A word graph is built by drawing edges between co-occurring words in the gathered corpus, where edges below a certain similarity threshold were discarded. His HyperLex algorithm detects hubs of this graph, which are interpreted as word senses. Disambiguation in this experiment is performed by computing the distance between context words and hubs in this graph. Di Marco and Navigli (2013) present a comprehensive study of several graph-based WSI methods, including CW, HyperLex, and curvature clustering (Dorow et al. 2005). Additionally, the authors propose two novel algorithms: Balanced Maximum Spanning Tree Clustering and Squares (B-MST), and Triangles and Diamonds (SquaT++). To construct graphs, authors use first-order and second-order relationships extracted from a background corpus as well as keywords from snippets. This research goes beyond intrinsic evaluations of induced senses and measures the impact of the WSI in the context of an information retrieval via clustering a"
J19-3002,S18-2016,0,0.036049,"Missing"
J19-3002,P14-1097,0,0.0305612,"Missing"
J19-3002,P03-1009,0,0.239319,"Missing"
J19-3002,Q13-1015,0,0.175147,"s (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clustering algorithms (partitionings) produce non-overlapping clusters, that is, Ci ∩ Cj = ∅ ⇐⇒ i 6= j, ∀Ci , Cj ∈ C, whereas fuzzy clustering algorithms permit cluster overlapping, tha"
J19-3002,D13-1064,0,0.0354668,"Missing"
J19-3002,D15-1200,0,0.369011,"st dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense"
J19-3002,S10-1011,0,0.187061,"ailable implementations have been used. During the evaluation, we delete clusters equal to or larger than the threshold of 150 words, as they can hardly represent any meaningful synset. Only the clusters produced by the MaxMax algorithm were actually affected by this threshold. Quality Measure. To evaluate the quality of the induced synsets, we transform them into synonymy pairs and computed precision, recall, and F1 -score on the basis of the overlap of these synonymy pairs with the synonymy pairs from the gold standard data sets. The F1 -score calculated this way is known as paired F-score (Manandhar et al. 2010; Hope and Keller 2013a). Let C be the set of obtained synsets and CG be the set of gold synsets. Given a synset containing n &gt; 1 words, we generate n(n2−1) pairs of synonyms, so we transform C into a set of pairs P and CG into a set of gold pairs PG . We then compute the numbers of positive and negative answers as follows: TP = |P ∪ PG | (9) FP = |P  PG | (10) FN = |PG  P| (11) where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives. As a result, we use the standard definitions of precision 2·Pr·Re TP as Pr = TPTP +FP , recall a"
J19-3002,N13-1051,0,0.418879,"nsformed into a triframe, which is a triple that is composed of the subjects fs ⊆ V, the verbs fv ⊆ V, and the objects fo ⊆ V. For example, the triples shown in Figure 9 will form a triframe ({man, people, woman}, {make, earn}, {profit, money} ). 5.2 Evaluation Currently, there is no universally accepted approach for evaluating unsupervised frame induction methods. All the previously developed methods were evaluated on completely different incomparable setups and used different input corpora (Titov and 454 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Klementiev 2012; Materna 2013; O’Connor 2013, etc.). We propose a unified methodology by treating the complex multi-stage frame induction task as a straightforward triple clustering task. 5.2.1 Experimental Setup. We compare our method, Triframes WATSET, to several available state-of-the-art baselines applicable to our data set of triples (Section 2.3). LDAFrames by Materna (2012, 2013) is a frame induction method based on topic modeling. Higher-Order Skip-Gram (HOSG) by Cotterell et al. (2017) generalizes the Skip-Gram model (Mikolov et al. 2013) by extending it from word-context co-occurrence matrices to tensors factori"
J19-3002,P09-1045,0,0.0378997,"ed approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semanti"
J19-3002,P07-3015,0,0.0440453,"Algorithm for Fuzzy Graph Clustering In this section, we present WATSET, a meta-algorithm for fuzzy graph clustering. Given a graph connecting potentially ambiguous objects (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clust"
J19-3002,C14-2023,0,0.0313933,"Missing"
J19-3002,W12-1901,0,0.0610861,"Missing"
J19-3002,D14-1113,0,0.136393,"s are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to succes"
J19-3002,P07-1068,0,0.0417874,"ng semantic classes from text, also known as semantic lexicon induction, has also been extensively explored in previous works. This is because inducing semantic classes directly from text has the potential to avoid the limited coverage problems of knowledge bases like Freebase, DBpedia (Bizer et al. 2009), or BabelNet (Navigli and Ponzetto 2012), which rely on Wikipedia (Hovy, Navigli, and Ponzetto 2013), as well as to allow for resource induction across domains (Hovy et al. 2011). Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization."
J19-3002,R15-1061,0,0.0343153,"Missing"
J19-3002,nivre-etal-2006-maltparser,0,0.0381336,"Missing"
J19-3002,S16-1206,1,0.914841,"ure 14 shows the distribution of the 82,115 noun synsets from WordNet 3.1 across the supersenses. In our experiments in this section, these classes are used as gold standard clustering of word senses as recorded in WordNet. One can observe a Zipfian-like power-law (Zipf 1949) distribution with a few clusters, such as artifact and person, accounting for a large fraction of all nouns in the resource. Overall, in this experiment we decided to focus on nouns, as the input distributional thesauri used in this experiment (as presented in Section 6.2) are most studied for modeling of noun semantics (Panchenko et al. 2016b). The WordNet supersenses were applied later also for word sense disambiguation as a system of broad sense labels (Flekova and Gurevych 2016). For BabelNet, there is a similar data set called BabelDomains (Camacho-Collados and Navigli 2017) produced by automatically labeling BabelNet synsets with 32 different domains based on the topics of Wikipedia featured articles. Despite the larger size, however, BabelDomains provides only a silver standard (being semi-automatically created). We thus opt in the following to use WordNet supersenses only, because they provide instead a gold standard creat"
J19-3002,L18-1286,1,0.835468,"zareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semantic classes instead of using a sophisticated parametric pipeline that performs a sequence of clustering and pruning steps. Another related strain of research to semantic class induction is d"
J19-3002,L18-1244,1,0.90656,"zareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semantic classes instead of using a sophisticated parametric pipeline that performs a sequence of clustering and pruning steps. Another related strain of research to semantic class induction is d"
J19-3002,P04-1035,0,0.0483492,"graph clustering. Given a graph connecting potentially ambiguous objects (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clustering algorithms (partitionings) produce non-overlapping clusters, that is, Ci ∩ Cj = ∅ ⇐⇒ i 6= j, ∀Ci ,"
J19-3002,N04-1041,0,0.0348718,", as well as to allow for resource induction across domains (Hovy et al. 2011). Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal superv"
J19-3002,P15-2070,0,0.0389199,"Missing"
J19-3002,W97-0322,0,0.281913,"es belonging to the same community and builds a new network whose nodes are the communities. These steps are repeated to maximize modularity of the clustering result. 2.2 Word Sense Induction Word Sense Induction is an unsupervised knowledge-free approach to Word Sense Disambiguation (WSD): It uses neither handcrafted lexical resources nor hand-annotated sense-labeled corpora. Instead, it induces word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego network clustering. ¨ Context clustering approaches, such as Pedersen and Bruce (1997) and Schutze (1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from th"
J19-3002,W16-1620,1,0.894029,"conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense induction using only such a paraphrase corpus as Paraphrase Database (Pavli"
J19-3002,D14-1162,0,0.0797303,"Missing"
J19-3002,N18-1202,0,0.0153532,"een Skip-Gram word vectors (Mikolov et al. 2013), we should note that such an approach assigns high values of similarity not just to synonymous words, but to antonymous and generally any lexically related words. This is a common problem with lexical embedding spaces, which we tried to evade by explicitly using a synonymy dictionary as an input. For example, “audio play” and “radio play,” or “accusative” and “oblique,” are semantically related expressions, but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -score in our synset induction experiments,"
J19-3002,N10-1013,0,0.0170646,"1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from these clusters. Reisinger and Mooney (2010) presented a multi-prototype vector space. Sparse tf–idf vectors are clustered, using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are comm"
J19-3002,W17-6933,1,0.844526,"n.01, x-axis.n.01, y-axis.n.01, z-axis.n.01, major axis.n.01, minor axis.n.01, optic axis.n.01, principal axis.n.01, semimajor axis.n.01, semiminor axis.n.01 specifically, the dimensions of the vector space represent salient syntactic dependencies of each word extracted using a dependency parser. For this, we use the JoBimText framework for computation of count-based distributional models from raw text collections (Biemann and Riedl 2013).36 Although similar graphs could be derived also from neural distributional models, such as Word2Vec (Mikolov et al. 2013), it was shown in Riedl (2016) and Riedl and Biemann (2017) that the quality of syntactically-based graphs is generally superior. The JoBimText framework involves several steps. First, it takes an unlabeled input text corpus and performs dependency parsing so as to extract features representing each word. Each word is represented by a bag of syntactic dependencies such as conj and(Ruby, · ) or prep in(code, · ), extracted from the dependencies of MaltParser (Nivre, Hall, and Nilsson 2006), which are further collapsed using the tool by Ruppert et al. (2015) in the notation of Stanford Dependencies (de Marneffe, MacCartney, and Manning 2006). Next, sema"
J19-3002,P10-1044,0,0.102279,"Missing"
J19-3002,N16-1173,0,0.0331879,"Missing"
J19-3002,J98-1004,0,0.710235,"nity and builds a new network whose nodes are the communities. These steps are repeated to maximize modularity of the clustering result. 2.2 Word Sense Induction Word Sense Induction is an unsupervised knowledge-free approach to Word Sense Disambiguation (WSD): It uses neither handcrafted lexical resources nor hand-annotated sense-labeled corpora. Instead, it induces word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego network clustering. ¨ Context clustering approaches, such as Pedersen and Bruce (1997) and Schutze (1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from these clusters. Reisi"
J19-3002,D07-1002,0,0.0693088,"nes. 2.3 Semantic Frame Induction Frame Semantics was originally introduced by Fillmore (1982) and further developed in the FrameNet project (Baker, Fillmore, and Lowe 1998). FrameNet is a lexical resource composed of a collection of semantic frames, relationships between them, and a corpus of frame occurrences in text. This annotated corpus gave rise to the development of frame parsers using supervised learning (Gildea and Jurafsky 2002; Erk and Pado´ 2006; Das et al. 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata 2007) and Textual Entailment (Burchardt et al. 2009; Ben Aharon, Szpektor, and Dagan 2010). However, frame-semantic resources are arguably expensive and time-consuming to build because of difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking domainspecific frame-based resources. Possible inroads are cross-lingual semantic annotation 2 http://ontopt.dei.uc.pt. 428 Ustalov et al. WATSET: Local-Global Graph Clustering with"
J19-3002,M92-1001,0,0.293429,"rdan (2003) for generating semantic frames and their respective frame-specific semantic roles at the same time. The authors evaluated their approach against the CPA corpus (Hanks and Pustejovsky 2005). Although Ritter, Mausam, and Etzioni (2010) have applied LDA for inducing structures similar to frames, their study is focused on the extraction of mutually related frame arguments. ProFinder (Cheung, Poon, and Vanderwende 2013) is another generative approach that also models both frames and roles as latent topics. The evaluation was performed on the in-domain information extraction task MUC-4 (Sundheim 1992) and on the text summarization task TAC-2010.3 Modi, Titov, and Klementiev (2012) build on top of an unsupervised semantic role labeling model (Titov and Klementiev 2012). The raw text of sentences from the FrameNet data is used for training. The FrameNet gold annotations are then used to evaluate the labeling of the obtained frames and roles, effectively clustering instances known during induction. Kawahara, Peterson, and Palmer (2014) harvest a huge collection of verbal predicates along with their argument instances and then apply the Chinese Restaurant Process clustering algorithm to group"
J19-3002,D08-1061,0,0.0384586,"of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to impr"
J19-3002,W02-1028,0,0.251425,"ate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we di"
J19-3002,C14-1016,0,0.0229688,"usters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a m"
J19-3002,P11-1145,0,0.0800752,"Missing"
J19-3002,E12-1003,0,0.0220272,"he CPA corpus (Hanks and Pustejovsky 2005). Although Ritter, Mausam, and Etzioni (2010) have applied LDA for inducing structures similar to frames, their study is focused on the extraction of mutually related frame arguments. ProFinder (Cheung, Poon, and Vanderwende 2013) is another generative approach that also models both frames and roles as latent topics. The evaluation was performed on the in-domain information extraction task MUC-4 (Sundheim 1992) and on the text summarization task TAC-2010.3 Modi, Titov, and Klementiev (2012) build on top of an unsupervised semantic role labeling model (Titov and Klementiev 2012). The raw text of sentences from the FrameNet data is used for training. The FrameNet gold annotations are then used to evaluate the labeling of the obtained frames and roles, effectively clustering instances known during induction. Kawahara, Peterson, and Palmer (2014) harvest a huge collection of verbal predicates along with their argument instances and then apply the Chinese Restaurant Process clustering algorithm to group predicates with similar arguments. The approach was evaluated on the verb cluster data set of Korhonen, Krymolowski, and Marx (2003). These and some other related approac"
J19-3002,W09-1127,0,0.152335,"ifficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking domainspecific frame-based resources. Possible inroads are cross-lingual semantic annotation 2 http://ontopt.dei.uc.pt. 428 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications transfer (Pado´ and Lapata 2009; Hartmann, Eckle-Kohler, and Gurevych 2016) or linking FrameNet to other lexical-semantic or ontological resources (Narayanan et al. 2003; Tonelli and Pighin 2009; Laparra and Rigau 2010; Gurevych et al. 2012, inter alia). One inroad for overcoming these issues is automatizing the process of FrameNet construction through unsupervised frame induction techniques, as investigated by the systems described next. LDA-Frames (Materna 2012, 2013) is an approach to inducing semantic frames using a latent Dirichlet allocation (LDA) by Blei, Ng, and Jordan (2003) for generating semantic frames and their respective frame-specific semantic roles at the same time. The authors evaluated their approach against the CPA corpus (Hanks and Pustejovsky 2005). Although Ritt"
J19-3002,P17-1145,1,0.797757,"but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -score in our synset induction experiments, we conducted a cross-resource evaluation between the used gold-standard data sets (Table 12). Similarly to the experimental setup described in Section 4.2.1, we transformed synsets from every data set into sets of synonymy pairs. Then, for every pair of gold standard data sets, we computed the pairwise precision, recall, and F1 -score by assessing synset-induced synonymy pairs of one data set on the pairs of another data set. As a result, we see that the lo"
J19-3002,P18-2010,1,0.815639,", Panchenko, and Biemann (2017), including an analysis of its computational complexity and run-time. We also describe a simplified version of WATSET that does not use the context similarity measure for propagating links in the original graph to the appropriate senses in the disambiguated graph. Three subsequent sections present different applications of the algorithm. Section 4 applies WATSET for unsupervised synset induction, referencing results by Ustalov, Panchenko, and Biemann. Section 5 shows frame induction with WATSET on the basis of a triclustering approach, as previously described by Ustalov et al. (2018). Section 6 presents new experiments on semantic class induction with WATSET. Section 7 concludes with the final remarks and pointers for future work. Table 1 shows several examples of linguistic structures on which we conduct experiments described in this article. With the exception of the type of input graph and the hyper-parameters of the WATSET algorithm, the overall pipeline remains similar in every described application. For instance, in Section 4 the input of the clustering algorithm is a graph of ambiguous synonyms and the output is an induced linguistic 1 This article builds upon and"
J19-3002,C02-1114,0,0.164904,"n, and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Thomason and Mooney (2017) performed multi-modal word sense induction by combining both language and vision signals. In this approach, word embeddings are learned from the ImageNet corpus (Deng et al. 2009) and visual features are obtained from a deep neural network. Running a k-means algorithm on the joint feature set produces WordNet-like synsets. Word ego network clustering methods cluster graphs of words semantically related to the ambiguous word (Lin 1998; Pantel and Lin 2002; Widdows and Dorow 2002; Biemann 2006; Hope and Keller 2013a). An ego network consists of a single node (ego), together with the nodes they are connected to (alters), and all the edges among those alters (Everett and Borgatti 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller 2013b). Graph edges represent semantic relationships between words derived using corpus-based methods (e.g., distributional semantics) or g"
J19-3002,P13-1086,0,0.0803084,"Missing"
J19-3002,zesch-etal-2008-extracting,0,0.076042,"Missing"
J19-3002,erk-pado-2006-shalmaneser,0,\N,Missing
J19-3002,bauer-etal-2012-dependency,0,\N,Missing
J19-3002,N10-1137,0,\N,Missing
J19-3002,W06-3812,1,\N,Missing
J19-3002,P10-2045,0,\N,Missing
J19-3002,P98-1013,0,\N,Missing
J19-3002,C98-1013,0,\N,Missing
J19-3002,P08-1119,0,\N,Missing
J19-3002,D09-1098,0,\N,Missing
K19-1024,L18-1659,1,0.823273,"Missing"
K19-1024,W18-6241,1,0.738008,"two ways of automatically labelling debate motions with the codes from the Manifesto Project: (1) similarity matching and (2) supervised classification. We tested both at the quasi-sentence level and we additionally exWith both of these approaches, we explored the use of the following combinations of sources of textual unigram features: the debate titles, which have been shown to be highly predictive of a 254 macro and micro weightings in order to offer an understanding of the quality overall, as well as for the different classes. motion’s opinion-topic in a supervised classification setting (Abercrombie and Batista-Navarro, 2018b), the debate motions themselves, and both the titles and motions together. Motions: Similarity Matching Supervised Classification We tested a range of supervised machine learning algorithms for the policy preference classification task, ranging from traditional approaches to recently developed pre-trained deep language representation models. We were particularly interested in assessing the performance of such approaches: (1) despite the limited training data available (1.6k motion quasi-sentences); and (2) in a cross-domain application (training on over 16k manifesto quasi-sentences, and tes"
K19-1024,D17-1318,1,0.842781,"the United Kingdom (UK) Parliament are of interest to scholars of political science as well as the media and members of the public who wish to monitor the actions of their 1 https://www.parliament.uk/documents/ rules-of-behaviour.pdf 249 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 249–259 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics around policies and policy domains. Topic modelling or detection methods, which tend to produce coarse overviews and output neutral topics such as ‘education’ or ‘transport’ (as in Menini et al. (2017), for instance), are therefore not suitable for our purposes. Rather, we seek to find the proposer of a motion’s position or policy preference towards each topic—in other words, an opiniontopic. Topic labels do exist for the Hansard transcripts, such as those produced by the House of Commons Library or parliamentary monitoring organsitions such as Public Whip.2 However, these are unsuitable due to, in the former case, the fact that they incorporate no opinion or policy preference information, and for the latter, being unsystematic, insufficient in both quantity and coverage of the topics that"
K19-1024,S15-1012,0,0.0150779,"tate Limitation. The included manifestos are coded at the quasi-sentence level—that is, units of text that span a sentence or part of a sentence, and which have been judged by the annotators to contain ‘exactly one statement or “message”’ (Werner et al., 2011), as in Example 2, in which a single sentence has been annotated as four quasi-sentences:6 et al., 2015). Inspired by work on analysis of text from other domains, such as product reviews and social media, much of the computer science research in this area has concentrated on classifying the sentiment polarity of individual speeches (e.g. Burford et al., 2015; Thomas et al., 2006; Yogatama et al., 2015). Political scientists meanwhile, have tended to focus on position scaling— the task of placing the combined contributions of a political actor on a (usually) one-dimensional scale, such as Left–Right (e.g. Glavaˇs et al., 2017b; Laver et al., 2003; Nanni et al., 2019a; Proksch and Slapin, 2010). In either case, the majority of this work does not take into consideration the topics or policy areas addressed in the speeches. Supervised classification approaches to opinion-topic identification have been explored in a number of papers. Abercrombie and B"
K19-1024,L18-1008,0,0.0293531,"e-trained deep language representation models. We were particularly interested in assessing the performance of such approaches: (1) despite the limited training data available (1.6k motion quasi-sentences); and (2) in a cross-domain application (training on over 16k manifesto quasi-sentences, and testing on the motion quasi-sentences). First, we examined the performance of Support Vector Machines (SVM) trained using lexical (tfidf) or word embedding (w-emb) features, which act as strong traditional baselines. We tested both pre-trained general purpose word embeddings from https://fasttext.cc (Mikolov et al., 2018) and in-domain vectors generated on the Hansard transcripts from Nanni et al. (2019b). We also report the results of a widely adopted neural network baseline for topic classification (see for instance Glavaˇs et al. (2017a) and Subramanian et al. (2018) in the context of manifesto quasi-sentences classification): a Convolutional Neural Network (CNN) with single convolution layer and a single max-pooling layer. We again tested the CNN with general purpose and indomain embeddings. As final skyline comparisons, we present the performance of (1) a pre-trained BERT (large, cased) model (Devlin et a"
K19-1024,S16-2016,1,0.87845,"Missing"
K19-1024,W17-2906,1,0.924595,"Missing"
K19-1024,N18-1202,0,0.024826,"in party political manifestos, previous studies have focused on topical segmentation (Glavaˇs et al., 2016) and classification of sentences into the seven coarsegrained policy domains (Glavaˇs et al., 2017a; Zirn et al., 2016). Meanwhile, Subramanian et al. (2018) recently presented a deep learning model that classifies manifesto sentences with the finer-grained code-level scheme of the Manifesto Project, as well as placing them on a LeftRight scale. In order to contribute to these research efforts and following recent advancements in deep language representation models (Devlin et al., 2018; Peters et al., 2018), we test the potential of BERT (Bidirectional Encoder Representations from Transformers) for policy-topic classification on both debate motions and manifestos. To secure your first job we will create 3 million new apprenticeships; 411: Technology and Infrastructure take everyone earning less than 12,500 out of Income Tax altogether 404: Economic Planning and pass a law to ensure we have a Tax-Free Minimum Wage in this country; 412: Controlled Economy (2) and continue to create a fairer welfare system where benefits are capped to the level that makes work pay so you are rewarded for working ha"
K19-1024,E17-2109,1,0.765474,"Missing"
K19-1024,N18-1178,0,0.153406,"f Herzog et al. (2018), who use labels from the Comparative Agendas Project (CAP).7 However, while they seek to discover latent topics present in the corpus, we wish to determine the policy-topic of each individual debate/motion. Rather than employ labelled manifesto data, as we do, they use the descriptions of the CAP codes. Concerning policy identification in party political manifestos, previous studies have focused on topical segmentation (Glavaˇs et al., 2016) and classification of sentences into the seven coarsegrained policy domains (Glavaˇs et al., 2017a; Zirn et al., 2016). Meanwhile, Subramanian et al. (2018) recently presented a deep learning model that classifies manifesto sentences with the finer-grained code-level scheme of the Manifesto Project, as well as placing them on a LeftRight scale. In order to contribute to these research efforts and following recent advancements in deep language representation models (Devlin et al., 2018; Peters et al., 2018), we test the potential of BERT (Bidirectional Encoder Representations from Transformers) for policy-topic classification on both debate motions and manifestos. To secure your first job we will create 3 million new apprenticeships; 411: Technolo"
K19-1024,D15-1251,0,0.0128109,"e coded at the quasi-sentence level—that is, units of text that span a sentence or part of a sentence, and which have been judged by the annotators to contain ‘exactly one statement or “message”’ (Werner et al., 2011), as in Example 2, in which a single sentence has been annotated as four quasi-sentences:6 et al., 2015). Inspired by work on analysis of text from other domains, such as product reviews and social media, much of the computer science research in this area has concentrated on classifying the sentiment polarity of individual speeches (e.g. Burford et al., 2015; Thomas et al., 2006; Yogatama et al., 2015). Political scientists meanwhile, have tended to focus on position scaling— the task of placing the combined contributions of a political actor on a (usually) one-dimensional scale, such as Left–Right (e.g. Glavaˇs et al., 2017b; Laver et al., 2003; Nanni et al., 2019a; Proksch and Slapin, 2010). In either case, the majority of this work does not take into consideration the topics or policy areas addressed in the speeches. Supervised classification approaches to opinion-topic identification have been explored in a number of papers. Abercrombie and BatistaNavarro (2018b) obtain good performance"
K19-1024,W06-1639,0,0.428509,"the governments, political parties, and individual Members of Parliament (MPs) who propose them. As all speeches given and all votes cast in the House are responses to one of these proposals, the motions are key to any understanding and analysis of the opinions and positions expressed in the subsequent speeches given in parliamentary debates. By definition, debate motions convey the stated policy preferences of the MPs or parties who propose them. They therefore express polarity— positive or negative—towards some target, such as a piece of legislation, policy, or state of affairs. As noted by Thomas et al. (2006), the polarity of a debate proposal can strongly affect the language used by debate participants to either support or oppose it, effectively acting as a polarity shifter on the ensuing speeches. Analysis of debate motions is therefore a key first step in automatically determining the positions presented and opinions expressed by all speakers in the wider debates. Additionally, there are further challenges associated with this task that differentiate it from the forms of sentiment analysis typically performed in other domains. Under Parliament’s Rules of Behaviour,1 debate participants use an e"
L16-1056,S15-2151,1,0.906247,"Missing"
L16-1056,P99-1016,0,0.356186,"ctured as follows. In Section 3., the process of creating the database is described. Section 4. describes bot the released dataset and Java API to programmatically access to the tuples database; and in Section 5., we trace some conclusions about the released resource/API and we discuss about the impact on potential new applications in the field of text understanding. 2. Related work In the past, many different methods have been developed for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scr"
L16-1056,C92-2082,0,0.814626,"ale in many NLP task (restricted not only on ontology learning). The rest of this paper is structured as follows. In Section 3., the process of creating the database is described. Section 4. describes bot the released dataset and Java API to programmatically access to the tuples database; and in Section 5., we trace some conclusions about the released resource/API and we discuss about the impact on potential new applications in the field of text understanding. 2. Related work In the past, many different methods have been developed for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used t"
L16-1056,R11-2016,0,0.173486,"sily recognizable and indisputably indicate the lexical relation of interest. In particular we focused on 59 patterns that we collected from the past literature. In Table 2 we show the full list of patterns we used for the extraction phase. Eight patterns come from Ponzetto and Strube (2011), where isa patterns were used to induce a taxonomy from Wikipedia. Other isa-patterns were collected from Orna-Montesinos (2011), where patterns for the term “building” were extracted on a set of specialized textbooks in the field of construction engineering. Remaining patterns were finally collected from Klaussner and Zhekova (2011) where the authors extract isa relations from selected Wikipedia pages. The patterns identified in literature are then translated into regular expressions. For example, (with respect to Table 2) the pattern p5 (i.e. “N Ph such as N Pt ” where N Pt indicates the hyponym and N Ph the hypernym) was translated into the following regular expression: (p{L}|d)[""']?,?ssuchsass['""]?(p{L}|d) As the corpus is read line by line, the length of line has an impact on the performance of a regular expression. Lines 5 http://webdatacommons.org/framework/ 4 http://commoncrawl.org 3 Tuple Extraction a"
L16-1056,D10-1108,0,0.0144066,"relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the type of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. In this paper we present a novel, open resource consisting of more than 400 million tuples extracted from the Com1 360 https://commoncrawl.org Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relations extraction consist of two phases. First the authors bootstrap the terminology harvesting with DAP of the kind “animals such as lions and *”, so it is possible to discover new terms such as “cats”. Next, for each pair of terms in the discovered terminology e.g. (“lions”,“cats”) they automatically create a DAP −1 of the kind “* such as lions and cats” and discover new hypernyms e.g. “felines”. The above mentioned works focus o"
L16-1056,P10-1134,0,0.0726956,"al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the type of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. In this paper we present a novel, open resource consisting of more than 400 million tuples extracted from the Com1 360 https://commoncrawl.org Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relations extraction consist of two phases. First the authors bootstrap the terminology harvesting with DAP of the kind “animals such as lions and *”, so it is possible to discover new terms such as “cats”. Next, for each"
L16-1056,S16-1206,1,0.0806322,"he left and right modifiers (i.e. lt , lh , rt , rh ) changes. From each tuple variant we can also access to the list of: 365 • Pattern: the set containing the patterns which matched the “isa” relation; as the Java API we developed to let programmatically query the tuples database. Our database represents a first step towards a more complex semantic resource such as full-fledged taxonomies. In fact, we already used it as part of a SemEval competition on taxonomy induction (Bordea et al., 2015), where we achieved a competitive performance by leveraging our isa relations harvested from the Web (Panchenko et al., 2016). • Pay-level domain: the Urls of the Web documents from where the relation was extracted; • Documents: entities of the kind (pay-level domain, sentence) from which the relation was extracted. 4.2. Java API to access the MongoDB We next describe the Java Application Programming Interface to programmatically query the database. The class ”TuplesDb“ is the main class from where access the tuples database and can be instantiated as follows: The WebIsaDatabase and the Java API can be downloaded at http://webdatacommons.org/isadb/. Acknowledgements ... TuplesDb tDb = TuplesDb . getInstance () ; ..."
L16-1056,N03-1033,0,0.0373474,"ed each word of a noun phrase. With the lemmatized entities a further refactoring step is taken, which targets noun phrases that contain multiple nouns. To be more specific, this regards noun phrases, which are located in front of the pattern and contain nouns separated by a preposition (complex NPS). The issue regarding these entities is, that it is not trivial to determine, which of these nouns is in a hyponymy relation with the entity behind the pattern. For example given a pattern “* such as *”: Noun phrase identification: In order to identify noun phrases we used the Stanford POS Tagger (Toutanova et al., 2003) to obtain the mapping of words to lexical categories. The noun phrase identification basically entails a selection of allowed part of speech (POS) tags for the pre/post modifiers and for the head noun. We defined as head noun allowed tags: singular noun or mass (NN), plural noun (NNS), singular proper noun (NNP), plural proper noun (NNPS). For the premodifier, we added the following tags we selected for the previous set: adjective (JJ), comparative adjective (JJR), superlative adjective (JJS) and past participle verb (VBN). The allowed tags for the postmodfier include: adjective (JJ), compara"
L16-1056,J13-3007,1,0.818122,"techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the type of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. In this paper we present a novel, open resource consisting of more than 400 million tuples extracted from the Com1 360 https://commoncrawl.org Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called"
L18-1093,P98-1013,0,0.595481,"(bs) ∪ Bf (F )) and A.c(w) equals the number of occurrences of the word w in A. For instance, with respect to the excerpts of Tables 3 and 4 we obtain w(Communication, bn:00085007v) = 15.0. 3. Using the Enriched Representations for Word Frame Disambiguation We evaluate our extensions of Framester profiles following the experimental setting of Gangemi et al. (2016b), and compare the extended and the original profiles in a task of Word Frame Disambiguation (WFD). 3.1. Dataset: FrameNet Full Text Documents To create a silver standard we processed all 108 documents from the FrameNet 1.7 dataset (Baker et al., 1998) with BabelFy (Moro et al., 2014)3 . By combining the original frame annotations with the automatically generated entity links we collected a total of 81,706 annotations, which we use in our experimental setting as a silver standard. 3.2. Word Frame Disambiguation Following the WFD approach described in Gangemi et al. (2016b) we implemented a simple word frame disambiguator, where for each provided annotation in our silver standard we try to predict a frame label only on the basis of the BabelNet synsets generated through BabelFy. In order to provide the most suitable frame label F for the pro"
L18-1093,E17-1056,1,0.906433,"usually spontaneous – the rules by which we interpret meaning. Here, the reference to the ‘Reading aloud’ frame from FrameNet1 could be triggered on the basis of the occurrences of the verbs in the sentence: hear, understanding and interpret. Such connections, in turn, could be provided by a hybrid resource where distributional representations from text have been explicitly linked to semantic knowledge repositories, since hybrid resources of this kind have been shown in the past to improve performance on lexical understanding (Panchenko et al., 2017) as well as taxonomy learning and cleaning (Faralli et al., 2017). In this paper, we bridge the gap between distributional and frame semantics by linking distributional semantic representations to Framester, a knowledge graph that acts as a hub between resources like FrameNet, BabelNet and DBpedia, among others. As a result of this, we introduce a new lexical resource that enriches the Framester knowledge graph with distributional features extracted from text, and show how this hybrid resource yields better results on the task of recognizing frames in running text. Joining distributional and frame semantics builds upon and 1 587 https://framenet.icsi.berkel"
L18-1093,Q14-1019,0,0.0297903,"the number of occurrences of the word w in A. For instance, with respect to the excerpts of Tables 3 and 4 we obtain w(Communication, bn:00085007v) = 15.0. 3. Using the Enriched Representations for Word Frame Disambiguation We evaluate our extensions of Framester profiles following the experimental setting of Gangemi et al. (2016b), and compare the extended and the original profiles in a task of Word Frame Disambiguation (WFD). 3.1. Dataset: FrameNet Full Text Documents To create a silver standard we processed all 108 documents from the FrameNet 1.7 dataset (Baker et al., 1998) with BabelFy (Moro et al., 2014)3 . By combining the original frame annotations with the automatically generated entity links we collected a total of 81,706 annotations, which we use in our experimental setting as a silver standard. 3.2. Word Frame Disambiguation Following the WFD approach described in Gangemi et al. (2016b) we implemented a simple word frame disambiguator, where for each provided annotation in our silver standard we try to predict a frame label only on the basis of the BabelNet synsets generated through BabelFy. In order to provide the most suitable frame label F for the provided BabelNet synset label bs: 1"
L18-1093,W17-1909,1,0.92743,"we hear someone READ a text, our understanding of what we hear is usually spontaneous – the rules by which we interpret meaning. Here, the reference to the ‘Reading aloud’ frame from FrameNet1 could be triggered on the basis of the occurrences of the verbs in the sentence: hear, understanding and interpret. Such connections, in turn, could be provided by a hybrid resource where distributional representations from text have been explicitly linked to semantic knowledge repositories, since hybrid resources of this kind have been shown in the past to improve performance on lexical understanding (Panchenko et al., 2017) as well as taxonomy learning and cleaning (Faralli et al., 2017). In this paper, we bridge the gap between distributional and frame semantics by linking distributional semantic representations to Framester, a knowledge graph that acts as a hub between resources like FrameNet, BabelNet and DBpedia, among others. As a result of this, we introduce a new lexical resource that enriches the Framester knowledge graph with distributional features extracted from text, and show how this hybrid resource yields better results on the task of recognizing frames in running text. Joining distributional and f"
L18-1093,D17-1270,0,0.0292348,"Missing"
L18-1093,C98-1013,0,\N,Missing
L18-1164,S10-1011,0,0.460828,"any language for which a tokenizer, part-of-speech tagger, lemmatizer, and a sense inventory are available. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 presents the Watasense word sense disambiguation system, presents its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. 2. Related Work Although the problem of WSD has been addressed in many SemEval campaigns (Navigli et al., 2007; Agirre et al., 2010; Manandhar et al., 2010, inter alia), we focus here on word sense disambiguation systems rather than on the research methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-"
L18-1164,P13-4007,0,0.0247526,"arch methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the seman"
L18-1164,Q14-1019,0,0.0545051,"The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vecto"
L18-1164,S07-1006,0,0.112434,"Missing"
L18-1164,D17-2016,1,0.945865,"for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce ou"
L18-1164,W16-1620,1,0.943397,"ory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce our sense inventory from synonymy dictionaries and not distributional word vectors. While this requires more manually created resources, a potential advantage of our approach is that the resulting inventory contains less noise. 3. Watasense, an Unsupervised System for Word Sense Disambiguation Wa"
L18-1164,P17-1145,1,0.925562,"present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index. Keywords: word sense disambiguation, system, synset induction 1. Introduction Word sense disambiguation (WSD) is a natural language processing task of identifying the particular word senses of polysemous words used in a sentence. Recently, a lot of attention was paid to the problem of WSD for the Russian language (Lopukhin and Lopukhina, 2016; Lopukhin et al., 2017; Ustalov et al., 2017). This problem is especially difficult because of both linguistic issues – namely, the rich morphology of Russian and other Slavic languages in general – and technical challenges like the lack of software and language resources required for addressing the problem. To address these issues, we present Watasense, an unsupervised system for word sense disambiguation. We describe its architecture and conduct an evaluation on three datasets for Russian. The choice of an unsupervised system is motivated by the absence of resources that would enable a supervised system for under-resourced languages. W"
L18-1164,P10-4014,0,0.0428215,"s its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. 2. Related Work Although the problem of WSD has been addressed in many SemEval campaigns (Navigli et al., 2007; Agirre et al., 2010; Manandhar et al., 2010, inter alia), we focus here on word sense disambiguation systems rather than on the research methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy"
L18-1244,P11-1062,0,0.0300176,"arge-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts. While this approach performs a hard cluste"
L18-1244,W06-3812,1,0.812843,"age” semantic classes. Similarly to the induced word senses, the semantic classes are labeled with hypernyms. In contrast to the induced word senses, which represent a local clustering of word senses (related to a given word) semantic classes represent a global sense clustering of word senses. One sense c, such as “apple#0”, can appear only in a single cluster. of related ambiguous terms (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induced sense inventory sense network is scale-free, cf. (Steyvers and Tenenbaum, 2005). Our experiments show that a glob"
L18-1244,S15-2151,1,0.859596,"ypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical"
L18-1244,S16-1168,0,0.0609794,"thermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a SemEval’16 task on taxonomy induction. Keywords: semantic classes, distributional semantics, hypernyms, co-hyponyms, word sense induction 1. Introduction Hypernyms are useful in various applications, such as question answering (Zhou et al., 2013), query expansion (Gong et al., 2005), and semantic role labelling (Shi and Mihalcea, 2005) as they can help to overcome sparsity of statistical models. Hypernyms are also the building blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017)."
L18-1244,2016.gwc-1.10,1,0.335932,"to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel,"
L18-1244,P99-1016,0,0.383204,"use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extract"
L18-1244,P14-1113,0,0.0239862,"e-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distrib"
L18-1244,D17-1185,1,0.891904,"Missing"
L18-1244,C92-2082,0,0.731648,"models. Hypernyms are also the building blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-"
L18-1244,heylen-etal-2008-modelling,0,0.0368325,"ango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised method for post-processing of noisy hypernymy relations based on clustering of graphs of word senses induced from text. The idea to use distributional semantics to find hypernyms seems natural and has been widely used. However, the existing methods used distributional, yet sense-unaware and local features. We are the first to use global sense-aware distributional structure via the induced semantic classes to improve"
L18-1244,N15-1098,1,0.894317,"Missing"
L18-1244,C14-2023,0,0.0820281,"of 4,870 relations using lexical split by hyponyms. All relations from Hcluster and Horig of one hyponym were included in the sample. These relations were subsequently annotated by human judges using crowdsourcing. We asked crowdworkers to provide a binary judgment about the correctness of each hypernymy relation as illustrated in Figure 7. 6.2.3. Results Overall, 298 annotators completed 4,870 unique tasks each labeled 6.9 times on average, resulting in a total of 33,719 binary human judgments about hypernyms. We obtained a fair agreement among annotators of 0.548 in terms of the Randolph κ (Meyer et al., 2014). Since CrowdFlower reports a confidence for each answer, we selected N = 3 most confident answers per pair and aggregated them using weighted majority voting. The ties were broken pessimistically, i.e. by treating a hypernym as irrelevant. Results for N ∈ 3, 5, 6 varied less than by 0.002 in terms of F-score. The task received the rating of a 4.4 out of 5.0 according to the annotator’s feedback mechanism. Table 5 presents results of the experiment. Since each pair received a binary score, we calculated Precision, Recall, and F-measure of two compared methods. Our denoising method improves the"
L18-1244,P06-2075,0,0.0444314,"ing their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for bu"
L18-1244,S15-1021,0,0.0148946,"obal Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account n"
L18-1244,D14-1113,0,0.099783,"s (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induced sense inventory sense network is scale-free, cf. (Steyvers and Tenenbaum, 2005). Our experiments show that a global clustering of this network can lead to a discovery of giant components, which are useless in our context as they represent no semantic class. To overcome this problem, we re-build the sense network as described below. 3.2. Representing Senses with Ego Networks To perform a global clustering of senses, we represent each induced sense s by a second-order ego network (Everett and Borgatti, 2005)."
L18-1244,S16-1206,1,0.906671,"Missing"
L18-1244,D17-2016,1,0.832821,"aluate our approach, we performed three experiments. A large-scale crowdsourcing study indicated a high plausibility of extracted semantic classes according to human judgment. Besides, we demonstrated that our approach helps to improve precision and recall of a hypernymy extraction method. Finally, we showed how the proposed semantic classes can be used to improve domain taxonomy induction from text. While we have demonstrated the utility of our approach for hypernym extraction and taxonomy induction, we believe that the induced semantic classes can be useful in other tasks. For instance, in (Panchenko et al., 2017) these semantic classes were used as an inventory for word sense disambiguation to deal with out of vocabulary words. 8. Acknowledgements This research was supported by Deutscher Akademischer Austauschdienst (DAAD), Deutsche Forschungsgemeinschaft (DFG) under the project ”Joining Ontologies and Semantics Induced from Text” (JOIN-T), and by the Ministry of Education and Science of the Russian Federation Agreement no. 02.A03.21.0006. We are grateful to three anonymous reviewers for their helpful comments. Finally, we are grateful to Dirk Johannßen for providing feedback on an early version of th"
L18-1244,P06-1015,0,0.0996737,"rnymy relations using distributionally induced semantic classes, represented by clusters of induced word senses labeled with noisy hypernyms. The word postfix, such as #1, is an ID of an induced sense. The wrong hypernyms outside the cluster labels are removed, while the missing ones not present in the noisy database of hypernyms are added. 2. 2.1. Related Work Extraction of Hypernyms In her pioneering work, Hearst (1992) proposed to extract hypernyms based on lexical-syntactic patterns from text. Snow et al. (2004) learned such patterns automatically based on a set of hyponym-hypernym pairs. Pantel and Pennacchiotti (2006) presented another approach for weakly supervised extraction of similar extraction patterns. These approaches use some training pairs of hypernyms to bootstrap the pattern discovery process. For instance, Tjong Kim Sang (2007) used web snippets as a corpus for extraction of hypernyms. More recent approaches exploring the use of distributional word representations for extraction of 1541 1 https://github.com/uhh-lt/mangosteen §3 Induction of Semantic Classes Sense Ego-Networks §3.3 Global Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clusteri"
L18-1244,N04-1041,0,0.578714,"roposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts. While this approach performs a hard clustering and does not label clusters, these drawbacks are addressed in (Pantel and Lin, 2002), where words can belong to several clusters, thus representing senses, and in (Pantel and Ravichandran, 2004), where authors aggregate hypernyms per cluster, which come from Hearst patterns. The main difference to our approach is that we explicitly represent senses both in clusters and in their hypernym labels, which enables us to connect our sense clusters into a global taxonomic structure. Consequently, we are the first to use semantic classes to improve hypernymy extraction. Ustalov et al. (2017b) proposed a synset induction approach based on global clustering of word senses. The authors used the graph constructed of dictionary synonyms, while we use distributionally-induced graphs of senses. 3. U"
L18-1244,C14-1097,0,0.161869,"mantic Classes Sense Ego-Networks §3.3 Global Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regul"
L18-1244,L16-1056,1,0.932779,"rnym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et"
L18-1244,P16-1226,0,0.239693,"earning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised me"
L18-1244,steinberger-etal-2006-jrc,0,0.0164921,"used to improve results of other state-of-the-art hypernymy extraction approaches, such as HypeNET (Shwartz et al., 2016). 6.3. Experiment 3: Improving Domain Taxonomy Induction In this section, we show how the labeled semantic classes can be used for induction of domain taxonomies. 6.3.1. SemEval 2016 Task 13 We use the taxonomy extraction evaluation dataset by Bordea et al. (2016), featuring gold standard taxonomies for three domains (Food, Science, Environment) and four languages (English, Dutch, French, and Italian) on the basis of existing lexical resources, such as WordNet and Eurovoc (Steinberger et al., 2006).7 Participants were supposed to build a taxonomy provided a vocabulary of a domain. Since our other experiments were conducted on English, we used the English part of the task. The evaluation is 1547 7 http://eurovoc.europa.eu System / Domain, Dataset Food, WordNet Science, WordNet Food, Combined Science, Combined Science, Eurovoc Environment, Eurovoc WordNet 1.0000 1.0000 0.5870 0.5760 0.6243 n.a. Baseline JUNLP NUIG-UNLP QASSIT TAXI USAAR 0.0022 0.1925 n.a. n.a. 0.3260 0.0021 0.0016 0.0494 0.0027 0.2255 0.2255 0.0008 0.0019 0.2608 n.a. n.a. 0.2021 0.0000 0.0163 0.1774 0.0090 0.5757 0.3634 0"
L18-1244,P07-2042,0,0.0798013,"Missing"
L18-1244,E17-2087,1,0.887253,"us). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) perf"
L18-1244,P17-1145,1,0.586116,"Missing"
L18-1244,J13-3007,1,0.933854,"to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http:"
L18-1244,P16-1158,0,0.0305763,"uced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples"
L18-1244,2005.jeptalnrecital-recital.1,0,0.0247916,"-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised method for post-processing of noisy hypernymy relations based on clustering of graphs of word senses induced from text. The idea to use distributional semantics to find hypernyms seems natural and has been widely used. However, the existing methods used distributional, yet sense-unaware and local features. We are the first to use global sense-aware distributional structure via the induced semant"
L18-1244,C14-1212,0,0.297993,"uilding blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper"
L18-1244,C02-1114,0,0.354924,"ogy#0, language#0, format#2, app#0 Table 2: Sample of the induced sense clusters representing “fruits” and “programming language” semantic classes. Similarly to the induced word senses, the semantic classes are labeled with hypernyms. In contrast to the induced word senses, which represent a local clustering of word senses (related to a given word) semantic classes represent a global sense clustering of word senses. One sense c, such as “apple#0”, can appear only in a single cluster. of related ambiguous terms (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induc"
L18-1286,C14-1076,0,0.0222431,"identified in the 251.92 billion tokens output corpus. 3.3.3. Dependency Parsing To make large-scale parsing of texts possible, a parser needs to be not only reasonably accurate but also fast. Unfortunately, the most accurate parsers, such as Stanford parser based on the PCFG grammar (De Marneffe et al., 2006), according to our experiments, take up to 60 minutes to process 1 Mb of text on a single core, which was prohibitively slow for our use-case (details of the hardware configuration are available in Section 3.5.). We tested all versions of the Stanford, Malt (Hall et al., 2010), and Mate (Ballesteros and Bohnet, 2014) parsers for English available via the DKPro Core framework. To dependency-parse texts, we selected the Malt parser, due to an optimal ratio of efficiency and effectiveness (parsing of 1 Mb of text per core in 1–4 minutes). This parser was successfully used in the past for the construction of linguistically analyzed web corpora, such as P UK WAC (Baroni et al., 2009) and ENCOW16 (Sch¨afer, 2015). While more accurate parsers exist, e.g. the Stanford parser, according to our experiments, even the neural-based version of this parser is substantially slower. On the other hand, as shown by Chen and"
L18-1286,P01-1005,0,0.0320709,"is a combination of Wikipedia with two We compute syntactic count-based distributional representations of words using the JoBimText framework (Biemann 1821 30 https://github.com/uhh-lt/josimtext other corpora, we can reach the even better result by training the model (with exactly the same parameters) on the dependency-based features extracted from the full D EP CC corpus. This model substantially outperforms also the prior state of the art models, e.g. (Baroni et al., 2014) and (Gerz et al., 2016), on the SimVerb dataset, through the sheer size of the input corpus, as previously shown, e.g. (Banko and Brill, 2001) inter alia. 5.3.3. Differences in Performance for Test/Train Sets For the SimVerb dataset, the absolute performance on the test part (SimVerb500) is higher than the absolute performance on the train part (SimVerb300) for almost all models, including the baselines. We attribute this to a specific split of the data in the dataset: our models do not use the training data to learn verb representations. 6. Conclusion In this paper, we introduced a new web-scale corpus of English texts extracted from the C OMMON C RAWL, the largest openly available linguistically analyzed corpus to date, according"
L18-1286,P14-1023,0,0.148916,"e any copyrights as the authors of this derivative resource, but while using the D EP CC corpus you need to make sure to respect the Terms of Use of the original C OMMON C RAWL dataset it is based on.29 22 https://aws.amazon.com https://aws.amazon.com/ec2 24 https://aws.amazon.com/emr 25 https://www.elastic.co/guide/en/kibana/ current/lucene-query.html 26 23 https://www.elastic.co https://www.elastic.co/products/kibana 28 https://github.com/uhh-lt/josimtext 29 http://commoncrawl.org/terms-of-use 27 1820 Model SimVerb3500 SimVerb3000 SimVerb500 SimLex222 Wikipedia+ukWaC+BNC: Count SVD 500-dim (Baroni et al., 2014) PolyglotWikipedia: SGNS BOW 300-dim (Gerz et al., 2016) 8B: SGNS BOW 500-dim (Gerz et al., 2016) 8B: SGNS DEPS 500-dim (Gerz et al., 2016) PolyglotWikipedia:SGNS DEPS 300-dim (Gerz et al., 2016) 0.196 0.274 0.348 0.356 0.313 0.186 0.333 0.350 0.351 0.304 0.259 0.265 0.378 0.389 0.401 0.200 0.328 0.307 0.385 0.390 Wikipedia: LMI DEPS wpf-1000 fpw-2000 Wikipedia+ukWac+GigaWord: LMI DEPS wpf-1000 fpw-2000 D EP CC: LMI DEPS wpf-1000 fpw-1000 D EP CC: LMI DEPS wpf-1000 fpw-2000 D EP CC: LMI DEPS wpf-2000 fpw-2000 D EP CC: LMI DEPS wpf-5000 fpw-5000 0.283 0.376 0.400 0.404 0.399 0.382 0.284 0.368 0"
L18-1286,D14-1082,0,0.0495504,"t, 2014) parsers for English available via the DKPro Core framework. To dependency-parse texts, we selected the Malt parser, due to an optimal ratio of efficiency and effectiveness (parsing of 1 Mb of text per core in 1–4 minutes). This parser was successfully used in the past for the construction of linguistically analyzed web corpora, such as P UK WAC (Baroni et al., 2009) and ENCOW16 (Sch¨afer, 2015). While more accurate parsers exist, e.g. the Stanford parser, according to our experiments, even the neural-based version of this parser is substantially slower. On the other hand, as shown by Chen and Manning (2014), the performance of the Malt parser is only about 1.5–2.5 points below the neural-based Stanford parser. In particular, we used the stack model based on the projective transition system with the Malt.19 11 http://commoncrawl.org/2016/02/ http://index.commoncrawl.org/ CC-MAIN-2016-07 13 s3://commoncrawl/crawl-data/ CC-MAIN-2016-07 14 s3://commoncrawl/contrib/c4corpus/ CC-MAIN-2016-07 12 15 https://hadoop.apache.org https://uima.apache.org 17 https://github.com/uhh-lt/lefex 18 stanfordnlp-model-ner-en-all.3class.distsim.crf, 20.04.2015 19 The used model is de.tudarmstadt.ukp.dkpro.core.maltpars"
L18-1286,de-marneffe-etal-2006-generating,0,0.284992,"Missing"
L18-1286,W14-5201,0,0.0353294,"Missing"
L18-1286,P05-1045,0,0.0307133,"r the CC-BY license. 3.3. Linguistic Analysis of Texts Linguistic analysis consists of four stages presented in Figure 1 and is implemented using the Apache Hadoop framework15 for parallelization and the Apache UIMA framework16 for integration of linguistic analysers via the DKPro Core library (Eckart de Castilho and Gurevych, 2014).17 3.3.1. POS Tagging and Lemmatization For morphological analysis of texts, we used OpenNLP part-of-speech tagger and Stanford lemmatizer. 3.3.2. Named Entity Recognition To detect occurrences of persons, locations, and organizations we use the Stanford NER tool (Finkel et al., 2005).18 Overall, 7.48 billion occurrences of named entities were identified in the 251.92 billion tokens output corpus. 3.3.3. Dependency Parsing To make large-scale parsing of texts possible, a parser needs to be not only reasonably accurate but also fast. Unfortunately, the most accurate parsers, such as Stanford parser based on the PCFG grammar (De Marneffe et al., 2006), according to our experiments, take up to 60 minutes to process 1 Mb of text on a single core, which was prohibitively slow for our use-case (details of the hardware configuration are available in Section 3.5.). We tested all v"
L18-1286,P14-1097,0,0.38862,"Missing"
L18-1286,P14-2050,0,0.0413078,"art largescale experiments with syntax-aware models without the need of long and resource-intensive preprocessing. We built an index of sentences and their linguistic meta-data accessible though an interactive web-based search interface or via a RESTful API. In our experiments on the verb similarity task, a distributional model trained on the new corpus outperformed models trained on the smaller corpora, like Wikipedia, reaching new state of the art of verb similarity on the SimVerb3500 dataset. The corpus can be used in various contexts, ranging from training of syntax-based word embeddings (Levy and Goldberg, 2014) to unsupervised induction of word senses (Biemann et al., 2018) and frame structures (Kawahara et al., 2014). A promising direction of future work is using the proposed technology for building corpora in multiple languages. 7. Acknowledgements This research was supported by the Deutsche Forschungsgemeinschaft (DFG) under the project ”Joining Ontologies and Semantics Induced from Text” (JOIN-T). We are grateful to Amazon for providing required computational resources though the “AWS Cloud Credits for Research” program. Finally, we thank Kiril Gashteovski and three anonymous reviewers for their"
L18-1286,D14-1162,0,0.0799156,"Missing"
L18-1286,D14-1101,0,0.0153647,"Ginter, 2014). The texts were morphologically and syntactically analyzed. In addition, distributional vector space representations of the words were obtained using the word2vec toolkit (Mikolov et al., 2013). The resources were made available under an open license. GloVe (Pennington et al., 2014) is an unsupervised model for learning distributional word representations similar to word2vec. The authors distribute10 two models trained on the English part of a C OMMON C RAWL corpus (comprising respectively 42 and 820 billion of tokens), which are often used to build neural NLP systems, such as (Tsuboi, 2014). The models were trained on the C OMMON C RAWL documents texts tokenized with the Stanford tokenizer. In addition, the smaller training corpus was lowercased. 10 1817 https://nlp.stanford.edu/projects/glove The Web Term Vectors, Distributional Thesaurus §3.3 WARC web crawls §3.2 Filtered preprocessed documents §3.1 Linguistic Analysis: Preprocessing: Crawling Web Pages: lefex (Apache Hadoop) C4Corpus (Apache Hadoop) CCBot (Apache Nutch) POS Tagging (OpenNLP) §5.2 Comp. of Distributional Model: JoBimText (Apache Spark) Lemmatization (Stanford) DepCC: Dependency Parsed Corpus Named Entity Recog"
L18-1286,L16-1146,0,0.0555073,"Missing"
L18-1286,J15-4004,0,0.010721,"tes models based on bag-of-word features, while “DEPS” denotes syntax-based models. SimVerb3000 and SimVerb500 are train and test partitions of the SimVerb3500, while the SimLex222 dataset is composed of verb pairs from the SimLex999 dataset. The best results in a section are boldfaced, the best results overall are underlined. 5. Evaluation: Verb Similarity Task As an example of potential use-case, we demonstrate the utility of the corpus and the overall methodology on a verb similarity task. This task structurally is the same as the word similarity tasks based on such datasets as SimLex-999 (Hill et al., 2015). Namely, a system is given two words as input and needs to predict a scalar value which characterizes semantic similarity of the input words. While in the word similarity task the input pairs are words of various parts of speeches (nouns, adjectives, etc.), in this paper we only consider verb pairs. We chose this task since verb meaning is largely defined by the meaning of its arguments (Fillmore, 1982), therefore dependency-based features seem relevant for building distributional representations of verbs. 5.1. Datasets: SimVerb3500 and SimLex222 Recently a new challenging dataset for verb re"
L18-1286,W13-3520,0,0.087056,"Missing"
L18-1286,D16-1235,0,0.0315627,"Missing"
L18-1321,S15-2151,1,0.842906,"Missing"
L18-1321,S16-1168,1,0.849431,"the methods underlying Probase are able to extract approximately 25 million pairs. The interest in the hypernymy extraction task is also illustrated by two shared tasks organised within the SemEval framework: TExEval (Taxonomy Extraction Evaluation) focused on finding hyponym-hypernym relations between a list of domain-specific English terms and subsequent taxonomy construction (Bordea et al., 2015), whereas TExEval-2 introduced a multilingual setting for this task, covering four different languages (English, Dutch, Italian and French) from domains as diverse as environment, food and science (Bordea et al., 2016). Our MIsA is an extension of the WebIsADb framework (Seitner et al., 2016) - a publicly available database with more than 400 million English hypernymy relations extracted from the CommonCrawl web corpus - where: 1. we investigate and evaluate the performance of a collection of existing and new lexico-syntactic patterns for five languages of interest (i.e., English (EN), Spanish (ES), French (FR), Italian (IT), Dutch (NL)); 2. we release a new standalone, language-independent and easy to adapt/configure extractor, which is ready to ex2040 Extractor Document splitting corpus Sentence splitting"
L18-1321,P99-1016,0,0.0412159,"key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, hypernymy relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. In the past, many different methods have been developed for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms that are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scra"
L18-1321,C92-2082,0,0.386809,"ween a generic term (hypernym) and a specific instance of it (hyponym). These relations play a key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, hypernymy relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. In the past, many different methods have been developed for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms that are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to"
L18-1321,R11-2016,0,0.0148277,"like &quot;pure-bred dogs such as a bulldog or pug&quot; (where N Pc = &quot;pure-bred dogs&quot; and N Pt = &quot;bulldog&quot;|&quot;pug&quot; is a sequence of concepts) and to produce multiple hypernymy relations from a single match (e.g., (bulldog, pure-bred dogs) and (pug, pure-bred dogs)). Some patterns are directly selected or translated from literature works, such as: i) Ponzetto and Strube (2011), where isa patterns were used to induce a taxonomy from Wikipedia; ii) Orna-Montesinos (2011), where patterns for the term “building” were extracted on a set of specialized textbooks in the field of construction engineering; iii) Klaussner and Zhekova (2011) where the authors extract IsA relations from selected Wikipedia pages and iv) research describing lexico-syntactic patterns for languages other than English, such as Lefever et al. (2014) for Dutch, Séguéla (2001) for French and Galicia-Haro and Gelbukh (2014) and Ortega-Mendoza et al. (2007) for Spanish. The remaining are brand-new experimental patterns, whose selection is dictated mainly from the experience of experts in the field of NLP. In Section 5., we provide a manual assessment of the quality of the most productive patterns that were selected for the five considered languages. 3. Extr"
L18-1321,D10-1108,0,0.0111677,"e parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the hypernym of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relation extraction consists of two phases. First, the authors bootstrap the terminology harvesting with DAP of the kind “animals such as lions and *”, so it is possible to discover new terms such as “cats”. Next, for each pair of terms in the discovered terminology, e.g. (“lions”,“cats”), they automatically create a DAP −1 of the kind “* such as lions and cats” and discover new hypernyms (e.g. “felines”). The above mentioned works fo"
L18-1321,P10-1134,0,0.0155,", 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the hypernym of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relation extraction consists of two phases. First, the authors bootstrap the terminology harvesting with DAP of the kind “animals such as lions and *”, so it is possible to discover new terms such as “cats”. Next, for each pair of terms in the discovered terminology, e.g. (“lions”,“cats”), they automatically create a DAP −1 of the kind “* such as lions and cats”"
L18-1321,L16-1056,1,0.809232,"n pairs. The interest in the hypernymy extraction task is also illustrated by two shared tasks organised within the SemEval framework: TExEval (Taxonomy Extraction Evaluation) focused on finding hyponym-hypernym relations between a list of domain-specific English terms and subsequent taxonomy construction (Bordea et al., 2015), whereas TExEval-2 introduced a multilingual setting for this task, covering four different languages (English, Dutch, Italian and French) from domains as diverse as environment, food and science (Bordea et al., 2016). Our MIsA is an extension of the WebIsADb framework (Seitner et al., 2016) - a publicly available database with more than 400 million English hypernymy relations extracted from the CommonCrawl web corpus - where: 1. we investigate and evaluate the performance of a collection of existing and new lexico-syntactic patterns for five languages of interest (i.e., English (EN), Spanish (ES), French (FR), Italian (IT), Dutch (NL)); 2. we release a new standalone, language-independent and easy to adapt/configure extractor, which is ready to ex2040 Extractor Document splitting corpus Sentence splitting POS tagging Pattern matching tuples profile Figure 1: Pipeline for the ext"
L18-1321,N03-1033,0,0.0149757,"ment of the quality of the extracted hypernymy relations for the five most productive patterns per language (30 patterns in total). For each pattern, a random sample of 100 extracted hypernym tuples was manually verified by the annotators, who assigned one of the following three labels to each matched hypernym pattern: 2. Sentence splitting: since the context of the extraction is a single sentence, we split each document in separated one-line sentences; 1. Correct: correctly extracted hypernym tuple. 3. POS tagging: each sentence is processed with a POS tagger (we use the Stanford POS-tagger (Toutanova et al., 2003) for the EN, FR and ES corpora and TreeTagger (Schmid, 1994) for the IT and NL corpora) to allow identification of N P s in the next step; 2. Partially correct: the extracted hypernym tuple is not complete (missing hyponyms, part of the instance/class is missing, e.g. Operation Little Switch was an exchange of sick and wounded prisoners resulting in (Operation Little Switch, exchange)) or is too context-dependent or vague (e.g. John Laurence is a friend resulting in (John Laurence, friend)). 4. Pattern matching: in this final step we find all matches between the lexico-syntactic patterns and t"
L18-1321,J13-3007,1,0.787811,"g techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms that are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the hypernym of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relation extraction consists of two phases. First, the authors bootstrap the terminology ha"
L18-1615,W14-1214,0,0.241936,"Missing"
L18-1615,W11-1603,0,0.00995647,"English and Spanish. At the beginning of 2016, the Newsela corpora contained around 2,000 original news articles in English and around 250 original news articles in Spanish (both with their corresponding manually simplified versions at four different simplification levels). The current state-of-the-art systems for automatic sentencealignment of original and manually simplified text are the Greedy Structural WikNet (GSWN) method (Hwang et al., 2015) used for sentence-alignment of original and simple English Wikipedia, and the HMM-based (using Hidden Markov Model and Viterbi algorithm) method (Bott and Saggion, 2011) used for sentence-alignment of the Spanish Simplext corpus (Saggion et al., 2015). The HMMbased method can be applied to any language as it does not require any language-specific resources. It is based on two hypothesis: (H1) that the original order of information is preserved, and (H2) that every ‘simple’ sentence has a corresponding ‘original’ sentence. The GSWN method does not assume H1 or H2, but it only allows for ‘1-1’ sentence alignments (which is very restricting for TS) and it is language-dependent as it requires the English Wiktionary2 . In this paper, we present a freely available"
L18-1615,W12-2910,0,0.199518,"offsets of ‘complex’ text snippets that correspond to the previous and the next aligned ‘simple’ snippets. For instance, if L = {(s1 , c4 ), (s3 , c7 )} and U = {s2 }, then the search space for the alignments of s2 is reduced to {c4 ...c7 }. We denote the MST-LIS alignment strategy by adding ‘*’ to the name of the similarity method (e.g. C3G*). simpler versions (at different levels of simplification) for each of them, and sentence-aligned them with seven different alignment strategies offered by the CATS tool: C3G, C3G*, CWASA, CWASA*, WAVG, WAVG*, C3G-2step, and the HMM-based alignment tool (Bott et al., 2012). Then we asked two native speakers of English (first trained on additional 3 original articles and their corresponding simplified versions) and two native speakers of Spanish (first trained in the same manner) to classify the obtained sentence pairs (a total of approx. 3,500 sentence-pairs for each language) in one of the four classes: 2.2.1. Modeling Sentence Splitting and Compression In both alignment strategies (MST and MST-LIS), we allow the same original sentence to be aligned with multiple simple sentences, in order to allow for modeling both sentence splitting and sentence compression"
L18-1615,N15-1022,0,0.242225,"ntences and their manual simplifications. The parallel TS corpus for Brazilian Portuguese, compiled for the purposes of the PorSimples project (Alu´ısio et al., 2008) contains around 4,500 aligned sentences, and the parallel TS corpus for Spanish, compiled for the purposes of the Simplext project (Saggion et al., 2015) contains only around 1,000 aligned sentences. The largest existing TS comparable corpora is the English Wikipedia – Simple English Wikipedia (EW–SEW), consisting of 170,000 sentence pairs (Kauchak, 2013), or 150,000 full matches and 130,000 partial matches in the newer version (Hwang et al., 2015). In both cases, the sentences were automatically aligned from comparable English Wikipedia and Simple English Wikipedia articles. However, the use of EW–SEW dataset for modeling TS has been disputed (Amancio and ˇ Specia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of the most common operations in text simplification. The Newsela corpora1 of document-aligned news"
L18-1615,P13-1151,0,0.145199,"TS is the scarcity and limited size of parallel TS corpora which would contain original sentences and their manual simplifications. The parallel TS corpus for Brazilian Portuguese, compiled for the purposes of the PorSimples project (Alu´ısio et al., 2008) contains around 4,500 aligned sentences, and the parallel TS corpus for Spanish, compiled for the purposes of the Simplext project (Saggion et al., 2015) contains only around 1,000 aligned sentences. The largest existing TS comparable corpora is the English Wikipedia – Simple English Wikipedia (EW–SEW), consisting of 170,000 sentence pairs (Kauchak, 2013), or 150,000 full matches and 130,000 partial matches in the newer version (Hwang et al., 2015). In both cases, the sentences were automatically aligned from comparable English Wikipedia and Simple English Wikipedia articles. However, the use of EW–SEW dataset for modeling TS has been disputed (Amancio and ˇ Specia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of th"
L18-1615,P15-2135,1,0.888286,"Missing"
L18-1615,P17-2016,1,0.914231,"pecia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of the most common operations in text simplification. The Newsela corpora1 of document-aligned news texts, manually simplified at four different simplification levels have been freely available for a few years for research purposes. These corpora have several advantages over the EW– ˇ SEW dataset (Xu et al., 2015; Stajner et al., 2017): (1) simplified texts present direct simplifications of the original articles; (2) simplification was performed by trained human editors, following strict guidelines; (3) by sentencealigning those corpora one can get training material for simplifications at various levels, i.e. train different simplification models depending on the intended reader group; and (4) they provide comparable training material in two languages, English and Spanish. At the beginning of 2016, the Newsela corpora contained around 2,000 original news articles in English and around 250 original news articles in Spanish ("
N06-1025,J96-1002,0,0.0286631,"Missing"
N06-1025,J02-3001,0,0.0686141,"Missing"
N06-1025,gimenez-marquez-2004-svmtool,0,0.0210029,"Missing"
N06-1025,N01-1008,0,0.470214,"s that constituents have with predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically developed for coreference resolution but simply taken ‘off-the-shelf’ an"
N06-1025,H05-1003,0,0.054662,"e best of our knowledge, we do not know of any previous work using Wikipedia or SRL for coreference resolution. In the case of SRL, this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than (still related) work involving predicate argument statistics. Kehler et al. (2004) observe no significant improvement due to predicate argument statistics. The improvement reported by Yang et al. (2005) is rather caused by their 193 twin-candidate model than by the semantic knowledge. Employing SRL is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 3 Coreference Resolution Using Semantic Knowledge Sources 3.1 Corpora Used To establish a competitive coreference resolver, the system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we moved on and developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1 . Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where"
N06-1025,O97-1002,0,0.00579472,"eatures In the baseline system semantic information is limited to WordNet semantic class matching. Unfortunately, a WordNet semantic class lookup exhibits problems such as coverage, sense proliferation and ambiguity4 , which make the WN CLASS feature very noisy. We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen et al., 2004). The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998). In our case, the measures are obtained by computing the similarity scores between the head lemmata of each potential antecedent-anaphor pair. In order to overcome the sense disambiguation problem, we factorise over all possible sense pairs: given a candidate pair, we take the cross product of each antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARITY, we compute the similarity score for all synset pairs, and create the following features. WN SIMILARITY BEST the highest similarity score from all hSENSEREi ,n , SENSEREj ,m i synset pairs. WN SIMILARI"
N06-1025,N04-1037,0,0.39369,"the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically develope"
N06-1025,W00-0730,0,0.10097,"Missing"
N06-1025,P04-1018,0,0.810468,"n extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sources. More specifically, whether a machine learning based"
N06-1025,J05-3004,0,0.0691769,"predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically developed for coreference resolution but simply taken ‘off-the-shelf’ and applied to our task without"
N06-1025,P02-1014,0,0.929837,"2003 data. In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sources. More specifical"
N06-1025,J05-1004,0,0.0236358,"ng features. WIKI RELATEDNESS BEST the highest relatedness score from all hCREi ,n , CREj ,m i category pairs. WIKI RELATEDNESS AVG the average relatedness score from all hCREi ,n , CREj ,m i category pairs. 3.6 Semantic Role Features The last semantic knowledge enhancement for the baseline system uses SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by 196 the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only when the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument label is always defined with respect to a predicate. Given such level of semantic information available at the RE level, we introduce two new features6 . I SEMROLE the semantic predicate pairs of REi . role argumentJ SEMROLE the sema"
N06-1025,N04-3012,0,0.0360973,"e U(nknown), T(rue) and F(alse). Note that in contrast to Ng & Cardie (2002) we interpret ALIAS as a lexical feature, as it solely relies on string comparison and acronym string matching. 194 3.4 WordNet Features In the baseline system semantic information is limited to WordNet semantic class matching. Unfortunately, a WordNet semantic class lookup exhibits problems such as coverage, sense proliferation and ambiguity4 , which make the WN CLASS feature very noisy. We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen et al., 2004). The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998). In our case, the measures are obtained by computing the similarity scores between the head lemmata of each potential antecedent-anaphor pair. In order to overcome the sense disambiguation problem, we factorise over all possible sense pairs: given a candidate pair, we take the cross product of each antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARI"
N06-1025,N04-1030,0,0.0618894,"pairs. That is, we take the cross product of each antecedent and anaphor category to form pairs of ‘Wikipedia synsets’. For each measure WIKI RELATEDNESS, we compute the relatedness score for all category pairs, and create the following features. WIKI RELATEDNESS BEST the highest relatedness score from all hCREi ,n , CREj ,m i category pairs. WIKI RELATEDNESS AVG the average relatedness score from all hCREi ,n , CREj ,m i category pairs. 3.6 Semantic Role Features The last semantic knowledge enhancement for the baseline system uses SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by 196 the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only when the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument"
N06-1025,J01-4004,0,0.940881,"nt Extraction (ACE) 2003 data. In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sourc"
N06-1025,J00-4003,0,0.134349,"he semantic relationships that constituents have with predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically developed for coreference resolution but simply"
N06-1025,M95-1005,0,0.252567,"pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument label is always defined with respect to a predicate. Given such level of semantic information available at the RE level, we introduce two new features6 . I SEMROLE the semantic predicate pairs of REi . role argumentJ SEMROLE the semantic predicate pairs of REj . role argumentFor the ACE 2003 data, 11,406 of 32,502 automatically extracted noun phrases were tagged with 2,801 different argument-predicate pairs. 4 Experiments 4.1 Performance Metrics We report in the following tables the MUC score (Vilain et al., 1995). Scores in Table 2 are computed for all noun phrases appearing in either the key or the system response, whereas Tables 3 and 4 refer to scoring only those phrases which appear in both the key and the response. We therefore discard those responses not present in the key, as we are interested in establishing the upper limit of the improvements given by our semantic features. That is, we want to define a baseline against which to establish the contribution of the semantic information sources explored here for coreference resolution. In addition, we report the accuracy score for all three types"
N06-1025,P94-1019,0,0.471927,"a lexical feature, as it solely relies on string comparison and acronym string matching. 194 3.4 WordNet Features In the baseline system semantic information is limited to WordNet semantic class matching. Unfortunately, a WordNet semantic class lookup exhibits problems such as coverage, sense proliferation and ambiguity4 , which make the WN CLASS feature very noisy. We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen et al., 2004). The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998). In our case, the measures are obtained by computing the similarity scores between the head lemmata of each potential antecedent-anaphor pair. In order to overcome the sense disambiguation problem, we factorise over all possible sense pairs: given a candidate pair, we take the cross product of each antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARITY, we compute the similarity score for all synset pairs, and create the following features. W"
N06-1025,P05-1021,0,0.134489,"contrast to Harabagiu et al. (2001), who weight WordNet relations differently in order to compute the confidence measure of the path. To the best of our knowledge, we do not know of any previous work using Wikipedia or SRL for coreference resolution. In the case of SRL, this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than (still related) work involving predicate argument statistics. Kehler et al. (2004) observe no significant improvement due to predicate argument statistics. The improvement reported by Yang et al. (2005) is rather caused by their 193 twin-candidate model than by the semantic knowledge. Employing SRL is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 3 Coreference Resolution Using Semantic Knowledge Sources 3.1 Corpora Used To establish a competitive coreference resolver, the system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we moved on and developed and tested the syst"
N06-1025,P03-1023,0,0.783702,"paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sources. More specifically, whether a machi"
N07-3003,C92-2082,0,0.0127715,"ce of our coreference resolution system, as well as further bringing forward Wikipedia as a direct competitor of manuallydesigned resources such as WordNet. In order to make the task feasible, we are currently concentrating on inducing is-a vs. not-is-a semantic relations. This simplifies the task, but still allows us to compute measures of semantic similarity. As we made limited use of the large amount of text in Wikipedia, we are now trying to integrate text and categorization. This includes extracting semantic relations expressed in the encyclopedic definitions by means of Hearst patterns (Hearst, 1992), detection of semantic variations (Morin & Jacquemin, 1999) between category labels, as well as using the categorized pages as bag-of-words to compute scores of idf-based semantic overlap (Monz & de Rijke, 2001) between categories. Further work will then concentrate on making this information available to our coreference resolution system, e.g. via semantic similarity computation. Finally, since Wikipedia is available in many languages, we believe it is worth performing experiments in a multilingual setting. Accordingly, we are currently testing a website2 that will allow us to collect word r"
N07-3003,I05-1082,0,0.0230926,"from a widely used standard resource such as WordNet (Fellbaum, 1998), but also that including semantic knowledge mined from Wikipedia into an NLP system dealing with coreference resolution is in fact beneficial. 2.1 WikiRelate! Computing Semantic Relatedness Using Wikipedia Semantic relatedness measures have been proven to be useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), paraphrase detection (Mihalcea et al., 2006) and spelling correction (Budanitsky & Hirst, 2006). Approaches to measuring semantic relatedness that 9 Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 9–12, c Rochester, April 2007. 2007 Association for Computational Linguistics Composers Musical activists Jazz composers page : Fela Kuti relatedness measure(s) computation ""Fela Kuti"" query ""John Zorn"" query Musicians page : John Zorn page query and retrieval, category extraction search for a connecting path along the category network Figure 1: Wikipedia-based semantic relatedness com"
N07-3003,P05-1005,0,0.0314325,"ing relations between concepts, and we computed measures of semantic relatedness from it. We did not show only that Wikipedia-based measures of semantic relatedness are competitive with the ones computed from a widely used standard resource such as WordNet (Fellbaum, 1998), but also that including semantic knowledge mined from Wikipedia into an NLP system dealing with coreference resolution is in fact beneficial. 2.1 WikiRelate! Computing Semantic Relatedness Using Wikipedia Semantic relatedness measures have been proven to be useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), paraphrase detection (Mihalcea et al., 2006) and spelling correction (Budanitsky & Hirst, 2006). Approaches to measuring semantic relatedness that 9 Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 9–12, c Rochester, April 2007. 2007 Association for Computational Linguistics Composers Musical activists Jazz composers page : Fela Kuti relatedness measure(s) computation ""Fela Kuti"""
N07-3003,P99-1050,0,0.0218315,"as further bringing forward Wikipedia as a direct competitor of manuallydesigned resources such as WordNet. In order to make the task feasible, we are currently concentrating on inducing is-a vs. not-is-a semantic relations. This simplifies the task, but still allows us to compute measures of semantic similarity. As we made limited use of the large amount of text in Wikipedia, we are now trying to integrate text and categorization. This includes extracting semantic relations expressed in the encyclopedic definitions by means of Hearst patterns (Hearst, 1992), detection of semantic variations (Morin & Jacquemin, 1999) between category labels, as well as using the categorized pages as bag-of-words to compute scores of idf-based semantic overlap (Monz & de Rijke, 2001) between categories. Further work will then concentrate on making this information available to our coreference resolution system, e.g. via semantic similarity computation. Finally, since Wikipedia is available in many languages, we believe it is worth performing experiments in a multilingual setting. Accordingly, we are currently testing a website2 that will allow us to collect word relatedness judgements from native speak2 Available at http:/"
N07-3003,P05-3019,0,0.0213119,"concepts, and we computed measures of semantic relatedness from it. We did not show only that Wikipedia-based measures of semantic relatedness are competitive with the ones computed from a widely used standard resource such as WordNet (Fellbaum, 1998), but also that including semantic knowledge mined from Wikipedia into an NLP system dealing with coreference resolution is in fact beneficial. 2.1 WikiRelate! Computing Semantic Relatedness Using Wikipedia Semantic relatedness measures have been proven to be useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), paraphrase detection (Mihalcea et al., 2006) and spelling correction (Budanitsky & Hirst, 2006). Approaches to measuring semantic relatedness that 9 Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 9–12, c Rochester, April 2007. 2007 Association for Computational Linguistics Composers Musical activists Jazz composers page : Fela Kuti relatedness measure(s) computation ""Fela Kuti"" query ""John Zorn"" query M"
N07-3003,N06-1025,1,0.915876,"come from rediscovering the use of symbolic knowledge, i.e. the deployment of large scale knowledge bases. Arguments for the necessity of symbolically encoded knowledge for AI and NLP date back at least to McCarthy (1959). Symbolic approaches using knowledge bases, however, are expensive and timeconsuming to maintain. They also have a limited and arbitrary coverage. In our work we try to overcome such problems by relying on a wide coverage on-line encyclopedia developed by a large amount of users, namely Wikipedia. That is, we are interested in whether and how Wikipedia can be integrated into Ponzetto & Strube (2006) and Strube & Ponzetto (2006) aimed at showing that ‘the encyclopedia that anyone can edit’ can be indeed used as a semantic resource for research in NLP. In particular, we assumed its category tree to represent a semantic network modelling relations between concepts, and we computed measures of semantic relatedness from it. We did not show only that Wikipedia-based measures of semantic relatedness are competitive with the ones computed from a widely used standard resource such as WordNet (Fellbaum, 1998), but also that including semantic knowledge mined from Wikipedia into an NLP system deali"
N07-3003,J01-4004,0,0.157326,"Missing"
N07-3003,P05-1047,0,0.0241287,"of semantic relatedness are competitive with the ones computed from a widely used standard resource such as WordNet (Fellbaum, 1998), but also that including semantic knowledge mined from Wikipedia into an NLP system dealing with coreference resolution is in fact beneficial. 2.1 WikiRelate! Computing Semantic Relatedness Using Wikipedia Semantic relatedness measures have been proven to be useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), paraphrase detection (Mihalcea et al., 2006) and spelling correction (Budanitsky & Hirst, 2006). Approaches to measuring semantic relatedness that 9 Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 9–12, c Rochester, April 2007. 2007 Association for Computational Linguistics Composers Musical activists Jazz composers page : Fela Kuti relatedness measure(s) computation ""Fela Kuti"" query ""John Zorn"" query Musicians page : John Zorn page query and retrieval, category extraction search for a connecting path along the category ne"
N07-3003,J06-1003,0,\N,Missing
P07-2013,P05-3002,0,0.0196779,"e accessed using robust Open Source applications, e.g. the MediaWiki software5 , integrated within a Linux, Apache, MySQL and PHP (LAMP) software bundle. The architecture of the API consists of the following modules: 1. RDBMS: at the lowest level, the encyclopedia content is stored in a relational database management system (e.g. MySQL). 2. MediaWiki: a suite of PHP routines for interacting with the RDBMS. 3. WWW-Wikipedia Perl library6 : responsible for 4 In contrast to WordNet::Similarity, which due to the structural variations between the respective wordnets was reimplemented for German by Gurevych & Niederlich (2005). 5 6 http://www.mediawiki.org http://search.cpan.org/dist/WWW-Wikipedia 50 Using the API The API provides factory classes for querying Wikipedia, in order to retrieve encyclopedia entries as well as relatedness scores for word pairs. In practice, the Java library provides a simple programmatic interface. Users can accordingly access the library using only a few methods given in the factory classes, e.g. getPage(word) for retrieving Wikipedia articles titled word or getRelatedness(word1,word2), for computing the relatedness between word1 and word2, and display(path) for displaying a path found"
P07-2013,I05-1082,0,0.0217934,"in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c r R o o m  e L g i t e o s M c r n t a X f a r o t t c h p s : x ( p e s e y t a e u e r r r y q e y g s r e C r u o r t e a P q g l t P u R e a b L T s  t e Q ilr L e a r M R S H C C e X : p : : : a r"
P07-2013,P05-1005,0,0.0161109,"etrieving Wikipedia articles titled word or getRelatedness(word1,word2), for computing the relatedness between word1 and word2, and display(path) for displaying a path found between two Wikipedia articles in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c"
P07-2013,P05-3019,0,0.0154194,"ticles titled word or getRelatedness(word1,word2), for computing the relatedness between word1 and word2, and display(path) for displaying a path found between two Wikipedia articles in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c r R o o m  e L g i t e o"
P07-2013,N04-3012,0,0.0414517,"Michael Strube EML Research gGmbH Schloss-Wolfsbrunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp 3 Abstract We present an API for computing the semantic relatedness of words in Wikipedia. 1 The API computes semantic relatedness by: 1. taking a pair of words as input; Introduction The last years have seen a large amount of work in Natural Language Processing (NLP) using measures of semantic similarity and relatedness. We believe that the extensive usage of such measures derives also from the availability of robust and freely available software that allows to compute them (Pedersen et al., 2004, WordNet::Similarity). In Ponzetto & Strube (2006) and Strube & Ponzetto (2006) we proposed to take the Wikipedia categorization system as a semantic network which served as basis for computing the semantic relatedness of words. In the following we present the API we used in our previous work, hoping that it will encourage further research in NLP using Wikipedia1 . 2 The Application Programming Interface Measures of Semantic Relatedness Approaches to measuring semantic relatedness that use lexical resources transform these resources into a network or graph and compute relatedness using paths"
P07-2013,N06-1025,1,0.816102,"brunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp 3 Abstract We present an API for computing the semantic relatedness of words in Wikipedia. 1 The API computes semantic relatedness by: 1. taking a pair of words as input; Introduction The last years have seen a large amount of work in Natural Language Processing (NLP) using measures of semantic similarity and relatedness. We believe that the extensive usage of such measures derives also from the availability of robust and freely available software that allows to compute them (Pedersen et al., 2004, WordNet::Similarity). In Ponzetto & Strube (2006) and Strube & Ponzetto (2006) we proposed to take the Wikipedia categorization system as a semantic network which served as basis for computing the semantic relatedness of words. In the following we present the API we used in our previous work, hoping that it will encourage further research in NLP using Wikipedia1 . 2 The Application Programming Interface Measures of Semantic Relatedness Approaches to measuring semantic relatedness that use lexical resources transform these resources into a network or graph and compute relatedness using paths in it (see Budanitsky & Hirst (2006) for an extensi"
P07-2013,P05-1047,0,0.0198923,"path) for displaying a path found between two Wikipedia articles in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c r R o o m  e L g i t e o s M c r n t a X f a r o t t c h p s : x ( p e s e y t a e u e r r r y q e y g s r e C r u o r t e a P q g l t P u R e a b"
P07-2013,P94-1019,0,0.0196934,"me approach with Roget’s Thesaurus while Hirst & St-Onge (1998) apply a similar strategy to WordNet. 1 The software can be freely downloaded at http://www. eml-research.de/nlp/download/wikipediasimilarity.php. 2. retrieving the Wikipedia articles they refer to (via a disambiguation strategy based on the link structure of the articles); 3. computing paths in the Wikipedia categorization graph between the categories the articles are assigned to; 4. returning as output the set of paths found, scored according to some measure definition. The implementation includes path-length (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), information-content (Resnik, 1995; Seco et al., 2004) and text-overlap (Lesk, 1986; Banerjee & Pedersen, 2003) measures, as described in Strube & Ponzetto (2006). The API is built on top of several modules and can be used for tasks other than Wikipedia-based relatedness computation. On a basic usage level, it can be used to retrieve Wikipedia articles by name, optionally using disambiguation patterns, as well as to find a ranked set of articles satisfying a search query (via integration with the Lucene2 text search engine). Additionally, it provides functionality f"
P07-2013,J06-1003,0,\N,Missing
P08-4003,P05-1022,0,0.0193699,"as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables c"
P08-4003,N07-1011,0,0.0838311,"Missing"
P08-4003,P05-1045,0,0.0128475,"RT is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A s"
P08-4003,W00-0730,0,0.105306,"grated MMAX2 functionality (annotation 1 An open source version of BART is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE m"
P08-4003,P00-1023,0,0.0981402,"Missing"
P08-4003,E06-1015,1,0.765228,"pFigure 2: Example system configuration ment. The set of feature extractors that the system uses is set in an XML description file, which allows for straightforward prototyping and experimentation with different feature sets. Learning BART provides a generic abstraction layer that maps application-internal representations to a suitable format for several machine learning toolkits: One module exposes the functionality of the the WEKA machine learning toolkit (Witten and Frank, 2005), while others interface to specialized state-of-the art learners. SVMLight (Joachims, 1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. SVM Classification uses a Java Native Interface-based wrapper replacing SVMLight/TK’s svm classify program to improve the classification speed. Also included is a Maximum entropy classifier that is based upon Robert Dodier’s translation of Liu and Nocedal’s (1989) L-BFGS optimization code, with a function for programmatic feature combination.2 Training/Testing The training and testing phases slightly differ from each other. In the training phase, the pairs that are to be used as training examples have to be selected in a process of sample selection"
P08-4003,P06-1055,0,0.0140385,"unking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables. These objects are grouped into equivalence class"
P08-4003,N06-1025,1,0.924334,"Missing"
P08-4003,qiu-etal-2004-public,0,0.168384,"Missing"
P08-4003,J01-4004,0,0.934827,"er NLP applications. Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al. (2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers. 1 A number of systems that perform coreference resolution are publicly available, such as G UITAR (Steinberger et al., 2007), which handles the full coreference task, and JAVA RAP (Qiu et al., 2004), which only resolves pronouns. However, literature on coreference resolution, if providing a baseline, usually uses the algorithm and feature set of Soon et al. (2001) for this purpose. Introduction Coreference resolution refers t"
P08-4003,N03-1033,0,0.00525991,"forming qualitative error analysis using integrated MMAX2 functionality (annotation 1 An open source version of BART is available from http://www.sfs.uni-tuebingen.de/˜versley/BART/. 10 diff, visual display). Preprocessing consists in marking up noun chunks and named entities, as well as additional information such as part-of-speech tags and merging these information into markables that are the starting point for the mentions used by the coreference resolution proper. Starting out with a chunking pipeline, which uses a classical combination of tagger and chunker, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford Named Entity Recognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on th"
P08-4003,uryupina-2006-coreference,0,0.0608766,"Missing"
P08-4003,wellner-vilain-2006-leveraging,0,0.0159334,"cognizer (Finkel et al., 2005), the desire to use richer syntactic representations led to the development of a parsing pipeline, which uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assign POS tags and uses base NPs as chunk equivalents, while also providing syntactic trees that can be used by feature extractors. BART also supports using the Berkeley parser (Petrov et al., 2006), yielding an easy-to-use Java-only solution. To provide a better starting point for mention detection on the ACE corpora, the Carafe pipeline uses an ACE mention tagger provided by MITRE (Wellner and Vilain, 2006). A specialized merger then discards any base NP that was not detected to be an ACE mention. To perform coreference resolution proper, the mention-building module uses the markables created by the pipeline to create mention objects, which provide an interface more appropriate for coreference resolution than the MiniDiscourse markables. These objects are grouped into equivalence classes by the resolution process and a coreference layer is written into the document, which can be used for detailed error analysis. Feature Extraction BART’s default resolver goes through all mentions and looks for p"
P08-4003,P06-1006,1,0.890114,"Missing"
P08-4003,P04-1018,0,\N,Missing
P08-4003,I05-1063,1,\N,Missing
P09-5006,poesio-etal-2002-acquiring,1,\N,Missing
P09-5006,N01-1008,0,\N,Missing
P09-5006,H05-1013,0,\N,Missing
P09-5006,J05-3004,0,\N,Missing
P09-5006,P04-1018,0,\N,Missing
P09-5006,N06-1025,1,\N,Missing
P09-5006,P02-1014,0,\N,Missing
P09-5006,J01-4004,0,\N,Missing
P09-5006,P07-1067,0,\N,Missing
P09-5006,P03-1023,0,\N,Missing
P10-1023,E06-1002,0,0.0938351,"Missing"
P10-1023,D09-1030,0,0.0679928,"Missing"
P10-1023,J96-2004,0,0.0214704,"Missing"
P10-1023,W06-1663,0,0.0350835,"sting gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These include EuroWordNet (Vossen, 1998), MultiWordNet (Pianta et al., 2002), the Multilingual Central Repo"
P10-1023,2007.mtsummit-papers.24,0,0.0161327,"ilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic"
P10-1023,P95-1032,0,0.0224447,"ordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations be"
P10-1023,J93-1004,0,0.121046,"(Vossen, 1998), MultiWordNet (Pianta et al., 2002), BalkaNet (Tufis¸ et al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic"
P10-1023,P08-1088,0,0.0179436,"(Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions In this paper we have pre"
P10-1023,P10-1154,1,0.129943,"nstant independent of s. As a result, determining the most appropriate sense s consists of finding the sense s that maximizes the joint probability p(s, w). We estimate p(s, w) as: p(s, w) = score(s, w) X , score(s0 , w0 ) s0 ∈SensesWN (w), w0 ∈SensesWiki (w) where score(s, w) = |Ctx(s) ∩ Ctx(w) |+ 1 (we add 1 as a smoothing factor). Thus, in our algorithm we determine the best sense s by computing the intersection of the disambiguation contexts of s and w, and normalizing by the scores summed over all senses of w in Wikipedia and WordNet. More details on the mapping algorithm can be found in Ponzetto and Navigli (2010). 3.3 3.4 Example We now illustrate the execution of our methodology by way of an example. Let us focus on the Wikipage BALLOON ( AIRCRAFT ). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S ∪ W , where S is the WordNet synset to which sense s belongs, and W includes: 4"
P10-1023,W08-2231,0,0.0104265,"unspecified relations from the link structure of Wikipedia. This result is essentially achieved by complementing WordNet with Wikipedia, as well as by leveraging the multilingual structure of the latter. Previous attempts at linking the two resources have been proposed. These include associating Wikipedia pages with the most frequent WordNet sense (Suchanek et al., 2008), extracting domain information from Wikipedia and providing a manual mapping to WordNet concepts (Auer et al., 2007), a model based on vector spaces (Ruiz-Casado et al., 2005), a supervised approach using keyword extraction (Reiter et al., 2008), as well as automatically linking Wikipedia categories to WordNet based on structural information (Ponzetto and Navigli, 2009). In contrast to previous work, BabelNet is the first proposal that integrates the relational structure of WordNet with the semi-structured information from Wikipedia into a unified, widecoverage, multilingual semantic network. Table 4: Precision of BabelNet on synonyms in WordNet (WN), Wikipedia (Wiki) and their intersection (WN ∩ Wiki): percentage and total number of words (in parentheses) are reported. not find translations in major editions of bilingual dictionarie"
P10-1023,2007.mtsummit-papers.53,0,0.0211011,"t al., 2004), Arabic WordNet (Black et al., 2006), the Multilingual Central Repository (Atserias et al., 2004), bilingual electronic dictionaries such as EDR (Yokoi, 1995), and fullyfledged frameworks for the development of multilingual lexicons (Lenci et al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions I"
P10-1023,P06-1101,0,0.107103,"ting wordnets in non-English languages. While BabelNet currently includes 6 languages, links to freely-available wordnets6 can immediately be established by utilizing the English WordNet as an interlanguage index. Indeed, BabelNet can be extended to virtually any language of interest. In fact, our translation method allows it to cope with any resource-poor language. As future work, we plan to apply our method to other languages, including Eastern European, Arabic, and Asian languages. We also intend to link missing concepts in WordNet, by establishing their most likely hypernyms – e.g., a` la Snow et al. (2006). We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon’s Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. Finally, we aim to apply BabelNet to a variety of applications which are known to benefit from a wide-coverage knowledge resource. We have already shown that the English-only subset of BabelNet allows simple knowledge-based algorithms to compete with supervised systems in standard coarse-grained and domain-specific WSD settings (Ponzetto and Navigli, 2010). We plan in th"
P10-1023,P07-2045,0,0.00504736,"age BALLOON ( AIRCRAFT ). The word is polysemous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S ∪ W , where S is the WordNet synset to which sense s belongs, and W includes: 4 We use the Google Translate API. An initial prototype used a statistical machine translation system based on Moses (Koehn et al., 2007) and trained on Europarl (Koehn, 2005). However, we found such system unable to cope with many technical names, such as in the domains of sciences, literature, history, etc. 219 this end we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf. Section 3.2.1). The context thus includes, among others, the following words: aircraft, wind, airship, lighter-thanair. We now construct the disambiguation context for the two WordNet senses of balloon (cf. Section 3.2.2), namely the aircraft (#1) and the toy (#2) senses. To do so, we include w"
P10-1023,2005.mtsummit-papers.11,0,0.00843092,"ous both in Wikipedia and WordNet. In the first phase of our methodology we aim to find a mapping µ(BALLOON ( AIRCRAFT )) to an appropriate WordNet sense of the word. To Translating Babel Synsets So far we have linked English Wikipages to WordNet senses. Given a Wikipage w, and provided it is mapped to a sense s (i.e., µ(w) = s), we create a babel synset S ∪ W , where S is the WordNet synset to which sense s belongs, and W includes: 4 We use the Google Translate API. An initial prototype used a statistical machine translation system based on Moses (Koehn et al., 2007) and trained on Europarl (Koehn, 2005). However, we found such system unable to cope with many technical names, such as in the domains of sciences, literature, history, etc. 219 this end we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf. Section 3.2.1). The context thus includes, among others, the following words: aircraft, wind, airship, lighter-thanair. We now construct the disambiguation context for the two WordNet senses of balloon (cf. Section 3.2.2), namely the aircraft (#1) and the toy (#2) senses. To do so, we include words from their synsets, hypernyms, hy"
P10-1023,W09-2413,0,0.223667,"Missing"
P10-1023,P09-1030,0,0.0230873,"t al., 2000). As it is often the case with manually assembled resources, these lexical knowledge repositories are hindered by high development costs and an insufficient coverage. This barrier has led to proposals that acquire multilingual lexicons from either parallel text (Gale and Church, 1993; Fung, 1995, inter alia) or monolingual corpora (Sammer and Soderland, 2007; Haghighi et al., 2008). The disambiguation of bilingual dictionary glosses has also been proposed to create a bilingual semantic network from a machine readable dictionary (Navigli, 2009a). Recently, Etzioni et al. (2007) and Mausam et al. (2009) presented methods to produce massive multilingual translation dictionaries from Web resources such as online lexicons and Wiktionaries. However, while providing lexical resources on a very large scale for hundreds of thousands of language pairs, these do not encode semantic relations between concepts denoted by their lexical entries. 7 Conclusions In this paper we have presented a novel methodology for the automatic construction of a large multilingual lexical knowledge resource. Key to our approach is the establishment of a mapping between a multilingual encyclopedic knowledge repository (Wi"
P10-1023,H93-1061,0,0.490473,"ES , globusCA , pallone aerostaticoIT , ballonFR , montgolfi`ereFR high wind blow gas WordNet Figure 1: An illustrative overview of BabelNet. poor languages with the aid of Machine Translation. The result is an “encyclopedic dictionary”, that provides concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations. 2 using (a) the human-generated translations provided in Wikipedia (the so-called inter-language links), as well as (b) a machine translation system to translate occurrences of the concepts within sense-tagged corpora, namely SemCor (Miller et al., 1993) – a corpus annotated with WordNet senses – and Wikipedia itself (Section 3.3). We call the resulting set of multilingual lexicalizations of a given concept a babel synset. An overview of BabelNet is given in Figure 1 (we label vertices with English lexicalizations): unlabeled edges are obtained from links in the Wikipedia pages (e.g. BALLOON ( AIRCRAFT ) links to W IND), whereas labeled ones from WordNet3 (e.g. balloon1n haspart gasbag1n ). In this paper we restrict ourselves to concepts lexicalized as nouns. Nonetheless, our methodology can be applied to all parts of speech, but in that case"
P10-1023,E09-1068,1,0.453393,"resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource. 1 Introduction In many research areas of Natural Language Processing (NLP) lexical knowledge is exploited to perform tasks effectively. These include, among others, text summarization (Nastase, 2008), Named Entity Recognition (Bunescu and Pas¸ca, 2006), Question Answering (Harabagiu et al., 2000) and text categorization (Gabrilovich and Markovitch, 2006). Recent studies in the difficult task of Word Sense Disambiguation (Navigli, 2009b, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and Rigau, 2006): richer knowledge sources can be of great benefit to both knowledge-lean systems (Navigli and Lapata, 2010) and supervised classifiers (Ng and Lee, 1996; Yarowsky and Florian, 2002). Various projects have been undertaken to make lexical knowledge available in a machine readable format. A pioneering endeavor was WordNet (Fellbaum, 1998), a computational lexicon of English based on psycholinguistic theories. Subsequent projects have also tackled the significant problem of multilinguality. These"
P10-1023,bel-etal-2000-simple,0,\N,Missing
P10-1023,D08-1080,0,\N,Missing
P10-1023,P96-1006,0,\N,Missing
P10-1023,kunze-lemnitzer-2002-germanet,0,\N,Missing
P10-1023,1995.mtsummit-1.17,0,\N,Missing
P10-1154,agirre-de-lacalle-2004-publicly,0,0.114731,"Missing"
P10-1154,E09-1005,0,0.248885,"ms to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text. Thus, our results go one step beyond previous findings (Cuadros and Rigau, 2006; Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm. We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++. In fact, more sophisticated approaches, such as Personalized PageRank (Agirre and Soroa, 2009), could be still applied to yield even higher performance. We leave such exploration to future work. Moreover, while the mapping has been used to enrich WordNet with a large amount of semantic edges, the method can be reversed and applied to the encyclopedic resource itself, that is Wikipedia, to perform disambiguation with the corresponding sense inventory (cf. the task of wikification proposed by Mihalcea and Csomai (2007) and Milne and Witten (2008b)). In this paper, we focused on English Word Sense Disambiguation. However, since WordNet++ is part of a multilingual semantic network (Navigli"
P10-1154,E06-1002,0,0.206374,"Missing"
P10-1154,J96-2004,0,0.0207335,"ding monosemous words). We selected a random sample of 1,000 Wikipages and asked an annotator with previous experience in lexicographic annotation to provide the correct WordNet sense for each page title (an empty sense label was given if no correct mapping was possible). 505 non-empty mappings were found, i.e. Wikipedia pages with a corresponding WordNet sense. In order to quantify the quality of the annotations and the difficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample. We computed the inter-annotator agreement using the kappa coefficient (Carletta, 1996) and found out that our annotators achieved an agreement coefficient κ of 0.9, indicating almost perfect agreement. Table 1 summarizes the performance of our disambiguation algorithm against the manually annotated dataset. Evaluation is performed in terms of standard measures of precision (the ratio of correct sense labels to the non-empty labels output by the mapping algorithm), recall (the ratio of correct sense labels to the total of non-empty laR ). bels in the gold standard) and F1 -measure ( P2P+R We also calculate accuracy, which accounts for 1526 Structure Gloss Structure + Gloss MFS B"
P10-1154,S07-1054,0,0.0177181,"Missing"
P10-1154,N09-1004,0,0.012826,"Missing"
P10-1154,W02-0817,0,0.0100596,"acted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the k"
P10-1154,P85-1037,0,0.197171,"better than supervised ones, and we show that, given enough knowledge, simple algorithms perform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of c"
P10-1154,W06-1663,0,0.657423,"nd tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the words in the English lexicon with high accuracy. In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet (Fellbaum, 1998). However, it has been demonstrated that the amount of lexical and semantic information Roberto Navigli Dipartimento di Informatica Sapienza Universit`a di Roma navigli@di.uniroma1.it contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006). Several methods have been proposed to automatically extend existing resources (cf. Section 2) and it has been shown that highlyinterconnected semantic networks have a great impact on WSD (Navigli and Lapata, 2010). However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet (Navigli and Velardi, 2005) or sophisticated WSD algorithms (Agirre et al., 2009). The contributions of this paper are two-fold. First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend"
P10-1154,C08-1021,0,0.106657,"ased on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale. To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). However, its current status indicat"
P10-1154,J06-1005,0,0.0117262,"ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of sta"
P10-1154,W99-0501,0,0.0910581,"Missing"
P10-1154,C92-2082,0,0.241669,"rform better than more sophisticated ones. 2 Related Work In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and"
P10-1154,S07-1068,0,0.110289,"Missing"
P10-1154,H05-1053,0,0.0362288,"PT SSI MFS BL Random BL Nouns only P/R/F1 81.0 85.5 81.1 N/A 82.3 84.1 77.4 63.5 All words P/R/F1 79.1 81.7 77.0 73.6 82.5 83.2 78.9 62.7 Algorithm k-NN † Static PR † Personalized PR † ExtLesk Degree MFS BL Random BL Table 3: Performance on Semeval-2007 coarsegrained all-words WSD with MFS as a back-off strategy when no sense assignment is attempted. 6 The differences between the results in bold in each column of the table are not statistically significant at p &lt; 0.05. Finance P/R/F1 43.4 39.6 46.9 45.6 47.8 37.1 19.6 Table 4: Performance on the Sports and Finance sections of the dataset from Koeling et al. (2005): † indicates results from Agirre et al. (2009). 4.3 gree, due to its lower recall. Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems. We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) – a knowledge-based system that participated o"
P10-1154,J03-4004,0,0.0147539,"psala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a v"
P10-1154,N07-1025,0,0.0615322,"kipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mappin"
P10-1154,H93-1061,0,0.907827,"Missing"
P10-1154,D08-1080,0,0.0123223,"ation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappi"
P10-1154,P10-1023,1,0.712486,"the corresponding semantic relation (soda2n , syrup1n ) to WordNet3 . Thus, WordNet++ represents an extension of WordNet which includes semantic associative relations between synsets. These are originally 3 Note that such relations are unlabeled. However, for our purposes this has no impact, since our algorithms do not distinguish between is-a and other kinds of relations in the lexical knowledge base (cf. Section 4.2). found in Wikipedia and then integrated into WordNet by means of our mapping. In turn, WordNet++ represents the English-only subset of a larger multilingual resource, BabelNet (Navigli and Ponzetto, 2010), where lexicalizations of the synsets are harvested for many languages using the so-called Wikipedia inter-language links and applying a machine translation system. 4 Experiments We perform two sets of experiments: we first evaluate the intrinsic quality of our mapping (Section 4.1) and then quantify the impact of WordNet++ for coarse-grained (Section 4.2) and domainspecific WSD (Section 4.3). 4.1 Evaluation of the Mapping Experimental setting. We first conducted an evaluation of the mapping quality. To create a gold standard for evaluation, we started from the set of all lemmas contained bot"
P10-1154,S07-1006,1,0.791308,"Missing"
P10-1154,E09-1068,1,0.876613,"In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets. 1 Introduction Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context (Navigli, 2009b). In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledgebased methods. In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses. Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the"
P10-1154,P06-1100,0,0.00552179,"for the enrichment of existing resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods repr"
P10-1154,W08-2231,0,0.0244515,"ing text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting. Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet. Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), a model based on vector spaces (RuizCasado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008). These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section. 3 Extending WordNet Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations connecting Wikipedia pages are transferred to WordNet. As a result, an extended version of WordNet is produced, that we call WordNet++."
P10-1154,P98-2181,0,0.13059,"Missing"
P10-1154,P09-1024,0,0.00803243,"the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on previous insights from Bunescu and Pas¸ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses. Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD. We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet,"
P10-1154,P09-1051,0,0.00741371,"d Expert project (Chklovski and Mihalcea, 2002). However, its current status indicates that the project remains a mainly academic attempt. In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost. Previous work aimed at transforming its content into a knowledge base includes opendomain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009). Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and Strube, 2007b; Milne and Witten, 2008a), coreference resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009). In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system. Our proposal builds on pre"
P10-1154,P06-1101,0,0.0556263,"resources such as WordNet. These in1522 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522–1531, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics clude proposals to extract semantic information from dictionaries (e.g. Chodorow et al. (1985) and Rigau et al. (1998)), approaches using lexicosyntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pennacchiotti and Pantel, 2006; Snow et al., 2006). Other approaches include the extraction of semantic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a). Other works rely on the disambiguation of collocations, either obtained from specialized learner’s dictionaries (Navigli and Velardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004). But while most of these methods represent state-of-the-a"
P10-1154,C98-2176,0,\N,Missing
P10-1154,W01-0703,0,\N,Missing
P12-3012,W11-0104,0,0.0185717,"trated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011). In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010). Our vision of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical knowledge base; second, tools to effectively query"
P12-3012,S07-1054,0,0.0230853,"Missing"
P12-3012,P11-1061,0,0.00401357,"– a wide-coverage multilingual lexical knowledge base – and multilingual knowledge-rich Word Sense Disambiguation (WSD). Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 1 Introduction In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntacticosemantic (Peirsman and Pad´o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language: however, this objective is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the"
P12-3012,S10-1026,0,0.0182807,": Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al., 2007). unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005). Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambiguation API against the best unsupervised and supervised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicating the benefits of using rich knowledge bases like BabelNet. In the CL-WSD evaluation, instead, using BabelNet allows us to surpass the best unsupervised system by a substantial margin, thus indicating the viability of high-performing WSD with a multilingual lexical knowledge base. While our performance still lags behind the application of supervised techniques to this task (cf. also results from Lefever a"
P12-3012,P11-1057,0,0.0592351,"2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011). In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010). Our vision of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical knowledge base; second, tools to effectively query, retrieve and exploit its information for disambiguation. Nevertheless, to date, no integrated"
P12-3012,2005.mtsummit-papers.11,0,0.00989679,"n different languages. Since the selected Babel synset can contain multiple translations in a target language for the given English word, we use for this task an Algorithm NUS-PT SUSSX-FR Degree MFS BL Random BL Nouns only 82.3 81.1 84.7 77.4 63.5 All words 82.5 77.0 82.3 78.9 62.7 Dutch French German Italian Spanish Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al., 2007). unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005). Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambiguation API against the best unsupervised and supervised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicating the benefits of using rich knowledge bases like BabelNet. In the CL-WSD e"
P12-3012,S07-1068,0,0.0178267,"7 77.4 63.5 All words 82.5 77.0 82.3 78.9 62.7 Dutch French German Italian Spanish Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al., 2007). unsupervised approach where we return for each test instance only the most frequent translation found in the synset, as given by its frequency of alignment obtained from the Europarl corpus (Koehn, 2005). Tables 1 and 2 summarize our results in terms of recall (the primary metric for WSD tasks): for each SemEval task, we benchmark our disambiguation API against the best unsupervised and supervised systems, namely SUSSX-FR (Koeling and McCarthy, 2007) and NUS-PT (Chan et al., 2007) for Coarse-WSD, and T3-COLEUR (Guo and Diab, 2010) and UvT-v (van Gompel, 2010) for CL-WSD. In the Coarse-WSD task our API achieves the best overall performance on the nouns-only subset of the data, thus supporting previous findings indicating the benefits of using rich knowledge bases like BabelNet. In the CL-WSD evaluation, instead, using BabelNet allows us to surpass the best unsupervised system by a substantial margin, thus indicating the viability of high-performing WSD with a multilingual lexical knowledge base. While our performance still lags behind the"
P12-3012,P11-2055,0,0.0842271,"Missing"
P12-3012,P11-1033,0,0.0384722,"easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 1 Introduction In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntacticosemantic (Peirsman and Pad´o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language: however, this objective is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever an"
P12-3012,P11-1134,0,0.0373594,"aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 1 Introduction In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntacticosemantic (Peirsman and Pad´o, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al., 2011). These research trends would seem to indicate the time is ripe for developing methods capable of performing semantic analysis of texts written in any language: however, this objective is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEv"
P12-3012,W09-2412,0,0.202612,"Missing"
P12-3012,P10-1023,1,0.154385,"ive is still far from being attained, as is demonstrated by research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009, WSD) continuing to be focused primarily on English. While the lack of resources has hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks on cross-lingual WSD (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010). In addition, new research on the topic has explored the translation of sentences into many languages (Navigli and Ponzetto, 2010; Lefever et al., 2011; Banea and Mihalcea, 2011), as well as the projection of monolingual knowledge onto another language (Khapra et al., 2011). In our research we focus on knowledge-based methods and tools for multilingual WSD, since knowledge-rich WSD has been shown to achieve high performance across domains (Agirre et al., 2009; Navigli et al., 2011) and to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010). Our vision of knowledge-rich multilingual WSD requires two fundamental components: first, a wide-coverage multilingual lexical k"
P12-3012,S07-1006,1,0.102631,"rring within this graph (line 5–10). Finally, we output the sense distributions of each word in lines 11–18. The disambiguation method, in turn, can be called by any other Java program in a way similar to the one highlighted by 70 the main method of lines 21–26, where we disambiguate the sample sentence ‘bank bonuses are paid in stocks’ (note that each input word can be written in any of the 6 languages, i.e. we could mix languages). 4 Experiments We benchmark our API by performing knowledgebased WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al., 2007, Coarse-WSD, henceforth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. For both experimental settings we use a standard graphbased algorithm, Degree (Navigli and Lapata, 2010), which has been previously shown to yield a highly competitive performance on different lexical disambiguation tasks (Ponzetto and Navigli, 2010). Given a semantic graph for the input context, Degree selects the sense of the target word with the highest vertex degree. In addition, in the CL-WSD setting we need to output appropriate lexicalization(s) in different languages. Since the sel"
P12-3012,E06-2006,1,0.832026,"cused on visual browsing of wide-coverage knowledge bases (Tylenda et al., 2011; Navigli and Ponzetto, 2012) by means of an API which allows the user to programmatically query and search BabelNet. This knowledge resource, in turn, can be used for eas71 Degree 15.52 22.94 17.15 18.03 22.48 T3-Coleur 10.56 21.75 13.05 14.67 19.64 UvT-v 17.70 − − − 23.39 Table 2: Performance on SemEval-2010 cross-lingual WSD (Lefever and Hoste, 2010). ily performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contributions, our toolkit for multilingual WSD takes previous work from Navigli (2006), in which an online interface for graph-based monolingual WSD is presented, one step further by adding a multilingual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD using WordNet-based measures of semantic relatedness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual ‘encyclopedic dictionary’ bringing together the lexicographic and encyclopedic knowledge from W"
P12-3012,P05-3019,0,0.0122101,"n SemEval-2010 cross-lingual WSD (Lefever and Hoste, 2010). ily performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contributions, our toolkit for multilingual WSD takes previous work from Navigli (2006), in which an online interface for graph-based monolingual WSD is presented, one step further by adding a multilingual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD using WordNet-based measures of semantic relatedness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual ‘encyclopedic dictionary’ bringing together the lexicographic and encyclopedic knowledge from WordNet and Wikipedia. Other recent projects on creating multilingual knowledge bases from Wikipedia include WikiNet (Nastase et al., 2010) and MENTA (de Melo and Weikum, 2010): both these resources offer structured information complementary to BabelNet – i.e., large amounts of facts about entities (MENTA), and explicit semantic relations harvested from Wikipedia categories (WikiNet)."
P12-3012,N04-3012,0,0.00948591,"freely available to the research community on a multilingual scale. Previous endeavors are either not freely available (EuroWordNet (Vossen, 1998)), or are only accessible via a Web interface (cf. the Multilingual Research Repository (Atserias et al., 2004) and MENTA (de Melo and Weikum, 2010)), thus providing no programmatic access. And this is despite the fact that the availability of easy-to-use libraries for efficient information access is known to foster top-level research – cf. the widespread use of semantic similarity measures in NLP, thanks to the availability of WordNet::Similarity (Pedersen et al., 2004). With the present contribution we aim to fill this gap in multilingual tools, providing a multi-tiered contribution consisting of (a) an Application Programming Interface (API) for efficiently accessing the information available in BabelNet (Navigli and 67 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 67–72, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics bn:00008364n WIKIWN 08420278n 85 WN:EN:bank WIKI:EN:Bank WIKI:DE:Bank WIKI:IT:Banca WIKIRED:DE:Finanzinstitut WN:EN:banking_company WNTR:ES:banco WNTR"
P12-3012,N10-1135,0,0.0634823,"Missing"
P12-3012,S10-1053,0,0.141569,"Missing"
P12-3012,P10-4014,0,0.0116172,"performing multilingual and cross-lingual WSD out-of-the-box. In comparison with other contributions, our toolkit for multilingual WSD takes previous work from Navigli (2006), in which an online interface for graph-based monolingual WSD is presented, one step further by adding a multilingual dimension as well as a full-fledged API. Our work also complements previous attempts by NLP researchers to provide the community with freely available tools to perform state-of-the-art WSD using WordNet-based measures of semantic relatedness (Patwardhan et al., 2005), as well as supervised WSD techniques (Zhong and Ng, 2010). We achieve this by building upon BabelNet, a multilingual ‘encyclopedic dictionary’ bringing together the lexicographic and encyclopedic knowledge from WordNet and Wikipedia. Other recent projects on creating multilingual knowledge bases from Wikipedia include WikiNet (Nastase et al., 2010) and MENTA (de Melo and Weikum, 2010): both these resources offer structured information complementary to BabelNet – i.e., large amounts of facts about entities (MENTA), and explicit semantic relations harvested from Wikipedia categories (WikiNet). Acknowledgments The authors gratefully acknowledge the sup"
P12-3012,W09-2413,0,\N,Missing
P12-3012,P10-1154,1,\N,Missing
P12-3012,S10-1002,0,\N,Missing
P12-3012,nastase-etal-2010-wikinet,0,\N,Missing
P13-5004,D11-1145,0,0.0389238,"nd social networks are capable of providing real-time information for a wide vari5 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 5–6, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ety of different social phenomena, including consumer confidence and presidential job approval polls (O’Connor et al., 2010), as well as stock market prices (Bollen et al., 2011; Ruiz et al., 2012). We focus in particular on applications that use social media for health surveillance in order to monitor, for instance, flu epidemics (Aramaki et al., 2011), as well as crisis management systems that leverage them for tracking natural disasters like earthquakes (Sakaki et al., 2010; Neubig et al., 2011) and tsunami (Zielinski and B¨urgel, 2012; Zielinski et al., 2013). gathering items in social media. In Proc. of WSDM10, pages 301–310. Graham Neubig, Yuichiroh Matsubayashi, Masato Hagiwara, and Koji Murakami. 2011. Safety information mining – what can NLP do in a disaster –. In Proceedings of IJCNLP-11, pages 965–973. Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: linking text sentime"
P13-5004,N10-1020,0,0.0298653,"ariety of social phenomena like political events, activism and stock prices, as well as to detect emerging events such as natural disasters (earthquakes, tsunami, etc.). 2. Analyzing and extracting structured information from social media. We provide an indepth overview of contributions aimed at tapping the wealth of information found within Twitter and other micro-blogs. We first show how social media can be used for many different NLP tasks, ranging from pre-processing tasks like PoS tagging (Gimpel et al., 2011) and Named Entity Recognition (Ritter et al., 2011) through high-end discourse (Ritter et al., 2010) and information extraction applications like event detection (Popescu et al., 2011; Ritter et al., 2012) and topic tracking (Lin et al., 2011). We then focus on novel tasks and challenges opened up by social media such as geoparsing, which aims to predict the location (including its geographic coordinates) of a message or user based on his posts (Gelernter and Mushegian, 2011; Han et al., 2012), and methods to automatically establish the credibility of usergenerated content by making use of contextual and metadata features (Castillo et al., 2011). The main purpose of this tutorial is to intro"
P13-5004,D11-1141,0,0.0141692,"in turn, can be used to measure the pulse of a variety of social phenomena like political events, activism and stock prices, as well as to detect emerging events such as natural disasters (earthquakes, tsunami, etc.). 2. Analyzing and extracting structured information from social media. We provide an indepth overview of contributions aimed at tapping the wealth of information found within Twitter and other micro-blogs. We first show how social media can be used for many different NLP tasks, ranging from pre-processing tasks like PoS tagging (Gimpel et al., 2011) and Named Entity Recognition (Ritter et al., 2011) through high-end discourse (Ritter et al., 2010) and information extraction applications like event detection (Popescu et al., 2011; Ritter et al., 2012) and topic tracking (Lin et al., 2011). We then focus on novel tasks and challenges opened up by social media such as geoparsing, which aims to predict the location (including its geographic coordinates) of a message or user based on his posts (Gelernter and Mushegian, 2011; Han et al., 2012), and methods to automatically establish the credibility of usergenerated content by making use of contextual and metadata features (Castillo et al., 201"
P13-5004,P11-2008,0,0.0723795,"Missing"
P13-5004,W11-0704,0,0.0217872,"vents, topics and trends. Furthermore, this information can be used to build high-end socially intelligent applications that tap the wisdom of the crowd on a large scale, thus successfully bridging the gap between computational text analysis and real-world, mission-critical applications such as financial forecasting and natural crisis management. Tutorial Outline 1. Social media and the wisdom of the crowd. We review the resources which will be the focus of the tutorial, i.e. Twitter and micro-blogging in general, and present their most prominent and distinguishing aspects (Kwak et al., 2010; Gouws et al., 2011), namely: (i) instant short-text messaging, including its specific linguistic characteristics (e.g., non-standard spelling, shortenings, logograms, etc.) and other features – i.e., mentions (@), hashtags (#), shortened URLs, etc.; (ii) a dynamic network structure where users are highly 3. Exploiting social media for real-world applications: trend detection, social sensing and crisis management. We present methods to detect emerging events and breaking news from social media (Mathioudakis et al., 2010; Petrovi´c et al., 2010, inter alia). Thanks to their highly dynamic environment and continuou"
P13-5004,C12-1064,0,0.0279885,"edia can be used for many different NLP tasks, ranging from pre-processing tasks like PoS tagging (Gimpel et al., 2011) and Named Entity Recognition (Ritter et al., 2011) through high-end discourse (Ritter et al., 2010) and information extraction applications like event detection (Popescu et al., 2011; Ritter et al., 2012) and topic tracking (Lin et al., 2011). We then focus on novel tasks and challenges opened up by social media such as geoparsing, which aims to predict the location (including its geographic coordinates) of a message or user based on his posts (Gelernter and Mushegian, 2011; Han et al., 2012), and methods to automatically establish the credibility of usergenerated content by making use of contextual and metadata features (Castillo et al., 2011). The main purpose of this tutorial is to introduce social media as a resource to the Natural Language Processing (NLP) community both from a scientific and an application-oriented perspective. To this end, we focus on micro-blogs such as Twitter, and show how it can be successfully mined to perform complex NLP tasks such as the identification of events, topics and trends. Furthermore, this information can be used to build high-end socially"
P13-5004,N10-1021,0,\N,Missing
P13-5004,I11-1108,0,\N,Missing
P17-2014,P15-2011,1,0.177095,"Missing"
P17-2014,P82-1020,0,0.855934,"Missing"
P17-2014,N15-1022,0,0.379655,"nd that the quality of simplifications in Simple English Wikipedia has been disputed before (Amancio and Specia, 2014; Xu et al., 2015), for tuning and testing we use the dataset previously released by Xu et al. (2016), which contains 2000 sentences for tuning and 359 for testing, each with eight simplification variants obtained by eight Amazon Mechanical Turkers.3 The tune subset is also used as reference corpus in combination with BLEU and SARI to select the best beam size and hypothesis for prediction reranking. Dataset To train our models, we use the publicly available dataset provided by Hwang et al. (2015) based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia (EW–SEW). We discard the uncategorized matches, and use only good matches and partial matches which were above the 0.45 threshold (Hwang et al., 2015), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches). It is one of the largest freely available resources for text simplification, and unlike the previously used EW–SEW corpus2 (Kauchak, 2013), which only contains full matches (167K pairs), the newer dataset also contains partial matches. Therefore, it is n"
P17-2014,P13-1151,0,0.6984,"approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words and entities. Instead, we make use of alignment probabilities between the predictions and the original sentences to retrieve the original words. yt 2.1 Furthermore, we are interested to explore whether large scale pre-trained embeddings can improve text simplification models. Kauchak (2013) indicates that combining normal data with simplified data can increase the performance of ATS systems. Therefore, we construct a secondary model (NTSw2v) using a combination of pre-trained word2vec from Google News corpus (Mikolov et al., 2013a) of size 300 and locally trained embeddings of size 200. To ensure good representations of lowˇ uˇrek and frequency words, we use word2vec (Reh˚ Sojka, 2010; Mikolov et al., 2013b) to train skipgram with hierarchical softmax and we set a window of 10 words. Following Garten et al. (2015) who showed that simple concatenation can improve the word represe"
P17-2014,P17-4012,0,0.0225325,"(Bojar et al., 2016). Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables. The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016). Automated text simplification (ATS) systems are meant to transform original texts into differ∗ Liviu P. Dinu1 2 Neural Text Simplification (NTS) We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), hidden states of size 500 and 500 hidden units, and a 0.3 dropout probability (Srivastava et al., 2014). The vocabulary size is set to 50,000 and we train the model for 15 epochs with plain SGD optimizer, and after epoch 8 we halve the Both authors have contributed equally to this work 85 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 85–91 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https:"
P17-2014,J10-4005,0,0.0113865,"troduction Neural sequence to sequence models have been successfully used in many applications (Graves, 2012), from speech and signal processing to text processing or dialogue systems (Serban et al., 2015). Neural machine translation (Cho et al., 2014; Bahdanau et al., 2014) is a particular type of sequence to sequence model that recently attracted a lot of attention from industry (Wu et al., 2016) and academia, especially due to the capability to obtain state-of-the-art results for various translation tasks (Bojar et al., 2016). Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables. The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016). Automated text simplification (ATS) systems are meant to transform original texts into differ∗ Liviu P. Dinu1 2 Neural Text Simplification (NTS) We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmid"
P17-2014,P15-2135,1,0.839028,"nter, University of Bucharest, Romania 2 Data and Web Science Group, University of Mannheim, Germany 3 Oracle Corporation, Romania {sergiu.nisioi,ldinu}@fmi.unibuc.ro {sanja,simone}@informatik.uni-mannheim.de Abstract ent (simpler) variants which would be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the appl"
P17-2014,P16-1100,0,0.0236322,"Missing"
P17-2014,D15-1166,0,0.0238032,"ges 85–91 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2014 learning rate. At the end of each epoch we save the current state of the model and predict the perplexity values of the models on the development set. We employ early-stopping and select the model resulted from the epoch with the best perplexity to avoid over-fitting. The parameters are initialized over uniform distribution with support [-0.1, 0.1]. Additionally, for the decoder we employ global attention in combination with input feeding as described by Luong et al. (2015). The architecture1 is depicted in Figure 1, with the input feeding approach represented only for the last hidden state of the decoder. method, to the input at the next step, presumably making the model keep track of anterior alignment decisions. Luong et al. (2015) showed this approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words"
P17-2014,1983.tc-1.13,0,0.485554,"Missing"
P17-2014,P12-1107,0,0.670247,"Missing"
P17-2014,W13-4813,0,0.0618928,"d be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare ou"
P17-2014,Q16-1029,0,0.720017,"In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervi"
P17-2014,W16-4912,0,0.266157,"tempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervised lexical simplification systems. We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS sy"
P17-2014,P02-1040,0,0.118354,"tSimplification 86 described previously (NTS and NTS-w2v). Beam search works by generating the first k hypotheses at each step ordered by the log-likelihood of the target sentence given the input sentence. By default, we use a beam size of 5 and take the first hypothesis, but we also observe that higher beam size and lower-ranked hypotheses can generate good simplification results. Therefore, we generate the first two candidate hypotheses for each beam size from 5 to 12. We then attempt to find the best beam size and hypothesis based on two metrics: the traditional MT-evaluation metric, BLEU (Papineni et al., 2002; Bird et al., 2009) with NIST smoothing (Bird et al., 2009), and SARI (Xu et al., 2016), a recent text-simplification metric. 2.3 in the corpus. A brief analysis of the vocabulary is rendered in Table 1. The dataset we use contains an abundant amount of named entities and consequently a large amount of low frequency words, but the majority of entities are not part of the model’s 50,000 words vocabulary due to their small frequency. These words are replaced with ’UNK’ symbols during training. At prediction time, we replace the unknown words with the highest probability score from the attention"
P17-2014,C10-1152,0,0.729324,"ariants which would be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evalu"
P17-2014,N13-1092,0,\N,Missing
P17-2014,W16-2301,0,\N,Missing
P17-2014,P05-1045,0,\N,Missing
P17-2016,J03-1002,0,0.0481646,"Missing"
P17-2016,W10-1607,0,0.0716558,"Missing"
P17-2016,W16-4912,0,0.0987186,"in {sanja,simone,heiner}@informatik.uni-mannheim.de marc.franco@symanto.net, prosso@prhlt.upv.es 1 Abstract (EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015). For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches. The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaˇs ˇ and Stajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning. However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far. The Newsela corpora1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3–4 different complexity levels following strict guidelines (Xu et al., 2015). Although it was suggested that it has better quality than the EW–SEW corpus (Xu et al., 2015), Newsela has not yet been used for training end-to-end ATS syste"
P17-2016,W11-1603,0,0.346565,"ish is the English Wikipedia – Simple English Wikipedia 1 Freely available: https://newsela.com/data/ 97 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2016 3 able software;2 (2) compare the performances of lexically- and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification (Bott and Saggion, 2011) by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PBSMT) approach to text simplifications. 2 Approach Having a set of ‘simple’ text snippets S and a set of ‘complex’ text snippets C, we offer two strategies (Section 3.1) to obtain the alignments (si , cj ), where si ∈ S, cj ∈ C. Each alignment strategy, in turn, can use one of the three methods (Section 3.2) to calculate similarity scores between text snippets (either paragraphs or"
P17-2016,P15-2011,1,0.868371,"Missing"
P17-2016,P08-1040,0,0.0631219,"even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems. 1 Introduction Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning. It has attracted much attention recently as it could make texts more accessible to wider audiences (Alu´ısio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; ˇ Stajner and Popovi´c, 2016). However, the state-of-the-art ATS systems still do not reach satisfying performances and require ˇ some human post-editing (Stajner and Popovi´c, 2016). While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Speˇ cia, 2010; Stajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training. The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simpl"
P17-2016,N15-1022,0,0.197548,"ext Simplification Systems 1 ˇ Sanja Stajner , Marc Franco-Salvador2,3 , Simone Paolo Ponzetto1 , Paolo Rosso3 , Heiner Stuckenschmidt1 DWS Research Group, University of Mannheim, Germany 2 Symanto Research, Nuremberg, Germany 3 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {sanja,simone,heiner}@informatik.uni-mannheim.de marc.franco@symanto.net, prosso@prhlt.upv.es 1 Abstract (EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015). For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches. The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaˇs ˇ and Stajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning. However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far. The Newsela corpora1 offers over 2,000 original news articl"
P17-2016,P15-2135,1,0.883593,"xts more accessible to wider audiences (Alu´ısio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; ˇ Stajner and Popovi´c, 2016). However, the state-of-the-art ATS systems still do not reach satisfying performances and require ˇ some human post-editing (Stajner and Popovi´c, 2016). While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Speˇ cia, 2010; Stajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training. The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simple English Wikipedia 1 Freely available: https://newsela.com/data/ 97 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2016 3 able software;2 (2) compare the performances of lexically- and semantically-based alignment meth"
P17-2016,P07-2045,0,0.0103568,"3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the ‘direct’ sentence-alignment C3G method. 5 Extrinsic Evaluation Finally, we test our new English Newsela (C3G2s) sentence-alignments (both for the neighbouring levels – neighb. and for all levels – all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method6 (Bott and Saggion, 2011) in the ATS task using standard PBSMT models7 in the Moses toolkit (Koehn et al., 2007). We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning (Xu et al., 2016) and the first 70 sentence 8 Both freely available from: https://github.com/ cocoxu/simplification/ 9 We use the output of the original SBMT (Xu et al., 2016) ˇ and LightLS (Glavaˇs and Stajner, 2015) systems, obtained from the authors. 10 Those cases in which the two annotators did not agree are additionally evaluated by a third annotator to obtain majority. 6 Given that the performance of the HMM-method was poor for non-neighboring"
P17-2016,W16-3411,1,0.914076,"Missing"
P17-2016,N03-1017,0,0.0759083,"Missing"
P17-2016,Q16-1029,0,0.186509,"he HMM method on the paragraph-alignment 0–1 task, there is still significant difference in the obtained scores (in some cases, our methods led to good matches whereas the HMM only led to partial matches). 99 Sentence 0–1 0–4 3–4 C3G 98.3 56.1 81.1 C3G* 96.7 54.7 78.8 CWASA 98.3 45.3 79.7 CWASA* 96.1 42.1 76.4 WAVG 97.8 56.1 79.7 WAVG* 96.1 50.0 79.7 C3G-2s 98.5 57.8 83.5 HMM 86.2 25.2 65.6 Method pairs of their test set8 for our human evaluation. Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS (Xu et al., 2016) which is not freely available. We compare: (1) the performance of the standard PBSMT model which uses only the already available EW–SEW dataset (Hwang et al., 2015) with the performances of the same PBSMT models but this time using the combination of the EW–SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW–SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (Xu et al., 2016), and one of the recently proposed unsupervised lexical simplification systems, the LightLS ˇ system (Glavaˇs and Stajner, 2015).9 We perform th"
P18-2010,P98-1013,0,0.679639,"ully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation"
P18-2010,bauer-etal-2012-dependency,0,0.336218,"Missing"
P18-2010,P10-2045,0,0.591263,"Missing"
P18-2010,W06-3812,1,0.865724,"56 Algorithm 1 Triframes frame induction Input: an embedding model v ∈ V → ~v ∈ Rd , a set of SVO triples T ⊆ V 3 , the number of nearest neighbors k ∈ N, a graph clustering algorithm C LUSTER. Output: a set of triframes F . 1: S ← {t → ~ t ∈ R3d : t ∈ T } 0 0 ~ 2: E ← {(t, t ) ∈ T 2 : t0 ∈ NNS k (t), t 6= t } 3: F ← ∅ 4: for all C ∈ C LUSTER(T, E) do 5: fs ← {s ∈ V : (s, v, o) ∈ C} 6: fv ← {v ∈ V : (s, v, o) ∈ C} 7: fo ← {o ∈ V : (s, v, o) ∈ C} 8: F ← F ∪ {(fs , fv , fo )} 9: return F ate sense-aware representation that is clustered using the Chinese Whispers (CW) hard clustering algorithm (Biemann, 2006). We chose WATSET due to its performance on the related synset induction task, its fuzzy nature, and the ability to find the number of frames automatically. 3 Evaluation Input Corpus. In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings model trained on the Google News corpus (Mikolov et al., 2013). All evaluated algorithms are executed on the same set of triples, eliminating variations due to different corpora or pre-processing. G = (T, E)"
P18-2010,J02-3001,0,0.262426,"automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks requiring expertise in the underlying knowledge. Consequently, such resources exist only for a few languages (Boas,"
P18-2010,E12-1059,0,0.0392429,"chmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent ye"
P18-2010,Q16-1015,0,0.188294,"Missing"
P18-2010,S17-1025,0,0.132917,"ional Linguistics The contributions of this paper are three-fold: (1) we are the first to apply triclustering algorithms for unsupervised frame induction, (2) we propose a new approach to triclustering, achieving state-of-the-art performance on the frame induction task, (3) we propose a new method for the evaluation of frame induction enabling straightforward comparison of approaches. In this paper, we focus on the simplest setup with subject-verbobject (SVO) triples and two roles, but our evaluation framework can be extended to more roles. In contrast to the recent approaches like the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major i"
P18-2010,P14-1097,0,0.532208,"r a few languages (Boas, 2009) and even English is lacking domain-specific frame-based resources. Possible inroads are cross-lingual semantic annotation transfer (Pad´o and Lapata, 2009; Hartmann Triclustering. In this work, we cast the frame induction problem as a triclustering task (Zhao and Zaki, 2005; Ignatov et al., 2015), namely a generalization of standard clustering and biclustering (Cheng and Church, 2000), aiming at simultaneously clustering objects along three dimensions (cf. Table 1). First, using triclustering allows to avoid sequential nature of frame induction approaches, e.g. (Kawahara et al., 2014), where two independent clusterings are needed. Second, benchmarking frame induction as triclustering against other methods on dependency triples allows to abstract away the evaluation of the frame induction algorithm from other factors, e.g., the input corpus or pre-processing steps, thus allowing a fair comparison of different induction models. 55 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55–62 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The contributions of this paper are three"
P18-2010,P03-1009,0,0.454771,"the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major issue with unsupervised frame induction task is that these and some other related approaches, e.g., (O’Connor, 2013), were all evaluated in completely different incomparable settings, and used different input corpora. In this paper, we propose a methodology to resolve this issue. 2 The Triframes Algorithm Our approach to frame induction relies on graph clustering. We focused on a simple setup using two roles and the SVO triples, arguing that it still can be useful, as frame roles are primarily expressed by subjects and objects, giving rise to semantic structures extracted in an unsup"
P18-2010,L18-1286,1,0.87147,"E ← {(t, t ) ∈ T 2 : t0 ∈ NNS k (t), t 6= t } 3: F ← ∅ 4: for all C ∈ C LUSTER(T, E) do 5: fs ← {s ∈ V : (s, v, o) ∈ C} 6: fv ← {v ∈ V : (s, v, o) ∈ C} 7: fo ← {o ∈ V : (s, v, o) ∈ C} 8: F ← F ∪ {(fs , fv , fo )} 9: return F ate sense-aware representation that is clustered using the Chinese Whispers (CW) hard clustering algorithm (Biemann, 2006). We chose WATSET due to its performance on the related synset induction task, its fuzzy nature, and the ability to find the number of frames automatically. 3 Evaluation Input Corpus. In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings model trained on the Google News corpus (Mikolov et al., 2013). All evaluated algorithms are executed on the same set of triples, eliminating variations due to different corpora or pre-processing. G = (T, E) by constructing the edge set E ⊆ T 2 . For that, we compute k ∈ N nearest neighbors of each triple vector ~t ∈ R3d and establish cosine similarity-weighted edges between the corresponding triples. Then, we assume that the triples representing similar contexts appear in simil"
P18-2010,N16-1173,0,0.0731203,"us on the simplest setup with subject-verbobject (SVO) triples and two roles, but our evaluation framework can be extended to more roles. In contrast to the recent approaches like the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major issue with unsupervised frame induction task is that these and some other related approaches, e.g., (O’Connor, 2013), were all evaluated in completely different incomparable settings, and used different input corpora. In this paper, we propose a methodology to resolve this issue. 2 The Triframes Algorithm Our approach to frame induction relies on graph clustering. We focused on a simple setup using two roles an"
P18-2010,laparra-rigau-2010-extended,0,0.0592218,"data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques."
P18-2010,D07-1002,0,0.442772,"FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks requiring expertise in the underlying knowledge. Consequently, such resources exist only for a few languages (Boas, 2009) and even English is lacking domain-specific frame-based resources. Possible inroads are cross-lingual semantic annotation transfer (Pad´o and Lapata, 2009; Hartmann Triclusterin"
P18-2010,M92-1001,0,0.788705,"Missing"
P18-2010,N13-1051,0,0.267798,"Missing"
P18-2010,P11-1145,0,0.0980018,"Missing"
P18-2010,E12-1003,0,0.34924,"Missing"
P18-2010,W09-1127,0,0.141963,"of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised fr"
P18-2010,P17-1145,1,0.73043,"frames F as presented in Algorithm 1. The hyper-parameters of the algorithm are the number of nearest neighbors for establishing edges (k) and the graph clustering algorithm C LUSTER. During the concatenation of the vectors for words forming triples, the (|T |× 3d)-dimensional vector space S is created. Thus, given the triple t ∈ T , we denote the k nearest neighbors extraction procedure of its concatenated embedding from S as NNSk (~t) ⊆ T . We used k = 10 nearest neighbors per triple. To cluster the nearest neighbor graph of SVO triples G, we use the WATSET fuzzy graph clustering algorithm (Ustalov et al., 2017). It treats the vertices T of the input graph G as the SVO triples, induces their senses, and constructs an intermediDatasets. We cast the complex multi-stage frame induction task as a straightforward triple clustering task. We constructed a gold standard set of triclusters, each corresponding to a FrameNet frame, similarly to the one illustrated in Table 1. To construct the evaluation dataset, we extracted frame annotations from the over 150 thousand sentences from the FrameNet 1.7 (Baker et al., 1998). Each sentence contains data about the frame, FEE, and its arguments, which were used to ge"
P18-2010,erk-pado-2006-shalmaneser,0,\N,Missing
P18-2010,J14-1002,0,\N,Missing
P18-2010,E17-2028,0,\N,Missing
P19-1490,E17-1088,0,0.0390417,"t al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language. Another instructive baseline is the"
P19-1490,W13-3520,0,0.0279406,"alisation performed by CLEAR, and analyse its performance in comparison with distributional word vectors and non-specialised cross-lingual word embeddings. 4.1 Experimental Setup Distributional Vectors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual const"
P19-1490,P18-1073,0,0.226842,"pecialisation of the English distributional space, and then 2) translates all test examples in the target language to English relying on the bilingual dictionary D.10 All LE reasoning is then conducted monolingually in English. The TRANS baseline is also used in cross-lingual graded LE evaluation. For cross-lingual datasets without English (e.g., DE-IT), we again translate all words to English and use the English specialised space for graded LE assertions. In addition, for each language pair we also report results of two stateof-the-art cross-lingual word embedding models (Smith et al., 2017; Artetxe et al., 2018), showing the better scoring one in each run (XEMB). For ungraded LE evaluation, in addition to TRANS, we compare CLEAR to two bestperforming baselines from (Upadhyay et al., 2018): they couple two methods for inducing syntactic cross-lingual vectors: 1) B I -S PARSE (Vyas and Carpuat, 2016) and 2) CL-D EP (Vuli´c, 2017) with an LE scorer based on the distributional inclusion hypothesis (Geffet and Dagan, 2005). For more details we refer the reader to (Upadhyay et al., 2018). 9 The translations in PanLex were derived from various sources (e.g., glossaries, dictionaries, automatic inference). T"
P19-1490,Q17-1010,0,0.0292145,"my constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior"
P19-1490,P13-1133,0,0.119829,"Missing"
P19-1490,P05-1014,0,0.443497,"and category vagueness from cognitive science (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Co"
P19-1490,S17-2002,0,0.301956,"peakers.5 Cross-Lingual Datasets. The cross-lingual CL HYPERLEX datasets were then constructed automatically, leveraging word pair translations and scores in three target languages. To this end, we follow the methodology of Camacho-Collados et al. (2015, 2017), used previously for creating cross-lingual semantic similarity datasets. In short, we first intersect aligned concept pairs (obtained through translation) in two languages: e.g., father-ancestor in English and padre-antenato in Italian are used 5 As opposed to (Hill et al., 2015; Gerz et al., 2016; Vuli´c et al., 2017), but similar to (Camacho-Collados et al., 2017; Pilehvar et al., 2018) we did not divide the dataset into smaller tranches; each annotator scored the entire target-language dataset instead. The target languages were selected based on the availability of native speakers; the total number of annotations was restricted by the annotation budget. 4965 Monolingual Datasets 50 picture Person Fahrrad cibo rekreacija 5.90 4.0 0.25 3.25 5.75 Cross-Lingual Datasets (CL - HYPERLEX) EN - DE EN - IT EN - HR DE - IT DE - HR IT- HR dinosaur eye religija Medikation Form aritmetica Kreatur viso belief trattamento prizma matematika EN DE IT HR 40 Percentage"
P19-1490,D16-1235,1,0.922237,"Missing"
P19-1490,P15-2001,0,0.450197,"al lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automatically derive a collection of six cross-lingual GR - LE datasets: CL - HYPERLEX. We analyse in detail the cross-lingual datasets (e.g., by comparing the scores to human-elicited ratings), demonstrating their robustness and reliability. In order to provide a competitive baseline on new monolingual and cross-lingual datasets, we next introduce a cross-lingual specialisation/retrofitting method termed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel): starting from any two monolingual distributional spaces, CLEAR induces a bilingual cross-lingual space that reflects the as"
P19-1490,D18-1269,0,0.159875,"05; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium"
P19-1490,D16-1136,0,0.0207865,"embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language. Another instruc"
P19-1490,ehrmann-etal-2014-representing,0,0.625,"5; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proc"
P19-1490,P14-1113,0,0.103701,"6, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proceedings of the 57t"
P19-1490,P18-1004,1,0.894858,"Missing"
P19-1490,P19-1070,1,0.857159,"Missing"
P19-1490,D17-1185,1,0.786418,"Missing"
P19-1490,P19-1476,1,0.771144,"Missing"
P19-1490,L18-1550,0,0.0176646,"ors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in othe"
P19-1490,J15-4004,0,0.693533,"ation of concepts (i.e., it is not tied to a particular language), a graded LE repository has so far been created only for English: it is the HyperLex dataset of Vuli´c et al. (2017). Starting from the established data creation protocol for HyperLex, in this work we compile similar HyperLex datasets in three other languages and introduce novel multilingual and cross-lingual GR - LE tasks. Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the GR - LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymyhypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the GR - LE “To what degree...” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates strong LE relation between the concepts X"
P19-1490,W19-4310,1,0.844555,"Missing"
P19-1490,kamholz-etal-2014-panlex,0,0.0402875,"glish such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR t"
P19-1490,P15-2020,1,0.932684,"Missing"
P19-1490,P14-2050,0,0.0537507,"lyse the usefulness of cross-lingual graded LE specialisation performed by CLEAR, and analyse its performance in comparison with distributional word vectors and non-specialised cross-lingual word embeddings. 4.1 Experimental Setup Distributional Vectors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dicti"
P19-1490,W14-0405,0,0.0262033,"Missing"
P19-1490,S13-2005,0,0.197578,"Missing"
P19-1490,N15-1100,0,0.0526911,"rces: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based E"
P19-1490,P09-1034,0,0.0344902,"actual language (the example shows English and Spanish words with the respective prefixes en_ and es_) is reflected by their small co−−−−−−→ sine distances (e.g., the small angle between en_beagle − − − − − − − → −−−−−→ and en_animal), while simultaneously and − es_perro higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space. An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space. terpretability (Padó et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-sem"
P19-1490,D14-1162,0,0.0828019,"n LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 langua"
P19-1490,N16-1118,0,0.0352914,"Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex"
P19-1490,N18-1202,0,0.0212284,"rom WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al."
P19-1490,S10-1002,0,0.0931379,"with the respective prefixes en_ and es_) is reflected by their small co−−−−−−→ sine distances (e.g., the small angle between en_beagle − − − − − − − → −−−−−→ and en_animal), while simultaneously and − es_perro higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space. An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space. terpretability (Padó et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automa"
P19-1490,D18-1169,0,0.0500479,"E), 0.909 (EN - IT), and 0.905 (EN - HR). LE 3 Methodology In order to provide benchmarking graded LE scores on new monolingual and cross-lingual evaluation sets, we now introduce a novel method that can capture GR - LE cross-lingually. CLEAR ( CrossLingual Lexical Entailment Attract-Repel) is a cross-lingual extension of the monolingual LEAR specialisation method (Vuli´c and Mrkši´c, 2018), a state-of-the-art vector space fine-tuning method which specialises any input distributional vector 6 Similarity benchmarks report much lower Pairwise-IAA scores: 0.61 on SimVerb-3500 (Gerz et al., 2016; Pilehvar et al., 2018), and 0.67 on SimLex-999 (Hill et al., 2015) and on WordSim-353 (Finkelstein et al., 2002) CLEAR Specialisation. A high-level overview of the CLEAR specialisation method is provided in Figure 3. The input to the method is as follows: 1) two independently trained monolingual word vector spaces in two languages L1 and L2 ; 2) sets of external lexical constraints in the resource-rich language L1 (e.g., English) extracted from an external lexical resource such as WordNet (Fellbaum, 1998) or BabelNet (Ehrmann et al., 2014); and 3) a bilingual L1 -L2 dictionary D. The goal is to fine-tune input word"
P19-1490,P17-1163,0,0.0216099,"Missing"
P19-1490,Q17-1022,1,0.924323,"Missing"
P19-1490,S12-1053,0,0.0762876,"Missing"
P19-1490,D18-1026,1,0.864529,"Missing"
P19-1490,P18-2101,1,0.869814,"Missing"
P19-1490,P18-2057,0,0.13905,"ion “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song"
P19-1490,E14-4008,0,0.0904314,"ce (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multil"
P19-1490,N16-1142,0,0.265208,"uman judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rat"
P19-1490,P16-1226,0,0.17476,"Missing"
P19-1490,C14-1212,0,0.0844206,"from cognitive science (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b)"
P19-1490,E17-1007,0,0.171718,"Missing"
P19-1490,D14-1161,0,0.062278,"ome from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al.,"
P19-1490,P14-1068,0,0.0674999,"Missing"
P19-1490,L18-1558,0,0.0166563,"2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proceedings of the 57th Annual Meeting of the Association for Computational"
P19-1490,N18-1056,0,0.255581,"for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In cont"
P19-1490,E17-2065,1,0.908149,"Missing"
P19-1490,J17-4004,1,0.784862,"Missing"
P19-1490,N18-1103,1,0.896402,"Missing"
P19-1490,D17-1270,1,0.902928,"Missing"
P19-4004,D13-1191,0,0.0252605,"litical text scaling – one of the central tasks in quantitative political science, where the goal is to quantify positions of politicians and/or parties on a scale based on the textual content they produce – has not received any attention by the NLP community until last year, whereas it has been at the core of political science research for almost two decades. At the same time, NLP researchers have addressed closely related tasks such as election prediction (O’Connor et al., 2010), ideology classification (Hirst et al., 2010), stance detection (Thomas et al., 2006), and agreement measurement (Gottipati et al., 2013), all rarely considered in the same format by the textas-data political science community. In summary, these two communities have been largely agnostic of one another, resulting in NLP researchers not contributing to relevant research questions in political science and political scientists not employing cutting-edge NLP methodology for their tasks. The development and adoption of natural language processing (NLP) methods by the political science community dates back to over twenty years ago. In the last decade the usage of computational methods for text analysis has drastically expanded in sco"
P19-4004,W16-2102,1,0.888838,"Missing"
P19-4004,N18-1178,0,0.0198472,"ial Overview 3. Topical Analysis of Political Texts. Next, we focus on a large body of work of topical analysis of political texts, covering unsupervised topic induction, including dictionarybased, topic-modelling and text segmentation approaches (Quinn et al., 2006, 2010; Grimmer, 2010; Albaugh et al., 2013; Glavaˇs et al., 2016; Menini et al., 2017), as well as supervised topic classification studies (Hillard et al., 2008; Collingwood and Wilkerson, 2012; Karan et al., 2016). We will also cover more recent work on cross-lingual topic classification in political texts (Glavaˇs et al., 2017a; Subramanian et al., 2018). We will further emphasize topic classification models that exploit large manually anotated corpora from CMP (Zirn et al., 2016; Subramanian et al., 2017) and CAP (Karan et al., 2016; Albaugh et al., 2013) projects, which we cover in the previous part. This introductory tutorial aims to systematically organise and analyse the overall body of research in computational analysis of political texts. This body of work has been split between two largely disjoint research communities – researchers in natural language processing and researchers in political science – and the tutorial is designed bear"
P19-4004,U17-1003,0,0.018116,"opic induction, including dictionarybased, topic-modelling and text segmentation approaches (Quinn et al., 2006, 2010; Grimmer, 2010; Albaugh et al., 2013; Glavaˇs et al., 2016; Menini et al., 2017), as well as supervised topic classification studies (Hillard et al., 2008; Collingwood and Wilkerson, 2012; Karan et al., 2016). We will also cover more recent work on cross-lingual topic classification in political texts (Glavaˇs et al., 2017a; Subramanian et al., 2018). We will further emphasize topic classification models that exploit large manually anotated corpora from CMP (Zirn et al., 2016; Subramanian et al., 2017) and CAP (Karan et al., 2016; Albaugh et al., 2013) projects, which we cover in the previous part. This introductory tutorial aims to systematically organise and analyse the overall body of research in computational analysis of political texts. This body of work has been split between two largely disjoint research communities – researchers in natural language processing and researchers in political science – and the tutorial is designed bearing this in mind. We first explain the role that textual data plays in political analyses and then proceed to examine the concrete resources and tasks addr"
P19-4004,W06-1639,0,0.0203801,"heim.de 1 Introduction community. For example, political text scaling – one of the central tasks in quantitative political science, where the goal is to quantify positions of politicians and/or parties on a scale based on the textual content they produce – has not received any attention by the NLP community until last year, whereas it has been at the core of political science research for almost two decades. At the same time, NLP researchers have addressed closely related tasks such as election prediction (O’Connor et al., 2010), ideology classification (Hirst et al., 2010), stance detection (Thomas et al., 2006), and agreement measurement (Gottipati et al., 2013), all rarely considered in the same format by the textas-data political science community. In summary, these two communities have been largely agnostic of one another, resulting in NLP researchers not contributing to relevant research questions in political science and political scientists not employing cutting-edge NLP methodology for their tasks. The development and adoption of natural language processing (NLP) methods by the political science community dates back to over twenty years ago. In the last decade the usage of computational metho"
S10-1021,broscheit-etal-2010-extending,1,0.899254,"sign their basic properties (number, gender etc). The feature extraction module describes pairs of mentions {Mi , Mj }, i &lt; j as a set of features. The decoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the encoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 3 Language-specific issues Below we briefly describe our language-specific extensions to BART. These issues are addressed in more details in our recent papers (Broscheit et al., 2010; Poesio et al., 2010). 3.1 Mention Detection Robust mention detection is an essential component of any coreference resolution system. BART supports different pipelines for mention detection. The 104 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Basic features Syntactic features LanguagePlugin Mention (with basic properties): Parser Unannotated Text Knowledge-based features - Number - Gender - Mention Type - Modifiers Dep-to-Const Converter Coreference Chains Morp"
S10-1021,finthammer-cramer-2008-exploring,0,0.0135445,"et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R - SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj . We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraints, allowing for lower-case characters in the abbreviations. Our pilot experiments suggest that, although"
S10-1021,P06-1055,0,0.00843219,"Number - Gender - Mention Type - Modifiers Dep-to-Const Converter Coreference Chains Morphology Decoder Mention Factory Preprocessing MaxEnt Classifier Figure 1: BART architecture choice of a pipeline depends crucially on the availability of linguistic resources for a given language. For English and German, we use the Parsing Pipeline and Mention Factory to extract mentions. The parse trees are used to identify minimal and maximal noun projections, as well as additional features such as number, gender, and semantic class. For English, we use parses from a state-of-the-art constituent parser (Petrov et al., 2006) and extract all base noun phrases as mentions. For German, the SemEval dependency tree is transformed to a constituent representation and minimal and maximal phrases are extracted for all nominal elements (pronouns, common nouns, names), except when the noun phrase is in a non-referring syntactic position (for example, expletive “es”, predicates in copula constructions). For Italian, we use the EMD Pipeline and Mention Factory. The Typhoon (Zanoli et al., 2009) and DEMention (Biggio et al., 2009) systems were used to recognize mentions in the test set. For each mention, its head and extension"
S10-1021,poesio-etal-2010-creating,1,0.825324,"ies (number, gender etc). The feature extraction module describes pairs of mentions {Mi , Mj }, i &lt; j as a set of features. The decoder generates training examples through a process of sample selection and learns a pairwise classifier. Finally, the encoder generates testing examples through a (possibly distinct) process of sample selection, runs the classifier and partitions the mentions into coreference chains. 3 Language-specific issues Below we briefly describe our language-specific extensions to BART. These issues are addressed in more details in our recent papers (Broscheit et al., 2010; Poesio et al., 2010). 3.1 Mention Detection Robust mention detection is an essential component of any coreference resolution system. BART supports different pipelines for mention detection. The 104 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104–107, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Basic features Syntactic features LanguagePlugin Mention (with basic properties): Parser Unannotated Text Knowledge-based features - Number - Gender - Mention Type - Modifiers Dep-to-Const Converter Coreference Chains Morphology Decoder Mention"
S10-1021,schmid-etal-2004-smor,0,0.007832,"tween the two. German. We have tested extra features for German in our previous study (Broscheit et al., 2010). The NodeDistance feature measures the number of clause nodes (SIMPX, R - SIMPX) and prepositional phrase nodes (PX) along the path between Mj and Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj . We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraint"
S10-1021,J01-4004,0,0.426265,"tances are modeled as feature vectors (cf. Table 1) and are handed over to a binary classifier that decides, given the features, whether the anaphor and the candidate are coreferent or not. All the feature values are computed automatically, without any manual intervention. Basic feature set. We use the same set of relatively language-independent features as a backbone of our system, extending it with a few languagespecific features for each subtask. Most of them are used by virtually all the state-of-the-art coreference resolution systems. A detailed description can be found, for example, in (Soon et al., 2001). English. Our English system is based on a novel model of coreference. The key concept of our model is a Semantic Tree – a filecard associated with each discourse entity containing the following fields: • Types: the list of types for mentions of a given entity. For example, if an entity contains the mention “software from India”, the shallow predicate “software” is added to the types. • Attributes: this field collects the premodifiers. For instance, if one of the mentions is “the expensive software” the shallow attribute “expensive” is added to the list of attributes. • Relations: this field"
S10-1021,P08-4003,1,0.868153,"i@fbk.eu Abstract 2 BART Architecture BART (Versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering. For the SemEval task 1 on Coreference Resolution, BART runs have been submitted for German, English, and Italian. BART relies on a maximum entropy-based classifier for pairs of mentions. A novel entitymention approach based on Semantic Trees is at the moment only supported for English. 1 Introduction This paper presents a multilingual coreference resolution system based on BART (Versley et al., 2008). BART is a modular toolkit for coreference resolution that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART has originally been created and tested for English, but its flexible modular architecture ensures its portability to other languages and domains. In SemEval-2010 task 1 on Coreference Resolution, BART has shown reliable performance for English, German and Italian. In our SemEval experiments, we mainly focus on extending BART to cover multiple languages. Given a corpus in a new language, one can re-train BART to obtain baseline"
S10-1021,P94-1019,0,0.0122284,"d Mi in the parse tree. The PartialMorphMatch feature is a substring match with a morphological extension for common nouns. In German the frequent use of noun composition makes a simple string match for common nouns unfeasible. The feature checks for a match between the noun stems of Mi and Mj . We extract the morphology with SMOR/Morphisto (Schmid et al., 2004). The GermanetRelatedness feature uses the Pathfinder library for GermaNet (Finthammer and Cramer, 2008) that computes and discretizes raw scores into three categories of semantic relatedness. In our experiments we use the measure from Wu and Palmer (1994), which has been found to be the best performing on our development data. Italian. We have designed a feature to cover Italian aliasing patterns. A list of company/person designators (e.g., “S.p.a” or “D.ssa”) has been manually crafted. We have collected patterns of name variants for locations. Finally, we have relaxed abbreviation constraints, allowing for lower-case characters in the abbreviations. Our pilot experiments suggest that, although a universal aliasing algorithm is able to resolve some coreference links between NEs, creating a language-specific module boosts the system’s performan"
S10-1027,2005.mtsummit-papers.11,0,0.0121288,"he computation of P R(vi ), where vi is either animal or biotechnology. Finally, following V´eronis (2004), a MST is built with the target word as its root and the root hubs of GM L forming its first level. By using a multilingual graph, we are able to obtain MSTs which contain translation nodes and edges. 3.4 4 Experimental Setting. We submitted two runs for the task (UHD-1 and UHD-2 henceforth). Since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. Europarl (Koehn, 2005) for UHD-1, augmented with the JRC-Acquis corpus (Steinberger et al., 2006) for UHD-21 . Both corpora were tagged and lemmatized with TreeTagger (Schmid, 1994) and word aligned using GIZA++ (Och and Ney, 2003). For German, in order to avoid the sparseness deriving from the high productivity of compounds, we performed a morphological analysis using Morphisto (Zielinski et al., 2009). To build the multilingual graph (Section 3.2), we used a minimum frequency threshold of 2 occurrences for a word to be inserted as a node, and retained only those edges with a weight less or equal to 0.7. After con"
S10-1027,P09-1030,0,0.0193562,"vj ∈ Vl , i 6= j if vi and vj co-occur in Cl then EM L ← EM L ∪ (vi , vj ) return GM L (vs , t, vl ) receives a translation label t. Formally, let Cvs ⊆ Cs be the contexts where vs and w cooccur, and Cvl the word-aligned contexts in language l of Cvs , where vs is translated as vl . Then each edge between nodes vs and vl is labeled with a translation label t (lines 14-15): this includes a translation of w in Cvl , its frequency of translation and the information of whether the translation is monosemous, as found in a multilingual dictionary, i.e. EuroWordNet (Vossen, 1998) and PanDictionary (Mausam et al., 2009). Finally, the multilingual graph is further extended by inserting all possible co-occurrence edges (vi , vj ) ∈ El between the nodes for the target language l (lines 1619, i.e. we apply the step from Section 3.1 to l and Cl ). As a result of the algorithm, the multilingual graph is returned (line 20). The multilingual graph is built based on a wordaligned multilingual parallel corpus and a multilingual dictionary. The pseudocode is presented in Algorithm 1. We start with the monolingual graph from the source language (line 1) and then for each target language l ∈ L in turn, we add the transla"
S10-1027,P10-1023,1,0.877464,"Missing"
S10-1027,J03-1002,0,0.0028271,"el. By using a multilingual graph, we are able to obtain MSTs which contain translation nodes and edges. 3.4 4 Experimental Setting. We submitted two runs for the task (UHD-1 and UHD-2 henceforth). Since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. Europarl (Koehn, 2005) for UHD-1, augmented with the JRC-Acquis corpus (Steinberger et al., 2006) for UHD-21 . Both corpora were tagged and lemmatized with TreeTagger (Schmid, 1994) and word aligned using GIZA++ (Och and Ney, 2003). For German, in order to avoid the sparseness deriving from the high productivity of compounds, we performed a morphological analysis using Morphisto (Zielinski et al., 2009). To build the multilingual graph (Section 3.2), we used a minimum frequency threshold of 2 occurrences for a word to be inserted as a node, and retained only those edges with a weight less or equal to 0.7. After constructing the multilingual graph, we additionally removed those translations with a frequency count lower than 10 (7 in the case of German, due to the large amount of compounds). Finally, the translations gene"
S10-1027,steinberger-etal-2006-jrc,0,\N,Missing
S10-1027,W06-1669,0,\N,Missing
S10-1027,W09-2413,0,\N,Missing
S16-1206,baroni-bernardini-2004-bootcat,0,0.0434616,"awl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is around 300 MB. While Lefever (2015) shows that such small in-domain corpora can be already useful for taxonomy extraction, we assumed that better results can be obtained if bigger dom"
S16-1206,S16-1168,0,0.255599,"Missing"
S16-1206,S16-1205,0,0.309212,"and languages (NL, FR, IT) for the multilingual setting. The BestComp lists the respective best scores across four our competitors. The best scores excluding the baseline are set in boldface. Definitions of the measures are available in Section 4. that improves structure of the resource. These united mechanisms are not used in other submissions to the challenge. The NUIG-UNLP team (Pocostales, 2016) relies on vector directionality in dense word embedding spaces. Such approximation of patterns based on distributional similarity provided good recall, but attained low precision. The QASSIT team (Cleuziou and Moreno, 2016), who ranked second in the competition, uses patterns to extract hypernym candidates, but they rely solely on the Wikipedia. Subsequently, an optimization technique based on genetic algorithms is used to learn the parametrization of a so-called pretopological space, which leads to desired structural properties of the resulting taxonomy. While we use simpler optimization procedure based on supervised learning, TAXI outperforms QASSIT in terms of comparisons with the gold standard. Possible reasons why our method performs better are (1) QASSIT use no substring features, (2) this team relies on s"
S16-1206,goldhahn-etal-2012-building,0,0.0150556,"e use three general purpose corpora in our approach presented in Table 1: Wikipedia, 59G and CommonCrawl1 . 1 https://commoncrawl.org 1321 Wikipedia 59G CommonCrawl FocusedCrawl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is aro"
S16-1206,S15-2152,0,0.0318236,"co-syntactic patterns to harvest hypernyms from the Web. The extracted hypernym relation graph is subsequently pruned. Veraldi et al. (2013) proposed a graph-based algorithm to learn a taxonomy from textual definitions, extracted from a corpus and the Web. An optimal branching algorithm is used to induce a taxonomy. Finally, Bordea et al. (2015) introduced the first shared task on Taxonomy Extraction Evaluation to provide a common ground for evaluation. Six systems participated in the competition. The top system in this challange used features based on substrings and co-occurrence statistics (Grefenstette, 2015). Lefever et al. (2015) reached the second place gathered hypernyms from patterns, substrings and WordNet. Tan et al. (2015) used word embeddings, reaching the third place. 3 Taxonomy Induction Method Our approach is characterized by scalability and simplicity, assuming that being able to process larger input data is more important than the sophisticated extraction inference. Our approach to taxonomy induction takes as input a set of domain terms and general-domain text corpora and outputs a taxonomy. It consists of four steps. Firstly, we crawl domain-specific corpora based on terminology of"
S16-1206,C92-2082,0,0.718669,"some labeled examples might be utilized to tune the extraction and induction process, we avoid relying on structured lexical resources such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2010). We rather envision a situation where a taxonomy shall be induced in a new domain or a new language for which such resources do not Related Work The extraction of taxonomic relationships from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated i"
S16-1206,R11-2017,0,0.141238,"Missing"
S16-1206,P10-1150,0,0.0134154,"ncrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning task is defined as the problem of finding the taxonomy that maximizes the probability of individ1320 Proceedings of SemEval-2016, pages 1320–1327, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics ual relations extracted by the classifiers. Kozareva and Hovy (2010) start from a set of root terms and use Hearst-like lexico-syntactic patterns to harvest hypernyms from the Web. The extracted hypernym relation graph is subsequently pruned. Veraldi et al. (2013) proposed a graph-based algorithm to learn a taxonomy from textual definitions, extracted from a corpus and the Web. An optimal branching algorithm is used to induce a taxonomy. Finally, Bordea et al. (2015) introduced the first shared task on Taxonomy Extraction Evaluation to provide a common ground for evaluation. Six systems participated in the competition. The top system in this challange used fea"
S16-1206,S15-2157,0,0.0549555,"ch presented in Table 1: Wikipedia, 59G and CommonCrawl1 . 1 https://commoncrawl.org 1321 Wikipedia 59G CommonCrawl FocusedCrawl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is around 300 MB. While Lefever (2015) shows th"
S16-1206,P10-1023,1,0.702747,", to adapt the method to a new domain or language, only a small amount of manual labour is needed. 2 1 Introduction In this paper, we describe TAXI – a taxonomy induction method first presented at the SemEval 2016 challenge on Taxonomy Extraction Evaluation (Bordea et al., 2016). We consider taxonomy induction as a process that should – as much as possible – be driven solely on the basis of raw text processing. While some labeled examples might be utilized to tune the extraction and induction process, we avoid relying on structured lexical resources such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2010). We rather envision a situation where a taxonomy shall be induced in a new domain or a new language for which such resources do not Related Work The extraction of taxonomic relationships from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which ar"
S16-1206,S16-1202,0,0.035602,"eline BestComp TAXI 0 0 0 0.009 0.016 0.189 64.28 178.22 64.94 40.50 34.89 1.00 0.009 0.016 0.189 n.a. 0.298 0.625 Table 3: Overall scores obtained by averaging the results over domains (Environment, Science, Food) and languages (NL, FR, IT) for the multilingual setting. The BestComp lists the respective best scores across four our competitors. The best scores excluding the baseline are set in boldface. Definitions of the measures are available in Section 4. that improves structure of the resource. These united mechanisms are not used in other submissions to the challenge. The NUIG-UNLP team (Pocostales, 2016) relies on vector directionality in dense word embedding spaces. Such approximation of patterns based on distributional similarity provided good recall, but attained low precision. The QASSIT team (Cleuziou and Moreno, 2016), who ranked second in the competition, uses patterns to extract hypernym candidates, but they rely solely on the Wikipedia. Subsequently, an optimization technique based on genetic algorithms is used to learn the parametrization of a so-called pretopological space, which leads to desired structural properties of the resulting taxonomy. While we use simpler optimization pro"
S16-1206,L16-1572,1,0.831085,"already useful for taxonomy extraction, we assumed that better results can be obtained if bigger domain-specific corpora are used. We therefore follow a different approach based on focused crawling, where BootCat is used only for initialization of seed URLs. We use the provided taxonomy terms as input for the BootCat method, generate 1,000 random triples, and use the retrieved URLs as a starting point for further crawling. Focused crawling is an extension to standard web crawling where URLs, expected to point to relevant web documents, are prioritized for download (Chakrabarti et al., 1999). Remus and Biemann (2016) introduced a focused crawling approach based on language modeling. The idea is that relevant web documents refer to other relevant web documents, where the relevance of a web document is computed by considering a statistical n-gram language model of a small, initially provided, domain-defining corpus. We provide a domain-defining corpus for each category by using Wikipedia articles, that are directly contained in the matching Wikipedia category. For example, for the the Food domain we used the Wikipedia articles of Category:Foods to build a language model of the Food domain. The language mode"
S16-1206,L16-1056,1,0.0775824,"PatternSim. This system was used to process English and French corpora. It encodes patterns in the form of finite state transducers implemented with the Unitex corpus processor.2 PatternSim relies on 10 English patterns yielding average precision of top 5 extracted semantic relations per word of 0.69 (Panchenko et al., 2012). For French, 9 hypernym extraction patterns are used providing precision at top 5 of 0.63 (Panchenko et al., 2013). WebISA. In addition to PattaMaika and PatternSim, we used a publicly available database of English hypernym relations extracted from the CommonCrawl corpus (Seitner et al., 2016). We used 108 million hypernym relations with frequency above one. This collection of relations was harvested using a regexp-based implementation of 59 patterns collected from the literature. Combination of hypernyms. Result of the extraction are 18 collections of hypernym relations listed in Table 2. Even the huge WebISA collection extracted from tens of terabytes of text does not provide hypernyms for all rare taxonomic terms, such as “ground and whole bean coffee” and “black sesame rice cake”. On the other hand, most of the collections contain many noisy relations. For instance, frequent re"
S16-1206,P06-1101,0,0.455322,"se sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning task is defined as the problem of finding the taxonomy that maximizes the probability of individ1320 Proceedings of SemEval-2016, pages 1320–1327, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics ual relations extracted by the classifiers. Kozareva and Hovy (2010) start from a set of root terms and use Hearst-like lexico-synt"
S16-1206,S15-2155,0,0.132777,"Missing"
S16-1206,S16-1203,0,0.384121,"r taxonomy extraction, we assumed that better results can be obtained if bigger domain-specific corpora are used. We therefore follow a different approach based on focused crawling, where BootCat is used only for initialization of seed URLs. We use the provided taxonomy terms as input for the BootCat method, generate 1,000 random triples, and use the retrieved URLs as a starting point for further crawling. Focused crawling is an extension to standard web crawling where URLs, expected to point to relevant web documents, are prioritized for download (Chakrabarti et al., 1999). Remus and Biemann (2016) introduced a focused crawling approach based on language modeling. The idea is that relevant web documents refer to other relevant web documents, where the relevance of a web document is computed by considering a statistical n-gram language model of a small, initially provided, domain-defining corpus. We provide a domain-defining corpus for each category by using Wikipedia articles, that are directly contained in the matching Wikipedia category. For example, for the the Food domain we used the Wikipedia articles of Category:Foods to build a language model of the Food domain. The language mode"
S16-1206,J13-3007,1,0.79304,"= j; (ti , tj ) ∈ T × T }. The pairs classified using the positive class are added to the taxonomy. nodes with out degree equal to zero, and the presence of cycles. Second, system outputs were compared against the corresponding domain gold standards and performances are evaluated in terms of Fscore. Here precision and recall are based on the number of edges in common with the gold standard taxonomy over the number of system edges and over the number of gold standard edges respectively. To better compare against gold standard taxonomies the task included the evaluation of a cumulative measure (Velardi et al., 2013), namely Cumulative Fowlkes & Mallows Measure (F&M), where the similarity between the system and the reference taxonomies are measured as the combination of the hierarchical cluster similarities. Finally, the organizers performed manual quality assessment to estimate the precision of the hypernyms. To compute this measure, annotators labeled a sample of 100 hypernym relations as correct or wrong. The taxonomy extraction was evaluated on four languages, namely English, Dutch, French and Italian, and three different domains (Food, Science and Environment). A detailed description of the evaluatio"
S16-1206,P09-1031,0,0.0656634,"ips from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning"
S16-1206,S15-2151,1,\N,Missing
S16-2016,A00-2004,0,0.383052,"as paragraphs), many texts, especially on the web, lack any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007). Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013) and a measure ˇ c et al., of semantic relatedn"
S16-2016,N09-1040,0,0.382839,"different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl 125 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 125–130, Berlin, Germany, August 11-12, 2016. and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different s"
S16-2016,P07-1061,0,0.0277584,"me state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert¨oz et al., 2004). Unlike these works, we use the dense semantic representations of words and sentences (i.e., embeddings), which have been shown to outperform sparse semantic vectors on a range of NLP tasks. Also, instead of looking for minimal cuts in the relatedness graph, we exploit the maximal cliques of the relatedness graph be"
S16-2016,P03-1071,0,0.253557,"this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments’ latent vectors. More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA"
S16-2016,P94-1002,0,0.663193,"the best-performing topic modeling-based TS method on a real-world dataset of political manifestos. 2 Related Work Automated text segmentation received a lot of attention in NLP and IR communities due to its usefulness for text summarization and text indexing. Text segmentation can be performed in two different ways, namely (1) with the goal of obtaining linear segmentations (i.e. detecting the sequence of different segments in a text) , or (2) in order to obtain hierarchical segmentations (i.e. defining a structure of subtopics between the detected segments). Like the majority of TS methods (Hearst, 1994; Brants et al., 2002; Misra et al., 2009; Riedl 125 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 125–130, Berlin, Germany, August 11-12, 2016. and Biemann, 2012), in this work we focus on linear segmentation of text, but there is also a solid body of work on hierarchical TS, where each toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments an"
S16-2016,J97-1003,0,0.322462,"gments (e.g., as paragraphs), many texts, especially on the web, lack any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007). Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013) and a measure ˇ c et al., of sema"
S16-2016,P06-1004,0,0.0793795,", whereas Riedl and Biemann (2012) introduced TopicTiling, an LDA-driven extension of Hearst’s TextTiling algorithm where segments are, represented as dense vectors of dominant topics of terms they contain (instead of as sparse term vectors). Riedl and Biemann (2012) show that TopicTiling outperforms at-that-time state-of-the-art methods for unsupervised linear segmentation (Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003; Fragkou et al., 2004; Misra et al., 2009) and that it is also faster than other LDA-based methods (Misra et al., 2009). In the most closely related work to ours, Malioutov and Barzilay (2006) proposed a graphbased TS approach in which they first construct the fully connected graph of sentences, with edges weighted via the cosine similarity between bagof-words sentence vectors, and then run the minimum normalized multiway cut algorithm to obtain the segments. Similarly, Ferret (2007) builds the similarity graph, only between words instead of between sentences, using sparse co-occurrence vectors as semantic representations for words. He then identifies topics by clustering the word similarity graph via the Shared Nearest Neighbor algorithm (Ert¨oz et al., 2004). Unlike these works,"
S16-2016,J02-1002,0,0.74952,"Missing"
S16-2016,W12-3307,0,0.638491,"k any explicit segmentation. Linear text segmentation aims to represent texts as sequences of semantically coherent segments. Besides improving readability and understandability of texts for readers, automated text segmentation is beneficial for NLP and IR tasks such as text summarization (Angheluta et al., 2002; Dias et al., 2007) and passage retrieval (Huang et al., 2003; Dias et al., 2007). Whereas early approaches to unsupervised text segmentation measured the coherence of segments via raw term overlaps between sentences (Hearst, 1997; Choi, 2000), more recent methods (Misra et al., 2009; Riedl and Biemann, 2012) addressed the issue of sparsity of term-based representations by replacing term-vectors with vectors of latent topics. A topical representation of text is, however, merely a vague approximation of its meaning. Considering that the goal of TS is to identify semantically coherent segments, we propose a TS algorithm aiming to directly capture the semantic relatedness between segments, instead of approximating it via topical similarity. We employ word embeddings (Mikolov et al., 2013) and a measure ˇ c et al., of semantic relatedness of short texts (Sari´ 2012) to construct a relatedness graph of"
S16-2016,S12-1060,1,0.717444,"Missing"
S16-2016,P01-1064,0,0.127417,"ch toplevel segment is further broken down (Yaari, 1997; Eisenstein, 2009). Hearst (1994) introduced TextTiling, one of the first unsupervised algorithms for linear text segmentation. She exploits the fact that words tend to be repeated in coherent segments and measures the similarity between paragraphs by comparing their sparse term-vectors. Choi (2000) introduced the probabilistic algorithm using matrix-based ranking and clustering to determine similarities between segments. Galley et al. (2003) combined contentbased information with acoustic cues in order to detect discourse shifts whereas Utiyama and Isahara (2001) and Fragkou et al. (2004) minimized different segmentation cost functions with dynamic programming. The first segmentation approach based on topic modeling (Brants et al., 2002) employed the probabilistic latent semantic analysis (pLSA) to derive latent representations of segments and determined the segmentation based on similarities of segments’ latent vectors. More recent models (Misra et al., 2009; Riedl and Biemann, 2012) employed the latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute the latent topics and displayed superior performance to previous models on standard synthet"
S18-1132,S17-2097,0,0.054092,"Missing"
S18-1132,W15-1506,0,0.216085,"ion extraction. Given abstracts 826 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 826–830 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Outside SemEval, Zelenko et al. (2003) proposed different kernel methods combined with the SVM and Voted Perceptron learning algorithms for extracting person-affiliation and organization-location relations. Another kernel-based approach using different sources of syntactic information was presented by Zhao and Grishman (2005). Our neural approach is inspired by the work of Nguyen and Grishman (2015), who employ a CNN with word embedding and position embedding lookup. Embeddings are concatenated for obtaining positionally and semantically sensitive token representations and then fed into a convolutional layer, followed by a maximum pooling layer and a fully connected feed-forward network with a softmax classifier. 3 3.1 word embeddings pretrained on Wikipedia 2014 and Gigaword 5.1 Experimental Setup. We experimented with two classifiers, namely linear SVM and kNN. Both models were wrapped in a 5-fold cross validation in order to obtain performance estimates on the whole training set. Furt"
S18-1132,S17-2091,0,0.125878,"rpora. SemEval 2018 Task 7 (G´abor et al., 2018) addresses this problem with a shared task on extracting and classifying semantic relations in scientific papers. The task is divided into two subtasks: Related Work A series of supervised systems for extracting keyphrases and relations in scientific publications was presented in the context of the SemEval task 10 in 2017 (Augenstein et al., 2017). In contrast to the present task formulation, the set of relations consisted only of two, namely hyponymy and synonymy. For this task, the best systems were those based on neural approaches. Lee et al. (2017) obtained the highest score by employing a convolutional neural network with a specific embedding layer encoding manually crafted features such as the word, the position of the word and the part-of-speech (POS) tag. The second best system was a neural end-to-end model by Ammar et al. (2017). It predicted the relation types based on a context-sensitive representation of the keyphrases which they obtained by using a variety of information, e.g., entity type embeddings and syntactic and sequential path information generated by a bidirectional Long Short-Term Memory (LSTM) layer. As opposed to the"
S18-1132,P05-1052,0,0.0819647,"sification. This subtask addresses the whole end-to-end pipeline of relation extraction. Given abstracts 826 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 826–830 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Outside SemEval, Zelenko et al. (2003) proposed different kernel methods combined with the SVM and Voted Perceptron learning algorithms for extracting person-affiliation and organization-location relations. Another kernel-based approach using different sources of syntactic information was presented by Zhao and Grishman (2005). Our neural approach is inspired by the work of Nguyen and Grishman (2015), who employ a CNN with word embedding and position embedding lookup. Embeddings are concatenated for obtaining positionally and semantically sensitive token representations and then fed into a convolutional layer, followed by a maximum pooling layer and a fully connected feed-forward network with a softmax classifier. 3 3.1 word embeddings pretrained on Wikipedia 2014 and Gigaword 5.1 Experimental Setup. We experimented with two classifiers, namely linear SVM and kNN. Both models were wrapped in a 5-fold cross validati"
S18-1132,S17-2168,0,0.0196001,"ploying a convolutional neural network with a specific embedding layer encoding manually crafted features such as the word, the position of the word and the part-of-speech (POS) tag. The second best system was a neural end-to-end model by Ammar et al. (2017). It predicted the relation types based on a context-sensitive representation of the keyphrases which they obtained by using a variety of information, e.g., entity type embeddings and syntactic and sequential path information generated by a bidirectional Long Short-Term Memory (LSTM) layer. As opposed to the former systems, the approach of Barik and Marsi (2017), ranked third, was based on manually crafted features and more traditional classifiers such as decision trees and SVMs. 1. Relation Classification. Given an existing relation between two entities and their context, the task is to predict the label of the relation out of the set of possible classes, namely USAGE, RESULT, MODEL-FEATURE, PART WHOLE, TOPIC, COMPARISON. The task is decomposed into two different scenarios according to the data used, namely with either manually annotated entities (1.1) or noisy data with automatically extracted entities (1.2). 2. Relation Extraction and Classificati"
S18-1132,S17-2171,0,0.0463174,"Missing"
S19-2018,P98-1013,0,0.915854,"mEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov"
S19-2018,J05-1004,0,0.383238,"he runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats li"
S19-2018,Q17-1010,0,0.0444771,"e frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dataset contained phrasal verbs, such as fa"
S19-2018,W11-0110,0,0.0286894,"ns for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dataset contained phrasal verbs, such as fall back and buy out, we have considered only the first word in the phrase. If this word is not present in the model v"
S19-2018,D17-1070,0,0.0377806,"each highlighted verb according to the frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dat"
S19-2018,N18-1202,0,0.0333785,"which is usually the predicate. The goal is to label each highlighted verb according to the frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model i"
S19-2018,S18-2016,0,0.0742481,"ures. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embeddings) for grouping verb"
S19-2018,S19-2003,0,0.200367,"e induction is similar to the word sense induction approach by Arefyev et al. (2018), which uses tf–idf-weighted context word embeddings for a shared task on word sense induction by Panchenko et al. (2018). In this unsupervised task, our approach for clustering mainly consists of exploring the effectiveness of already available pre-trained models.1 Main contributions of this paper are: We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to t"
S19-2018,E12-1003,0,0.059716,"Missing"
S19-2018,J93-2004,0,0.0648084,"Missing"
S19-2018,P18-2010,1,0.635096,"on of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embeddings) for grouping verbs to frame type cluster"
S19-2018,W12-1901,0,0.0329549,"th syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embed"
titze-etal-2014-dbpedia,C08-1082,0,\N,Missing
titze-etal-2014-dbpedia,N03-1033,0,\N,Missing
titze-etal-2014-dbpedia,W04-2214,0,\N,Missing
titze-etal-2014-dbpedia,E12-1059,0,\N,Missing
versley-etal-2008-bart-modular,N07-1011,0,\N,Missing
versley-etal-2008-bart-modular,qiu-etal-2004-public,0,\N,Missing
versley-etal-2008-bart-modular,N03-1033,0,\N,Missing
versley-etal-2008-bart-modular,P00-1023,0,\N,Missing
versley-etal-2008-bart-modular,P06-1006,1,\N,Missing
versley-etal-2008-bart-modular,P05-1022,0,\N,Missing
versley-etal-2008-bart-modular,P04-1018,0,\N,Missing
versley-etal-2008-bart-modular,I05-1063,1,\N,Missing
versley-etal-2008-bart-modular,P06-1055,0,\N,Missing
versley-etal-2008-bart-modular,N06-1025,1,\N,Missing
versley-etal-2008-bart-modular,J01-4004,0,\N,Missing
versley-etal-2008-bart-modular,E06-1015,1,\N,Missing
versley-etal-2008-bart-modular,wellner-vilain-2006-leveraging,0,\N,Missing
versley-etal-2008-bart-modular,uryupina-2006-coreference,0,\N,Missing
versley-etal-2008-bart-modular,W00-0730,0,\N,Missing
versley-etal-2008-bart-modular,P05-1045,0,\N,Missing
W05-0633,J02-3001,0,0.12467,"tactic and semantic information. Accordingly, we make use of both clausal, chunk and deep syntactic (tree structure) features, named entity information, as well as statistical representations for lexical item encoding. The set of features and their encoding reflect the necessity of limiting the complexity and dimensionality of the input space. They also provide the classifier with enough information. We explore here the use of a minimal set of compact features for semantic role prediction, and show that a feature-based 2.1.1 Tree node mapping of semantic arguments and named entities Following Gildea & Jurafsky (2002), (i) labels matching more than one constituent due to nonbranching nodes are taken as labels of higher constituents, (ii) in cases of labels with no corresponding parse constituent, these are assigned to the partial match given by the constituent spanning the shortest portion of the sentence beginning at the label’s span left boundary and lying entirely within it. We drop the role or named entity label if such suitable constituent could not be found2 . 1 All other processing steps assume a uniform treatment of both training and test data. 2 The percentage of roles for which no valid tree node"
W05-0633,J05-1004,0,0.0330164,"et. 1 2 System description 2.1 Preprocessing During preprocessing the predicates’ semantic arguments are mapped to the nodes in the parse trees, a set of hand-crafted shallow tree pruning rules are applied, probability distributions for feature representation are generated from training data1 , and feature vectors are extracted. Those are finally fed into the classifier for semantic role classification. Introduction This paper presents a system for the CoNLL 2005 Semantic Role Labeling shared task (Carreras & M`arquez, 2005), which is based on the current release of the English PropBank data (Palmer et al., 2005). For the 2005 edition of the shared task are available both syntactic and semantic information. Accordingly, we make use of both clausal, chunk and deep syntactic (tree structure) features, named entity information, as well as statistical representations for lexical item encoding. The set of features and their encoding reflect the necessity of limiting the complexity and dimensionality of the input space. They also provide the classifier with enough information. We explore here the use of a minimal set of compact features for semantic role prediction, and show that a feature-based 2.1.1 Tree"
W05-0633,J03-4003,0,\N,Missing
W05-0633,W05-0620,0,\N,Missing
W10-2415,W03-0430,0,0.00801887,"ld standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically P ERS, L OC, O RG, M ISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers 93"
W10-2415,P08-1003,0,0.039537,"Missing"
W10-2415,P06-1102,0,0.0194731,"Missing"
W10-2415,U06-1013,0,0.0287566,"ty Recognition and Classification Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto Department of Computational Linguistics Heidelberg University, Germany {ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de Abstract addressed the problem of recognizing and categorizing fine-grained NE classes (such as biologist, composer, or athlete) in an open-domain setting. Fine-grained NERC is expected to be beneficial for a wide spectrum of applications, including Information Retrieval (Mandl and WomserHacker, 2005), Information Extraction (Pas¸ca et al., 2006a) or Question-Answering (Pizzato et al., 2006). However, manually compiling widecoverage gazetteers for fine-grained NE classes is time-consuming and error-prone. Also, without an extrinsic evaluation, it is difficult to define a priori which classes are relevant for a particular domain or task. Finally, prior research in FG-NERC is difficult to evaluate, due to the diversity of NE classes and datasets used. Accordingly, in the interest of a general approach, we address the challenge of capturing a broad range of NE classes at various levels of conceptual granularity. By turning FG-NERC into a widely applicable task, applications are free"
W10-2415,E06-1003,0,0.0545995,"Missing"
W10-2415,W02-2024,0,0.0209751,"tual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically P ERS, L OC, O RG, M ISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers 93 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics 2 Related work fu"
W10-2415,W03-0420,0,0.059724,"nces for a given target class c, we add all instances labeled with the hyponyms of c to I. All other instances (not in that subtree) are labeled as being Outside (O-) a NE. This approach ensures that, for each node, the dataset contains two classes (NE and O) only, and implicitly ‘propagates’ the instances up the tree. As a result, non-leaf nodes that did not have any instance in the original dataset become populated. Also, the classification of classes at higher levels is based on larger datasets. NERC using a MaxEnt tagger Our baseline system is modeled following a Maximum Entropy approach (Bender et al., 2003, inter alia). The MaxEnt model produces a probability for each class label t (the NE tag) of a classification instance, conditioned on its context of occurrence h. This probability is calculated by:   n X 1 λj fj (h, t) (1) exp  P (t|h) = Z(h) j=1 where fj (h, t) is the j-th feature with associated weight λj and Z(h) is a normalization constant to ensure a proper probability distribution.3 Given a word wi to be classified as Beginning, Inside or Outside (IOB) of a NE, we extract as features: 1. Context words. The words occurring within i+2 the context window wi−2 = wi−2 . . . wi+2 . 2. Wo"
W10-2415,E06-1002,0,0.10149,"Missing"
W10-2415,C02-1130,0,0.578916,"Missing"
W10-2415,W03-0425,0,0.0098922,"ure frequency cutoff. The best configuration is found by optimizing the F1 measure on the development data with various feature representations. The chosen features are: 1, 2 (with n = 3), 4, 5, 6, 7 and 8. Evaluation on the test set is performed blindly, using this feature set. The results are presented in Table 2. The MaxEnt labeler achieves performance comparable with the CoNLL-2003 task participants, ranking 12th among the 16 systems participating in the task, with a 2 point margin off the F1 of the most similar system of Bender et al. (2003) and 7 points below the best-performing system (Florian et al., 2003). The former used a relatively complex set of features and different gazetteers extracted from unannotated data. The latter combined four diverse classifiers, namely a robust linear classifier, maximum entropy, transformationbased learning and a hidden Markov model. They used different feature sets, unannotated data and an additional NE tagger. In comparison, our NERC system is simpler and based on a small set of features that can be easily obtained for many languages. Besides, it does not make use of any external resources and still shows state-of-the-art performance on the overall data. 6 6."
W10-2415,D07-1026,0,0.0336034,"Missing"
W10-2415,C08-1034,0,0.0579714,"Missing"
W10-2415,W09-1125,0,0.580182,"Missing"
W10-2415,W00-0730,0,0.110963,"sly associates proper names with their corresponding semantic class. Mapping to the WordNet person domain. In order to perform a hierarchical classification of people, we need a taxonomy for the domain at hand. We achieve this by mapping the extracted class labels to WordNet synsets. In our setting, we map against all synsets found under person#n#1, Pattern-based extraction of NE-concept pairs. NEs are often introduced by so-called appositional structures as in (1), which overtly express which semantic class (here, painter) the NE (Kandinsky) belongs to. Appositions involving 1 We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking. 95 Level #C 1 1 2 29 3 57 4 63 5 37 6 18 7 6 8 2 all 213 which are direct hypernyms of at least one instance in WordNet (CW N pers+Inst ).2 Since our goal is to map class labels to synsets (i.e. our future NE classes), we check each class label candidate against all synonyms contained in the synset. At this point we have to deal with two cases: two extracted class label candidates (synonyms such as doctor, physician) will map to a single synset, while ambiguous class labels (e.g. director) can be mapped to more than one synset. In the latter case, we heuristically"
W10-2415,W02-1006,0,0.0271859,"our baseline MaxEnt tagger is very local, including at most the two preceding and succeeding words. Hence, the classifier is not able to capture informative contextual clues in a larger context. Previous work has related FG-NERC to WSD approaches (Alfonseca and Manandhar, 2002). Accordingly, we investigate two context-sensitive approaches inspired from WSD proposals, which consider a more global context for classification. We first define a new feature set to induce a new MaxEnt model (MaxEnt-B) which only uses lexical features from a larger context window, as used in standard supervised WSD (Lee and Ng, 2002): 1. PoS context. The part-of-speech occurring within the context window posi+3 i−3 = posi−3 . . . posi+3 . 2. Local collocation. Local collocations Cnm surrounding wi . We use C−2,−1 and C1,2 . 3. Content words in surrounding context. We i+3 consider all unigrams in contexts wi−3 = wi−3 . . . wi+3 of wi (crossing sentence boundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation symbols. We define a feature vector of length 10 using the 10 most frequent content words. Given a classification instance, the feature corresponding to token"
W10-2415,W00-0726,0,\N,Missing
W14-5410,N10-1125,0,0.025429,"Missing"
W15-2808,N12-1094,0,0.0546677,"Missing"
W15-2808,P14-5010,0,0.00390836,"n Mountain Ground Grass Vegetation Amount 16 14 13 13 11 11 10 9 8 8 Table 1: Most common 10 labels and entities of test data selection. Figure 3: Figure 1 with segmentation masks. The segments are labeled with: mammal-other, mountain, woman, sand-desert entities), with an average of 6.15 and 7.26, respectively. An overview of object representations in the amount of labels and entities, and their distribution within the test data is given by Table 1. From each of the 39 images we use the textual image segment labels (in the latter referred to as label) and the captions. With Stanford CoreNLP (Manning et al., 2014) we extract the nouns (NN/NNS) from the captions (in the latter referred to as ‘entity’). If a noun is mentioned in plural form (NNS), we use the lemma instead (e.g., horses is stored as horse). The extracted entities and labels are stored and further processed image-wise, so that only links between an image segment and an entity from the corresponding caption can be created. With WordNet and the similarity measure according to WUP (Wu and Palmer, 1994), we calculated the similarity between every label and every entity. A link is stored between the most similar label and entity. Whereas we all"
W15-2808,D14-1162,0,0.0802929,"methods to generate and extract features. Then the score Hθ (Q, S) between a query Q and a segment S, can be obtained by maximizing over M (Lan et al., 2012; Joachims, 2002): Hθ (Q, S) = arg maxM Fθ (Q, S, M ), where θ is the feature vector consisting of at least one feature or a combination of features. We now proceed to describe such features in details. • What constitutes a literal class of imagecaption pair? • Which method or measure is required to classify a pair as being literal? 2.2 Ranking SVM with Textual Features GloVe-based cosine similarity: We use the distributional vectors from Pennington et al. (2014) to provide us with a semantic representation of the captions. For each noun of the caption, the GloVe vector calculated on a pre-trained model (Wikipedia 2014, 300d) is used to calculate semantic similarity as: • Are we able to derive methods and measures to approach the detection of non-literal pairs? • How to differentiate literal, non-literal, and not-related classes from each other? As a first step towards answering these questions, we focus here on detecting literal text-image usages. Therefore, we focus on a dataset of images and captions with literal usages. Our hunch is that the more"
W15-2808,N10-1125,0,0.0342833,"r interdisciplinary work which aims at bringing together processing of visual data such as video and images with NLP and text mining techniques. However, while most of the research efforts so far concentrated on the problem of image-to-text and video-to-text generation – namely, the automatic generation of natural language descriptions of images (Kulkarni et al., 2011; Yang et al., 2011; Gupta et al., 2012), and videos (Das et al., 2013b; Krishnamoorthy et al., 2013) – few researchers focused on the complementary, yet more challenging, task of associating images or videos to arbitrary texts – Feng and Lapata (2010) and Das et 45 for an animal as such, and being able to create the non-literal link between these two topics. In the short term, a necessary step is to develop a model that does not rely on manually defined enrichments of the dataset (e.g., textual labels or segmentation masks). We will accordingly look at ways to perform predictions about regions of interest from the linear SVM and work without the bounding boxes from the dataset. To this end, our dataset needs to be extended, so that we can apply our improved methods also on non-literal imagecaption pairings. In the long term, we need to dir"
W15-2808,J13-2003,0,0.0298821,"Missing"
W15-2808,P94-1019,0,0.100639,"m each of the 39 images we use the textual image segment labels (in the latter referred to as label) and the captions. With Stanford CoreNLP (Manning et al., 2014) we extract the nouns (NN/NNS) from the captions (in the latter referred to as ‘entity’). If a noun is mentioned in plural form (NNS), we use the lemma instead (e.g., horses is stored as horse). The extracted entities and labels are stored and further processed image-wise, so that only links between an image segment and an entity from the corresponding caption can be created. With WordNet and the similarity measure according to WUP (Wu and Palmer, 1994), we calculated the similarity between every label and every entity. A link is stored between the most similar label and entity. Whereas we allow to link multiple segments to one entity. This is done to be able to link multiple instances of one object in an image to the lemmatized entity. To simplify the method with respect to any ambiguity, we used the most frequent sense in WordNet. Overall, the method results in precision of 0.538 and F1 measure of 0.493, thus providing us with a baseline approach with results comparable to the original ones from Weegar et al.. calante et al., 2010)). Each"
W15-2808,W13-1302,0,0.0230581,"ey also show, that a post-processing, e.g., reranking with aggregation can have positive impacts. 4 Related Work Recent years have seen a growing interest for interdisciplinary work which aims at bringing together processing of visual data such as video and images with NLP and text mining techniques. However, while most of the research efforts so far concentrated on the problem of image-to-text and video-to-text generation – namely, the automatic generation of natural language descriptions of images (Kulkarni et al., 2011; Yang et al., 2011; Gupta et al., 2012), and videos (Das et al., 2013b; Krishnamoorthy et al., 2013) – few researchers focused on the complementary, yet more challenging, task of associating images or videos to arbitrary texts – Feng and Lapata (2010) and Das et 45 for an animal as such, and being able to create the non-literal link between these two topics. In the short term, a necessary step is to develop a model that does not rely on manually defined enrichments of the dataset (e.g., textual labels or segmentation masks). We will accordingly look at ways to perform predictions about regions of interest from the linear SVM and work without the bounding boxes from the dataset. To this end,"
W15-2808,D11-1041,0,\N,Missing
W17-1909,E03-1020,0,0.0844048,"hese hybrid methods, our approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pag"
W17-1909,W06-3812,1,0.424857,"approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Val"
W17-1909,N15-1184,0,0.0163787,"based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using t"
W17-1909,N15-1059,0,0.154874,"D) task. In particular, the contribution of this paper is a new unsupervised knowledge-based approach to WSD based on the hybrid aligned resource (HAR) introduced by Faralli et al. (2016). The key difference of our approach from prior hybrid methods based on sense embeddings, e.g. (Rothe and Sch¨utze, 2015), is that we rely on sparse lexical representations that make the sense representation readable and allow to straightforwardly use this representation for word sense disambiguation, as will be shown below. In contrast to hybrid approaches based on sparse interpretable representations, e.g. (Camacho-Collados et al., 2015a), our method requires no mapping of texts to a sense inventory and thus can be applied to larger text collections. By linking symbolic distributional sense representations to lexical resources, we are able to improve representations of senses, leading to performance gains in word sense disambiguation. We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks re"
W17-1909,N13-1092,0,0.0846982,"Missing"
W17-1909,P15-1072,0,0.0758306,"D) task. In particular, the contribution of this paper is a new unsupervised knowledge-based approach to WSD based on the hybrid aligned resource (HAR) introduced by Faralli et al. (2016). The key difference of our approach from prior hybrid methods based on sense embeddings, e.g. (Rothe and Sch¨utze, 2015), is that we rely on sparse lexical representations that make the sense representation readable and allow to straightforwardly use this representation for word sense disambiguation, as will be shown below. In contrast to hybrid approaches based on sparse interpretable representations, e.g. (Camacho-Collados et al., 2015a), our method requires no mapping of texts to a sense inventory and thus can be applied to larger text collections. By linking symbolic distributional sense representations to lexical resources, we are able to improve representations of senses, leading to performance gains in word sense disambiguation. We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks re"
W17-1909,N15-1165,0,0.0236298,"l, 2013). Word Sense Induction. In DTs, entries of polysemous terms are mixed, i.e. they contain related terms of several senses. The Chinese Whispers (Biemann, 2006) graph clustering is applied to the ego-network (Everett and Borgatti, 2005) of the each term, as defined by its related terms and connections between then observed in the DT to derive word sense clusters. Rothe and Sch¨utze (2015) proposed a method that learns sense embedding using word embeddings and the sense inventory of WordNet. The approach was evaluated on the WSD tasks using features based on the learned sense embeddings. Goikoetxea et al. (2015) proposed a method for learning word embeddings using random walks on a graph of a lexical resource. Nieto Pi˜na and Johansson (2016) used a similar approach based on random walks on a WordNet to learn sense embeddings. Labeling Word Senses with Hypernyms. Hearst (1992) patterns are used to extract hypernyms from the corpus. These hypernyms are assigned to senses by aggregating hypernym 73 PCZ ID mouse:0 mouse:1 keyboard:0 keyboard:1 WordNet ID mouse:1 mouse:4 keyboard:1 keyboard:1 Related Terms rat:0, rodent:0, monkey:0, ... keyboard:1, computer:0, printer:0 ... piano:1, synthesizer:2, organ:"
W17-1909,P15-2003,0,0.0258444,"pproach is to train the standard skipgram model on a pre-disambiguated corpus using the Babelfy WSD system (Moro et al., 2014). NASARI (Camacho-Collados et al., 2015a) relies on Wikipedia and WordNet to produce vector representations of senses. In this approach, a sense is represented in lexical or sense-based feature spaces. The links between WordNet and Wikipedia are retrieved from BabelNet. MUFFIN (Camacho-Collados et al., 2015b) adapts several ideas from NASARI, extending the method to the multi-lingual case by using BabelNet synsets instead of monolingual WordNet synsets. The approach of Chen et al. (2015) to learning sense embeddings starts from initialization of sense vectors using WordNet glosses. It proceeds by performing a more conventional context clustering, similar what is found to unsupervised methods such as (Neelakantan et al., 2014; Bartunov et al., 2016). Building a Distributional Thesaurus (DT). At this stage, a similarity graph over terms is induced from a corpus, where each entry consists of the most similar 200 terms for a given term using the JoBimText method (Biemann and Riedl, 2013). Word Sense Induction. In DTs, entries of polysemous terms are mixed, i.e. they contain relat"
W17-1909,C92-2082,0,0.455908,"lated terms and connections between then observed in the DT to derive word sense clusters. Rothe and Sch¨utze (2015) proposed a method that learns sense embedding using word embeddings and the sense inventory of WordNet. The approach was evaluated on the WSD tasks using features based on the learned sense embeddings. Goikoetxea et al. (2015) proposed a method for learning word embeddings using random walks on a graph of a lexical resource. Nieto Pi˜na and Johansson (2016) used a similar approach based on random walks on a WordNet to learn sense embeddings. Labeling Word Senses with Hypernyms. Hearst (1992) patterns are used to extract hypernyms from the corpus. These hypernyms are assigned to senses by aggregating hypernym 73 PCZ ID mouse:0 mouse:1 keyboard:0 keyboard:1 WordNet ID mouse:1 mouse:4 keyboard:1 keyboard:1 Related Terms rat:0, rodent:0, monkey:0, ... keyboard:1, computer:0, printer:0 ... piano:1, synthesizer:2, organ:0 ... keypad:0, mouse:1, screen:1 ... Hypernyms animal:0, species:1, ... device:1, equipment:3, ... instrument:2, device:3, ... device:1, technology:0 ... Context Clues rat:conj and, white-footed:amod, ... click:-prep of, click:-nn, .... play:-dobj, electric:amod, .. co"
W17-1909,S07-1015,0,0.22111,"n context. For each test instance consisting of a target word and its context, we select the sense whose corresponding sense representation has the highest cosine similarity with the target word’s context. improvements in the results. Further expansion of the sense representation with context clues (cf. Table 1) provide a modest further improvement on the SemEval-2007 dataset and yield no further improvement on the case of the Senseval-3 dataset. 4 Comparison to the state-of-the-art. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau, 2008), BabelNet, WN+XWN (Cuadros and Rigau, 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best configuration reported in the original paper (KnowNet-20), which extends each sense with 20 keywords. BabelNet in its core relies on a mapping of WordNet synsets and Wikipedia articles to obtain enriched sense representations. The WN+XWN system is the topranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau, 2007). It alleviates sparsity by combining WordNet with the eXtended WordNet (Mihalcea and"
W17-1909,C08-1021,0,0.0324758,"from the hybrid aligned resource. form WSD in context. For each test instance consisting of a target word and its context, we select the sense whose corresponding sense representation has the highest cosine similarity with the target word’s context. improvements in the results. Further expansion of the sense representation with context clues (cf. Table 1) provide a modest further improvement on the SemEval-2007 dataset and yield no further improvement on the case of the Senseval-3 dataset. 4 Comparison to the state-of-the-art. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau, 2008), BabelNet, WN+XWN (Cuadros and Rigau, 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best configuration reported in the original paper (KnowNet-20), which extends each sense with 20 keywords. BabelNet in its core relies on a mapping of WordNet synsets and Wikipedia articles to obtain enriched sense representations. The WN+XWN system is the topranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau, 2007). It alleviates sparsity by combining Word"
W17-1909,W16-1401,0,0.0467827,"Missing"
W17-1909,N15-1070,0,0.0148651,"al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using the word sense inventory of WordNet. 3 Unsupervised Knowledge-based WSD using Hybrid Aligned Resource We rely on the hybrid aligned lexical semantic resource proposed by Faralli et al. (2016) to perform WSD. We start with a short description of this resource and then discuss how it is used for WSD. 3.1 Construction of the Hybrid Aligned Resource (HAR) The hybrid aligned resource links two lexical semantic networks using the method of Faralli et al. (2016): a corpus-based distributionallyinduced network and a man"
W17-1909,W16-1620,1,0.89163,"Missing"
W17-1909,S10-1011,0,0.0341299,"original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only fine-grained sense annotations. In all experi"
W17-1909,P15-2004,0,0.0159052,"on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using the word sense inventory of WordNet. 3 Unsupervised Knowledge-based WSD using Hybrid Aligned Resource We rely on the hybrid aligned lexical semantic resource"
W17-1909,W04-0807,0,0.198258,"we use the scores reported in the respective original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only"
W17-1909,S07-1016,0,0.019055,"rted in the respective original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only fine-grained sense an"
W17-1909,Q14-1019,0,0.0758595,"s of the lexical resource and relations between them (WordNet ID). Each sense in the PCZ network is subsequently linked to a sense of the knowledgebased network based on their similarity calculated on the basis of lexical representations of senses and their neighbors. The construction of the PCZ involves the following steps (Faralli et al., 2016): Iacobacci et al. (2015) proposed to learn sense embeddings on the basis of the BabelNet lexical ontology (Navigli and Ponzetto, 2012). Their approach is to train the standard skipgram model on a pre-disambiguated corpus using the Babelfy WSD system (Moro et al., 2014). NASARI (Camacho-Collados et al., 2015a) relies on Wikipedia and WordNet to produce vector representations of senses. In this approach, a sense is represented in lexical or sense-based feature spaces. The links between WordNet and Wikipedia are retrieved from BabelNet. MUFFIN (Camacho-Collados et al., 2015b) adapts several ideas from NASARI, extending the method to the multi-lingual case by using BabelNet synsets instead of monolingual WordNet synsets. The approach of Chen et al. (2015) to learning sense embeddings starts from initialization of sense vectors using WordNet glosses. It proceeds"
W17-1909,P15-1173,0,0.0342251,"Missing"
W17-1909,P14-2089,0,0.0231136,"igli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into ac"
W17-1909,D14-1113,0,0.0724977,"mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistic"
W17-1909,P15-1010,0,\N,Missing
W17-2906,S16-2016,1,0.866077,"Missing"
W17-2906,E17-2109,1,0.849705,"Missing"
W17-2906,D14-1162,0,0.0907744,"tion to topically label political texts (Karan et al., 2016; Zirn et al., 2016). Existing classification models utilize discrete representation of text (i.e., bag of words) and can thus exploit only monolingual data (i.e., train and predict same language instances ). In contrast, in this work, we aim to exploit multilingual data – topically-coded CMP manifestos in different languages. We propose a classification model that can be trained on multilingual corpus of political texts.To this effect, we induce semantic representations of texts from ubiquitous word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) and induce a joint multilingual embedding space via the linear translation matrices (Mikolov et al., 2013a). We then experiment with two classification models, support vector machines (SVM) and convolutional neural network (CNN) that use embeddings from the joint multilingual space as input. Experimental results offer evidence that topic classifiers leveraging multilingual training sets outperform monolingual classifiers. In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we ex"
W17-2906,W16-2102,1,0.781964,"Missing"
W17-2906,D14-1181,0,0.0034212,"rmed using the linear translation model proposed by Mikolov et al. (2013a), who observed that there exists a linear translation between embedding spaces independently trained on different corpora. Given a set of N word translations pairs {wsi , wti }N i=1 , we learn a translation matrix M that projects the embedding vectors from the source space to the target space. Let S be the matrix composed of embeddings of all source words wsi Convolutional Neural Network Recently, convolutional neural networks (LeCun and Bengio, 1998, CNN) have yielded best performance on many text classification tasks (Kim, 2014; Severyn and Moschitti, 2015). CNN is a feed-forward neural network consisting of one or more convolution layers. Each convolution layer consists of a set of filters matrices (parameters of the model optimized during training). In text classification, the convolution operation is computed sequentially between each filter matrix and each slice (of the same size as filter) of the embedding matrix representing the input text. Each convolution layer is coupled with a pooling layer, in which only the subset of largest convolution scores produced by each filter is retained and used as input either"
W17-2906,D13-1010,0,0.0198065,"difficult to ensure annotation consistency, especially across different countries and languages (Mikhaylov et al., 2012). Nonetheless, manually coded manifestos remain the crucial data source for studies in computational political science (Lowe et al., 2011; Nanni et al., 2016). 2 Related Work The recent adoption of NLP methods had led to significant advances in the field of Computational Social Science (CSS) (Lazer et al., 2009) and political science in particular (Grimmer and Stewart, 2013). Among other tasks, researchers have addressed the identification of political differences from text (Sim et al., 2013; Menini and Tonelli, 2016), positioning of political entities on a leftright spectrum (Slapin and Proksch, 2008; Glavaˇs et al., 2017), as well as the detection of political events (Nanni et al., 2017) and prominent topics (Lauscher et al., 2016) in political texts. For what concerns the analysis of manifestos, 1 https://manifestoproject.wzb.eu/ coding_schemes/mp_v5 42 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 42–46, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics previous studies have focused"
W17-2906,C16-1232,0,0.0605231,"e annotation consistency, especially across different countries and languages (Mikhaylov et al., 2012). Nonetheless, manually coded manifestos remain the crucial data source for studies in computational political science (Lowe et al., 2011; Nanni et al., 2016). 2 Related Work The recent adoption of NLP methods had led to significant advances in the field of Computational Social Science (CSS) (Lazer et al., 2009) and political science in particular (Grimmer and Stewart, 2013). Among other tasks, researchers have addressed the identification of political differences from text (Sim et al., 2013; Menini and Tonelli, 2016), positioning of political entities on a leftright spectrum (Slapin and Proksch, 2008; Glavaˇs et al., 2017), as well as the detection of political events (Nanni et al., 2017) and prominent topics (Lauscher et al., 2016) in political texts. For what concerns the analysis of manifestos, 1 https://manifestoproject.wzb.eu/ coding_schemes/mp_v5 42 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 42–46, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics previous studies have focused on topical segmentation (Gl"
W17-5030,P15-2011,1,0.880809,"Missing"
W17-5030,C12-1023,0,0.029599,"s and sentences (Just and Carpenter, 1980). A series of studies investigating the effects of word frequency, verb complexity and lexical ambiguity (Juhasz and Rayner, 2003; Rayner et al., 2012), as well as contextual effects on word perception (Ehrlich and Rayner, 1981) concluded that long, rare and ambiguous words are more likely to be fixated longer and their processing requires more cognitive effort from the reader. These are also words that are likely to be replaced with shorter and more frequent ones during lexical simplification aimed at making text more accessible to wider populations (Bott et al., 2012; Glavaˇs and ˇ Stajner, 2015). Eye tracking has also been extensively used for the investigation of reading-related disorders owing to its capacity to provide information about the online processing of the text. For example, aphasic readers show “qualitatively different gaze fixation patterns” when answering comprehension questions (Dickey et al., 2007) and readers with dyslexia have been found to exhibit longer fixation durations and less efficient scanning techniques (Kim and Lombardino, 2016). In spite of the decades-long tradition of using gaze data to investigate word processing among ne"
W17-5030,S12-1066,0,0.0292614,"72 http://alt.qcri.org/semeval2016/task11/ https://simple.wikipedia.org feature (Wr´obel, 2016). In an earlier organised shared task on English Lexical Substitution at SemEval-2012,4 which had the aim of providing a framework for evaluation of lexical simplification systems, for each given sentence containing one target ‘complex’ word and four substitution candidates, participating systems were competing in ranking the four given substitution candidates according to their simplicity, i.e. how easy they are to be understood by fluent but non-native English speakers. The best performing system (Jauhar and Specia, 2012) used a combination of collocational features and four psycholinguistic measures extracted from the MRC (Machine Readable Dictionary) Psycholinguistic Database (Coltheart, 1981): Next, we identify which particular words (in their specific contexts) impose heavier cognitive load on each group of participants by clustering them as challenging or not, based on viewing time of each participant individually (Section 4.1), and then classifying them into four classes depending on the number of participants who found them challenging (Section 4.2). Finally, we investigate the lexical properties which"
W17-5030,N16-1050,0,0.18899,"utistic and non-autistic adolescents while reading individual sentences, suggesting that the reading task imposed an overall heavier cognitive load on the participants from the ASD group. Brock et al. (2008) also used gaze data1 and showed that both the ASD and the control participants were able to use context to successfully dis1.2 Complex Word Identification Complex Word Identification (CWI) task received high attention only recently, with findings suggesting that using a CWI module at the beginning of a lexical simplification (LS) pipeline significantly improves performances of LS systems (Paetzold and Specia, 2016c) and with the recently organised SemEval-2016 CWI shared task.2 The goal of the shared task was building CWI systems which would identify challenging words for non-native English speakers. The dataset consisted of sentences (without context), each with one content word (noun, verb, adjective, or adverb) marked as a target word. The training dataset contained 200 sentences, where each target word was annotated by 20 non-native English speakers as ‘easy’ or ‘complex’, depending on whether they understood its meaning or not. The participants were asked to mark the word as ‘complex’ even if they"
W17-5030,L16-1491,0,0.197677,"utistic and non-autistic adolescents while reading individual sentences, suggesting that the reading task imposed an overall heavier cognitive load on the participants from the ASD group. Brock et al. (2008) also used gaze data1 and showed that both the ASD and the control participants were able to use context to successfully dis1.2 Complex Word Identification Complex Word Identification (CWI) task received high attention only recently, with findings suggesting that using a CWI module at the beginning of a lexical simplification (LS) pipeline significantly improves performances of LS systems (Paetzold and Specia, 2016c) and with the recently organised SemEval-2016 CWI shared task.2 The goal of the shared task was building CWI systems which would identify challenging words for non-native English speakers. The dataset consisted of sentences (without context), each with one content word (noun, verb, adjective, or adverb) marked as a target word. The training dataset contained 200 sentences, where each target word was annotated by 20 non-native English speakers as ‘easy’ or ‘complex’, depending on whether they understood its meaning or not. The participants were asked to mark the word as ‘complex’ even if they"
W17-5030,S16-1146,0,0.027834,"Missing"
W17-6809,W11-4533,0,0.0171607,"ures. STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains. We also show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fa"
W17-6809,S12-1059,0,0.0677119,"Missing"
W17-6809,R11-1055,0,0.106629,"gnitive science clearly suggests that human meaning representations are grounded in our perceptual system and sensori-motor experience (Harnad, 1990; Lakoff and Johnson, 1999; Louwerse, 2011, inter alia), previous STS models relied exclusively on linguistic processing and textual information. To the best of our knowledge, there has not yet been an STS method that leveraged visual information and combined linguistic and visual input into a visually-informed multi-modal STS system. However, such visually-informed models have been successfully used in other tasks such as selectional preferences (Bergsma and Goebel, 2011), detecting semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova et al., 2016), to name only a few. Another important property of visual data is their expected language invariance,1 exploited in recent work on multi-modal modeling in cross-lingual settings (Bergsma and Durme, 2011; Kiela et al., 2015; Vuli´c et al., 2016; Specia et al., 2016). Supported by these findings, in this work we show that our multi-modal STS framework may be straightforwardly ext"
W17-6809,S16-1089,0,0.0314869,"Missing"
W17-6809,S13-1005,0,0.014395,"sed STS Measures In the previous section we explained the different levels at which we may combine visual and linguistic representations. However, we still have to define the actual STS measures that compute similarity scores for given pairs of sentences. We propose two simple unsupervised scores for measuring textual similarity. Both scores are agnostic of the actual modality used: this means that we can swap linguistic, visual, and multi-modal vectors as desired without altering the actual STS measure. Optimal aligment similarity. Following the ideas from successful unsupervised STS models (Han et al., 2013; Sultan et al., 2014), we aim to align words between the two sentences at hand. Aiming to devise language-independent STS models (i.e., language-specific tools that could help better align the words are off-limits), we can resort to word similarity measures as the sole information source guiding the alignment process. This STS measure is based on the optimal alignment between the words of the two input sentences. Given the similarity scores for all pairs of words between the sentences S1 and S2 , we are looking for an alignment {(wSi 1 , wSi 2 )}N i=1 (N is the number of aligned pairs, equal"
W17-6809,S16-1116,0,0.0124176,"show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014;"
W17-6809,P16-4010,0,0.0893966,"in all experiments).2 Example images for the four languages we consider in our experiments (cf. Section 5) are shown in Figure 1. We next run a deep convolutional neural network (CNN) pre-trained on the ImageNet classification task (Russakovsky et al., 2015) and extract the 4096dimensional vector from the pre-softmax layer to represent each image. We opt for the VGG network (Simonyan and Zisserman, 2014) which, according to Kiela et al. (2016), has a slight edge on the two other alternatives – AlexNet (Krizhevsky et al., 2012) and GoogLeNet (Szegedy et al., 2015). We used the MMFeat toolkit (Kiela, 2016) to facilitate the process of image retrieval and CNN-based feature extraction. Visual similarity. Because we retrieve more than one image per word, our visual representation of the word is a set of image embedding vectors. This allows for different visual similarity measures taking as input two sets of image embeddings (Kiela et al., 2015), given in Table 1. 3.3 Multi-Modal Representations In order to compute multi-modal STS scores, one can combine linguistic and visual embeddings of words and sentences in a number of ways. Here, we explore three different levels of combining visual and lingu"
W17-6809,D14-1005,0,0.57417,". These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (Bergsma and Durme, 2011; Kiela et al., 2015). As such, they could serve as a natural cross-language bridge in cross-lingual STS. In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visual information from images alongside linguisti"
W17-6809,P14-2135,0,0.277321,"f similarity scores rather than at the embedding level. Thus, it may be applied both for computing word and sentence similarities. Let sim v be the similarity measure (cf. Section 4) for two words or sentences computed using their visual representations, and let sim t be their similarity computed using their linguistic representations. The late-fusion similarity is then computed as the linear combination of the uni-modal similarities, i.e., as a · sim v + b · sim t . The default late-fusion model uses a = b = 0.5. Selective inclusion of visual information. Previous studies (Hill et al., 2013; Kiela et al., 2014) show that visual signal does not improve the semantic representation equally for all concepts. In fact, the inclusion of visual information deteriorates semantic representations for abstract concepts (e.g., honesty, love, freedom). In order to selectively include the visual information, we need a measure reflecting the quality of the visual signal. To this end, we use the image dispersion score (Kiela et al., 2014). A concept’s image dispersion is the cosine distance between image embeddings, averaged over all pairs of images obtained for the concept w: id (w) = 1 X I(w) 2  1 − cos(ei , ej )"
W17-6809,P15-2020,1,0.923477,"Missing"
W17-6809,D16-1043,0,0.672896,"Missing"
W17-6809,D15-1015,1,0.919942,"Missing"
W17-6809,J99-4009,0,0.936816,") measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). W"
W17-6809,D14-1162,0,0.0807577,"ision, as we were unable to consistently retrieve images for whole sentences as queries. 3.1 Linguistic Representations We use the ubiquitous word embeddings as the linguistic representations of words. Aiming to make our approach language-independent, we opted for embedding models that require nothing but the large corpora as input. Due to the common 1 Using a simple example from Vuli´c et al. (2016), bicycles resemble each other irrespective of whether we call them bicycle, v´elo, fiets, bicicletta, or Fahrrad; see also Fig. 1 3 usage, we chose the Skip-Gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) embeddings. For the cross-lingual STS setting, the words of the two languages have to be projected to the same embedding space. To achieve this, we employ the translation matrix model of Mikolov et al. (2013), who have shown that the linear mapping can be established between independently trained embedding spaces. Given a set of translation pairs {si , ti }ni=1 , si ∈ Rds , ti ∈ Rdt (with ds and dt being the sizes of source and target embeddings, respectively), we obtain the translation matrix M ∈ Rds ×dt by minimizing the sum: n X ksi M − ti k2 i=1 Once learned, the matrix M is used to proje"
W17-6809,J03-3002,0,0.0405059,"al Similarity (STS) measures. STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains. We also show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic"
W17-6809,D13-1115,0,0.125683,"Missing"
W17-6809,S12-1060,1,0.897428,"Missing"
W17-6809,N16-1020,0,0.0485671,"s relied exclusively on linguistic processing and textual information. To the best of our knowledge, there has not yet been an STS method that leveraged visual information and combined linguistic and visual input into a visually-informed multi-modal STS system. However, such visually-informed models have been successfully used in other tasks such as selectional preferences (Bergsma and Goebel, 2011), detecting semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova et al., 2016), to name only a few. Another important property of visual data is their expected language invariance,1 exploited in recent work on multi-modal modeling in cross-lingual settings (Bergsma and Durme, 2011; Kiela et al., 2015; Vuli´c et al., 2016; Specia et al., 2016). Supported by these findings, in this work we show that our multi-modal STS framework may be straightforwardly extended to cross-lingual settings. 3 Multi-Modal Concept Representations Our multi-modal STS measures combine – at different fusion levels – linguistic and visual concept representations. We obtain linguistic and visual r"
W17-6809,D12-1130,0,0.369012,"16; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (Bergsma and Durme, 2011; Kiela et al., 2015). As such, they could serve as a natural cross-language bridge in cross-lingual STS. In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visua"
W17-6809,W16-2346,0,0.0396104,"Missing"
W17-6809,S14-2039,0,0.0157147,"n the previous section we explained the different levels at which we may combine visual and linguistic representations. However, we still have to define the actual STS measures that compute similarity scores for given pairs of sentences. We propose two simple unsupervised scores for measuring textual similarity. Both scores are agnostic of the actual modality used: this means that we can swap linguistic, visual, and multi-modal vectors as desired without altering the actual STS measure. Optimal aligment similarity. Following the ideas from successful unsupervised STS models (Han et al., 2013; Sultan et al., 2014), we aim to align words between the two sentences at hand. Aiming to devise language-independent STS models (i.e., language-specific tools that could help better align the words are off-limits), we can resort to word similarity measures as the sole information source guiding the alignment process. This STS measure is based on the optimal alignment between the words of the two input sentences. Given the similarity scores for all pairs of words between the sentences S1 and S2 , we are looking for an alignment {(wSi 1 , wSi 2 )}N i=1 (N is the number of aligned pairs, equal to the number of words"
W17-6809,P16-2031,1,0.632732,"Missing"
W17-6809,S15-2045,0,\N,Missing
W17-6809,S12-1051,0,\N,Missing
W17-6809,W13-2609,0,\N,Missing
W17-6809,S16-1081,0,\N,Missing
