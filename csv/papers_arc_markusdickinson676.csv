W18-0510,Annotating picture description task responses for content analysis,2018,0,1,2,1,28641,levi king,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Given that all users of a language can be creative in their language usage, the overarching goal of this work is to investigate issues of variability and acceptability in written text, for both non-native speakers (NNSs) and native speakers (NSs). We control for meaning by collecting a dataset of picture description task (PDT) responses from a number of NSs and NNSs, and we define and annotate a handful of features pertaining to form and meaning, to capture the multi-dimensional ways in which responses can vary and can be acceptable. By examining the decisions made in this corpus development, we highlight the questions facing anyone working with learner language properties like variability, acceptability and native-likeness. We find reliable inter-annotator agreement, though disagreements point to difficult areas for establishing a link between form and meaning."
li-dickinson-2017-gender,Gender Prediction for {C}hinese Social Media Data,2017,0,6,2,0,2007,wen li,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Social media provides users a platform to publish messages and socialize with others, and microblogs have gained more users than ever in recent years. With such usage, user profiling is a popular task in computational linguistics and text mining. Different approaches have been used to predict users{'} gender, age, and other information, but most of this work has been done on English and other Western languages. The goal of this project is to predict the gender of users based on their posts on Weibo, a Chinese micro-blogging platform. Given issues in Chinese word segmentation, we explore character and word n-grams as features for this task, as well as using character and word embeddings for classification. Given how the data is extracted, we approach the task on a per-post basis, and we show the difficulties of the task for both humans and computers. Nonetheless, we present encouraging results and point to future improvements."
W16-2020,A Multilinear Approach to the Unsupervised Learning of Morphology,2016,14,0,2,0,33934,anthony meyer,"Proceedings of the 14th {SIGMORPHON} Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,None
W16-0512,Shallow Semantic Reasoning from an Incomplete Gold Standard for Learner Language,2016,26,1,2,1,28641,levi king,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We investigate questions of how to reason about learner meaning in cases where the set of correct meanings is never entirely complete, specifically for the case of picture description tasks (PDTs). To operationalize this, we explore different models of representing and scoring non-native speaker (NNS) responses to a picture, including bags of dependencies, automatically determining the relevant parts of an image from a set of native speaker (NS) responses. In more exploratory work, we examine the variability in both NS and NNS responses, and how different system parameters correlate with the variability. In this way, we hope to provide insight for future system development, data collection, and investigations into learner language."
W16-0523,Cost-Effectiveness in Building a Low-Resource Morphological Analyzer for Learner Language,2016,18,0,2,1,34080,scott ledbetter,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we describe the development of a morphological analyzer for learner Hungarian, outlining extensions to a resourcelight system that can be developed by different types of experts. Specifically, we discuss linguistic rule writing, resource creation, and different system settings, and our evaluation showcases the amount of improvement one gets for differing levels and kinds of effort, enabling other researchers to spend their time and energy as effectively as possible."
W15-1619,On Grammaticality in the Syntactic Annotation of Learner Language,2015,24,3,1,1,28642,markus dickinson,Proceedings of The 9th Linguistic Annotation Workshop,0,"We examine some non-canonical annotation categories that license missing material (ellipses and enumerations). In extending these categories to learner data, the distinctions seem to require an annotator to determine whether a sentence is grammatical or not when deciding between particular analyses. We unpack the assumptions surrounding the annotation of learner language and how these particular phenomena compare to competing analyses, pointing out the implications for annotation practice and second language analysis."
W15-0604,Automatic morphological analysis of learner {H}ungarian,2015,39,1,2,1,34080,scott ledbetter,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we describe a morphological analyzer for learner Hungarian, built upon limited grammatical knowledge of Hungarian. The rule-based analyzer requires very few resources and is flexible enough to do both morphological analysis and error detection, in addition to some unknown word handling. As this is work-in-progress, we demonstrate its current capabilities, some areas where analysis needs to be improved, and an initial foray into how the system output can support the analysis of interlanguage grammars."
W14-3504,Leveraging known Semantics for Spelling Correction,2014,20,2,2,1,28641,levi king,Proceedings of the third workshop on {NLP} for computer-assisted language learning,0,"Focusing on applications for analyzing learner language which evaluate semantic appropriateness and accuracy, we build from previous work which modeled some aspects of interaction, namely a picture description task (PDT), with the goal of integrating a spelling correction component in this context. After parsing a sentence and extracting semantic relations, a surprising number of analysis failures stem from misspellings, deviating from expected input in ways that can be modeled when the content of the interaction is known. We thus explore the use of spelling correction tools and language modeling to correct misspellings that often lead to errors in obtaining semantic forms, and we show that such tools can significantly reduce the number of unanalyzable cases. The work is useful for any context where image descriptions or some expected content is available, but not necessarily expected linguistic forms."
S14-2060,{IUCL}: Combining Information Sources for {S}em{E}val Task 5,2014,14,1,4,0,38986,alex rudnick,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We describe the Indiana University system for SemEval Task 5, the L2 writing assistant task, as well as some extensions to the system that were completed after the main evaluation. Our team submitted translations for all four language pairs in the evaluation, yielding the top scores for English-German. The system is based on combining several information sources to arrive at a final L2 translation for a given L1 text fragment, incorporating phrase tables extracted from bitexts, an L2 language model, a multilingual dictionary, and dependency-based collocational models derived from large samples of targetlanguage text."
W13-1702,Shallow Semantic Analysis of Interactive Learner Sentences,2013,25,9,2,1,28641,levi king,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Focusing on applications for analyzing learner language which evaluate semantic appropriateness and accuracy, we collect data from a task which models some aspects of interaction, namely a picture description task (PDT). We parse responses to the PDT into dependency graphs with an an off-the-shelf parser, then use a decision tree to classify sentences into syntactic types and extract the logical subject, verb, and object, finding 92% accuracy in such extraction. The specific goal in this paper is to examine the challenges involved in extracting these simple semantic representations from interactive learner sentences."
W13-1723,Inter-annotator Agreement for Dependency Annotation of Learner Language,2013,26,9,2,1,37026,marwa ragheb,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper reports on a study of interannotator agreement (IAA) for a dependency annotation scheme designed for learner English. Reliably-annotated learner corpora are a necessary step for the development of POS tagging and parsing of learner language. In our study, three annotators marked several layers of annotation over different levels of learner texts, and they were able to obtain generally high agreement, especially after discussing the disagreements among themselves, without researcher intervention, illustrating the feasibility of the scheme. We pinpoint some of the problems in obtaining full agreement, including annotation scheme vagueness for certain learner innovations, interface design issues, and difficult syntactic constructions. In the process, we also develop ways to calculate agreements for sets of dependencies."
W13-1101,Does Size Matter? Text and Grammar Revision for Parsing Social Media Data,2013,23,7,2,0,13028,mohammad khan,Proceedings of the Workshop on Language Analysis in Social Media,0,"We explore improving parsing social media and other web data by altering the input data, namely by normalizing web text, and by revising output parses. We find that text normalization improves performance, though spell checking has more of a mixed impact. We also find that a very simple tree reviser based on grammar comparisons performs slightly but significantly better than the baseline and well outperforms a machine learning model. The results also demonstrate that, more than the size of the training data, the goodness of fit of the data has a great impact on the parser."
R13-1046,Towards Domain Adaptation for Parsing Web Data,2013,23,6,2,0,13028,mohammad khan,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing."
I13-1199,Detecting and Correcting Learner {K}orean Particle Omission Errors,2013,26,3,2,0,38683,ross israel,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"We detect errors in Korean post-positional particle usage, focusing on optimizing omission detection, as omissions are the single-biggest factor in particle errors for learners of Korean. We also develop a system for predicting the correct choice of a particle. For omission detection, we model the task largely on English grammatical error detection, but employ Korean-specific features and filters; likewise, output analysis and the omission correction system illustrate how unique properties of Korean, such as the distinct types of particles used, need to be accounted for in adapting the system, thereby moving the field one step closer to robust multi-lingual methods."
W12-3617,Developing Learner Corpus Annotation for {K}orean Particle Errors,2012,18,9,2,0,41732,sunhee lee,Proceedings of the Sixth Linguistic Annotation Workshop,0,"We aim to sufficiently define annotation for post-positional particle errors in L2 Korean writing, so that future work on automatic particle error detection can make progress. To achieve this goal, we outline the linguistic properties of Korean particles in learner data. Given the agglutinative nature of Korean and the range of functions of particles, this annotation effort involves issues such as defining the tokens and target forms."
W12-2011,Predicting Learner Levels for Online Exercises of {H}ebrew,2012,21,6,1,1,28642,markus dickinson,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"We develop a system for predicting the level of language learners, using only a small amount of targeted language data. In particular, we focus on learners of Hebrew and predict level based on restricted placement exam exercises. As with many language teaching situations, a major problem is data sparsity, which we account for in our feature selection, learning algorithm, and in the setup. Specifically, we define a two-phase classification process, isolating individual errors and linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features."
W12-2038,Sense-Specific Lexical Information for Reading Assistance,2012,20,5,2,0,42393,soojeong eom,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"To support vocabulary acquisition and reading comprehension in a second language, we have developed a system to display sense-appropriate examples to learners for difficult words. We describe the construction of the system, incorporating word sense disambiguation, and an experiment we conducted testing it on a group of 60 learners of English as a second language (ESL). We show that sense-specific information in an intelligent reading system helps learners in their vocabulary acquisition, even if the sense information contains some noise from automatic processing. We also show that it helps learners, to some extent, with their reading comprehension."
eom-etal-2012-using,Using semi-experts to derive judgments on word sense alignment: a pilot study,2012,22,3,2,0,42393,soojeong eom,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The overall goal of this project is to evaluate the performance of word sense alignment (WSA) systems, focusing on obtaining examples appropriate to language learners. Building a gold standard dataset based on human expert judgments is costly in time and labor, and thus we gauge the utility of using semi-experts in performing the annotation. In an online survey, we present a sense of a target word from one dictionary with senses from the other dictionary, asking for judgments of relatedness. We note the difficulty of agreement, yet the utility in using such results to evaluate WSA work. We find that one's treatment of related senses heavily impacts the results for WSA."
dickinson-ledbetter-2012-annotating,Annotating Errors in a {H}ungarian Learner Corpus,2012,8,13,1,1,28642,markus dickinson,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We are developing and annotating a learner corpus of Hungarian, composed of student journals from three different proficiency levels written at Indiana University. Our annotation marks learner errors that are of different linguistic categories, including phonology, morphology, and syntax, but defining the annotation for an agglutinative language presents several issues. First, we must adapt an analysis that is centered on the morpheme rather than the word. Second, and more importantly, we see a need to distinguish errors from secondary corrections. We argue that although certain learner errors require a series of corrections to reach a target form, these secondary corrections, conditioned on those that come before, are our own adjustments that link the learner's productions to the target form and are not representative of the learner's internal grammar. In this paper, we report the annotation scheme and the principles that guide it, as well as examples illustrating its functionality and directions for expansion."
C12-2094,Defining Syntax for Learner Language Annotation,2012,24,9,2,1,37026,marwa ragheb,Proceedings of {COLING} 2012: Posters,0,"We discuss making syntactic annotation for learner language more precise, by clarifying the properties which the layers of annotation refer to. Building from previous proposals which split linguistic annotation into multiple layers to capture non-canonical properties of learner language, we lay out the questions which must be asked for grammatical annotation and provide some answers. Our investigation points to the layer of distributional syntax being based on properties of the target language (L2) and largely redundant with the other layers. We show, for example, that subcategorization seems to better be able to underspecify annotation for situations where no single correct solution can be found. While this paves the way for applying the annotation to larger corpus efforts, it also represents a significant step in elucidating syntax for non-canonical language."
C12-1038,Problems in Evaluating Grammatical Error Detection Systems,2012,45,31,2,0,2196,martin chodorow,Proceedings of {COLING} 2012,0,"Many evaluation issues for grammatical error detection have previously been overlooked, making it hard to draw meaningful comparisons between different approaches, even when they are evaluated on the same corpus. To begin with, the three-way contingency between a writerxe2x80x99s sentence, the annotatorxe2x80x99s correction, and the systemxe2x80x99s output makes evaluation more complex than in some other NLP tasks, which we address by presenting an intuitive evaluation scheme. Of particular importance to error detection is the skew of the data xe2x80x90 the low frequency of errors as compared to non-errors xe2x80x90 which distorts some traditional measures of performance and limits their usefulness, leading us to recommend the reporting of raw measurements (true positives, false negatives, false positives, true negatives). Other issues that are particularly vexing for error detection focus on defining these raw measurements: specifying the size or scope of an error, properly treating errors as graded rather than discrete phenomena, and counting non-errors. We discuss recommendations for best practices with regard to reporting the results of system evaluation for these cases, recommendations which depend upon making clear onexe2x80x99s assumptions and applications for error detection. By highlighting the problems with current error detection evaluation, the field will be better able to move forward."
W11-2929,Detecting Dependency Parse Errors with Minimal Resources,2011,31,10,1,1,28642,markus dickinson,Proceedings of the 12th International Conference on Parsing Technologies,0,"To detect errors in automatically-obtained dependency parses, we take a grammar-based approach. In particular, we develop methods that incorporate n-grams of different lengths and use information about possible parse revisions. Using our methods allows annotators to focus on problematic parses, with the potential to find over half the parse errors by examining only 20% of the data, as we demonstrate. A key result is that methods using a small gold grammar outperform methods using much larger grammars containing noise. To perform annotation error detection on newly-parsed data, one only needs a small grammar."
W11-1410,Developing Methodology for {K}orean Particle Error Detection,2011,19,6,1,1,28642,markus dickinson,Proceedings of the Sixth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We further work on detecting errors in post-positional particle usage by learners of Korean by improving the training data and developing a complete pipeline of particle selection. We improve the data by filtering non-Korean data and sampling instances to better match the particle distribution. Our evaluation shows that, while the data selection is effective, there is much work to be done with preprocessing and system optimization."
W10-1805,Consistency Checking for Treebank Alignment,2010,16,0,1,1,28642,markus dickinson,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This paper explores ways to detect errors in aligned corpora, using very little technology. In the first method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping. Treating the target string as a label, we examine each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways."
W10-1502,Building a {K}orean Web Corpus for Analyzing Learner Language,2010,18,5,1,1,28642,markus dickinson,Proceedings of the {NAACL} {HLT} 2010 Sixth Web as Corpus Workshop,0,"Post-positional particles are a significant source of errors for learners of Korean. Following methodology that has proven effective in handling English preposition errors, we are beginning the process of building a machine learner for particle error detection in L2 Korean writing. As a first step, however, we must acquire data, and thus we present a methodology for constructing large-scale corpora of Korean from the Web, exploring the feasibility of building corpora appropriate for a given topic and grammatical construction."
P10-1075,Detecting Errors in Automatically-Parsed Dependency Relations,2010,22,16,1,1,28642,markus dickinson,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We outline different methods to detect errors in automatically-parsed dependency corpora, by comparing so-called dependency rules to their representation in the training data and flagging anomalous ones. By comparing each new rule to every relevant rule from training, we can identify parts of parse trees which are likely erroneous. Even the relatively simple methods of comparison we propose show promise for speeding up the annotation process."
dickinson-jochim-2010-evaluating,Evaluating Distributional Properties of Tagsets,2010,25,2,1,1,28642,markus dickinson,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We investigate which distributional properties should be present in a tagset by examining different mappings of various current part-of-speech tagsets, looking at English, German, and Italian corpora. Given the importance of distributional information, we present a simple model for evaluating how a tagset mapping captures distribution, specifically by utilizing a notion of frames to capture the local context. In addition to an accuracy metric capturing the internal quality of a tagset, we introduce a way to evaluate the external quality of tagset mappings so that we can ensure that the mapping retains linguistically important information from the original tagset. Although most of the mappings we evaluate are motivated by linguistic concerns, we also explore an automatic, bottom-up way to define mappings, to illustrate that better distributional mappings are possible. Comparing our initial evaluations to POS tagging results, we find that more distributional tagsets can sometimes result in worse accuracy, underscring the need to carefully define the properties of a tagset."
C10-1030,Generating Learner-Like Morphological Errors in {R}ussian,2010,20,11,1,1,28642,markus dickinson,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"To speed up the process of categorizing learner errors and obtaining data for languages which lack error-annotated data, we describe a linguistically-informed method for generating learner-like morphological errors, focusing on Russian. We outline a procedure to select likely errors, relying on guiding stem and suffix combinations from a segmented lexicon to match particular error categories and relying on grammatical information from the original context."
W09-0905,Categorizing Local Contexts as a Step in Grammatical Category Induction,2009,25,1,1,1,28642,markus dickinson,Proceedings of the {EACL} 2009 Workshop on Cognitive Aspects of Computational Language Acquisition,0,"Building on the use of local contexts, or frames, for human category acquisition, we explore the treatment of contexts as categories. This allows us to examine and evaluate the categorical properties that local unsupervised methods can distinguish and their relationship to corpus POS tags. From there, we use lexical information to combine contexts in a way which preserves the intended category, providing a platform for grammatical category induction."
E09-1023,Correcting Dependency Annotation Errors,2009,26,5,1,1,28642,markus dickinson,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Building on work detecting errors in dependency annotation, we set out to correct local dependency errors. To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning. For the task, we define a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally."
W08-0901,Developing Online {ICALL} Resources for {R}ussian,2008,2,0,1,1,28642,markus dickinson,Proceedings of the Third Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
P08-1042,Ad Hoc Treebank Structures,2008,20,7,1,1,28642,markus dickinson,Proceedings of ACL-08: HLT,1,"We outline the problem of ad hoc rules in treebanks, rules used for specific constructions in one data set and unlikely to be used again. These include ungeneralizable rules, erroneous rules, rules for ungrammatical text, and rules which are not consistent with the rest of the annotation scheme. Based on a simple notion of rule equivalence and on the idea of finding rules unlike any others, we develop two methods for detecting ad hoc rules in flat treebanks and show they are successful in detecting such rules. This is done by examining evidence across the grammar and without making any reference to context."
dickinson-lee-2008-detecting,Detecting Errors in Semantic Annotation,2008,28,8,1,1,28642,markus dickinson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We develop a method for detecting errors in semantic predicate-argument annotation, based on the variation n-gram error detection method. After establishing an appropriate data representation, we detect inconsistencies by searching for identical text with varying annotation. By remaining data-driven, we are able to detect inconsistencies arising from errors at lower layers of annotation."
dickinson-jochim-2008-simple,A Simple Method for Tagset Comparision,2008,18,5,1,1,28642,markus dickinson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Based on the idea that local contexts predict the same basic category across a language, we develop a simple method for comparing tagsets across corpora. The principle differences between tagsets are evidenced by variation in categories in one corpus in the same contexts where another corpus exhibits only a single tag. Such mismatches highlight differences in the definitions of tags which are crucial when porting technology from one annotation scheme to another."
C08-1026,Representations for category disambiguation,2008,18,5,1,1,28642,markus dickinson,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"As it serves as a basis for POS tagging, category induction, and human category acquisition, we investigate the information needed to disambiguate a word in a local context, when using corpus categories. Specifically, we increase the recall of an error detection method by abstracting the word to be disambiguated to a representation containing information about some of its inherent properties, namely the set of categories it can potentially have. This work thus provides insights into the relation of corpus categories to categories derived from local contexts."
E06-1034,From Detecting Errors to Automatically Correcting Them,2006,15,5,1,1,28642,markus dickinson,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Faced with the problem of annotation errors in part-of-speech (POS) annotated corpora, we develop a method for automatically correcting such errors. Building on top of a successful error detection method, we first try correcting a corpus using two off-the-shelf POS taggers, based on the idea that they enforce consistency; with this, we find some improvement. After some discussion of the tagging process, we alter the tagging model to better account for problematic tagging distinctions. This modification results in significantly improved performance, reducing the error rate of the corpus."
W05-0103,{``}Language and {C}omputers{''}: Creating an Introduction for a General Undergraduate Audience,2005,3,1,2,0,22390,chris brew,Proceedings of the Second {ACL} Workshop on Effective Tools and Methodologies for Teaching {NLP} and {CL},0,"This paper describes the creation of Language and Computers, a new course at the Ohio State University designed to be a broad overview of topics in computational linguistics, focusing on applications which have the most immediate relevance to students. This course satisfies the mathematical and logical analysis requirement at Ohio State by using natural language systems to motivate students to exercise and develop a range of basic skills in formal and computational analysis. In this paper we discuss the design of the course, focusing on the success we have had in offering it, as well as some of the difficulties we have faced."
P05-1040,Detecting Errors in Discontinuous Structural Annotation,2005,15,26,1,1,28642,markus dickinson,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Consistency of corpus annotation is an essential property for the many uses of annotated corpora in computational and theoretical linguistics. While some research addresses the detection of inconsistencies in positional annotation (e.g., part-of-speech) and continuous structural annotation (e.g., syntactic constituency), no approach has yet been developed for automatically detecting annotation errors in discontinuous structural annotation. This is significant since the annotation of potentially discontinuous stretches of material is increasingly relevant, from tree-banks for free-word order languages to semantic and discourse annotation.In this paper we discuss how the variation n-gram error detection approach (Dickinson and Meurers, 2003a) can be extended to discontinuous structural annotation. We exemplify the approach by showing how it successfully detects errors in the syntactic annotation of the German TIGER corpus (Brants et al., 2002)."
E03-1068,Detecting Errors in Part-of-Speech Annotation,2003,17,85,1,1,28642,markus dickinson,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose a new method for detecting errors in gold-standard part-of-speech annotation. The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings. Two further techniques, closed-class analysis and finite-state tagging guide patterns, are discussed. The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Tree-bank."
