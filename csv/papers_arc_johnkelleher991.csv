2021.mwe-1.7,Finding {BERT}{'}s Idiomatic Key,2021,-1,-1,2,0,4929,vasudevan nedumpozhimana,Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021),0,"Sentence embeddings encode information relating to the usage of idioms in a sentence. This paper reports a set of experiments that combine a probing methodology with input masking to analyse where in a sentence this idiomatic information is taken from, and what form it takes. Our results indicate that BERT{'}s idiomatic key is primarily found within an idiomatic expression, but also draws on information from the surrounding context. Also, BERT can distinguish between the disruption in a sentence caused by words missing and the incongruity caused by idiomatic usage."
2021.emnlp-main.648,Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods,2021,-1,-1,2,0,9951,peru bhardwaj,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model{'}s predictions on test instances. We use these influential triples as adversarial deletions. We further propose a heuristic method to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62{\%} over the baselines."
2021.acl-long.147,Poisoning Knowledge Graph Embeddings via Relation Inference Patterns,2021,-1,-1,2,0,9951,peru bhardwaj,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs. To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph. Specifically, to degrade the model{'}s prediction confidence on target facts, we propose to improve the model{'}s prediction confidence on a set of decoy facts. Thus, we craft adversarial additions that can improve the model{'}s prediction confidence on decoy facts through different inference patterns. Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets. We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern."
2020.spnlp-1.5,Energy-based Neural Modelling for Large-Scale Multiple Domain Dialogue State Tracking,2020,-1,-1,3,1,14543,anh trinh,Proceedings of the Fourth Workshop on Structured Prediction for NLP,0,"Scaling up dialogue state tracking to multiple domains is challenging due to the growth in the number of variables being tracked. Furthermore, dialog state tracking models do not yet explicitly make use of relationships between dialogue variables, such as slots across domains. We propose using energy-based structure prediction methods for large-scale dialogue state tracking task in two multiple domain dialogue datasets. Our results indicate that: (i) modelling variable dependencies yields better results; and (ii) the structured prediction output aligns with the dialogue slot-value constraint principles. This leads to promising directions to improve state-of-the-art models by incorporating variable dependencies into their prediction process."
2020.lrec-1.602,{E}nglish {W}ord{N}et Random Walk Pseudo-Corpora,2020,-1,-1,4,1,17865,filip klubivcka,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This is a resource description paper that describes the creation and properties of a set of pseudo-corpora generated artificially from a random walk over the English WordNet taxonomy. Our WordNet taxonomic random walk implementation allows the exploration of different random walk hyperparameters and the generation of a variety of different pseudo-corpora. We find that different combinations of parameters result in varying statistical properties of the generated pseudo-corpora. We have published a total of 81 pseudo-corpora that we have used in our previous research, but have not exhausted all possible combinations of hyperparameters, which is why we have also published a codebase that allows the generation of additional WordNet taxonomic pseudo-corpora as needed. Ultimately, such pseudo-corpora can be used to train taxonomic word embeddings, as a way of transferring taxonomic knowledge into a word embedding space."
2020.coling-main.174,Language-Driven Region Pointer Advancement for Controllable Image Captioning,2020,-1,-1,3,0,21265,annika lindh,Proceedings of the 28th International Conference on Computational Linguistics,0,"Controllable Image Captioning is a recent sub-field in the multi-modal task of Image Captioning wherein constraints are placed on which regions in an image should be described in the generated natural language caption. This puts a stronger focus on producing more detailed descriptions, and opens the door for more end-user control over results. A vital component of the Controllable Image Captioning architecture is the mechanism that decides the timing of attending to each region through the advancement of a region pointer. In this paper, we propose a novel method for predicting the timing of region pointer advancement by treating the advancement step as a natural part of the language structure via a NEXT-token, motivated by a strong correlation to the sentence structure in the training data. We find that our timing agrees with the ground-truth timing in the Flickr30k Entities test data with a precision of 86.55{\%} and a recall of 97.92{\%}. Our model implementing this technique improves the state-of-the-art on standard captioning metrics while additionally demonstrating a considerably larger effective vocabulary size."
2020.coling-main.197,Style versus Content: A distinction without a (learnable) difference?,2020,-1,-1,4,0,21292,somayeh jafaritazehjani,Proceedings of the 28th International Conference on Computational Linguistics,0,"Textual style transfer involves modifying the style of a text while preserving its content. This assumes that it is possible to separate style from content. This paper investigates whether this separation is possible. We use sentiment transfer as our case study for style transfer analysis. Our experimental methodology frames style transfer as a multi-objective problem, balancing style shift with content preservation and fluency. Due to the lack of parallel data for style transfer we employ a variety of adversarial encoder-decoder networks in our experiments. Also, we use of a probing methodology to analyse how these models encode style-related features in their latent spaces. The results of our experiments which are further confirmed by a human evaluation reveal the inherent trade-off between the multiple style transfer objectives which indicates that style cannot be usefully separated from content within these style-transfer systems."
W19-5910,Capturing Dialogue State Variable Dependencies with an Energy-based Neural Dialogue State Tracker,2019,0,0,3,1,14543,anh trinh,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"Dialogue state tracking requires the population and maintenance of a multi-slot frame representation of the dialogue state. Frequently, dialogue state tracking systems assume independence between slot values within a frame. In this paper we argue that treating the prediction of each slot value as an independent prediction task may ignore important associations between the slot values, and, consequently, we argue that treating dialogue state tracking as a structured prediction problem can help to improve dialogue state tracking performance. To support this argument, the research presented in this paper is structured into three stages: (i) analyzing variable dependencies in dialogue data; (ii) applying an energy-based methodology to model dialogue state tracking as a structured prediction task; and (iii) evaluating the impact of inter-slot relationships on model performance. Overall we demonstrate that modelling the associations between target slots with an energy-based formalism improves dialogue state tracking performance in a number of ways."
W19-4109,Energy-Based Modelling for Dialogue State Tracking,2019,0,0,3,1,14543,anh trinh,Proceedings of the First Workshop on NLP for Conversational AI,0,"The uncertainties of language and the complexity of dialogue contexts make accurate dialogue state tracking one of the more challenging aspects of dialogue processing. To improve state tracking quality, we argue that relationships between different aspects of dialogue state must be taken into account as they can often guide a more accurate interpretation process. To this end, we present an energy-based approach to dialogue state tracking as a structured classification task. The novelty of our approach lies in the use of an energy network on top of a deep learning architecture to explore more signal correlations between network variables including input features and output labels. We demonstrate that the energy-based approach improves the performance of a deep learning dialogue state tracker towards state-of-the-art results without the need for many of the other steps required by current state-of-the-art methods."
W19-3904,Multi-Element Long Distance Dependencies: Using {SP}k Languages to Explore the Characteristics of Long-Distance Dependencies,2019,0,0,2,0,17867,abhijit mahalunkar,Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges,0,"In order to successfully model Long Distance Dependencies (LDDs) it is necessary to under-stand the full-range of the characteristics of the LDDs exhibited in a target dataset. In this paper, we use Strictly k-Piecewise languages to generate datasets with various properties. We then compute the characteristics of the LDDs in these datasets using mutual information and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden strings, and (v) dataset size. This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs. This leads us to the challenge of modelling multi-element long-distance dependencies. Our results suggest that attention mechanisms in neural networks may aide in modeling datasets with multi-element long-distance dependencies. However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue."
R19-1121,Persistence pays off: Paying Attention to What the {LSTM} Gating Mechanism Persists,2019,0,1,2,1,25356,giancarlo salton,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Recurrent Neural Network Language Models composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results in Language Modeling. However, these models still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading. In this paper we demonstrate an effective mechanism for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information."
R19-1150,Bigger versus Similar: Selecting a Background Corpus for First Story Detection Based on Distributional Similarity,2019,0,0,3,0,7254,fei wang,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"The current state of the art for First Story Detection (FSD) are nearest neighbour-based models with traditional term vector representations; however, one challenge faced by FSD models is that the document representation is usually defined by the vocabulary and term frequency from a background corpus. Consequently, the ideal background corpus should arguably be both large-scale to ensure adequate term coverage, and similar to the target domain in terms of the language distribution. However, given these two factors cannot always be mutually satisfied, in this paper we examine whether the distributional similarity of common terms is more important than the scale of common terms for FSD. As a basis for our analysis we propose a set of metrics to quantitatively measure the scale of common terms and the distributional similarity between corpora. Using these metrics we rank different background corpora relative to a target corpus. We also apply models based on different background corpora to the FSD task. Our results show that term distributional similarity is more predictive of good FSD performance than the scale of common terms; and, thus we demonstrate that a smaller recent domain-related corpus will be more suitable than a very large-scale general corpus for FSD."
2019.gwc-1.18,"Synthetic, yet natural: Properties of {W}ord{N}et random walk corpora and the impact of rare words on embedding performance",2019,0,0,4,1,17865,filip klubivcka,Proceedings of the 10th Global Wordnet Conference,0,"Creating word embeddings that reflect semantic relationships encoded in lexical knowledge resources is an open challenge. One approach is to use a random walk over a knowledge graph to generate a pseudo-corpus and use this corpus to train embeddings. However, the effect of the shape of the knowledge graph on the generated pseudo-corpora, and on the resulting word embeddings, has not been studied. To explore this, we use English WordNet, constrained to the taxonomic (tree-like) portion of the graph, as a case study. We investigate the properties of the generated pseudo-corpora, and their impact on the resulting embeddings. We find that the distributions in the psuedo-corpora exhibit properties found in natural corpora, such as Zipf{'}s and Heaps{'} law, and also observe that the proportion of rare words in a pseudo-corpus affects the performance of its embeddings on word similarity."
W18-1401,Exploring the Functional and Geometric Bias of Spatial Relations Using Neural Language Models,2018,0,2,3,0.540541,12367,simon dobnik,Proceedings of the First International Workshop on Spatial Language Understanding,0,"The challenge for computational models of spatial descriptions for situated dialogue systems is the integration of information from different modalities. The semantics of spatial descriptions are grounded in at least two sources of information: (i) a geometric representation of space and (ii) the functional interaction of related objects that. We train several neural language models on descriptions of scenes from a dataset of image captions and examine whether the functional or geometric bias of spatial descriptions reported in the literature is reflected in the estimated perplexity of these models. The results of these experiments have implications for the creation of models of spatial lexical semantics for human-robot dialogue systems. Furthermore, they also provide an insight into the kinds of the semantic knowledge captured by neural language models trained on spatial descriptions, which has implications for image captioning systems."
L18-1317,Is it worth it? Budget-related evaluation metrics for model selection,2018,0,0,3,1,17865,filip klubivcka,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
salton-etal-2017-idiom,Idiom Type Identification with Smoothed Lexical Features and a Maximum Margin Classifier,2017,22,1,3,1,25356,giancarlo salton,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"In our work we address limitations in the state-of-the-art in idiom type identification. We investigate different approaches for a lexical fixedness metric, a component of the state-of the-art model. We also show that our Machine Learning based approach to the idiom type identification task achieves an F1-score of 0.85, an improvement of 11 points over the state-of the-art."
I17-1045,Attentive Language Models,2017,11,10,3,1,25356,giancarlo salton,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this paper, we extend Recurrent Neural Network Language Models (RNN-LMs) with an attention mechanism. We show that an {``}attentive{''} RNN-LM (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs. We also show that an {``}attentive{''} RNN-LM needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset."
P16-1019,Idiom Token Classification using Sentential Distributed Semantics,2016,24,2,3,1,25356,giancarlo salton,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Idiom token classification is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase. In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification. We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification. Importantly, however, our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context. We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier."
W14-5401,The Effect of Sensor Errors in Situated Human-Computer Dialogue,2014,8,2,2,0,38236,niels schutte,Proceedings of the Third Workshop on Vision and Language,0,"Errors in perception are a problem for computer systems that use sensors to perceive the environment. If a computer system is engaged in dialogue with a human user, these problems in perception lead to problems in the dialogue. We present two experiments, one in which participants interact through dialogue with a robot with perfect perception to fulfil a simple task, and a second one in which the robot is affected by sensor errors and compare the resulting dialogues to determine whether the sensor problems have an impact on dialogue success."
W14-5405,Exploration of functional semantics of prepositions from corpora of descriptions of visual scenes,2014,10,3,2,0.540541,12367,simon dobnik,Proceedings of the Third Workshop on Vision and Language,0,"We present a method of extracting functional semantic knowledge from corpora of descriptions of visual scenes. Such knowledge is required for interpretation and generation of spatial descriptions in tasks such as visual search. We identify semantic classes of target and landmark objects related by each preposition by abstracting over WordNet taxonomy. The inclusion of such knowledge in visual search should equip robots with a better, more human-like spatial cognition."
W14-1007,An Empirical Study of the Impact of Idioms on Phrase Based Statistical Machine Translation of {E}nglish to {B}razilian-{P}ortuguese,2014,12,9,3,1,25356,giancarlo salton,Proceedings of the 3rd Workshop on Hybrid Approaches to Machine Translation ({H}y{T}ra),0,This paper describes an experiment to evaluate the impact of idioms on Statistical Machine Translation (SMT) process using the language pair English/BrazilianPortuguese. Our results show that on sentences containing idioms a standard SMT system achieves about half the BLEU score of the same system when applied to sentences that do not contain idioms. We also provide a short error analysis and outline our planned work to overcome this limitation.
W14-0806,Evaluation of a Substitution Method for Idiom Transformation in Statistical Machine Translation,2014,13,4,3,1,25356,giancarlo salton,Proceedings of the 10th Workshop on Multiword Expressions ({MWE}),0,"We evaluate a substitution based technique for improving Statistical Machine Translation performance on idiomatic multiword expressions. The method operates by performing substitution on the original idiom with its literal meaning before translation, with a second substitution step replacing literal meanings with idioms following translation. We detail our approach, outline our implementation and provide an evaluation of the method for the language pair English/Brazilian-Portuguese. Our results show improvements in translation accuracy on sentences containing either morphosyntactically constrained or unconstrained idioms. We discuss the consequences of our results and outline potential extensions to this process."
S14-2037,{DIT}: Summarisation and Semantic Expansion in Evaluating Semantic Similarity,2014,5,1,2,0,38964,magdalena kacmajor,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper describes an approach to implementing a tool for evaluating semantic similarity. We investigated the potential benefits of (1) using text summarisation to narrow down the comparison to the most important concepts in both texts, and (2) leveraging WordNet information to increase usefulness of cosine comparisons of short texts. In our experiments, text summarisation using a graph-based algorithm did not prove to be helpful. Semantic and lexical expansion based upon word relationships defined in WordNet increased the agreement of cosine similarity values with human similarity judgements."
S14-2109,{TCDSCSS}: Dimensionality Reduction to Evaluate Texts of Varying Lengths - an {IR} Approach,2014,1,1,3,0,23570,arun jayapal,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper provides system description of the cross-level semantic similarity task for the SEMEVAL-2014 workshop. Crosslevel semantic similarity measures the degree of relatedness between texts of varying lengths such as Paragraph to Sentence and Sentence to Phrase. Latent Semantic Analysis was used to evaluate the cross-level semantic relatedness between the texts to achieve above baseline scores, tested on the training and test datasets. We also tried using a bag-of-vectors approach to evaluate the semantic relatedness. This bag-of-vectors approach however did not produced encouraging results."
J09-2005,Applying Computational Models of Spatial Prepositions to Visually Situated Dialog,2009,48,54,1,1,4930,john kelleher,Computational Linguistics,0,"This article describes the application of computational models of spatial prepositions to visually situated dialog systems. In these dialogs, spatial prepositions are important because people often use them to refer to entities in the visual context of a dialog. We first describe a generic architecture for a visually situated dialog system and highlight the interactions between the spatial cognition module, which provides the interface to the models of prepositional semantics, and the other components in the architecture. Following this, we present two new computational models of topological and projective spatial prepositions. The main novelty within these models is the fact that they account for the contextual effect which other distractor objects in a visual scene can have on the region described by a given preposition. We next present psycholinguistic tests evaluating our approach to distractor interference on prepositional semantics, and illustrate how these models are used for both interpretation and generation of prepositional expressions."
W08-1136,"Referring Expression Generation Challenge 2008 {DIT} System Descriptions ({DIT}-{FBI}, {DIT}-{TVAS}, {DIT}-{CBSR}, {DIT}-{RBR}, {DIT}-{FBI}-{CBSR}, {DIT}-{TVAS}-{RBR})",2008,2,1,1,1,4930,john kelleher,Proceedings of the Fifth International Natural Language Generation Conference,0,None
W06-2101,Spatial Prepositions in Context: The Semantics of near in the Presence of Distractor Objects,2006,13,12,2,0,45609,fintan costello,Proceedings of the Third {ACL}-{SIGSEM} Workshop on Prepositions,0,"The paper examines how people's judgements of proximity between two objects are influenced by the presence of a third object. In an experiment participants were presented with images containing three shapes in different relative positions, and asked to rate the acceptability of a locative expression such as 'the circle is near the triangle' as descriptions of those images. The results showed an interaction between the relative positions of objects and the linguistic roles that those objects play in the locative expression: proximity was a decreasing function of the distance between the head object in the expression and the prepositional clause object, and an increasing function the distance between the head and the third, distractor object. This finding leads us to a new account for the semantics of spatial prepositions such as near."
P06-1094,Proximity in Context: An Empirically Grounded Computational Model of Proximity for Processing Topological Spatial Expressions,2006,21,26,1,1,4930,john kelleher,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"The paper presents a new model for context dependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene. The paper discusses novel psycholinguistic experimental data that tests and verifies the model. The model has been implemented, and enables a conversational robot to identify objects in a scene through topological spatial relations (e.g. X near Y). The model can help motivate the choice between topological and projective prepositions."
P06-1131,Incremental Generation of Spatial Referring Expressions in Situated Dialog,2006,30,71,1,1,4930,john kelleher,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents an approach to incrementally generating locative expressions. It addresses the issue of combinatorial explosion inherent in the construction of relational context models by: (a) contextually defining the set of objects in the context that may function as a landmark, and (b) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations, and visual and discourse salience."
W05-1607,A Context-dependent Algorithm for Generating Locative Expressions in Physically Situated Environments,2005,26,13,1,1,4930,john kelleher,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"This paper presents a framework for generating locative expressions. The framework addresses the issue of combinatorial explosion inherent in the construction of relational context models by: (a) contextually defining the set of objects in the context that may function as a landmark, and (b) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations."
